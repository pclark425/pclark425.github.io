<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8629 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8629</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8629</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-154.html">extraction-schema-154</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-278481056</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2505.05515v1.pdf" target="_blank">Nature's Insight: A Novel Framework and Comprehensive Analysis of Agentic Reasoning Through the Lens of Neuroscience</a></p>
                <p><strong>Paper Abstract:</strong> Autonomous AI is no longer a hard-to-reach concept, it enables the agents to move beyond executing tasks to independently addressing complex problems, adapting to change while handling the uncertainty of the environment. However, what makes the agents truly autonomous? It is agentic reasoning, that is crucial for foundation models to develop symbolic logic, statistical correlations, or large-scale pattern recognition to process information, draw inferences, and make decisions. However, it remains unclear why and how existing agentic reasoning approaches work, in comparison to biological reasoning, which instead is deeply rooted in neural mechanisms involving hierarchical cognition, multimodal integration, and dynamic interactions. In this work, we propose a novel neuroscience-inspired framework for agentic reasoning. Grounded in three neuroscience-based definitions and supported by mathematical and biological foundations, we propose a unified framework modeling reasoning from perception to action, encompassing four core types, perceptual, dimensional, logical, and interactive, inspired by distinct functional roles observed in the human brain. We apply this framework to systematically classify and analyze existing AI reasoning methods, evaluating their theoretical foundations, computational designs, and practical limitations. We also explore its implications for building more generalizable, cognitively aligned agents in physical and virtual environments. Finally, building on our framework, we outline future directions and propose new neural-inspired reasoning methods, analogous to chain-of-thought prompting. By bridging cognitive neuroscience and AI, this work offers a theoretical foundation and practical roadmap for advancing agentic reasoning in intelligent systems. The associated project can be found at: https://github.com/BioRAILab/Awesome-Neuroscience-Agent-Reasoning .</p>
                <p><strong>Cost:</strong> 0.024</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8629.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8629.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PaLM-540B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PaLM (540B)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A very large language model (540B parameters) referenced as achieving state-of-the-art on grade-school math via chain-of-thought prompting; cited in the survey as an exemplar where CoT unlocked strong multi-step arithmetic reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>PaLM: Scaling Language Modeling with Pathways.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PaLM-540B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A large transformer-based language model by Google (referenced at 540B scale in the paper) used as an example of large LLMs whose reasoning improves under Chain-of-Thought prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>540B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>GSM8K (grade-school math)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Multi-step arithmetic word problems requiring stepwise symbolic arithmetic and intermediate reasoning steps.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Chain-of-Thought (CoT) prompting to elicit intermediate reasoning steps before the final answer.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Reported in the survey as achieving state-of-the-art (SoTA) accuracy on GSM8K with CoT prompting (no numeric value provided in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Improves substantially over standard direct prompting (i.e., without CoT); the paper frames this as a major gain compared to non-CoT prompting baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>CoT performance depends on prompt engineering and exemplar selection; gains are especially pronounced on math/symbolic tasks but smaller on many other task types. CoT is unguided and can still fail with long or error-prone step sequences (error accumulation).</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Survey conclusion: CoT unlocks strong symbolic/mathematical performance in very large LLMs (e.g., PaLM-540B), but it is brittle—reliant on careful prompt design, and limited in generalization outside math/symbolic domains.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "Nature's Insight: A Novel Framework and Comprehensive Analysis of Agentic Reasoning Through the Lens of Neuroscience", 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8629.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8629.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large instruction-tuned transformer LLM (OpenAI) discussed in the survey with respect to inductive reasoning evaluations; reported to match humans on attribute induction in a cited study but to struggle on non-monotonic reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Inductive reasoning in humans and large language models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large pretrained and instruction-tuned transformer language model (OpenAI). Exact size and training corpora are not specified in this survey; referenced for empirical evaluation of inductive reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>not specified in this paper</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Attribute induction tasks (inductive reasoning benchmark referenced in Han et al. 2024)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Tasks that require generalizing attributes or properties from limited examples (inductive generalization), testing how models form attribute-based generalizations.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Direct evaluation of LLM inductive capabilities (no special augmentation described in survey); used as a testbed to compare model behavior to human inductive reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Reported in the survey (citing Han et al. 2024) as 'comparable to humans' on attribute induction tasks (no numeric accuracy provided here).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared to human participants; the paper notes GPT-4 performs similarly on many attribute-induction items but differs on non-monotonic cases.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Struggles with non-monotonic reasoning and exhibits qualitative differences from human inductive processes (sensitivity to rule changes, failures to revise assumptions).</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>LLMs like GPT-4 can approximate human-like inductive generalization in some settings but do not fully replicate human inductive strategies, especially when inferences require revising prior assumptions or handling non-monotonic updates.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "Nature's Insight: A Novel Framework and Comprehensive Analysis of Agentic Reasoning Through the Lens of Neuroscience", 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8629.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8629.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DeepSeek-R1 / R1-Zero</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DeepSeek-R1 (and R1-Zero)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A family of LLMs trained with large-scale reinforcement learning incentives (R1-Zero trained purely with RL), discussed as improving some reasoning behaviors (e.g., reflection, verification) yet failing on very long chain tasks due to accumulation of probabilistic errors.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DeepSeek-R1 (and R1-Zero)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>An LLM-based system trained with reinforcement learning approaches described in the survey (R1-Zero trained only via RL; R1 built with further phases to improve readability); emphasizes reward modeling (accuracy and format rewards) and self-improvement.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>not specified in this paper</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Long-chain arithmetic and symbolic reasoning tasks (example: game-of-24 long CoT instance)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Problems requiring extended sequences of intermediate steps (long chain-of-thought) where each sub-step must be correct to reach a correct final answer.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Reinforcement learning-based fine-tuning (process rewards, group relative policy optimization, MCTS-based PRMs, step-wise verifier models), self-evolution and reflection mechanisms.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Survey reports improvements in emergent behaviors (reflection, verification) and progressive reasoning capability, but also documents severe failure on a long CoT example (game of 24) where the model 'failed terribly' despite thousands of tokens—no numeric metrics provided.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>R1-Zero (RL-only) is positioned as the training-stage baseline; R1 extends R1-Zero with supervised phases to improve readability. Despite such training, long-chain tasks still fail compared to ideal symbolic solvers.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Error accumulation across many intermediate probabilistic steps causes catastrophic failure on very long reasoning chains; RL-induced capabilities do not eliminate the probabilistic limitations of LLMs for arbitrarily long, exact symbolic derivations.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Reinforcement learning and process-reward supervision improve step-wise capabilities and enable advanced behaviors like reflection, but they do not fundamentally overcome the probabilistic error accumulation that limits long-horizon, strictly correct symbolic reasoning in LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "Nature's Insight: A Novel Framework and Comprehensive Analysis of Agentic Reasoning Through the Lens of Neuroscience", 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8629.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8629.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Chain-of-Thought (CoT) & variants</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Thought prompting and structured variants (ToT, GoT, LoT, Automate-CoT, Promptless-CoT, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prompting and decoding strategies that elicit or structure intermediate reasoning steps from LLMs; survey discusses CoT's positive impact on math and symbolic tasks and lists structural extensions to improve search, backtracking, and logical grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Chain-of-thought prompting elicits reasoning in large language models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLMs (general) using Chain-of-Thought and variants</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A family of prompting and decoding methods applied to transformer LLMs to produce explicit intermediate reasoning traces; variants include Tree-of-Thoughts (ToT), Graph-of-Thoughts (GoT), Logical Thoughts (LoT), Automate-CoT, Active-Prompt, and Promptless-CoT.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>various (applies to multiple LLM scales)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Strict logical and multi-step reasoning benchmarks (e.g., GSM8K, symbolic/math tasks, deductive/inductive benchmarks mentioned in survey)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Tasks that require multi-step, structured inference such as arithmetic word problems, symbolic puzzles, and formal/deductive logical reasoning where intermediate steps are beneficial or necessary.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Eliciting/structuring intermediate reasoning via few-shot exemplars (CoT), automated CoT example selection (Automate-CoT), branching search over thought trajectories (ToT/GoT), logic-grounded thought formats (LoT), and methods that remove the need for hand-crafted exemplars (Promptless-CoT).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Survey reports substantial improvements on math and symbolic tasks (e.g., SoTA on GSM8K when CoT used) but smaller gains or fragile improvements on many other task types; no unified numeric metric provided in survey.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>CoT vs direct prompting: CoT substantially outperforms direct prompting on many structured reasoning benchmarks; structured variants (ToT/GoT) aim to outperform linear CoT by exploring multiple reasoning trajectories.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>CoT is sensitive to exemplar choice and prompt engineering; error accumulation across many steps; CoT gives biggest gains in math/symbolic tasks but limited improvements on tasks outside these domains; branching methods increase compute and need robust evaluation/verification.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>CoT demonstrates that providing intermediate structure improves LLM reasoning in domains amenable to symbolic decomposition, but it remains brittle, often probabilistic, and benefits from additional mechanisms (verification, search, RL, or symbolic grounding) to ensure correctness on strict logical tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "Nature's Insight: A Novel Framework and Comprehensive Analysis of Agentic Reasoning Through the Lens of Neuroscience", 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8629.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8629.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Natural Program / LOGICGUIDE (deductive methods)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Natural Program and LOGICGUIDE (methods for improving deductive coherence in LLMs)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Representative methods cited for improving deductive reasoning in LLMs via structured stepwise verification (Natural Program) and formal logical constraint guidance (LOGICGUIDE); both aim to constrain generation and verify steps to improve deductive reliability.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLMs augmented with Natural Program / LOGICGUIDE techniques</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Methodological approaches that structure LLM outputs into verifiable, stepwise programs or impose formal-logic guidance during generation (e.g., stepwise verification, constraint-driven decoding).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>various (method applicable across model scales)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Deductive reasoning / compositional proofs and logical inference benchmarks (survey references deductive tasks and datasets used to probe LLM deductive generalization).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Tasks that require deriving logically certain conclusions from explicit premises (deduction), including compositional proofs, proof-by-contradiction, and case-based deductive problems.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Stepwise verification formats (Natural Program) and integration of formal logical systems to constrain model generation (LOGICGUIDE); these approaches enforce intermediate checks and use symbolic constraints to reduce hallucination.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Survey states these methods improve reliability and consistency in deductive tasks (no numeric scores provided); they help models generalize in compositional proofs but models still struggle on longer proofs and some proof styles.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Improvements relative to unconstrained LLM generation and plain CoT; methods aim to close gap toward symbolic solvers but do not fully match formal provers.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>LLMs still struggle with long deductive chains, case-based reasoning, and proof-by-contradiction style arguments; enforcing logical constraints helps but does not fully eliminate hallucinations or brittle failures.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Constraining LLM generation with stepwise verification or formal-logic scaffolds improves deductive behavior and interpretability, but the survey emphasizes that deeper integration with symbolic verification or neuro-symbolic planners will be needed for rigorous, scalable logical reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "Nature's Insight: A Novel Framework and Comprehensive Analysis of Agentic Reasoning Through the Lens of Neuroscience", 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Chain-of-thought prompting elicits reasoning in large language models. <em>(Rating: 2)</em></li>
                <li>PaLM: Scaling Language Modeling with Pathways. <em>(Rating: 2)</em></li>
                <li>DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning. <em>(Rating: 2)</em></li>
                <li>Inductive reasoning in humans and large language models. <em>(Rating: 2)</em></li>
                <li>Tree of thoughts: Deliberate problem solving with large language models. <em>(Rating: 2)</em></li>
                <li>Hypothesis search: Inductive reasoning with language models. <em>(Rating: 1)</em></li>
                <li>Natural Program <em>(Rating: 1)</em></li>
                <li>LOGICGUIDE <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8629",
    "paper_id": "paper-278481056",
    "extraction_schema_id": "extraction-schema-154",
    "extracted_data": [
        {
            "name_short": "PaLM-540B",
            "name_full": "PaLM (540B)",
            "brief_description": "A very large language model (540B parameters) referenced as achieving state-of-the-art on grade-school math via chain-of-thought prompting; cited in the survey as an exemplar where CoT unlocked strong multi-step arithmetic reasoning.",
            "citation_title": "PaLM: Scaling Language Modeling with Pathways.",
            "mention_or_use": "mention",
            "model_name": "PaLM-540B",
            "model_description": "A large transformer-based language model by Google (referenced at 540B scale in the paper) used as an example of large LLMs whose reasoning improves under Chain-of-Thought prompting.",
            "model_size": "540B",
            "reasoning_task_name": "GSM8K (grade-school math)",
            "reasoning_task_description": "Multi-step arithmetic word problems requiring stepwise symbolic arithmetic and intermediate reasoning steps.",
            "method_or_approach": "Chain-of-Thought (CoT) prompting to elicit intermediate reasoning steps before the final answer.",
            "performance": "Reported in the survey as achieving state-of-the-art (SoTA) accuracy on GSM8K with CoT prompting (no numeric value provided in this paper).",
            "baseline_comparison": "Improves substantially over standard direct prompting (i.e., without CoT); the paper frames this as a major gain compared to non-CoT prompting baselines.",
            "limitations_or_failures": "CoT performance depends on prompt engineering and exemplar selection; gains are especially pronounced on math/symbolic tasks but smaller on many other task types. CoT is unguided and can still fail with long or error-prone step sequences (error accumulation).",
            "insights_or_conclusions": "Survey conclusion: CoT unlocks strong symbolic/mathematical performance in very large LLMs (e.g., PaLM-540B), but it is brittle—reliant on careful prompt design, and limited in generalization outside math/symbolic domains.",
            "uuid": "e8629.0",
            "source_info": {
                "paper_title": "Nature's Insight: A Novel Framework and Comprehensive Analysis of Agentic Reasoning Through the Lens of Neuroscience",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "GPT-4",
            "name_full": "GPT-4",
            "brief_description": "A large instruction-tuned transformer LLM (OpenAI) discussed in the survey with respect to inductive reasoning evaluations; reported to match humans on attribute induction in a cited study but to struggle on non-monotonic reasoning.",
            "citation_title": "Inductive reasoning in humans and large language models.",
            "mention_or_use": "mention",
            "model_name": "GPT-4",
            "model_description": "Large pretrained and instruction-tuned transformer language model (OpenAI). Exact size and training corpora are not specified in this survey; referenced for empirical evaluation of inductive reasoning.",
            "model_size": "not specified in this paper",
            "reasoning_task_name": "Attribute induction tasks (inductive reasoning benchmark referenced in Han et al. 2024)",
            "reasoning_task_description": "Tasks that require generalizing attributes or properties from limited examples (inductive generalization), testing how models form attribute-based generalizations.",
            "method_or_approach": "Direct evaluation of LLM inductive capabilities (no special augmentation described in survey); used as a testbed to compare model behavior to human inductive reasoning.",
            "performance": "Reported in the survey (citing Han et al. 2024) as 'comparable to humans' on attribute induction tasks (no numeric accuracy provided here).",
            "baseline_comparison": "Compared to human participants; the paper notes GPT-4 performs similarly on many attribute-induction items but differs on non-monotonic cases.",
            "limitations_or_failures": "Struggles with non-monotonic reasoning and exhibits qualitative differences from human inductive processes (sensitivity to rule changes, failures to revise assumptions).",
            "insights_or_conclusions": "LLMs like GPT-4 can approximate human-like inductive generalization in some settings but do not fully replicate human inductive strategies, especially when inferences require revising prior assumptions or handling non-monotonic updates.",
            "uuid": "e8629.1",
            "source_info": {
                "paper_title": "Nature's Insight: A Novel Framework and Comprehensive Analysis of Agentic Reasoning Through the Lens of Neuroscience",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "DeepSeek-R1 / R1-Zero",
            "name_full": "DeepSeek-R1 (and R1-Zero)",
            "brief_description": "A family of LLMs trained with large-scale reinforcement learning incentives (R1-Zero trained purely with RL), discussed as improving some reasoning behaviors (e.g., reflection, verification) yet failing on very long chain tasks due to accumulation of probabilistic errors.",
            "citation_title": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning.",
            "mention_or_use": "mention",
            "model_name": "DeepSeek-R1 (and R1-Zero)",
            "model_description": "An LLM-based system trained with reinforcement learning approaches described in the survey (R1-Zero trained only via RL; R1 built with further phases to improve readability); emphasizes reward modeling (accuracy and format rewards) and self-improvement.",
            "model_size": "not specified in this paper",
            "reasoning_task_name": "Long-chain arithmetic and symbolic reasoning tasks (example: game-of-24 long CoT instance)",
            "reasoning_task_description": "Problems requiring extended sequences of intermediate steps (long chain-of-thought) where each sub-step must be correct to reach a correct final answer.",
            "method_or_approach": "Reinforcement learning-based fine-tuning (process rewards, group relative policy optimization, MCTS-based PRMs, step-wise verifier models), self-evolution and reflection mechanisms.",
            "performance": "Survey reports improvements in emergent behaviors (reflection, verification) and progressive reasoning capability, but also documents severe failure on a long CoT example (game of 24) where the model 'failed terribly' despite thousands of tokens—no numeric metrics provided.",
            "baseline_comparison": "R1-Zero (RL-only) is positioned as the training-stage baseline; R1 extends R1-Zero with supervised phases to improve readability. Despite such training, long-chain tasks still fail compared to ideal symbolic solvers.",
            "limitations_or_failures": "Error accumulation across many intermediate probabilistic steps causes catastrophic failure on very long reasoning chains; RL-induced capabilities do not eliminate the probabilistic limitations of LLMs for arbitrarily long, exact symbolic derivations.",
            "insights_or_conclusions": "Reinforcement learning and process-reward supervision improve step-wise capabilities and enable advanced behaviors like reflection, but they do not fundamentally overcome the probabilistic error accumulation that limits long-horizon, strictly correct symbolic reasoning in LLMs.",
            "uuid": "e8629.2",
            "source_info": {
                "paper_title": "Nature's Insight: A Novel Framework and Comprehensive Analysis of Agentic Reasoning Through the Lens of Neuroscience",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "Chain-of-Thought (CoT) & variants",
            "name_full": "Chain-of-Thought prompting and structured variants (ToT, GoT, LoT, Automate-CoT, Promptless-CoT, etc.)",
            "brief_description": "Prompting and decoding strategies that elicit or structure intermediate reasoning steps from LLMs; survey discusses CoT's positive impact on math and symbolic tasks and lists structural extensions to improve search, backtracking, and logical grounding.",
            "citation_title": "Chain-of-thought prompting elicits reasoning in large language models.",
            "mention_or_use": "mention",
            "model_name": "LLMs (general) using Chain-of-Thought and variants",
            "model_description": "A family of prompting and decoding methods applied to transformer LLMs to produce explicit intermediate reasoning traces; variants include Tree-of-Thoughts (ToT), Graph-of-Thoughts (GoT), Logical Thoughts (LoT), Automate-CoT, Active-Prompt, and Promptless-CoT.",
            "model_size": "various (applies to multiple LLM scales)",
            "reasoning_task_name": "Strict logical and multi-step reasoning benchmarks (e.g., GSM8K, symbolic/math tasks, deductive/inductive benchmarks mentioned in survey)",
            "reasoning_task_description": "Tasks that require multi-step, structured inference such as arithmetic word problems, symbolic puzzles, and formal/deductive logical reasoning where intermediate steps are beneficial or necessary.",
            "method_or_approach": "Eliciting/structuring intermediate reasoning via few-shot exemplars (CoT), automated CoT example selection (Automate-CoT), branching search over thought trajectories (ToT/GoT), logic-grounded thought formats (LoT), and methods that remove the need for hand-crafted exemplars (Promptless-CoT).",
            "performance": "Survey reports substantial improvements on math and symbolic tasks (e.g., SoTA on GSM8K when CoT used) but smaller gains or fragile improvements on many other task types; no unified numeric metric provided in survey.",
            "baseline_comparison": "CoT vs direct prompting: CoT substantially outperforms direct prompting on many structured reasoning benchmarks; structured variants (ToT/GoT) aim to outperform linear CoT by exploring multiple reasoning trajectories.",
            "limitations_or_failures": "CoT is sensitive to exemplar choice and prompt engineering; error accumulation across many steps; CoT gives biggest gains in math/symbolic tasks but limited improvements on tasks outside these domains; branching methods increase compute and need robust evaluation/verification.",
            "insights_or_conclusions": "CoT demonstrates that providing intermediate structure improves LLM reasoning in domains amenable to symbolic decomposition, but it remains brittle, often probabilistic, and benefits from additional mechanisms (verification, search, RL, or symbolic grounding) to ensure correctness on strict logical tasks.",
            "uuid": "e8629.3",
            "source_info": {
                "paper_title": "Nature's Insight: A Novel Framework and Comprehensive Analysis of Agentic Reasoning Through the Lens of Neuroscience",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "Natural Program / LOGICGUIDE (deductive methods)",
            "name_full": "Natural Program and LOGICGUIDE (methods for improving deductive coherence in LLMs)",
            "brief_description": "Representative methods cited for improving deductive reasoning in LLMs via structured stepwise verification (Natural Program) and formal logical constraint guidance (LOGICGUIDE); both aim to constrain generation and verify steps to improve deductive reliability.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "LLMs augmented with Natural Program / LOGICGUIDE techniques",
            "model_description": "Methodological approaches that structure LLM outputs into verifiable, stepwise programs or impose formal-logic guidance during generation (e.g., stepwise verification, constraint-driven decoding).",
            "model_size": "various (method applicable across model scales)",
            "reasoning_task_name": "Deductive reasoning / compositional proofs and logical inference benchmarks (survey references deductive tasks and datasets used to probe LLM deductive generalization).",
            "reasoning_task_description": "Tasks that require deriving logically certain conclusions from explicit premises (deduction), including compositional proofs, proof-by-contradiction, and case-based deductive problems.",
            "method_or_approach": "Stepwise verification formats (Natural Program) and integration of formal logical systems to constrain model generation (LOGICGUIDE); these approaches enforce intermediate checks and use symbolic constraints to reduce hallucination.",
            "performance": "Survey states these methods improve reliability and consistency in deductive tasks (no numeric scores provided); they help models generalize in compositional proofs but models still struggle on longer proofs and some proof styles.",
            "baseline_comparison": "Improvements relative to unconstrained LLM generation and plain CoT; methods aim to close gap toward symbolic solvers but do not fully match formal provers.",
            "limitations_or_failures": "LLMs still struggle with long deductive chains, case-based reasoning, and proof-by-contradiction style arguments; enforcing logical constraints helps but does not fully eliminate hallucinations or brittle failures.",
            "insights_or_conclusions": "Constraining LLM generation with stepwise verification or formal-logic scaffolds improves deductive behavior and interpretability, but the survey emphasizes that deeper integration with symbolic verification or neuro-symbolic planners will be needed for rigorous, scalable logical reasoning.",
            "uuid": "e8629.4",
            "source_info": {
                "paper_title": "Nature's Insight: A Novel Framework and Comprehensive Analysis of Agentic Reasoning Through the Lens of Neuroscience",
                "publication_date_yy_mm": "2025-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Chain-of-thought prompting elicits reasoning in large language models.",
            "rating": 2,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "PaLM: Scaling Language Modeling with Pathways.",
            "rating": 2,
            "sanitized_title": "palm_scaling_language_modeling_with_pathways"
        },
        {
            "paper_title": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning.",
            "rating": 2,
            "sanitized_title": "deepseekr1_incentivizing_reasoning_capability_in_llms_via_reinforcement_learning"
        },
        {
            "paper_title": "Inductive reasoning in humans and large language models.",
            "rating": 2,
            "sanitized_title": "inductive_reasoning_in_humans_and_large_language_models"
        },
        {
            "paper_title": "Tree of thoughts: Deliberate problem solving with large language models.",
            "rating": 2,
            "sanitized_title": "tree_of_thoughts_deliberate_problem_solving_with_large_language_models"
        },
        {
            "paper_title": "Hypothesis search: Inductive reasoning with language models.",
            "rating": 1,
            "sanitized_title": "hypothesis_search_inductive_reasoning_with_language_models"
        },
        {
            "paper_title": "Natural Program",
            "rating": 1,
            "sanitized_title": "natural_program"
        },
        {
            "paper_title": "LOGICGUIDE",
            "rating": 1,
            "sanitized_title": "logicguide"
        }
    ],
    "cost": 0.0236745,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Nature's Insight: A Novel Framework and Comprehensive Analysis of Agentic Reasoning Through the Lens of Neuroscience
7 May 2025</p>
<p>Zinan Liu 
Haoran Li 
Jingyi Lu 
Gaoyuan Ma 
Xu Hong xuhong@ntu.edu.sg 
Senior Member, IEEEGiovanni Iacca giovanni.iacca@unitn.it 
Arvind Kumar arvkumar@kth.se 
Shaojun Tang shaojuntang@ust.hk 
Lin Wang linwang@ntu.edu.sg 
Hong Xu </p>
<p>School of EEE
Nanyang Technological University (NTU)
Singapore</p>
<p>School of Cyber Science and Technology
University of Science and Technology of China
China</p>
<p>School of Integrated Circuits and Electronics
Beijing Institute of Technology
China</p>
<p>School of Social Sciences
Nanyang Technological University
Singapore</p>
<p>Department of Information Engineering and Computer Science
University of Trento
Italy</p>
<p>Division of Computational Science and Technol-ogy
KTH Royal Institute of Technology
Sweden</p>
<p>Bioscience and Biomedical Engineering
Hong Kong University of Science &amp; Technology</p>
<p>Nature's Insight: A Novel Framework and Comprehensive Analysis of Agentic Reasoning Through the Lens of Neuroscience
7 May 20250521483F53878BFBF7570638D12BC30DarXiv:2505.05515v1[q-bio.NC]Agentic reasoningcognitive neuroscienceneuroscience-inspired AIhuman-aligned AI Conscious accessattention regulationcross-module integration. Perceptualinteractive
Autonomous AI is no longer a hard-to-reach concept-it enables the machines (agents) to move beyond executing tasks to independently addressing complex problems, adapting to change while handling the uncertainty of the environment.However, what makes the agents truly autonomous?It is agentic reasoning, that is crucial for foundation models to develop symbolic logic, statistical correlations, or large-scale pattern recognition to process information, draw inferences, and make decisions.However, it remains unclear why and how existing agentic reasoning approaches work, in comparison to biological reasoning, which instead is deeply rooted in neural mechanisms involving hierarchical cognition, multimodal integration, and dynamic interactions.In this work, we propose a novel neuroscience-inspired framework for agentic reasoning.Grounded in three cognitive neuroscience-based definitions of reasoning, supported by corresponding mathematical formulations and biological reasoning pathways, we develop a unified framework that models the full reasoning process from sensory input to action.Within this framework, we identify four core reasoning types-perceptual, dimensional, logical, and interactive-inspired by distinct functional roles observed in the human brain.We apply this framework to systematically classify and analyze existing AI reasoning methods, evaluating their theoretical foundations, computational designs, and practical limitations.We further explore the implications for developing more generalizable and cognitively aligned agents in both physical and virtual settings.Finally, based on our framework, we outline future directions for AI reasoning and introduce new reasoning methods inspired by neural models, analogous to chainof-thought prompting.By bridging cognitive neuroscience and AI, this work offers a theoretical foundation and practical roadmap for advancing agentic reasoning in intelligent systems.The associated project can be found at: https://github.com/BioRAILab/Awesome-Neuroscience-Agent-Reasoning.</p>
<p>I. INTRODUCTION</p>
<p>R EASONING is the process of drawing conclusions from premises [1].It forms a cornerstone of human intelligence [2]- [5] and enables individuals to interpret the world, anticipate future events, and solve complex problems across a wide range of domains.Similarly, for artificial agents, reasoning is fundamental to adaptive decision-making, generalization, and problem-solving in dynamic environments.As shown in Fig. 2, recent years have witnessed a surge in research interest surrounding agentic reasoning, particularly in Large Language Model (LLM)-based reasoning, highlighting the growing impact of large language models in this field.In the development of autonomous artificial intelligence (AI)-systems capable of independently perceiving, reasoning, and acting in complex, uncertain environments, reasoning stands as a critical prerequisite.Unlike narrow AI systems that excel in specialized tasks but struggle with abstraction and transfer learning, autonomous AI requires robust reasoning mechanisms to synthesize information, infer hidden relationships, and adaptively navigate novel situations without explicit human intervention.Therefore, advancing the reasoning capabilities of AI agents is not merely an incremental improvement-it is a necessary step toward building more intelligent, self-directed agents that can move beyond pattern recognition and reactive behavior.</p>
<p>Human reasoning is a continuous and dynamic cycle that enables individuals to process information, generate inferences, take actions, and refine knowledge over time as shown in Fig. 1 (left).This process begins with multi-modal perception, where external stimuli-such as visual, auditory, and textual inputs [6], [7]-are integrated with prior knowledge and lived experience.While this may resemble Bayesian inference processes used in artificial agents, human reasoning exhibits distinct capabilities: it operates in highly uncertain, open-ended environments, leverages abstract analogies, and adapts flexibly in real-time based on minimal cues.For instance, a human can intuitively infer a person's emotional state from subtle shifts in tone, gesture, or eye movement and adjust behavior accordingly, something current AI agents still struggle to do reliably [8].This kind of nuanced, socially grounded inference arises not just from data-driven computation but from embodied experiences, neural priors, and a deep contextual understanding of the world.Once information is processed, the human mind engages in inference mechanisms,</p>
<p>Information Processing</p>
<p>Fig. 1.The proposed neuroscience-inspired framework for agentic reasoning.The left panel illustrates the human brain's reasoning process, where sensory inputs are processed through modality-specific cortices and integrated in higher association areas such as the parietal and prefrontal cortices.This enables abstract reasoning and decision-making, supported by predictive coding mechanisms and memory retrieval from the hippocampus.Inspired by this cognitive flow, the right panel presents a corresponding architecture for AI agents, consisting of sensory input, multi-level information processing, foundational understanding (via foundation models), factual memory storage (knowledge base), and a centralized reasoning module for adaptive and context-aware decision-making.White arrows denote top-down predictive signals based on predictive coding; black arrows represent the forward reasoning process; and dashed lines indicate the conceptual mapping between human brain functions and agent modules.evaluating possibilities, predicting outcomes, and formulating decisions [9].These inferences are not static; they evolve in response to feedback, continuously updating internal cognitive models [10].The reasoning outputs manifest as actions [11] that interact with the environment, and critically, the results of these actions are internalized as structured memory and knowledge.This recursive reasoning-action loop enables continual learning, robust generalization, and effective decision-making in dynamic, ambiguous scenarios.</p>
<p>Human reasoning mechanisms provide valuable insights for enhancing AI agents' ability to handle complex tasks.One notable example is how the brain tackles high-complexity reasoning problems under limited attentional resources.Due to the bottleneck in attentional capacity [12], [13], humans cannot process large amounts of information simultaneously.Instead, they rely on serial reasoning, where problems are broken down into manageable steps and solved iteratively as shown in Tab.I (ACR-T [14]).This principle directly aligns with Chain of Thought (CoT) [15] reasoning for LLMs, which structures problem-solving as a sequence of intermediate inference steps to enhance accuracy and coherence.By mirroring this stepwise approach, AI agents can better manage computational complexity and improve reasoning performance.</p>
<p>Despite these insights, AI agentic reasoning mechanisms still fall short of human cognition, particularly in autonomous agents navigating dynamic and unpredictable envi-  The vertical axis represents the number of publications (in thousands), while the horizontal axis denotes the publication year.The figure highlights a significant rise in "LLM agentic reasoning" publications since 2023, reflecting the impact of large language models on the field.ronments.Most AI models, including LLMs and reinforcement learning agents, rely on static architectures and feedforward processing, lacking the iterative refinement and feedback mechanism [16], [17].Unlike human cognition, which continuously integrates new information to refine understanding, AI systems typically cannot adjust their reasoning in real time.</p>
<p>Another key limitation is long-term adaptability.Humans not only adjust their immediate reasoning steps but also update their internal mental models [18] when exposed to new experiences.In contrast, AI agents typically operate within fixed training paradigms, restricting their ability to incorporate novel knowledge into existing frameworks.This rigidity leads to poor generalization in novel or complex scenarios.Furthermore, AI agents struggle with multi-modal integration.Human cognition seamlessly combines sensory inputs-such as vision, sound, and touch-into a coherent understanding.For example, we can easily relate derivatives to slopes in mathematics, drawing analogies across different domains [19].In contrast, AI models process each modality separately, limiting their ability to perform cross-modal reasoning and effectively interpret ambiguous situations.Finally, agentic reasoning lacks global comprehension and causal inference.Many models, especially LLMs, rely on autoregressive predictions based on local context rather than a holistic understanding.This results in strong pattern recognition but weak causal reasoning, long-term dependencies, and counterfactual thinking-key elements of human intelligence essential for complex decision-making.</p>
<p>To address these challenges, this paper explores agentic reasoning inspired by neuroscience, examining how current agentic reasoning mechanisms compare to human cognitive processes.By analyzing how agents process information, adapt to new knowledge, integrate memory, and perform cross-modal reasoning, we aim to highlight both strengths and limitations in existing agent systems.Our goal is to provide insights that guide the development of more flexible, adaptive, and robust reasoning models, ultimately advancing AI agents toward greater autonomy and generalization capabilities.</p>
<p>Our paper is the first to systematically examine agentic reasoning from a neuroscience perspective, distinguishing itself from prior works that primarily focus on foundation models such as LLMs [20]- [22] and multimodal models [23], [24] as they are more akin to relatively static and passive knowledge repositories in the human brain rather than complete reasoning systems.In contrast, [20] primarily explores reasoning mechanisms within LLMs, while [22] differentiates between heuristic and deliberate reasoning, but neither provides a systematic discussion on how an AI agent, as a whole, performs reasoning.Our key contribution lies in establishing a comprehensive agentic reasoning framework that spans from sensory to motor action, grounded in neuroscience principles as shown in Fig. 1 (right).This structured definition lays the foundation for future research on enhancing AI agents' reasoning capabilities.</p>
<p>In Sec.II, we establish the conceptual and theoretical basis for agentic reasoning by integrating insights from cognitive neuroscience.We begin by introducing three formal definitions of reasoning derived from neuroscientific perspectives, which are complemented by corresponding mathematical formulations and grounded in biological reasoning processes observed in the brain.Building on these foundations, we develop a unified framework that captures the full reasoning cycle-from sensory perception to decision-making and action.Central to this framework is the identification of four core reasoning modalities: perceptual, dimensional, logical, and interactive.These categories reflect distinct functional subsystems within the human brain and serve as the organizational backbone for our subsequent analysis.In Sec.III, rather than merely categorizing existing reasoning methods, we systematically reinterpret and introduce them through the lens of our neuroscience-inspired agentic reasoning framework.By situating current approaches within this structured paradigm, we examine their underlying technical mechanisms, assess the extent to which they align with human cognitive processes, and identify key limitations that hinder their generalization.This analytical perspective not only clarifies the current landscape of AI reasoning but also reveals critical gaps and opportunities for future development.In Sec.IV, we systematically categorize existing reasoning tasks and datasets based on our proposed agentic reasoning framework.Rather than providing a general overview, we align benchmarks with specific reasoning types-perceptual, dimensional, logical, and interactive reasoning-allowing for a structured analysis of current evaluation methods.Furthermore, we identify key gaps in existing benchmarks and propose new challenge tasks that better capture the complexities of real-world AI agentic reasoning, paving the way for more comprehensive and rigorous evaluation standards.Sec.V examines the applications of current reasoning methods.Here, we review practical implementations of AI agentic reasoning techniques across diverse domains, including autonomous navigation, visual question answering, robotics, and human-agent interaction.The discussion not only illustrates the strengths and limitations of existing approaches but also underscores the importance of multi-modal integration and dynamic decisionmaking in real-world scenarios.Finally, Sec.VI explores future directions for agentic reasoning by identifying key limitations of current AI agents and drawing insights from neuroscience.Rather than merely outlining general trends, we propose new research directions inspired by cognitive models.We examine additional cognitive architectures and mechanisms that could inspire more advanced AI agentic reasoning paradigms.By leveraging established neuroscience models, we highlight potential pathways for improving AI agents' adaptability, sequential inference, and knowledge integration, providing a biologically motivated perspective on the future of agentic reasoning.In summary, our major contributions are as follows:</p>
<p>• Establishing a Neuroscience-Based AI Agentic Reasoning Framework.Unlike prior surveys that primarily focus on reasoning in foundation models, we are the first to systematically examine agentic reasoning from a neuroscience perspective.We construct a comprehensive framework spanning from perception to action, providing a structured foundation for future research.• A Framework-Based Analysis with Systematic Analysis of Reasoning Methods.Unlike conventional taskbased surveys, our work adopts a novel framework-based approach.We systematically categorize and analyze existing reasoning methods within our neuro-inspired framework, evaluating their technical characteristics, alignment with human cognition, and key challenges.• Identifying Limitations and Proposing Future Di-rections.We systematically identify key limitations in current agentic reasoning models, including challenges in adaptability, generalization, and multistep reasoning.Based on these insights, we propose future research directions to enhance agentic reasoning capabilities.</p>
<p>• Developing an Open-Source Repository for Agentic Reasoning Research.To facilitate future studies, we curate and release a structured repository that organizes benchmark tasks, datasets, and reasoning-related papers based on our proposed framework, serving as a valuable resource for advancing AI agentic reasoning.We will continuously update the repository to enhance its utility.</p>
<p>II. NEUROSCIENCE-INSPIRED AGENTIC REASONING</p>
<p>Understanding the nature of reasoning requires an interdisciplinary approach, drawing insights from cognitive science, psychology, and neuroscience.From a neuro-scientific perspective, reasoning is not a singular or isolated cognitive function but rather a dynamic and multi-faceted process that enables individuals to derive conclusions, solve problems, and make decisions.It involves the interaction of memory, perception, and executive functions, orchestrated across various specialized neural circuits.Reasoning allows for adaptive responses to novel situations by leveraging prior experiences while continuously incorporating new information [25], [26].</p>
<p>The underlying mechanisms of reasoning can be characterized by three fundamental principles [27].First, reasoning operates as a hybrid process, integrating prior knowledge with newly acquired information to support both familiar and innovative outcomes across varied contexts.Second, it functions as an integrative and recursive system that synthesizes multiple diverse inputs into a coherent output, whether a mental representation or a physical action.This output can, in turn, serve as a new input for subsequent reasoning, enabling continuous refinement and dynamic adaptation.Third, reasoning follows a structured, multistep progression, ensuring that mental processes are systematically navigated toward a conclusion.These principles collectively define reasoning as a core cognitive function with a structured yet flexible nature.Hybrid Nature of Reasoning.Reasoning is inherently a complex hybrid process that synthesizes prior knowledge with new information, as shown in Fig. 3. Some outcomes arise from novel recombination of past experiences, while others depend on the integration of entirely new inputs.This dual mechanism highlights the balance between learned patterns and the generation of original solutions, which makes reasoning both adaptive and deeply creative.Recursive Input-Output Integration.As an essential cognitive mechanism, reasoning processes diverse inputs to produce meaningful outputs.These outputs can manifest as internally generated thoughts or externally executed actions, both of which result from complex neural computations.Specifically, it can be considered as a recursive cognitive process in which outputs often serve as new inputs, enabling continuous refinement of thought and behavior.The ability to combine information from different sources is fundamental for logical deduction, problem solving, and decision making.Multistep Structured Process.Reasoning follows a structured, multistep progression in which various cognitive pathways contribute to the final outcome.Each step builds upon previous elements, ensuring that reasoning is not merely reactive but follows a deliberate and organized trajectory toward a conclusion.This structured approach underpins the sequential nature of logical inference and systematic thought.Table I illustrates this multistep process as observed in neuroscience models, highlighting how different stages unfold over time.</p>
<p>Deciding What to Wear Based on Weather</p>
<p>A. Foundation of Reasoning A Hybrid Process</p>
<p>For reasoning to maintain its complexity, it must go beyond automatic recall.Implicit memories, such as those described by Knowlton, Mangels, and Squire in 1996 [34], do not qualify as reasoning because they evoke behavior without conscious deliberation.Instead, these are classic examples of learning, where past experiences directly influence actions without the cognitive synthesis characteristic of reasoning [35].In contrast, whenever we integrate new information, whether through unfamiliar data or novel structuring of prior knowledge, we engage in the dynamic and complex process of reasoning.At times, reasoning relies heavily on well-established facts, while in other cases, it leans toward innovation and spontaneity.However, in most cases, reasoning occurs through a combination of previous knowledge and new information.Even when dealing primarily with known facts, reasoning still requires assembling these elements in a novel way [36].If a thought process merely outputs prior knowledge without reconfiguration, it ceases to be reasoning and instead resembles a learned or reflexive behavior.</p>
<p>As described by the first definition, reasoning does not function in isolation.In contrast, it relies on the interplay between what is already known and what is newly encountered.Reasoning is inherently a hybrid process, blending prior knowledge with new information [37].Although there are rare instances where reasoning occurs with entirely unfamiliar information to generate a completely novel conclusion, which we might call creative reasoning, most reasoning involves some degree of prior knowledge.This balance between past experiences and novel inputs allows us to 'think on our feet' and adapt in real time, making solutions as we go [38].In this context, new information refers to knowledge that was</p>
<p>Categories of Reasoning</p>
<p>Miller and Cohen's Model [28] Cognitive control in the prefrontal cortex (PFC) for task management.</p>
<p>Sequential steps in cognitive control: 1. Active maintenance of goal representations (PFC).</p>
<ol>
<li>Bias signals guide neural pathways.</li>
</ol>
<p>Adjustments made in neural maps</p>
<p>Executive control, decision-making, task management.</p>
<p>Logical, Interactive</p>
<p>Banich's Cascade of Control Model [29] Brain regions work in a sequence to manage attention and response.</p>
<p>Sequential cascade: 1. Posterior DLPFC selects attentional set.2. Mid-DLPFC selects task-relevant representation.</p>
<ol>
<li>Posterior ACC selects the response.4. Anterior ACC evaluates the response Attention regulation, error correction, decision-making.</li>
</ol>
<p>Logical, interactive.</p>
<p>Baddeley's Working</p>
<p>Memory Model [30], [31] The components of working memory: central executive, phonological loop, visuospatial sketchpad, episodic buffer.</p>
<p>Multi-component process: Adaptive Control of Thought-Rational (ACT-R) [14] Cognitive model based on discrete cognitive operations for declarative and procedural knowledge.</p>
<p>Step-by-step task execution: 1. Chunks (declarative) stored in memory.</p>
<ol>
<li>Procedural knowledge (productions) guides task execution, following a seriation-based sequence.3. Modules (e.g., visual, manual) interact with environment.</li>
</ol>
<p>Task execution, problem-solving, cognitive coordination.</p>
<p>Logical, interactive.</p>
<p>SOAR [32] Symbolic cognitive architecture using production rules for goal-directed behavior.</p>
<p>Sequential steps in goal-directed behavior:</p>
<ol>
<li>Encodes problems into symbolic states.2. Uses production rules to decompose goals.3. Applies search and learning in symbolic space.</li>
</ol>
<p>Executive planning, symbolic manipulation, goal decomposition.</p>
<p>Logical, interactive.</p>
<p>Global Workspace Theory (GWT) [33] Consciousness emerges from globally broadcasting selected information.</p>
<p>Introduction of Novel Data.Novel data refers to entirely new inputs that were previously unknown to the reasoning system [39].This can be commonly described as the continuous process of human beings' perception of the world with biological sensors.This type of information is external and requires active incorporation into the reasoning process.When encountering novel data, reasoning must adjust its existing knowledge structures, infer relationships, and possibly revise prior beliefs.Unlike simple recall or application of learned rules, reasoning in the presence of novel data demands more dynamic adaptation.This is particularly evident in real-time decision-making scenarios, where an agent or human must process unexpected inputs and generate new conclusions.The ability to integrate novel data is crucial for reasoning to remain flexible, ensuring that decisions are not solely based on outdated or incomplete prior knowledge.For example, a human doctor encountering a rare disease case must synthesize unfamiliar symptoms with known medical principles to form a diagnosis, rather than relying solely on past cases.On the other hand, from an intelligent agent perspective, a robotic vision system designed for warehouse navigation may encounter an obstacle type it has never seen before.Instead of failing, it must reason about potential workarounds using its existing spatial models and decision framework.</p>
<p>Context-Independent Knowledge consists of abstract principles, rules, or axioms that, while already known, were previously inactive in the reasoning process.Unlike novel data, which introduces external newness, context-independent knowledge is retrieved and applied in a novel context.This also includes knowledge that was not previously activated in discourse, as well as updates that shift probability distributions or modify existing reasoning structures, often emerging as the focal point of inference [40].The reasoning process relies on dynamically incorporating such knowledge, allowing for the synthesis of new conclusions beyond mere memorization.By retrieving and restructuring fundamental principles, reasoning remains adaptable, enabling generalization across different domains and situations.Like when mathematicians solve a problem in an unfamiliar domain (e.g., applying graph theory concepts to network security), they may retrieve abstract mathematical principles that were not originally associated with the current problem but are applicable.By bringing the problem to what agents can do, a reinforcement learning agent trained to play chess may generalize strategic principles (e.g., controlling the center of the board) when encountering an entirely new board position it has never seen in training.</p>
<p>Modification of Existing Knowledge and Revision of Assumptions.These two categories are closely related to each other, both involving updates to knowledge.The prior category adds new, external facts that change the model without necessarily contradicting prior assumptions.However, the revision of assumptions involves adjusting or invalidating previous assumptions based on new evidence that contradicts earlier beliefs or conclusions, which is a key characteristic of nonmonotonic reasoning [41].For instance, a person assumes a friend is at home because their car is parked outside, but upon learning that the friend took an Uber, they revise their assumption.Brought this further into the agent's perspective, a robot designed to monitor household activities may initially infer that a person is at home based on sensor data like car location.Upon receiving new information (e.g., a GPS update or direct communication from a smart device indicating the person took an Uber), the robot revises its belief, updating its internal model to reflect the change in the person's status.This is similar to how an AI agent in a logistics system might update its delivery assumptions based on live traffic data, revising expected arrival times in real time.</p>
<p>Thus, complexity is central to reasoning.It cannot be reduced to mere repetition of past knowledge.Rather, it thrives on the interplay between what we know and what we learn.Understanding this hybrid nature of reasoning lays the groundwork for examining how such processes are instantiated in the human brain, particularly through the lens of neuroscience.</p>
<p>B. Mathematical Foundation of Reasoning Behavior</p>
<p>Building upon the conceptual foundation of reasoning as a hybrid process, we now shift our focus to its formal representation.To bridge biological insights with computational understanding, mathematical modeling provides a formal framework for capturing reasoning behavior, which can be used to analyze, simulate, and predict reasoning behavior.By abstracting cognitive mechanisms into mathematical forms-such as logic-based systems, probabilistic models, or optimization frameworks-we gain not only deeper theoretical insights but also practical tools for designing intelligent agents and understanding human cognition at a larger scale.</p>
<p>Mathematical models provide several advantages in the study of reasoning.First, they offer a precise and unambiguous way to describe reasoning processes, ensuring clarity in theoretical frameworks.Unlike purely descriptive approaches, mathematical formulations enable predictability, allowing researchers to anticipate outcomes based on specific inputs.This predictability is particularly valuable in fields like AI and neuroscience, where computational models of reasoning must be robust and reliable.Additionally, mathematical representations facilitate the implementation of reasoning mechanisms in computational systems, making them essential for AI agent applications such as natural language processing, decisionmaking, and automated theorem proving.By formalizing reasoning mathematically, researchers can also develop generalizable frameworks that apply across multiple disciplines, from cognitive science to robotics and machine learning.</p>
<p>Several mathematical frameworks have been developed to represent reasoning processes.Serving as the cornerstone, Bayesian Brain Theory (BBT) [42], [43] suggests that the brain functions as a probabilistic inference machine, continuously updating its beliefs about the environment using Bayesian inference.Bayesian inference models reasoning as a probabilistic process in which prior beliefs are updated in response to new evidence.This approach is useful in dealing with uncertainty and dynamic environments.Predictive coding [44], [45], another influential model, describes how the brain minimizes errors in perception and cognition by continuously updating internal models of the world.The free energy principle [46], [47] extends this idea further, proposing that the brain functions as an optimization system that seeks to minimize uncertainty in its predictions.In addition to these probabilistic approaches, formal logic remains a crucial mathematical tool for reasoning.Logic-based models, such as propositional and first-order logic, provide a structured framework for deductive reasoning and are widely used in rule-based AI agent systems.Furthermore, decision theory and optimization techniques frame reasoning as a problem of selecting the best action based on a cost or reward function.</p>
<p>The reasoning process in the brain can be understood as a continuous cycle of perception, inference, and decisionmaking, governed by probabilistic models.The brain receives sensory inputs from the environment, interprets them through predictive models, and updates its internal beliefs based on new information.This process can be mathematically formulated using principles from Bayesian inference, predictive coding, and free energy minimization.The following sections will explore these mathematical representations in greater detail, illustrating how they contribute to our understanding of the reasoning process.</p>
<p>1) Bayesian Inference in the Brain: The brain updates its belief about a hidden state H given sensory data D using Bayes' theorem:
P (H|D) = P (D|H)P (H) P (D) ,(1)
where P (H|D) is the posterior probability, representing the updated belief after observing D, P (D|H) is the likelihood, describing the probability of the data given H, P (H) is the prior probability, representing prior beliefs about H, and P (D) is the evidence, normalizing the probability distribution.</p>
<p>2) Predictive Coding Model: Predictive coding suggests that the brain minimizes the difference between sensory input x t and its internal predictions xt :
ϵ t = x t − xt ,(2)
where x t is the actual sensory observation, xt is the predicted sensory input, and ϵ t is the prediction error.</p>
<p>The brain refines its internal model by minimizing ϵ t , adjusting beliefs through an optimization process:
dH dt ∝ − ∂F ∂H ,(3)
where F is the variational free energy.
F = D KL (Q(H)||P (H|D)),(4)
where Q(H) is the approximate posterior, P (H|D) is the true posterior, and D KL (Q||P ) is the Kullback-Leibler (KL) divergence, measuring the difference between the two distributions.Minimizing F ensures that the brain's internal model aligns with reality.4) Decision-Making as Bayesian Optimization: Decisionmaking in BBT can be formulated as a Bayesian reinforcement learning problem, where the brain selects an optimal policy π * that maximizes expected rewards:
π * = arg max π T t=0 E P (st|st−1,at−1) [R(s t , a t )],(5)
where s t is the state at time t, a t is the action taken at time t, R(s t , a t ) is the reward function, E represents the expectation over possible state transitions.Bayesian Brain Theory models cognition as a probabilistic inference system.The brain continually updates its beliefs using Bayesian inference, minimizes prediction errors via predictive coding, optimizes free energy for efficient learning, and makes decisions based on Bayesian optimization principles.</p>
<p>While mathematical models provide a powerful abstraction of reasoning behavior, it is equally crucial to examine how reasoning unfolds biologically within the brain.This motivates an exploration of the neural substrates and pathways involved in reasoning from a neuroscience perspective.However, reasoning is not a monolithic process.It manifests in various forms, each characterized by different structures, objectives, and mechanisms.To appreciate the breadth of reasoning behaviors, it is important to explore their underlying typologies.</p>
<p>C. Reasoning Process of Neuroscience</p>
<p>Building on the hybrid model of reasoning, it becomes essential to investigate how these cognitive mechanisms are realized biologically.Neuroscience offers a compelling perspective by mapping reasoning onto neural substrates and examining the functional architecture that supports it.From prefrontal cortex activity to dynamic network interactions, neuroscience provides insight into how the brain orchestrates reasoning processes in structured and uncertain environments.</p>
<p>From a neuroscience perspective, the reasoning process involves the collaboration of multiple brain regions (Fig. 1 left).The reasoning process begins with the brain receiving various modality-specific sensory inputs from the external environment.For instance, visual information is first captured by the retina and transmitted via the lateral geniculate nucleus (LGN) of the thalamus to the primary visual cortex (V1) in the occipital lobe [6], while auditory information is processed through the medial geniculate nucleus (MGN) of the thalamus and sent to Heschl's gyrus in the temporal lobe [7].These sensory pathways rely on a combination of electrical signaling along axons and chemical neurotransmission at synapses, where neurotransmitters (e.g., glutamate, GABA) mediate the transfer of information across neurons.These primary sensory cortices extract fundamental features such as edges, motion, frequency, and pitch before relaying the processed information to higher-order association areas.</p>
<p>The parietal lobe plays a crucial role in multimodal integration, particularly in spatial awareness, numerical reasoning, and body coordination [48].Here, sensory inputs from updated knowledge (e.g., vision and audition) are combined, allowing the brain to construct a coherent representation of the environment.Meanwhile, according to the theory of Predictive Coding [10], the brain is hypothesized to actively generate predictive signals for expected stimuli [10].These top-down signals are sent back to the sensory cortices, where they are compared against incoming sensory inputs.Any discrepancies between prediction and perception trigger updates to the brain's internal model.This adaptive updating is biologically implemented through synaptic plasticity, the process by which the strength of synaptic connections between neurons is modified based on experience.A well-studied form of this mechanism is spike-timing dependent plasticity (STDP), where the precise timing of spikes between presynaptic and postsynaptic neurons determines whether synaptic weights are strengthened or weakened.</p>
<p>As information is integrated, it is processed in the prefrontal cortex (PFC), which serves as the central hub for abstract thinking, decision-making, and logical reasoning [27].The PFC refines predictions, evaluates uncertainty, and formulates complex cognitive responses based on contextual memory and learned experiences [9].During this process, different parts of the brain need to stay coordinated.This is achieved in part through neural oscillations-rhythmic patterns of brain activity that help different brain areas communicate efficiently.These oscillations play an important role in maintaining attention and keeping information in working memory during reasoning.In addition, the decision-making process can be described by the drift-diffusion model (DDM), which suggests that the brain gradually accumulates evidence over time before making a choice.This helps explain why some decisions take longer than others and how the brain balances speed and accuracy.</p>
<p>Once a decision is made, the information is passed to the motor cortex [11], where it is translated into actions.</p>
<p>Notably, reasoning is not limited to immediate perceptionaction cycles but is deeply intertwined with memory mechanisms.The hippocampus, in conjunction with the cerebral cortex, plays a vital role in episodic memory formation and retrieval [49].Through hippocampal-cortical interactions, new experiences are encoded into long-term memory via synaptic plasticity mechanisms such as STDP, which adjust synaptic strengths based on neural activity patterns.These changes reinforce the knowledge base that supports future reasoning.Over time, frequently used information undergoes systems consolidation, transferring from the hippocampus to cortical networks, enabling more efficient recall and inference [50].</p>
<p>Thus, neural reasoning is an iterative, predictive, and memory-driven process, integrating sensory information, updating internal models, and leveraging past experiences to guide cognition and behavior.While neuroscience helps uncover where and how reasoning occurs in the brain, it is equally important to understand how this process can be formalized and abstracted into structured models that can be simulated, predicted, and analyzed.</p>
<p>D. Agent Reasoning Framework</p>
<p>Inspired by the biological reasoning process, we propose an Agent Reasoning Framework that mirrors the layered structure of human cognition, as illustrated on the right side of Fig. 1.The reasoning pipeline begins with multimodal sensory modules, which are then integrated by the information processing module and used to update the knowledge base, working alongside the foundation model to support more complex higher-level reasoning.</p>
<p>1) Multimodal Input Module: The multimodal input module, as the first layer of the Agent Reasoning Framework, is responsible for transmitting information from the external world to the internal cognitive system.This module corresponds to the sensory systems in the biological brain, capable of receiving various forms of sensory stimuli from the environment, such as vision, hearing, language, and touch, and transmitting these signals into the internal representation space in a structured form.At this stage, the agent does not passively receive all sensory information but possesses the ability for active perception and selective attention.This module dynamically allocates attention resources based on the goals and context of the current task, enhancing relevant input and reducing redundant or sensory content.As a result, the system can maintain a stable and focused perceptual state in the face of complex, changing environments, laying the foundation for subsequent information processing and reasoning.</p>
<p>2) Information Processing Module: In the human brain, sensory signals from different modalities, such as vision, audition, and touch, are ultimately converted into a unified electrochemical signal format for transmission and processing.This unified encoding mechanism enables the brain to efficiently integrate information across modalities, forming stable and coherent internal representations.Inspired by this neural mechanism, our agent requires an information processing module to map input signals from multiple sensory channels into a shared, high-dimensional representation space.This module would not rely on modality-specific encoding paths but instead utilize a modality-agnostic unified representation, enabling natural flow and mutual activation of information across different channels.This unified representation mechanism enhances the system's ability to understand complex scenarios and provides a continuous, composable foundation for knowledge retrieval and subsequent reasoning, thereby establishing a neural-like structural foundation for multimodal cross-domain reasoning.</p>
<p>3) Knowledge Base Module: Human knowledge acquisition relies not only on the accumulation of past experiences but also heavily on continuous interaction with the external environment.In the brain, the long-term memory system gradually absorbs knowledge from repeated perception and actions, while the working memory system dynamically retrieves information relevant to the current context, enabling flexible responses to changing environments.Inspired by this dual-memory mechanism, our framework requires a dualchannel knowledge system.On one hand, the agent maintains an internal, continually updated knowledge base that accumulates experience from long-term interaction and provides stable, context-rich support for reasoning.On the other hand, the agent incorporates a time-sensitive retrieval mechanism that enables real-time access to external knowledge sources, allowing for rapid integration of novel or dynamic information.These two systems work in close coordination during the reasoning process: internal knowledge ensures coherence and personalized adaptation, while external knowledge offers flexibility and broad coverage.Together, they form a brain-inspired dynamic knowledge architecture that integrates accumulation and activation, enabling the agent to sustain robust and timely reasoning even in complex, evolving, or unfamiliar scenarios.</p>
<p>4) Foundation Models Module: In our framework, the foundation model plays a crucial dual role, drawing inspiration from the brain's memory execution system.It serves as an advanced understanding engine, responsible for interpreting and processing external inputs in a highly adaptive and dynamic manner.Essentially, it acts as a highly efficient executor of memory, continuously trained and updated within a knowledge base to enhance comprehension.Much like how the brain constantly refines its cognitive models through the integration of long-term memory and accumulated experiences, our foundation model strengthens its understanding of the world by learning from ongoing data and constantly evolving contexts.However, the foundation model does not merely serve as a knowledge repository; it also functions as a versatile reasoning assistant.It supports various types of reasoning tasks by seamlessly integrating multimodal sensory inputs, structured knowledge, and prior reasoning rules.Based on these reasoning rules, the foundation model assists in executing specific reasoning tasks.For instance, according to logical inference rules, it could support tasks like deducing conclusions from a set of premises or identifying contradictions within a series of statements.Similarly, it may apply spatial reasoning rules to help with tasks like predicting the movement of objects in a dynamic environment.In this way, the foundation model acts as a flexible and robust platform, applying learned rules and representations to assist in higher-level reasoning and decisionmaking.It does not replace specialized reasoning modules but rather scaffolds them by preprocessing inputs, suggesting candidate inferences, and enforcing structured knowledge derived from past interactions.As a result, the foundation model plays a dual role: it serves as a cognitive substrate for understanding, while also facilitating adaptive, flexible, and modular reasoning across complex, real-world tasks.</p>
<p>5) Reasoning Module: In the reasoning system of our brain-inspired agent, the reasoning module serves as the core component responsible for organizing task-specific reasoning rules and leveraging the foundation model to execute them.Inspired by the neural mechanisms of the prefrontal cortex, which governs rule extraction and decision control, and the parietal cortex, which integrates multimodal information and constructs spatiotemporal representations, we believe that the reasoning module should exhibit a task-oriented and structured architecture.For various types of reasoning-perceptual, dimensional, logical, and interactive-it derives tailored reasoning strategies and execution paths, utilizing the foundation model as a reasoning assistant to perform the cognitive computations required by each task.For example, in logical reasoning, the model can apply inference rules such as modus ponens ("If A, then B") to conduct conditional judgments and generate conclusions.The specific reasoning tasks and their categorization are discussed in detail in Sec II-E.More importantly, reasoning is not a static process but one that supports dynamic adaptation.The outcomes of reasoning are not only written back into the knowledge base to provide contextual support for future tasks, but they also continuously refine the foundation model's internal reasoning mechanisms.Through repeated task execution and feedback accumulation, the model gradually develops more adaptive reasoning structures and strategy selection capabilities, thereby enhancing its generalization and responsiveness across diverse tasks.This closedloop structure of "rule-guidance − → model execution − → result feedback − → rule refinement" forms the most autonomous and growth-driven core of brain-inspired reasoning.</p>
<p>Compared to prominent cognitive frameworks, our model offers a more comprehensive ability to handle multimodal inputs, dynamic reasoning tasks, and continuous updates to the knowledge base.Unlike SOAR [32], which emphasizes a unified cognitive system but is limited in handling dynamic environments and multimodal inputs, our framework continuously updates its knowledge base, allowing reasoning outcomes to adapt to changing environments and enhancing reasoning efficiency and adaptability.In contrast to Global Workspace Theory (GWT) [33], which focuses on information integration but lacks flexible knowledge base updates capacity, our model ensures continuous accumulation and updating of knowledge, enabling more efficient information flow in reasoning and decision-making.Additionally, while Dual-Process Theory [51] distinguishes between fast, automatic responses and slower, deliberate reasoning without effectively integrating the two systems, our framework supports both rapid responses and more precise, adaptive reasoning by integrating flexible knowledge updates and reasoning modules.Overall, our framework combines unified multimodal representations, continuous knowledge updates, and flexible reasoning modules, allowing for efficient handling of complex reasoning tasks and adaptation to dynamic real-world environments, showcasing unique strengths in comprehensiveness and adaptability.</p>
<p>E. Classifications of Reasoning Behavior</p>
<p>Reasoning encompasses a diverse array of cognitive strategies, each serving distinct functions in human thought and problem-solving.Building on insights into the neural mechanisms underlying reasoning, we now examine how reasoning behaviors are classified within cognitive science and psychology.Decades of research into the neural basis of reasoning have produced several influential theoretical frameworks.Synthesizing the most widely accepted hypotheses [27], [52], reasoning can be categorized into four primary types, as illustrated in Fig. 4: Perceptual Reasoning, Dimensional Reasoning, Logical Reasoning, and Interactive Reasoning.Each category represents a distinct mode of information processing, ranging from interpreting sensory inputs to applying formal logic, analyzing multi-dimensional relationships, and engaging in collaborative, context-sensitive reasoning.</p>
<p>Perceptual Reasoning refers to the cognitive ability to acquire, interpret, and manipulate information derived from sensory modalities such as vision, audition, and touch.From a neuroscience perspective, this form of reasoning is closely associated with activity in the occipital and parietal lobes [53], [54], which are involved in processing visual input and integrating multi-sensory information.It enables individuals to detect patterns, make inferences, and solve problems without completely relying on verbal or linguistic cues.Core components of perceptual reasoning include relational reasoning, such as analogy detection, relational matching, and instance comparison, all of which are fundamental in tasks like matrix reasoning and visual puzzle-solving.These processes engage neural mechanisms responsible for feature extraction, similarity assessment, and categorical abstraction.For example, participants may be asked to identify shared attributes among objects, recognize visual analogies, or distinguish meaningful differences between stimuli.Perceptual reasoning thus underpins a wide range of nonverbal cognitive functions and is a foundational element in intelligence testing and adaptive behavior in dynamic environments.</p>
<p>Dimensional Reasoning [55], [56] involves the integration of cognitive processes across multiple representational domains, such as spatial configurations [57], temporal dynamics [58], and abstract hierarchical relationships.This form of reasoning engages higher-order cognitive functions to interpret and manipulate complex, multi-dimensional structures.From a neuroscience standpoint, dimensional reasoning recruits distributed neural circuits, particularly involving the parietal cortex for spatial manipulation, the prefrontal cortex for maintaining abstract rules and hierarchies, and the medial temporal lobe for encoding temporal sequences and eventbased dependencies.Tasks requiring dimensional reasoning often involve understanding 3D object relationships, predicting dynamic system behavior, or analyzing the interdependence of At the center, a hierarchical reasoning pipeline, spanning data sensory input, information processing, higher-order cognition, and conclusion generation, mirrors the flow of information in biological systems.Surrounding this core are five major categories of reasoning behaviors: perceptual reasoning, driven by multisensory integration; dimensional reasoning, encompassing spatial and temporal inference; relation reasoning, involving analogical thinking and relational matching; logical reasoning, covering inductive, deductive, and abductive logic; and interactive reasoning, focusing on agent-agent and agent-human collaboration within dynamic environments.Together, these components establish a neurocognitively grounded taxonomy that bridges biological inspiration and computational implementation in artificial intelligence systems.</p>
<p>multiple variables.These abilities are foundational in domains such as engineering, mathematics, and the physical sciences, where interpreting structured, multi-variable information is critical.Empirical investigations commonly assess dimensional reasoning through nonverbal problem-solving tasks, such as mental rotation, hierarchical pattern completion, and sequential logic exercises, each of which probes the brain's capacity to synthesize and navigate complex cognitive representations across multiple axes of abstraction.Importantly, dimensional reasoning also plays a pivotal role in agent-based reasoning systems, where an autonomous agent must interpret high-dimensional sensory inputs and dynamically adapt to changing task constraints within complex environments.Logical Reasoning [59], [60] follows structured principles of inference and is divided into inductive, deductive, and abductive reasoning.Inductive reasoning moves from specific observations to broader generalizations, forming conclusions that are possibly true.Deductive reasoning, in contrast, starts from general premises and derives specific, logically certain conclusions.Abductive reasoning works by finding the most plausible explanation for given evidence, commonly used in diagnostics and hypothesis generation.These logical processes form the foundation of rational thinking and decision-making.</p>
<p>Interactive Reasoning focuses on the dynamic exchange of information between humans, agents, and the environment.Unlike other reasoning types that occur within an individ-ual's mind, interactive reasoning involves collaboration and adaptation, where agents refine their understanding through interaction.This is important in AI-driven decision-making, autonomous systems, and cooperative problem-solving, where reasoning is influenced by external inputs and evolving conditions.In essence, reasoning behavior can be understood through these four categories, each playing a crucial role in human cognition and artificial intelligence.Whether derived from sensory data, structured logic, multi-dimensional analysis, or collaborative engagement, reasoning enables intelligent systems to interpret the world, make informed decisions, and adapt to complex scenarios.</p>
<p>III. COMPREHENSIVE ANALYSIS OF AGENTIC REASONING</p>
<p>Having explored reasoning from a neuro-cognitive perspective and its mathematical foundations, we now shift our focus to reasoning in agents.Reasoning of agent seeks to replicate, enhance, or extend human cognitive abilities through computational models, enabling intelligent systems to process information, infer conclusions, and make decisions.Over the years, research in AI has developed diverse approaches to reasoning, each with its own underlying principles and methods.These approaches can be broadly categorized based on how they represent knowledge, handle uncertainty, interact with external environments, and apply logical structures.</p>
<p>Neuro-inspired Agentic Reasoning</p>
<p>Perception-based Reasoning §III-A Visual Reasoning</p>
<p>VLM-based</p>
<p>GeReA [61], LISA [62], VLM-R1 [63], Kn-vlm [64], etc.</p>
<p>LLM-based</p>
<p>Cola [65], VCTP [66], Vis-CoT [67], VAP [68], etc.</p>
<p>Symbolic-based VISPROG [69], Exo ViP [70], ViperGPT [71], etc.</p>
<p>RL-based HYDRA [72], Vision-R1 [73], Visual-rft [74], Medvlm-r1 [75], etc.</p>
<p>Lingual Reasoning</p>
<p>CoT-based CoT [15], CoD [76], CoUT [77], ToT [78], GoT [79], LoT [80], Automate-CoT [81], Active-Prompt [82], etc.</p>
<p>RL-Based</p>
<p>DeepSeek-R1 [83], SPAG [84], DIVERSE [85], Rest-mcts* [86], Math-Shepherd [87], OmegaPRM [88], Step-DPO [89] etc.</p>
<p>Auditory Reasoning</p>
<p>CF-CLAP [90], BAT [91], LTU [92], etc.</p>
<p>Tactile Reasoning OCTOPI [93], TALON [ [94], FUSE [95], ReACT [96], etc.</p>
<p>Dimension-based Reasoning §III-B Spatial Reasoning</p>
<p>Geometric Reasoning SpatialVLM [97], LocVLM [98], SpatialPIN [99], etc.</p>
<p>Topological Reasoning</p>
<p>StarCraftImage [100], GRASP [101], Q-Spatial [102], Nejatishahidin et al. [103], etc.</p>
<p>Physical Reasoning</p>
<p>ZeroVLM [104], TopV-Nav [105], DANet [106], SpatialCoT [107], VLMnav [108], etc.</p>
<p>Temporal Reasoning LLM-Based MTAM [109], PromptCast [110], TG-LLM [111], TSQA [112], TempoGPT [113], Auto-TTE [114], etc.</p>
<p>Graph-Based</p>
<p>Know-Evolve [115], TiPNN [116], CTRN [117], HSTT [118], CENET [119],THCN [120] etc.</p>
<p>Logic-based Reasoning §III-C Inductive Reasoning</p>
<p>HypoSearch [121], IHR [122], etc.</p>
<p>Deductive Reasoning</p>
<p>Natural Program [123], LOGICGUIDE [124], etc.</p>
<p>Abductive Reasoning VAR [125], MAR [126], etc.</p>
<p>Interaction-based</p>
<p>Reasoning §III-D</p>
<p>Reasoning based on Agent-Agent Interaction</p>
<p>Cooperative Interaction DERA [127], RoCo [128], etc.</p>
<p>Adversarial Interaction</p>
<p>ChatEval [129], MAD [130], etc.</p>
<p>Reasoning based on Agent-Human Interaction Hierarchical Partnership Model</p>
<p>LISSA [131], PEER [132], etc.</p>
<p>Symbiotic Partnership Model</p>
<p>SAPIEN [133], Cicero [134], etc.To better understand the landscape of AI reasoning and finding valuable potential directions, based on the inspiration from the neuroscience perspective on the reasoning behavior, we introduce a taxonomy that organizes majority reasoning approaches into four main categories as classified in Fig. 4: dimension-based reasoning, perception-based reasoning, interaction-based reasoning, and logic-based reasoning.Each category reflects a distinct perspective on how reasoning can be structured and applied in AI systems.Dimension-based reasoning examines how abstract representations, such as spatial, temporal, or multi-modal structures, influence reasoning capabilities.Perception-based reasoning focuses on how AI systems extract and process information from raw sensory inputs, often using neural models to interpret visual, auditory, or textual data.Interaction-based reasoning explores reasoning within dynamic environments, emphasizing real-world engagement through learning, adaptation, and collaboration with humans or other agents.Logic-based reasoning, rooted in formal symbolic methods, remains a cornerstone of AI, providing structured frameworks for rule-based inference, knowledge representation, and verification.</p>
<p>By classifying AI reasoning into these four main categories as shown in Fig. 5, this taxonomy offers a structured lens through which we can analyze existing research, identify trends, and assess the strengths and limitations of different approaches.Subsequently, we explore each category in detail, highlighting its core principles, representative methodologies, and recent advancements in the field.This classification not only facilitates a clearer understanding of AI reasoning but also provides insights into how these approaches can be integrated to create more robust and versatile intelligent systems.</p>
<p>To ensure a comprehensive and high-quality survey of AI reasoning research, we established a rigorous selection benchmark for choosing relevant papers.Our selection process prioritizes papers published in top-tier conferences and journals across multiple research domains related to agented reasoning on AI or Robotics, such as NeurIPS, CVPR, TPAMI, JMLR, ICRA, and TOR.Our selection criterion extends beyond publication venues to include relevance across different reasoning paradigms.Given that reasoning in AI spans multiple subfields, we categorized papers based on their contributions to dimension-based, perception-based, interaction-based, and logic-based reasoning, ensuring balanced representation across all reasoning approaches.We also considered interdisciplinary relevance, including works from cognitive science, neuroscience, and formal logic that contribute to AI reasoning methodologies.Since relevant work from Nature and its subjournals is also included, such as Nature Neuroscience, Nature Communications, and Nature Machine Intelligence.</p>
<p>To maintain a balance between classical and emerging trends, we selected both foundational papers that have shaped AI reasoning and recent advancements that reflect the latest breakthroughs in neural-symbolic integration, large-scale reasoning models, and interactive AI systems.By applying these selection benchmarks, we ensured that our survey provides a comprehensive, well-structured, and up-to-date overview of AI reasoning, capturing both theoretical developments and practical implementations across diverse domains.</p>
<p>A. Perception-based Reasoning</p>
<p>Perception lies at the heart of intelligent behavior, serving as the primary interface between an agent and its environment.Perceptual reasoning refers to the ability of AI systems to interpret, integrate, and infer knowledge from raw sensory inputs-such as vision, language, audio, and tactile signalsto support higher-level cognition and decision-making.Unlike symbolic or logic-based reasoning that operates over abstract representations, perceptual reasoning grounds inference in multimodal sensory data, enabling agents to make sense of complex, ambiguous, or noisy inputs.This form of reasoning is particularly vital in real-world, unstructured environments where direct perception must inform tasks like object recognition, scene understanding, language grounding, or human-robot interaction.Vision language models (VLMs), audio-visual transformers, and multimodal fusion networks exemplify contemporary approaches that perform reasoning directly over perceptual streams.These systems must align modalities, resolve cross-modal ambiguities, and extract structured semantics from unstructured inputs.Perceptual reasoning thus acts as a bridge between low-level perception and highlevel cognition, equipping agents with the ability to derive meaningful conclusions from what they see, hear, or feel.In the following subsections, we investigate key techniques and models that enable perceptual reasoning, analyzing their architectures, reasoning strategies, and the challenges they face in aligning perception with intelligent behavior.</p>
<p>1) Visual Reasoning: The visual reasoning capabilities of Artificial Intelligence (AI) are principally evidenced in the comprehension, analysis, and inference of imagery and video data, propelling intelligent systems towards advanced stages of cognitive evolution.This inferential process encompasses not only fundamental object identification and detection but also extends to a profound understanding of object attributes, spatial relationships, and causal linkages.Such capabilities facilitate AI systems in demonstrating human-like reasoning skills across various tasks, including Visual Question Answering (VQA), image captioning, and video understanding.In contrast to conventional symbolic logic-based reasoning, visual reasoning necessitates the processing of extensive visual datasets and integrates multimodal information for comprehensive analysis.This approach significantly bolsters the precision and logical coherence of the resultant inferences.</p>
<p>Based on underlying technologies, visual reasoning is divided into Vision-Language Model (VLM)-based, Large-Language Model (LLM)-based, Symbolic-based, and Reinforcement Learning (RL)-based approaches.VLM-based methods [61]- [64], [140] achieve cross-modal information complementarity through multimodal fusion (images and text), enhancing the model's global perception of complex visual logic and its ability to capture local details.For example, GeReA [61] proposes a new visual reasoning framework by inputting relevant visual information (regions in images related to questions) and linguistic information (questions and associated human prompts) into pretrained VLMs to generate question-aware prompt captions, combining image-question pairs with similar samples to feed into a multimodal reason-  LLM-based methods can be divided into two categories: Direct Invocation of LLMs [65], [68], [141] and VCoT(Visual Chain-of-Thought) [66], [67], [142]- [144].Among these, Cola [65] directly invokes LLMs for reasoning.It processes input images independently through multiple VLMs, generating visual descriptions and candidate answers.An LLM acts as a reasoning coordinator to analyze these descriptions and answers, identifying points of consensus and conflict, combining world knowledge for logical inference, and generating the final answer along with reasoning evidence.In terms of VCoT, VisCoT [67] mimics human visual scanning and reasoning processes by dynamically focusing and conducting multi-turn reasoning, progressively deriving and locating key information to generate more accurate and interpretable answers, significantly enhancing the model's reasoning capabilities in complex visual scenarios as shown in Fig. 6(b).VCTP [66] adopts a three-stage reasoning approach called "See-Think-Confirm" to progressively accomplish knowledge-driven visual reasoning tasks.Initially (See), the model analyzes the image, detects all possible objects, and generates a global visual description.Next (Think), the LLM combines the question to select key visual concepts, generates region-specific descriptions, and reasons towards preliminary answers.Finally (Confirm), the LLM produces reasoning evidence and verifies the inference against visual evidence through cross-modal validation.</p>
<p>Symbolic-based approaches [69]- [71], [145] aim to address the issues of data dependency, insufficient interpretability, and task rigidity-where most models require task-specific The puppy is running through the grass with a yellow toy in its mouth, which looks to be an activity of fetching.[75], [146] enables more flexible visual reasoning in open-world tasks, achieving stronger intelligent interaction capabilities.For instance, HYDRA [72] adopts incremental reasoning, storing and utilizing historical information to improve reasoning stability, and dynamically optimizes decisions through reinforcement learning to reduce error propagation.Vision-R1 [73] enhances the reasoning capabilities of VLMs through reinforcement learning, employing Group Relative Policy Optimization (GRPO) for training while incorporating Hierarchical Formatted Reward Refinement Function (HFRRF) to ensure reasoning quality.</p>
<p>Is the tower located between the two buildings in</p>
<p>Current VLMs perform well in simple VQA tasks but exhibit significant limitations when handling complex visual tasks, such as abstract reasoning.The primary constraint lies in the inability of current visual encoders to effectively extract abstract visual features, such as spatial relationships and geometric structures, resulting in insufficient sensitivity to implicit geometric rules within images.Additionally, existing VLMs predominantly rely on contrastive learning or generative training paradigms, which struggle to capture intricate vision-language associations.These models depend heavily on text-driven reasoning rather than directly extracting logical relationships from visual features.To enhance the capability of VLMs in processing complex visual tasks, it is worth considering structural innovations in visual encoders to extract richer visual semantic information, as well as introducing benchmarks specifically designed for abstract reasoning.</p>
<p>Despite the latest GPT-o3 model proposing a new paradigm in visual understanding by integrating images into the chain of thought and performing transformation operations on images during the visual reasoning process to enhance visual comprehension and flexibility, it still exhibits certain limitations in VQA.While the GPT-o3 model demonstrates exceptional performance in parsing whiteboard sketches to derive formulas, inferring geographical locations from landscapes, and answering detailed questions about images, it remains constrained in some aspects.For instance, when presented with an image depicting six fingers, the GPT-o3 model is unable to accurately identify the number of fingers.</p>
<p>2) Lingual Reasoning: Neuroscientific studies have indicated that human reasoning does not primarily rely on the language centers of the brain [147].This biological distinction highlights a fundamental gap between natural and artificial reasoning mechanisms.In contrast, AI reasoning remains heavily dependent on large language models (LLMs), which serve as the main framework for linguistic reasoning.Although scaling LLMs has led to notable performance improvements, they continue to struggle with fundamental linguistic reasoning tasks, such as mathematical reasoning and commonsense inference.To address these limitations, strategies have been proposed.Most mainstream approaches can be categorized as either Chain-of-Thought (CoT)-based or Reinforcement Learning (RL)-based methods, as illustrated in Figure 7.</p>
<p>CoT is a type of reasoning where the LLM generates intermediate reasoning steps before arriving at a final answer.It was first discovered at Google [15] when researchers prompted LLMs with a method called chain-of-thought prompting.This method gives the LLMs not only the questions and their final answers, but also step-by-step reasoning process examples.Experiments on LLMs show that CoT prompting improves performance on various arithmetic, commonsense, and symbolic reasoning tasks, with PaLM 540B [148] achieving SoTA accuracy on the GSM8K [149] benchmark.This initial finding had 2 main issues: (1) it is overly reliant on prompt engineering, (2) the reasoning format is quite unguided.To overcome the first issue, researchers came up with several solutions.Automatic Prompt Augmentation and Selection with Chain-of-Thought (Automate-CoT) [81] allows automatic augmentation of rational chains from a small labeled dataset.It enables a quick adaptation of the CoT technique to different tasks, overcoming the challenge that real-world rational chains are usually unavailable.Other similar solutions include Active-Prompt [82], which also improves adaptability of CoT on different tasks, and Promptless-CoT [150] from Google, which changes the decoding strategy, allowing the LLMs to do CoT using their inherent reasoning abilities without the need for prompts.Other solutions that structure the reasoning format were proposed to tackle the second problem.Tree of thoughts (ToT) [78] was introduced to overcome the limitations of token-level, left-to-right decision-making and generalizes CoT.It allows LLMs to consider multiple different reasoning paths, self-evaluate choices, and backtrack.Graph of thoughts (GoT) [79] improved on ToT, modelling LLM-generated information as a graph with units of information ("thoughts") as vertices and edges corresponding to dependencies.Logical thoughts (LoT) [80] used logical principles to ground the reasoning process, so that they experience less hallucinations.</p>
<p>CoT prompting, while effective in eliciting reasoning capabilities in large language models (LLMs), has inherent limitations.As noted in [151], CoT yields substantial performance improvements primarily on tasks involving logic and mathematics, but offers considerably smaller gains on other task types.This is attributed to CoT's primary advantage in enhancing symbolic execution, wherein it still underperforms relative to dedicated symbolic solvers.As a result, CoT remains limited when compared to human neuroscience-inspired models of reasoning, particularly in its ability to generalize across diverse reasoning domains.</p>
<p>CoT-based methods mainly aim to unlock the reasoning capabilities of LLMs by prompting them to reason in steps.A newer and different method aims to enhance the innate reasoning abilities of LLMs during the training process, and that is the RL-based methods.Earlier RL methods mainly used reward modeling, and each has its own focus on top of that.Some put effort into verifiers, with earlier works like Diverse Verifier On Reasoning Step (DIVERSE) [85].It generates prompts to explore different reasoning paths using verifiers to filter answers, then verifies each step individually.A follow-up research introduced Math-Shepherd [87], which is a process reward model (PRM) that also verifies LLMs stepby-step.Others use MCTS to design the PRM.ReST-MCTS<em> [86] integrates process reward guidance with MCTS</em>, allowing collection of higher-quality reasoning traces.Similarly, OmegaPRM [88] uses a divide-and-conquer style MCTS to identify errors in CoT to allow quick and efficient collection of process-supervision data.Another kind is direct preference optimization (DPO), with notable works like AgentQ [152] and Step-DPO [89].DeepSeek R1-Zero and DeepSeek R1 [83] introduced many new methods, such as Group Relative Policy Optimisation (GRPO), Reinforcement Fine Tuning (ReFT), and rule-based RL.R1-Zero was trained exclusively using large-scale RL without any preliminary supervised finetuning (SFT).Two types of rewards were modeled: accuracy rewards and format rewards.Then it is set to self-evolve, with its reasoning abilities improving steadily and even showing sophisticated reasoning behaviors like reflection.R1 was built upon R1-Zero by incorporating additional training phases, addressing readability issues, and further enhancing reasoning capabilities.Many follow-ups were done regarding the methods used in DeepSeek.Logic-RL [153] leverages rule-based RL, fostering advanced reasoning skills such as reflection, verification, and summarization.Reinforced Functional Token Tuning (RFTT) [154] explored ReFT.It embeds a rich set of learnable functional tokens directly into the model vocabulary and uses an SFT phase to allow learning of the tokens.</p>
<p>Despite all the complex reasoning abilities that evolved during the RL training process, DeepSeek R1 is still, in essence, a LLM, which is grounded in probabilistic prediction, even using advanced methods like CoT prompting offer limited help as the reasoning task becomes bigger and more complex.As shown in the Fig. 8 below, a very lengthy CoT example of solving the game of 24 with the numbers 2, 2, 2, 9 ended up with DeepSeek R1 failing terribly, despite using tens of thousands of tokens.This is because with longer thought processes, the number of intermediate steps will increase as well, and each step is based on possibility, with an error rate.More steps will cause the error rate to gradually accumulate, resulting in a ridiculous result.</p>
<p>While CoT and RL-based techniques improve symbolic reasoning and step-wise deduction, they remain limited in flexibility and generalization.Humans often reason with sparse information, draw connections across seemingly unrelated concepts, and refine their thought processes through internal dialogue and self-reflection -capabilities that LLMs still struggle to replicate reliably.In the future, more sophisticated mechanisms that support analogical reasoning, hierarchical abstraction, and reflective self-correction need to be developed.These may include structured memory systems, interactive reasoning loops, or neuro-symbolic hybrids that combine statistical fluency with logical rigor.Additionally, the field lacks comprehensive evaluation benchmarks that go beyond Fig. 8.Even with complex step-by-step CoT prompting that reflects how people would actually approach a reasoning problem on a good model, such as DeepSeek-R1 trained with sophisticated RL methods, the results are still terrible.This is due to the fact that no matter how good the prompting or the training methods are, LLMs are based on probability at the end of the day, and cannot reach the same reasoning capabilities as humans.</p>
<p>arithmetic or commonsense tasks, to test for deeper cognitive traits such as creativity, philosophical reasoning, and moral judgment.Filling this gap will be essential for pushing the frontier of lingual reasoning in AI.</p>
<p>3) Auditory Reasoning: Auditory reasoning in the context of AI refers to the ability of an AI system to interpret, understand, and reason based on auditory information (i.e., sound or speech).It involves processing audio data, particularly speech, extracting meaningful insights from it, which can be used to make decisions, understand context, or respond appropriately.</p>
<p>One major method for achieving this goal is integrating the sensing ability of a perceptual model with the reasoning ability of LLMs, creating what is called large audio language models (LALMs) or audio large language models (ALLMs).Researchers from MIT integrated a traditional audio model Audio Spectrogram Transformer (AST) [155], with the large language model LLaMA [156], creating a model called listen, think, understand (LTU) [92].It adopted a multi-modal approach, fully exploiting the LLM's ability to integrate multimodal input, by inputting audio-text pairs.The text is responsible for describing sounds, which is then fed to a text tokenizer and then a text embedding.The audio is processed by the AST and then projected to the LLM.This method achieved remarkable results, outperforming conventional audio-text models in classification tasks.But more importantly, it exhibits emerging audio reasoning and comprehension abilities that are truly absent in existing audio models.</p>
<p>LTU also uses what is called audio-text representation in the training data, offering many advantages over the previously classification-based method, but still struggles to distinguish between sounds in similar conditions, which is still a gap between the auditory reasoning abilities of humans and AI.A solution to this is called counterfactual training.This paper [90] proposes a novel framework that integrates counterfactual reasoning into audio-text representation learning.This approach utilizes a two-step prompting mechanism with large language models (LLMs) to generate counterfactual captions.These captions are then employed to enhance the model's ability to distinguish between subtle audio variations in similar contexts.For instance, differentiating between the sounds of fireworks and gunshots at an outdoor event.</p>
<p>Despite recent advances, AI systems still lag behind humans in auditory reasoning.Humans can effortlessly distinguish subtle sound differences, infer causes of sounds, and understand context-rich auditory scenes-capabilities that current models struggle with, especially in ambiguous or noisy settings.Future work could focus on improving context sensitivity and robustness to ambiguity, for instance, by enhancing counterfactual reasoning or integrating richer world knowledge into LALMs.Additionally, incorporating temporal reasoning and sound event causality could bring models closer to humanlike understanding, allowing them not just to hear, but to truly comprehend auditory experiences.</p>
<p>4) Tactile Reasoning: Tactile reasoning in artificial intelligence refers to the capability of AI systems to understand the characteristics, shapes, hardness, textures, and other attributes of objects through sensing and analyzing tactile information, thereby making decisions or performing tasks.It includes not only immediate reactions to object contact but also involves deep analysis of tactile data to assist robots or intelligent systems in performing precise operations and interactions in complex environments.The introduction of tactile reasoning makes the perception of AI in the physical world more comprehensive, thus improving the accuracy and adaptability of task execution, especially having significant application value in areas such as robotic grasping, manipulation, and human-machine interaction.</p>
<p>FuSe [95] adopts multimodal contrastive loss (aligning tactile, visual, and language data) and multimodal generative loss (enabling robots to generate natural language descriptions based on perceptions), enabling robots to understand and utilize tactile information.For example, after touching an object, a robot could generate a description ("this object feels soft") or complete a task according to tactile cues ("pick up the object that feels like a rope").This method significantly enhances the inference and decision-making capabilities of robots in scenarios with limited visual input.OCTOPI [93] acquires physical properties of objects (such as hardness, roughness, and bumpiness) using tactile videos, and transforms these tactile data into feature representations through a VLM visual encoder, then aligns them with LLM to achieve the integration of tactile signals and language reasoning.Through inferring these physical properties, it is capable of describing object attributes, comparing objects, and executing scene reasoning tasks based on tactile information, such as assessing the ripeness of an avocado.OCTOPI excels in physical reasoning tasks, particularly when visual information is incomplete.TALON [94] collects tactile data of gestures and object grasps using Hand-Scan sensors while combining it with visual information from cameras.By processing visual and tactile data through a visual encoder and multilayer perceptron (MLP), the model aligns features from both modalities into a language model.Ultimately, it uses LLM to synthesize visual, tactile, and linguistic information for inference and output, such as accurately recognizing gestures or objects.This multimodal fusion enables TALON to demonstrate higher recognition accuracy in complex tasks, especially where visual information is lacking.By integrating VLM with tactile feedback, ReAct [96] achieves the perception and reasoning about liquid objects.Initially, robots observe the liquid container visually to acquire basic color and shape information, followed by collecting tactile feedback (e.g., force/torque data) through shaking the container.After processing these tactile data into time-series graphs and integrating them with visual data into the VLM, the model leverages its physical common sense to infer the physical properties of liquids (such as viscosity).By comparing expected and actual liquid characteristics, the model ultimately identifies the type of liquid.</p>
<p>B. Dimension-based Reasoning</p>
<p>Reasoning of agents often relies on structured representations of information, and one fundamental way to classify reasoning processes is through their dependence on dimensional factors.Dimension-based reasoning refers to approaches that incorporate spatial and temporal structures into inference and decision-making.These dimensions play a crucial role in various AI applications, from robotic navigation and scene understanding to event prediction and dynamic planning.</p>
<p>Spatial reasoning enables AI systems to interpret and manipulate objects, relationships, and movements in physical or abstract spaces.It is essential for applications such as robotics, geographic information systems (GIS), computer vision, and spatial problem-solving.Temporal reasoning, on the other hand, focuses on how events unfold over time, capturing sequences, durations, and dependencies.This dimension is critical for areas like automated planning, natural language understanding, and forecasting future events.Both space and time serve as structural backbones for many reasoning tasks, guiding how AI models perceive, infer, and interact with the world.Subsequently, we explore AI reasoning techniques that leverage spatial and temporal dimensions, highlighting key methodologies, advancements, and challenges in each area.</p>
<p>1) Spatial Reasoning: Spatial reasoning in AI focuses on the ability to interpret, analyze, and manipulate spatial relationships between objects, environments, and abstract structures.This form of reasoning is essential for tasks that require an understanding of geometry, topology, and spatial configurations, enabling AI systems to perform navigation, object recognition, and spatial problem-solving.Unlike purely symbolic reasoning, spatial reasoning often involves processing continuous data, integrating perception with structured representations to make sense of spatial relationships.</p>
<p>Advancements in spatial reasoning span across multiple domains, including robotics, GIS, computer vision, and cognitive modeling.In robotics, spatial reasoning allows agents to navigate dynamic environments by mapping surroundings and planning motion trajectories [159].In GIS applications, AI leverages spatial inference to analyze geographic patterns and optimize resource allocation [160].In the domain of computer vision, spatial reasoning enhances scene understanding, enabling AI to infer object locations, orientations, and interactions.These diverse applications highlight the importance of spatial reasoning as a key dimension in AI research.</p>
<p>According to Fig. 5, we classify spatial reasoning as geometric reasoning, topological reasoning, and physical reasoning based on their underlying principles and applications.These categories capture distinct aspects of how AI systems interpret and manipulate spatial information, ranging from precise numerical computations to qualitative spatial relations and interactions with the physical world.Although many approaches based on deep learning, such as convolutional neural networks (CNN) [161] and graph neural networks (GNN) [162], contribute to the learning of spatial representation, our focus here is on how different reasoning methods explicitly process spatial relationships and perform inference.</p>
<p>Each category of spatial reasoning offers unique strengths and applications as shown in Fig. 9. Geometric Reasoning involves precise spatial relationships, including metricbased inferences, coordinate transformations, and visualspatial grounding.This approach is widely used in robotics, remote sensing, and VLMs.For example, the metric reasoning [163] in LLMs explores how LLMs perform metric-based spatial inference within GIS systems, while SpatialVLM [97]  TKDE'2023 LLM Prompt-based Forecasting TG-LLM [111] ACL'2024 Graph&amp;LLM Temporal Graph Enhances LLMs' Reasoning HSTT [118] TIP'2024 Graph&amp;Transformer Hierarchical Event Graph T3 [158] ICLR'2025 LLM Temporal Reasoning via Text</p>
<p>Spatial Positional Encoding</p>
<p>Pyramid is 3 cm above the cylinder.</p>
<p>Coordinate Extraction</p>
<p>GNN Blocks</p>
<p>Cube is next to both pyramid and cylinder.</p>
<p>Objects</p>
<p>The pyramid will fall onto the cylinder.</p>
<p>Scene Relations</p>
<p>Physical Variables Neural Predictor</p>
<p>Fig. 9. Pipeline for spatial reasoning in object-centric environments.The figure illustrates a multi-level architecture for spatial reasoning classified further into geometric, topological, and physical reasoning.Given a set of objects in a scene, the system focuses differently on extracting their 3D coordinates, relational structures, and physical variables.Spatial positional encodings and scene graphs are then fed into transformer and GNN blocks to reason about spatial configurations (e.g., "Cube is next to both pyramid and cylinder") and predict physical outcomes (e.g., "The pyramid will fall onto the cylinder").</p>
<p>enhances spatial reasoning in vision-language models by incorporating spatial priors.The study of Geometric Reasoning in AI has evolved significantly, with early work focusing on structured visual representations and coordinate-based spatial inference.One of the foundational contributions in this area is DA-Net [106], which demonstrates how AI models can infer 3D spatial relationships from textual descriptions.More recent advancements, such as SpatialCoT [107], leverage coordinate alignment and chain-of-thought (CoT) reasoning to enhance spatial inference in embodied AI planning.Topological Reasoning, on the other hand, focuses on qualitative spatial relationships such as adjacency, containment, and connectivity.Unlike geometric methods that rely on precise measurements, topological approaches are robust to variations in scale and perspective, making them particularly valuable for GIS, commonsense AI, and qualitative spatial reasoning (QSR) tasks.RoomSpace-100, a study in QSR [164] introduces a real-world simulation benchmark for qualitative reasoning, while GRASP [101] provides a grid-based evaluation framework for commonsense spatial inference.These studies highlight the importance of structured spatial reasoning and its role in AI-driven interpretation of real-world environments.It has been widely explored in qualitative spatial inference.Early frameworks such as Region Connection Calculus (RCC-8) [165] laid the foundation for modern topological reasoning.More recent efforts, such as Q-spatial [102], propose novel methods for quantitative spatial reasoning using reference objects, while the recent resaerch in probabilistic approach for spatial relations recognition [103] demonstrates how objectcentric spatial representations improve grounded spatial inference in vision models.</p>
<p>Physical Reasoning extends beyond static spatial structures, incorporating physics-based inference, object interactions, and spatially grounded decision-making.This category is particularly relevant in embodied AI, robotics, and real-world navigation.For example, TopV-Nav [105] explores how multimodal large language models (MLLMs) can leverage top-view spatial representations for object navigation, and VLMnav [108] investigates how spatial reasoning can be framed as a questionanswering task for zero-shot navigation.These approaches aim to bridge perception and reasoning, enabling AI to interact effectively in complex spatial environments.One of the earliest contributions in this area, Qualitative Process Theory (QPT) [166], provided a framework for reasoning about object interactions and force propagation using qualitative models.More recently, ZeroVLM [104] explores how AI models can improve spatial awareness by leveraging 3D scene reconstruction, significantly enhancing spatially grounded decision-making in multimodal AI systems.</p>
<p>2) Temporal Reasoning: Temporal reasoning in AI focuses on the ability to interpret, analyze, and manipulate temporal relationships between events, states, and actions over time.This form of reasoning is essential for tasks that require understanding of sequences, durations, and temporal dependencies, enabling AI systems to perform planning, activity recognition, and time-based inference.Temporal reasoning often involves processing dynamic and continuous data, integrating temporal patterns with learned representations to understand how situations evolve and unfold across time.</p>
<p>Currently, the mainstream approaches to temporal reasoning primarily rely on Large Language Models (LLMs) and graph methods.Therefore, we categorize temporal reasoning into two major types: LLM-based and Graph-based approaches, as shown in Fig. 5. Notably, we do not discuss sequencebased methods [167]- [172], as they primarily rely on recurrent neural networks such as RNN [173], LSTM [174], GRU [175], and Transformer [176] as their fundamental architectures, which are inherently designed to model sequential dependencies.These methods leverage the sequence encoding capabilities of such foundation models without explicitly incorporating temporal reasoning mechanisms.Instead, in this section, we focus on how different methods explicitly capture temporal information and perform reasoning over the time domain.</p>
<p>Temporal reasoning with large language models (LLM) can be categorized into two main approaches.As shown in 10 (a), the first approach directly leverages the reasoning capabilities of LLMs, transforming traditional time-series problemssuch as prediction, ordering, and temporal calculations-into a question-answering format.This allows LLMs to utilize their extensive pre-trained knowledge for inference.A representative method, PromptCast [110], reformulates temporal numerical inputs and outputs into prompts.For instance, a time-series forecasting problem can be transformed into: Context: "From t 1 to t obs , the average temperature of region U m was x m t1:t obs on each day."Question:"What is the temperature going to be on t obs+1 ?"Answer: "The temperature will be x m obs+1 degrees."However, due to the limited availability of temporal reasoning data in LLM training, enhancing their reasoning ability requires specialized datasets and fine-tuning strategies.Several methods [111], [112], [158], [177] address this limitation by constructing task-specific datasets.For example, TSQA [112] introduces a temporal-awareness module to generate time-sensitive embeddings, improving the model's sensitivity to temporal information.Additionally, TSQA employs contrastive reinforcement learning to refine its temporal reasoning abilities.Specifically, it constructs negative samples in two forms: Distant negatives, which correspond to entities and relations from different time periods.Close negatives, which are answers related to other events occurring within the same time frame.The positive samples are the ground truth answers.By leveraging contrastive learning and reinforcement learning, TSQA enhances the model's ability to learn the correct answers while mitigating the generation of incorrect ones.Another notable approach, TG-LLM [111], fine-tunes two large models to facilitate the transformation between text-to-graph and graph-to-temporal question answering pairs, thereby constructing a high-quality temporal reasoning dataset.Experimental results demonstrate that training on this dataset significantly improves the temporal reasoning capabilities of LLMs.The second approach encodes time-series signals into tokenized representations within LLMs [109], [113], [114], enabling them to process and reason over temporal data, as shown in Fig. 10 (b).Since pure textual features cannot fully capture the complexity of time-series data, many methods integrate additional modalities with language features for reasoning.A representative approach is MATM [109], which first encodes electroencephalogram (EEG) signals using an EEG encoder to obtain high-dimensional EEG features.Simul- taneously, a text encoder extracts high-dimensional language features from textual input.These features are then aligned and processed by an LLM to generate the final output.The core idea is to align multimodal signals while leveraging the reasoning capabilities of LLMs to solve tasks.A similar approach, TempoGPT [113], maps time-series data into discrete temporal tokens.A shared embedding layer is used to align both text tokens and temporal tokens before employing an LLM-based question-answering framework for sequence prediction.This process mirrors the multimodal information fusion mechanism in the human brain, where reasoning is not limited to a single modality but instead integrates multiple information sources.Compared to unimodal reasoning, this approach enhances inference accuracy by leveraging a more comprehensive representation of the data.Graph-based approaches [115]- [119] typically incorporate temporal information, extending traditional knowledge graphs into temporal knowledge graphs (TKGs) and leveraging conventional graph-based reasoning methods, as shown in Fig. 10  (c).For instance, Know-Evolve [115] models fact occurrences in temporal knowledge graphs as a temporal point process and employs a deep recurrent network to capture the dynamic evolution of entity embeddings, enabling structured temporal reasoning.TiPNN [116] employs a unified history temporal graph to comprehensively capture and encapsulate historical information.It then defines query-aware temporal paths on this graph to model historical path information relevant to a given query, enabling effective reasoning.Similarly, CTRN [117] extracts implicit temporal features and relation representations for each temporal reasoning query using BERT and an entity-time module.These features are then integrated to generate implicit temporal relation representations, which are used for reasoning.Notably, HSTT [118] effectively addresses the video question-answering (VideoQA) problem by constructing an event graph.This approach organizes multilevel visual concepts and their spatiotemporal relationships into a structured event graph, which guides the model in accurately encoding contextual information between nodes.The reasoning process is formulated as a question-answering task.Specifically, the method classifies visual elements into four categories: Objects, Relations, Scenes, and Actions.Objects are linked by Relations, forming a Scene within a single frame, while multiple Scenes over time constitute an Action.For temporal order questions, the reasoning process starts from Objects in the question text and traces upward through the graph to locate corresponding Actions at specific time points.Conversely, when querying object information at a given timestamp, the reasoning follows a top-down approach-starting from Actions and tracing down through the graph to identify relevant Objects.This structured approach enables more precise spatiotemporal reasoning, improving performance on VideoQA tasks.</p>
<p>Current temporal reasoning methods face several key challenges.First, existing approaches often struggle with complex time series, particularly in dynamic environments where reasoning capabilities are limited.Many models rely on fixed time windows and linear structures, failing to effectively adapt to nonlinear and fluctuating temporal patterns.Second, current temporal reasoning models are limited in their ability to reason over long time spans, making it difficult to capture long-term dependencies, which restricts their application in long-term prediction and complex tasks.The need for real-time reasoning is especially critical, as it requires AI systems to handle rapidly changing dynamic data and make quick decisions.Current methods are relatively weak in this regard.Finally, most temporal reasoning methods show limited performance in multimodal data fusion, especially in effectively integrating time-related data from different sources.Future temporal reasoning methods need to enhance their ability to process nonlinear and dynamic time series, improve performance in long-term dependency reasoning, and advance multimodal data integration.Additionally, real-time reasoning will be a crucial area of development, as AI systems must be able to quickly adapt to changing temporal patterns and respond immediately, providing more reliable reasoning and decisionmaking support in practical applications.</p>
<p>C. Logic-based Reasoning</p>
<p>In the field of AI reasoning, neuro-symbolic learning [178] has emerged as a crucial approach to logical reasoning, integrating the learning capabilities of neural networks [161], [173], [174], [179] with the structured representations in symbolic logic to build more powerful reasoning systems.Traditional symbolic systems rely on logical rules and knowledge graphs, excelling in structured data processing but struggling with unstructured data.In contrast, neural networks are adept at learning patterns from perceptual data but lack transparent reasoning mechanisms.Neuro-symbolic approaches aim to bridge these limitations by constructing a complementary reasoning framework as shown in Fig. 11.</p>
<p>On the one hand, neural networks can optimize the search process of symbolic reasoning, accelerating solution space exploration and improving inference efficiency.For instance, methods such as pLogicNet [180] and ExpressGNN [181] leverage neural networks to parameterize the posterior computation of probabilistic graphical models, significantly enhancing symbolic reasoning capabilities.Additionally, inductive logic programming (ILP) methods like NLIL [182] can automatically induce logical rules from data, providing new knowledge for symbolic reasoning and further improving its inference performance.On the other hand, symbolic reasoning imposes structured constraints on neural network learning, improving generalization and interpretability.For example, the neuro-symbolic concept learner (NS-CL) [183] integrates visual perception, semantic parsing, and symbolic reasoning to convert visual scenes into object-based symbolic representations, using executable logic programs to complete visual question answering (VQA) tasks.A classic example, Deep-ProbLog [184], [185], combines deep learning with probabilistic logic programming by introducing "neural predicates" as interfaces that map continuous embeddings from neural networks to discrete logical expressions in symbolic reasoning.By leveraging gradient semiring optimization [186] tools, it enables end-to-end training, facilitating efficient collaboration between neural networks and symbolic reasoning, thereby enhancing model interpretability and inference capability.BPGR [187] follows a similar approach, using neural networks to accelerate symbolic reasoning while leveraging symbolic knowledge to refine neural models.</p>
<p>Overall, neuro-symbolic reasoning integrates the information extraction capabilities of neural networks with the logical</p>
<p>Train Examples</p>
<p>Generate Hypotheses</p>
<p>Each number is a multiple of the previous one.</p>
<p>Each number in the sequence is the square of the previous one.</p>
<p>The sequence is an arithmetic sequence.c) are the processes in inductive reasoning, deductive reasoning, and abductive reasoning, respectively.We refer to the flowcharts from the recent methods HypoSearch [121], Natural Program [123] , and MAR [126].</p>
<p>Implement</p>
<p>… The man in a T-shirt chokedon food is vomiting into thetoilet.</p>
<p>Relation Memory
A Awake G Get up … … P Pick up H Hold H Hold T Throw Graph Construction A V … B C D X Pick up Hold Throw P H T G Node Split A V … B C D P H T G X 2 X 1 X Action Chain Search A P H T G X 2 X 1 V … B C D Vomit Devour (a) (b) (c)
inference mechanisms of symbolic reasoning.This approach aligns with our definition of AI agent reasoning, where information is acquired from the environment and processed within an internal representation to facilitate logical inference and decision-making.Beyond neuro-symbolic learning, we now delve into a more detailed discussion of different aspects of logical reasoning, including inductive reasoning, deductive reasoning, and abductive reasoning.1) Inductive: Inductive reasoning is a form of inference that derives general principles from limited observations.For example, after observing multiple white swans, one may infer that all swans are white.Han et al. (2024) [188] found that GPT-4 [189] performs comparably to humans in attribute induction tasks, accurately inferring attribute-based generalizations in most cases.However, research also indicates that it struggles with non-monotonic reasoning and exhibits differences from human inductive reasoning.This suggests that large models can serve as useful tools for studying inductive reasoning while also requiring further refinement to enhance their reasoning capabilities.</p>
<p>Current approaches to inductive reasoning primarily rely on hypothesis generation and selection strategies, which involve generating candidate rules, filtering valid rules, and integrating symbolic execution or program execution to validate and optimize reasoning performance.For instance, HypoSearch [121] improves the inductive reasoning ability of large language models by generating hypotheses at multiple levels of abstraction and transforming them into executable Python programs.Specifically, this approach first prompts the model to generate multiple abstract hypotheses about a given problem in natural language.These hypotheses are then translated into executable code, tested on observed data, and generalized to new inputs for validation as illustrated in Fig. 12 (a).IHR [122] adopts an iterative propose-select-refine mechanism, making the inductive reasoning process more aligned with human cognition.Their findings indicate that while large models excel at generating candidate hypotheses, they exhibit significant limitations in rule application, such as failing to correctly apply their own proposed rules and demonstrating high sensitivity to 2) Deductive: Deductive reasoning follows strict logical rules to derive necessarily true conclusions from given premises.For example, given the premises "All humans are mortal" and "Socrates is a human," we can deduce the conclusion that "Socrates is mortal."[190] investigates the generalization ability of deductive reasoning by testing multiple deductive rules, revealing that LLMs can generalize in compositional proofs but struggle with longer reasoning processes, particularly in case-based reasoning and proof by contradiction, where explicit demonstrations are required.</p>
<p>Recent research mainly focuses on enhancing the deductive reasoning ability of large language models (LLMs).One key approach, Natural Program [123], is structured stepwise verification.This method, exemplified by the Natural Programs format, enables models to verify their reasoning through step-by-step decomposition.As a result, it improves reasoning reliability and consistency, as shown in Fig. 12  (b).Additionally, [124] introduces guided reasoning tools "LOGICGUIDE", which integrates formal logical systems to constrain model generation, ensuring logical coherence and reducing hallucinated reasoning.This method has shown particular effectiveness in structured domains like legal reasoning.</p>
<p>3) Abductive: Abductive Reasoning aims to identify the most plausible hypotheses to explain observed phenomena.For example, upon seeing wet streets, one might infer that "it has just rained."Abductive reasoning is widely applied in realworld scenarios, particularly in scientific discovery, medical diagnosis, and causal inference.Compared to deductive and inductive reasoning, abductive reasoning presents three distinct challenges: (i) it requires imagination to hypothesize beyond observed facts; (ii) it seeks to uncover reasonable causal structures among observed events; and (iii) it is closely tied to everyday reasoning, where conclusions must be drawn under incomplete or ambiguous information.</p>
<p>Current research enhances abductive reasoning by modeling causal relations more explicitly, either through causalaware neural architectures or through symbolic graph-based reasoning that guides plausible hypothesis generation.One critical approach is causality-aware hierarchical reasoning.For instance, VAR [125] proposed REASONER (Causal and Cascaded Reasoning Transformer), which builds upon a Transformer encoder-decoder architecture.It employs a directional positional embedding strategy to capture causal dependencies among premise events, enabling the model to construct dis-criminative representations.Additionally, REASONER adopts a cascaded decoding mechanism, leveraging a confidenceguided multistep reasoning strategy to optimize premisehypothesis matching and improve reasoning reliability.Another line of research focuses on causal structure modeling and symbolic reasoning to enhance the abductive reasoning capabilities of LLMs.For example, MAR [126] introduced a hierarchical causal reasoning model, which captures causal dependencies between premise events and incrementally refines hypothesis generation, improving coherence and logical consistency.Furthermore, MAR proposed graph-aware reasoning as shown in Fig. 12 (c), which leverages the reasoning capabilities of symbolic networks.By utilizing Dijkstra's algorithm to search for the optimal causal path within an event graph, this approach enhances hypothesis selection and improves inference accuracy.</p>
<p>Despite recent advancements, current methods in logical reasoning have significant limitations [121]- [123], [125], [126], [180], [180], [181], [184], [185], [188], [190].Many models rely on shallow pattern matching or probabilistic associations rather than deep, structured inference, which undermines their reliability in complex environments.Additionally, existing systems often fail to seamlessly integrate inductive, deductive, and abductive reasoning, limiting their ability to handle multi-faceted tasks.The interpretability of these models remains another challenge, as reasoning processes are frequently opaque, reducing trust and hindering error analysis.Furthermore, causal reasoning, especially in abductive and counterfactual scenarios, is still underdeveloped, with most models focusing on correlation rather than causal relationships.Finally, current systems are not well-aligned with human cognitive strategies, such as proof by contradiction or analogical reasoning, which affects their practical usability in dynamic, real-world settings.</p>
<p>D. Interaction-based Reasoning</p>
<p>As introduced in Chapter II, socialization has always been an indispensable and important part of human behavior.The ability to reason within interactive contexts, understanding others' intentions, predicting their actions, and adapting responses accordingly, is a defining characteristic of human intelligence.In artificial intelligence, interaction-based reasoning extends this capability to machines, enabling them to engage meaningfully with other agents, whether human or artificial.Unlike reasoning in static or isolated environments,</p>
<p>Agent-agent Interaction</p>
<p>Human-agent Interaction
Equality</p>
<p>Collaborative Decision</p>
<p>Computing alternate route to Station 4 to avoid blocked aisle.</p>
<p>Starting assembly on Unit 12 … Batch C will arrive at Line B in 45 seconds</p>
<p>Given the congestion near Line A and the delay in material supply… Fig. 13.Taxonomy of agent-agent and human-agent interaction reasoning across equality and inequality dimensions.This framework categorizes interaction paradigms based on the axis of equality and the nature of interaction, agent-agent versus human-agent.In the top-left quadrant (Cooperative Interaction), agents coordinate as equals, sharing reasoning tasks.The top-right quadrant (Symbiotic Partnership) illustrates human-agent collaboration rooted in mutual reasoning, where the human and AI exchange insights, feedback, and jointly derive decisions.In the bottom-left quadrant (Adversarial Interaction), agents engage in performance-driven or security-sensitive debates, exposing reasoning conflicts and uncertainty in unequal conditions in this way to find an acceptable final solution.Finally, the bottom-right quadrant (Hierarchical Partnership) depicts human-led task delegation to agent subordinates, where agents reason within limited autonomy, executing spatial, causal, and temporal reasoning under top-down directives.We highlight how reasoning manifests differently across interaction types and control hierarchies.</p>
<p>interaction-based reasoning requires AI to dynamically process multi-agent interactions, shared goals, competing incentives, and evolving communication patterns.Recent advancements in LLMs, multi-agent reinforcement learning (MARL), and neuro-symbolic AI have significantly enhanced AI's ability to perform interaction-based reasoning.AI agents can now coordinate tasks, resolve conflicts, and align with human expectations in increasingly complex environments.However, challenges still remain, especially in high-risk applications where AI-driven decisions impact human lives.In this section, we classify interaction-based reasoning into two primary categories, which could be further discovered in Fig. 13: AI-AI reasoning, which focuses on multi-agent systems and autonomous coordination between artificial agents, and AI-Human reasoning, which explores how AI systems interact, collaborate, and align with human cognition and decisionmaking.The following subsections examine these categories in detail, analyzing key methodologies, research advancements, and open challenges in this rapidly evolving field.</p>
<p>1) Reasoning based on Agent-Agent Interaction: Multiagent reasoning is a foundational concept in artificial intelligence, tracing back to Minsky's Society of Mind theory [194], which proposed that intelligence emerges through interactions among multiple specialized sub-agents.This view laid the groundwork for distributed artificial intelligence (DAI) and multi-agent systems (MAS), where reasoning emerges not from isolated cognition, but from the coordination, negotiation, and sometimes competition among autonomous agents.</p>
<p>Modern approaches to agent-agent reasoning can be broadly categorized into cooperative and adversarial interactions.In cooperative scenarios, agents collaborate toward shared goals through explicit communication, planning, and joint decisionmaking.For example, DERA [127] enables decentralized emergent role allocation by learning specialized agent roles in team-based environments, while RoCo [128] introduces rolebased coordination mechanisms using large language models (LLMs) to facilitate structured cooperation among AI agents.Conversely, adversarial interaction focuses on competitive</p>
<p>Category</p>
<p>Method Publication Backbone Highlights</p>
<p>Agent-Agent</p>
<p>CaPo [191] ICLR'2025 LLM Long-term cooperative planning CoELA [192] ICLR'2024 Cognitive architecture Modular framework for cooperation DERA [127] CoRR'2023 Reward augmentation Improved decentralized coordination RoCo [128] ICRA'2024 Robust MARL Resilient multi-agent cooperation ChatEval [129] CoRR'2023 Dialogue evaluation Benchmark for cooperative agents MAD [130] CoRR'2023 Multi-agent dialogue Encouraging cooperative behaviors Agent-Human PEER [132] ICLR'2023 Iterative editing model Collaborative text refinement LangGround [193] NeurIPS'2024 MARL Human-interpretable agent communication LISSA [131] IVA'2020 Virtual agent Socially aware human interaction Cicero [134] Science'2022 NLP + planning Strategic dialogue in games dynamics, where agents must reason strategically and respond to their opponents.These settings simulate negotiation, deception, or contest-based environments.MAD [130] introduces mechanisms for fostering diversity in agent behaviors by simulating adversarial dialogues, while ChatEval [129] evaluates agent dialogue quality through multi-agent debate, highlighting how adversarial reasoning can be used for robust evaluation and self-improvement.Both forms of interaction emphasize the importance of contextual reasoning, adaptive communication, and joint intentionality, revealing how collective intelligence emerges from the interplay between agents, whether aligned or opposed.</p>
<p>2) Reasoning based on Agent-Human Interaction: As AI systems transition from passive tools to active collaborators, reasoning in agent-human interaction becomes critical.This domain emphasizes how AI agents understand, adapt, and work with humans in meaningful and trustworthy ways.Unlike autonomous systems that operate in isolation, interactive agents continuously integrate human input, ensuring alignment with human preferences, ethical norms, and situational nuances.Two key models have emerged in this area: hierarchical directive interaction and symbiotic partnership interaction.</p>
<p>In hierarchical directive models, humans occupy a supervisory or instructional role, providing commands or feedback that guide the AI's behavior.These systems emphasize controllability and transparency.For instance, LISSA [131] is a virtual agent that supports elderly users through socially assistive dialogue, relying on structured turn-taking and human feedback.Similarly, PEER [132] introduces a promptingbased framework where human-crafted examples serve as soft directives that guide model behavior through few-shot prompting.In contrast, symbiotic partnership models aim to establish more egalitarian collaborations, where agents reason about human goals, adapt dynamically, and co-evolve with their human counterparts.SAPIEN [133] introduces a multiagent platform where embodied agents and humans co-reason about physical tasks in shared environments.Meanwhile, Cicero [134], developed for the game Diplomacy, showcases advanced strategic reasoning and natural language dialogue to negotiate and coordinate with humans in real time, achieving human-level performance in a deeply social and adversarial environment.These approaches highlight the shift from one-way control to two-way reasoning, where agents not only respond to instructions but also anticipate needs, explain their reasoning, and build trust through adaptive, contextsensitive interaction.Furthermore, interactive learning serves as a powerful mechanism for improving AI reasoning over time.Instead of relying solely on static datasets, agent-human dialogue enables continuous refinement.Through feedback, clarification, and real-world conversations, AI systems can improve their ability to infer intent, resolve ambiguity, and respond appropriately to nuanced human behavior.This realtime adaptability is crucial for deploying AI in high-stakes, dynamic settings such as healthcare, education, and legal reasoning, where interpretability and responsiveness are paramount.</p>
<p>IV. BENCHMARKS AND DATASETS</p>
<p>To advance the development of intelligent agents capable of human-like reasoning, it is essential to evaluate their performance across a diverse set of cognitive dimensions.Reasoning in AI spans multiple modalities and domains, as we introduced before.A wide array of benchmarks has been proposed to capture these aspects, each designed to test different reasoning capabilities in isolation or combination.In this section, we organize and describe representative datasets across these categories, highlighting their design focus, task structure, and relevance for training or evaluating generalist reasoning models.Apart from this, some potential needs for improving reasoning benchmarks have also been discussed.</p>
<p>1) Visual: In the field of visual reasoning, VQA v1.0 [195] contains 250k images, 760k open-ended questions about these, and 10 million answers to these questions, to support freeform and open-ended visual question answering tasks.VQA v2.0 [196] improves upon VQA v1.0 [195] by associating two similar images with each question, reducing language bias in the dataset.GQA [198] contains 113K images and 22M questions covering various reasoning skills, generated using scene graph structures and computational linguistics approaches, offering fine-grained control over the distribution of the dataset and supporting new evaluation metrics.GQA-OOD [237] introduces distribution shifts into the validation and test sets based on the GQA dataset [198], allowing for the assessment of models and algorithms under Out-Of-Distribution (OOD) settings, proposing a new evaluation SPARQA [217] 2021 Situated QA Dimension (Spatial) 6k QA pairs GRiT [218] 2022 Spatial graph reasoning Dimension (Spatial) 48k graphs TQA [219] 2017 Science diagram QA Dimension (Spatial) 26.3k questions CoDraw [220] 2019 Collaborative spatial grounding Dimension (Spatial) 10k dialogues TouchDown [221] 2019 Navigation Dimension (Spatial) Current VQA benchmark datasets in the general domain encompass a wide range of task types, from simple object recognition to complex scene understanding and logical reasoning.However, they still face several challenges, including: bias and imbalance in question types, with a tendency towards simple object recognition rather than complex scene understanding and logical reasoning; the singularity of answers, often providing only one "correct" answer while neglecting the multiplicity and subjectivity inherent in real-world scenarios; insufficient systematic support for the need of additional commonsense or background knowledge, which limits the model's ability to handle questions requiring external knowledge; and evaluation metrics that predominantly focus on accuracy, lacking consideration for aspects such as model interpretability and uncertainty estimation.These factors collectively constrain the effectiveness and development potential of existing VQA systems in practical applications.</p>
<p>2) Lingual: MR-Ben [202] is a process-based benchmark that demands meta-reasoning skills (e.g., locate and analyze errors in automatically generated reasoning steps).It is suited for evaluating system-2 slow thinking, mirroring the human cognitive process.It comprises 5,975 questions across a wide range of subjects.RM-Bench [203] is a novel benchmark designed to evaluate reward models based on their sensitivity to subtle content differences and resistance to style biases.LR 2 Bench [204] is a novel benchmark designed to evaluate the Long-china Reflective Reasoning capabilities of LLMs.It contains 850 samples across 6 CSPs.Big-Math [205] is a dataset of over 250k high-quality math questions that have verifiable answers, are open-ended, and have closed-form solutions.It is an order of magnitude larger than common math reasoning datasets, with problems filtered to best suit RL.LongReason [206] is a new synthetic benchmark consisting of 794 multiple-choice reasoning questions with diverse reasoning patterns across different task categories.It is useful for evaluating the long-context reasoning abilities of LLMs.BIG-Bench Extra Hard [207] is a new benchmark designed to push the boundaries of LLM reasoning evaluation.It replaces each task in BBH (BIG-Bench Hard) with a novel task that probes a similar reasoning capability with significantly increased difficulty.ResearchBench [208] is the first largescale benchmark for evaluating LLMs with a near-sufficient set of sub-tasks of scientific discovery.MastermindEval [209] is a simple, scalable, and interpretable deductive reasoning benchmark inspired by the board game Mastermind.It supports agentic evaluation and deductive reasoning evaluation.Z1 [210] is a dataset of 107k simple and complex coding problems paired with their short and long solution trajectories.</p>
<p>3) Auditory &amp; Tactile: Clotho [212] is a dataset for audio captioning.It was built with a focus on audio content and caption diversity, and the splits of the data are not hampering the training or evaluation of methods.AudioCaps [211] is a large-scale dataset for audio captioning, created using audio clips from AudioSet.It provides crowd-sourced natural language descriptions focused on general audio events, and contains both expert-annotated and user-generated captions to support diverse training and evaluation settings.Tac-Quad [215] contains paired multi-sensor, multi-modal tactile data, supporting fine-grained tactile tasks (e.g., cross-sensor generation) and coarse-grained tactile tasks (e.g., cross-sensor matching).The dataset includes 17,524 fine-grained contact frames from 25 objects and 55,082 coarse-grained contact frames from 99 objects.FoTa [213] contains over 3 million tactile images from 13 camera-based tactile sensors, covering 11 tasks.Touch100k [214] contains 100,147 tactile-languagevision multimodal data entries, providing multi-granularity tactile descriptions and supporting tasks such as material property recognition and robotic grasping prediction.FuSe [95] consists of 27,000+ robot trajectories and includes a variety of sensory data (vision, touch, audio, proprioception) and language instructions.It is used for fine-tuning robot policies on heterogeneous sensory modalities, like touch and sound.</p>
<p>Current tactile benchmark datasets generally face challenges such as limited scale, insufficient diversity, and restricted practical applicability.Although existing datasets have integrated multimodal data and support a variety of evaluation tasks, their scale remains inadequate for training complex models.The coverage of materials and interaction modes is relatively homogeneous, and there is a strong dependency on specific hardware.Future tactile benchmark datasets should evolve towards deeper integration of multimodal data and the combination of simulated and real-world data to address these limitations and enhance their versatility and utility.</p>
<p>4) Spatial: RAVEN [216] is a visual reasoning dataset with 1.12M analogy problems designed to assess spatial structure understanding.It emphasizes rule-based pattern recognition in matrix-style puzzles, evaluating relational and hierarchical spatial reasoning.SPARQA [217] provides 6k situated QA pairs requiring spatial-temporal comprehension of visual scenes.It challenges models to resolve object relationships within complex visual layouts.GRiT [218] consists of 48k graph-structured instances for spatial reasoning, combining image understanding with structured representations to evaluate relational perception.TQA [219] introduces 26.3k sciencerelated QA examples involving diagrams, testing models on layout interpretation in educational contexts.CoDraw [220] presents 10k dialogues where one agent guides another in recreating a scene through spatially grounded instructions, emphasizing collaborative and referential spatial understanding.TouchDown [221] contains 9,326 navigation tasks in real street-view environments, testing how well models interpret spatial descriptions for geolocated reasoning.Roomto-Room [222] offers 21,567 trajectory samples in 3D environments, focusing on natural language instruction-following grounded in spatial scenes.SpatialSense [223] includes 5,000 images with captions annotated for spatial relations, enabling textual extraction of spatial predicates like "above," "next to," or "under."Current spatial reasoning benchmarks primarily address fundamental tasks such as object localization, spatial relation classification, and basic navigation.However, these tasks often rely on static or simplified environments that fail to capture the complexity of real-world spatial cognition.</p>
<p>Future datasets should incorporate dynamic and interactive spatial scenarios, such as embodied navigation in cluttered or unfamiliar environments, multi-agent spatial collaboration, and context-aware spatial planning, to better evaluate the adaptability, generalization, and compositional reasoning capabilities of AI systems in realistic spatial settings.</p>
<p>5) Temporal: Time-Sensitive QA [224] is a dataset with 68k questions from WikiData, used to assess LLMs' ability in time-sensitive QA.TempLama [225]  is a time reasoning with 10 tasks and 526.7k multiple-choice questions.It evaluates reasoning in event sequences, arithmetic, frequency, and duration, revealing that current models fall short of robust, human-level performance in understanding implicit time.Current temporal reasoning datasets mainly focus on basic time understanding tasks such as event ordering and duration estimation.Future benchmarks and datasets should emphasize dynamic event prediction and causal reasoning over time to better reflect real-world temporal inference challenges.</p>
<p>6) Logic: ReColr [230]is a reading comprehension dataset focusing on logical reasoning, split into EASY and HARD sets to evaluate model performance on logical reasoning without exploiting dataset biases.Models struggle on the HARD set, highlighting the need for enhanced reasoning abilities.Logic-NLI [231] is a diagnostic dataset to evaluate language models on first-order logic (FOL) reasoning, with tasks separating logical inference from commonsense reasoning, aiming to test accuracy, robustness, and traceability.FOLIO [232] is a dataset designed for reasoning in natural language with first-order logic annotations, evaluating logical correctness and reasoning capabilities in models.AR-LSAT [233] focuses on three LSAT tasks (analytical reasoning, logical reasoning, and reading comprehension), pushing models to demonstrate their ability to handle complex reasoning and symbolic knowledge.LogiQA 2.0 [234] evaluates models' logical reasoning abilities through multiple-choice questions on various logical patterns, testing the application of inference rules in natural language.LogicBench [235] tests logical reasoning across propositional, first-order, and non-monotonic logics with 25 distinct inference rules, evaluating models' ability to apply single inference rules in diverse logical scenarios.LINGOLY [236] assesses models' reasoning capabilities in low-resource or extinct languages, testing in-context identification and generalization of
Robot Dogs Humanoid Robots Unmanned Aerial Vehicles Robot Manipulators
Fig. 14.Representative categories of modern robotic platforms.We showcase four primary types of embodied robotic agents: robot dogs for agile terrain traversal, unmanned aerial vehicles (UAVs) for aerial sensing, humanoid robots designed for human-centric tasks, and robot manipulators specialized in precise physical interaction within structured environments.linguistic patterns in complex tasks.GSM8K [149] tests language models on grade school-level math problems, focusing on multi-step arithmetic reasoning.It challenges models to solve problems involving basic calculations and logic.</p>
<p>Current logical datasets often rely on synthetic patterns or exam-style questions, lacking real-world abstraction, multihop reasoning, and higher-order logic.Future datasets should emphasize diverse, scalable formats with generative reasoning techniques and better capture symbolic structure, uncertainty, and generalization potential.</p>
<p>V. APPLICATIONS OF REASONING</p>
<p>With the increasing sophistication of AI, reasoning has become a fundamental component in robotics and embodied agents, enabling them to operate in dynamic, real-world environments.Unlike traditional AI models that function in constrained digital spaces, these agents interact with the physical world, requiring advanced cognitive abilities to perceive, analyze, and act upon complex inputs.Reasoning is essential for tasks such as spatial navigation [239], [240], object manipulation [241], decision-making [242], and human collaboration [243], as it allows these systems to adapt to unpredictable conditions and refine their actions based on experience.As AI-powered robots transition from controlled laboratory settings to real-world applications, they must integrate multiple reasoning paradigms, from spatial and physical reasoning to social considerations.Embodied AI, in particular, necessitates a multi-faceted approach to reasoning, combining sensory data with logical inference to make real-time decisions.The challenges they face-such as uncertainty, partial observability, and the need for rapid response-further underscore the importance of efficient reasoning mechanisms.The following sections explore specific domains where reasoning plays a crucial role in robotics and embodied AI, highlighting how these systems process information, learn from interactions, and execute tasks in complex settings.</p>
<p>A. Physical Agents</p>
<p>Robotics and embodied agents operate in the physical world, requiring advanced reasoning abilities to perceive, plan, and execute complex actions in dynamic environments.As listed in Fig. 14, unlike purely digital AI systems, these agents continuously interpret sensor inputs, handle uncertainty, and make decisions to interact effectively with the real world [244].The reasoning capabilities of these agents are crucial not only for navigating the environment [245] but also for performing tasks with precision and adaptability [246].This section delves into the role of reasoning in robotics, emphasizing how it underpins decision-making and action execution.</p>
<p>A primary application of reasoning in robotics is autonomous navigation and path planning.Robots must analyze spatial layouts, detect obstacles, and compute optimal paths, often in complex, cluttered environments.This involves the integration of geometric reasoning with real-time sensor data, enabling the robot to adjust its movement dynamically.For instance, autonomous vehicles rely on reasoning to assess road conditions, predict pedestrian behavior, and execute split-second decisions, ensuring safety.Similarly, robots in warehouses must combine topological reasoning with sensor inputs to identify and optimize retrieval paths, minimizing delays and avoiding collisions.The ability to reason about the environment and predict changes in real-time is vital for both safety and efficiency.</p>
<p>Beyond navigation, reasoning is critical in object manipulation and interaction.Robots performing tasks like manufacturing, healthcare, and domestic assistance need to understand the physical properties of objects, such as weight, texture, and fragility.Physical reasoning in these contexts allows robots to adjust their actions based on these properties.For example, a robotic arm might use reasoning to adapt its grip strength based on the fragility of an object, preventing breakage.In domestic environments, service robots must reason about their surroundings to perform tasks like pouring liquids without spilling or assembling furniture.Through predictive reasoning, these robots can refine their actions, ensuring higher accuracy and adaptability in their operations.</p>
<p>In human-robot collaboration, reasoning plays an indispensable role in ensuring seamless interaction and synchronization between humans and robots.As robots are integrated into environments such as workplaces and homes, they must not only understand physical tasks but also the social dynamics of working with humans.For example, medical robots assisting in surgeries must synchronize their actions with the surgeon's movements, making real-time adjustments based on the procedure's progress.Likewise, exoskeletons and prosthetics rely on biomechanical reasoning to adapt to the user's movements, ensuring effective collaboration and safety.The ability to interpret human intent and non-verbal cues-such as gestures or postures-is critical in these contexts, requiring robot reason in a manner that goes beyond mere physical action execution.</p>
<p>Moreover, reasoning in physical agents extends beyond individual robots to multi-agent systems, where collective intelligence enhances task completion.In scenarios like drone swarms for environmental monitoring or robotic teams in disaster response, the reasoning process must accommodate interaction, coordination, and negotiation.Each agent must assess its capabilities in relation to others, deciding when to act independently or collaborate.In these systems, decentralized reasoning allows agents to share information and optimize performance collectively.In industrial settings, for instance, robotic arms may work together on assembly lines, adapting to real-time production requirements and collaborating to meet tight deadlines without relying on a centralized control system.</p>
<p>Overall, reasoning in embodied agents connects perception, decision-making, and action execution, enabling robots to navigate physical spaces, manipulate objects, and collaborate with humans and other agents effectively.The integration of advanced reasoning techniques with physical action is what sets embodied agents apart from purely digital systems, giving them the ability to function in the real world with both precision and adaptability.</p>
<p>B. Virtual Agents</p>
<p>In contrast to embodied agents, disembodied AI operates in purely digital and conceptual spaces, relying on reasoning to analyze data, simulate environments, and optimize decisionmaking.These systems do not interact with the physical world through sensors or actuators; instead, they engage with structured and unstructured data, abstract problem-solving, and multi-agent coordination.These capabilities are fundamental in knowledge-intensive domains, strategic problem-solving, and interactive AI systems.One of the most prominent applications of disembodied AI is in conversational agents and language models.Systems like ChatGPT [189] exemplify how AI can leverage reasoning to generate coherent, contextually relevant, and logically structured responses.These models process vast amounts of textual data, infer relationships between concepts, and dynamically adjust their outputs based on user input.Beyond simple text generation, their reasoning mechanisms allow them to engage in complex discussions, provide explanations, and even simulate problem-solving processes in technical and scientific domains.Another key area of disembodied AI is automated reasoning in knowledgebased systems.AI-driven assistants in law, science, and healthcare employ logical inference to analyze regulations, detect patterns in research data, and suggest optimal courses of action.These systems extend beyond retrieving pre-existing knowledge by applying reasoning to synthesize new insights, validate arguments, and reconcile conflicting information.For instance, automated theorem provers utilize formal logic to verify mathematical proofs, while AI-driven research assistants scan and analyze large corpora to identify emerging scientific trends.Strategic reasoning is another crucial application of disembodied AI, particularly in game theory, cybersecurity, and financial modeling.AI-driven trading systems, for instance, reason over market trends, competitor behaviors, and risk assessments to optimize investment strategies.In cybersecurity, reasoning enables AI to predict and counteract cyberthreats by simulating potential attack vectors and deploying defensive measures.Multi-agent strategic systems, such as those used in military simulations or competitive gaming, employ advanced reasoning to anticipate adversarial moves, negotiate optimal strategies, and make real-time adjustments.The intersection of reasoning and computational creativity also demonstrates the versatility of disembodied AI.From AI-generated art and music to AI-assisted code development and scientific discovery, reasoning allows these systems to explore novel possibilities while adhering to defined constraints.</p>
<p>VI. FUTURE DIRECTIONS AND INNOVATIONS</p>
<p>Based on our proposed architecture, which spans from multimodal input perception to final reasoning output, we identify several key directions and innovations for enhancing the reasoning capabilities of future AI systems.Multimodal Inputs: Toward Selective and Adaptive Multimodal Perception.Most current AI systems are limited to processing static, single-modality inputs, such as pure text or isolated images, which stands in stark contrast to the human ability to dynamically shift and integrate attention across sensory modalities.In real-world environments, perceptual input is continuous and situation-dependent: when visual information is degraded, humans instinctively rely more on auditory or tactile cues; when multiple modalities are simultaneously available, we selectively focus on those most relevant to the task at hand.Inspired by this, future AI agents should support selective and adaptive multimodal perception, where choosing the most relevant modality-or combination thereof-not only enhances robustness, but also forms the foundation for effective and context-sensitive reasoning.One promising approach is the development of a Dynamic Multimodal Mixture-of-Experts (DMMoE) architecture, which draws on the brain's adaptive gain control mechanisms [247], [248].As shown in Fig. 15, in this framework, individual expert networks are assigned to different modalities, such as vision, language, audio, and touch, or tasks, such as Text-to-Speech (TTS), Multi-modal Entity Recognition (MER), and Optical Character Recognition (OCR).A learnable gating network continuously modulates the activation of each expert based on real-time sensory salience and task relevance.Their outputs are integrated into a shared representation space, while a global scheduler determines whether to process them in parallel or sequentially, depending on task complexity and latency constraints.This setup allows for context-aware, flexible engagement with the most informative modalities, enhancing robustness under partial observability and improving computational efficiency.The modular design also supports extensibility: new sensory experts and adaptive routing strategies can be introduced via meta-learning or online adaptation-bringing AI perception one step closer to human-like flexibility.Information Processing: Toward Unified Modal Representations for Cross-Modal Reasoning.Most current AI reasoning systems rely on separate modality-specific encoders and late fusion strategies, which often struggle to handle realtime multi-modal inputs in a coherent and adaptive way.In contrast, the human brain processes different sensory signalssuch as vision, audition, and touch-by converting them into a common electrochemical format.This unified signal representation enables seamless cross-modal integration and lays a foundation for efficient reasoning across sensory domains.Inspired by this, future AI systems should aim to develop a shared representation space that transcends modalityspecific encodings, allowing for more fluid and consistent reasoning across multi-modal inputs.One potential solution is to draw from neuroscience-inspired models such as Spiking Neural Networks (SNNs) [249], which mimic the event-driven and temporally coded nature of neural processing.By aligning information from different sources in the time domain, SNNs may provide a biologically plausible and computationally efficient path toward building unified representations for robust cross-modal reasoning, as shown in Fig. 15.Knowledge Base: Dual Memory Systems for Dynamic and Time-Sensitive Reasoning.Current AI models largely depend on static, pre-trained knowledge bases, which significantly limit their ability to reason over dynamic, evolving facts-especially those involving temporal information or long-term dependencies.In contrast, humans construct and update internal knowledge representations through continuous interaction with the world, while simultaneously drawing on external sources to verify or complement what they know.Inspired by this, future AI agents should be equipped with a dual knowledge architecture consisting of: (i) an offline, interaction-driven knowledge base that incrementally integrates information from the agent's embodied experience and dialogue history, and (ii) an online, timesensitive retrieval system that dynamically accesses up-todate information from external sources such as the internet or structured databases as shown in Fig. 15.This dual system not only enables AI agents to maintain a grounded and contextrich internal model of the world, but also to adapt flexibly when confronted with novel, uncertain, or time-critical reasoning scenarios.It is particularly crucial for tasks involving temporal causality, evolving facts, or multi-step reasoning under uncertainty.One promising direction is the development of an adaptive retrieval-controller architecture, which orchestrates when and how to consult internal versus external knowledge, based on current reasoning needs, confidence levels, and task requirements.Unlike traditional Retrieval-Augmented Generation (RAG) [250], which passively fetches documents to support static answers, this controller actively monitors reasoning progress, identifies knowledge gaps, and strategically queries the appropriate knowledge base-allowing for more robust, grounded, and temporally coherent reasoning.Foundation Models: Dual Role as Understanding Engines and Reasoning Assistants.Although large language models (LLMs) [83], [189], [251] and vision language models (VLMs) [252], [253] exhibit basic inference capabilities, their core strength lies in high-fidelity understanding-providing rich, reliable representations that feed into specialized reasoning modules.To strengthen this role, future work must prioritize the creation of higher-quality, diverse, and temporally annotated datasets capturing real-world concepts, contexts, and cross-modal relationships.Simultaneously, foundation models should serve as reasoning assistants, leveraging their learned statistical patterns to pre-process inputs, generate candidate hypotheses, and enforce structured heuristic rules derived from the different reasoning tasks.In this capacity, they scaffold subsequent specialized processes without replacing them.By enhancing dataset quality and embracing this dual role, foundation models will become indispensable both for understanding complex inputs and for guiding structured, modular reasoning in AI agents.Perceptual Reasoning: Toward Structured Intermediate Representations.Human neuroscience suggests that perception may construct relational maps rather than isolated feature lists.Functional Magnetic Resonance Imaging (fMRI) studies indicate that the parahippocampal place area (PPA) [254] encodes scene layouts through relational graphs, where nodes correspond to spatial anchors (e.g., landmarks) and edges represent boundary topology (e.g., adjacency, containment).Complementary evidence from hippocampal-entorhinal circuits suggests that cognitive maps link locations and events via node-edge structures, supporting both spatial navigation and episodic memory [255]- [258].Inspired by these biological mechanisms, emerging AI approaches propose embedding scene graphs as intermediate representations, elevating objects and their relationships to explicit components of the model's internal state.By explicitly modeling entities (nodes) and relations (edges), such architectures enable relational queries (e.g., structural support analysis) and improve robustness against occlusion through context propagation, akin to cortical scene-completion processes.A promising implementation integrates a Vision Transformer (ViT) backbone with a graph neural network (GNN).The ViT first detects entities and estimates pairwise relation scores via self-attention, while a dynamic GNN refines edge weights through graph attention layers (GATs), enforcing constraints like physical plausibility.This graph-centric loop-reminiscent of hippocampal replay for memory consolidation-enhances both interpretability and adaptability in complex environments.While direct biological equivalence remains unproven, this synergy between neural principles and AI design marks a step toward human-like perceptual reasoning.Dimensional Reasoning: Towards Continuous Spatiotemporal Neural Differential Reasoning.Current methods-such as 4D Gaussian splatting [259], [260]-discretely model spatial and temporal dimensions but fail to capture the fluid, continuously evolving nature of dynamic environments encountered by embodied agents.Inspired by the human parietal lobe, which seamlessly integrates spatial awareness with temporal sequencing, future AI systems should build continuous implicit representations of the world that jointly encode 3D structure and time-varying dynamics.As shown in Fig. 16, one promising direction is to leverage Neural Ordinary Differential Equations (Neural ODEs) [261] to learn the continuous-time evolution of scene geometry-rather than relying on predefined static parameters-and to integrate an event-driven spatiotemporal graph attention mechanism that dynamically selects and weights critical nodes (e.g., objects in motion and key events) as they occur.By forming a high-resolution 4D embedding that updates in real time, this framework enables fine-grained trajectory prediction, causal event inference, and long-horizon planning, thereby equipping embodied agents with more precise, coherent, and adaptable dimensional reasoning capabilities.Logical Reasoning: Toward Structured, Causal, and Human-Aligned Inference.Logical reasoning plays a pivotal role in enabling AI agents to derive conclusions from premises, test hypotheses, and make consistent, interpretable decisions.</p>
<p>Recent advancements in neuro-symbolic systems have laid a solid foundation by combining neural networks' ability to process perceptual input with the rule-based rigor of symbolic logic.However, existing models often treat logic superficially-relying on surface pattern matching or probabilistic associations-rather than deeply modeling structured inference.Inspired by this, future AI reasoning systems should move toward causality-aware, structure-constrained, and hierarchy-guided logical inference.This involves three directions.First, systems should encode and manipulate logic in structured, interpretable formats-such as program sketches or graph-based logic trees-enabling models to explicitly construct and verify reasoning chains across inductive, deductive, and abductive paradigms.Second, to align more closely with human-like reasoning, agents should be equipped with mechanisms to perform counterfactual thinking and proofby-contradiction, which are essential in scientific reasoning and legal argumentation.Third, logical reasoning must be grounded in causality: models should learn to represent and reason over causal graphs, distinguishing correlation from explanation.One promising direction is to develop a neurosymbolic planner that unifies symbolic logic programs with causal event graphs, enabling agents to simulate multiple inference trajectories, evaluate plausibility, and select the most coherent explanation-especially under partial observability.These structured logical systems will serve as the backbone of AI agents, supporting robust, transparent, and generalizable decision-making in complex environments.</p>
<p>Interactive Reasoning: Toward Intention-Aware and Socially-Coherent Agents.Interactive reasoning enables These systems would allow agents to simulate the beliefs and objectives of others while adjusting their own policy accordingly-akin to the theory of mind in humans.Technically, this could be achieved by coupling behavior trajectory modeling with learned causal priors, enabling agents to infer not only what others are doing, but why they are doing it.Additionally, grounding interaction within structured environments-via symbolic scene graphs, affordance maps, or dialogue ontologies-could provide an interpretable substrate for multi-agent reasoning.Importantly, interactive reasoning should extend beyond agent-agent coordination to encompass rich human-agent collaboration.Here, the agent must not only align with human preferences, but also continuously refine its behavior through interactive feedback and few-shot corrections.This demands a hybrid of reinforcement learning, online imitation, and neuro-symbolic adaptation, where agents can learn from sparse demonstrations and ambiguous signals in real-time.Such systems will ultimately support agents that are socially coherent, goal-aligned, and capable of evolving through interaction, paving the way for truly collaborative artificial intelligence.Just as Chain of Thought (CoT) [15] is inspired by the serial reasoning in ACT-R [14], many cognitive models from neuroscience can also provide valuable insights for AI reasoning architectures.For instance: Miller and Cohen's Model (PFC Cognitive Control).</p>
<p>Envisioning the Future of AI Agents</p>
<p>• Goal-Driven Multi-Step Reasoning: AI models, such as LLMs and reinforcement learning agents, can maintain a goal vector or context vector throughout reasoning, continuously biasing outputs toward task objectives.This could be implemented via a global task descriptor or a dynamic context-tracking mechanism that ensures the system remains aligned with the overarching goal • Cascaded Attention Scheduling: AI models handling multimodal or multitask inputs can incorporate a cascaded attention module [265], [266], where an initial coarsegrained filter (akin to posterior DLPFC) identifies relevant features, a mid-layer refines them, and a final layer (analogous to ACC) determines the output.This hierarchical filtering reduces noise and enhances robustness in complex environments.• Multi-Stage Decision Pipelines: Reinforcement learning and structured decision-making [267] can benefit from stage-wise decision decomposition, where a high-level policy selects focus areas before lower-level policies refine actions.This helps in stepwise strategy formulation and adaptive control.Baddeley's Working Memory Model.</p>
<p>• Multi-Buffer Memory Architectures: AI reasoning systems can implement dedicated memory buffers [268] for different modalities-e.g., separate caches for text sequences (like a phonological loop) and visual data (like a visuospatial sketchpad), orchestrated by a central executive module for reasoning and decision-making.• Parallel Perception &amp; Serial Control: Inspired by human memory constraints, AI models can parallelize low-level sensory processing while keeping high-level decisionmaking serial [269], [270].Transformer-based architectures [176] or RNNs [173] could benefit from separate caching mechanisms for different input modalities, with a reinforcement learning-based controller managing crossmodal interactions.Predictive Coding.</p>
<p>• Iterative Generation &amp; Correction: AI models can incorporate a self-supervised feedback loop [271], where generated outputs are iteratively compared against predefined input constraints, and if discrepancies exceed a certain threshold, the system refines its internal representation or reasoning path before producing the final output.This is particularly relevant for generative AI, where multiple iterations can improve coherence and accuracy.• Hierarchical Error Feedback: A layered architecture can mirror top-down priors and bottom-up corrections, where high-level modules predict global context (e.g., discourse structure in NLP or object relations in vision), while lower layers validate fine-grained details.This could enhance error correction in self-driving systems or autonomous robotics by integrating predictive models with real-time sensory updates.• Predicting Key Tokens: Predictive coding enables the brain to quickly adapt to environmental changes and optimize the understanding of causal relationships.Multimodal large language models perform nearly perfectly on simple feature recognition tasks, but their performance in causal reasoning remains significantly below human level [272].Inspired by the minimization of prediction error, future multimodal large language models could predict important visual tokens in advance during the visual encoding stage, retain key tokens, and improve reasoning speed [273], [274] while enhancing the reasoning capabilities of these models.Adaptive Control of Thought-Rational (ACT-R).</p>
<p>• Explicit Chain-of-Thought (CoT) Reasoning: AI models can adopt stepwise rule-based reasoning, akin to ACT-R's production system, ensuring that each reasoning step updates working memory before proceeding.This would make CoT-based inference more structured, preventing reasoning jumps or inconsistencies.Global Workspace Theory (GWT).</p>
<p>• Global Broadcasting Mechanism: GWT posits that consciousness emerges from the competition among multiple specialized modules for access to a central global workspace; once information enters this workspace, it is broadcast system-wide.AI systems can simulate this mechanism by introducing a global attention pool or a shared blackboard architecture within multimodal models.When salient information from a specific modality or task reaches a predefined threshold, it can be "broadcast" to other modules, enabling dynamic resource allocation and cross-module coordination.This mechanism offers significant inspiration for dynamic task scheduling and attention routing in large-scale AI systems.</p>
<p>VII. CONCLUSION</p>
<p>This survey is the first to systematically explore AI agent reasoning from a neuroscience perspective, offering a comprehensive framework that spans from perception to action.We defined AI agent reasoning by formulating three precise definitions and clarifying key concepts based on insights from neuroscience, which laid the foundation for our novel taxonomy of reasoning processes.We systematically analyzed existing methods within this framework, identified key limitations in current models-such as challenges in adaptability and multi-step reasoning-and proposed future research directions, which were further inspired by our framework and neuroscience models, offering new insights for advancing AI reasoning techniques.Additionally, we released an opensource repository organizing benchmark tasks, datasets, and research papers, which will be continuously updated to support future AI reasoning research.</p>
<p>Fig. 2 .
2
Fig.2.Google Scholar results for research topics related to agentic reasoning.The vertical axis represents the number of publications (in thousands), while the horizontal axis denotes the publication year.The figure highlights a significant rise in "LLM agentic reasoning" publications since 2023, reflecting the impact of large language models on the field.</p>
<p>Fig. 3 .
3
Fig. 3.The hybrid nature of reasoning in humans and AI agents.Reasoning is a fusion of prior knowledge and new information, forming a hybrid process.This section provides examples: 1) Human Reasoning, deciding what to wear based on past knowledge and weather forecasts, and 2) Agentic Reasoning, adjusting navigation in response to unexpected obstacles.</p>
<p>"Fig. 4 .
4
Fig.4.The overview of the reasoning process and classification of reasoning behavior from a neuro-perspective.This diagram presents a comprehensive framework of reasoning inspired by human cognitive and neural mechanisms.At the center, a hierarchical reasoning pipeline, spanning data sensory input, information processing, higher-order cognition, and conclusion generation, mirrors the flow of information in biological systems.Surrounding this core are five major categories of reasoning behaviors: perceptual reasoning, driven by multisensory integration; dimensional reasoning, encompassing spatial and temporal inference; relation reasoning, involving analogical thinking and relational matching; logical reasoning, covering inductive, deductive, and abductive logic; and interactive reasoning, focusing on agent-agent and agent-human collaboration within dynamic environments.Together, these components establish a neurocognitively grounded taxonomy that bridges biological inspiration and computational implementation in artificial intelligence systems.</p>
<p>Fig. 5 .
5
Fig.5.Taxonomy of Agentic Reasoning Techniques Inspired by Neuroscience.This hierarchical structure organizes reasoning methods in artificial agents based on cognitive mechanisms inspired by neuroscience, including dimensional, perceptual, logical, and interactive reasoning, highlighting the integration of biologically plausible mechanisms into artificial intelligence systems.This taxonomy highlights how agents can emulate human-like reasoning across diverse tasks and environments.</p>
<p>ing model for joint knowledge-image-question representation learning.LISA[62] fine-tunes the VLM model by introducing a new token ⟨SEG⟩ in the model vocabulary as a segmentation output marker, decoding its hidden layer embeddings into segmentation masks, enhancing the reasoning and segmentation capabilities of VLMs as shown in Fig.6(a).</p>
<p>Fig. 6 .
6
Fig. 6.Structure of different visual reasoning methods.(a) VLM-based approach [62] enhances its reasoning and segmentation capabilities.(b) LLMbased approach[67] that enhances the model's performance in handling complex visual tasks by dynamically focusing on key image regions and incorporating multi-turn reasoning to progressively derive detailed information for generating accurate and interpretable answers.(c) Symbolic-based approach[69] that generates executable Python-like visual programs based on language instructions to solve vision tasks.</p>
<p>Fig. 7 .
7
Fig. 7. Evolution timeline for LLM lingual reasoning methods.(a): Evolution timeline for CoT-based methods: CoT prompting was first introduced in 2022.Throughout 2023 and 2024, 2 main types of optimizations existed.One aimed to better structure and guide reasoning (ToT, LoT, GoT etc.), the other focused on prompt optimization and automation (Automate-CoT, Active-Prompt, promptless CoT).The method reached its maturity and consolidating phase with fewer novel frameworks and more refinements, benchmarking, and integration into broader systems.(b): Evolution timeline for RL-based methods: Relatively new compared to CoT, and starting to flourish following the success of DeepSeek R1.Early attempts in 2023 and 2024 focused mostly on reward modelling (DIVERSE, StepDPO, Rest-MCTS* .etc).Later attempts in 2024 and 2025 focused mainly on Reinforced Fine-tuning, rule-based RL, supervised fine-tuning, and various other methods.</p>
<p>Fig. 10 .
10
Fig. 10.Structure of different temporal reasoning methods.(a) and (b) are LLM-based approaches.(a) primarily leverages the intrinsic reasoning ability of LLMs, where common methods involve constructing task-specific datasets and fine-tuning LLMs.(b) maps time-series data and text into the same space, then utilizes LLMs for output generation.(c) is a graph-based approach, which typically constructs a temporal knowledge graph and applies traditional graph techniques for reasoning.</p>
<p>Fig. 11 .
11
Fig. 11.The main process of neuro-symbolic learning.Continuous multimodal signals are first processed by neural systems to extract structured and discrete representations, which serve as inputs to symbolic systems.These symbolic systems then perform logical reasoning to produce the final outputs.</p>
<p>Example 1 [ 2 [ 3 [
123
, , , ] Example , , , ] Example , , , ]</p>
<p>11 Reasoning 6 .
116
There are 53 maple trees currently in the park.After the workers finish planting new maple trees, there will be 64 in total.How many trees did they plant today?QuestionInformation: "There are 53 maple trees in the park.After planting, there will be 64."Based on this, reasoning process is " Calculate the number of trees planted: 64 -53 = 11."Double-check the reasoning process, let's analyze its correctness, andend with "yes" or "no".Verification Question-Related Premises1.53 maple trees currently in the park. 2. After planting trees, there will be 64 in total.3. The workers plant some maple trees.4. How many trees did they plant today?Reasoning 5. (by 2. &amp;1.) Step 1: Calculate the number of maple trees the workers will plant: 64 -53 = (by 4.&amp;5.)Final Step: The original question is 4. How many maple trees did theworkers plant today?We do not miss information on the rewritten labels.So the answer to this question is the workers planted 11 maple trees today.</p>
<p>Fig. 12 .
12
Fig. 12. (a), (b) and (c) are the processes in inductive reasoning, deductive reasoning, and abductive reasoning, respectively.We refer to the flowcharts from the recent methods HypoSearch[121], Natural Program[123] , and MAR[126].</p>
<p>use a deep ensemble for better accuracy.Might boost accuracy, but it increases computational cost … True, but single models fail to capture uncertainty in critical cases… Agreed.What if we use a smaller ensemble with … ?Performance Security Based on the terrain map, I'll take the east route.If I clear this rubble now, the others can access the blocked zone safely.I'll rescan this zone in 5 minutes, let's sync results.Focus on the collapsed structure, closest to where Drone1 is scanning</p>
<p>evaluates masked language models' time-sensitive knowledge, based on Wikidata's 2020 snapshot.It contains 50,310 queries focused on facts that changed after 2010, testing knowledge retention and reasoning over time.StreamingQA [226] examines LLMs' adaptability in dynamic environments with 146k questions based on 2007-2020 news data.It supports realistic time-based QA evaluations, posing challenges with news redundancy, noise, and contradictions.TempReason [227] has over 400k questions for time reasoning in closed-book, open-book, and reasoning QA.It introduces a framework combining time span extraction and reinforcement learning to enhance time reasoning abilities.MenatQA [228] includes 2,853 questions to evaluate LLMs' time reasoning using factors like scope, order, and counterfactuals.It shows that model performance varies with size, time bias, and provided time info.TRAM [229]</p>
<p>Fig. 15 .
15
Fig.15.An overview of our proposed AI agent system architecture designed to facilitate reasoning through multimodal perception and dynamic knowledge integration.Multimodal inputs are encoded into a unified representation via biologically inspired processing mechanisms.Dynamic Multimodal Mixtureof-Experts (DMMoE) selectively engages modality-specific and task-specific experts based on real-time salience and task relevance.Foundation models serve dual roles as high-fidelity understanding engines and flexible reasoning assistants.Knowledge is organized into a dual system: an interaction-driven offline knowledge base capturing embodied experiences, and a time-sensitive online retrieval mechanism accessing dynamic external information.This framework enables adaptive, robust, and temporally coherent reasoning across complex real-world scenarios.</p>
<p>Fig. 16 .
16
Fig. 16.Framework for continuous spatiotemporal neural differential reasoning in embodied agents.Inspired by the human parietal lobe, this architecture integrates dynamic spatiotemporal event spaces with 3D structural information and time-varying dynamics using Neural ODEs.The resulting high-resolution 4D embeddings support trajectory prediction, causal event inference, and long-horizon planning for embodied agents.</p>
<p>Fig. 17 .
17
Fig. 17.Future AI agents should possess the ability to reason about others from a first-person perspective-inferring hidden intentions, anticipating responses, and adapting strategies in real-time to maintain cooperation.</p>
<p>Prefrontal Cortex Motor Cortex Parietal Lob Temporal Lobe Occipital Lobe Auditory Cortex Visual Cortex Hippocampus Knowledge Biological Reasoning Agent Reasoning Foundation Models Knowledge Base Multimodal Inputs Logical Reasoning Dimensional Reasoning Perceptual Reasoning Interactive Reasoning</p>
<p>TABLE I MULTISTEP
I
REASONING MODELS IN NEUROSCIENCE.ABBREVIATIONS: PFC MEANS PREFRONTAL CORTEX, DLPFC MEANS DORSOLATERAL PREFRONTAL CORTEX, AND ACC MEANS ANTERIOR CINGULATE CORTEX.
ModelsKey InsightMultistep ProcessType of Reasoning in Neuroscience</p>
<p>TABLE III REPRESENTATIVE
III
WORKS IN PERCEPTION-BASED REASONING.
CategoryMethodPublicationBackboneHighlightsVISPROG [69]CVPR'2023Neuro-symbolicVisual ProgrammingVisualLisa [62] Cola [65]CVPR'2024 NeurIPS'2023VLM LLMReasoning Segmentation LLM Coordinates VLMsVisCoT [67]NeurIPS'2024LLMVisual Chain-of-ThoughtSPAG [84]NeurIPS'2024LLMSelf-playing Adversarial Language GameCoT Prompting [15]NeurIPS'2022LLMChain-of-Thought promptingLoT [80]COLING'2024LLMGrounding CoT Reasoning With LogicRBRLHF [83]arXiv'2025LLMRule-based RL With Human FeedbackSelf-Consistency [135]NeurIPS'2022LLMNew decoding strategy sampling diverse reasoning pathsToT [78]NeurIPS'2023LLMGeneralizes CoTLingualGoT [79]AAAI'2023LLMModelling LLM information as a graphAutomate-CoT [81]EMNLP'2023LLMAutomatically augmenting rational chainsActive-Prompt [82]ACL'2023LLMNew method for choosing task-specific CoT exemplarsFine-Tune-CoT [136]ACL'2023LLMLarge teacher models fine-tune smaller modelsAoT [137]EMNLP'2024LLMPrompting abstract-to-concrete thinkingCoC [138]ICML'2024LLMCombining code-writing with LM simulationICoT [139]CVPR'2025VLMImage-incorported multimodal Chain-of-ThoughtAuditoryLTU [92] CF-CLAP [90]ICLR'2024 ICASSP'2024AST, LLM CLAPModel Integration and Multi-modal Reasoning Counter Factual LearningTactileReAct [96]IROS'2024VLMReasoning and Perception of Liquid Objects</p>
<p>Vision Encoder Large Language Model Vision Encoder Visual Sampler What activity is the puppy engaging in ? [142, 118, 320, 252]</p>
<p>Introducing RL into VLM-based visual reasoning aims to improve the decision-making capability, controllability, and generalization ability of the model.Traditional VLMs mainly rely on supervised learning (SL) for training, but SL is often constrained by static data distributions, making it difficult to adapt to complex reasoning tasks in open environments.By integrating RL, the model can be optimized using reward mechanisms, allowing it to adjust strategies during multistep reasoning processes, thus improving the accuracy and coherence of answers.Additionally, RL helps the model better balance different reasoning paths, avoiding stereotypical errors in reasoning and increasing adaptability to long-tail questions.Combining RL with VLMs [72]-
typically optimized for specific tasks, struggling to adapt toopen-ended, combinatory real-world demands. As shown inFig. 6(c), VISPROG [69] leverages the in-context learningcapabilities of large language models (LLMs) to automaticallygenerate executable Python-like visual programs based onnatural language instructions. It breaks down complex tasksand invokes existing computer vision (CV) models or Pythonlogic operations to complete the tasks. ViperGPT [71] executesvisual reasoning tasks by generating Python code. Whenreceiving a visual query, ViperGPT uses a large languagemodel to generate an executable Python program that callsmultiple visual modules (e.g., object detection, depth estima-tion, etc.) and performs logical reasoning and mathematicalcomputations. Exo ViP [70] builds upon VISPROG [69] byincorporating an "Exoskeleton" validation module to detectand correct errors during the reasoning process. It also employstree search to select the optimal reasoning path, preventingerror propagation, thereby improving the accuracy and robust-ness of compositional visual reasoning.
annotated data for training, limiting scalability; end-to-end models lack transparency, making it difficult to analyze error sources; and existing models for VQA and CV tasks are</p>
<p>TABLE IV REPRESENTATIVE
IV
WORKS IN DIMENSION-BASED REASONING.
CategoryMethodPublicationBackboneHighlightsSpatialVLM [97]CVPR'2024VLMDirect Spatial QueriesLocVLM [98]CVPR'2024VLMEncoding Image Coordinates within LanguageSpatialSpatialRGPT [157]NeurIPS'2024VLMRegion Representation ModuleSpatialPIN [99]NeurIPS'20243D priorsSpatial grounding for VLMsTextVQA [106]TIP'2023Weak supervisionText-based visual QA reasoningMTAM [109]EMNLP'2023LLMEEG-Language AlignmentPromptCast [110]Temporal</p>
<p>TABLE V REPRESENTATIVE
V
WORKS IN LOGIC-BASED REASONING.
CategoryMethodPublicationBackboneHighlightsInductiveHypoSearch [121] IHR [122]ICLR'2024 ICLR'2024Python Program LLM&amp; SymbolicsMulti-level Hypothesis Generation Iterative Hypothesis RefinementDeductiveNatural Program [123] LogicGuide [124]NeurIPS'2023 TMLR'2024CoT LLMStep-by-step Self-Verification State-Driven Incremental Constraint GuidanceAbductiveMAR [126] VAR [125]ACL'2023 CVPR'2022Graph&amp;Symbolics TransformerSymbolic Progressive Action Chain Inference Causal Cascaded Reasoningminor input perturbations.</p>
<p>TABLE VI REPRESENTATIVE
VI
WORKS IN INTERACTION-BASED REASONING.</p>
<p>• Error Detection &amp; Adaptation: Inspired by PFC's biasadjustment mechanism, AI systems can incorporate selfmonitoring modules to periodically assess reasoning accuracy.If an inconsistency arises, the model can trigger a self-correction strategy, such as self-reflection in LLMs to regenerate more goal-aligned responses [262]-[264].Banich's Cascade of Control Model.</p>
<p>NameYear Task Type Contents VQA v1.0[195]2015 Open-ended VQA Perception (Visual) 10M answers VQA v2.0[196]2017 Open-ended VQA Perception (Visual) 250,000 questions CLEVR[197]2017 Compositional Visual Reasoning Perception (Visual) 864,968 questions GQA[198]2019 Real-World Visual Reasoning Perception (Visual) 22M questions NLVR2[199]2019 Visual Reasoning Perception (Visual) 107,292 image-question pairs OK-VQA[200]2019 Knowledge-based VQA Perception (Visual) 14,055 image-question pairs A-OKVQA[201]2022 Knowledge-based VQA Perception (Visual) 24,903 questions Super-CLEVR[197]2023 Visual Reasoning Perception (Visual) 30k images MR-Ben[202]2024 Mathematical Reasoning Evaluation Perception (Lingual) 6k questions RM-Bench[203]2025 Reward Model Evaluation Perception (Lingual) N/A LR 2 Bench Bench[204]2025 Reflective Reasoning Evaluation Perception (Lingual) 850 samples Big-Math[205]2025 Mathematical Problem Solving Perception (Lingual) 250k questions LongReason[206]2025 Long-Chain Reasoning Perception (Lingual) 794 questions Big-Bench Extra Hard[207]2025 Complex Reasoning Perception (Lingual) 1000+ tasks ResearchBench[208]2025 Scientific Reasoning Perception(Lingual) 3000+ tasks MastermindEval[209]2025 Deductive Reasoning Perception (Lingual) 1500+ tasks Z1[210]2025
The Routledge dictionary of philosophy. Michael Proudfoot, Lacey , Alan Robert, 2009Routledge</p>
<p>Artificial intelligence: a modern approach. Stuart J Russell, Peter Norvig, 2016Pearson</p>
<p>Artificial intelligence: Structures and strategies for complex problem solving. Elham S Khorasani, Scalable Computing: Practice and Experience. 20089</p>
<p>Computational Intelligence: a logical approach. David Poole, Alan Mackworth, Randy Goebel, 1998</p>
<p>Artificial intelligence: a new synthesis. Nils J Nilsson, 1998Elsevier</p>
<p>Cortico-cortical feedback engages active dendrites in visual cortex. Fis ¸ek, Mehmet, Dustin Herrmann, Alexander Egea-Weiss, Matilda Cloves, Lisa Bauer, Tai-Ying Lee, Russell , Lloyd E Häusser, Michael , Nature. 61779622023</p>
<p>Relating structure to function: Heschl's gyrus and acoustic processing. Catherine Warrier, Patrick Wong, Virginia Penhune, Robert Zatorre, Todd Parrish, Daniel Abrams, Nina Kraus, Journal of Neuroscience. 2912009</p>
<p>Feeling our way to machine minds: People's emotions when perceiving mind in artificial intelligence. Daniel B Shank, Christopher Graves, Alexander Gott, Patrick Gamez, Sophia Rodriguez, Computers in Human Behavior. 982019</p>
<p>Prefrontal cortex. Joaquin M Fuster, Comparative neuroscience and neurobiology. Springer2008</p>
<p>Predictive coding in the visual cortex: a functional interpretation of some extra-classical receptive-field effects. Rajesh Rao, Pn, Dana H Ballard, Nature Neuroscience. 211999</p>
<p>Motor cortex-to act or not to act?. Christian Ebbesen, Laut, Michael Brecht, Nature Reviews Neuroscience. 18112017</p>
<p>The working memory costs of a central attentional bottleneck in multitasking. Pauldy Otermans, Cj, Andrew Parton, Andre J Szameitat, Psychological Research. 8662022</p>
<p>A unified attentional bottleneck in the human brain. Michael N Tombu, Asplund , Christopher L Dux, Paul E Godwin, Douglass , Martin , Justin W Marois, René , Proceedings of the National Academy of Sciences. 108334312011</p>
<p>The atomic components of thought. John R Anderson, Christian J Lebiere, 2014Psychology Press</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Chi , Ed , Le , Quoc V Zhou, Denny Others, Advances in Neural Information Processing Systems. 202235</p>
<p>The distinct modes of vision offered by feedforward and recurrent processing. Victor Lamme, Af, Pieter R Roelfsema, Trends in Neurosciences. 23112000</p>
<p>Feedforward and feedback interactions between visual cortical areas use different population activity patterns. João D Semedo, Anna I Jasper, Amin Zandvakili, Krishna , Aravind Aschner, Machens Amir, Christian K Kohn, Adam , Yu , Byron M , Nature Communications. 13110992022</p>
<p>Updating mental models in predictive reasoning. Rodrigo , María J Vega, Manuel De, Javier Castaneda, European Journal of Cognitive Psychology. 421992</p>
<p>Thinking as Analogy-Making: Toward a Neural Process Account of General Intelligence. Keith J Holyoak, The Journal of Neuroscience. 4518e15552420252025</p>
<p>A survey of reasoning with foundation models. Jiankai Sun, Chuanyang Zheng, Enze Xie, Zhengying Liu, Ruihang Chu, Jianing Qiu, Jiaqi Xu, Mingyu Ding, Hongyang Li, Mengzhe Geng, Others, arXiv:2312.115622023arXiv preprint</p>
<p>Stop overthinking: A survey on efficient reasoning for large language models. Yang Sui, Chuang , Yu-Neng And Wang, Guanchu Zhang, Jiamu Zhang, Tianyi Yuan, Jiayi Liu, Hongyi , Wen , Andrew , Chen , Hanjie Hu, Xia Others, arXiv:2503.164192025arXiv preprint</p>
<p>From System 1 to System 2: A Survey of Reasoning Large Language Models. Zhong-Zhi And Li, Duzhen Zhang, Ming-Liang And Zhang, Jiaxin Zhang, Zengyan Liu, Yuxuan Yao, Haotian Xu, Junhao Zheng, Pei-Jie And Wang, Xiuyi Chen, Others, arXiv:2502.174192025arXiv preprint</p>
<p>Multimodal chain-of-thought reasoning: A comprehensive survey. Yaoting Wang, Shengqiong Wu, Yuecheng Zhang, William Wang, Ziwei Liu, Jiebo Luo, Fei, Hao, arXiv:2503.126052025arXiv preprint</p>
<p>Why Reasoning Matters? A Survey of Advancements in Multimodal Reasoning (v1). Jing Bi, Susan Liang, Xiaofei Zhou, Pinxin Liu, Junjia Guo, Yunlong Tang, Luchuan Song, Chao Huang, Guangyu Sun, Jinxi He, Others, arXiv:2504.031512025arXiv preprint</p>
<p>A case-based reasoner adaptive to different cognitive tasks. Isabelle Bichindaritz, International Conference on Case-Based Reasoning. Springer1995</p>
<p>Ontologyoriented case-based reasoning (CBR) approach for trainings adaptive delivery. Dounia Mansouri, Aboubekeur Hamdi-Cherif, Proceedings of the WSEAS International Conference on Computers. the WSEAS International Conference on Computers2011</p>
<p>Reasoning: The neuroscience of how we think. Daniel Krawczyk, 2017Academic Press</p>
<p>An integrative theory of prefrontal cortex function. Earl K Miller, Jonathan D Cohen, Annual Review of Neuroscience. 2412001</p>
<p>Executive function: The search for an integrated account. Marie T Banich, Current directions in psychological science. 1822009</p>
<p>Working memory. Alan Baddeley, Memory. 2020</p>
<p>The episodic buffer: a new component of working memory?. Trends in cognitive sciences. 4112000</p>
<p>SOAR: An architecture for general intelligence. John E Laird, Allen Newell, Paul S Rosenbloom, Artificial Intelligence. 3311987</p>
<p>A cognitive theory of consciousness. Bernard J Baars, 1993Cambridge University Press</p>
<p>A neostriatal habit learning system in humans. Barbara J Knowlton, Jennifer A Mangels, Larry R Squire, Science. 27352801996</p>
<p>Implicit memory and transformative learning theory: Unconscious cognition. Edward W Taylor, Annual Adult Education Research Conference Proceedings. 1997262Oklahoma State University, Occupational and Adult Education</p>
<p>The case for implicit category learning. Edward E Smith, Cognitive, Affective, &amp; Behavioral Neuroscience. 812008</p>
<p>Learning strategy differentially impacts memory connections in children and adults. Zahra Abolghasem, Tiffany Teng, H-T Nexha, Elida Zhu, Cherrie , Jean , Cindy S Castrillon, Mariana , Che , Eric , Di Nallo, Eva V Schlichting, Margaret L , Developmental Science. 264e133712023</p>
<p>Handbook of research on instructional systems and educational technology. Terry Kidd, Morris Jr, Lonnie R , 2017IGI Global</p>
<p>The Impact of New Information. Kjell J Sunnevåg, 2009Palgrave MacmillanLondon, UK</p>
<p>Learning Through New Information-A Changing Structure Oriented Approach. A M Sandi, 1978SpringerBerlin Heidelberg, Germany</p>
<p>Nonmonotonic Reasoning," in Handbook of Knowledge Representation, ser. Foundations of Artificial Intelligence, Frank van Harmelen and Vladimir Lifschitz and Bruce Porter. Gerhard Brewka, Ilkka Niemelä, Mirosław Truszczyński, 2008Elsevier3</p>
<p>The Bayesian brain: the role of uncertainty in neural coding and computation. David C Knill, Alexandre Pouget, Trends in Neurosciences. 27122004</p>
<p>Bayes in the brain-on Bayesian modelling in neuroscience. Matteo Colombo, Peggy Seriès, The British journal for the philosophy of science. 2012</p>
<p>Predictive coding. Yanping Huang, Rajesh Rao, Pn, Wiley Interdisciplinary Reviews: Cognitive Science. 252011</p>
<p>With or without you: predictive coding and Bayesian inference in the brain. Laurence Aitchison, Máté Lengyel, Current opinion in neurobiology. 462017</p>
<p>The free-energy principle: a unified brain theory?. Karl Friston, Nature Reviews Neuroscience. 1122010</p>
<p>A free energy principle for the brain. Karl Friston, James Kilner, Lee Harrison, Journal of Physiology. 1001-32006</p>
<p>Parietal lobe: from action organization to intention understanding. Leonardo Fogassi, Pier Ferrari, Francesco, Benno Gesierich, Stefano Rozzi, Fabian Chersi, Giacomo Rizzolatti, Science. 30857222005</p>
<p>The hippocampus. James J Knierim, Current Biology. 25232015</p>
<p>Structure and function of the cerebral cortex. Stewart Shipp, Current Biology. 17122007</p>
<p>Dual-processing accounts of reasoning, judgment, and social cognition. Jonathan Evans, B T St, Annual Review of Psychology. 5912008</p>
<p>Building machines that learn and think like people. Brenden M Lake, Tomer D Ullman, Joshua B Tenenbaum, Gershman, J Samuel, Behavioral and brain sciences. 40e2532017</p>
<p>Bias in the brain: a diffusion model analysis of prior probability and potential payoff. Martijn J Mulder, Eric-Jan And Wagenmakers, Roger Ratcliff, Boekel, Wouter And Forstmann, U Birte, Journal of Neuroscience. 3272012</p>
<p>Brain networks of perceptual decision-making: an fMRI ALE meta-analysis. Max C Keuken, Christa Müller-Axt, Robert Langner, Eickhoff , Simon B Forstmann, Birte U Neumann, Jane , Frontiers in human neuroscience. 84452014</p>
<p>Spatial and temporal reasoning. Oliviero Stock, 1998Springer Science &amp; Business Media</p>
<p>On the dimensionality of Reasoning. Klaus D Kubinger, Psychological Test and Assessment Modeling. 6532023</p>
<p>Geometry and spatial reasoning. Douglas H Clements, Battista, T Michael, Handbook of research on mathematics teaching and learning: A project of the National Council of Teachers of Mathematics. 1992</p>
<p>Mental models and temporal reasoning. Walter Schaeken, Johnson-Laird Ydewalle, Gery , Cognition. 6031996</p>
<p>Logical reasoning with diagrams. Gerard Allwein, Jon Barwise, 1996Oxford University Press</p>
<p>Logical reasoning in formal and everyday reasoning tasks. Hugo Bronkhorst, Gerrit Roorda, Cor Suhre, Martin Goedhart, International Journal of Science and Mathematics Education. 182020</p>
<p>GeReA: Question-Aware Prompt Captions for Knowledge-based Visual Question Answering. Ziyu Ma, Shutao Li, Bin Sun, Jianfei Cai, Zuxiang Long, Fuyan Ma, arXiv:2402.025032024arXiv preprint</p>
<p>LISA: Reasoning Segmentation via Large Language Model. Xin Lai, Zhuotao Tian, Yukang Chen, Yanwei Li, Yuhui Yuan, Shu Liu, Jiaya Jia, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2024</p>
<p>VLM-R1: A Stable and Generalizable R1-style Large Vision-Language Model. Haozhan Shen, Zilun Zhang, Qianqian Zhang, Ruochen Xu, Tiancheng Zhao, 2025</p>
<p>KN-VLM: KNowledge-guided Vision-and-Language Model for visual abductive reasoning. Kuo Tan, Zhaobo Qi, Jianping Zhong, Yuanrong Xu, Weigang Zhang, Multimedia Systems. 3121462025</p>
<p>Large language models are visual reasoning coordinators. Liangyu Chen, Bo Li, Sheng Shen, Jingkang Yang, Chunyuan Li, Kurt Keutzer, Darrell , Trevor Liu, Ziwei , Advances in Neural Information Processing Systems. 202336140</p>
<p>Visual chain-of-thought prompting for knowledge-based visual reasoning. Zhenfang Chen, Qinhong Zhou, Yikang Shen, Yining Hong, Zhiqing Sun, Dan Gutfreund, Chuang Gan, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202438</p>
<p>Visual CoT: Advancing Multi-Modal Language Models with a Comprehensive Dataset and Benchmark for Chain-of-Thought Reasoning. Hao Shao, Qian, Xiao Shengju, Han Song, Guanglu Zong, Zhuofan Wang, Letian Liu, Yu Li, Hongsheng , Advances in Neural Information Processing Systems. 202437</p>
<p>Enhancing LLM Reasoning via Vision-Augmented Prompting. Ziyang Xiao, Dongxiang Zhang, Xiongwei Han, Xiaojin Fu, Yu , Wing Yin, Tao Zhong, Sai Wu, Yuan Wang, Jianwei Yin, Gang Chen, Advances in Neural Information Processing Systems. 202437</p>
<p>Visual programming: Compositional visual reasoning without training. Tanmay Gupta, Aniruddha Kembhavi, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition202314962</p>
<p>ExoViP: Step-by-step Verification and Exploration with Exoskeleton Modules for Compositional Visual Reasoning. Yuxuan Wang, Alan Yuille, Zhuowan Li, Zilong Zheng, arXiv:2408.022102024arXiv preprint</p>
<p>ViperGPT: Visual Inference via Python Execution for Reasoning. Dídac Surís, Sachit Menon, Carl Vondrick, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer Vision202311898</p>
<p>HYDRA: A Hyper Agent for Dynamic Compositional Visual Reasoning. Fucai Ke, Zhixi Cai, Simindokht Jahangard, Weiqing Wang, Pari Haghighi, Delir, Hamid Rezatofighi, European Conference on Computer Vision. Springer2024</p>
<p>Vision-R1: Incentivizing Reasoning Capability in Multimodal Large Language Models. Wenxuan Huang, Bohan Jia, Zijie Zhai, Shaosheng Cao, Zheyu Ye, Fei Zhao, Yao Hu, Shaohui Lin, arXiv:2503.067492025arXiv preprint</p>
<p>Ziyu Liu, Zeyi Sun, Yuhang Zang, Xiaoyi Dong, Yuhang Cao, Haodong Duan, Dahua Lin, Jiaqi Wang, arXiv:2503.01785Visual-RFT: Visual Reinforcement Fine-Tuning. 2025arXiv preprint</p>
<p>MedVLM-R1: Incentivizing Medical Reasoning Capability of Vision-Language Models (VLMs) via Reinforcement Learning. Jiazhen Pan, Che Liu, Junde Wu, Fenglin Liu, Jiayuan Zhu, Li , Hongwei Bran, Chen Chen, Cheng Ouyang, Daniel Rueckert, arXiv:2502.196342025arXiv preprint</p>
<p>Chain of draft: Thinking faster by writing less. Silei Xu, Wenhao Xie, Lingxiao Zhao, Pengcheng He, arXiv:2502.186002025arXiv preprint</p>
<p>SmartAgent: Chain-of-User-Thought for Embodied Personalized Agent in Cyber World. Jiaqi Zhang, Chen Gao, Liyuan Zhang, Yong Li, Hongzhi Yin, arXiv:2412.074722024arXiv preprint</p>
<p>Tree of thoughts: Deliberate problem solving with large language models. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, Karthik Narasimhan, Advances in Neural Information Processing Systems. 202336</p>
<p>Graph of thoughts: Solving elaborate problems with large language models. Maciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Michal Podstawski, Lukas Gianinazzi, Joanna Gajda, Tomasz Lehmann, Hubert Niewiadomski, Piotr Nyczyk, Others, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202438690</p>
<p>Enhancing zero-shot chain-of-thought reasoning in large language models through logic. Xufeng Zhao, Mengdi Li, Wenhao Lu, Cornelius Weber, Lee , Jae Hee, Kun Chu, Stefan Wermter, arXiv:2309.133392023arXiv preprint</p>
<p>Automatic prompt augmentation and selection with chain-of-thought from labeled data. Kashun Shum, Shizhe Diao, Tong Zhang, arXiv:2302.128222023arXiv preprint</p>
<p>Active prompting with chain-of-thought for large language models. Shizhe Diao, Pengcheng Wang, Yong Lin, Rui Pan, Xiang Liu, Tong Zhang, arXiv:2302.122462023arXiv preprint</p>
<p>DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning. Daya Guo, Yang , Dejian Zhang, Haowei Song, Junxiao Zhang, Ruoyu Xu, Runxin Zhu, Qihao Ma, Shirong Wang, Peiyi Bi, Xiao Others, arXiv:2501.129482025arXiv preprint</p>
<p>Self-playing adversarial language game enhances LLM reasoning. Pengyu Cheng, Tianhao Hu, Han Xu, Zhisong Zhang, Yong Dai, Lei Han, Xiaolong Li, Others, Advances in Neural Information Processing Systems. 202437543</p>
<p>Making large language models better reasoners with step-aware verifier. Yifei Li, Zeqi Lin, Shizhuo Zhang, Qiang Fu, Bei Chen, Jian-Guang And Lou, Weizhu Chen, arXiv:2206.023362022arXiv preprint</p>
<p>ReST-MCTS*: LLM Self-Training via Process Reward Guided Tree Search. Dan Zhang, Sining Zhoubian, Ziniu Hu, Yisong Yue, Yuxiao Dong, Jie Tang, Advances in Neural Information Processing Systems. 202437</p>
<p>Peiyi Wang, Lei Li, Zhihong Shao, Xu, Damai Dai, Yifei Li, Deli Chen, Yu Wu, Zhifang Sui, arXiv:2312.08935Math-Shepherd: Verify and Reinforce LLMs Step-by-step without Human Annotations. 2023arXiv preprint</p>
<p>Improve mathematical reasoning in language models by automated process supervision. Liangchen Luo, Yinxiao Liu, Rosanne Liu, Samrat Phatale, Harsh Lara, Yunxuan Li, Lei Shu, Yun Zhu, Lei Meng, Jiao Sun, Others, arXiv:2406.0659220242arXiv preprint</p>
<p>Step-DPO: Step-wise Preference Optimization for Long-chain Reasoning of LLMs. Xin Lai, Zhuotao Tian, Yukang Chen, Senqiao Yang, Xiangru Peng, Jiaya Jia, arXiv:2406.186292024arXiv preprint</p>
<p>Learning Audio Concepts from Counterfactual Natural Language. Ali Vosoughi, Luca Bondi, Ho-Hsiang And Wu, Chenliang Xu, Proceedings of IEEE International Conference on Acoustics, Speech and Signal Processing. IEEE International Conference on Acoustics, Speech and Signal ProcessingIEEE2024</p>
<p>BAT: Learning to Reason about Spatial Sounds with Large Language Models. Zhisheng Zheng, Puyuan Peng, Ziyang Ma, Chen , Xie Choi, Eunsol Harwath, David , arXiv:2402.015912024arXiv preprint</p>
<p>Listen, think, and understand. Yuan Gong, Hongyin Luo, Alexander H Liu, Leonid Karlinsky, James Glass, arXiv:2305.107902023arXiv preprint</p>
<p>Octopi: Object Property Reasoning with Large Tactile-Language Models. Samson Yu, Lin , Kelvin , Xiao , Anxing Duan, Jiafei Soh, Harold , arXiv:2405.027942024arXiv preprint</p>
<p>TALON: Improving Large Language Model Cognition with Tactility-Vision Fusion. Xinyi Jiang, Guoming Wang, Huanhuan Li, Qinghua Xia, Rongxing Lu, Siliang Tang, IEEE Conference on Industrial Electronics and Applications. IEEE2024</p>
<p>Beyond sight: Finetuning generalist robot policies with heterogeneous sensors via language grounding. Joshua Jones, Mees, Oier, Carmelo Sferrazza, Kyle Stachowicz, Pieter Abbeel, Sergey Levine, arXiv:2501.046932025arXiv preprint</p>
<p>Vision-language model-based physical reasoning for robot liquid perception. Wenqiang Lai, Tianwei Zhang, Tin Lam, Lun, Yuan Gao, IEEE/RSJ International Conference on Intelligent Robots and Systems. IEEE. 2024</p>
<p>SpatialVLM: Endowing Vision-Language Models with Spatial Reasoning Capabilities. Boyuan Chen, Zhuo Xu, Sean Kirmani, Brain Ichter, Dorsa Sadigh, Leonidas Guibas, Fei Xia, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition202414465</p>
<p>Learning to Localize Objects Improves Spatial Reasoning in Visual-LLMs. Kanchana Ranasinghe, Satya Shukla, Narayan, Omid Poursaeed, Ryoo , Michael S Lin, Tsung- Yu, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition202412987</p>
<p>SpatialPIN: Enhancing Spatial Reasoning Capabilities of Vision-Language Models through Prompting and Interacting 3D Priors. Chenyang Ma, Kai Lu, Ta-Ying And Cheng, Niki Trigoni, Andrew Markham, arXiv:2403.134382024arXiv preprint</p>
<p>StarCraftImage: A Dataset For Prototyping Spatial Reasoning Methods For Multi-Agent Environments. Sean Kulinski, Nicholas R Waytowich, James Z Hare, David I Inouye, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition20232213</p>
<p>GRASP: A Grid-Based Benchmark for Evaluating Commonsense Spatial Reasoning. Zhisheng Tang, Mayank Kejriwal, arXiv:2407.018922024arXiv preprint</p>
<p>Reasoning paths with reference objects elicit quantitative spatial reasoning in large vision-language models. Yuan-Hong And Liao, Rafid Mahmood, Sanja Fidler, David Acuna, arXiv:2409.097882024arXiv preprint</p>
<p>Structured Spatial Reasoning with Open Vocabulary Object Detectors. Negar Nejatishahidin, Madhukar Vongala, Reddy, Jana Kosecka, arXiv:2410.073942024arXiv preprint</p>
<p>I Know About "Up"! Enhancing Spatial Reasoning in Visual Language Models Through 3D Reconstruction. Zaiqiao Meng, Zhou, Hao, Yifang Chen, arXiv:2407.141332024arXiv preprint</p>
<p>TopV-Nav: Unlocking the Top-View Spatial Reasoning Potential of MLLM for Zero-shot Object Navigation. Linqing Zhong, Chen Gao, Zihan Ding, Yue Liao, Si Liu, arXiv:2411.164252024arXiv preprint</p>
<p>Weakly-Supervised 3D Spatial Reasoning for Text-Based Visual Question Answering. Hao Li, Jinfa Huang, Jin , Peng Song, Guoli Wu, Qi Chen, Jie , IEEE Transactions on Image Processing. 322023</p>
<p>SpatialCoT: Advancing Spatial Reasoning through Coordinate Alignment and Chain-of-Thought for Embodied Task Planning. Yuecheng Liu, Chi , Dafeng Wu, Shiguang Zhang, Zhanguang Hu, Yaochen Zhang, Lingfeng Zhang, Yingxue Wu, Shuang Cao, Tongtong Huang, Guowei Others, arXiv:2501.100742025arXiv preprint</p>
<p>End-to-End Navigation with Vision Language Models: Transforming Spatial Reasoning into Question-Answering. Dylan Goetting, Himanshu Singh, Gaurav, Antonio Loquercio, arXiv:2411.057552024arXiv preprint</p>
<p>Can brain signals reveal inner alignment with human languages?. Jielin Qiu, William Han, Jiacheng Zhu, Mengdi Xu, Douglas Weber, Bo Li, Ding Zhao, 2023Association for Computational Linguisticsin Findings of the</p>
<p>PromptCast: A New Prompt-based Learning Paradigm for Time Series Forecasting. Hao Xue, Flora D Salim, IEEE Transactions on Knowledge and Data Engineering. 36112023</p>
<p>Large language models can learn temporal reasoning. Siheng Xiong, Ali Payani, Ramana Kompella, Faramarz Fekri, arXiv:2401.068532024arXiv preprint</p>
<p>Enhancing temporal sensitivity and reasoning for time-sensitive question answering. Wanqi Yang, Yanda Li, Meng Fang, Ling Chen, arXiv:2409.169092024arXiv preprint</p>
<p>TempoGPT: Enhancing Temporal Reasoning via Quantizing Embedding. Haochuan Zhang, Chunhua Yang, Jie Han, Liyang Qin, Xiaoli Wang, arXiv:2501.073352025arXiv preprint</p>
<p>Text-to-ECG: 12-Lead Electrocardiogram Synthesis conditioned on Clinical Text Reports. Hyunseung Chung, Jiho Kim, Joon-Myoung And Kwon, Ki-Hyun Jeon, Lee , Min Sung, Edward Choi, Proceedings of IEEE International Conference on Acoustics, Speech and Signal Processing. IEEE International Conference on Acoustics, Speech and Signal ProcessingIEEE2023</p>
<p>Know-evolve: Deep temporal reasoning for dynamic knowledge graphs. Rakshit Trivedi, Hanjun Dai, Yichen Wang, Le Song, International Conference on Machine Learning. 2017</p>
<p>Temporal inductive path neural network for temporal knowledge graph reasoning. Hao Dong, Wang , Pengyang , Xiao , Meng Ning, Zhiyuan Wang, Pengfei Zhou, Yuanchun , Artificial Intelligence. 3291040852024</p>
<p>An improving reasoning network for complex question answering over temporal knowledge graphs. Songlin Jiao, Zhenfang Zhu, Wenqing Wu, Zicheng Zuo, Jiangtao Qi, Wenling Wang, Guangyuan Zhang, Peiyu Liu, Applied Intelligence. 5372023</p>
<p>Event Graph Guided Compositional Spatial-Temporal Reasoning for Video Question Answering. Ziyi Bai, Ruiping Wang, Difei Gao, Xilin Chen, IEEE Transactions on Image Processing. 332024</p>
<p>Temporal knowledge graph reasoning with historical contrastive learning. Yi Xu, Junjie Ou, Hui Xu, Luoyi Fu, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202337</p>
<p>THCN: A Hawkes Process Based Temporal Causal Convolutional Network for Extrapolation Reasoning in Temporal Knowledge Graphs. Tingxuan Chen, Jun Long, Zidong Wang, Shuai Luo, Jincai Huang, Liu Yang, IEEE Transactions on Knowledge and Data Engineering. 2024</p>
<p>Ruocheng Wang, Eric Zelikman, Poesia, Gabriel, Yewen Pu, Nick Haber, Noah D Goodman, arXiv:2309.05660Hypothesis search: Inductive reasoning with language models. 2023arXiv preprint</p>
<p>Phenomenal yet puzzling: Testing inductive reasoning capabilities of language models with hypothesis refinement. Linlu Qiu, Liwei Jiang, Ximing Lu, Melanie Sclar, Valentina Pyatkin, Chandra Bhagavatula, Wang , Bailin Kim, Yoon Choi, Yejin Dziri, Nouha Others, arXiv:2310.085592023arXiv preprint</p>
<p>Deductive verification of chain-of-thought reasoning. Zhan Ling, Yunhao Fang, Xuanlin Li, Zhiao Huang, Mingu Lee, Roland Memisevic, Hao Su, Advances in Neural Information Processing Systems. 202336433</p>
<p>Certified deductive reasoning with language models. Gabriel Poesia, Kanishk Gandhi, Eric Zelikman, Noah D Goodman, arXiv:2306.040312023arXiv preprint</p>
<p>Visual abductive reasoning. Chen Liang, Wang , Wenguan Zhou, Tianfei Yang, Yi , Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition202215575</p>
<p>Multi-modal action chain abductive reasoning. Mengze Li, Tianbao Wang, Jiahe Xu, Kairong Han, Shengyu Zhang, Zhou Zhao, Jiaxu Miao, Wenqiao Zhang, Shiliang Pu, Fei Wu, Proceedings of the Annual Meeting of the Association for Computational Linguistics. the Annual Meeting of the Association for Computational Linguistics2023</p>
<p>DERA: enhancing large language model completions with dialog-enabled resolving agents. Varun Nair, Elliot Schumacher, Geoffrey Tso, Anitha Kannan, arXiv:2303.170712023arXiv preprint</p>
<p>RoCo: Dialectic Multi-Robot Collaboration with Large Language Models. Zhao Mandi, Shreeya Jain, Shuran Song, IEEE International Conference on Robotics and Automation. IEEE2024</p>
<p>Chateval: Towards better LLM-based evaluators through multi-agent debate. Chi-Min And Chan, Weize Chen, Yusheng Su, Jianxuan Yu, Wei Xue, Shanghang Zhang, Jie Fu, Zhiyuan Liu, arXiv:2308.072012023arXiv preprint</p>
<p>Encouraging divergent thinking in large language models through multi-agent debate. Tian Liang, He, Zhiwei, Wenxiang Jiao, Xing Wang, Yan Wang, Wang , Rui Yang, Yujiu Shi, Shuming Tu, Zhaopeng , arXiv:2305.191182023arXiv preprint</p>
<p>A virtual conversational agent for teens with autism spectrum disorder: Experimental results and design lessons. Mohammad Ali, Rafayet, Seyedeh Razavi, Zahra, Raina Langevin, Al Mamun, Abdullah , Kane , Benjamin Rawassizadeh, Reza Schubert, Lenhart K Hoque, Ehsan , Proceedings of the ACM International Conference on Intelligent Virtual Agents. the ACM International Conference on Intelligent Virtual Agents2020</p>
<p>Timo Schick, Jane Dwivedi-Yu, Zhengbao Jiang, Fabio Petroni, Patrick Lewis, Izacard, Gautier, You, Qingfei, Christoforos Nalmpantis, Edouard Grave, Sebastian Riedel, arXiv:2208.11663PEER: A Collaborative Language Model. 2022arXiv preprint</p>
<p>SAPIEN: affective virtual agents powered by large language models. Masum Hasan, Cengiz Ozel, Sammy Potter, Ehsan Hoque, International Conference on Affective Computing and Intelligent Interaction Workshops and Demos. IEEE2023</p>
<p>Human-level play in the game of Diplomacy by combining language models with strategic reasoning. Bakhtin , Anton Brown, Noam Dinan, Emily Farina, Gabriele Flaherty, Colin Fried, Daniel Goff, Andrew Gray, Jonathan Hu, Hengyuan Others, Meta Fundamental AI Research Diplomacy Team (FAIR). 2022378</p>
<p>Self-consistency improves chain of thought reasoning in language models. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Chi , Ed Narang, Sharan Chowdhery, Aakanksha Zhou, Denny , arXiv:2203.111712022arXiv preprint</p>
<p>Large language models are reasoning teachers. Namgyu Ho, Laura Schmid, Se- Yun, Young, arXiv:2212.100712022arXiv preprint</p>
<p>Abstraction-of-Thought Makes Language Models Better Reasoners. Ruixin Hong, Hongming Zhang, Xiaoman Pan, Dong Yu, Changshui Zhang, arXiv:2406.124422024arXiv preprint</p>
<p>Chain of code: Reasoning with a language model-augmented code emulator. Chengshu Li, Jacky Liang, Andy Zeng, Xinyun Chen, Karol Hausman, Dorsa Sadigh, Sergey Levine, Li Fei-Fei, Fei Xia, Brian Ichter, arXiv:2312.044742023arXiv preprint</p>
<p>Interleavedmodal chain-of-thought. Jun Gao, Yongqi Li, Ziqiang Cao, Wenjie Li, arXiv:2411.194882024arXiv preprint</p>
<p>Q&amp;A Prompts: Discovering Rich Visual Clues through Mining Question-Answer Prompts for VQA requiring Diverse World Knowledge. Haibo Wang, Weifeng Ge, European Conference on Computer Vision. Springer2024</p>
<p>Improving zero-shot visual question answering via large language models with reasoning question prompts. Yunshi Lan, Xiang Li, Xin Liu, Yang Li, Wei Qin, Weining Qian, Proceedings of the ACM International Conference on Multimedia. the ACM International Conference on Multimedia2023</p>
<p>Visual chain of thought: bridging logical gaps with multimodal infillings. Daniel Rose, Himakunthala, Vaishnavi, Andy Ouyang, Ryan He, Alex Mei, Yujie Lu, Michael Saxon, Chinmay Sonar, Diba Mirza, William Wang, Yang, arXiv:2305.023172023arXiv preprint</p>
<p>End-to-End Chart Summarization via Visual Chain-of-Thought in Vision-Language Models. Raymond Choi, Frank Burns, Chase Lawrence, arXiv:2502.175892025arXiv preprint</p>
<p>LLaVA-o1: Let Vision Language Models Reason Step-by-Step. Guowei Xu, Jin , Peng Hao, Li Song, Yibing Sun, Lichao Yuan, Li , arXiv:2411.104402024arXiv preprint</p>
<p>Zero-shot visual reasoning through probabilistic analogical mapping. Taylor Webb, Shuhao Fu, Trevor Bihl, Keith J Holyoak, Hongjing Lu, Nature Communications. 14151442023</p>
<p>VLM-RL: A Unified Vision Language Models and Reinforcement Learning Framework for Safe Autonomous Driving. Zilin Huang, Zihao Sheng, Yansong Qu, Junwei You, Sikai Chen, arXiv:2412.155442024arXiv preprint</p>
<p>Dissociating language and thought in human reasoning. John P Coetzee, Johnson , Micah A Lee, Youngzie Wu, Allan D Iacoboni, Marco Monti, Martin M , Brain Sciences. 131672022</p>
<p>PaLM: Scaling Language Modeling with Pathways. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Mishra, Gaurav, Adam Roberts, Paul Barham, Chung , Hyung Won, Charles Sutton, Gehrmann , Sebastian Others, Journal of Machine Learning Research. 242402023</p>
<p>Training verifiers to solve math word problems. Karl Cobbe, Kosaraju, Vineet, Mohammad Bavarian, Chen , Mark Jun, Heewoo Kaiser, Lukasz Plappert, Matthias Tworek, Jerry Hilton, Jacob Nakano, Reiichiro Others, arXiv:2110.141682021arXiv preprint</p>
<p>Chain-of-thought reasoning without prompting. Xuezhi Wang, Denny Zhou, arXiv:2402.102002024arXiv preprint</p>
<p>To CoT or not to CoT? Chain-of-thought helps mainly on math and symbolic reasoning. Zayne Sprague, Fangcong Yin, Juan Rodriguez, Diego, Dongwei Jiang, Manya Wadhwa, Prasann Singhal, Xinyu Zhao, Xi Ye, Kyle Mahowald, Greg Durrett, arXiv:2409.121832024arXiv preprint</p>
<p>Agent Q: Advanced Reasoning and Learning for Autonomous AI Agents. Pranav Putta, Edmund Mills, Naman Garg, Sumeet Motwani, Chelsea Finn, Divyansh Garg, Rafael Rafailov, arXiv:2408.071992024arXiv preprint</p>
<p>Logic-RL: Unleashing LLM Reasoning with Rule-Based Reinforcement Learning. Tian Xie, Zitian Gao, Qingnan Ren, Haoming Luo, Yuqian Hong, Bryan Dai, Joey Zhou, Kai Qiu, Zhirong Wu, Chong Luo, arXiv:2502.147682025arXiv preprint</p>
<p>Reasoning with reinforced functional token tuning. Kongcheng Zhang, Qi Yao, Baisheng Lai, Jiaxing Huang, Wenkai Fang, Dacheng Tao, Mingli Song, Shunyu Liu, arXiv:2502.133892025arXiv preprint</p>
<p>AST: Audio Spectrogram Transformer. Yuan Gong, Chung , Yu-An And Glass, James , arXiv:2104.017782021arXiv preprint</p>
<p>LLaMA: Open and Efficient Foundation Language Models. Hugo Touvron, Thibaut Lavril, Izacard, Gautier, Xavier Martinet, Marie-Anne And Lachaux, Timothée Lacroix, Rozière, Baptiste, Naman Goyal, Eric Hambro, Azhar, arXiv:2302.139712023arXiv preprint</p>
<p>SpatialRGPT: Grounded Spatial Reasoning in Vision Language Models. An-Chieh And Cheng, Hongxu Yin, Yang Fu, Qiushan Guo, Ruihan Yang, Jan Kautz, Xiaolong Wang, Sifei Liu, arXiv:2406.015842024arXiv preprint</p>
<p>Temporal reasoning transfer from text to video. Lei Li, Yuanxin Liu, Linli Yao, Peiyuan Zhang, An , Chenxin Wang, Lean Sun, Xu, Lingpeng Kong, Qi Liu, arXiv:2410.061662024arXiv preprint</p>
<p>Learning spatial models for navigation. Susan L Epstein, Anoop Aroor, Matthew Evanusa, Elizabeth I Sklar, Simon Parsons, International Conference on Spatial Information Theory. Springer2015</p>
<p>Artificial Intelligence Geographic Information Systems-AI GIS. Zakaria Ahmed, Yehia, International Journal of Advanced Engineering and Business Sciences. 512024</p>
<p>Deep learning. Yann Lecun, Yoshua Bengio, Geoffrey Hinton, Nature. 52175532015</p>
<p>The graph neural network model. Franco Scarselli, Marco Gori, Tsoi , Ah Chung, Markus Hagenbuchner, Gabriele Monfardini, IEEE Transactions on Neural Networks. 2012008</p>
<p>Metric Reasoning in Large Language Models. Kent O'sullivan, Nicole R Schneider, Hanan Samet, Proceedings of the ACM International Conference on Advances in Geographic Information Systems, ser. SIGSPATIAL '24. the ACM International Conference on Advances in Geographic Information Systems, ser. SIGSPATIAL '24New York, NY, USAACM2024</p>
<p>Reframing spatial reasoning evaluation in language models: A real-world simulation benchmark for qualitative reasoning. Fangjun Li, David C Hogg, Anthony G Cohn, arXiv:2405.150642024arXiv preprint</p>
<p>Spatial representation and reasoning in RCC-8 with Boolean region terms. Frank Wolter, Michael Zakharyaschev, Proceedings of the European Conference on Artificial Intelligence. the European Conference on Artificial IntelligenceCiteseer2000</p>
<p>Qualitative process theory. Kenneth D Forbus, Artificial Intelligence. 241-31984</p>
<p>AZTR: Aerial Video Action Recognition with Auto Zoom and Temporal Reasoning. Xijun Wang, Ruiqi Xian, Tianrui Guan, Celso M De Melo, Stephen M Nogar, Aniket Bera, Dinesh Manocha, 2023IEEE</p>
<p>ReasonNet: End-to-End Driving with Temporal and Global Reasoning. Hao Shao, Wang , Letian Chen, Ruobing Waslander, Steven L Li, Hongsheng Liu, Yu , Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2023733</p>
<p>Discovering spatio-temporal rationales for video question answering. Yicong Li, Xiao , Junbin Feng, Chun , Wang , Xiang Chua, Tat-Seng , Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer Vision202313878</p>
<p>3D deformable convolution temporal reasoning network for action recognition. Yangjun Ou, Zhenzhong Chen, Journal of Visual Communication and Image Representation. 931038042023</p>
<p>TKN: Transformer-based Keypoint Prediction Network For Real-time Video Prediction. Haoran Li, Pengyuan Zhou, Yihang Lin, Yanbin Hao, Haiyong Xie, Yong Liao, arXiv:2303.098072023arXiv preprint</p>
<p>JSTR: Joint Spatio-Temporal Reasoning for Event-based Moving Object Detection. Hanyu Zhou, Zhiwei Shi, Dong, Hao, Shihan Peng, Yi Chang, Luxin Yan, 2024IEEE10656</p>
<p>Finding structure in time. Jeffrey L Elman, Cognitive science. 1421990</p>
<p>Long short-term memory. Sepp Hochreiter, Jürgen Schmidhuber, Neural Computation. 981997</p>
<p>Learning phrase representations using RNN encoder-decoder for statistical machine translation. Kyunghyun Cho, Van Merriënboer, Bart Gulcehre, Caglar Bahdanau, Dzmitry Bougares, Fethi Schwenk, Holger Bengio, Yoshua , arXiv:1406.10782014arXiv preprint</p>
<p>Attention is all you need. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, Illia Polosukhin, Advances in Neural Information Processing Systems. 201730</p>
<p>Back to the future: Towards explainable temporal reasoning with large language models. Chenhan Yuan, Qianqian Xie, Jimin Huang, Sophia Ananiadou, Proceedings of the ACM Web Conference. the ACM Web Conference2024</p>
<p>A survey on neural-symbolic learning systems. Dongran Yu, Bo Yang, Dayou Liu, Hui Wang, Shirui Pan, Neural Networks. 1662023</p>
<p>Gradient-based learning applied to document recognition. Yann Lecun, Léon Bottou, Yoshua Bengio, Patrick Haffner, Proceedings of the IEEE. 86111998</p>
<p>Probabilistic logic neural networks for reasoning. Meng Qu, Jian Tang, Advances in Neural Information Processing Systems. 201932</p>
<p>Efficient probabilistic logic reasoning with graph neural networks. Yuyu Zhang, Xinshi Chen, Yang , Yuan Ramamurthy, Arun Li, Bo Qi, Yuan Song, Le , arXiv:2001.118502020arXiv preprint</p>
<p>Learn to explain efficiently via neural logic inductive learning. Yuan Yang, Le Song, arXiv:1910.024812019arXiv preprint</p>
<p>The neuro-symbolic concept learner: Interpreting scenes, words, and sentences from natural supervision. Jiayuan Mao, Chuang Gan, Pushmeet Kohli, Joshua B Tenenbaum, Jiajun Wu, arXiv:1904.125842019arXiv preprint</p>
<p>DeepProbLog: Neural Probabilistic Logic Programming. Robin Manhaeve, Sebastijan Dumancic, Angelika Kimmig, Thomas Demeester, De Raedt, Luc , Advances in Neural Information Processing Systems. 312018</p>
<p>Approximate inference for neural probabilistic logic programming. Robin Manhaeve, Giuseppe Marra, De Raedt, Luc , Proceedings of the International Conference on Principles of Knowledge Representation and Reasoning. IJCAI Organization. the International Conference on Principles of Knowledge Representation and Reasoning. IJCAI Organization2021</p>
<p>Parameter estimation for probabilistic finite-state transducers. Jason Eisner, Proceedings of the Annual Meeting of the Association for Computational Linguistics. the Annual Meeting of the Association for Computational Linguistics2002</p>
<p>A probabilistic graphical model based on neural-symbolic reasoning for visual relationship detection. Dongran Yu, Bo Yang, Qianhao Wei, Anchen Li, Shirui Pan, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2022618</p>
<p>Inductive reasoning in humans and large language models. Simon Han, Ransom Jerome, Keith J Perfors, Andrew Kemp, Charles , Cognitive Systems Research. 831011552024</p>
<p>. Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Akkaya, Florencia Ilge And Aleman, Leoni, Diogo Almeida, Janko Altenschmidt, Sam Altman, Anadkat , arXivpreprintarXiv:2303.08774Shyamal and others. 2023GPT-4 Technical Report</p>
<p>Testing the general deductive reasoning capacity of large language models using ood examples. Abulhair Saparov, Richard Pang, Yuanzhe, Padmakumar, Vishakh, Nitish Joshi, Mehran Kazemi, Najoung Kim, He He, Advances in Neural Information Processing Systems. 202336</p>
<p>CaPo: Cooperative Plan Optimization for Efficient Embodied Multi-Agent Cooperation. Jie Liu, Pan Zhou, Yingjun Du, Ah-Hwee And Tan, Snoek, G M Cees, Jan-Jakob And Sonke, Efstratios Gavves, arXiv:2411.046792024arXiv preprint</p>
<p>Building cooperative embodied agents modularly with large language models. Hongxin Zhang, Weihua Du, Jiaming Shan, Qinhong Zhou, Yilun Du, Joshua B Tenenbaum, Tianmin Shu, Chuang Gan, arXiv:2307.024852023arXiv preprint</p>
<p>Language grounded multi-agent reinforcement learning with human-interpretable communication. Huao Li, Nourkhiz Mahjoub, Hossein, Behdad Chalaki, Tadiparthi, Vaishnav, Kwonjoon Lee, Moradi Pari, Ehsan, Charles Lewis, Katia Sycara, Advances in Neural Information Processing Systems. 202437933</p>
<p>Society of mind. Marvin Minsky, 1988Simon and Schuster</p>
<p>VQA: Visual Question Answering. Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C Zitnick, Lawrence, Devi Parikh, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer Vision2015</p>
<p>Making the V in VQA Matter: Elevating the Role of Image Understanding in Visual Question Answering. Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, Devi Parikh, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2017</p>
<p>Super-CLEVR: A Virtual Benchmark to Diagnose Domain Robustness in Visual Reasoning. Zhuowan Li, Xingrui Wang, Elias Stengel-Eskin, Adam Kortylewski, Wufei Ma, Van Durme, Alan L Benjamin And Yuille, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition202314973</p>
<p>GQA: A New Dataset for Real-World Visual Reasoning and Compositional Question Answering. Drew A Hudson, Christopher D Manning, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2019</p>
<p>A corpus for reasoning about natural language grounded in photographs. Alane Suhr, Stephanie Zhou, Ally Zhang, Iris Zhang, Huajun Bai, Yoav Artzi, arXiv:1811.004912018arXiv preprint</p>
<p>OK-VQA: A Visual Question Answering Benchmark Requiring External Knowledge. Kenneth Marino, Mohammad Rastegari, Ali Farhadi, Roozbeh Mottaghi, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2019</p>
<p>A-OKVQA: A Benchmark for Visual Question Answering using World Knowledge. Dustin Schwenk, Apoorv Khandelwal, Christopher Clark, Kenneth Marino, Roozbeh Mottaghi, European Conference on Computer Vision. Springer2022</p>
<p>Zhongshen Zeng, Yinhong Liu, Yingjia Wan, Jingyao Li, Pengguang Chen, Jianbo Dai, Yuxuan Yao, Rongwu Xu, Zehan Qi, Wanru Zhao, Linling Shen, Jianqiao Lu, Haochen Tan, Yukang Chen, Hao Zhang, Zhan Shi, Bailin Wang, Zhijiang Guo, Jiaya Jia, MR-Ben: A Meta-Reasoning Benchmark for Evaluating System-2 Thinking in LLMs. 2024Advances in Neural Information Processing Systems</p>
<p>RM-Bench: Benchmarking Reward Models of Language Models with Subtlety and Style. Yantao Liu, Zijun Yao, Rui Min, Yixin Cao, Lei Hou, Juanzi Li, International Conference on Learning Representations. 2025</p>
<p>LR 2 Bench: Evaluating Long-chain Reflective Reasoning Capabilities of Large Language Models via Constraint Satisfaction Problems. Jianghao Chen, Zhenlin Wei, Zhenjiang Ren, Ziyong Li, Jiajun Zhang, arXiv:2502.178482025arXiv preprint</p>
<p>Alon Albalak, Duy Phung, Nathan Lile, Rafael Rafailov, Kanishk Gandhi, Louis Castricato, Anikait Singh, Chase Blagden, Violet Xiang, Dakota Mahan, Others, arXiv:2502.17387Big-Math: A Large-Scale, High-Quality Math Dataset for Reinforcement Learning in Language Models. 2025arXiv preprint</p>
<p>LongReason: A Synthetic Long-Context Reasoning Benchmark via Context Expansion. Zhan Ling, Kang Liu, Yan , Kai Yang, Yifan Lin, Weijian , Fan Ting-Han And Shen, Lingfeng Du, Zhengyin Chen, Jiecao , arXiv:2501.150892025arXiv preprint</p>
<p>Big-bench extra hard. Mehran Kazemi, Bahare Fatemi, Hritik Bansal, John Palowitch, Chrysovalantis Anastasiou, Sanket Mehta, Vaibhav, Lalit K Jain, Virginia Aglietti, Disha Jindal, Peter Chen, Others, arXiv:2502.191872025arXiv preprint</p>
<p>ResearchBench: Benchmarking LLMs in Scientific Discovery via Inspiration-Based Task Decomposition. Yujie Liu, Zonglin Yang, Tong Xie, Jinjie Ni, Ben Gao, Yuqiang Li, Shixiang Tang, Wanli Ouyang, Erik Cambria, Dongzhan Zhou, arXiv:2503.212482025arXiv preprint</p>
<p>Jonas Golde, Patrick Haller, Fabio Barth, Alan Akbik, arXiv:2503.05891MastermindEval: A Simple But Scalable Reasoning Benchmark. 2025arXiv preprint</p>
<p>Zhaojian Yu, Yinghao Wu, Yilun Zhao, Arman Cohan, Xiao-Ping Zhang, arXiv:2504.00810Z1: Efficient Test-time Scaling with Code. 2025arXiv preprint</p>
<p>AudioCaps: Generating Captions for Audios in The Wild. Chris Kim, Dongjoo, Byeongchang Kim, Hyunmin Lee, Gunhee Kim, Proceedings of the Conference of the North American Chapter. the Conference of the North American ChapterHuman Language Technologies2019</p>
<p>Clotho: An Audio Captioning Dataset. Konstantinos Drossos, Lipping, Samuel, Tuomas Virtanen, Proceedings of IEEE International Conference on Acoustics, Speech and Signal Processing. IEEE International Conference on Acoustics, Speech and Signal ProcessingIEEE2020</p>
<p>Transferable tactile transformers for representation learning across diverse sensors and tasks. Jialiang Zhao, Yuxiang Ma, Lirui Wang, Edward H Adelson, arXiv:2406.136402024arXiv preprint</p>
<p>Touch100k: A large-scale touchlanguage-vision dataset for touch-centric multimodal representation. Ning Cheng, Changhao Guan, Jing Gao, Weihao Wang, You Li, Fandong Meng, Jie Zhou, Fang, Bin, Jinan Xu, Wenjuan Han, arXiv:2406.038132024arXiv preprint</p>
<p>Any-Touch: Learning Unified Static-Dynamic Representation across Multiple Visuo-tactile Sensors. Ruoxuan Feng, Jiangyu Hu, Wenke Xia, Tianci Gao, Ao Shen, Yuhao Sun, Fang, Bin, Di Hu, arXiv:2502.121912025arXiv preprint</p>
<p>RAVEN: A Dataset for Relational and Analogical Visual rEasoNing. Chi Zhang, Feng Gao, Baoxiong Jia, Jiajun Lu, Zhu, Song-Chun, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2019</p>
<p>SPARQA: A Spatial Reasoning Question Answering Dataset for Visual Scene Understanding. Ethan Perez, Aniruddha Kembhavi, C Zitnick, Lawrence, Ali Farhadi, Hannaneh Hajishirzi, 2021Findings of the Association for Computational Linguistics</p>
<p>GRiT: General Robust Image Task Benchmark for Spatial Graph Reasoning. Xiaojian Yang, Yuncheng Li, Xin Wang, Trevor Darrell, Advances in Neural Information Processing Systems. 2022</p>
<p>You need to pay attention: Fine-grained visual question answering. Aniruddha Kembhavi, Tejas Salvato, Eric Kolve, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2017</p>
<p>CoDraw: Collaborative Drawing as a Testbed for Grounded Goal-driven Communication. Jae Kim, Sung, Proceedings of the Annual Meeting of the Association for Computational Linguistics. the Annual Meeting of the Association for Computational Linguistics2019</p>
<p>Touchdown: Natural language navigation and spatial reasoning in visual street environments. Howard Chen, Alane Suhr, Dipendra Misra, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2019</p>
<p>Visionand-language navigation: Interpreting visually-grounded navigation instructions in real environments. Peter Anderson, Qi Wu, Damien Teney, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2018</p>
<p>SpatialSense: An Adversarially Crowdsourced Benchmark for Spatial Relation Recognition. Yi-Lin And Yang, Rowan Zellers, Ali Farhadi, Yejin Choi, Findings of the Association for Computational Linguistics. 2019</p>
<p>A dataset for answering time-sensitive questions. Wenhu Chen, Xinyi Wang, William Wang, Yang, arXiv:2108.063142021arXiv preprint</p>
<p>Time-aware language models as temporal knowledge bases. Bhuwan Dhingra, Cole , Jeremy R , Eisenschlos , Julian Martin, Gillick, Jacob Daniel And Eisenstein, William W Cohen, 2022Transactions of the Association for Computational Linguistics10</p>
<p>StreamingQA: A Benchmark for Adaptation to New Knowledge over Time in Question Answering Models. Adam Liska, Tomas Kocisky, Elena Gribovskaya, Tayfun Terzi, Eren Sezener, Devang Agrawal, D 'autume, Cyprien De Masson, Tim Scholtes, Manzil Zaheer, Susannah Young, Others, International Conference on Machine Learning. PMLR202213622</p>
<p>Towards benchmarking and improving the temporal reasoning capability of large language models. Qingyu Tan, Hwee Ng, Tou, Lidong Bing, arXiv:2306.089522023arXiv preprint</p>
<p>MenatQA: A New Dataset for Testing the Temporal Comprehension and Reasoning Abilities of Large Language Models. Yifan Wei, Yisong Su, Huanhuan Ma, Xiaoyan Yu, Fangyu Lei, Yuanzhe Zhang, Jun Zhao, Kang Liu, arXiv:2310.051572023arXiv preprint</p>
<p>TRAM: Benchmarking Temporal Reasoning for Large Language Models. Yuqing Wang, Yun Zhao, arXiv:2310.008352023arXiv preprint</p>
<p>ReClor: A Reading Comprehension Dataset Requiring Logical Reasoning. Weihao Yu, Zihang Jiang, Yanfei Dong, Jiashi Feng, arXiv:2002.043262020arXiv preprint</p>
<p>Diagnosing the first-order logical reasoning ability through LogicNLI. Jidong Tian, Yitian Li, Wenqing Chen, Xiao , Liqiang He, Jin Hao, Yaohui , Proceedings of the Conference on Empirical Methods in Natural Language Processing. the Conference on Empirical Methods in Natural Language Processing2021</p>
<p>FOLIO: Natural Language Reasoning with First-Order Logic. Simeng Han, Hailey Schoelkopf, Yilun Zhao, Zhenting Qi, Martin Riddell, Wenfei Zhou, James Coady, David Peng, Yujie Qiao, Luke Benson, Others, arXiv:2209.008402022arXiv preprint</p>
<p>From lsat: The progress and challenges of complex reasoning. Siyuan Wang, Zhongkun Liu, Wanjun Zhong, Ming Zhou, Zhongyu Wei, Zhumin Chen, Nan Duan, IEEE/ACM Transactions on Audio, Speech, and Language Processing. 302022</p>
<p>LogiQA 2.0-An Improved Dataset for Logical Reasoning in Natural Language Understanding. Hanmeng Liu, Jian Liu, Leyang Cui, Zhiyang Teng, Nan Duan, Ming Zhou, Yue Zhang, IEEE/ACM Transactions on Audio, Speech, and Language Processing. 312023</p>
<p>Log-icBench: A Benchmark for Evaluation of Logical Reasoning. Mihir Parmar, Neeraj Varshney, Nisarg Patel, Mashetty, Santosh, Man Luo, Arindam Mitra, Chitta Baral, 2023</p>
<p>Andrew M Bean, Simi Hellsten, Harry Mayne, Jabez Magomere, Chi , Ethan A Chi, Ryan , Hale , Scott A Kirk, Hannah Rose, arXiv:2406.06196LINGOLY: A Benchmark of Olympiad-Level Linguistic Reasoning Puzzles in Low-Resource and Extinct Languages. 2024arXiv preprint</p>
<p>Roses are red, violets are blue... but should VQA expect them to. Corentin Kervadec, Grigory Antipov, Moez Baccouche, Christian Wolf, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2021</p>
<p>CLEVR: A Diagnostic Dataset for Compositional Language and Elementary Visual Reasoning. Justin Johnson, Bharath Hariharan, Laurens Van Der Maaten, Li Fei-Fei, Lawrence Zitnick, Ross Girshick, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2017</p>
<p>Navigation through unknown and dynamic open spaces using topological notions. Sergio Miguel-Tomé, Connection Science. 3022018</p>
<p>Spatial representation and reasoning for human-robot collaboration. William G Kennedy, Magdalena D Bugajska, Marge , Matthew , Adams , William , Fransen , Benjamin R Perzanowski, Dennis Schultz, Alan C Trafton, J Gregory, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence20077</p>
<p>Integrated commonsense reasoning and deep learning for transparent decision making in robotics. Tiago Mota, Mohan Sridharan, Aleš Leonardis, SN Computer Science. 242422021</p>
<p>Trust-aware decision making for humanrobot collaboration: Model learning and planning. Min Chen, Stefanos Nikolaidis, Harold Soh, David Hsu, Siddhartha Srinivasa, ACM Transactions on Human-Robot Interaction. 922020</p>
<p>Vision AI-based human-robot collaborative assembly driven by autonomous robots. Sichao Liu, Jianjing Zhang, Lihui Wang, Robert X Gao, CIRP annals. 7312024</p>
<p>Embodied artificial intelligence: Trends and challenges. Rolf Pfeifer, Fumiya Iida, 2004Lecture notes in computer science</p>
<p>Visuomotor navigation for embodied robots with spatial memory and semantic reasoning cognition. Qiming Liu, Guangzhan Wang, Zhe Liu, Hesheng Wang, IEEE Transactions on Neural Networks and Learning Systems. 2024</p>
<p>Hazard challenge: Embodied decision making in dynamically changing environments. Qinhong Zhou, Sunli Chen, Yisong Wang, Haozhe Xu, Weihua Du, Hongxin Zhang, Yilun Du, Joshua B Tenenbaum, Chuang Gan, arXiv:2401.129752024arXiv preprint</p>
<p>Sensory gain control (amplification) as a mechanism of selective attention: electrophysiological and neuroimaging evidence. Steven A Hillyard, Edward K Vogel, Steven J Luck, Philosophical Transactions of the Royal Society of London. Series B: Biological Sciences. 35313731998</p>
<p>Hearing in complex environments: auditory gain control, attention, and hearing loss. Benjamin D Auerbach, Howard J Gritton, Frontiers in neuroscience. 202216799787</p>
<p>Spiking neural networks. Ghosh-Dastidar, Samanwoy, Hojjat Adeli, International Journal of Neural Systems. 19042009</p>
<p>Retrieval-augmented generation for large language models: A survey. Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Haofen Wang, Haofen Wang, arXiv:2312.1099720232arXiv preprint</p>
<p>. An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Others, arXivpreprintarXiv:2412.151152024Qwen2.5 Technical Report</p>
<p>Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond. Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, Jingren Zhou, arXiv:2308.129662023arXiv preprint</p>
<p>Aaron Hurst, Adam Lerer, Goucher , Adam P Perelman, Adam , Ramesh , Aditya Clark, Aidan Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, Others, arXiv:2410.21276GPT-4o System Card. 2024arXiv preprint</p>
<p>The parahippocampal place area: recognition, navigation, or encoding. Russell Epstein, Alison Harris, Damian Stanley, Nancy Kanwisher, Neuron. 2311999</p>
<p>A cortical representation of the local visual environment. Russell Epstein, Nancy Kanwisher, Nature. 39266761998</p>
<p>Parahippocampal and retrosplenial contributions to human spatial navigation. Russell A Epstein, Trends in cognitive sciences. 12102008</p>
<p>Human hippocampal and entorhinal neurons encode the temporal structure of experience. P Tacikowski, G Kalender, D Ciliberti, I Fried, Nature. 63580372024</p>
<p>The cognitive map in humans: spatial navigation and beyond. R A Epstein, E Z Patai, J B Julian, H J Spiers, Nature neuroscience. 20112017</p>
<p>4D Gaussian Splatting for Real-Time Dynamic Scene Rendering. Guanjun Wu, Taoran Yi, Jiemin Fang, Lingxi Xie, Xiaopeng Zhang, Wei Wei, Wenyu Liu, Qi Tian, Xinggang Wang, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition202420320</p>
<p>Align Your Gaussians: Text-to-4D with Dynamic 3D Gaussians and Composed Diffusion Models. Huan Ling, Kim , Seung Wook, Antonio Torralba, Sanja Fidler, Karsten Kreis, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2024</p>
<p>Neural ordinary differential equations. Ricky Tq Chen, Yulia Rubanova, Jesse Bettencourt, David K Duvenaud, Advances in Neural Information Processing Systems. 201831</p>
<p>Agent-R: Training Language Model Agents to Reflect via Iterative Self-Training. Siyu Yuan, Zehui Chen, Zhiheng Xi, Junjie Ye, Zhengyin Du, Jiecao Chen, arXiv:2501.114252025arXiv preprint</p>
<p>Vision-language models can selfimprove reasoning via reflection. Kanzhi Cheng, Yantao Li, Fangzhi Xu, Jianbing Zhang, Zhou, Hao, Yang Liu, arXiv:2411.008552024arXiv preprint</p>
<p>Meta-Reflection: A Feedback-Free Reflection Learning Framework. Yaoke Wang, Yun Zhu, Bao , Xintong Zhang, Wenqiao Dai, Suyang Chen, Kehan Li, Wenqiang Huang, Gang Tang, Siliang Zhuang, Yueting , arXiv:2412.137812024arXiv preprint</p>
<p>Cascaded attention: Adaptive and gated graph attention network for multiagent reinforcement learning. Shuhan Qi, Xinhao Huang, Peixi Peng, Xuzhong Huang, Jiajia Zhang, Xuan Wang, IEEE Transactions on Neural Networks and Learning Systems. 3532022</p>
<p>Bidirectional cascaded multimodal attention for multiple choice visual question answering. Sushmita Upadhyay, Sanjaya Tripathy, Shankar, Machine Vision and Applications. 20253641</p>
<p>Multi-Stage Production Decisions Based on Monte Carlo and Markov Decision Algorithms. Zhihao Zhao, Jiahe Zhao, Jiaqi Wang, Bingrui Xu, Heyu Gao, Ji Liu, International Conference on Data Analytics, Computing and Artificial Intelligence. IEEE2024</p>
<p>Zhiwei Wang, Yunji Wang, Zhongwang Zhang, Zhangchen Zhou, Jin , Hui Hu, Tianyang Sun, Jiacheng Li, Zhenguo Zhang, Yaoyu Xu, Zhi-Qin John, arXiv:2405.15302The Buffer Mechanism for Multi-Step Information Reasoning in Language Models. 2024arXiv preprint</p>
<p>Central attention is serial, but midlevel and peripheral attention are parallel-A hypothesis. Benjamin J Tamber-Rosenau, René Marois, Attention, Perception, &amp; Psychophysics. 782016</p>
<p>Brain mechanisms of serial and parallel processing during dual-task performance. Mariano Sigman, Stanislas Dehaene, Journal of Neuroscience. 28302008</p>
<p>SelFee: Iterative Self-Revising LLM Empowered by Self-Feedback Generation. Seonghyeon Ye, Jo , Yongrae Kim, Doyoung Kim, Sungdong Hwang, Hyeonbin Seo, Minjoon , Blog post. 2023</p>
<p>Visual cognition in multimodal large language models. Luca M Schulze Buschoff, Elif Akata, Matthias Bethge, Eric Schulz, Nature Machine Intelligence. 2025</p>
<p>To-kenCarve: Information-Preserving Visual Token Compression in Multimodal Large Language Models. Xudong Tan, Peng Ye, Chongjun Tu, Jianjian Cao, Yaoxin Yang, Lin Zhang, Dongzhan Zhou, Tao Chen, arXiv:2503.105012025arXiv preprint</p>
<p>Ting Liu, Liangtao Shi, Richang Hong, Yue Hu, Quanjun Yin, Linfeng Zhang, arXiv:2411.10803Multi-Stage Vision Token Dropping: Towards Efficient Multimodal Large Language Model. 2024arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6464 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6464</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6464</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-127.html">extraction-schema-127</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <p><strong>Paper ID:</strong> paper-11155af5ccd1889277f4269f6bb349a7633554f4</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/11155af5ccd1889277f4269f6bb349a7633554f4" target="_blank">TravelPlanner: A Benchmark for Real-World Planning with Language Agents</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Machine Learning</p>
                <p><strong>Paper TL;DR:</strong> This work proposes TravelPlanner, a new planning benchmark that focuses on travel planning, a common real-world planning scenario and provides a challenging yet meaningful testbed for future language agents.</p>
                <p><strong>Paper Abstract:</strong> Planning has been part of the core pursuit for artificial intelligence since its conception, but earlier AI agents mostly focused on constrained settings because many of the cognitive substrates necessary for human-level planning have been lacking. Recently, language agents powered by large language models (LLMs) have shown interesting capabilities such as tool use and reasoning. Are these language agents capable of planning in more complex settings that are out of the reach of prior AI agents? To advance this investigation, we propose TravelPlanner, a new planning benchmark that focuses on travel planning, a common real-world planning scenario. It provides a rich sandbox environment, various tools for accessing nearly four million data records, and 1,225 meticulously curated planning intents and reference plans. Comprehensive evaluations show that the current language agents are not yet capable of handling such complex planning tasks-even GPT-4 only achieves a success rate of 0.6%. Language agents struggle to stay on task, use the right tools to collect information, or keep track of multiple constraints. However, we note that the mere possibility for language agents to tackle such a complex problem is in itself non-trivial progress. TravelPlanner provides a challenging yet meaningful testbed for future language agents.</p>
                <p><strong>Cost:</strong> 0.022</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6464.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6464.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>NotebookWrite</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>NotebookWrite tool (TravelPlanner)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An external short-term notebook tool provided by the TravelPlanner environment that agents must write searched tool outputs into; the Planner reads only from this notebook to assemble final plans, acting as an explicit working-memory buffer to avoid context-token blowup.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>ReAct-based two-stage language agent (uses NotebookWrite)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>In the TravelPlanner two-stage setup, ReAct-style agents perform Thought/Action/Observation cycles to call search tools and immediately write selected results into the Notebook via NotebookWrite; the downstream Planner is only allowed to read information from this notebook when producing a final plan.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>various (GPT-4-Turbo, GPT-3.5-Turbo, Gemini Pro, Mistral-7B-32K, Mixtral-8x7B-MoE)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>external short-term notebook (tool-based working memory)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>raw text entries / short descriptions of selected tool search results (structured entries written by the agent into Notebook)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td>explicit write action (NotebookWrite) by agent after each tool call; Planner reads the notebook contents (no learned retrieval controller described; access is direct/explicit)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>TravelPlanner (two-stage mode)</td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>planning (tool-augmented, multi-constraint real-world planning)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>0.6% final pass rate (test) for GPT-4-Turbo using ReAct in two-stage mode where agents use NotebookWrite (reported final pass rate meeting all constraints)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>4.4% final pass rate (test) for Direct GPT-4-Turbo in sole-planning mode where agents are given necessary information and do not call tools/Notebook (reported)</td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Final Pass Rate (% of feasible plans that meet all constraints)</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>Using the Notebook prevents context-token accumulation (addresses token limit) but two-stage agents (which must both collect info and manage notebook writes) perform substantially worse than sole-planning agents; authors discuss a multitasking/capacity trade-off where tool-use + notebook overhead reduces end-to-end planning performance.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Agents under-write or under-use the Notebook (fewer entries than reference plans), leading to incomplete information or hallucination; agents also make argument errors when calling tools and get trapped in dead loops, suggesting the notebook alone does not solve failures in tool interaction or global planning (see Table 5 and Sections 5.1–5.2).</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Jian Xie, Kai Zhang, Jiangjie Chen, Tinghui Zhu, Renze Lou, Yuandong Tian, Yanghua Xiao, Yu Su, TravelPlanner: A Benchmark for Real-World Planning with Language Agents</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'TravelPlanner: A Benchmark for Real-World Planning with Language Agents', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6464.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6464.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ReAct</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ReAct: Synergizing reasoning and acting in language models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A framework that interleaves reasoning (Thought) and acting (Action) enabling LLMs to call external tools and incorporate observations; used in TravelPlanner's two-stage experiments for iterative information collection.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>ReAct: Synergizing reasoning and acting in language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>ReAct-based agent (with NotebookWrite integration in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>ReAct agents in TravelPlanner alternate Thought/Action/Observation to call search tools; after tool calls they are instructed to write selected results into NotebookWrite, using the notebook as short-term memory for later planning.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>various (experiments used GPT-3.5-Turbo, GPT-4-Turbo, others)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>external short-term notebook (working memory produced by tool outputs)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>structured short descriptions of tool outputs written to Notebook (text passages representing search results)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td>explicit action-based writes to NotebookWrite after each tool call; Planner consumes Notebook contents to produce plans</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>TravelPlanner (two-stage mode)</td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>planning (tool-augmented, multi-constraint planning)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>ReAct (GPT-3.5-Turbo) achieved ~0.7% final pass rate (test) in two-stage experiments where agents used NotebookWrite; ReAct (GPT-4-Turbo) achieved 0.6% final pass rate (test) as reported for two-stage mode.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Direct (GPT-3.5-Turbo) in sole-planning mode had 0% final pass rate (test); Direct (GPT-4-Turbo) in sole-planning had 4.4% final pass rate (test). The paper contrasts two-stage ReAct runs (tool+notebook) with sole-planning runs where tools/notebook were not used.</td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Final Pass Rate (percentage)</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>ReAct is effective for iterative tool interaction but in multi-constraint, long-horizon TravelPlanner it often fails to convert internal reasoning into correct tool calls and to track global constraints; adding tool-use + memory increases system complexity and reduces overall success versus providing curated info.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Frequent tool-argument errors, dead loops (repetitive invalid actions), incomplete notebook entries (insufficient information), and failure to maintain global constraints (budget, minimum nights) — indicating ReAct + notebook does not solve coordinated long-horizon planning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>S. Yao, J. Zhao, D. Yu, N. Du, I. Shafran, K. R. Narasimhan, Y. Cao, ReAct: Synergizing reasoning and acting in language models. ICLR, 2022</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'TravelPlanner: A Benchmark for Real-World Planning with Language Agents', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6464.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6464.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Reflexion</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reflexion: Language agents with verbal reinforcement learning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A language-agent strategy that stores prior failed attempts and uses a reflection model to produce high-level corrective guidance for subsequent trials; evaluated in TravelPlanner to see if reflective memory helps planning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Reflexion: Language agents with verbal reinforcement learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Reflexion agent (as implemented in TravelPlanner experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Reflexion maintains a log of prior attempts and uses a reflection model to generate insights and corrective high-level advice; in TravelPlanner this mechanism serves as episodic/trajectory memory to guide subsequent planning attempts.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>GPT-3.5-Turbo (and evaluated with GPT-4-Turbo in validation for some runs)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>episodic log memory (history of prior attempts / failure traces)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>textual transcripts of previous attempts, thoughts, observations and reflection outputs (verbal narratives)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td>reflection model queries past attempts (verbalized history) to produce high-level guidance; implemented as prompt-based retrieval of earlier trajectories (no learned differentiable controller described)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>TravelPlanner (sole-planning and two-stage evaluations)</td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>planning (iterative improvement via reflection)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Reflexion (GPT-3.5-Turbo) reported ~0.6% final pass rate (test) in experiments; Reflexion (GPT-4-Turbo) achieved 3.3% final pass rate on validation in sole-planning (reported in Appendix B.3 / Table B.3).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Direct or non-reflective baselines for the same LLMs had lower or comparable final pass rates in some settings (e.g., Direct GPT-3.5 had 0% final pass rate on test), but overall Reflexion did not substantially solve multi-constraint failures.</td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Final Pass Rate (percentage)</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>While reflexive memory can increase delivery rate (more attempts completed), the paper notes Reflexion agents still often fail to align their deliberative reflections with executed actions — showing a gap between reflective guidance and action correctness.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Agents using Reflexion exhibited mismatches between 'thoughts' and executed actions (selecting costlier items despite reasoning to minimize cost), and still suffered from dead loops, hallucinations, and failure to satisfy all constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>N. Shinn, F. Cassano, A. Gopinath, K. R. Narasimhan, S. Yao, Reflexion: Language agents with verbal reinforcement learning. NeurIPS, 2023</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'TravelPlanner: A Benchmark for Real-World Planning with Language Agents', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6464.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6464.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Memory summarization</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Memory summarization techniques (e.g., interactive reading / summary-based memories)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Mentioned as a family of techniques to compress and maintain long histories for agents (cited works include Chen et al., 2023; Zhou et al., 2023; Liang et al., 2023), used to extend effective context for LLM agents.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>memory summarization techniques (general)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Described in related work as methods to summarize or condense past interactions or long context into compressed memories that can be kept in-context or externally and fed back to the LLM.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>summarized/condensed memory (working or long-term compressed summaries)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>textual summaries / condensed notes of past context</td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td>summary injection into context or retrieval of summaries (prompt-based); paper cites works but does not implement them in experiments</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>memory augmentation / long-context handling (general mention)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>Paper cites these methods as widely used to enhance memory but does not report Trade-offs itself; implicit trade-offs include condensation loss vs. context savings (not quantified in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Not evaluated in TravelPlanner experiments; authors note general limitations of existing agents handling long-horizon multi-constraint planning despite such memory techniques being proposed in related work.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>cited works: H. Chen et al., 'Walking down the memory maze: Beyond context limit through interactive reading' (arXiv 2023); W. Zhou et al., 'RecurrentGPT: Interactive generation of (arbitrarily) long text.' (arXiv 2023); X. Liang et al., 'Unleashing infinite-length input capacity for large-scale language models with self-controlled memory system' (arXiv 2023)</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'TravelPlanner: A Benchmark for Real-World Planning with Language Agents', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6464.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e6464.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Retrieval / MemoryBank</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Retrieval-augmented memory methods (e.g., MemoryBank)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Mentioned in related work as retrieval-based memory systems that store past information and retrieve relevant items (cited Andreas 2022; Park et al. 2023; Zhong et al. 2023 - MemoryBank).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Memorybank: Enhancing large language models with long-term memory</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>retrieval-augmented memory systems (MemoryBank and related)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Related work notes retrieval-style long-term memory systems (external stores indexed for similarity search) as a way to augment LLM agents with persistent memory; TravelPlanner mentions retrieval as one of the widely employed techniques.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>external vector store / retrieval-augmented memory (long-term memory)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>stored passages or embeddings (raw text passages / embedding vectors) as per cited systems (e.g., MemoryBank)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td>similarity search / retrieval (cited but not implemented in TravelPlanner experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>memory augmentation / retrieval</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>Paper references retrieval as beneficial but does not report direct trade-offs; typical trade-offs (not measured here) include retrieval latency and storage/maintenance complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Not evaluated in this benchmark; TravelPlanner demonstrates that even with tool access and notebook-style short-term memory, agents still fail at multi-constraint planning, implying retrieval alone may not suffice.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>W. Zhong, L. Guo, Q. Gao, Y. Wang, 'Memorybank: Enhancing large language models with long-term memory' (arXiv 2023)</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'TravelPlanner: A Benchmark for Real-World Planning with Language Agents', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6464.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e6464.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RecurrentGPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RecurrentGPT: Interactive generation of (arbitrarily) long text</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Mentioned in related work as a technique for interactive generation and handling arbitrarily long text via recurrent memory mechanisms.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Recurrentgpt: Interactive generation of (arbitrarily) long text</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>RecurrentGPT (method)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Cited as an example of systems that implement recurrent memory/summarization to allow handling very long contexts; mentioned as part of memory summarization techniques.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>recurrent / recurrent summarized memory</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>recursively updated summaries or state representations</td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td>recurrent summarization/roll-forward of context (described in cited work; TravelPlanner only references it)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>long-context generation / memory</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Not evaluated in this paper; mentioned as an existing technique in related literature.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>W. Zhou, Y. Jiang, P. Cui, T. Wang, Z. Xiao, H. Hou, R. Cotterell, M. Sachan, 'Recurrentgpt: Interactive generation of (arbitrarily) long text' (arXiv 2023)</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'TravelPlanner: A Benchmark for Real-World Planning with Language Agents', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6464.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e6464.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Generative Agents</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generative Agents: Interactive simulacra of human behavior</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Cited as related work on agents that use memory and retrieval to simulate human-like long-term behaviors (Park et al., 2023), referenced in the context of retrieval and memory-equipped agents.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Generative agents: Interactive simulacra of human behavior</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Generative Agents (Park et al., 2023)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Referenced as an example of agents that employ memory mechanisms (retrieval, summaries) to simulate sustained behaviors across time; TravelPlanner cites it in the discussion of retrieval-augmented memory techniques.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>episodic / retrieval memory</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>stored observations and summarized memories (textual episodic records)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td>retrieval from stored episodes (as described in cited work; not implemented in TravelPlanner experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>simulated human behavior / long-term memory use</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>Mentioned in related work; TravelPlanner does not evaluate generative agents directly.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Not evaluated in this paper; included as a pointer to retrieval/memory literature.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>J. S. Park, J. O’Brien, C. J. Cai, M. R. Morris, P. Liang, M. S. Bernstein, 'Generative agents: Interactive simulacra of human behavior' (UIST, 2023)</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'TravelPlanner: A Benchmark for Real-World Planning with Language Agents', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Memorybank: Enhancing large language models with long-term memory <em>(Rating: 2)</em></li>
                <li>Recurrentgpt: Interactive generation of (arbitrarily) long text <em>(Rating: 2)</em></li>
                <li>Reflexion: Language agents with verbal reinforcement learning <em>(Rating: 2)</em></li>
                <li>ReAct: Synergizing reasoning and acting in language models <em>(Rating: 2)</em></li>
                <li>Generative agents: Interactive simulacra of human behavior <em>(Rating: 1)</em></li>
                <li>Walking down the memory maze: Beyond context limit through interactive reading <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6464",
    "paper_id": "paper-11155af5ccd1889277f4269f6bb349a7633554f4",
    "extraction_schema_id": "extraction-schema-127",
    "extracted_data": [
        {
            "name_short": "NotebookWrite",
            "name_full": "NotebookWrite tool (TravelPlanner)",
            "brief_description": "An external short-term notebook tool provided by the TravelPlanner environment that agents must write searched tool outputs into; the Planner reads only from this notebook to assemble final plans, acting as an explicit working-memory buffer to avoid context-token blowup.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "ReAct-based two-stage language agent (uses NotebookWrite)",
            "agent_description": "In the TravelPlanner two-stage setup, ReAct-style agents perform Thought/Action/Observation cycles to call search tools and immediately write selected results into the Notebook via NotebookWrite; the downstream Planner is only allowed to read information from this notebook when producing a final plan.",
            "model_size": "various (GPT-4-Turbo, GPT-3.5-Turbo, Gemini Pro, Mistral-7B-32K, Mixtral-8x7B-MoE)",
            "memory_used": true,
            "memory_type": "external short-term notebook (tool-based working memory)",
            "memory_representation": "raw text entries / short descriptions of selected tool search results (structured entries written by the agent into Notebook)",
            "memory_access_mechanism": "explicit write action (NotebookWrite) by agent after each tool call; Planner reads the notebook contents (no learned retrieval controller described; access is direct/explicit)",
            "task_name": "TravelPlanner (two-stage mode)",
            "task_category": "planning (tool-augmented, multi-constraint real-world planning)",
            "performance_with_memory": "0.6% final pass rate (test) for GPT-4-Turbo using ReAct in two-stage mode where agents use NotebookWrite (reported final pass rate meeting all constraints)",
            "performance_without_memory": "4.4% final pass rate (test) for Direct GPT-4-Turbo in sole-planning mode where agents are given necessary information and do not call tools/Notebook (reported)",
            "has_comparative_results": true,
            "performance_metric": "Final Pass Rate (% of feasible plans that meet all constraints)",
            "tradeoffs_reported": "Using the Notebook prevents context-token accumulation (addresses token limit) but two-stage agents (which must both collect info and manage notebook writes) perform substantially worse than sole-planning agents; authors discuss a multitasking/capacity trade-off where tool-use + notebook overhead reduces end-to-end planning performance.",
            "limitations_or_failure_cases": "Agents under-write or under-use the Notebook (fewer entries than reference plans), leading to incomplete information or hallucination; agents also make argument errors when calling tools and get trapped in dead loops, suggesting the notebook alone does not solve failures in tool interaction or global planning (see Table 5 and Sections 5.1–5.2).",
            "citation": "Jian Xie, Kai Zhang, Jiangjie Chen, Tinghui Zhu, Renze Lou, Yuandong Tian, Yanghua Xiao, Yu Su, TravelPlanner: A Benchmark for Real-World Planning with Language Agents",
            "uuid": "e6464.0",
            "source_info": {
                "paper_title": "TravelPlanner: A Benchmark for Real-World Planning with Language Agents",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "ReAct",
            "name_full": "ReAct: Synergizing reasoning and acting in language models",
            "brief_description": "A framework that interleaves reasoning (Thought) and acting (Action) enabling LLMs to call external tools and incorporate observations; used in TravelPlanner's two-stage experiments for iterative information collection.",
            "citation_title": "ReAct: Synergizing reasoning and acting in language models",
            "mention_or_use": "use",
            "agent_name": "ReAct-based agent (with NotebookWrite integration in this paper)",
            "agent_description": "ReAct agents in TravelPlanner alternate Thought/Action/Observation to call search tools; after tool calls they are instructed to write selected results into NotebookWrite, using the notebook as short-term memory for later planning.",
            "model_size": "various (experiments used GPT-3.5-Turbo, GPT-4-Turbo, others)",
            "memory_used": true,
            "memory_type": "external short-term notebook (working memory produced by tool outputs)",
            "memory_representation": "structured short descriptions of tool outputs written to Notebook (text passages representing search results)",
            "memory_access_mechanism": "explicit action-based writes to NotebookWrite after each tool call; Planner consumes Notebook contents to produce plans",
            "task_name": "TravelPlanner (two-stage mode)",
            "task_category": "planning (tool-augmented, multi-constraint planning)",
            "performance_with_memory": "ReAct (GPT-3.5-Turbo) achieved ~0.7% final pass rate (test) in two-stage experiments where agents used NotebookWrite; ReAct (GPT-4-Turbo) achieved 0.6% final pass rate (test) as reported for two-stage mode.",
            "performance_without_memory": "Direct (GPT-3.5-Turbo) in sole-planning mode had 0% final pass rate (test); Direct (GPT-4-Turbo) in sole-planning had 4.4% final pass rate (test). The paper contrasts two-stage ReAct runs (tool+notebook) with sole-planning runs where tools/notebook were not used.",
            "has_comparative_results": true,
            "performance_metric": "Final Pass Rate (percentage)",
            "tradeoffs_reported": "ReAct is effective for iterative tool interaction but in multi-constraint, long-horizon TravelPlanner it often fails to convert internal reasoning into correct tool calls and to track global constraints; adding tool-use + memory increases system complexity and reduces overall success versus providing curated info.",
            "limitations_or_failure_cases": "Frequent tool-argument errors, dead loops (repetitive invalid actions), incomplete notebook entries (insufficient information), and failure to maintain global constraints (budget, minimum nights) — indicating ReAct + notebook does not solve coordinated long-horizon planning.",
            "citation": "S. Yao, J. Zhao, D. Yu, N. Du, I. Shafran, K. R. Narasimhan, Y. Cao, ReAct: Synergizing reasoning and acting in language models. ICLR, 2022",
            "uuid": "e6464.1",
            "source_info": {
                "paper_title": "TravelPlanner: A Benchmark for Real-World Planning with Language Agents",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Reflexion",
            "name_full": "Reflexion: Language agents with verbal reinforcement learning",
            "brief_description": "A language-agent strategy that stores prior failed attempts and uses a reflection model to produce high-level corrective guidance for subsequent trials; evaluated in TravelPlanner to see if reflective memory helps planning.",
            "citation_title": "Reflexion: Language agents with verbal reinforcement learning",
            "mention_or_use": "use",
            "agent_name": "Reflexion agent (as implemented in TravelPlanner experiments)",
            "agent_description": "Reflexion maintains a log of prior attempts and uses a reflection model to generate insights and corrective high-level advice; in TravelPlanner this mechanism serves as episodic/trajectory memory to guide subsequent planning attempts.",
            "model_size": "GPT-3.5-Turbo (and evaluated with GPT-4-Turbo in validation for some runs)",
            "memory_used": true,
            "memory_type": "episodic log memory (history of prior attempts / failure traces)",
            "memory_representation": "textual transcripts of previous attempts, thoughts, observations and reflection outputs (verbal narratives)",
            "memory_access_mechanism": "reflection model queries past attempts (verbalized history) to produce high-level guidance; implemented as prompt-based retrieval of earlier trajectories (no learned differentiable controller described)",
            "task_name": "TravelPlanner (sole-planning and two-stage evaluations)",
            "task_category": "planning (iterative improvement via reflection)",
            "performance_with_memory": "Reflexion (GPT-3.5-Turbo) reported ~0.6% final pass rate (test) in experiments; Reflexion (GPT-4-Turbo) achieved 3.3% final pass rate on validation in sole-planning (reported in Appendix B.3 / Table B.3).",
            "performance_without_memory": "Direct or non-reflective baselines for the same LLMs had lower or comparable final pass rates in some settings (e.g., Direct GPT-3.5 had 0% final pass rate on test), but overall Reflexion did not substantially solve multi-constraint failures.",
            "has_comparative_results": true,
            "performance_metric": "Final Pass Rate (percentage)",
            "tradeoffs_reported": "While reflexive memory can increase delivery rate (more attempts completed), the paper notes Reflexion agents still often fail to align their deliberative reflections with executed actions — showing a gap between reflective guidance and action correctness.",
            "limitations_or_failure_cases": "Agents using Reflexion exhibited mismatches between 'thoughts' and executed actions (selecting costlier items despite reasoning to minimize cost), and still suffered from dead loops, hallucinations, and failure to satisfy all constraints.",
            "citation": "N. Shinn, F. Cassano, A. Gopinath, K. R. Narasimhan, S. Yao, Reflexion: Language agents with verbal reinforcement learning. NeurIPS, 2023",
            "uuid": "e6464.2",
            "source_info": {
                "paper_title": "TravelPlanner: A Benchmark for Real-World Planning with Language Agents",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Memory summarization",
            "name_full": "Memory summarization techniques (e.g., interactive reading / summary-based memories)",
            "brief_description": "Mentioned as a family of techniques to compress and maintain long histories for agents (cited works include Chen et al., 2023; Zhou et al., 2023; Liang et al., 2023), used to extend effective context for LLM agents.",
            "citation_title": "here",
            "mention_or_use": "mention",
            "agent_name": "memory summarization techniques (general)",
            "agent_description": "Described in related work as methods to summarize or condense past interactions or long context into compressed memories that can be kept in-context or externally and fed back to the LLM.",
            "model_size": null,
            "memory_used": true,
            "memory_type": "summarized/condensed memory (working or long-term compressed summaries)",
            "memory_representation": "textual summaries / condensed notes of past context",
            "memory_access_mechanism": "summary injection into context or retrieval of summaries (prompt-based); paper cites works but does not implement them in experiments",
            "task_name": null,
            "task_category": "memory augmentation / long-context handling (general mention)",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_comparative_results": false,
            "performance_metric": null,
            "tradeoffs_reported": "Paper cites these methods as widely used to enhance memory but does not report Trade-offs itself; implicit trade-offs include condensation loss vs. context savings (not quantified in this paper).",
            "limitations_or_failure_cases": "Not evaluated in TravelPlanner experiments; authors note general limitations of existing agents handling long-horizon multi-constraint planning despite such memory techniques being proposed in related work.",
            "citation": "cited works: H. Chen et al., 'Walking down the memory maze: Beyond context limit through interactive reading' (arXiv 2023); W. Zhou et al., 'RecurrentGPT: Interactive generation of (arbitrarily) long text.' (arXiv 2023); X. Liang et al., 'Unleashing infinite-length input capacity for large-scale language models with self-controlled memory system' (arXiv 2023)",
            "uuid": "e6464.3",
            "source_info": {
                "paper_title": "TravelPlanner: A Benchmark for Real-World Planning with Language Agents",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Retrieval / MemoryBank",
            "name_full": "Retrieval-augmented memory methods (e.g., MemoryBank)",
            "brief_description": "Mentioned in related work as retrieval-based memory systems that store past information and retrieve relevant items (cited Andreas 2022; Park et al. 2023; Zhong et al. 2023 - MemoryBank).",
            "citation_title": "Memorybank: Enhancing large language models with long-term memory",
            "mention_or_use": "mention",
            "agent_name": "retrieval-augmented memory systems (MemoryBank and related)",
            "agent_description": "Related work notes retrieval-style long-term memory systems (external stores indexed for similarity search) as a way to augment LLM agents with persistent memory; TravelPlanner mentions retrieval as one of the widely employed techniques.",
            "model_size": null,
            "memory_used": true,
            "memory_type": "external vector store / retrieval-augmented memory (long-term memory)",
            "memory_representation": "stored passages or embeddings (raw text passages / embedding vectors) as per cited systems (e.g., MemoryBank)",
            "memory_access_mechanism": "similarity search / retrieval (cited but not implemented in TravelPlanner experiments)",
            "task_name": null,
            "task_category": "memory augmentation / retrieval",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_comparative_results": false,
            "performance_metric": null,
            "tradeoffs_reported": "Paper references retrieval as beneficial but does not report direct trade-offs; typical trade-offs (not measured here) include retrieval latency and storage/maintenance complexity.",
            "limitations_or_failure_cases": "Not evaluated in this benchmark; TravelPlanner demonstrates that even with tool access and notebook-style short-term memory, agents still fail at multi-constraint planning, implying retrieval alone may not suffice.",
            "citation": "W. Zhong, L. Guo, Q. Gao, Y. Wang, 'Memorybank: Enhancing large language models with long-term memory' (arXiv 2023)",
            "uuid": "e6464.4",
            "source_info": {
                "paper_title": "TravelPlanner: A Benchmark for Real-World Planning with Language Agents",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "RecurrentGPT",
            "name_full": "RecurrentGPT: Interactive generation of (arbitrarily) long text",
            "brief_description": "Mentioned in related work as a technique for interactive generation and handling arbitrarily long text via recurrent memory mechanisms.",
            "citation_title": "Recurrentgpt: Interactive generation of (arbitrarily) long text",
            "mention_or_use": "mention",
            "agent_name": "RecurrentGPT (method)",
            "agent_description": "Cited as an example of systems that implement recurrent memory/summarization to allow handling very long contexts; mentioned as part of memory summarization techniques.",
            "model_size": null,
            "memory_used": true,
            "memory_type": "recurrent / recurrent summarized memory",
            "memory_representation": "recursively updated summaries or state representations",
            "memory_access_mechanism": "recurrent summarization/roll-forward of context (described in cited work; TravelPlanner only references it)",
            "task_name": null,
            "task_category": "long-context generation / memory",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_comparative_results": false,
            "performance_metric": null,
            "tradeoffs_reported": null,
            "limitations_or_failure_cases": "Not evaluated in this paper; mentioned as an existing technique in related literature.",
            "citation": "W. Zhou, Y. Jiang, P. Cui, T. Wang, Z. Xiao, H. Hou, R. Cotterell, M. Sachan, 'Recurrentgpt: Interactive generation of (arbitrarily) long text' (arXiv 2023)",
            "uuid": "e6464.5",
            "source_info": {
                "paper_title": "TravelPlanner: A Benchmark for Real-World Planning with Language Agents",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Generative Agents",
            "name_full": "Generative Agents: Interactive simulacra of human behavior",
            "brief_description": "Cited as related work on agents that use memory and retrieval to simulate human-like long-term behaviors (Park et al., 2023), referenced in the context of retrieval and memory-equipped agents.",
            "citation_title": "Generative agents: Interactive simulacra of human behavior",
            "mention_or_use": "mention",
            "agent_name": "Generative Agents (Park et al., 2023)",
            "agent_description": "Referenced as an example of agents that employ memory mechanisms (retrieval, summaries) to simulate sustained behaviors across time; TravelPlanner cites it in the discussion of retrieval-augmented memory techniques.",
            "model_size": null,
            "memory_used": true,
            "memory_type": "episodic / retrieval memory",
            "memory_representation": "stored observations and summarized memories (textual episodic records)",
            "memory_access_mechanism": "retrieval from stored episodes (as described in cited work; not implemented in TravelPlanner experiments)",
            "task_name": null,
            "task_category": "simulated human behavior / long-term memory use",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_comparative_results": false,
            "performance_metric": null,
            "tradeoffs_reported": "Mentioned in related work; TravelPlanner does not evaluate generative agents directly.",
            "limitations_or_failure_cases": "Not evaluated in this paper; included as a pointer to retrieval/memory literature.",
            "citation": "J. S. Park, J. O’Brien, C. J. Cai, M. R. Morris, P. Liang, M. S. Bernstein, 'Generative agents: Interactive simulacra of human behavior' (UIST, 2023)",
            "uuid": "e6464.6",
            "source_info": {
                "paper_title": "TravelPlanner: A Benchmark for Real-World Planning with Language Agents",
                "publication_date_yy_mm": "2024-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Memorybank: Enhancing large language models with long-term memory",
            "rating": 2,
            "sanitized_title": "memorybank_enhancing_large_language_models_with_longterm_memory"
        },
        {
            "paper_title": "Recurrentgpt: Interactive generation of (arbitrarily) long text",
            "rating": 2,
            "sanitized_title": "recurrentgpt_interactive_generation_of_arbitrarily_long_text"
        },
        {
            "paper_title": "Reflexion: Language agents with verbal reinforcement learning",
            "rating": 2,
            "sanitized_title": "reflexion_language_agents_with_verbal_reinforcement_learning"
        },
        {
            "paper_title": "ReAct: Synergizing reasoning and acting in language models",
            "rating": 2,
            "sanitized_title": "react_synergizing_reasoning_and_acting_in_language_models"
        },
        {
            "paper_title": "Generative agents: Interactive simulacra of human behavior",
            "rating": 1,
            "sanitized_title": "generative_agents_interactive_simulacra_of_human_behavior"
        },
        {
            "paper_title": "Walking down the memory maze: Beyond context limit through interactive reading",
            "rating": 1,
            "sanitized_title": "walking_down_the_memory_maze_beyond_context_limit_through_interactive_reading"
        }
    ],
    "cost": 0.022352499999999997,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>TravelPlanner: A Benchmark for Real-World Planning with Language Agents</h1>
<p>Jian Xie<em> * Kai Zhang ${ }^{\text {</em> }}$ * Jiangjie Chen<em> Tinghui Zhu</em> ${ }^{\text {a }}$ Renze Lou<em><br></em>Yuandong Tian<em> Yanghua Xiao</em> Yu Su<em><br>${ }^{\text {a }}$ Fudan University ${ }^{\text {a }}$ The Ohio State University<br>${ }^{\text {</em> }}$ The Pennsylvania State University ${ }^{\text {M }}$ Meta AI<br>jianxie22@m.fudan.edu.cn, shawyh@fudan.edu.cn, {zhang.13253, su.809}@osu.edu<br>https://osu-nlp-group.github.io/TravelPlanner</p>
<h4>Abstract</h4>
<p>Planning has been part of the core pursuit for artificial intelligence since its conception, but earlier AI agents mostly focused on constrained settings because many of the cognitive substrates necessary for human-level planning have been lacking. Recently, language agents powered by large language models (LLMs) have shown interesting capabilities such as tool use and reasoning. Are these language agents capable of planning in more complex settings that are out of the reach of prior AI agents? To advance this investigation, we propose TravelPlanner, a new planning benchmark that focuses on travel planning, a common real-world planning scenario. It provides a rich sandbox environment, various tools for accessing nearly four million data records, and 1,225 meticulously curated planning intents and reference plans. Comprehensive evaluations show that the current language agents are not yet capable of handling such complex planning tasks-even GPT-4 only achieves a success rate of $0.6 \%$. Language agents struggle to stay on task, use the right tools to collect information, or keep track of multiple constraints. However, we note that the mere possibility for language agents to tackle such a complex problem is in itself non-trivial progress. TravelPlanner provides a challenging yet meaningful testbed for future language agents.</p>
<h2>1. Introduction</h2>
<p>Planning is a hallmark of human intelligence. It is an evolutionary feat built upon numerous other capacities: using various tools to iteratively collect information and make de-</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>cisions, recording intermediate plans (in working memory or on a physical device) for deliberation, and exploring alternative plans by running simulations, which in turn depends on a world model (Mattar \&amp; Lengyel, 2022; Ho et al., 2022). For decades, researchers have been attempting to develop AI agents to mimic humans' planning capability (Russell \&amp; Norvig, 2010; Georgievski \&amp; Aiello, 2015; Karpas \&amp; Magazzeni, 2020; Lin et al., 2023; Zhang et al., 2024), but often in constrained settings (Campbell et al., 2002; Silver et al., 2016; 2017) because many of the cognitive substrates necessary for human-level planning have been lacking. AI agents that can work robustly in the largely unconstrained settings in which humans operate remain a distant goal.</p>
<p>The advent of large language models (LLMs; OpenAI (2022; 2023); Touvron et al. (2023a;b); Jiang et al. (2023)) brings new light to this classic problem. A new generation of language agents (Su, 2023; Sumers et al., 2023; Xie et al., 2023) powered by LLMs has emerged, characterized by their capability of using language as a vehicle for thought and communication. These agents have shown interesting capabilities, such as tool use (Schick et al., 2023; Patil et al., 2023; Qin et al., 2024) and various forms of reasoning (Wei et al., 2022; Yao et al., 2022; Lewkowycz et al., 2022), potentially fulfilling the role of some of the cognitive substrates that were lacking in earlier AI agents. Researchers have therefore investigated their potential in an array of planning tasks ranging from classic planning settings like Blocksworld (Valmeekam et al., 2023) to embodied agents (Huang et al., 2022; Ichter et al., 2022; Song et al., 2023; Wang et al., 2023) and web agents (Deng et al., 2023; Zhou et al., 2024). However, the planning settings in existing work still largely follow the conventional setting that focuses on single-objective optimization with fixed ground truths. An agent is tasked with predicting from a pre-defined set of actions, just now made by an LLM-powered agent.</p>
<p>Are language agents capable of planning in more complex yet realistic settings, closer to those in which humans operate? To advance this investigation, we propose TravelPlanner, a new planning benchmark that focuses on a common</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1. Overview of TravelPlanner. Given a query, language agents are tasked with employing various search tools to gather information. Based on the collected information, language agents are expected to deliver a plan that not only satisfies the user's needs specified in the query but also adheres to commonsense constraints.</p>
<p>Real-world planning scenario—travel planning. This is a challenging, time-consuming task even for humans (but most people can do it successfully, with the right tools and enough time): 1) Planning a multi-day itinerary is inherently <em>long-horizon</em>, involving a large number of interdependent decisions on places, lodging, transportation, dining, etc. 2) Travel planning involves many <em>constraints</em>, ranging from explicit constraints such as budget and various user needs to implicit commonsense constraints, e.g., people cannot teletransport to another city without using some means of transportation. 3) Travel planning requires strong <em>agency</em> to proactively acquire necessary information using various tools (e.g., to search flights and restaurants) from the partially observable environment and deliberate over the collected information to further the planning while being mindful of all the explicit and implicit constraints. Planning tasks of such complexity are out of the reach of prior AI agents (Russell &amp; Norvig, 2010).</p>
<p>TravelPlanner provides a rich sandbox environment with around four million data entries crawled from the Internet that can be accessed via six tools. We also meticulously curate 1,225 diverse user queries (along with their reference plans), each imposing a different combination of constraints. A representative example is illustrated in Figure 1.</p>
<p>We comprehensively evaluate five LLMs, such as GPT-4 (OpenAI, 2023), Gemini (G Team et al., 2023), and Mixtral (Jiang et al., 2024), and four planning strategies, such as ReAct (Yao et al., 2022) and Reflexion (Shinn et al., 2023), on their capability of delivering complete plans and following constraints. The main findings are as follows:</p>
<ul>
<li>
<p>State-of-the-art LLMs cannot handle complex planning tasks like those in TravelPlanner. GPT-4 successfully produces a plan that meets all the constraints for a few tasks (0.6%), while all other LLMs fail to complete any tasks.</p>
</li>
<li>
<p>Existing planning strategies such as ReAct and Reflexion, which may be effective for simpler planning settings, are insufficient for the multi-constraint tasks in TravelPlanner. They often fail to convert their reasoning into the right actions correctly and keep track of global or multiple constraints. Language agents need more sophisticated planning strategies to approach human-level planning.</p>
</li>
<li>
<p>Further analyses reveal many common failure modes of existing language agents, such as argument errors in tool use, being trapped in dead loops, and hallucinations.</p>
</li>
</ul>
<p>Although most of our findings lean negatively toward the current language agents, we should note that the mere possibility for an artificial agent to tackle such a complex task is non-trivial progress in itself. TravelPlanner provides a challenging yet meaningful testbed for future agents to hill-climb toward human-level planning in complex settings.</p>
<p>Finally, a silver lining: while our well-trained human annotators averagely take 12 minutes to manually annotate a plan, a language agent can produce a plan in just 1–2 minutes automatically. Perhaps one day, language agents will become capable enough to help automate away many of such tedious tasks for us.</p>
<h2>2. Related Work</h2>
<h3>2.1. Large Language Model based Agents</h3>
<p>Empowered by large language models (LLMs), language agents have the capability to decompose complex tasks and arrive at solutions through a series of reasoned actions. Notable examples such as AutoGPT (AutoGPT, 2023), BabyAGI (Nakajima, 2023), and HuggingGPT (Shen et al., 2023) have illuminated the community with their impressive abilities. Current LLM-powered language agents, equipped with Memory, Tool-use, and Planning modules, have seen a substantial improvement in their general abilities (Weng, 2023). Memory in language agents refers to their ability to acquire and process information. It is divided into two types: long-term memory, which is the parametric memory inherent in LLMs, and short-term memory, also known as in-context learning (Brown et al., 2020) or working memory. Techniques like memory summarization (Chen et al., 2023; Zhou et al., 2023; Liang et al., 2023) and retrieval (Andreas, 2022; Park et al., 2023; Zhong et al., 2023) are widely employed to enhance the memory capabilities of language agents. Moreover, by interacting with external tools, language agents expand their potential capabilities significantly. This tool-augmentation paradigm has been validated as effective in previous work (Nakano et al., 2021; Lu et al., 2023; Ge et al., 2023; Xie et al., 2023). We further discuss the planning module in Section 2.2.</p>
<h3>2.2. Planning</h3>
<p>Planning, a hallmark of human intelligence, entails a sequence of actions that involve decomposing tasks, searching for solutions, and making final decisions (Hayes-Roth \&amp; Hayes-Roth, 1979; Grafman et al., 2004; Su, 2023). This skill is crucial for achieving human-level intelligence and has been widely studied in areas such as robotics (McDermott, 1992; Alterovitz et al., 2016) and transportation scheduling (Cross \&amp; Estrada, 1994; Pinedo, 2005). The emergence of language agents powered by LLMs has further intensified discussions around their planning capabilities (Liu et al., 2023a; Valmeekam et al., 2023). Previous research has demonstrated that language agents can effectively decompose tasks and engage in step-by-step reasoning, leading to significant improvements (Wei et al., 2022; Yuan et al., 2023; Zheng et al., 2024). Furthermore, to optimize solution searches in fewer steps, classical data structures like trees and graphs have been employed in prior studies (Yao et al., 2023; Besta et al., 2023), enhancing the planning capabilities of language agents. In addition, methods involving feedback from the environment (Yao et al., 2022; Shinn et al., 2023) have also been shown to be beneficial. However, while these planning abilities have shown promise in specific tasks, the effectiveness of these planning strategies in scenarios with multiple constraints remains uncertain.</p>
<h3>2.3. Evaluation of Language Agents</h3>
<p>Previous studies typically assess LLM-powered language agents in focused domains: arithmetic reasoning targeting correct solutions (Roy \&amp; Roth, 2015; Cobbe et al., 2021; Patel et al., 2021); tool-use evaluating agents’ proficiency in employing tools and reporting results (Li et al., 2023; Xu et al., 2023; Zhuang et al., 2023); and web navigation, testing agents’ ability to locate specific websites (Deng et al., 2023; Zhou et al., 2024; Liu et al., 2024). However, the complexity of the real-world implies that previous evaluation methods, which focus on single objective and fixed ground truths, may fall short of capturing the full scope of agents’ capabilities. To address this, we introduce TravelPlanner for comprehensive evaluations, assessing whether language agents can generate feasible solutions facing various objectives, referred to as constraints in this paper.</p>
<h2>3. TravelPlanner</h2>
<h3>3.1. Overview</h3>
<p>We introduce TravelPlanner, a benchmark crafted for evaluating language agents in tool-use and complex planning within multiple constraints. Grounding to travel planning, a real-world use-case that naturally includes diverse constraints such as user needs and commonsense constraints, TravelPlanner evaluates whether agents can develop flexible travel plans by collecting information via diverse tools and making decisions while satisfying the constraints.</p>
<p>TravelPlanner comprises 1,225 queries in total. The queries in TravelPlanner are divided into nine groups. This classification is based on two criteria: the duration of travel and the number of hard constraints. The dataset is divided into the training, validation, and test set. The training set includes 5 queries per group with human-annotated plans ( 45 pairs in total), the validation set includes 20 queries per group (180 in total), and the test set includes 1,000 queries. Detailed distributions are shown in Table A.1.</p>
<h3>3.2. Constraint Introduction</h3>
<p>In order to assess whether agents can perceive, understand, and satisfy various constraints to formulate a feasible plan, as outlined in Table 1, we include three types of constraints:</p>
<ul>
<li>Environment Constraints: The real-world is dynamic, necessitating agents to be adaptable. For instance, flights to a particular destination may be unavailable at certain times (e.g., no flights from Seattle to San Francisco in Figure 1), possibly because tickets are sold out. In such cases, the agent must dynamically seek an alternative, like changing the destination of the flight or the way of transportation. To simulate this, we introduce environment constraints within TravelPlanner to test the adaptability of agents in planning.</li>
</ul>
<p>Table 1. Constraint description. The environment constraints are manifested through the feedback received from the environment, assessing whether the language agent can adjust its plan appropriately. The commonsense constraints and hard constraints are evaluated based on how well the language agent’s plan aligns with these specific criteria.</p>
<table>
<thead>
<tr>
<th>Constraint</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>Environment Constraint</td>
<td></td>
</tr>
<tr>
<td>Unavailable Transportation</td>
<td>There is no available flight or driving information between the two cities.</td>
</tr>
<tr>
<td>Unavailable Attractions</td>
<td>There is no available attraction information in the queried city.</td>
</tr>
<tr>
<td>Commonsense Constraint</td>
<td></td>
</tr>
<tr>
<td>Within Sandbox</td>
<td>All information in the plan must be within the closed sandbox; otherwise, it will be considered a hallucination.</td>
</tr>
<tr>
<td>Complete Information</td>
<td>No key information should be left out of the plan, such as the lack of accommodation during travel.</td>
</tr>
<tr>
<td>Within Current City</td>
<td>All scheduled activities for the day must be located within that day’s city(s).</td>
</tr>
<tr>
<td>Reasonable City Route</td>
<td>Changes in cities during the trip must be reasonable.</td>
</tr>
<tr>
<td>Diverse Restaurants</td>
<td>Restaurant choices should not be repeated throughout the trip.</td>
</tr>
<tr>
<td>Diverse Attractions</td>
<td>Attraction choices should not be repeated throughout the trip.</td>
</tr>
<tr>
<td>Non-conf. Transportation</td>
<td>Transportation choices within the trip must be reasonable. For example, having both “self-driving” and “flight” would be considered a conflict.</td>
</tr>
<tr>
<td>Minimum Nights Stay</td>
<td>The number of consecutive days spent in a specific accommodation during the trip must meet the corresponding required minimum number of nights’ stay.</td>
</tr>
<tr>
<td>Hard Constraint</td>
<td></td>
</tr>
<tr>
<td>Budget</td>
<td>The total budget of the trip.</td>
</tr>
<tr>
<td>Room Rule</td>
<td>Room rules include “No parties”, “No smoking”, “No children under 10”, “No pets”, and “No visitors”.</td>
</tr>
<tr>
<td>Room Type</td>
<td>Room types include “Entire Room”, “Private Room”, “Shared Room”, and “No Shared Room”.</td>
</tr>
<tr>
<td>Cuisine</td>
<td>Cuisines include “Chinese”, “American”, “Italian”, “Mexican”, “Indian”, “Mediterranean”, and “French”.</td>
</tr>
<tr>
<td>Transportation</td>
<td>Transportation options include “No flight” and “No self-driving”.</td>
</tr>
</tbody>
</table>
<p>Table 2. The number of data entries in the database.</p>
<table>
<thead>
<tr>
<th>Tool</th>
<th>Data Entries (#)</th>
</tr>
</thead>
<tbody>
<tr>
<td>CitySearch</td>
<td>312</td>
</tr>
<tr>
<td>FlightSearch</td>
<td>3,827,361</td>
</tr>
<tr>
<td>DistanceMatrix</td>
<td>17,603</td>
</tr>
<tr>
<td>RestaurantSearch</td>
<td>9,552</td>
</tr>
<tr>
<td>AttractionSearch</td>
<td>5,303</td>
</tr>
<tr>
<td>AccommodationSearch</td>
<td>5,064</td>
</tr>
</tbody>
</table>
<ul>
<li>
<p>Commonsense Constraints: Agents, functioning in real-world and serving humans, should consider commonsense when designing plans. For instance, repeatedly visiting the same attraction is not typical. To evaluate agents’ understanding and utilization of commonsense during planning, we include the commonsense constraint in TravelPlanner.</p>
</li>
<li>
<p>Hard Constraints: A crucial ability for agents is to effectively satisfy personalized user needs. To evaluate this, TravelPlanner incorporates various user needs, such as budget constraints. These user needs are termed hard constraints. The hard constraint measures the agent’s generalization ability with regard to different user needs.</p>
</li>
</ul>
<h3>3.3. Benchmark Construction Pipeline</h3>
<p>This section outlines the construction pipeline of TravelPlanner, which involves the following steps: 1) Environment and evaluation setup. 2) Diverse travel query design. 3) Reference plan annotation. 4) Quality check.</p>
<p>Environment Setting. In TravelPlanner, we create a static and closed sandbox environment for consistent and unbiased evaluations. This setup ensures that all agents access the same unchanging information from our static databases, avoiding the variability and potential biases introduced by dynamic data. To offer various travel options that align with the real-world, we ensure the database for each tool in TravelPlanner contains rich information. The database size of each tool is listed in Table 2. For more tool details, please refer to Appendix A.2 and A.3. Additionally, agents are instructed to use the “NotebookWrite” tool to record necessary information for planning. This tool is integrated to evaluate agents’ working memory management and prevents maximum token limit caused by context accumulation.</p>
<p>Query Construction. To create diverse queries for TravelPlanner, we begin with several fundamental elements, including departure city, destination, and specific date range, randomly chosen to form the skeleton of each query. Subsequently, we adjust the duration of travel and the number of hard constraints to create different levels of complexity.</p>
<p>The duration of the travel—3, 5, or 7 days—determines the number of cities included in the plan. Specifically, 3-day plans focus on one city, while 5 days and 7 days involve visiting one randomly chosen state, with trips to 2 cities for the 5-day plans and 3 cities for the 7-day plans, respectively. A greater number of days requires more frequent tool usage by language agents, thus increasing the difficulty of managing the long-horizon aspect of planning. The uncertain destination challenges agents to decide on multiple cities, where they must consider factors such as inter-city connectivity.</p>
<p>Furthermore, we introduce diverse user needs as hard constraints to add further complexity and realism. The difficulty levels are categorized as follows:</p>
<ul>
<li>Easy: Queries at this level are primarily budgetconstrained for a single person. The initial budget for each query is determined using a set of crafted heuristic rules.</li>
<li>Medium: In addition to budget constraints, medium queries introduce an additional hard constraint, randomly selected from a constraint pool including cuisine type, room type, and room rule. Furthermore, the number of people varies between 2 and 8 , which influences the calculation of costs for both transportation and accommodation.</li>
<li>Hard: Hard queries include additional transportation preference into the constraint pool, along with all the constraints in medium queries. Each hard query contains three hard constraints randomly selected from the constraint pool.</li>
</ul>
<p>This method ensures the diversity of queries. Minor changes in these elements can lead to significantly different plans. Finally, based on elements, we utilize GPT-4 (OpenAI, 2023) to generate natural language queries.</p>
<p>Human Annotation. To ensure every query has at least one feasible plan, we invite 20 graduate students to meticulously annotate plans for synthesized queries. One plan is deemed eligible only if it meets all the constraints outlined in our evaluation script, which is detailed in Section 3.4. This rigorous process resulted in the creation of 1,225 validated query-plan pairs. We pay annotators an average of $\$ 0.80$ for each plan they annotate.</p>
<p>Quality Control. To ensure the quality of each natural language query and its corresponding annotated plan, the authors performed a detailed review of every query and plan, rectifying any errors found. Additionally, to ensure the challenges, we re-calibrate each query's budget using the costs from corresponding human-annotated plans. This approach replaces the initial heuristic-generated budgets, which might be too high, thus reducing the number of feasible plans. Through multiple stages of human verification, we ensure the high quality of each query in TravelPlanner and the presence of at least one feasible solution.</p>
<h3>3.4. Evaluation</h3>
<p>To ensure a comprehensive evaluation of the plans offered by agents, we assess them from multiple dimensions. Specifically, we first extract key components ${ }^{1}$, including transportation, restaurants, attractions, and accommodations, which are initially presented as natural language. These components are then organized into a formally structured plan,</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>which will be evaluated automatically through pre-defined scripts. The evaluation criteria include the following:</p>
<ul>
<li>Delivery Rate: This metric assesses whether agents can successfully deliver a final plan within a limited number of steps. Falling into dead loops, experiencing numerous failed attempts, or reaching the maximum number of steps ( 30 steps in our experimental setting) will result in failure.</li>
<li>Commonsense Constraint Pass Rate: Comprising eight commonsense dimensions, this metric evaluates whether a language agent can incorporate commonsense into their plan without explicit instructions.</li>
<li>Hard Constraint Pass Rate: This metric measures whether a plan satisfies all explicitly given hard constraints in the query, which aims to test the agents' ability to adapt their plans to diverse user needs.</li>
<li>Final Pass Rate: This metric represents the proportion of feasible plans that meet all aforementioned constraints among all tested plans. It serves as an indicator of agents' proficiency in producing plans that meet a practical standard.</li>
</ul>
<p>We do not separately assess environment constraints since their impact is inherently reflected in the "Within Sandbox" and "Complete Information" metrics. For instance, when cities lack transportation or attractions, agents typically resort to hallucination or opt not to provide an answer, reflecting the impact of environment constraints.</p>
<p>For the Commonsense Constraint Pass Rate and Hard Constraint Pass Rate, we utilize two evaluation strategies: micro and macro. The micro strategy calculates the ratio of passed constraints to the total number of constraints. The Micro Pass Rate is defined as:</p>
<p>$$
\text { Micro Pass Rate }=\frac{\sum_{p \in P} \sum_{c \in C_{p}} \mathbb{1}<em P="P" _in="\in" p="p">{\text {passed }(c, p)}}{\sum</em>
$$}\left|C_{p}\right|</p>
<p>where $P$ represents the set of all plans being evaluated, $C_{p}$ denotes the set of constraints applicable to a specific plan $p$ in $P$, and passed $(X, Y)$ is a function determining whether $Y$ meets constraints $X$.</p>
<p>The macro strategy calculates the ratio of plans that pass all commonsense or hard constraints among all tested plans. We define the Macro Pass Rate as:</p>
<p>$$
\text { Macro Pass Rate }=\frac{\sum_{p \in P} \mathbb{1}<em p="p">{\text {passed }\left(C</em>
$$}, p\right)}}{|P|</p>
<p>These two metrics evaluate an agent's capability of following individual constraints vs. all the constraints holistically.</p>
<h3>3.5. Sole-Planning Setting</h3>
<p>While TravelPlanner is designed to assess the overall abilities of agents in tool-use and planning (two-stage mode),</p>
<p>Table 3. Main results of different LLMs and planning strategies on the TravelPlanner validation and test set. The best results are marked in bold. When the collected information is insufficient, Gemini Pro tends to directly refuse to provide the plan. Interviews with annotators reveal that manually annotating a plan averagely takes around 12 minutes. However, language agents, such as GPT-3.5-Turbo, can accomplish this task in just 1 to 2 minutes, showcasing their efficiency.</p>
<table>
<thead>
<tr>
<th></th>
<th>Validation (#180)</th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th>Test (#1,000)</th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Delivery Rate</td>
<td>$\begin{gathered} \text { Commonsense } \ \text { Pass Rate } \end{gathered}$</td>
<td></td>
<td>Hard Constraint Pass Rate</td>
<td></td>
<td>Final Pass Rate</td>
<td>Delivery Rate</td>
<td>$\begin{gathered} \text { Commonsense } \ \text { Pass Rate } \end{gathered}$</td>
<td></td>
<td>Hard Constraint Pass Rate</td>
<td></td>
<td>Final Pass Rate</td>
</tr>
<tr>
<td></td>
<td></td>
<td>Micro</td>
<td>Macro</td>
<td>Micro</td>
<td>Macro</td>
<td></td>
<td></td>
<td>Micro</td>
<td>Macro</td>
<td>Micro</td>
<td>Macro</td>
<td></td>
</tr>
<tr>
<td>Greedy Search</td>
<td>100</td>
<td>74.4</td>
<td>0</td>
<td>60.8</td>
<td>37.8</td>
<td>0</td>
<td>100</td>
<td>72.0</td>
<td>0</td>
<td>52.4</td>
<td>31.8</td>
<td>0</td>
</tr>
<tr>
<td></td>
<td>Two-stage</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Mistral-7B-32K (Jiang et al., 2023)</td>
<td>8.9</td>
<td>5.9</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>7.0</td>
<td>4.8</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Gemini Pro (G Team et al., 2023)</td>
<td>28.9</td>
<td>18.9</td>
<td>0</td>
<td>0.5</td>
<td>0.6</td>
<td>0</td>
<td>39.1</td>
<td>24.9</td>
<td>0</td>
<td>0.6</td>
<td>0.1</td>
<td>0</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>GPT-3.5-Turbo (OpenAI, 2022)</td>
<td>86.7</td>
<td>54.0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>91.8</td>
<td>57.9</td>
<td>0</td>
<td>0.5</td>
<td>0.6</td>
<td>0</td>
</tr>
<tr>
<td>GPT-4-Turbo (OpenAI, 2023)</td>
<td>89.4</td>
<td>61.1</td>
<td>2.8</td>
<td>15.2</td>
<td>10.6</td>
<td>0.6</td>
<td>93.1</td>
<td>63.3</td>
<td>2.0</td>
<td>10.5</td>
<td>5.5</td>
<td>0.6</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Direct ${ }_{\text {GPT-3.5-Turbo }}$</td>
<td>100</td>
<td>60.2</td>
<td>4.4</td>
<td>11.0</td>
<td>2.8</td>
<td>0</td>
<td>100</td>
<td>59.5</td>
<td>2.7</td>
<td>9.5</td>
<td>4.4</td>
<td>0.6</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>ReAct ${ }_{\text {GPT-3.5-Turbo }}$</td>
<td>82.2</td>
<td>47.6</td>
<td>3.9</td>
<td>11.4</td>
<td>6.7</td>
<td>0.6</td>
<td>81.6</td>
<td>45.9</td>
<td>2.5</td>
<td>10.7</td>
<td>3.1</td>
<td>0.7</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Reflexion ${ }_{\text {GPT-3.5-Turbo }}$</td>
<td>93.9</td>
<td>53.8</td>
<td>2.8</td>
<td>11.0</td>
<td>2.8</td>
<td>0</td>
<td>92.1</td>
<td>52.1</td>
<td>2.2</td>
<td>9.9</td>
<td>3.8</td>
<td>0.6</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Direct $_{\text {Mistral-8x7B-MoE }}$</td>
<td>100</td>
<td>68.1</td>
<td>5.0</td>
<td>3.3</td>
<td>1.1</td>
<td>0</td>
<td>99.3</td>
<td>67.0</td>
<td>3.7</td>
<td>3.9</td>
<td>1.6</td>
<td>0.7</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Direct $_{\text {Gemini Pro }}$</td>
<td>93.9</td>
<td>65.0</td>
<td>8.3</td>
<td>9.3</td>
<td>4.4</td>
<td>0.6</td>
<td>93.7</td>
<td>64.7</td>
<td>7.9</td>
<td>10.6</td>
<td>4.7</td>
<td>2.1</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Direct $_{\text {GPT-4-Turbo }}$</td>
<td>100</td>
<td>80.4</td>
<td>17.2</td>
<td>47.1</td>
<td>22.2</td>
<td>4.4</td>
<td>100</td>
<td>80.6</td>
<td>15.2</td>
<td>44.3</td>
<td>23.1</td>
<td>4.4</td>
</tr>
</tbody>
</table>
<p>we also setup a simplified mode solely evaluating agents’ planning skills (sole-planning mode). In this setting, we utilize human-annotated plans to pre-determine the destination cities, and provide detailed and necessary information directly to agents, such as restaurants in the provided cities. This eliminates the need for tool calling as agents don’t need to collect information from scratch via tools anymore.</p>
<h2>4. Experiments</h2>
<p>We evaluate the performance of various LLMs and planning strategies on TravelPlanner. In the two-stage mode, we use the ReAct <em>(Yao et al., 2022)</em> framework for information collection, which is recognized for its effective iteration with tools <em>(Zhuang et al., 2023)</em> while varying the foundation LLMs. This approach allows us to assess how different LLMs perform under a uniform tool-use framework. The agents are required to give the plan directly based on the information collected by themselves, without employing any other planning strategies. In the sole-planning mode, our evaluation goes beyond varying LLMs to include different planning strategies. This aims to assess if the strategies proven effective in other planning benchmarks maintain their efficacy in TravelPlanner. All experiments are conducted in a zero-shot setting.</p>
<h3>4.1. Baselines</h3>
<p>Greedy Search. To evaluate the effectiveness of traditional rule-based strategies within TravelPlanner, we include greedy search as a baseline and set cost as the optimization objective. Please refer to Appendix B. 1 for more details.</p>
<p>LLMs. Due to the long context window requirement of ReAct and the massive information as text, we limit our consideration to LLMs capable of handling inputs exceeding 8 K in length. As a result, our selection includes three closedsource LLMs: GPT-3.5-Turbo <em>(OpenAI, 2022)</em>, GPT-4Turbo <em>(OpenAI, 2023)</em>, and Gemini Pro <em>(G Team et al., 2023)</em>, as well as two open-source LLMs: Mistral-7B32K <em>(Jiang et al., 2023)</em> and Mixtral-8x7B-MoE <em>(Jiang et al., 2024)</em>. For all these models, we adopt the official instruction formats whenever available.</p>
<p>Planning Strategies. To explore the effectiveness of current planning strategies, we evaluate four representative ones: Direct, ZS-CoT <em>(Wei et al., 2022)</em>, ReAct <em>(Yao et al., 2022)</em>, and Reflexion <em>(Shimi et al., 2023)</em>. For the implementation details, please refer to Appendix B.1. We do not include ToT <em>(Yao et al., 2023)</em> and GoT <em>(Besta et al., 2023)</em> because they require extensive exploration of the search space, prohibitively costly for problems as complex as TravelPlanner. Also, given their performance close to ReAct in complex tasks <em>(Zhuang et al., 2024)</em>, the potential benefits of these methods may be limited.</p>
<h3>4.2. Main Results</h3>
<p>In this section, we discuss the performance of various LLMs and planning strategies on TravelPlanner (Table 3). We have the following observations:</p>
<p>TravelPlanner poses a significant challenge. In the twostage mode, GPT-4-Turbo with ReAct achieves only $0.6 \%$ in the final pass rate, and none of the other LLMs can pass any</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2. Tool-use error distribution on the test set. An early stop will be triggered if the agent either makes three consecutive failed attempts or repetitive actions, indicating a dead loop.
of the tasks. Even given all the necessary information in the sole-planning mode, existing planning strategies like ReAct and Reflexion still struggle with planning in TravelPlanner, even though they have shown their effectiveness in more conventional planning tasks. It is noteworthy that the bestperforming agent still falls short on hard constraints even when compared to greedy search. This poor performance underlines the difficulty of TravelPlanner and shows that current agents still struggle with complex planning.</p>
<p>Agents show a substantial gap between the two modes. The comparison of the two modes reveals the agents' struggles in fiddling with both information collection and planning. Across all metrics, the scores of any model in the two-stage mode are lower than those in the sole-planning mode, with the largest gap reaching over $30 \%$. Similar to humans, language agents also seem to have a limited "cognitive capacity" and their performance deteriorates when multitasking. We provide a further analysis in Section 5.2.</p>
<p>Agents struggle in obtaining a high macro pass rate. While some agents achieve high micro scores, their macro scores remain low. This pattern shows that although agents manage to satisfy some constraints, they often overlook some other constraints in the meantime. Consequently, this indicates the current agents fail to consider multiple constraints holistically, a critical requirement for navigating the complex tasks in TravelPlanner.</p>
<p>In summary, TravelPlanner poses a great challenge to current agents. The SoTA LLMs and planning strategies, which often show equal or superior to human-level performance on many traditional tasks, are still far from sufficient for complex planning tasks that humans are capable of. TravelPlanner provides a challenging yet meaningful benchmark for the development of more capable language agents.</p>
<h2>5. In-Depth Analysis</h2>
<h3>5.1. Tool-Use Error Analysis</h3>
<p>As shown in Table 3, even based on GPT-4-Turbo, agents still make mistakes in the process of information collection and thus fail to deliver a plan. This problem is more severe in Gemini Pro and Mixtral. To delve into the underlying causes, we categorize all error types in Figure 2. We find:</p>
<p>Table 4. Constraint pass rate of GPT-4-Turbo on test set. The results of the sole-planning mode are based on the Direct strategy.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Constraint Type</th>
<th style="text-align: center;">Two-stage</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Sole-planning</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Easy</td>
<td style="text-align: center;">Medium</td>
<td style="text-align: center;">Hard</td>
<td style="text-align: center;">Easy</td>
<td style="text-align: center;">Medium</td>
<td style="text-align: center;">Hard</td>
</tr>
<tr>
<td style="text-align: center;">Commonsense Constraint</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Within Sandbox</td>
<td style="text-align: center;">37.4</td>
<td style="text-align: center;">31.2</td>
<td style="text-align: center;">33.9</td>
<td style="text-align: center;">76.4</td>
<td style="text-align: center;">71.5</td>
<td style="text-align: center;">79.3</td>
</tr>
<tr>
<td style="text-align: center;">Complete Information</td>
<td style="text-align: center;">53.4</td>
<td style="text-align: center;">52.9</td>
<td style="text-align: center;">58.0</td>
<td style="text-align: center;">94.5</td>
<td style="text-align: center;">96.4</td>
<td style="text-align: center;">96.2</td>
</tr>
<tr>
<td style="text-align: center;">Within Current City</td>
<td style="text-align: center;">69.3</td>
<td style="text-align: center;">67.3</td>
<td style="text-align: center;">68.3</td>
<td style="text-align: center;">89.1</td>
<td style="text-align: center;">80.8</td>
<td style="text-align: center;">82.4</td>
</tr>
<tr>
<td style="text-align: center;">Reasonable City Route</td>
<td style="text-align: center;">44.5</td>
<td style="text-align: center;">45.6</td>
<td style="text-align: center;">54.9</td>
<td style="text-align: center;">99.4</td>
<td style="text-align: center;">99.7</td>
<td style="text-align: center;">99.1</td>
</tr>
<tr>
<td style="text-align: center;">Diverse Restaurants</td>
<td style="text-align: center;">85.1</td>
<td style="text-align: center;">81.4</td>
<td style="text-align: center;">86.8</td>
<td style="text-align: center;">91.1</td>
<td style="text-align: center;">89.8</td>
<td style="text-align: center;">87.8</td>
</tr>
<tr>
<td style="text-align: center;">Diverse Attractions</td>
<td style="text-align: center;">94.3</td>
<td style="text-align: center;">90.4</td>
<td style="text-align: center;">94.0</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">100.0</td>
</tr>
<tr>
<td style="text-align: center;">Non-conf. Transportation</td>
<td style="text-align: center;">70.1</td>
<td style="text-align: center;">73.3</td>
<td style="text-align: center;">83.1</td>
<td style="text-align: center;">60.1</td>
<td style="text-align: center;">56.5</td>
<td style="text-align: center;">87.5</td>
</tr>
<tr>
<td style="text-align: center;">Minimum Nights Stay</td>
<td style="text-align: center;">46.8</td>
<td style="text-align: center;">46.2</td>
<td style="text-align: center;">51.1</td>
<td style="text-align: center;">37.4</td>
<td style="text-align: center;">28.8</td>
<td style="text-align: center;">30.1</td>
</tr>
<tr>
<td style="text-align: center;">Hard Constraint</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Budget</td>
<td style="text-align: center;">10.1</td>
<td style="text-align: center;">8.4</td>
<td style="text-align: center;">4.4</td>
<td style="text-align: center;">37.4</td>
<td style="text-align: center;">35.1</td>
<td style="text-align: center;">25.1</td>
</tr>
<tr>
<td style="text-align: center;">Room Rule</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">5.6</td>
<td style="text-align: center;">11.3</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">31.5</td>
<td style="text-align: center;">43.6</td>
</tr>
<tr>
<td style="text-align: center;">Cuisine</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">10.8</td>
<td style="text-align: center;">11.4</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">57.5</td>
<td style="text-align: center;">46.7</td>
</tr>
<tr>
<td style="text-align: center;">Room Type</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">12.4</td>
<td style="text-align: center;">13.8</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">45.7</td>
<td style="text-align: center;">56.7</td>
</tr>
<tr>
<td style="text-align: center;">Transportation</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">18.6</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">77.5</td>
</tr>
<tr>
<td style="text-align: center;">Final</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Final Pass Rate</td>
<td style="text-align: center;">1.1</td>
<td style="text-align: center;">0.3</td>
<td style="text-align: center;">0.3</td>
<td style="text-align: center;">8.0</td>
<td style="text-align: center;">2.7</td>
<td style="text-align: center;">2.2</td>
</tr>
</tbody>
</table>
<p>Table 5. Comparison of the numbers of different tool uses between agent (GPT-4-Turbo) and reference. The results of agent are based on the number of entries written into the "Notebook".</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Average</th>
<th style="text-align: center;">Agent</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Reference</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">3-day 5-day 7-day 3-day 5-day 7-day</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">FlightSearch</td>
<td style="text-align: center;">0.7</td>
<td style="text-align: center;">1.2</td>
<td style="text-align: center;">0.8</td>
<td style="text-align: center;">2.0</td>
<td style="text-align: center;">3.0</td>
<td style="text-align: center;">4.0</td>
</tr>
<tr>
<td style="text-align: center;">DistanceMatrix</td>
<td style="text-align: center;">0.3</td>
<td style="text-align: center;">0.6</td>
<td style="text-align: center;">1.2</td>
<td style="text-align: center;">2.0</td>
<td style="text-align: center;">3.0</td>
<td style="text-align: center;">4.0</td>
</tr>
<tr>
<td style="text-align: center;">RestaurantSearch</td>
<td style="text-align: center;">0.9</td>
<td style="text-align: center;">1.5</td>
<td style="text-align: center;">2.4</td>
<td style="text-align: center;">1.0</td>
<td style="text-align: center;">2.0</td>
<td style="text-align: center;">3.0</td>
</tr>
<tr>
<td style="text-align: center;">AttractionSearch</td>
<td style="text-align: center;">0.9</td>
<td style="text-align: center;">1.7</td>
<td style="text-align: center;">2.4</td>
<td style="text-align: center;">1.0</td>
<td style="text-align: center;">2.0</td>
<td style="text-align: center;">3.0</td>
</tr>
<tr>
<td style="text-align: center;">AccommodationSearch</td>
<td style="text-align: center;">0.9</td>
<td style="text-align: center;">1.6</td>
<td style="text-align: center;">2.5</td>
<td style="text-align: center;">1.0</td>
<td style="text-align: center;">2.0</td>
<td style="text-align: center;">3.0</td>
</tr>
</tbody>
</table>
<p>1) Agents incorrectly use tools. Except for GPT-4-Turbo, other LLMs-based agents all have argument error problems to varying degrees. It sheds light that the use of simple tools still poses a significant challenge for agents. 2) Agents trap in dead loops. Even with GPT-4-Turbo, invalid actions and repetitive action loops contribute to $37.3 \%$ and $6.0 \%$ of errors, respectively. Despite receiving feedback that actions are invalid or yield null results, agents persistently repeat these actions. This suggests that agents fail to dynamically adjust their plans based on environment feedback.</p>
<h3>5.2. Planning Error Analysis</h3>
<p>We detail each constraint pass rate in Table 4, from which we have the following observations:</p>
<p>The number of hard constraints affects the performance of agents. Agents consistently exhibit pass rates below $10 \%$ across all levels of difficulty, and this performance deteriorates further as more constraints are introduced. This trend highlights that current agents struggle with multi-constraint tasks, a key aspect of TravelPlanner.</p>
<p>Comprehensive information collection is essential for agents to plan effectively. In the sole-planning mode, agent performance improves over the two-stage mode. To investi-</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Tool-use</th>
<th style="text-align: center;">Direct</th>
<th style="text-align: center;">Reflexion</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Query: Please plan a travel itinerary for me. I'm departing from Cincinnati and heading to Norfolk for three days. The dates of travel are from March 10th to March 12th, 2022. I have a budget of $\$ 1,400$ for this trip.</td>
<td style="text-align: center;">Query: Please create a travel itinerary for a solo traveler departing from Jacksonville and heading to Los Angeles for a period of 3 days, from March 25th to March 27th, 2022. The budget for this trip is now set at $\$ 2,400$.</td>
<td style="text-align: center;">Query: Can you help construct a travel plan that begins in Philadelphia and includes visits to 3 cities in Virginia? The trip duration is for 7 days, from March 15th to March 21st, 2022, with a total budget of $\$ 1,800$.</td>
</tr>
<tr>
<td style="text-align: center;">Trajectory: <br> Action 1: FlightSearch[Cincinnati, Norfolk, 2023-03-10] <br> Observation 1: There is no flight from Cincinnati to Norfolk on 2023-03-10 <br> Action 2: FlightSearch[Cincinnati, Norfolk, 2023-03-09] <br> Action 3: FlightSearch[Cincinnati, Norfolk, 2023-03-11] <br> Action 4: FlightSearch[Cincinnati, Richmond, 2023-03-10] <br> Action 8: FlightSearch[Cincinnati, Charlotte, 2023-03-10] <br> Action 9: I'm sorry, but as an AI language model, I am unable to execute actions or observe their outcomes.</td>
<td style="text-align: center;">Plan: <br> Day 1: <br> transportation: Flight Number: F3765485, from Jacksonville to Los Angeles <br> accommodation: Midtown East 2 Bed/2 Bath Elevator Building Apt (7-day minimum night) <br> Day 2: <br> accommodation: Midtown East 2 Bed/2 Bath Elevator Building Apt (7-day minimum night) <br> Day 3: <br> transportation: Flight Number: F3765485, from Los Angeles to Jacksonville</td>
<td style="text-align: center;">Trajectory: <br> Observation 3: The cost of your current plan is $\mathbf{3 1 6 . 0}$ dollars. <br> Thought 4: I need to select a more budget-friendly accommodation to keep the overall trip within the $\$ 1,800$ budget. <br> Action 4: CostEnquiry[("transportation": "-", "breakfast": "-", "attraction": "Virginia Museum", "lunch": "Guang Zhou Chinese Restaurant, Richmond", "dinner": "Sandpiper Restaurant \&amp; Lounge, Richmond", "accommodation": "2 bd 2 bathroom Apartment in Upper East Side, Richmond")] Observation 4: The cost of your plan is 389.0 dollars.</td>
</tr>
</tbody>
</table>
<p>Figure 3. Case studies of failures. Agents fail to finalize plans due to repeated errors like incorrect dates, confusion with information details leading to hallucinatory answers, and disconnects between reasoning and actions. All cases are gained from GPT-4-Turbo based agents. For details of GPT-4-Turbo with the Reflexion strategy, please refer to Appendix B.2. We provide more cases in Appendix C.2.
gate this, Table 5 shows agents in two-stage mode use tools less effectively compared to the reference plans. This comparison indicates that agents often fail to finish completed information collection. Consequently, they either generate made-up information or omit specific details. This leads to low pass rates for the "Within Sandbox" and "Complete Information" constraints. Additionally, this discrepancy becomes more pronounced with an increase in the duration of travel. This emphasizes the need for agents to improve their capabilities in long-horizon tasks.</p>
<p>Agents struggle with global planning scenarios. Global constraints "Minimum Nights Stay" and "Budget" demand a holistic approach to planning, necessitating that agents not only assess their current decisions but also anticipate future implications. Current LLMs' auto-regressive nature limits them to independently obtain outcomes from multiple future branches. This highlights the necessity and urgent need of new strategies, such as backtracking for adjusting or employing heuristic methods for forward-looking planning.</p>
<h3>5.3. Case Studies</h3>
<p>To investigate the drawbacks of current agents in-depth, we provide several failure cases in Figure 3. We conclude with the following features:</p>
<p>Agents fail to complete a plan due to the inability to rectify persistent errors. In tool-use scenarios, agents often fail to deliver a plan even when all preceding steps are executed correctly. Further investigation reveals that this issue often stems from incorrect date inputs. As shown in the left part of Figure 3, despite correct execution, agents repeatedly use incorrect dates. This leads to null results, as the data in the TravelPlanner sandbox is based on 2022. Such re-
peated failures eventually cause the agents to stop planning. This indicates a significant limitation: current agents cannot self-correct their initial and incorrect assumptions.</p>
<p>Agents produce hallucinatory answers due to information confusion. To understand why agents provide hallucinatory answers even when supplied with sufficient information in the sole-planning mode, we conduct a detailed analysis. We observe a tendency for agents to confuse one piece of information with another. As shown in the middle part of Figure 3, agents mistakenly use the same flight number for both departure and return flights. Such errors result in hallucinations, as the information provided in the plan does not align with the data in the sandbox. This suggests that agents might be lost when faced with mass information, known as "Lost in the Middle" (Liu et al., 2023b).</p>
<p>Agents struggle to align their actions with their reasoning. To understand the reasons behind the lower delivery rate of Reflexion (Shinn et al., 2023), we examine specific examples. As illustrated in the right part of Figure 3, we observe a discrepancy between what agents think and what they do. Despite recognizing the necessity to minimize costs, they tend to randomly select items, some of which may be more expensive. This discrepancy demonstrates that agents struggle to synchronize their actions with their analytical reasoning, severely impeding their delivery rate.</p>
<h2>6. Conclusion</h2>
<p>We introduce TravelPlanner, a benchmark grounded in realworld scenarios, designed to assess the multi-constraint planning and tool-use abilities of current language agents. Our benchmark presents a significant challenge: even the most advanced language agent frameworks only achieve</p>
<p>a mere $0.6 \%$ score in the final pass rate. Further analysis shows that these agents are unable to take all constraints into consideration to deliver feasible plans.</p>
<p>TravelPlanner's intricate logic and general applicability stand as vital components in the progressive development of language agents, thus contributing to the broader quest for AI abilities. We envision TravelPlanner as a catalyst for future research, aiming to enhance agents' performance in increasingly complex scenarios, hill-climbing towards human-level cognitive capabilities.</p>
<h2>7. Impact Statements</h2>
<p>TravelPlanner aims to provide an effective benchmark for complex planning in future research. Some of the data in the TravelPlanner environment is derived from publicly available data on the Internet, and the content involved does not represent the authors' viewpoints. We realize that everyone's definition of commonsense may be different. Our current evaluation criteria are based on the authors' consensus, and we encourage additional discussions to enrich our commonsense dimension, aiming for a more thorough evaluation. We will release our evaluation scripts to foster innovation and aid the development of new methods. We encourage the use of evaluation feedback in training set, such as implementing reinforcement learning techniques, to enhance learning. However, we strictly prohibit any form of cheating in the validation and test sets to uphold the fairness and reliability of the benchmark's evaluation process.</p>
<h2>References</h2>
<p>Alterovitz, R., Koenig, S., and Likhachev, M. Robot planning in the real world: Research challenges and opportunities. Ai Magazine, 2016.</p>
<p>Andreas, J. Language models as agent models. In Findings of EMNLP, 2022.</p>
<p>AutoGPT. Autogpt, 2023. URL https://github.com/ Significant-Gravitas/AutoGPT.</p>
<p>Besta, M., Blach, N., Kubicek, A., Gerstenberger, R., Gianinazzi, L., Gajda, J., Lehmann, T., Podstawski, M., Niewiadomski, H., Nyczyk, P., et al. Graph of thoughts: Solving elaborate problems with large language models. arXiv preprint arXiv:2308.09687, 2023.</p>
<p>Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al. Language models are few-shot learners. In Proceedings of NeurIPS, 2020.</p>
<p>Campbell, M., Hoane Jr, A. J., and Hsu, F.-h. Deep blue. Artificial intelligence, 2002.</p>
<p>Chen, H., Pasunuru, R., Weston, J., and Celikyilmaz, A. Walking down the memory maze: Beyond context limit through interactive reading. arXiv preprint arXiv:2310.05029, 2023.</p>
<p>Cobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H., Kaiser, L., Plappert, M., Tworek, J., Hilton, J., Nakano, R., et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021.</p>
<p>Cross, S. and Estrada, R. Dart: an example of accelerated evolutionary development. In Proceedings of Workshop on RSP, 1994.</p>
<p>Deng, X., Gu, Y., Zheng, B., Chen, S., Stevens, S., Wang, B., Sun, H., and Su, Y. Mind2web: Towards a generalist agent for the web. In Proceedings of NeurIPS, 2023.</p>
<p>G Team, G., Anil, R., Borgeaud, S., Wu, Y., Alayrac, J.-B., Yu, J., Soricut, R., Schalkwyk, J., Dai, A. M., Hauth, A., et al. Gemini: a family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023.</p>
<p>Ge, Y., Hua, W., Ji, J., Tan, J., Xu, S., and Zhang, Y. Openagi: When llm meets domain experts. In Proceedings of NeurIPS, 2023.</p>
<p>Georgievski, I. and Aiello, M. Htn planning: Overview, comparison, and beyond. Artificial Intelligence, 2015.</p>
<p>Grafman, J., Spector, L., and Rattermann, M. J. Planning and the brain. In The cognitive psychology of planning. 2004.</p>
<p>Hayes-Roth, B. and Hayes-Roth, F. A cognitive model of planning. Cognitive science, 1979.</p>
<p>Ho, M. K., Saxe, R., and Cushman, F. Planning with theory of mind. Trends in Cognitive Sciences, 2022.</p>
<p>Huang, W., Abbeel, P., Pathak, D., and Mordatch, I. Language models as zero-shot planners: Extracting actionable knowledge for embodied agents. In Proceedings of ICML, 2022.</p>
<p>Ichter, B., Brohan, A., Chebotar, Y., Finn, C., Hausman, K., Herzog, A., Ho, D., Ibarz, J., Irpan, A., Jang, E., Julian, R., Kalashnikov, D., Levine, S., Lu, Y., Parada, C., Rao, K., Sermanet, P., Toshev, A., Vanhoucke, V., Xia, F., Xiao, T., Xu, P., Yan, M., Brown, N., Ahn, M., Cortes, O., Sievers, N., Tan, C., Xu, S., Reyes, D., Rettinghouse, J., Quiambao, J., Pastor, P., Luu, L., Lee, K., Kuang, Y., Jesmonth, S., Joshi, N. J., Jeffrey, K., Ruano, R. J., Hsu, J., Gopalakrishnan, K., David, B., Zeng, A., and Fu, C. K. Do as I can, not as I say: Grounding language in robotic affordances. In Proceedings of CoRL, 2022.</p>
<p>Jiang, A. Q., Sablayrolles, A., Mensch, A., Bamford, C., Chaplot, D. S., Casas, D. d. l., Bressand, F., Lengyel, G., Lample, G., Saulnier, L., et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023.</p>
<p>Jiang, A. Q., Sablayrolles, A., Roux, A., Mensch, A., Savary, B., Bamford, C., Chaplot, D. S., de las Casas, D., Hanna, E. B., Bressand, F., Lengyel, G., Bour, G., Lample, G., Lavaud, L. R., Saulnier, L., Lachaux, M.A., Stock, P., Subramanian, S., Yang, S., Antoniak, S., Scao, T. L., Gervet, T., Lavril, T., Wang, T., Lacroix, T., and Sayed, W. E. Mixtral of experts. arXiv preprint arXiv:2401.04088, 2024.</p>
<p>Karpas, E. and Magazzeni, D. Automated planning for robotics. Annual Review of Control, Robotics, and Autonomous Systems, 2020.</p>
<p>Lewkowycz, A., Andreassen, A., Dohan, D., Dyer, E., Michalewski, H., Ramasesh, V. V., Slone, A., Anil, C., Schlag, I., Gutman-Solo, T., Wu, Y., Neyshabur, B., GurAri, G., and Misra, V. Solving quantitative reasoning problems with language models. In Proceedings of NeurIPS, 2022.</p>
<p>Li, M., Zhao, Y., Yu, B., Song, F., Li, H., Yu, H., Li, Z., Huang, F., and Li, Y. Api-bank: A comprehensive benchmark for tool-augmented llms. In Proceedings of EMNLP, 2023.</p>
<p>Liang, X., Wang, B., Huang, H., Wu, S., Wu, P., Lu, L., Ma, Z., and Li, Z. Unleashing infinite-length input capacity for large-scale language models with self-controlled memory system. arXiv preprint arXiv:2304.13343, 2023.</p>
<p>Lin, J., Tomlin, N., Andreas, J., and Eisner, J. Decisionoriented dialogue for human-ai collaboration. arXiv preprint arXiv:2305.20076, 2023.</p>
<p>Liu, B., Jiang, Y., Zhang, X., Liu, Q., Zhang, S., Biswas, J., and Stone, P. Llm+ p: Empowering large language models with optimal planning proficiency. arXiv preprint arXiv:2304.11477, 2023a.</p>
<p>Liu, N. F., Lin, K., Hewitt, J., Paranjape, A., Bevilacqua, M., Petroni, F., and Liang, P. Lost in the middle: How language models use long contexts. arXiv preprint arXiv:2307.03172, 2023b.</p>
<p>Liu, X., Yu, H., Zhang, H., Xu, Y., Lei, X., Lai, H., Gu, Y., Ding, H., Men, K., Yang, K., et al. Agentbench: Evaluating llms as agents. In Proceedings of ICLR, 2024.</p>
<p>Lu, P., Peng, B., Cheng, H., Galley, M., Chang, K.-W., Wu, Y. N., Zhu, S.-C., and Gao, J. Chameleon: Plug-and-play compositional reasoning with large language models. In Proceedings of NeurIPS, 2023.</p>
<p>Mattar, M. G. and Lengyel, M. Planning in the brain. Neuron, 2022.</p>
<p>McDermott, D. Robot planning. AI magazine, 1992.
Nakajima, Y. Task-driven autonomous agent utilizing gpt-4, pinecone, and langchain for diverse applications, 2023.</p>
<p>Nakano, R., Hilton, J., Balaji, S., Wu, J., Ouyang, L., Kim, C., Hesse, C., Jain, S., Kosaraju, V., Saunders, W., et al. Webgpt: Browser-assisted question-answering with human feedback. arXiv preprint arXiv:2112.09332, 2021.</p>
<p>OpenAI. Chatgpt, 2022. URL https://openai.com/ blog/chatgpt.</p>
<p>OpenAI. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023.</p>
<p>Park, J. S., O’Brien, J., Cai, C. J., Morris, M. R., Liang, P., and Bernstein, M. S. Generative agents: Interactive simulacra of human behavior. In Proceedings of UIST, 2023.</p>
<p>Patel, A., Bhattamishra, S., and Goyal, N. Are NLP models really able to solve simple math word problems? In Proceedings of NAACL, 2021.</p>
<p>Patil, S. G., Zhang, T., Wang, X., and Gonzalez, J. E. Gorilla: Large language model connected with massive apis. arXiv preprint arXiv:2305.15334, 2023.</p>
<p>Pinedo, M. Planning and scheduling in manufacturing and services. Springer, 2005.</p>
<p>Qin, Y., Liang, S., Ye, Y., Zhu, K., Yan, L., Lu, Y., Lin, Y., Cong, X., Tang, X., Qian, B., et al. Toolllm: Facilitating large language models to master 16000+ real-world apis. In Proceedings of ICLR, 2024.</p>
<p>Roy, S. and Roth, D. Solving general arithmetic word problems. In Proceedings of EMNLP, 2015.</p>
<p>Russell, S. J. and Norvig, P. Artificial intelligence a modern approach. 2010.</p>
<p>Schick, T., Dwivedi-Yu, J., Dessi, R., Raileanu, R., Lomeli, M., Hambro, E., Zettlemoyer, L., Cancedda, N., and Scialom, T. Toolformer: Language models can teach themselves to use tools. In Proceedings of NeurIPS, 2023.</p>
<p>Shen, Y., Song, K., Tan, X., Li, D., Lu, W., and Zhuang, Y. HuggingGPT: Solving AI tasks with chatGPT and its friends in hugging face. In Proceedings of NeurIPS, 2023.</p>
<p>Shinn, N., Cassano, F., Gopinath, A., Narasimhan, K. R., and Yao, S. Reflexion: Language agents with verbal reinforcement learning. In Proceedings of NeurIPS, 2023.</p>
<p>Silver, D., Huang, A., Maddison, C. J., Guez, A., Sifre, L., Van Den Driessche, G., Schrittwieser, J., Antonoglou, I., Panneershelvam, V., Lanctot, M., et al. Mastering the game of go with deep neural networks and tree search. nature, 2016.</p>
<p>Silver, D., Hasselt, H., Hessel, M., Schaul, T., Guez, A., Harley, T., Dulac-Arnold, G., Reichert, D., Rabinowitz, N., Barreto, A., et al. The predictron: End-to-end learning and planning. In Proceedings of ICML, 2017.</p>
<p>Song, C. H., Wu, J., Washington, C., Sadler, B. M., Chao, W.-L., and Su, Y. Llm-planner: Few-shot grounded planning for embodied agents with large language models. In Proceedings of ICCV, 2023.</p>
<p>Su, Y. Language agents: a critical evolutionary step of artificial intelligence. 2023. URL https://yusu.substack. com/p/language-agents.</p>
<p>Sumers, T. R., Yao, S., Narasimhan, K., and Griffiths, T. L. Cognitive architectures for language agents. arXiv preprint arXiv:2309.02427, 2023.</p>
<p>Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Rozière, B., Goyal, N., Hambro, E., Azhar, F., et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023a.</p>
<p>Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., et al. Llama 2: Open foundation and finetuned chat models. arXiv preprint arXiv:2307.09288, 2023b.</p>
<p>Valmeekam, K., Olmo, A., Sreedharan, S., and Kambhampati, S. Planbench: An extensible benchmark for evaluating large language models on planning and reasoning about change. In Proceedings of NeurIPS, 2023.</p>
<p>Wang, G., Xie, Y., Jiang, Y., Mandlekar, A., Xiao, C., Zhu, Y., Fan, L., and Anandkumar, A. Voyager: An openended embodied agent with large language models. arXiv preprint arXiv:2305.16291, 2023.</p>
<p>Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., Le, Q. V., Zhou, D., et al. Chain-of-thought prompting elicits reasoning in large language models. In Proceedings of NeurIPS, 2022.</p>
<p>Weng, L. Llm-powered autonomous agents. lilianweng.github.io, Jun 2023. URL https://lilianweng. github.io/posts/2023-06-23-agent/.</p>
<p>Xie, T., Zhou, F., Cheng, Z., Shi, P., Weng, L., Liu, Y., Hua, T. J., Zhao, J., Liu, Q., Liu, C., Liu, L. Z., Xu, Y., Su, H., Shin, D., Xiong, C., and Yu, T. Openagents: An open platform for language agents in the wild. arXiv preprint arXiv:2310.10634, 2023.</p>
<p>Xu, Q., Hong, F., Li, B., Hu, C., Chen, Z., and Zhang, J. On the tool manipulation capability of open-source large language models. arXiv preprint arXiv:2305.16504, 2023.</p>
<p>Yao, S., Zhao, J., Yu, D., Du, N., Shafran, I., Narasimhan, K. R., and Cao, Y. React: Synergizing reasoning and acting in language models. In Proceedings of ICLR, 2022.</p>
<p>Yao, S., Yu, D., Zhao, J., Shafran, I., Griffiths, T. L., Cao, Y., and Narasimhan, K. R. Tree of thoughts: Deliberate problem solving with large language models. In Proceedings of NeurIPS, 2023.</p>
<p>Yuan, S., Chen, J., Fu, Z., Ge, X., Shah, S., Jankowski, C., Xiao, Y., and Yang, D. Distilling script knowledge from large language models for constrained language planning. In Proceedings of ACL, 2023.</p>
<p>Zhang, Y., Yuan, S., Hu, C., Richardson, K., Xiao, Y., and Chen, J. Timearena: Shaping efficient multitasking language agents in a time-aware simulation. arXiv preprint arXiv:2402.05733, 2024.</p>
<p>Zheng, B., Gou, B., Kil, J., Sun, H., and Su, Y. Gpt-4v(ision) is a generalist web agent, if grounded. arXiv preprint arXiv:2401.01614, 2024.</p>
<p>Zhong, W., Guo, L., Gao, Q., and Wang, Y. Memorybank: Enhancing large language models with long-term memory. arXiv preprint arXiv:2305.10250, 2023.</p>
<p>Zhou, S., Xu, F. F., Zhu, H., Zhou, X., Lo, R., Sridhar, A., Cheng, X., Bisk, Y., Fried, D., Alon, U., et al. Webarena: A realistic web environment for building autonomous agents. In Proceedings of ICLR, 2024.</p>
<p>Zhou, W., Jiang, Y. E., Cui, P., Wang, T., Xiao, Z., Hou, Y., Cotterell, R., and Sachan, M. Recurrentgpt: Interactive generation of (arbitrarily) long text. arXiv preprint arXiv:2305.13304, 2023.</p>
<p>Zhuang, Y., Yu, Y., Wang, K., Sun, H., and Zhang, C. ToolQA: A dataset for LLM question answering with external tools. In Proceedings of NeurIPS, 2023.</p>
<p>Zhuang, Y., Chen, X., Yu, T., Mitra, S., Bursztyn, V., Rossi, R. A., Sarkhel, S., and Zhang, C. Toolchain<em>: Efficient action space navigation in large language models with a</em> search. In Proceedings of ICLR, 2024.</p>
<h1>Appendices</h1>
<p>Within this supplementary material, we elaborate on the following aspects:</p>
<ul>
<li>Appendix A: Benchmark Details</li>
<li>Appendix B: Experiment Details</li>
<li>Appendix C: Case Presentation</li>
</ul>
<h2>A. Benchmark Details</h2>
<h2>A.1. Dataset Distribution</h2>
<p>In Table A.1, we list the detailed group distribution on training, validation and test set.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Table A.1. Dataset distribution.</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Days</td>
<td style="text-align: center;">Easy</td>
<td style="text-align: center;">Medium</td>
<td style="text-align: center;">Hard</td>
</tr>
<tr>
<td style="text-align: center;">Training (#45)</td>
<td style="text-align: center;">3-day</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">5</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">5-day</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">5</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">7-day</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">5</td>
</tr>
<tr>
<td style="text-align: center;">Validation (#180)</td>
<td style="text-align: center;">3-day</td>
<td style="text-align: center;">20</td>
<td style="text-align: center;">20</td>
<td style="text-align: center;">20</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">5-day</td>
<td style="text-align: center;">20</td>
<td style="text-align: center;">20</td>
<td style="text-align: center;">20</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">7-day</td>
<td style="text-align: center;">20</td>
<td style="text-align: center;">20</td>
<td style="text-align: center;">20</td>
</tr>
<tr>
<td style="text-align: center;">Test (#1,000)</td>
<td style="text-align: center;">3-day</td>
<td style="text-align: center;">122</td>
<td style="text-align: center;">104</td>
<td style="text-align: center;">82</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">5-day</td>
<td style="text-align: center;">116</td>
<td style="text-align: center;">114</td>
<td style="text-align: center;">121</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">7-day</td>
<td style="text-align: center;">110</td>
<td style="text-align: center;">115</td>
<td style="text-align: center;">116</td>
</tr>
</tbody>
</table>
<h2>A.2. Tool Description</h2>
<p>In Table A.2, we list the detailed tool description. The original data for each tool is sourced from publicly available Internet data. We then modify this data, which includes adding, deleting, and altering certain keys and values to suit our requirements. In this way, we effectively avoid the problem of data contamination. For more details, please refer to Appendix A.3.</p>
<p>Table A.2. Tool description and the number of data entries in the database.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Tool</th>
<th style="text-align: left;">Data Entries(#)</th>
<th style="text-align: left;">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">CitySearch</td>
<td style="text-align: left;">312</td>
<td style="text-align: left;">Search cities in the given state.</td>
</tr>
<tr>
<td style="text-align: left;">FlightSearch</td>
<td style="text-align: left;">$3,827,361$</td>
<td style="text-align: left;">Search flight information for a specific date between two cities.</td>
</tr>
<tr>
<td style="text-align: left;">DistanceMatrix</td>
<td style="text-align: left;">17,603</td>
<td style="text-align: left;">Search the driving distance, time, and possible cost between two cities.</td>
</tr>
<tr>
<td style="text-align: left;">RestaurantSearch</td>
<td style="text-align: left;">9,552</td>
<td style="text-align: left;">Search restaurants in the given city.</td>
</tr>
<tr>
<td style="text-align: left;">AttractionSearch</td>
<td style="text-align: left;">5,303</td>
<td style="text-align: left;">Search attractions in the given city.</td>
</tr>
<tr>
<td style="text-align: left;">AccommodationSearch</td>
<td style="text-align: left;">5,064</td>
<td style="text-align: left;">Search accommodations in the given city.</td>
</tr>
<tr>
<td style="text-align: left;">NotebookWrite</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">Write the selected data entry into the Notebook tool with a short description.</td>
</tr>
</tbody>
</table>
<h2>A.3. Environment Database Construction</h2>
<p>FlightSearch For FlightSearch, we source original data from the Kaggle Flight Status Prediction dataset ${ }^{2}$. From this dataset, we extract data spanning from 2022-03-01 to 2022-04-01. We specifically included fields like "FlightDate", "DepTime", "ArrTime", "ActualElapsedTime", "Distance", "OriginCityName", and "DestCityName" while discarding other values. To incorporate "Price" into our dataset, we generate this value by multiplying the "Distance" by a random factor</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>ranging from 0.2 to 0.5 .
DistanceMatrix We utilize the Google Distance Matrix API ${ }^{3}$ to calculate the driving distance and estimated travel time between two cities. For the "self-driving" and "taxi" modes of transportation, we calculate the 'Price' by multiplying the 'Distance' by factors of 1 and 0.15 , respectively. To ensure consistency and reliability of data, we store the search results in our database, thereby creating a fixed dataset for our evaluations.</p>
<p>RestaurantSearch Our restaurant data is sourced from the Kaggle Zomato Restaurants Dataset ${ }^{4}$. From this dataset, we extract the "Restaurant Name" and "Average Cost" for each establishment. Subsequently, we randomly assign these restaurants to various cities. To align with the constraint requirements of TravelPlanner, we also randomly categorize each restaurant under the following cuisines: "Chinese", "American", "Italian", "Mexican", "Indian","Mediterranean", "Middle Eastern", "Korean", "Asian", "French".</p>
<p>AttractionSearch For AttractionSearch, we employ the Google Places API ${ }^{5}$ to gather information about attractions in each city. In TravelPlanner, we retain essential details such as "Name", "Address", "Phone", "Website", "Latitude", and "Longtitue" for each attraction. To maintain data consistency and reliability, we store these search results in our database, creating a standardized dataset for our analyses.</p>
<p>AccommodationSearch Our accommodation data is obtained from the Kaggle Airbnb Open Data Dataset ${ }^{6}$. From this dataset, we extract key details "NAME", "room type", "price", "minimum nights", "review rate number", and "maximum occupancy". Items are then randomly assigned to various cities. To meet the specific constraint requirements of TravelPlanner, we also assign each item random room rules, including "No parties", "No smoking", "No children under 10", "No pets", and "No visitors".</p>
<h1>B. Experiment Details</h1>
<h2>B.1. Baselines</h2>
<p>Greedy Search To assess the effectiveness of traditional search algorithms in TravelPlanner, we integrate a greedy search approach, focusing on minimizing costs. For 5 or 7-day travel plans, the first one or two cities in the returned city search result are selected as destinations. The transportation choice is based on the lowest cost option among flights, taxis, and self-driving. The diet component involves selecting the restaurant with the lowest average cost. The cheapest accommodation is chosen for lodging. For attractions, we opt for a random selection for each day of the itinerary.</p>
<p>Planning Strategy Current planning strategies have shown effectiveness in traditional tasks like mathematical problemsolving, but their capability to handle the more complex and constrained scenarios like TravelPlanner remains to be seen. To explore this, we evaluate four distinct planning strategies on TravelPlanner: 1) Direct: In this method, the query is input directly into the model along with instructions detailing the task and relevant information gathered. 2) ZS-CoT (Wei et al., 2022): This strategy enhances the reasoning process by requiring intermediate steps. Building on the Direct method, we add the prompt "Let's think step by step" to elicit reasoning. 3) ReAct (Yao et al., 2022): This strategy incorporates environmental feedback into the reasoning process. Thus, it enhances the language agent's reasoning ability by offering additional information. In TravelPlanner, we provide the cost associated with each entire day's plan as environmental feedback. 4) Reflexion (Shinn et al., 2023): This approach utilizes a reflection model to provide high-level insights on previous erroneous attempts. Such reflective guidance aids language agents in identifying and correcting flawed reasoning. In order to control the cost, we conduct tests on Direct using four different models, while the other strategies are evaluated using GPT-3.5-Turbo. Detailed instructions for each strategy are available in Appendix B.3.</p>
<h2>B.2. GPT-4-Turbo with Reflexion strategy in sole-planning mode.</h2>
<p>We provide the results of GPT-4-Turbo with Reflexion strategy on validation set in Table B.3.</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Table B.3. GPT-4-Turbo with Reflexion strategy on validation set.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Delivery <br> Rate</th>
<th style="text-align: center;">Commonsense <br> Pass Rate</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Hard Constraint <br> Pass Rate</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Final <br> Pass Rate</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Micro</td>
<td style="text-align: center;">Macro</td>
<td style="text-align: center;">Micro</td>
<td style="text-align: center;">Macro</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Reflexion $_{\text {GPT-4-Turbo }}$</td>
<td style="text-align: center;">80.6</td>
<td style="text-align: center;">62.9</td>
<td style="text-align: center;">6.1</td>
<td style="text-align: center;">52.4</td>
<td style="text-align: center;">40.0</td>
<td style="text-align: center;">3.3</td>
</tr>
</tbody>
</table>
<h1>B.3. Prompt List</h1>
<h2>B.3.1. Tool-use Prompt</h2>
<p>We tailor the ReAct (Yao et al., 2022) framework to suit our specific requirements in TravelPlanner. An example of the instruction prompt for our needs is as follows:</p>
<div class="codehilite"><pre><span></span><code><span class="n">Collect</span><span class="w"> </span><span class="n">information</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">query</span><span class="w"> </span><span class="n">plan</span><span class="w"> </span><span class="n">using</span><span class="w"> </span><span class="n">interleaving</span><span class="w"> </span><span class="err">&#39;</span><span class="n">Thought</span><span class="err">&#39;</span><span class="p">,</span><span class="w"> </span><span class="err">&#39;</span><span class="n">Action</span><span class="err">&#39;</span><span class="p">,</span><span class="w"> </span><span class="n">and</span>
<span class="n">Observation</span><span class="err">&#39;</span><span class="w"> </span><span class="n">steps</span><span class="p">.</span><span class="w"> </span><span class="n">Ensure</span><span class="w"> </span><span class="n">you</span><span class="w"> </span><span class="n">gather</span><span class="w"> </span><span class="n">valid</span><span class="w"> </span><span class="n">information</span><span class="w"> </span><span class="n">related</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">transportation</span><span class="p">,</span><span class="w"> </span><span class="n">dining</span><span class="p">,</span>
<span class="w">    </span><span class="n">attractions</span><span class="p">,</span><span class="w"> </span><span class="n">and</span><span class="w"> </span><span class="n">accommodation</span><span class="p">.</span><span class="w"> </span><span class="n">All</span><span class="w"> </span><span class="n">information</span><span class="w"> </span><span class="n">should</span><span class="w"> </span><span class="n">be</span><span class="w"> </span><span class="n">written</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="n">Notebook</span><span class="p">,</span><span class="w"> </span><span class="n">which</span>
<span class="n">will</span><span class="w"> </span><span class="n">then</span><span class="w"> </span><span class="n">be</span><span class="w"> </span><span class="n">input</span><span class="w"> </span><span class="n">into</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">Planner</span><span class="w"> </span><span class="n">tool</span><span class="p">.</span><span class="w"> </span><span class="n">Note</span><span class="w"> </span><span class="n">that</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">nested</span><span class="w"> </span><span class="n">use</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">tools</span><span class="w"> </span><span class="n">is</span><span class="w"> </span><span class="n">prohibited</span>
<span class="p">.</span><span class="w"> </span><span class="err">&#39;</span><span class="n">Thought</span><span class="err">&#39;</span><span class="w"> </span><span class="n">can</span><span class="w"> </span><span class="n">reason</span><span class="w"> </span><span class="n">about</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">current</span><span class="w"> </span><span class="n">situation</span><span class="p">,</span><span class="w"> </span><span class="n">and</span><span class="w"> </span><span class="err">&#39;</span><span class="n">Action</span><span class="err">&#39;</span><span class="w"> </span><span class="n">can</span><span class="w"> </span><span class="n">have</span><span class="w"> </span><span class="mi">8</span><span class="w"> </span><span class="n">different</span>
<span class="nl">types</span><span class="p">:</span>
<span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="w"> </span><span class="n">FlightSearch</span><span class="p">[</span><span class="n">Departure</span><span class="w"> </span><span class="n">City</span><span class="p">,</span><span class="w"> </span><span class="n">Destination</span><span class="w"> </span><span class="n">City</span><span class="p">,</span><span class="w"> </span><span class="n">Date</span><span class="p">]</span><span class="o">:</span>
<span class="nl">Description</span><span class="p">:</span>
<span class="n">A</span><span class="w"> </span><span class="n">flight</span><span class="w"> </span><span class="n">information</span><span class="w"> </span><span class="n">retrieval</span><span class="w"> </span><span class="n">tool</span><span class="p">.</span>
<span class="nl">Parameters</span><span class="p">:</span>
<span class="n">Departure</span><span class="w"> </span><span class="n">City</span><span class="o">:</span><span class="w"> </span><span class="n">The</span><span class="w"> </span><span class="n">city</span><span class="w"> </span><span class="n">you</span><span class="err">&#39;</span><span class="n">ll</span><span class="w"> </span><span class="n">be</span><span class="w"> </span><span class="n">flying</span><span class="w"> </span><span class="k">out</span><span class="w"> </span><span class="n">from</span><span class="p">.</span>
<span class="n">Destination</span><span class="w"> </span><span class="n">City</span><span class="o">:</span><span class="w"> </span><span class="n">The</span><span class="w"> </span><span class="n">city</span><span class="w"> </span><span class="n">you</span><span class="w"> </span><span class="n">aim</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">reach</span><span class="p">.</span>
<span class="nl">Date</span><span class="p">:</span><span class="w"> </span><span class="n">The</span><span class="w"> </span><span class="n">date</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">your</span><span class="w"> </span><span class="n">travel</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="n">YYYY</span><span class="o">-</span><span class="n">MM</span><span class="o">-</span><span class="n">DD</span><span class="w"> </span><span class="n">format</span><span class="p">.</span>
<span class="nl">Example</span><span class="p">:</span><span class="w"> </span><span class="n">FlightSearch</span><span class="p">[</span><span class="n">New</span><span class="w"> </span><span class="n">York</span><span class="p">,</span><span class="w"> </span><span class="n">London</span><span class="p">,</span><span class="w"> </span><span class="mi">2022-10</span><span class="mo">-01</span><span class="p">]</span><span class="w"> </span><span class="n">would</span><span class="w"> </span><span class="n">fetch</span><span class="w"> </span><span class="n">flights</span><span class="w"> </span><span class="n">from</span><span class="w"> </span><span class="n">New</span><span class="w"> </span><span class="n">York</span><span class="w"> </span><span class="n">to</span>
<span class="n">London</span><span class="w"> </span><span class="n">on</span><span class="w"> </span><span class="n">October</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="mf">2022.</span>
<span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="w"> </span><span class="n">DistanceMatrix</span><span class="p">[</span><span class="n">Origin</span><span class="p">,</span><span class="w"> </span><span class="n">Destination</span><span class="p">,</span><span class="w"> </span><span class="n">Mode</span><span class="p">]</span><span class="o">:</span>
<span class="nl">Description</span><span class="p">:</span><span class="w"> </span><span class="n">Estimate</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">distance</span><span class="p">,</span><span class="w"> </span><span class="n">time</span><span class="w"> </span><span class="n">and</span><span class="w"> </span><span class="n">cost</span><span class="w"> </span><span class="n">between</span><span class="w"> </span><span class="n">two</span><span class="w"> </span><span class="n">cities</span><span class="p">.</span>
<span class="nl">Parameters</span><span class="p">:</span>
<span class="nl">Origin</span><span class="p">:</span><span class="w"> </span><span class="n">The</span><span class="w"> </span><span class="n">departure</span><span class="w"> </span><span class="n">city</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">your</span><span class="w"> </span><span class="n">journey</span><span class="p">.</span>
<span class="nl">Destination</span><span class="p">:</span><span class="w"> </span><span class="n">The</span><span class="w"> </span><span class="n">destination</span><span class="w"> </span><span class="n">city</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">your</span><span class="w"> </span><span class="n">journey</span><span class="p">.</span>
<span class="nl">Mode</span><span class="p">:</span><span class="w"> </span><span class="n">The</span><span class="w"> </span><span class="n">method</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">transportation</span><span class="p">.</span><span class="w"> </span><span class="n">Choices</span><span class="w"> </span><span class="n">include</span><span class="w"> </span><span class="err">&#39;</span><span class="nb">self</span><span class="o">-</span><span class="n">driving</span><span class="err">&#39;</span><span class="w"> </span><span class="n">and</span><span class="w"> </span><span class="err">&#39;</span><span class="n">taxi</span><span class="err">&#39;</span><span class="p">.</span>
<span class="nl">Example</span><span class="p">:</span><span class="w"> </span><span class="n">DistanceMatrix</span><span class="p">[</span><span class="n">Paris</span><span class="p">,</span><span class="w"> </span><span class="n">Lyon</span><span class="p">,</span><span class="w"> </span><span class="nb">self</span><span class="o">-</span><span class="n">driving</span><span class="p">]</span><span class="w"> </span><span class="n">would</span><span class="w"> </span><span class="n">provide</span><span class="w"> </span><span class="n">driving</span><span class="w"> </span><span class="n">distance</span><span class="p">,</span><span class="w"> </span><span class="n">time</span>
<span class="n">and</span><span class="w"> </span><span class="n">cost</span><span class="w"> </span><span class="n">between</span><span class="w"> </span><span class="n">Paris</span><span class="w"> </span><span class="n">and</span><span class="w"> </span><span class="n">Lyon</span><span class="p">.</span>
<span class="p">(</span><span class="mi">3</span><span class="p">)</span><span class="w"> </span><span class="n">AccommodationSearch</span><span class="p">[</span><span class="n">City</span><span class="p">]</span><span class="o">:</span>
<span class="nl">Description</span><span class="p">:</span><span class="w"> </span><span class="n">Discover</span><span class="w"> </span><span class="n">accommodations</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="n">your</span><span class="w"> </span><span class="n">desired</span><span class="w"> </span><span class="n">city</span><span class="p">.</span>
<span class="nl">Parameter</span><span class="p">:</span><span class="w"> </span><span class="n">City</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">The</span><span class="w"> </span><span class="n">name</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">city</span><span class="w"> </span><span class="n">where</span><span class="w"> </span><span class="n">you</span><span class="err">&#39;</span><span class="n">re</span><span class="w"> </span><span class="n">seeking</span><span class="w"> </span><span class="n">accommodation</span><span class="p">.</span>
<span class="nl">Example</span><span class="p">:</span><span class="w"> </span><span class="n">AccommodationSearch</span><span class="p">[</span><span class="n">Rome</span><span class="p">]</span><span class="w"> </span><span class="n">would</span><span class="w"> </span><span class="n">present</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">list</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">hotel</span><span class="w"> </span><span class="n">rooms</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="n">Rome</span><span class="p">.</span>
<span class="p">(</span><span class="mi">4</span><span class="p">)</span><span class="w"> </span><span class="n">RestaurantSearch</span><span class="p">[</span><span class="n">City</span><span class="p">]</span><span class="o">:</span>
<span class="nl">Description</span><span class="p">:</span><span class="w"> </span><span class="n">Explore</span><span class="w"> </span><span class="n">dining</span><span class="w"> </span><span class="n">options</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">city</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">your</span><span class="w"> </span><span class="n">choice</span><span class="p">.</span>
<span class="nl">Parameter</span><span class="p">:</span><span class="w"> </span><span class="n">City</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">The</span><span class="w"> </span><span class="n">name</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">city</span><span class="w"> </span><span class="n">where</span><span class="w"> </span><span class="n">you</span><span class="err">&#39;</span><span class="n">re</span><span class="w"> </span><span class="n">seeking</span><span class="w"> </span><span class="n">restaurant</span><span class="p">.</span>
<span class="nl">Example</span><span class="p">:</span><span class="w"> </span><span class="n">RestaurantSearch</span><span class="p">[</span><span class="n">Tokyo</span><span class="p">]</span><span class="w"> </span><span class="n">would</span><span class="w"> </span><span class="n">show</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">curated</span><span class="w"> </span><span class="n">list</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">restaurants</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="n">Tokyo</span><span class="p">.</span>
<span class="p">(</span><span class="mi">5</span><span class="p">)</span><span class="w"> </span><span class="n">AttractionSearch</span><span class="p">[</span><span class="n">City</span><span class="p">]</span><span class="o">:</span>
<span class="nl">Description</span><span class="p">:</span><span class="w"> </span><span class="n">Find</span><span class="w"> </span><span class="n">attractions</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">city</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">your</span><span class="w"> </span><span class="n">choice</span><span class="p">.</span>
<span class="nl">Parameter</span><span class="p">:</span><span class="w"> </span><span class="n">City</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">The</span><span class="w"> </span><span class="n">name</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">city</span><span class="w"> </span><span class="n">where</span><span class="w"> </span><span class="n">you</span><span class="err">&#39;</span><span class="n">re</span><span class="w"> </span><span class="n">seeking</span><span class="w"> </span><span class="n">attractions</span><span class="p">.</span>
<span class="nl">Example</span><span class="p">:</span><span class="w"> </span><span class="n">AttractionSearch</span><span class="p">[</span><span class="n">London</span><span class="p">]</span><span class="w"> </span><span class="n">would</span><span class="w"> </span><span class="k">return</span><span class="w"> </span><span class="n">attractions</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="n">London</span><span class="p">.</span>
<span class="p">(</span><span class="mi">6</span><span class="p">)</span><span class="w"> </span><span class="n">CitySearch</span><span class="p">[</span><span class="n">State</span><span class="p">]</span>
<span class="nl">Description</span><span class="p">:</span><span class="w"> </span><span class="n">Find</span><span class="w"> </span><span class="n">cities</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">state</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">your</span><span class="w"> </span><span class="n">choice</span><span class="p">.</span>
<span class="nl">Parameter</span><span class="p">:</span><span class="w"> </span><span class="n">State</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">The</span><span class="w"> </span><span class="n">name</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">city</span><span class="w"> </span><span class="n">where</span><span class="w"> </span><span class="n">you</span><span class="err">&#39;</span><span class="n">re</span><span class="w"> </span><span class="n">seeking</span><span class="w"> </span><span class="n">cities</span><span class="p">.</span>
<span class="nl">Example</span><span class="p">:</span><span class="w"> </span><span class="n">CitySearch</span><span class="p">[</span><span class="n">California</span><span class="p">]</span><span class="w"> </span><span class="n">would</span><span class="w"> </span><span class="k">return</span><span class="w"> </span><span class="n">cities</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="n">California</span><span class="p">.</span>
<span class="p">(</span><span class="mi">7</span><span class="p">)</span><span class="w"> </span><span class="n">NotebookWrite</span><span class="p">[</span><span class="n">Short</span><span class="w"> </span><span class="n">Description</span><span class="p">]</span>
<span class="nl">Description</span><span class="p">:</span><span class="w"> </span><span class="n">Writes</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">new</span><span class="w"> </span><span class="n">data</span><span class="w"> </span><span class="n">entry</span><span class="w"> </span><span class="n">into</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">Notebook</span><span class="w"> </span><span class="n">tool</span><span class="w"> </span><span class="n">with</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="kt">short</span><span class="w"> </span><span class="n">description</span><span class="p">.</span><span class="w"> </span><span class="n">This</span>
<span class="n">tool</span><span class="w"> </span><span class="n">should</span><span class="w"> </span><span class="n">be</span><span class="w"> </span><span class="n">used</span><span class="w"> </span><span class="n">immediately</span><span class="w"> </span><span class="n">after</span><span class="w"> </span><span class="n">FlightSearch</span><span class="p">,</span><span class="w"> </span><span class="n">AccommodationSearch</span><span class="p">,</span><span class="w"> </span><span class="n">AttractionSearch</span>
</code></pre></div>

<p>, RestaurantSearch or DistanceMatrix. Only the data stored in Notebook can be seen by Planner. So you should write all the information you need into Notebook.
Parameters: Short Description - A brief description or label for the stored data.
You don't need to write all the information in the description.
The data you've searched for will be automatically stored in the Notebook.
Example: NotebookWrite[Flights from Rome to Paris in 2022-02-01] would store the informatrion of flights from Rome to Paris in 2022-02-01 in the Notebook.
(8) Planner[Query]</p>
<p>Description: A smart planning tool that crafts detailed plans based on user input and the information stroed in Notebook.
Parameters:
Query: The query from user.
Example: Planner[Give me a 3-day trip plan from Seattle to New York] would return a detailed 3-day trip plan.
You should use as many as possible steps to collect engough information to input to the Planner tool.</p>
<p>Each action only calls one function once. Do not add any description in the action.
Query: {query}</p>
<h1>B.3.2. DiRECT PLANNING PROMPT</h1>
<p>We provide the instruction prompt of Direct strategy as follows:</p>
<div class="codehilite"><pre><span></span><code>You<span class="w"> </span>are<span class="w"> </span>a<span class="w"> </span>proficient<span class="w"> </span>planner.<span class="w"> </span>Based<span class="w"> </span>on<span class="w"> </span>the<span class="w"> </span>provided<span class="w"> </span>information<span class="w"> </span>and<span class="w"> </span>query,<span class="w"> </span>please<span class="w"> </span>give<span class="w"> </span>me
a<span class="w"> </span>detailed<span class="w"> </span>plan,<span class="w"> </span>including<span class="w"> </span>specifics<span class="w"> </span>such<span class="w"> </span>as<span class="w"> </span>flight<span class="w"> </span>numbers<span class="w"> </span>(e.g.,<span class="w"> </span>F0123456),<span class="w"> </span>restaurant
names,<span class="w"> </span>and<span class="w"> </span>accommodation<span class="w"> </span>names.<span class="w"> </span>Note<span class="w"> </span>that<span class="w"> </span>all<span class="w"> </span>the<span class="w"> </span>information<span class="w"> </span>in<span class="w"> </span>your<span class="w"> </span>plan<span class="w"> </span>should<span class="w"> </span>be
derived<span class="w"> </span>from<span class="w"> </span>the<span class="w"> </span>provided<span class="w"> </span>data.<span class="w"> </span>You<span class="w"> </span>must<span class="w"> </span>adhere<span class="w"> </span>to<span class="w"> </span>the<span class="w"> </span>format<span class="w"> </span>given<span class="w"> </span>in<span class="w"> </span>the<span class="w"> </span>example.
Additionally,<span class="w"> </span>all<span class="w"> </span>details<span class="w"> </span>should<span class="w"> </span>align<span class="w"> </span>with<span class="w"> </span>commonsense.<span class="w"> </span>The<span class="w"> </span>symbol<span class="w"> </span>&#39;-&#39;<span class="w"> </span>indicates<span class="w"> </span>that
information<span class="w"> </span>is<span class="w"> </span>unnecessary.<span class="w"> </span>For<span class="w"> </span>example,<span class="w"> </span>in<span class="w"> </span>the<span class="w"> </span>provided<span class="w"> </span>sample,<span class="w"> </span>you<span class="w"> </span>do<span class="w"> </span>not<span class="w"> </span>need<span class="w"> </span>to<span class="w"> </span>plan
after<span class="w"> </span>returning<span class="w"> </span>to<span class="w"> </span>the<span class="w"> </span>departure<span class="w"> </span>city.<span class="w"> </span>When<span class="w"> </span>you<span class="w"> </span>travel<span class="w"> </span>to<span class="w"> </span>two<span class="w"> </span>cities<span class="w"> </span>in<span class="w"> </span>one<span class="w"> </span>day,<span class="w"> </span>you
should<span class="w"> </span>note<span class="w"> </span>it<span class="w"> </span>in<span class="w"> </span>the<span class="w"> </span>&#39;Current<span class="w"> </span>City&#39;<span class="w"> </span>section<span class="w"> </span>as<span class="w"> </span>in<span class="w"> </span>the<span class="w"> </span>example<span class="w"> </span>(i.e.,<span class="w"> </span>from<span class="w"> </span>A<span class="w"> </span>to<span class="w"> </span>B).
*****<span class="w"> </span>Example<span class="w"> </span>*****
Query:<span class="w"> </span>Could<span class="w"> </span>you<span class="w"> </span>create<span class="w"> </span>a<span class="w"> </span>challenging<span class="w"> </span>travel<span class="w"> </span>plan<span class="w"> </span>for<span class="w"> </span>7<span class="w"> </span>people<span class="w"> </span>from<span class="w"> </span>Roanoke<span class="w"> </span>to<span class="w"> </span>Illinois
spanning<span class="w"> </span>a<span class="w"> </span>week,<span class="w"> </span>from<span class="w"> </span>March<span class="w"> </span>8th<span class="w"> </span>to<span class="w"> </span>March<span class="w"> </span>14th,<span class="w"> </span>2022,<span class="w"> </span>with<span class="w"> </span>a<span class="w"> </span>budget<span class="w"> </span>of<span class="w"> </span>$30,2007<span class="w"> </span>The
preference<span class="w"> </span>is<span class="w"> </span>for<span class="w"> </span>an<span class="w"> </span>entire<span class="w"> </span>room,<span class="w"> </span>and<span class="w"> </span>we<span class="w"> </span>would<span class="w"> </span>not<span class="w"> </span>be<span class="w"> </span>taking<span class="w"> </span>any<span class="w"> </span>flights.<span class="w"> </span>In<span class="w"> </span>terms<span class="w"> </span>of
cuisine,<span class="w"> </span>we<span class="w"> </span>are<span class="w"> </span>interested<span class="w"> </span>in<span class="w"> </span>sampling<span class="w"> </span>some<span class="w"> </span>Italian<span class="w"> </span>and<span class="w"> </span>Chinese<span class="w"> </span>food.
Travel<span class="w"> </span>Plan:
Day<span class="w"> </span>1:
Current<span class="w"> </span>City:<span class="w"> </span>from<span class="w"> </span>Ithaca<span class="w"> </span>to<span class="w"> </span>Charlotte
Transportation:<span class="w"> </span>Flight<span class="w"> </span>Number:<span class="w"> </span>F3633413,<span class="w"> </span>from<span class="w"> </span>Ithaca<span class="w"> </span>to<span class="w"> </span>Charlotte,<span class="w"> </span>Departure<span class="w"> </span>Time:<span class="w"> </span>05:38,
Arrival<span class="w"> </span>Time:<span class="w"> </span>07:46
Breakfast:<span class="w"> </span>Nagaland&#39;s<span class="w"> </span>Kitchen,<span class="w"> </span>Charlotte
Attraction:<span class="w"> </span>The<span class="w"> </span>Charlotte<span class="w"> </span>Museum<span class="w"> </span>of<span class="w"> </span>History,<span class="w"> </span>Charlotte
Lunch:<span class="w"> </span>Cafe<span class="w"> </span>Maple<span class="w"> </span>Street,<span class="w"> </span>Charlotte
Dinner:<span class="w"> </span>Bombay<span class="w"> </span>Vada<span class="w"> </span>Pav,<span class="w"> </span>Charlotte
Accommodation:<span class="w"> </span>Affordable<span class="w"> </span>Spacious<span class="w"> </span>Refurbished<span class="w"> </span>Room<span class="w"> </span>in<span class="w"> </span>Bushwick!,<span class="w"> </span>Charlotte
Day<span class="w"> </span>2:
Current<span class="w"> </span>City:<span class="w"> </span>Charlotte
Transportation:<span class="w"> </span>-
Breakfast:<span class="w"> </span>Olive<span class="w"> </span>Tree<span class="w"> </span>Cafe,<span class="w"> </span>Charlotte
Attraction:<span class="w"> </span>The<span class="w"> </span>Mint<span class="w"> </span>Museum,<span class="w"> </span>Charlotte;Romare<span class="w"> </span>Bearden<span class="w"> </span>Park,<span class="w"> </span>Charlotte.
Lunch:<span class="w"> </span>Birbal<span class="w"> </span>Ji<span class="w"> </span>Dhaba,<span class="w"> </span>Charlotte
Dinner:<span class="w"> </span>Pind<span class="w"> </span>Balluchi,<span class="w"> </span>Charlotte
Accommodation:<span class="w"> </span>Affordable<span class="w"> </span>Spacious<span class="w"> </span>Refurbished<span class="w"> </span>Room<span class="w"> </span>in<span class="w"> </span>Bushwick!,<span class="w"> </span>Charlotte
Day<span class="w"> </span>3:
Current<span class="w"> </span>City:<span class="w"> </span>Charlotte
Transportation:<span class="w"> </span>Flight<span class="w"> </span>Number:<span class="w"> </span>F3786167,<span class="w"> </span>from<span class="w"> </span>Charlotte<span class="w"> </span>to<span class="w"> </span>Ithaca,
Departure<span class="w"> </span>Time:<span class="w"> </span>21:42,<span class="w"> </span>Arrival<span class="w"> </span>Time:<span class="w"> </span>23:26
</code></pre></div>

<div class="codehilite"><pre><span></span><code><span class="n">Breakfast</span><span class="o">:</span><span class="w"> </span><span class="n">Subway</span><span class="o">,</span><span class="w"> </span><span class="n">Charlotte</span>
<span class="n">Attraction</span><span class="o">:</span><span class="w"> </span><span class="n">Books</span><span class="w"> </span><span class="n">Monument</span><span class="o">,</span><span class="w"> </span><span class="n">Charlotte</span><span class="o">.</span>
<span class="n">Lunch</span><span class="o">:</span><span class="w"> </span><span class="n">Olive</span><span class="w"> </span><span class="n">Tree</span><span class="w"> </span><span class="n">Cafe</span><span class="o">,</span><span class="w"> </span><span class="n">Charlotte</span>
<span class="n">Dinner</span><span class="o">:</span><span class="w"> </span><span class="n">Kylin</span><span class="w"> </span><span class="n">Skybar</span><span class="o">,</span><span class="w"> </span><span class="n">Charlotte</span>
<span class="n">Accommodation</span><span class="o">:</span><span class="w"> </span><span class="o">-</span>
<span class="o">*****</span><span class="w"> </span><span class="n">Example</span><span class="w"> </span><span class="n">Ends</span><span class="w"> </span><span class="o">*****</span>
<span class="n">Given</span><span class="w"> </span><span class="n">information</span><span class="o">:</span><span class="w"> </span><span class="o">{</span><span class="n">text</span><span class="o">}</span>
<span class="n">Query</span><span class="o">:</span><span class="w"> </span><span class="o">{</span><span class="n">query</span><span class="o">}</span>
<span class="n">Travel</span><span class="w"> </span><span class="n">Plan</span><span class="o">:</span>
</code></pre></div>

<h1>B.3.3. REACT \&amp; REFLEXION PLANNING PROMPT</h1>
<p>The instruction prompts for the React and Reflexion planning strategies are provided as follows:</p>
<div class="codehilite"><pre><span></span><code><span class="n">You</span><span class="w"> </span><span class="n">are</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">proficient</span><span class="w"> </span><span class="n">planner</span><span class="p">.</span><span class="w"> </span><span class="n">Based</span><span class="w"> </span><span class="n">on</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">provided</span><span class="w"> </span><span class="n">information</span><span class="w"> </span><span class="n">and</span><span class="w"> </span><span class="n">query</span><span class="p">,</span><span class="w"> </span><span class="n">please</span><span class="w"> </span><span class="n">give</span><span class="w"> </span><span class="n">me</span>
<span class="n">a</span><span class="w"> </span><span class="n">detailed</span><span class="w"> </span><span class="n">plan</span><span class="p">,</span><span class="w"> </span><span class="n">including</span><span class="w"> </span><span class="n">specifics</span><span class="w"> </span><span class="n">such</span><span class="w"> </span><span class="n">as</span><span class="w"> </span><span class="n">flight</span><span class="w"> </span><span class="n">numbers</span><span class="w"> </span><span class="p">(</span><span class="n">e</span><span class="p">.</span><span class="n">g</span><span class="p">.,</span><span class="w"> </span><span class="n">F0123456</span><span class="p">),</span><span class="w"> </span><span class="n">restaurant</span>
<span class="n">names</span><span class="p">,</span><span class="w"> </span><span class="n">and</span><span class="w"> </span><span class="n">hotel</span><span class="w"> </span><span class="n">names</span><span class="p">.</span><span class="w"> </span><span class="n">Note</span><span class="w"> </span><span class="n">that</span><span class="w"> </span><span class="n">all</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">information</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="n">your</span><span class="w"> </span><span class="n">plan</span><span class="w"> </span><span class="n">should</span><span class="w"> </span><span class="n">be</span><span class="w"> </span><span class="n">derived</span><span class="w"> </span><span class="n">from</span>
<span class="n">the</span><span class="w"> </span><span class="n">provided</span><span class="w"> </span><span class="n">data</span><span class="p">.</span><span class="w"> </span><span class="n">You</span><span class="w"> </span><span class="n">must</span><span class="w"> </span><span class="n">adhere</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">format</span><span class="w"> </span><span class="n">given</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">example</span><span class="p">.</span><span class="w"> </span><span class="n">Additionally</span><span class="p">,</span><span class="w"> </span><span class="n">all</span>
<span class="n">details</span><span class="w"> </span><span class="n">should</span><span class="w"> </span><span class="n">align</span><span class="w"> </span><span class="n">with</span><span class="w"> </span><span class="n">common</span><span class="w"> </span><span class="n">sense</span><span class="p">.</span><span class="w"> </span><span class="n">Attraction</span><span class="w"> </span><span class="n">visits</span><span class="w"> </span><span class="n">and</span><span class="w"> </span><span class="n">meals</span><span class="w"> </span><span class="n">are</span><span class="w"> </span><span class="n">expected</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">be</span>
<span class="n">diverse</span><span class="p">.</span><span class="w"> </span><span class="n">The</span><span class="w"> </span><span class="n">symbol</span><span class="w"> </span><span class="sc">&#39;-&#39;</span><span class="w"> </span><span class="n">indicates</span><span class="w"> </span><span class="n">that</span><span class="w"> </span><span class="n">information</span><span class="w"> </span><span class="n">is</span><span class="w"> </span><span class="n">unnecessary</span><span class="p">.</span><span class="w"> </span><span class="n">For</span><span class="w"> </span><span class="n">example</span><span class="p">,</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="n">the</span>
<span class="n">provided</span><span class="w"> </span><span class="n">sample</span><span class="p">,</span><span class="w"> </span><span class="n">you</span><span class="w"> </span><span class="k">do</span><span class="w"> </span><span class="n">not</span><span class="w"> </span><span class="n">need</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">plan</span><span class="w"> </span><span class="n">after</span><span class="w"> </span><span class="n">returning</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">departure</span><span class="w"> </span><span class="n">city</span><span class="p">.</span><span class="w"> </span><span class="n">When</span><span class="w"> </span><span class="n">you</span>
<span class="n">travel</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">two</span><span class="w"> </span><span class="n">cities</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="n">one</span><span class="w"> </span><span class="n">day</span><span class="p">,</span><span class="w"> </span><span class="n">you</span><span class="w"> </span><span class="n">should</span><span class="w"> </span><span class="n">note</span><span class="w"> </span><span class="n">it</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="err">&#39;</span><span class="n">Current</span><span class="w"> </span><span class="n">City</span><span class="err">&#39;</span><span class="w"> </span><span class="n">section</span><span class="w"> </span><span class="n">as</span><span class="w"> </span><span class="k">in</span>
<span class="n">the</span><span class="w"> </span><span class="n">example</span><span class="w"> </span><span class="p">(</span><span class="n">i</span><span class="p">.</span><span class="n">e</span><span class="p">.,</span><span class="w"> </span><span class="n">from</span><span class="w"> </span><span class="n">A</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">B</span><span class="p">).</span><span class="w"> </span><span class="n">Solve</span><span class="w"> </span><span class="n">this</span><span class="w"> </span><span class="n">task</span><span class="w"> </span><span class="n">by</span><span class="w"> </span><span class="n">alternating</span><span class="w"> </span><span class="n">between</span><span class="w"> </span><span class="n">Thought</span><span class="p">,</span><span class="w"> </span><span class="n">Action</span><span class="p">,</span>
<span class="n">and</span><span class="w"> </span><span class="n">Observation</span><span class="w"> </span><span class="n">steps</span><span class="p">.</span><span class="w"> </span><span class="n">The</span><span class="w"> </span><span class="err">&#39;</span><span class="n">Thought</span><span class="err">&#39;</span><span class="w"> </span><span class="n">phase</span><span class="w"> </span><span class="n">involves</span><span class="w"> </span><span class="n">reasoning</span><span class="w"> </span><span class="n">about</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">current</span><span class="w"> </span><span class="n">situation</span><span class="p">.</span>
<span class="n">The</span><span class="w"> </span><span class="err">&#39;</span><span class="n">Action</span><span class="err">&#39;</span><span class="w"> </span><span class="n">phase</span><span class="w"> </span><span class="n">can</span><span class="w"> </span><span class="n">be</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">two</span><span class="w"> </span><span class="n">types</span><span class="o">:</span>
<span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="w"> </span><span class="n">CostEnquiry</span><span class="p">[</span><span class="n">Sub</span><span class="w"> </span><span class="n">Plan</span><span class="p">]</span><span class="o">:</span><span class="w"> </span><span class="n">This</span><span class="w"> </span><span class="n">function</span><span class="w"> </span><span class="n">calculates</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">cost</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">detailed</span><span class="w"> </span><span class="n">sub</span><span class="w"> </span><span class="n">plan</span><span class="p">,</span><span class="w"> </span><span class="n">which</span>
<span class="n">you</span><span class="w"> </span><span class="n">need</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">input</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">people</span><span class="w"> </span><span class="n">number</span><span class="w"> </span><span class="n">and</span><span class="w"> </span><span class="n">plan</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="n">JSON</span><span class="w"> </span><span class="n">format</span><span class="p">.</span><span class="w"> </span><span class="n">The</span><span class="w"> </span><span class="n">sub</span><span class="w"> </span><span class="n">plan</span><span class="w"> </span><span class="n">should</span>
<span class="n">encompass</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">complete</span><span class="w"> </span><span class="n">one</span><span class="o">-</span><span class="n">day</span><span class="w"> </span><span class="n">plan</span><span class="p">.</span><span class="w"> </span><span class="n">An</span><span class="w"> </span><span class="n">example</span><span class="w"> </span><span class="n">will</span><span class="w"> </span><span class="n">be</span><span class="w"> </span><span class="n">provided</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">reference</span><span class="p">.</span>
<span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="w"> </span><span class="n">Finish</span><span class="p">[</span><span class="n">Final</span><span class="w"> </span><span class="n">Plan</span><span class="p">]</span><span class="o">:</span><span class="w"> </span><span class="n">Use</span><span class="w"> </span><span class="n">this</span><span class="w"> </span><span class="n">function</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">indicate</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">completion</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">task</span><span class="p">.</span>
<span class="n">You</span><span class="w"> </span><span class="n">must</span><span class="w"> </span><span class="n">submit</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">final</span><span class="p">,</span><span class="w"> </span><span class="n">complete</span><span class="w"> </span><span class="n">plan</span><span class="w"> </span><span class="n">as</span><span class="w"> </span><span class="n">an</span><span class="w"> </span><span class="n">argument</span><span class="p">.</span>
<span class="o">*****</span><span class="w"> </span><span class="n">Example</span><span class="w"> </span><span class="o">*****</span>
<span class="nl">Query</span><span class="p">:</span><span class="w"> </span><span class="n">Could</span><span class="w"> </span><span class="n">you</span><span class="w"> </span><span class="n">create</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">challenging</span><span class="w"> </span><span class="n">travel</span><span class="w"> </span><span class="n">plan</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="mi">7</span><span class="w"> </span><span class="n">people</span><span class="w"> </span><span class="n">from</span><span class="w"> </span><span class="n">Roanoke</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">Illinois</span>
<span class="n">spanning</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">week</span><span class="p">,</span><span class="w"> </span><span class="n">from</span><span class="w"> </span><span class="n">March</span><span class="w"> </span><span class="mi">8</span><span class="n">th</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">March</span><span class="w"> </span><span class="mi">14</span><span class="n">th</span><span class="p">,</span><span class="w"> </span><span class="mi">2022</span><span class="p">,</span><span class="w"> </span><span class="n">with</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">budget</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">$30</span><span class="p">,</span><span class="mi">200</span><span class="o">?</span><span class="w"> </span><span class="n">The</span>
<span class="n">preference</span><span class="w"> </span><span class="n">is</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">an</span><span class="w"> </span><span class="n">entire</span><span class="w"> </span><span class="n">room</span><span class="p">,</span><span class="w"> </span><span class="n">and</span><span class="w"> </span><span class="n">we</span><span class="w"> </span><span class="n">would</span><span class="w"> </span><span class="n">not</span><span class="w"> </span><span class="n">be</span><span class="w"> </span><span class="n">taking</span><span class="w"> </span><span class="n">any</span><span class="w"> </span><span class="n">flights</span><span class="p">.</span><span class="w"> </span><span class="n">In</span><span class="w"> </span><span class="n">terms</span><span class="w"> </span><span class="n">of</span>
<span class="n">cuisine</span><span class="p">,</span><span class="w"> </span><span class="n">we</span><span class="w"> </span><span class="n">are</span><span class="w"> </span><span class="n">interested</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="n">sampling</span><span class="w"> </span><span class="n">some</span><span class="w"> </span><span class="n">Italian</span><span class="w"> </span><span class="n">and</span><span class="w"> </span><span class="n">Chinese</span><span class="w"> </span><span class="n">food</span><span class="p">.</span><span class="n">You</span><span class="w"> </span><span class="n">can</span><span class="w"> </span><span class="n">call</span>
<span class="n">CostEuquiry</span><span class="w"> </span><span class="n">like</span><span class="w"> </span><span class="n">CostEuquiry</span><span class="p">[{{</span><span class="s">&quot;people_number&quot;</span><span class="o">:</span><span class="w"> </span><span class="mi">7</span><span class="p">,</span><span class="s">&quot;day&quot;</span><span class="o">:</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="s">&quot;current_city&quot;</span><span class="o">:</span><span class="w"> </span><span class="s">&quot;from Ithaca to</span>
<span class="n">Charlotte</span><span class="s">&quot;,&quot;</span><span class="n">transportation</span><span class="s">&quot;: &quot;</span><span class="n">Flight</span><span class="w"> </span><span class="n">Number</span><span class="o">:</span><span class="w"> </span><span class="n">F3633413</span><span class="p">,</span><span class="w"> </span><span class="n">from</span><span class="w"> </span><span class="n">Ithaca</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">Charlotte</span><span class="p">,</span>
<span class="n">Departure</span><span class="w"> </span><span class="n">Time</span><span class="o">:</span><span class="w"> </span><span class="mo">05</span><span class="o">:</span><span class="mi">38</span><span class="p">,</span><span class="w"> </span><span class="n">Arrival</span><span class="w"> </span><span class="n">Time</span><span class="o">:</span><span class="w"> </span><span class="mo">07</span><span class="o">:</span><span class="mi">46</span><span class="s">&quot;,&quot;</span><span class="n">breakfast</span><span class="s">&quot;: &quot;</span><span class="n">Nagaland</span><span class="err">&#39;</span><span class="n">s</span><span class="w"> </span><span class="n">Kitchen</span><span class="p">,</span><span class="w"> </span><span class="n">Charlotte</span><span class="s">&quot;,&quot;</span>
<span class="n">attraction</span><span class="s">&quot;: &quot;</span><span class="n">The</span><span class="w"> </span><span class="n">Charlotte</span><span class="w"> </span><span class="n">Museum</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">History</span><span class="p">,</span><span class="w"> </span><span class="n">Charlotte</span><span class="s">&quot;,&quot;</span><span class="n">lunch</span><span class="s">&quot;: &quot;</span><span class="n">Cafe</span><span class="w"> </span><span class="n">Maple</span><span class="w"> </span><span class="n">Street</span><span class="p">,</span>
<span class="n">Charlotte</span><span class="s">&quot;,&quot;</span><span class="n">dinner</span><span class="s">&quot;: &quot;</span><span class="n">Bombay</span><span class="w"> </span><span class="n">Vada</span><span class="w"> </span><span class="n">Pav</span><span class="p">,</span><span class="w"> </span><span class="n">Charlotte</span><span class="s">&quot;,&quot;</span><span class="n">accommodation</span><span class="s">&quot;: &quot;</span><span class="n">Affordable</span><span class="w"> </span><span class="n">Spacious</span>
<span class="n">Refurbished</span><span class="w"> </span><span class="n">Room</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="n">Bushwick</span><span class="o">!</span><span class="p">,</span><span class="w"> </span><span class="n">Charlotte</span><span class="s">&quot;}}]</span>
<span class="n">You</span><span class="w"> </span><span class="n">can</span><span class="w"> </span><span class="n">call</span><span class="w"> </span><span class="n">Finish</span><span class="w"> </span><span class="n">like</span><span class="w"> </span><span class="n">Finish</span><span class="p">[</span>
<span class="nl">Day</span><span class="p">:</span><span class="w"> </span><span class="mi">1</span>
<span class="n">Current</span><span class="w"> </span><span class="n">City</span><span class="o">:</span><span class="w"> </span><span class="n">from</span><span class="w"> </span><span class="n">Ithaca</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">Charlotte</span>
<span class="nl">Transportation</span><span class="p">:</span><span class="w"> </span><span class="n">Flight</span><span class="w"> </span><span class="n">Number</span><span class="o">:</span><span class="w"> </span><span class="n">F3633413</span><span class="p">,</span><span class="w"> </span><span class="n">from</span><span class="w"> </span><span class="n">Ithaca</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">Charlotte</span><span class="p">,</span><span class="w"> </span><span class="n">Departure</span><span class="w"> </span><span class="n">Time</span><span class="o">:</span><span class="w"> </span><span class="mo">05</span><span class="o">:</span><span class="mi">38</span><span class="p">,</span>
<span class="n">Arrival</span><span class="w"> </span><span class="n">Time</span><span class="o">:</span><span class="w"> </span><span class="mo">07</span><span class="o">:</span><span class="mi">46</span>
<span class="nl">Breakfast</span><span class="p">:</span><span class="w"> </span><span class="n">Nagaland</span><span class="err">&#39;</span><span class="n">s</span><span class="w"> </span><span class="n">Kitchen</span><span class="p">,</span><span class="w"> </span><span class="n">Charlotte</span>
<span class="nl">Attraction</span><span class="p">:</span><span class="w"> </span><span class="n">The</span><span class="w"> </span><span class="n">Charlotte</span><span class="w"> </span><span class="n">Museum</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">History</span><span class="p">,</span><span class="w"> </span><span class="n">Charlotte</span>
<span class="nl">Lunch</span><span class="p">:</span><span class="w"> </span><span class="n">Cafe</span><span class="w"> </span><span class="n">Maple</span><span class="w"> </span><span class="n">Street</span><span class="p">,</span><span class="w"> </span><span class="n">Charlotte</span>
<span class="nl">Dinner</span><span class="p">:</span><span class="w"> </span><span class="n">Bombay</span><span class="w"> </span><span class="n">Vada</span><span class="w"> </span><span class="n">Pav</span><span class="p">,</span><span class="w"> </span><span class="n">Charlotte</span>
<span class="nl">Accommodation</span><span class="p">:</span><span class="w"> </span><span class="n">Affordable</span><span class="w"> </span><span class="n">Spacious</span><span class="w"> </span><span class="n">Refurbished</span><span class="w"> </span><span class="n">Room</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="n">Bushwick</span><span class="o">!</span><span class="p">,</span><span class="w"> </span><span class="n">Charlotte</span>
<span class="n">Day</span><span class="w"> </span><span class="mi">2</span><span class="o">:</span>
<span class="n">Current</span><span class="w"> </span><span class="n">City</span><span class="o">:</span><span class="w"> </span><span class="n">Charlotte</span>
<span class="nl">Transportation</span><span class="p">:</span><span class="w"> </span><span class="o">-</span>
<span class="nl">Breakfast</span><span class="p">:</span><span class="w"> </span><span class="n">Olive</span><span class="w"> </span><span class="n">Tree</span><span class="w"> </span><span class="n">Cafe</span><span class="p">,</span><span class="w"> </span><span class="n">Charlotte</span>
<span class="nl">Attraction</span><span class="p">:</span><span class="w"> </span><span class="n">The</span><span class="w"> </span><span class="n">Mint</span><span class="w"> </span><span class="n">Museum</span><span class="p">,</span><span class="w"> </span><span class="n">Charlotte</span><span class="p">;</span><span class="n">Romare</span><span class="w"> </span><span class="n">Bearden</span><span class="w"> </span><span class="n">Park</span><span class="p">,</span><span class="w"> </span><span class="n">Charlotte</span><span class="p">.</span>
<span class="nl">Lunch</span><span class="p">:</span><span class="w"> </span><span class="n">Birbal</span><span class="w"> </span><span class="n">Ji</span><span class="w"> </span><span class="n">Dhaba</span><span class="p">,</span><span class="w"> </span><span class="n">Charlotte</span>
<span class="nl">Dinner</span><span class="p">:</span><span class="w"> </span><span class="n">Pind</span><span class="w"> </span><span class="n">Balluchi</span><span class="p">,</span><span class="w"> </span><span class="n">Charlotte</span>
<span class="nl">Accommodation</span><span class="p">:</span><span class="w"> </span><span class="n">Affordable</span><span class="w"> </span><span class="n">Spacious</span><span class="w"> </span><span class="n">Refurbished</span><span class="w"> </span><span class="n">Room</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="n">Bushwick</span><span class="o">!</span><span class="p">,</span><span class="w"> </span><span class="n">Charlotte</span>
</code></pre></div>

<p>Day 3:
Current City: Charlotte
Transportation: Flight Number: F3786167, from Charlotte to Ithaca, Departure Time: 21:42, Arrival Time: $23: 26$
Breakfast: Subway, Charlotte
Attraction: Books Monument, Charlotte.
Lunch: Olive Tree Cafe, Charlotte
Dinner: Kylin Skybar, Charlotte
Accommodation: -]
<strong><em>*</em> Example Ends </strong>***</p>
<p>You must use Finish to indict you have finished the task. And each action only calls one function once.
Given information: {text}
Query: {query}</p>
<h1>B.3.4. QuERY GENERATION PROMPT</h1>
<p>The instruction prompt for query generation is provided as follows:</p>
<div class="codehilite"><pre><span></span><code><span class="nx">Given</span><span class="w"> </span><span class="nx">a</span><span class="w"> </span><span class="nx">JSON</span><span class="p">,</span><span class="w"> </span><span class="nx">please</span><span class="w"> </span><span class="nx">help</span><span class="w"> </span><span class="nx">me</span><span class="w"> </span><span class="nx">generate</span><span class="w"> </span><span class="nx">a</span><span class="w"> </span><span class="nx">natural</span><span class="w"> </span><span class="nx">language</span><span class="w"> </span><span class="nx">query</span><span class="p">.</span><span class="w"> </span><span class="nx">In</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">JSON</span><span class="p">,</span><span class="w"> </span><span class="err">&#39;</span><span class="nx">org</span><span class="err">&#39;</span><span class="w"> </span><span class="nx">denotes</span>
<span class="w">    </span><span class="nx">the</span><span class="w"> </span><span class="nx">departure</span><span class="w"> </span><span class="nx">city</span><span class="p">.</span><span class="w"> </span><span class="nx">When</span><span class="w"> </span><span class="err">&#39;</span><span class="nx">days</span><span class="err">&#39;</span><span class="w"> </span><span class="nx">exceeds</span><span class="w"> </span><span class="mi">3</span><span class="p">,</span><span class="w"> </span><span class="err">&#39;</span><span class="nx">visiting_city_number</span><span class="err">&#39;</span><span class="w"> </span><span class="nx">specifies</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">number</span><span class="w"> </span><span class="nx">of</span>
<span class="w">    </span><span class="nx">cities</span><span class="w"> </span><span class="nx">to</span><span class="w"> </span><span class="nx">be</span><span class="w"> </span><span class="nx">covered</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">destination</span><span class="w"> </span><span class="nx">state</span><span class="p">.</span><span class="w"> </span><span class="nx">Here</span><span class="w"> </span><span class="nx">are</span><span class="w"> </span><span class="nx">three</span><span class="w"> </span><span class="nx">examples</span><span class="p">.</span>
<span class="o">-----</span><span class="nx">EXAMPLE</span><span class="w"> </span><span class="mi">1</span><span class="o">-----</span>
<span class="nx">JSON</span><span class="p">:</span>
<span class="p">{</span><span class="s">&quot;org&quot;</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;Gulfport&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;dest&quot;</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;Charlotte&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;days&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">3</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;visiting_city_number&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;date&quot;</span><span class="p">:</span>
<span class="p">[</span><span class="s">&quot;2022-03-05&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;2022-03-06&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;2022-03-07&quot;</span><span class="p">],</span><span class="w"> </span><span class="s">&quot;people_number&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;constraint&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="s">&quot;room rule</span>
<span class="s">&quot;</span><span class="p">:</span><span class="w"> </span><span class="nx">null</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;cuisine&quot;</span><span class="p">:</span><span class="w"> </span><span class="nx">null</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;room type&quot;</span><span class="p">:</span><span class="w"> </span><span class="nx">null</span><span class="p">},</span><span class="w"> </span><span class="s">&quot;budget&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">1800</span><span class="p">}</span>
<span class="nx">QUERY</span><span class="p">:</span>
<span class="nx">Please</span><span class="w"> </span><span class="nx">design</span><span class="w"> </span><span class="nx">a</span><span class="w"> </span><span class="nx">travel</span><span class="w"> </span><span class="nx">plan</span><span class="w"> </span><span class="nx">departing</span><span class="w"> </span><span class="nx">Gulfport</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="nx">heading</span><span class="w"> </span><span class="nx">to</span><span class="w"> </span><span class="nx">Charlotte</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="mi">3</span><span class="w"> </span><span class="nx">days</span><span class="p">,</span>
<span class="nx">spanning</span><span class="w"> </span><span class="nx">March</span><span class="w"> </span><span class="mi">5</span><span class="nx">th</span><span class="w"> </span><span class="nx">to</span><span class="w"> </span><span class="nx">March</span><span class="w"> </span><span class="mi">7</span><span class="nx">th</span><span class="p">,</span><span class="w"> </span><span class="mi">2022</span><span class="p">,</span><span class="w"> </span><span class="nx">with</span><span class="w"> </span><span class="nx">a</span><span class="w"> </span><span class="nx">budget</span><span class="w"> </span><span class="nx">of</span><span class="w"> </span><span class="err">$</span><span class="mi">1800</span><span class="p">.</span>
<span class="o">-----</span><span class="nx">EXAMPLE</span><span class="w"> </span><span class="mi">2</span><span class="o">-----</span>
<span class="nx">JSON</span><span class="p">:</span>
<span class="p">{</span><span class="s">&quot;org&quot;</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;Omaha&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;dest&quot;</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;Colorado&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;days&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">5</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;visiting_city_number&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">2</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;date&quot;</span><span class="p">:</span>
<span class="p">[</span><span class="s">&quot;2022-03-14&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;2022-03-15&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;2022-03-16&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;2022-03-17&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;2022-03-18&quot;</span><span class="p">],</span><span class="w"> </span><span class="s">&quot;people_number&quot;</span><span class="p">:</span>
<span class="mi">7</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;constraint&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="s">&quot;room rule&quot;</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;pets&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;cuisine&quot;</span><span class="p">:</span><span class="w"> </span><span class="nx">null</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;room type&quot;</span><span class="p">:</span><span class="w"> </span><span class="nx">null</span><span class="p">},</span><span class="w"> </span><span class="s">&quot;budget&quot;</span><span class="p">:</span>
<span class="mi">35300</span><span class="p">}</span>
<span class="nx">QUERY</span><span class="p">:</span>
<span class="nx">Could</span><span class="w"> </span><span class="nx">you</span><span class="w"> </span><span class="nx">provide</span><span class="w"> </span><span class="nx">a</span><span class="w"> </span><span class="mi">5</span><span class="o">-</span><span class="nx">day</span><span class="w"> </span><span class="nx">travel</span><span class="w"> </span><span class="nx">itinerary</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="nx">a</span><span class="w"> </span><span class="nx">group</span><span class="w"> </span><span class="nx">of</span><span class="w"> </span><span class="mi">7</span><span class="p">,</span><span class="w"> </span><span class="nx">starting</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="nx">Omaha</span><span class="w"> </span><span class="k">and</span>
<span class="nx">exploring</span><span class="w"> </span><span class="mi">2</span><span class="w"> </span><span class="nx">cities</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="nx">Colorado</span><span class="w"> </span><span class="nx">between</span><span class="w"> </span><span class="nx">March</span><span class="w"> </span><span class="mi">14</span><span class="nx">th</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="nx">March</span><span class="w"> </span><span class="mi">18</span><span class="nx">th</span><span class="p">,</span><span class="w"> </span><span class="mi">2022</span><span class="p">?</span><span class="w"> </span><span class="nx">Our</span><span class="w"> </span><span class="nx">budget</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="nx">set</span>
<span class="nx">at</span><span class="w"> </span><span class="err">$</span><span class="mi">35</span><span class="p">,</span><span class="mi">300</span><span class="p">,</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="nx">it</span><span class="err">&#39;</span><span class="nx">s</span><span class="w"> </span><span class="nx">essential</span><span class="w"> </span><span class="nx">that</span><span class="w"> </span><span class="nx">our</span><span class="w"> </span><span class="nx">accommodations</span><span class="w"> </span><span class="nx">be</span><span class="w"> </span><span class="nx">pet</span><span class="o">-</span><span class="nx">friendly</span><span class="w"> </span><span class="nx">since</span><span class="w"> </span><span class="nx">we</span><span class="err">&#39;</span><span class="nx">re</span>
<span class="nx">bringing</span><span class="w"> </span><span class="nx">our</span><span class="w"> </span><span class="nx">pets</span><span class="p">.</span>
<span class="o">-----</span><span class="nx">EXAMPLE</span><span class="w"> </span><span class="mi">3</span><span class="o">-----</span>
<span class="nx">JSON</span><span class="p">:</span>
<span class="p">{</span><span class="s">&quot;org&quot;</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;Indianapolis&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;dest&quot;</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;Georgia&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;days&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">7</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;visiting_city_number&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">3</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;date&quot;</span><span class="p">:</span>
<span class="p">[</span><span class="s">&quot;2022-03-01&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;2022-03-02&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;2022-03-03&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;2022-03-04&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;2022-03-05&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;2022-03-06&quot;</span><span class="p">,</span>
<span class="s">&quot;2022-03-07&quot;</span><span class="p">],</span><span class="w"> </span><span class="s">&quot;people_number&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">2</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;constraint&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="s">&quot;room rule&quot;</span><span class="p">:</span><span class="w"> </span><span class="nx">null</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;cuisine&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="s">&quot;Bakery&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s">&quot;Indian&quot;</span><span class="p">],</span><span class="w"> </span><span class="s">&quot;room type&quot;</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;entire room&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;transportation&quot;</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;self driving&quot;</span><span class="p">},</span><span class="w"> </span><span class="s">&quot;budget&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">6200</span><span class="p">}</span>
<span class="nx">QUERY</span><span class="p">:</span>
<span class="nx">I</span><span class="err">&#39;</span><span class="nx">m</span><span class="w"> </span><span class="nx">looking</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="nx">a</span><span class="w"> </span><span class="nx">week</span><span class="o">-</span><span class="nx">long</span><span class="w"> </span><span class="nx">travel</span><span class="w"> </span><span class="nx">itinerary</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="mi">2</span><span class="w"> </span><span class="nx">individuals</span><span class="p">.</span><span class="w"> </span><span class="nx">Our</span><span class="w"> </span><span class="nx">journey</span><span class="w"> </span><span class="nx">starts</span><span class="w"> </span><span class="k">in</span>
<span class="nx">Indianapolis</span><span class="p">,</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="nx">we</span><span class="w"> </span><span class="nx">intend</span><span class="w"> </span><span class="nx">to</span><span class="w"> </span><span class="nx">explore</span><span class="w"> </span><span class="mi">3</span><span class="w"> </span><span class="nx">distinct</span><span class="w"> </span><span class="nx">cities</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="nx">Georgia</span><span class="w"> </span><span class="nx">from</span><span class="w"> </span><span class="nx">March</span><span class="w"> </span><span class="mi">1</span><span class="nx">st</span><span class="w"> </span><span class="nx">to</span>
<span class="nx">March</span><span class="w"> </span><span class="mi">7</span><span class="nx">th</span><span class="p">,</span><span class="w"> </span><span class="mi">2022</span><span class="p">.</span><span class="w"> </span><span class="nx">Our</span><span class="w"> </span><span class="nx">budget</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="nx">capped</span><span class="w"> </span><span class="nx">at</span><span class="w"> </span><span class="err">$</span><span class="mi">6</span><span class="p">,</span><span class="mi">200</span><span class="p">.</span><span class="w"> </span><span class="nx">For</span><span class="w"> </span><span class="nx">our</span><span class="w"> </span><span class="nx">accommodations</span><span class="p">,</span><span class="w"> </span><span class="nx">we</span><span class="err">&#39;</span><span class="nx">d</span><span class="w"> </span><span class="nx">prefer</span><span class="w"> </span><span class="nx">an</span>
<span class="nx">entire</span><span class="w"> </span><span class="nx">room</span><span class="p">.</span><span class="w"> </span><span class="nx">We</span><span class="w"> </span><span class="nx">plan</span><span class="w"> </span><span class="nx">to</span><span class="w"> </span><span class="nx">navigate</span><span class="w"> </span><span class="nx">our</span><span class="w"> </span><span class="nx">journey</span><span class="w"> </span><span class="nx">via</span><span class="w"> </span><span class="kp">self</span><span class="o">-</span><span class="nx">driving</span><span class="p">.</span><span class="w"> </span><span class="nx">In</span><span class="w"> </span><span class="nx">terms</span><span class="w"> </span><span class="nx">of</span><span class="w"> </span><span class="nx">food</span><span class="p">,</span><span class="w"> </span><span class="nx">we</span><span class="err">&#39;</span><span class="nx">re</span>
<span class="nx">enthusiasts</span><span class="w"> </span><span class="nx">of</span><span class="w"> </span><span class="nx">bakery</span><span class="w"> </span><span class="nx">items</span><span class="p">,</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="nx">we</span><span class="err">&#39;</span><span class="nx">d</span><span class="w"> </span><span class="nx">also</span><span class="w"> </span><span class="nx">appreciate</span><span class="w"> </span><span class="nx">indulging</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="nx">genuine</span><span class="w"> </span><span class="nx">Indian</span><span class="w"> </span><span class="nx">cuisine</span><span class="p">.</span>
<span class="o">-----</span><span class="nx">EXAMPLES</span><span class="w"> </span><span class="nx">END</span><span class="o">-----</span>
<span class="nx">JSON</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="nx">json</span><span class="p">}</span>
<span class="nx">QUERY</span><span class="p">:</span>
</code></pre></div>

<h2>B.3.5. KEY COMPONENTS EXTRACTION PROMPT</h2>
<p>The instruction prompt for plan key components extraction is provided as follows:</p>
<div class="codehilite"><pre><span></span><code>Please<span class="w"> </span>assist<span class="w"> </span>me<span class="w"> </span>in<span class="w"> </span>extracting<span class="w"> </span>valid<span class="w"> </span>information<span class="w"> </span>from<span class="w"> </span>a<span class="w"> </span>given<span class="w"> </span>natural<span class="w"> </span>language<span class="w"> </span>text<span class="w"> </span>and
reconstructing<span class="w"> </span>it<span class="w"> </span>in<span class="w"> </span>JSON<span class="w"> </span>format,<span class="w"> </span>as<span class="w"> </span>demonstrated<span class="w"> </span>in<span class="w"> </span>the<span class="w"> </span>following<span class="w"> </span>example.<span class="w"> </span>Use<span class="w"> </span>a<span class="w"> </span>&#39;;&#39;<span class="w"> </span>to
separate<span class="w"> </span>different<span class="w"> </span>attractions,<span class="w"> </span>with<span class="w"> </span>each<span class="w"> </span>attraction<span class="w"> </span>formatted<span class="w"> </span>as<span class="w"> </span>&#39;Name,<span class="w"> </span>City&#39;.<span class="w"> </span>If<span class="w"> </span>there&#39;s
information<span class="w"> </span>about<span class="w"> </span>transportation,<span class="w"> </span>ensure<span class="w"> </span>that<span class="w"> </span>the<span class="w"> </span>&#39;current_city&#39;<span class="w"> </span>aligns<span class="w"> </span>with<span class="w"> </span>the
destination<span class="w"> </span>mentioned<span class="w"> </span>in<span class="w"> </span>the<span class="w"> </span>transportation<span class="w"> </span>details<span class="w"> </span>(i.e.,<span class="w"> </span>the<span class="w"> </span>current<span class="w"> </span>city<span class="w"> </span>should<span class="w"> </span>follow
the<span class="w"> </span>format<span class="w"> </span>&#39;from<span class="w"> </span>A<span class="w"> </span>to<span class="w"> </span>B&#39;).<span class="w"> </span>Also,<span class="w"> </span>ensure<span class="w"> </span>that<span class="w"> </span>all<span class="w"> </span>flight<span class="w"> </span>numbers<span class="w"> </span>and<span class="w"> </span>costs<span class="w"> </span>are<span class="w"> </span>followed<span class="w"> </span>by
a<span class="w"> </span>colon<span class="w"> </span>(i.e.,<span class="w"> </span>&#39;Flight<span class="w"> </span>Number:&#39;<span class="w"> </span>and<span class="w"> </span>&#39;Cost:&#39;),<span class="w"> </span>consistent<span class="w"> </span>with<span class="w"> </span>the<span class="w"> </span>provided<span class="w"> </span>example.<span class="w"> </span>Each
item<span class="w"> </span>should<span class="w"> </span>include<span class="w"> </span>[&#39;day&#39;,<span class="w"> </span>&#39;current_city&#39;,<span class="w"> </span>&#39;transportation&#39;,<span class="w"> </span>&#39;breakfast&#39;,<span class="w"> </span>&#39;attraction&#39;,<span class="w"> </span>&#39;
lunch&#39;,<span class="w"> </span>&#39;dinner&#39;,<span class="w"> </span>&#39;accommodation&#39;].<span class="w"> </span>Replace<span class="w"> </span>non-specific<span class="w"> </span>information<span class="w"> </span>like<span class="w"> </span>&#39;eat<span class="w"> </span>at<span class="w"> </span>home/on
the<span class="w"> </span>road&#39;<span class="w"> </span>with<span class="w"> </span>&#39;-&#39;.<span class="w"> </span>Additionally,<span class="w"> </span>delete<span class="w"> </span>any<span class="w"> </span>&#39;$&#39;<span class="w"> </span>symbols.
-----EXAMPLE-----
<span class="w">    </span>[{{
<span class="w">        </span>&quot;days&quot;:<span class="w"> </span>1,
<span class="w">        </span>&quot;current_city&quot;:<span class="w"> </span>&quot;from<span class="w"> </span>Dallas<span class="w"> </span>to<span class="w"> </span>Peoria&quot;,
<span class="w">        </span>&quot;transportation&quot;:<span class="w"> </span>&quot;Flight<span class="w"> </span>Number:<span class="w"> </span>4044830,<span class="w"> </span>from<span class="w"> </span>Dallas<span class="w"> </span>to<span class="w"> </span>Peoria,<span class="w"> </span>Departure<span class="w"> </span>Time:
<span class="w">        </span>13:10,<span class="w"> </span>Arrival<span class="w"> </span>Time:<span class="w"> </span>15:01&quot;,
<span class="w">        </span>&quot;breakfast&quot;:<span class="w"> </span>&quot;-&quot;,
<span class="w">        </span>&quot;attraction&quot;:<span class="w"> </span>&quot;Peoria<span class="w"> </span>Historical<span class="w"> </span>Society,<span class="w"> </span>Peoria;Peoria<span class="w"> </span>Holocaust<span class="w"> </span>Memorial,<span class="w"> </span>Peoria
<span class="w">        </span>;&quot;,
<span class="w">        </span>&quot;lunch&quot;:<span class="w"> </span>&quot;-&quot;,
<span class="w">        </span>&quot;dinner&quot;:<span class="w"> </span>&quot;Tandoor<span class="w"> </span>Ka<span class="w"> </span>Zaika,<span class="w"> </span>Peoria&quot;,
<span class="w">        </span>&quot;accommodation&quot;:<span class="w"> </span>&quot;Bushwick<span class="w"> </span>Music<span class="w"> </span>Mansion,<span class="w"> </span>Peoria&quot;
<span class="w">    </span>}},
<span class="w">    </span>{{
<span class="w">        </span>&quot;days&quot;:<span class="w"> </span>2,
<span class="w">        </span>&quot;current_city&quot;:<span class="w"> </span>&quot;Peoria&quot;,
<span class="w">        </span>&quot;transportation&quot;:<span class="w"> </span>&quot;-&quot;,
<span class="w">        </span>&quot;breakfast&quot;:<span class="w"> </span>&quot;Tandoor<span class="w"> </span>Ka<span class="w"> </span>Zaika,<span class="w"> </span>Peoria&quot;,
<span class="w">        </span>&quot;attraction&quot;:<span class="w"> </span>&quot;Peoria<span class="w"> </span>Riverfront<span class="w"> </span>Park,<span class="w"> </span>Peoria;The<span class="w"> </span>Peoria<span class="w"> </span>PlayHouse,<span class="w"> </span>Peoria;Glen
<span class="w">        </span>Oak<span class="w"> </span>Park,<span class="w"> </span>Peoria;&quot;,
<span class="w">        </span>&quot;lunch&quot;:<span class="w"> </span>&quot;Cafe<span class="w"> </span>Hashtag<span class="w"> </span>LoL,<span class="w"> </span>Peoria&quot;,
<span class="w">        </span>&quot;dinner&quot;:<span class="w"> </span>&quot;The<span class="w"> </span>Curzon<span class="w"> </span>Room<span class="w"> </span>-<span class="w"> </span>Maidens<span class="w"> </span>Hotel,<span class="w"> </span>Peoria&quot;,
<span class="w">        </span>&quot;accommodation&quot;:<span class="w"> </span>&quot;Bushwick<span class="w"> </span>Music<span class="w"> </span>Mansion,<span class="w"> </span>Peoria&quot;
<span class="w">    </span>}},
<span class="w">    </span>{{
<span class="w">        </span>&quot;days&quot;:<span class="w"> </span>3,
<span class="w">        </span>&quot;current_city&quot;:<span class="w"> </span>&quot;from<span class="w"> </span>Peoria<span class="w"> </span>to<span class="w"> </span>Dallas&quot;,
<span class="w">        </span>&quot;transportation&quot;:<span class="w"> </span>&quot;Flight<span class="w"> </span>Number:<span class="w"> </span>4045904,<span class="w"> </span>from<span class="w"> </span>Peoria<span class="w"> </span>to<span class="w"> </span>Dallas,<span class="w"> </span>Departure<span class="w"> </span>Time:
<span class="w">        </span>07:09,<span class="w"> </span>Arrival<span class="w"> </span>Time:<span class="w"> </span>09:20&quot;,
<span class="w">        </span>&quot;breakfast&quot;:<span class="w"> </span>&quot;-&quot;,
<span class="w">        </span>&quot;attraction&quot;:<span class="w"> </span>&quot;-&quot;,
<span class="w">        </span>&quot;lunch&quot;:<span class="w"> </span>&quot;-&quot;,
<span class="w">        </span>&quot;dinner&quot;:<span class="w"> </span>&quot;-&quot;,
<span class="w">        </span>&quot;accommodation&quot;:<span class="w"> </span>&quot;-&quot;
<span class="w">    </span>}}]
-----EXAMPLE<span class="w"> </span>ENDS-----
Text:<span class="w"> </span>{text}
JSON:
</code></pre></div>

<h1>C. Case Presentation</h1>
<h2>C.1. Example of Query and Reference Plan</h2>
<p>we present an example of a query and its corresponding reference plan in our train set as follows:</p>
<div class="codehilite"><pre><span></span><code>{
    &quot;org&quot;: &quot;Indianapolis&quot;,
    &quot;dest&quot;: &quot;Colorado&quot;,
    &quot;days&quot;: 7,
    &quot;visiting_city_number&quot;: 3,
    &quot;date&quot;: [
        &quot;2022-03-11&quot;,
        &quot;2022-03-12&quot;,
</code></pre></div>

<div class="codehilite"><pre><span></span><code><span class="w">    </span><span class="s">&quot;2022-03-13&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s">&quot;2022-03-14&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s">&quot;2022-03-15&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s">&quot;2022-03-16&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s">&quot;2022-03-17&quot;</span>
<span class="p">],</span>
<span class="s">&quot;people_number&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">5</span><span class="p">,</span>
<span class="s">&quot;room rule&quot;</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;pets&quot;</span><span class="p">,</span>
<span class="s">&quot;cuisine&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span>
<span class="w">    </span><span class="s">&quot;Mexican&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s">&quot;Italian&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s">&quot;Mediterranean&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s">&quot;Indian&quot;</span>
<span class="p">],</span>
<span class="s">&quot;room type&quot;</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;entire room&quot;</span><span class="p">,</span>
<span class="s">&quot;transportation&quot;</span><span class="p">:</span><span class="w"> </span><span class="nx">null</span><span class="p">,</span>
<span class="s">&quot;budget&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">15100</span><span class="p">,</span>
<span class="s">&quot;query&quot;</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;Can you help with generating a 7-day travel plan for a party of 5? We&#39;re</span>
<span class="s">setting off from Indianapolis and planning to explore 3 cities in Colorado from March</span>
<span class="s">11th to March 17th, 2022. We have a budget of $15,100 for this trip. We&#39;ll be bringing</span>
<span class="s">    our pets, so pet-friendly accommodations are a must. We&#39;re also hoping to find places</span>
<span class="s">    that offer Mexican, Italian, Mediterranean, and Indian cuisines. Entire rooms for</span>
<span class="s">accommodations would be ideal.&quot;</span><span class="p">,</span>
<span class="s">&quot;level&quot;</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;hard&quot;</span><span class="p">,</span>
<span class="s">&quot;annotated plan&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span>
<span class="w">    </span><span class="p">{</span>
<span class="w">        </span><span class="s">&quot;days&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span>
<span class="w">        </span><span class="s">&quot;current_city&quot;</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;from Indianapolis to Grand Junction(Colorado)&quot;</span><span class="p">,</span>
<span class="w">        </span><span class="s">&quot;transportation&quot;</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;Self-driving, from Indianapolis to Grand Junction(Colorado)</span>
<span class="s">        , duration: 19 hours 21 mins, distance: 2,132 km, cost: 106&quot;</span><span class="p">,</span>
<span class="w">        </span><span class="s">&quot;breakfast&quot;</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;-&quot;</span><span class="p">,</span>
<span class="w">        </span><span class="s">&quot;attraction&quot;</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;-&quot;</span><span class="p">,</span>
<span class="w">        </span><span class="s">&quot;lunch&quot;</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;-&quot;</span><span class="p">,</span>
<span class="w">        </span><span class="s">&quot;dinner&quot;</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;Nukkadwala, Grand Junction(Colorado)&quot;</span><span class="p">,</span>
<span class="w">        </span><span class="s">&quot;accommodation&quot;</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;Lovely 1 BD on the Upper West Side, Grand Junction(Colorado)</span>
<span class="s">        &quot;</span>
<span class="w">    </span><span class="p">},</span>
<span class="w">    </span><span class="s">&quot;days&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">2</span><span class="p">,</span>
<span class="w">    </span><span class="s">&quot;current_city&quot;</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;Grand Junction(Colorado)&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s">&quot;transportation&quot;</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;-&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s">&quot;breakfast&quot;</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;Om Ji Bhature Wale, Grand Junction(Colorado)&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s">&quot;attraction&quot;</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;Museum of the West, Museums of Western Colorado, Grand Junction</span>
<span class="s">    (Colorado);Eureka! McConnell Science Museum, Grand Junction(Colorado);&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s">&quot;lunch&quot;</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;Penta Cafe, Grand Junction(Colorado)&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s">&quot;dinner&quot;</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;Kings Kulfi, Grand Junction(Colorado)&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s">&quot;accommodation&quot;</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;Lovely 1 BD on the Upper West Side, Grand Junction(Colorado)</span>
<span class="s">    &quot;</span>
<span class="w">    </span><span class="p">},</span>
<span class="w">    </span><span class="s">&quot;days&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">3</span><span class="p">,</span>
<span class="w">    </span><span class="s">&quot;current_city&quot;</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;from Grand Junction(Colorado) to Alamosa(Colorado)&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s">&quot;transportation&quot;</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;Self-driving, from Grand Junction(Colorado) to Alamosa(</span>
<span class="s">    Colorado), duration: 4 hours 37 mins, distance: 397 km, cost: 19&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s">&quot;breakfast&quot;</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;Punjab Da Pind, Grand Junction(Colorado)&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s">&quot;attraction&quot;</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;Alamosa Colorado Welcome Center, Alamosa(Colorado);Toivo Malm</span>
<span class="s">    Trail System, Alamosa(Colorado);&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s">&quot;lunch&quot;</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;Emperor&#39;s Lounge - The Taj Mahal Hotel, Alamosa(Colorado)&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s">&quot;dinner&quot;</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;Cafe Dalal Street, Alamosa(Colorado)&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s">&quot;accommodation&quot;</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;Sunny Chelsea Studio, Alamosa(Colorado)&quot;</span>
<span class="w">    </span><span class="p">},</span>
<span class="w">    </span><span class="s">&quot;days&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">4</span><span class="p">,</span>
<span class="w">    </span><span class="s">&quot;current_city&quot;</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;Alamosa(Colorado)&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s">&quot;transportation&quot;</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;-&quot;</span><span class="p">,</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code><span class="w">            </span><span class="s">&quot;breakfast&quot;</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;Good Luck Cafe, Alamosa(Colorado)&quot;</span><span class="p">,</span>
<span class="w">            </span><span class="s">&quot;attraction&quot;</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;Alamosa Archery Range, Alamosa(Colorado);Alamosa Riparian Park,</span>
<span class="s">            Alamosa(Colorado);Alamosa Sub, Alamosa(Colorado);&quot;</span><span class="p">,</span>
<span class="w">            </span><span class="s">&quot;lunch&quot;</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;Shri Durga Dosa Corner, Alamosa(Colorado)&quot;</span><span class="p">,</span>
<span class="w">            </span><span class="s">&quot;dinner&quot;</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;Lahore, Alamosa(Colorado)&quot;</span><span class="p">,</span>
<span class="w">            </span><span class="s">&quot;accommodation&quot;</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;Sunny Chelsea Studio, Alamosa(Colorado)&quot;</span>
<span class="w">    </span><span class="p">},</span>
<span class="w">    </span><span class="p">{</span>
<span class="w">        </span><span class="s">&quot;days&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">5</span><span class="p">,</span>
<span class="w">        </span><span class="s">&quot;current_city&quot;</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;from Alamosa(Colorado) to Denver(Colorado)&quot;</span><span class="p">,</span>
<span class="w">            </span><span class="s">&quot;transportation&quot;</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;Self-driving, from Alamosa(Colorado) to Denver(Colorado),</span>
<span class="s">            duration: 3 hours 38 mins, distance: 377 km, cost: 18&quot;</span><span class="p">,</span>
<span class="w">            </span><span class="s">&quot;breakfast&quot;</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;Hamburg To Hyderabad, Alamosa(Colorado)&quot;</span><span class="p">,</span>
<span class="w">            </span><span class="s">&quot;attraction&quot;</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;Denver Zoo, Denver(Colorado);&quot;</span><span class="p">,</span>
<span class="w">            </span><span class="s">&quot;lunch&quot;</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;The Fatty Bao ~ Asian Gastro Bar, Denver(Colorado)&quot;</span><span class="p">,</span>
<span class="w">            </span><span class="s">&quot;dinner&quot;</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;Woods Spice, Denver(Colorado)&quot;</span><span class="p">,</span>
<span class="w">            </span><span class="s">&quot;accommodation&quot;</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;Peaceful, beautiful home away , Denver(Colorado)&quot;</span>
<span class="w">    </span><span class="p">},</span>
<span class="w">    </span><span class="p">{</span>
<span class="w">        </span><span class="s">&quot;days&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">6</span><span class="p">,</span>
<span class="w">        </span><span class="s">&quot;current_city&quot;</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;Denver(Colorado)&quot;</span><span class="p">,</span>
<span class="w">        </span><span class="s">&quot;transportation&quot;</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;-&quot;</span><span class="p">,</span>
<span class="w">        </span><span class="s">&quot;breakfast&quot;</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;The Urban Socialite, Denver(Colorado)&quot;</span><span class="p">,</span>
<span class="w">        </span><span class="s">&quot;attraction&quot;</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;Denver Art Museum, Denver(Colorado);Denver Museum of Nature &amp;</span>
<span class="s">        Science, Denver(Colorado);&quot;</span><span class="p">,</span>
<span class="w">        </span><span class="s">&quot;lunch&quot;</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;Breaktym, Denver(Colorado)&quot;</span><span class="p">,</span>
<span class="w">        </span><span class="s">&quot;dinner&quot;</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;Chawla&#39;s\u5b8a, Denver(Colorado)&quot;</span><span class="p">,</span>
<span class="w">        </span><span class="s">&quot;accommodation&quot;</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;Peaceful, beautiful home away , Denver(Colorado)&quot;</span>
<span class="w">    </span><span class="p">},</span>
<span class="w">    </span><span class="p">{</span>
<span class="w">        </span><span class="s">&quot;days&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">7</span><span class="p">,</span>
<span class="w">        </span><span class="s">&quot;current_city&quot;</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;from Denver(Colorado) to Indianapolis&quot;</span><span class="p">,</span>
<span class="w">        </span><span class="s">&quot;transportation&quot;</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;Self-driving, from Denver(Colorado) to Indianapolis,</span>
<span class="s">        duration: 15 hours 37 mins, distance: 1,747 km, cost: 87&quot;</span><span class="p">,</span>
<span class="w">        </span><span class="s">&quot;breakfast&quot;</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;Starve Stalkers, Denver(Colorado)&quot;</span><span class="p">,</span>
<span class="w">        </span><span class="s">&quot;attraction&quot;</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;-&quot;</span><span class="p">,</span>
<span class="w">        </span><span class="s">&quot;lunch&quot;</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;-&quot;</span><span class="p">,</span>
<span class="w">        </span><span class="s">&quot;dinner&quot;</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;-&quot;</span><span class="p">,</span>
<span class="w">        </span><span class="s">&quot;accommodation&quot;</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;-&quot;</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">    </span><span class="p">]</span>
<span class="p">}</span>
</code></pre></div>

<h1>C.2. Additional Case Study</h1>
<p>Query: Please draw up a 3-day travel itinerary for one person, beginning in Oakland and heading to Tucson from March 15th to March 17th, 2022, with a budget of $\$ 1,400$.</p>
<h1>Plan:</h1>
<p>${$
"day": 3,
"current city": "Tucson"
"transportation": "-",
"breakfast": "Bakers Oven, Tucson",
"attraction": "San Xavier del Bac Mission, Tucson;
Tucson Museum Of Art, Tucson",
"lunch": "Pizza Street, Tucson",
"dinner": "-"
"accommodation": "-"
}</p>
<h2>Analysis:</h2>
<p>In this instance, the planned trip did not form a closed-loop trip, ending in Tucson on the third day. Additionally, despite the trip including a stay in Tucson, the language agent failed to schedule dinner or accommodation for that day.</p>
<p>Figure C.1. Case 4 (Two-stage Mode).</p>
<p>Query: Can you create a 5-day travel plan for 2 people departing from Syracuse to visit 2 cities in Georgia? We are planning to travel from March 16th to March 20th, 2022. Our budget is approximately $\$ 2,000$. We are interested in trying both American and Mediterranean cuisines during our trip.</p>
<h2>Trajectory:</h2>
<p>Action 13: FlightSearch[Syracuse, Atlanta, 2023-03-20]
Thought 14: Since I have exhausted the options for finding flights from Syracuse to any city in Georgia on the desired dates, and I am unable to search for alternative transportation methods due to the nested use of tools restriction, the only remaining step is to use the Planner tool with the information that has been gathered so far. Action 14: Planner[Create a 5-day travel plan from Syracuse to Georgia]</p>
<h2>Plan:</h2>
<p>Day 1:
transportation: Flight Number: F1234567, from Syracuse toAtlanta</p>
<h2>Analysis:</h2>
<p>In this example, the language agent's persistent date errors and fixation on airplane travel lead it to bypass valid information search. Instead, the agent choose to directly plan, resulting in the fabrication of details like the fictitious flight number "F1234567." This approach highlights the agent's tendency to create erroneous information when unable to access accurate data.</p>
<p>Figure C.2. Case 5 (Two-stage mode).</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{3}$ https://developers.google.com/maps/documentation/distance-matrix/overview?hl=en
${ }^{4}$ https://www.kaggle.com/datasets/shrutimehta/zomato-restaurants-data
${ }^{5}$ https://developers.google.com/maps/documentation/places/web-service/overview?hl=en
${ }^{6}$ https://www.kaggle.com/datasets/arianazmoudeh/airbnbopendata&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
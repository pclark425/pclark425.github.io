<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7349 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7349</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7349</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-139.html">extraction-schema-139</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being used to detect anomalies in lists or tabular data, including the methods, datasets, evaluation metrics, and results.</div>
                <p><strong>Paper ID:</strong> paper-276409129</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2502.11546v5.pdf" target="_blank">DCAD-2000: A Multilingual Dataset across 2000+ Languages with Data Cleaning as Anomaly Detection</a></p>
                <p><strong>Paper Abstract:</strong> The rapid development of multilingual large language models (LLMs) highlights the need for high-quality, diverse, and well-curated multilingual datasets. In this paper, we introduce DCAD-2000 (Data Cleaning as Anomaly Detection), a large-scale multilingual corpus constructed from newly extracted Common Crawl data and existing multilingual sources. DCAD-2000 covers 2,282 languages, 46.72TB of text, and 8.63 billion documents, spanning 155 high- and medium-resource languages and 159 writing scripts. To overcome the limitations of existing data cleaning approaches, which rely on manually designed heuristic thresholds, we reframe data cleaning as an anomaly detection problem. This dynamic filtering paradigm substantially improves data quality by automatically identifying and removing noisy or anomalous content. By fine-tuning LLMs on DCAD-2000, we demonstrate notable improvements in data quality, robustness of the cleaning pipeline, and downstream performance, particularly for low-resource languages across multiple multilingual benchmarks.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7349",
    "paper_id": "paper-276409129",
    "extraction_schema_id": "extraction-schema-139",
    "extracted_data": [],
    "potentially_relevant_new_papers": [],
    "cost": 0.01475,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>DCAD-2000: A Multilingual Dataset across 2000+ Languages with Data Cleaning as Anomaly Detection
24 Oct 2025</p>
<p>Yingli Shen 
Dept. of Comp. Sci. &amp; Tech
Tsinghua University</p>
<p>Wen Lai wen.lai@tum.de 
Technical University of Munich</p>
<p>Munich Center for Machine Learning</p>
<p>Shuo Wang 
Dept. of Comp. Sci. &amp; Tech
Tsinghua University</p>
<p>Xueren Zhang 
ModelBest Inc</p>
<p>Kangyang Luo 
Dept. of Comp. Sci. &amp; Tech
Tsinghua University</p>
<p>Alexander Fraser 
Technical University of Munich</p>
<p>Munich Center for Machine Learning</p>
<p>Maosong Sun 
Dept. of Comp. Sci. &amp; Tech
Tsinghua University</p>
<p>BNRist Center
Institute for AI
Tsinghua University</p>
<p>Jiangsu Collaborative Innovation Center for Language Ability
Jiangsu Normal University</p>
<p>DCAD-2000: A Multilingual Dataset across 2000+ Languages with Data Cleaning as Anomaly Detection
24 Oct 20258DB43FE421F6B75C3BF0F5C640DF7ED6arXiv:2502.11546v5[cs.CL]
The rapid development of multilingual large language models (LLMs) highlights the need for high-quality, diverse, and well-curated multilingual datasets.In this paper, we introduce DCAD-2000 (Data Cleaning as Anomaly Detection), a largescale multilingual corpus constructed from newly extracted Common Crawl data and existing multilingual sources.DCAD-2000 covers 2,282 languages, 46.72TB of text, and 8.63 billion documents, spanning 155 high-and medium-resource languages and 159 writing scripts.To overcome the limitations of existing data cleaning approaches, which rely on manually designed heuristic thresholds, we reframe data cleaning as an anomaly detection problem.This dynamic filtering paradigm substantially improves data quality by automatically identifying and removing noisy or anomalous content.By fine-tuning LLMs on DCAD-2000, we demonstrate notable improvements in data quality, robustness of the cleaning pipeline, and downstream performance, particularly for low-resource languages across multiple multilingual benchmarks.</p>
<p>Introduction</p>
<p>Large language models (LLMs) have achieved great progress on a variety of NLP tasks by leveraging vast amounts of training data [1].However, their performance remains heavily biased towards highresource languages [2,3].To improve the multilingual capabilities of LLMs, a common strategy is to incorporate large amounts of non-English data, either by continue pretraining [4] or by instruction tuning in multilingual settings [5].Therefore, constructing large-scale, high-quality multilingual datasets is crucial for enhancing the multilingual performance of LLMs.</p>
<p>Recent efforts have introduced several large multilingual corpora, including CulturaX [9], HPLT [13], Madlad-400 [10], MaLA [15], and Glotcc [12], which cover 167, 191, 419, 939, and 1,331 languages, respectively.While these datasets have made significant contributions, they exhibit three major limitations, as summarized in Table 1: (1) Outdated data sources: These datasets primarily rely on older Common Crawl snapshots 1 , which results in outdated knowledge and an elevated risk of hallucination [16].</p>
<p>(2) Limited coverage of high-and medium-resource languages2 : For instance, Fineweb-2 [14], despite supporting 1,915 languages, contains data from only 10 high-resource and 62 medium-resource languages.</p>
<p>(3) Insufficient data cleaning: Despite being cleaned, recent studies [18,19] indicate that these datasets still contain a significant amount of noise, which makes them difficult to directly employ in training multilingual LLMs.For example, Sailor [18] reports that 31.11% of Madlad-400 data could still be removed using more advanced cleaning.</p>
<p>Traditional data cleaning workflows [20] often rely on document-level heuristics (e.g., language identification; 21) and fixed thresholds to filter low-quality samples.However, these heuristic thresholds often fail to generalize across languages due to distributional differences in features such as word count, repetition ratios, and perplexity 3 .Notably, while Fineweb-2 fine-tunes thresholds for more than 1,000 languages, this process is computationally intensive and time-consuming.</p>
<p>To address these challenges, we introduce DCAD-2000, a new large-scale, high-quality multilingual dataset that can be directly applied to LLM training.DCAD-2000 covers 2282 languages (155 high/medium languages), incorporating the latest Common Crawl data (November 2024; CC-MAIN-2024-46) and existing multilingual datasets.Additionally, we propose a novel language-agnostic data cleaning approach that treats data cleaning as an anomaly detection [22] problem, distinguishing it from traditional threshold-based methods [14,23].Our approach extracts eight statistical features, including number of words, character/word repetition ratio, special character/word ratio, stopword ratio, flagged words ratio, language identification score and perplexity score.</p>
<p>Anomaly detection algorithms dynamically identify and remove outliers by recognizing deviations from typical document quality metrics.</p>
<p>We conduct a comprehensive analysis of DCAD-2000 with respect to document distribution, linguistic and geographical characteristics, writing scripts, and resource classification (Section 4).By finetuning LLMs on DCAD-2000, we validate the effectiveness of its data quality and data cleaning pipeline.Furthermore, we demonstrate the superiority of DCAD-2000 across various language categories (high, medium, low and very low) in multiple multilingual benchmarks, including SIB-200 [24], Glot500 [8] and FLORES-200 [25] (Section 5).</p>
<p>In summary, we make the following contributions:</p>
<p>• We propose a novel data cleaning framework that frames the task as anomaly detection, offering a language-agnostic and adaptive solution without manual threshold tuning.</p>
<p>• We release DCAD-2000, a comprehensive multilingual dataset covering over 2,282 languages, containing 8.63B of documents, 46.72TB of disk size and 159 writing scripts with metadata annotations.</p>
<p>• Extensive evaluation across multiple multilingual benchmarks demonstrates the effectiveness of both the data quality and the data cleaning pipeline.</p>
<p>Related Work</p>
<p>Multilingual Dataset for Pretraining.Enhancing the multilingual capabilities of LLMs often involves continuing pretraining on large-scale multilingual datasets [11,15].These datasets can be broadly categorized into curated corpora, domain-specific corpora, and web-crawled corpora.(I) Curated Corpora.Curated datasets are carefully gathered by experts from high-quality sources such as books [23], academic publications [26], and encyclopedia entries [27,28,29].(II) Domain-Specific Corpora.In addition to general-domain data, fine-tuning LLMs on domain-specific datasets is crucial for improving performance in specialized domains like finance [30], healthcare [31], legal [32], and education [33,34].(III) Web-Crawled Corpora.Web-crawled datasets, particularly those derived from Common Crawl, provide large-scale multilingual coverage by leveraging an open repository of over 250 billion web pages.These datasets include mC4 [6], CC-100 [35], OSCAR [7], Glotcc [12], Fineweb [36], and Fineweb-2 [14].While curated and domain-specific corpora offer high-quality content with limited language coverage, web-crawled corpora provide broader multilingual coverage but often suffer from noise and lower data quality [18,19].</p>
<p>Data Cleaning.Data cleaning is an essential step in preparing high-quality datasets for training robust LLMs.It involves filtering noisy, irrelevant, or harmful content and can be broadly classified into model-based and heuristic-based approaches [37].(I) Model-Based Methods.Model-based approaches employ classifiers or LLMs to distinguish between high-quality and low-quality data.</p>
<p>For instance, content safety models [38] filter out explicit or gambling-related content, while quality classifiers remove low-relevance text [39].LLM-based methods focus on generating prompts for cleaning [40] or integrating error detection and correction into the pipeline [41,42].(II) Heuristic-Based Methods.Heuristic approaches apply predefined rules to filter content at both document and sentence levels.At the document level, strategies include filtering by language identification scores or scoring documents with language models [9,23].At the sentence level, rules are applied to remove incomplete or irrelevant content, such as HTML tags or excessively short sentences [6,7].While model-based methods offer high precision but face scalability challenges, heuristic-based methods are more efficient yet less adaptable to diverse multilingual data.</p>
<p>DCAD-2000</p>
<p>To overcome the limitations of existing multilingual datasets, we introduce DCAD-2000, a large-scale, high-quality multilingual dataset constructed by integrating data from latest version of Common Crawl and existing multilingual datasets (Section 3.1).This dataset is cleaned using our proposed novel framework, which treats data cleaning as an anomaly detection problem (Section 3.2).The construction of DCAD-2000 is supported by robust computational resources, as detailed in Section 3.3.</p>
<p>Data Collection</p>
<p>To ensure comprehensive and robust multilingual data representation, DCAD-2000 integrates data from four main sources: MaLA, Fineweb, Fineweb-2, and newly extracted Common Crawl data.Each source is selected based on its unique contribution to multilingual coverage, data quality, and freshness, with careful consideration to complementarity to minimize redundancy.Specifically, MaLA and Fineweb-2 are prioritized due to their broad language coverage and high-quality curation, which complements other widely used datasets like mC4 [6] and OSCAR [7].</p>
<p>MaLA Corpus [11].The MaLA corpus covers 939 languages, aggregating data from diverse sources including Bloom [43], CC100 [35], Glot500 [8], among others.Deduplication is performed using MinHashLSH [44], which is particularly effective in removing near-duplicate entries that often arise from common web sources.Language codes are based on ISO 639-3 4 standards, and language-specific scripts are supported by GlotScript5 .</p>
<p>Fineweb Corpus [36].Fineweb is a high-quality English web dataset extracted from Common Crawl, consisting of over 15 trillion tokens and updated monthly.Data cleaning and deduplication are performed using the Datatrove library.).Using the Fineweb-2 pipeline 7 , we process 21.54TB of multilingual data, ensuring that the data remains fresh and suitable for downstream tasks.This further extends the multilingual data pool and enhances the coverage across underrepresented languages.</p>
<p>Data Cleaning as Anomaly Detection</p>
<p>Traditional data cleaning methods rely on fixed thresholds for document-level features, making them less adaptable to the diversity of multilingual data.To address this, we propose a novel framework that formulates data cleaning as an anomaly detection task, which involves feature extraction (Section 3.2.1)and anomaly detection (Section 3.2.2).</p>
<p>Feature Extraction</p>
<p>Inspired by Roots [23] and CulturaX [9], we extract eight statistical features from each document to evaluate text quality.Each feature is selected for its ability to capture important characteristics of the text, contributing to robust anomaly detection.Let t represent a document; the extracted features are:</p>
<p>• Number of Words, n w (t): Total number of tokens after language-specific tokenization, providing a coarse measure of document length and helping identify extremely short or excessively long outliers.• Character Repetition Ratio, r c (t): Fraction of repeated character sequences (e.g., "aaaaa"</p>
<p>or "!!!!!"), which often signal encoding artifacts, copy-paste errors, or spam-like content.• Word Repetition Ratio, r w (t): Proportion of repeated lexical items, useful for detecting low-information documents that exhibit looping or template-like patterns.• Special Characters Ratio, r s (t): Fraction of characters belonging to special symbol categories.We employ the curated language-specific symbol lists provided in the ROOTs Corpus [23], covering punctuation, numeric symbols, whitespace variants, and emojis.A high r s (t) may indicate adversarial inputs or unstructured noise.• Stopwords Ratio, r stop (t): Ratio of stopwords derived from Fineweb-2's multilingual stopword lexicons.This metric captures the functional-to-content word balance, offering a lightweight approximation of linguistic naturalness.• Flagged Words Ratio, r flag (t): Fraction of tokens that appear in curated lists of toxic or profane vocabulary such as Toxicity-200 [25] and community-maintained sources 8 .This feature enables early detection of harmful or sensitive content.• Language Identification (LID) Score, s lid (t): Confidence score produced by GlotLID [21], a language identifier supporting over 2,000 languages.Lower scores may indicate codeswitching, mislabeling, or mixed-script anomalies.• Perplexity Score, s ppl (t): We compute a language model perplexity score using KenLM [45] models trained per language on the November 2023 snapshot of multilingual Wikipedia 9 .This feature provides a lightweight proxy for linguistic fluency.</p>
<p>The feature vector for each document is defined as:</p>
<p>x = [n w (t), r c (t), r w (t), r s (t), r stop (t), r flag (t), s lid (t), s ppl (t)] ⊤ ∈ R 8 .</p>
<p>(1)</p>
<p>Anomaly Detection</p>
<p>After extracting feature vectors x ∈ R 8 , we standardize each feature to handle differences in scale.</p>
<p>The standardized value xj for the j-th feature is given by:
xj = x j − µ j σ j , j = 1, . . . , 8.(2)
where µ j and σ j are the mean and standard deviation of the j-th feature across the dataset.The standardized feature vector is:
x = x − µ σ .(3)
where µ = [µ 1 , µ 2 , . . ., µ 8 ] ⊤ and σ = [σ 1 , σ 2 , . . ., σ 8 ] ⊤ are the vectors of means and standard deviations, respectively.</p>
<p>Take Isolation Forest [46] as an example 10 , we compute an anomaly score ϕ(x) for each document.</p>
<p>The Isolation Forest algorithm assigns anomaly scores based on the average path length required to isolate a data point in a decision tree.Specifically, for a document represented by x, the anomaly score is defined as:
ϕ(x) = 2 − h(x) c(n) .(4
) where h(x) is the average path length for x across all trees in the Isolation Forest, and c(n) is the average path length of a point in a binary tree with n samples, given by:
c(n) = 2H(n − 1) − 2(n − 1) n .(5)
where H(i) is the i-th harmonic number, defined as
H(i) = i k=11
k .An anomaly score ϕ(x) : R 8 → R is defined to quantify how far a document deviates from typical data.Higher scores indicate a higher likelihood of anomalies.To classify a document, we use the decision rule:
f (x) = 1, if ϕ(x) &lt; τ, −1, if ϕ(x) ≥ τ. (6)
where τ ∈ R is a hyperparameter determined empirically or through cross-validation. 11nce the anomaly scores ϕ(x) are computed for all samples in the standardized dataset X = {x 1 , . . ., xN }, we partition the dataset into two subsets:
X keep = {x ∈ X : f (x) = 1},(7)X remove = {x ∈ X : f (x) = −1}.(8)
Following anomaly detection, the dataset is partitioned into a clean subset X keep and an anomalous subset X remove .The former is retained for downstream tasks such as model training, while the latter may be discarded or further examined for potential data quality issues.</p>
<p>Visualization</p>
<p>To qualitatively evaluate the separation achieved by our data cleaning framework, we present scatter plots of the eight feature dimensions in Figure 1, with data points color-coded by their anomaly labels.These visualizations facilitate the interpretation of decision boundaries and highlight the features that contribute most significantly to the detection process.We observe well-defined clusters separating anomalous and non-anomalous data points, with anomalies exhibiting distinct patterns compared to the majority of the data.Features such as the language identification score (s lid (t)) and perplexity score (s ppl (t)) are expected to be particularly discriminative in identifying anomalies, as they capture linguistic irregularities and unexpected text patterns.For example, low lid or unusually high ppl scores often indicate problematic text, such as spam, low-quality content, or noise.The framework effectively identifies and removes such low-quality text samples, which can be easily visualized by the separation of these points in the scatter plots.</p>
<p>Computational Resources</p>
<p>The construction of the DCAD-2000 dataset leveraged Ksyun servers 12 to process and clean the multilingual data efficiently.Each server instance is equipped with 32 CPU cores, 128GB of memory, and 100GB of disk storage, which is utilized for intermediate data handling and memory-intensive operations such as anomaly detection.The workload is managed using container orchestration tools, Kubernetes 13 , with up to 100 parallel tasks running per job to ensure scalability.</p>
<p>Dataset Analysis</p>
<p>In this section, we analyze the characteristics of DCAD-2000, focusing on document distribution across sources, geographic and script coverage, resource categorization of languages, and the effect of data cleaning on dataset size and quality.</p>
<p>Document</p>
<p>Distribution Across Data Sources.The DCAD-2000 dataset is derived from four primary sources: MaLA, Fineweb, Fineweb-2, and Newly Extracted Common Crawl data (New CC), as described in Section 3.1.Figure 2a presents the distribution of documents across these sources, with Fineweb-2 and New CC collectively contributing 47.5% and 39.3% of the total dataset, respectively.These two sources play a significant role in ensuring the dataset's emphasis on both language diversity (Fineweb-2) and corpus freshness (New CC).MaLA, though contributing 11.1% of the total dataset, brings in valuable content from non-Common Crawl sources, further enriching the diversity of the dataset, especially for low-resource languages.Geographical Coverage of Languages.Figure 2b shows the geographical distribution of languages in DCAD-2000, based on the number of unique languages available in each region, as classified by Glottolog. 14The dataset spans languages from all major world regions, with the largest proportions originating from Africa (28.6%),Papunesia (26.3%) and Eurasia (23.8%).This coverage ensures robust support for multilingual applications across varied regional contexts, including densely populated areas like Eurasia and sparsely populated regions such as Papunesia and Australia.While Eurasia is more heavily represented, this diversity of linguistic coverage helps ensure that the dataset remains useful for training LLMs in diverse regional environments.</p>
<p>Script Distribution.Figure 2c illustrates the distribution of languages in DCAD-2000 by writing system.The dataset supports 159 scripts, with the Latin script dominating at 79.4%, followed by Cyrillic (3.9%), Arabic (2.6%), and Devanagari (2.1%), among others.This diversity in scripts enables a wide range of cross-lingual and script-specific tasks.However, the inclusion of minority scripts, especially those with limited resources, poses unique challenges, such as optical character recognition (OCR) difficulties for certain scripts or inconsistent text quality.Despite these challenges, DCAD-2000 ensures comprehensive coverage by including data from diverse scripts.A complete list of supported scripts is provided in Appendix B.</p>
<p>Language Resource Classification.Following the classification approach proposed by Flores [17], we categorize languages in DCAD-2000 into four groups based on corpus size: high-resource, medium-resource, low-resource, and extremely low-resource.Table 1 shows the distribution across these categories.The dataset includes 155 high-and medium-resource languages, while low-resource languages make up a significant portion, which reflects DCAD-2000's commitment to supporting underrepresented languages.Notably, DCAD-2000 surpasses other corpora in its balance between high-resource and low-resource languages, which can have a significant impact on multilingual model training.The distribution of languages across categories ensures that the dataset is well-suited for developing models that perform effectively across diverse language resources.</p>
<p>Impact of Data Cleaning.We summarize the document count, token count, and disk size of the high/medium/low resource languages in DCAD-2000 before and after the data cleaning process.Complete details are provided in Appendix C. The cleaning process results in the removal of a substantial amount of noisy data, even from datasets like MaLA, Fineweb, and Fineweb-2, which had already been subject some cleaning.This aligns with the findings from [18,19].For example, in the MaLA dataset, 8.05 million documents are removed for the hbs_Latn language, which suggests the necessity of rigorous data cleaning to enhance dataset quality.Overall, the cleaning process removed approximately 7.69% of the documents across all languages, significantly improving the quality of the dataset by reducing noise and increasing relevance for model training (Section 5).</p>
<p>Evaluation</p>
<p>Following Fineweb-2 [14], we conduct a series of experiments on the FineTask benchmark 15 to evaluate the effectiveness of our proposed data cleaning pipeline and assess the quality of the DCAD-2000 dataset.FineTask comprises tasks in nine languages (i.e., Chinese, French, Arabic, Russian, Thai, Hindi, Turkish, Swahili, and Telugu), and covers a diverse set of NLP tasks, including reading comprehension, commonsense reasoning, natural language understanding, and text generation.To investigate the impact of different data cleaning strategies and anomaly detection algorithms, we continue pretraining on three typical LLMs: LLaMA-3.2-1B[47], Qwen-2.5-7B[48], and Aya-expanse-32B [49].</p>
<p>Additionally, we analyze the performance across different resource categories using the SIB-200 [24], Glot500-c [8], and FLORES-200 [25]</p>
<p>Aya-expanse-32B</p>
<p>Baseline Heuristic Threshold Our Impact of Different Data Cleaning Strategies.We evaluate the effectiveness of our proposed anomaly detection-based data cleaning framework by comparing model performance across various cleaning strategies.As illustrated in Figure 3, the baseline model trained on raw, unfiltered data consistently underperforms relative to all cleaning methods.This performance gap is primarily due to noisy, irrelevant, or inconsistent data that hinders model generalization.Traditional threshold-based filtering 16 , which removes low-quality samples using fixed rules based on features, yields modest improvements.In contrast, our anomaly detection-based approach dynamically identifies and filters anomalous or noisy data, resulting in significantly enhanced model performance.Models trained using our method achieve normalized accuracy improvements of approximately 5-20% over the baseline, and outperform the threshold-based approach by 3-10%.Threshold-based approaches trade accuracy for efficiency, whereas our framework, despite higher computational demands, uncovers subtle and cross-lingual data anomalies that fixed rules frequently overlook.</p>
<p>Comparison of Anomaly Detection Algorithms.We compare several classical anomaly detection algorithms to identify the most effective approach for constructing DCAD-2000.The evaluated methods include Isolation Forest (ISO_Forest;46), One-Class SVM (OC_SVM; 50), Local Outlier Factor (LOF;51), and K-Means [52], using implementations from scikit-learn 17 .We provide the comparison of different algorithms in Appendix D.3.Table 2 reports the performance of these algorithms in cleaning the dataset.While all anomaly detection methods outperform the unfiltered baseline, the performance of OC_SVM, LOF, and K-Means is notably inconsistent.These algorithms often require extensive parameter tuning (e.g., selecting the number of neighbors for LOF or the kernel type for OC_SVM), which introduces sensitivity to hyperparameters and increases computational overhead.In contrast, ISO_Forest demonstrates more stable and robust performance across experiments, attributed to its efficiency in handling noisy, high-dimensional multilingual data.Unlike other methods,  Comparison with Other Multilingual Datasets.To validate the quality of DCAD-2000, we compare it against existing multilingual corpora on the FineTask benchmark.These corpora include datasets constructed from New CC, MaLA, and Fineweb-2 as described in Section 3.1.As shown in Figure 4, models trained on DCAD-2000 consistently outperform those trained on other datasets, achieving higher normalized accuracy.The improvements can be attributed to the enhanced data quality, diversity, and reduced noise resulting from our comprehensive cleaning pipeline.Specifically, DCAD-2000 provides greater linguistic diversity and a more balanced representation of low-resource languages, leading to improved performance on tasks involving underrepresented languages like Swahili and Telugu.</p>
<p>Analysis by the Categories of Language Resources.Table 3 presents model performance across languages categorized by resource levels (High, Medium, Low, and Very Low).Across all benchmarks and model sizes, DCAD-2000 consistently outperforms Fineweb-2 and New CC.While the gains are modest for high-resource languages, improvements are substantial for low-and very low-resource languages, reaching up to +9.15 accuracy on SIB-200 and −53.23 NLL on Glot500-c, which highlights the effectiveness of our cleaning pipeline in improving data quality where it is most needed.The BLEU results on FLORES-200 further validate these trends, with notable improvements in both English-to-X and X-to-English translation tasks.These consistent gains across tasks and languages demonstrate that DCAD-2000 enables more balanced multilingual performance and is well-suited for training inclusive, high-quality language models.Manual Quality Evaluation of Cleaning Pipeline.To assess the effectiveness of our cleaning pipeline, we conduct a manual quality evaluation on five representative languages: English, Chinese, German, Japanese, and French.More specifically, we randomly sampled 100 retained and 100 deleted documents per language, with each document labeled by a proficient annotator as "Good," ''Borderline," or ''Bad."The evaluation revealed that the pipeline retained high-quality content with minimal residual noise ( 4.4%) and low false positive rates ( 5.2%).These results confirm the robustness of our unsupervised, anomaly-detection-based method in effectively removing low-quality content while preserving valuable data.Full details of the experimental setup and results can be found in the Appendix E.</p>
<p>Further Investigation.To evaluate the practical trade-offs between conventional heuristic filtering and our anomaly-based framework, we conduct a controlled cost-benefit analysis and found that DCAD incurs only minor computational overhead while improving downstream task performance; please refer to Appendix F for more details.To assess the robustness of different feature combinations, we performed an ablation study on the 8-dimensional feature vector and observed that each feature contributes meaningfully, with the Language Identification confidence score being particularly critical; please refer to Appendix G for more details.To justify the practical choice of anomaly detector and explore future extensions, we analyzed the trade-offs between classical and modern deep anomaly detection methods and highlighted the scalability, interpretability, and resource efficiency of Isolation Forest; please refer to Appendix H for more details.</p>
<p>Conclusion</p>
<p>In this paper, we introduce DCAD-2000, a large-scale multilingual dataset designed to address the increasing demand for high-quality and diverse training data for multilingual LLMs.Our dataset spans 2,282 languages, providing comprehensive coverage across various geographic regions, scripts (159 scripts), and larger coverage of high/medium resource languages (155 languages).To avoid manually setting thresholds during the data cleaning process, we propose a novel framework that reframes data cleaning as an anomaly detection task.This dynamic approach ensures effective identification and removal of anomalous data from noisy datasets.Empirical experiments demonstrate the effectiveness of our proposed data cleaning framework and the high quality of the DCAD-2000 dataset across multiple multilingual benchmarks.</p>
<p>Limitations</p>
<p>This work has the following limitations: (i) Although the proportion of high/medium/low resource languages in DCAD-2000 has greatly increased compare to existing multilingual datasets, a significant portion of the languages are still very low resource languages.Future work will explore to collect data for extremely low-resource languages through other modalities (e.g., images) through technologies like OCR. (ii) We evaluate the new data cleaning framework only on four classical anomaly detection algorithms; however, since the framework is algorithm-independent, it should also be effective with other anomaly detection algorithms.(iii) For language identification, we use GlotLID [21], a FastText-based model whose limitations in handling massive multilinguality have been discussed in previous works [53].However, since the data cleaning pipeline is language-agnostic, other language identification models can also be employed.(iv) We use a classical, feature-based anomaly detection algorithm rather than modern deep or embedding-based methods [54] because of the lack of clean reference distributions, the need for scalability across thousands of languages, and resource constraints.We will explore incorporating semantic embedding-based or lightweight deep anomaly detectors in future work to capture subtler anomalies that our current approach may miss.</p>
<p>NeurIPS Paper Checklist</p>
<p>Claims</p>
<p>Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope?</p>
<p>Answer: [Yes] Justification: The paper's contributions and scope are delineated in the abstract and Section 1, with the first and last paragraphs of Section 1 specifying each respectively.</p>
<p>Guidelines:</p>
<p>• The answer NA means that the abstract and introduction do not include the claims made in the paper.• The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations.A No or NA answer to this question will not be perceived well by the reviewers.• The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.• It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.</p>
<p>Limitations</p>
<p>Question: Does the paper discuss the limitations of the work performed by the authors?</p>
<p>Answer: [Yes]</p>
<p>Justification: The limitations of the work is discussed in the Section 7.</p>
<p>Guidelines:</p>
<p>• The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.• The authors are encouraged to create a separate "Limitations" section in their paper.</p>
<p>• The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally).The authors should reflect on how these assumptions might be violated in practice and what the implications would be.• The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs.In general, empirical results often depend on implicit assumptions, which should be articulated.• The authors should reflect on the factors that influence the performance of the approach.For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting.Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.• The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.• If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.• While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper.The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community.Reviewers will be specifically instructed to not penalize honesty concerning limitations.</p>
<p>Theory assumptions and proofs</p>
<p>Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?</p>
<p>Answer: [Yes]</p>
<p>Justification: The paper provides detailed assumptions and proofs in Section 5.</p>
<p>Guidelines:</p>
<p>• The answer NA means that the paper does not include theoretical results.</p>
<p>• All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.</p>
<p>• All assumptions should be clearly stated or referenced in the statement of any theorems.</p>
<p>• The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.• Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.• Theorems and Lemmas that the proof relies upon should be properly referenced.</p>
<p>Experimental result reproducibility</p>
<p>Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)?</p>
<p>Answer: [Yes] Justification: The paper provides sufficient reproducibility details, with data processing scripts and experimental code available at GitHub: https://github.com/yl-shen/DCAD-2000.</p>
<p>Guidelines:</p>
<p>• The answer NA means that the paper does not include experiments.</p>
<p>• If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.• If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.• Depending on the contribution, reproducibility can be accomplished in various ways.For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model.In general.releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.• While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution.For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm.(b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully.(c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset).(d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility.In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.</p>
<p>Open access to data and code</p>
<p>Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?Answer: [Yes]</p>
<p>Justification: The DCAD-2000 dataset is publicly available via the Hugging Face Datasets repository: https://huggingface.co/datasets/openbmb/DCAD-2000, with the corresponding code hosted on GitHub: https://github.com/yl-shen/DCAD-2000.</p>
<p>Guidelines:</p>
<p>• The answer NA means that paper does not include experiments requiring code.</p>
<p>• Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.• While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer.Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).Guidelines:</p>
<p>• The answer NA means that the paper does not include experiments.</p>
<p>• The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them.• The full details can be provided either with the code, in appendix, or as supplemental material.Guidelines:</p>
<p>• The answer NA means that the paper does not include experiments.</p>
<p>• The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.• The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.• The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).Guidelines:</p>
<p>• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.</p>
<p>• If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.• The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).</p>
<p>Broader impacts</p>
<p>Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed?</p>
<p>Answer: [Yes] Justification: Please refer to Appendix I.</p>
<p>Guidelines:</p>
<p>• The answer NA means that there is no societal impact of the work performed.</p>
<p>• If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.• Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.• The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments.However, if there is a direct path to any negative applications, the authors should point it out.For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation.On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.• The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.• If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).</p>
<p>Safeguards</p>
<p>Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)?</p>
<p>Answer: [NA] Justification: The paper poses no such risks.</p>
<p>Guidelines:</p>
<p>• The answer NA means that the paper poses no such risks.</p>
<p>• Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.• Datasets that have been scraped from the Internet could pose safety risks.The authors should describe how they avoided releasing unsafe images.• We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.</p>
<ol>
<li>Licenses for existing assets Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected?</li>
</ol>
<p>Answer: [Yes]</p>
<p>Justification: We politely cited the existing assets and read their usage license.</p>
<p>Guidelines:</p>
<p>• The answer NA means that the paper does not use existing assets.</p>
<p>• The authors should cite the original paper that produced the code package or dataset.</p>
<p>• The authors should state which version of the asset is used and, if possible, include a URL.</p>
<p>• The name of the license (e.g., CC-BY 4.0) should be included for each asset.</p>
<p>• For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.• If assets are released, the license, copyright information, and terms of use in the package should be provided.For popular datasets, paperswithcode.com/datasetshas curated licenses for some datasets.Their licensing guide can help determine the license of a dataset.• For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.• If this information is not available online, the authors are encouraged to reach out to the asset's creators.</p>
<p>New assets</p>
<p>Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets?</p>
<p>Answer: [Yes]</p>
<p>Justification: Comprehensive documentation for newly introduced assets (e.g., code, data) is provided in the supplementary material.</p>
<p>Guidelines:</p>
<p>• The answer NA means that the paper does not release new assets.</p>
<p>• Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates.This includes details about training, license, limitations, etc. • The paper should discuss whether and how consent was obtained from people whose asset is used.• At submission time, remember to anonymize your assets (if applicable).You can either create an anonymized URL or include an anonymized zip file.</p>
<p>Crowdsourcing and research with human subjects</p>
<p>Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)?</p>
<p>Answer: [NA]</p>
<p>Justification: The paper does not involve crowdsourcing nor research with human subjects.</p>
<p>Guidelines:</p>
<p>• The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.• Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.• According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.</p>
<p>Institutional review board (IRB) approvals or equivalent for research with human subjects</p>
<p>Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained?</p>
<p>Answer: [NA]</p>
<p>Justification: No human subjects were used on our work.</p>
<p>Guidelines:</p>
<p>• The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.• Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research.If you obtained IRB approval, you should clearly state this in the paper.• We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.• For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.</p>
<p>Declaration of LLM usage</p>
<p>Question: Does the paper describe the usage of LLMs if it is an important, original, or non-standard component of the core methods in this research?Note that if the LLM is used only for writing, editing, or formatting purposes and does not impact the core methodology, scientific rigorousness, or originality of the research, declaration is not required.</p>
<p>Answer: [NA]</p>
<p>Justification: Not applicable.</p>
<p>Guidelines:</p>
<p>• The answer NA means that the core method development in this research does not involve LLMs as any important, original, or non-standard components.• Please refer to our LLM policy (https://neurips.cc/Conferences/2025/LLM)for what should or should not be described.).We observe substantial variation in the average word count across languages within the same dataset.For instance, some languages exhibit an average word count as high as 4,000, indicating that their texts are generally longer, while others have an average word count ranging from 50 to 100, suggesting that their texts are typically shorter.This imbalanced distribution complicates the application of traditional fixed-threshold data cleaning methods across all languages.For example, setting a word count threshold of 800 (e.g., the median word count) may be suitable for many languages, but it would still misclassify a significant portion of data as low-quality.</p>
<p>Figure 5b illustrates the average word count distribution for Chinese across different data sources (MaLA, Fineweb-2, and New CC).We observe significant variation in the word count distribution for the same language across these sources.For example, the average word count for Chinese in the MaLA corpus is 690, while in New CC (CC-MAIN-2024-33), the average word count increases to 1,975.This discrepancy highlights the inadequacy of a single fixed threshold for data from different sources.Applying a uniform threshold could lead to incorrect cleaning of Chinese text from certain data sources, potentially compromising the representativeness and quality of the data.Consequently, it is essential to adopt flexible cleaning strategies tailored to the characteristics of each data source.</p>
<p>Figure 5c illustrates the variation in word count for Chinese across different shards in the Fineweb-2 dataset.We observe imbalanced word count distributions between shards, which further complicates the data cleaning process.For instance, some shards contain texts with word counts concentrated between 700 and 1,000, while others have texts primarily between 1,000 and 1,200.This shard-level variation suggests that fixed-threshold cleaning methods may perform inconsistently across different shards, fails to account for the unique characteristics of the data within each shard.Therefore, in the presence of such imbalanced distributions, it is crucial to implement a more flexible data cleaning approach.</p>
<p>B DCAD-2000 Grouped by Writting Scripts</p>
<p>As mentioned in Section 4, DCAD-2000 contains a total of 159 writing scripts.To provide a comprehensive overview, we list each of these scripts and their corresponding statistical information in Table 7 and Table 8.By presenting this information, we aim to highlight the broad range of writing systems represented by DCAD and emphasize its potential in various linguistic research and applications.</p>
<p>C Data Cleaning Statistics</p>
<p>In this section, we provide detailed data cleaning statistics (Table 9, 10, 11 and 12) for high-resource, mediumresource, and low-resource languages.For the data cleaning statistics of very low-resource languages, please refer to the open-source data statistics we released.</p>
<p>D Experimental Setup</p>
<p>In this section, we provide a comprehensive description of our experimental setup, including dataset preparation, model configurations, continued pretraining procedure, data cleaning and anomaly detection pipeline, evaluation metrics, and implementation details.</p>
<p>D.1 Evaluation Benchmarks</p>
<p>FineTask Benchmark.FineTask is a multilingual, multi-task benchmark covering nine typologically diverse languages: Chinese, French, Arabic, Russian, Thai, Hindi, Turkish, Swahili, and Telugu.The benchmark spans a wide range of NLP tasks including reading comprehension, common-sense reasoning, natural language understanding, and text generation.FineTask provides four evaluation metrics: Accuracy, Accuracy normalized over character length, Accuracy normalized over token length, and PMI Accuracy.However, according to statistical data, none of these metrics consistently perform well across all languages.Therefore, we chose to use normalized accuracy (norm accuracy) in our evaluation process.</p>
<p>Multilingual Benchmarks.To analyze performance across varying resource levels, we further evaluate on three established multilingual corpora:</p>
<p>• SIB-200 [24]: A suite of topic classification datasets across 205 languages.We use the raw accuracy on held-out test sets.</p>
<p>• Glot500-c [8]: A curated corpus spanning 500 languages for generation and language modeling.We compute negative log-likelihood (NLL) on held-out sentences:
NLL = − 1 T T t=1 log p θ (wt | w&lt;t),(9)
where T is the total token count in the evaluation set.</p>
<p>• FLORES-200 [25]: A benchmark for low-resource machine translation covering 200 languages.We translate from English into each target language (Eng-XX) and translate from other languages into English (X-Eng) and evaluate using SacreBLEU with default settings.</p>
<p>D.2 Pre-training and Evaluation Protocol</p>
<p>We perform continued pretraining on three representative decoder-only large language models (LLMs): LLaMA-3.2-1B[47], Qwen-2.5-7B[48], and Aya-expanse-32B [49].These models are selected to represent a diverse range of open-source models across different parameter scales, allowing us to investigate the effects of the different dataset, data cleaning pipeline across small, medium, and large model sizes.All models are accessed and managed through the HuggingFace Transformers library.</p>
<p>Given the limitations of computational resources, we refrain from full-parameter finetuning.Instead, we adopt Low-Rank Adaptation (LoRA; 55), a parameter-efficient fine-tuning technique that introduces trainable low-rank matrices into each transformer layer, substantially reducing the number of trainable parameters while maintaining competitive performance.</p>
<p>Our training pipeline closely follows the setup described in the LightEval repository 18 , a lightweight evaluation and fine-tuning framework developed by HuggingFace.This ensures reproducibility and consistency with widely adopted community practices.Experiments are conducted on NVIDIA A100 GPU with 80GB memory, which provides sufficient memory bandwidth and compute capability to support batch-level parallelism and efficient LoRA-based fine-tuning.All hyperparameters and task-specific configurations are aligned with those used in the Fineweb-2 benchmark, ensuring comparability with previous work and consistent evaluation conditions.</p>
<p>D.3 Statistical Anomaly Detection</p>
<p>We provide a detailed comparison of the anomaly detection algorithms evaluated for data cleaning in DCAD-2000.The methods are selected based on their popularity, conceptual diversity, and availability in scikit-learn 19 .</p>
<p>All experiments are conducted using the same eight-dimensional feature vectors described in Section 3.</p>
<p>Isolation Forest (ISO_Forest) [46] is an ensemble-based method that isolates anomalies instead of profiling normal data points.It constructs random binary trees by recursively selecting features and split values, and then uses the path length of each data point across the trees to assess anomaly scores.Shorter paths indicate higher likelihood of being an outlier.ISO_Forest is well-suited to high-dimensional and noisy data and requires minimal hyperparameter tuning.Its main drawback is higher computational cost relative to simpler methods, though it scales well with the number of samples.</p>
<p>One-Class SVM (OC_SVM) [50] is a kernel-based method that attempts to separate the data from the origin in a transformed feature space.It is sensitive to the choice of kernel function (e.g., RBF, linear) and associated parameters (e.g., gamma, nu).OC_SVM can be effective in capturing complex boundaries, but it often suffers from scalability issues and requires careful parameter tuning, especially in high-dimensional multilingual settings like DCAD-2000.</p>
<p>Local Outlier Factor (LOF) [51] is a density-based method that identifies anomalies based on local density deviation.It compares the local density of a data point with that of its neighbors.Points that have substantially lower density than their neighbors are considered outliers.The performance of LOF depends heavily on the number of neighbors chosen and tends to degrade in high-dimensional spaces due to the curse of dimensionality.It is also computationally expensive for large datasets.[52] is a clustering algorithm typically used for unsupervised partitioning of data.For anomaly detection, it is repurposed by measuring the distance of points from their assigned cluster centroids-points that are far from any centroid can be considered anomalous.K-Means is computationally efficient and easy to implement but lacks sensitivity to local structures and does not inherently model outliers.Its effectiveness depends on a suitable choice of the number of clusters.</p>
<p>K-Means</p>
<p>E Manual Quality Evaluation of Cleaning Pipeline</p>
<p>To validate the effectiveness of our cleaning pipeline and to assess the residual noise and false positives, we conduct a manual quality evaluation of the retained and deleted documents.This evaluation was performed on five representative languages: English, Chinese, German, Japanese, and French.These languages are selected to ensure diverse linguistic coverage, and the evaluation will be extended in future work to include additional languages, particularly low-resource languages, where automatic filtering may be more challenging.</p>
<p>For each of the five languages, we randomly sampled 100 documents retained by our pipeline (i.e., documents that were kept) and 100 documents that were removed (i.e., deleted by our pipeline).The annotation process was conducted by one proficient annotator per language.The key goal of this annotation process was to estimate both the quality of the documents retained by the pipeline and the false positives in the deleted set.The quality evaluation provides insight into how well the cleaning pipeline separates high-quality content from noisy or irrelevant data.The documents were labeled with the following quality ratings:</p>
<p>• Good: Documents that were coherent, meaningful, and of high quality.</p>
<p>• Borderline: Documents that were understandable but flawed, including minor corruption, weak coherence, or other small issues.</p>
<p>• Bad: Documents that were nonsensical, noisy, or semantically meaningless, such as machine translation errors, boilerplate content, spam, or mixed-language noise.Table 4 demonstrate that our cleaning pipeline effectively filters out low-quality content while preserving high-value data.Across all five languages, the proportion of retained documents rated as "Bad" (residual noise) averaged only 4.4%, indicating minimal contamination of retained documents by low-quality content.Similarly, the false positive rate (i.e., representing the proportion of high-quality documents mistakenly removed) was low, averaging 5.2%.The pipeline's precision, defined as the proportion of retained documents classified as "Good" or "Borderline", was 95.6%, while its recall, which measures the retention of "Good" documents, was 94.13%.These results demonstrate that the pipeline achieves both high precision and recall, effectively balancing the removal of noise with the preservation of valuable data.Overall, the findings validate the robustness of our unsupervised, anomaly-detection-based approach across multiple languages, with future work aimed at extending this evaluation to additional languages, particularly low-resource ones.</p>
<p>F Cost Benefit Analysis of Cleaning Strategies</p>
<p>To better evaluate the practical trade-offs between conventional heuristic filtering and our anomaly-based framework (DCAD), we conduct a controlled cost-benefit analysis on one million web documents under identical hardware conditions.As summarized in Table 5, the DCAD pipeline incurs only a minor computational overhead relative to the heuristic baseline (i.e., approximately two additional minutes of processing time and a 6 GB increase in peak memory usage).Although DCAD retains 11% fewer documents, it consistently yields superior downstream task performance (Section 5), highlighting its effectiveness in balancing data quality and computational efficiency.</p>
<p>G Feature Robustness Analysis</p>
<p>To evaluate the robustness of our anomaly detection framework with respect to feature design (Section 3.2.1),we conduct a one-feature-at-a-time ablation study.Given the combinatorial explosion of all possible subsets (2 8 − 1 = 255), we adopt a pragmatic protocol in which each feature is removed individually from the full 8-dimensional feature vector, and the cleaning process is repeated using the remaining seven features.We then fine-tune LLaMA-3.2-1B on each resulting filtered corpus and evaluate performance on FineTask-Arabic and FineTask-Turkish, following the same experimental setup as in Section 5.</p>
<p>As observed in Table 6, we have the following findings: (1) The full 8-feature configuration consistently outperforms all ablated variants, confirming that each feature contributes meaningfully to overall performance.</p>
<p>(2) The Language Identification (LID) confidence score (Feature 7) is particularly critical: its removal results in a substantial accuracy drop, likely due to the presence of mixed or misidentified language content that adversely affects multilingual model quality.</p>
<p>(3) Other features, such as repetition ratios and perplexity, provide modest gains individually; none are harmful or redundant when considered in isolation.</p>
<p>H Practical Choice of Anomaly Detector and Future Extensions</p>
<p>While modern deep anomaly detection methods, such as autoencoder-based reconstruction scoring [56] and contrastive outlier detection [57], have achieved strong performance in other domains, we deliberately adopt a classical algorithm, specifically Isolation Forest, in this work.This choice is motivated by three practical constraints inherent to large-scale multilingual corpus cleaning:</p>
<p>• Lack of a clean reference distribution.Autoencoder-based methods assume access to a predominantly clean training set to learn a reliable reconstruction prior.In our weakly supervised scenario covering 2,282 languages without dependable clean subsets, this assumption is violated, making such models prone to degenerate reconstructions on noisy data.</p>
<p>• Scalability across languages without supervision.Contrastive-learning-based outlier detection requires either labeled normal/abnormal pairs or implicitly curated positive anchors.Providing such supervision for thousands of languages would reintroduce the language-specific manual tuning that our language-agnostic pipeline explicitly avoids.</p>
<p>• Resource efficiency and feature interpretability.Our framework relies on explicit, interpretable quality features (e.g., repetition ratio, perplexity, LID confidence) rather than opaque embeddingspace distances.Classical anomaly detectors like Isolation Forest can operate directly on these CPU-computable features and scale to 46 TB of multilingual data without GPU dependency, making them well-suited for real-world data curation pipelines.</p>
<p>Nonetheless, extending DCAD to incorporate semantic embedding-based anomaly signals or lightweight deep novelty detection represents a promising direction for future work.We view our current feature-space approach as a foundational layer, onto which richer semantic detectors can be incrementally integrated once computational and language-coverage challenges are addressed.</p>
<p>I Ethics Statement</p>
<p>Our dataset integrates existing multilingual datasets, such as MaLA [11] and Fineweb-2 [14], and includes newly extracted data from Common Crawl, providing large-scale and high-quality training corpora to support the training of multilingual large language models (LLMs).Additionally, we propose a novel data cleaning method to filter out potentially toxic documents, reducing potential ethical concerns.However, performing fine-grained analysis on such a vast dataset (46.72TB) remains a significant challenge.To address this, we released the dataset for the community to explore and research extensively.Furthermore, since our dataset is derived from open-source datasets, we adhere to the open-source policies of these datasets to promote future research in multilingual LLMs, while mitigating potential ethical risks.Therefore, we believe our dataset does not pose greater societal risks than existing multilingual datasets.</p>
<p>Figure 1 :
1
Figure 1: Scatter plots of eight features extracted from a Chinese corpus during the data cleaning process, with data points color-coded according to their anomaly labels.The yellow points represent high-quality data, while the purple points indicate low-quality data.</p>
<p>Figure 2 :
2
Figure 2: Document distribution and linguistic diversity in DCAD-2000.</p>
<p>Figure 3 :
3
Figure 3: The performance comparison of models trained using various data cleaning methods.</p>
<p>Figure 4 :
4
Figure 4: Comparison of DCAD-2000 with existing multilingual corpora for three languages-French, Chinese, and Turkish-evaluated using different multilingual LLMs.</p>
<p>7 . 8 .
78
Experiment statistical significance Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments?Answer: [Yes] Justification: We report the impact of different anomaly detection algorithms and different data cleaning strategies in Section 5. Guidelines: • The answer NA means that the paper does not include experiments.• The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.• The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).• The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) • The assumptions made should be given (e.g., Normally distributed errors).• It should be clear whether the error bar is the standard deviation or the standard error of the mean.• It is OK to report 1-sigma error bars, but one should state it.The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.• For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g.negative error rates).• If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.Experiments compute resources Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments?Answer: [Yes] Justification: Computational resource specifications are documented in Section 3.3.</p>
<p>9 .
9
Code of ethics Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?Answer: [Yes] Justification: Our research strictly adheres to the NeurIPS Code of Ethics.</p>
<p>Figure 5 :
5
Figure 5: Distribution of average word counts across different languages, sources, and shards in the New CC dataset.</p>
<p>Figure</p>
<p>Figure5aillustrates the average word count distribution across different languages in the New CC dataset (CC-MAIN-2024-38).We observe substantial variation in the average word count across languages within the same dataset.For instance, some languages exhibit an average word count as high as 4,000, indicating that their texts are generally longer, while others have an average word count ranging from 50 to 100, suggesting that their texts are typically shorter.This imbalanced distribution complicates the application of traditional fixed-threshold data cleaning methods across all languages.For example, setting a word count threshold of 800 (e.g., the median word count) may be suitable for many languages, but it would still misclassify a significant portion of data as low-quality.</p>
<p>Table 1 :
1
Comparison of multilingual datasets constructed from Common Crawl (CC) and our constructed DCAD-2000, focusing on the latest CC version used, the total number of languages supported, distribution across resource categories (high, medium, low, very low), and training readiness.The CC version marked with underline indicates an inferred version due to the lack of explicit specification in the original paper.The "Training-Ready" column indicates whether the dataset is ready for training LLMs without requiring further data cleaning.
DatasetCC Version#Langs (total)#Langs (high)#Langs (medium)#Langs (low)#Langs (very low)Training-ReadymC4 [6]CC-MAIN-2020-34101043526✗OSCAR 23.01 [7] CC-MAIN-2022-491536422580✗Glot500 [8]CC-MAIN-2020-34511010879324✗CulturaX [9]CC-MAIN-2022-4916711472782✗Madlad-400 [10]CC-MAIN-2022-3341974639327✗MaLA [11]CC-MAIN-2022-49939112578735✗Glotcc [12]CC-MAIN-2023-501331010521269✗HPLT-v1.2 [13]CC-MAIN-2022-4019112533888✗Fineweb-2 [14]CC-MAIN-2024-1819151062491794✗DCAD-2000CC-MAIN-2024-462282131421242003✓</p>
<p>6For DCAD-2000, we incorporate data from the November 2024 release (CC-MAIN-2024-46) to ensure freshness and up-to-date relevance of the data.
Fineweb-2 Corpus [14]. Fineweb-2 expands Fineweb to include multilingual data, covering 1,915languages. It processes 96 Common Crawl dumps from 2013 (CC-MAIN-2013-20) to April 2024(CC-MAIN-2024-20). The deduplication process within Fineweb-2 is similarly handled using theDatatrove library, ensuring the exclusion of redundant entries and maintaining high-quality multilin-gual coverage.Newly Extracted Common Crawl Data. To incorporate the most recent multilingual data, weextract and process Common Crawl dumps from May 2024 (CC-MAIN-2024-22) to November 2024(CC-MAIN-2024-46</p>
<p>Table 2 :
2
The performance of various anomaly detection algorithms.Bold and underlined numbers indicates the best and second-best results respectively.
LLaMA-3.2-1BQwen-2.5-7BAya-expanse-32BBaseline Iso_Forest OC_SVM LOF K-Means Baseline Iso_Forest OC_SVM LOF K-Means Baseline Iso_Forest OC_SVM LOF K-MeansArabic0.070.210.180.210.140.630.710.680.650.680.690.750.700.710.69Turkish0.070.270.290.170.150.650.720.730.670.680.750.790.770.760.77Swahili0.080.290.250.190.190.250.340.270.350.270.350.440.360.370.41Russian0.100.240.190.180.150.740.790.750.750.760.770.820.790.800.79Telugu0.020.060.050.040.040.160.240.260.200.210.150.250.190.210.27Thai0.140.210.180.180.150.570.640.590.590.610.380.460.420.430.40Chinese0.120.320.280.250.210.750.820.770.760.780.690.750.710.710.73French0.110.350.370.300.230.740.800.760.760.750.740.790.760.760.76Hindi0.070.210.170.160.140.490.570.520.530.520.690.740.720.730.72</p>
<p>Table 3 :
3
Performance across different language categories.We use accuracy (↑) in SIB-200, negative log-likelihood (↓) in Glot500-c and BLEU (↑) in FLORES-200.Improvements are highlighted accordingly.
LLaMA-3.2-1BQwen-2.5-7BAya-expanse-32BFineweb-2 New CCDCAD-200Fineweb-2 New CCDCAD-200Fineweb-2 New CCDCAD-200H8.248.8610.37 ↑2.1333.4134.5338.26 ↑4.8541.7242.4147.93 ↑6.21SIB-200 (↑)M L7.31 6.067.92 6.459.15 ↑1.84 7.83 ↑1.7728.72 23.5829.86 24.2232.65 ↑3.93 27.12 ↑3.5432.25 26.8733.39 27.5738.16 ↑5.91 33.24 ↑6.37VL3.684.275.24 ↑1.5613.2515.4321.57 ↑8.3217.2319.526.38 ↑9.15H426.37403.58373.14 ↓53.23347.21334.18303.38 ↓43.83273.85257.24225.28 ↓48.57Glot500-c test (↓)M L446.28 503.38436.94 493.27423.75 ↓22.53 473.96 ↓29.42385.72 426.33389.24 419.25369.15 ↓16.57 404.28 ↓22.05326.92 372.62321.16 367.26302.53 ↓24.39 341.34 ↓31.28VL584.55569.34532.86 ↓51.69479.04463.36433.48 ↓45.56396.33392.33385.86 ↓10.47H3.143.825.26 ↑2.1215.2416.0718.47 ↑3.2323.4524.3326.33 ↑2.88FLORES-200 (↑)M2.752.943.89 ↑1.1412.8313.4615.49 ↑2.6619.3620.2121.62 ↑2.26(Eng-X)L2.272.413.14 ↑0.878.949.2810.25 ↑1.3116.6117.2418.36 ↑1.75VL1.852.052.35 ↑0.506.337.259.05 ↑2.7212.5113.1614.77 ↑2.26H3.943.984.26 ↑0.3216.3116.9218.84 ↑2.5323.8624.1326.94 ↑3.08FLORES-200 (↑)M3.523.663.80 ↑0.2813.6514.0516.27 ↑2.6220.4520.3622.53 ↑2.17(X-Eng)L3.053.123.24 ↑0.199.4710.2211.48 ↑2.0117.6717.8218.93 ↑1.26VL2.732.833.14 ↑0.417.287.819.65 ↑2.3713.2513.5615.88 ↑2.63
ISO_Forest delivers reliable results without intensive hyperparameter tuning, making it particularly suitable for large-scale multilingual datasets.However, ISO_Forest can be more computationally demanding than simpler methods like K-Means, especially in high-dimensional settings (our feature vectors have eight dimensions, as described in Section 3.2.2).Despite this trade-off, its robustness and scalability establish ISO_Forest as the most appropriate choice for data cleaning in DCAD-2000.</p>
<p>•</p>
<p>The instructions should contain the exact command and environment needed to run to reproduce the results.See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.•Theauthorsshould provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. • The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines.If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.•At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).•Providingas much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.6.Experimental setting/detailsQuestion: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results?
Answer: [Yes]Justification: Detailed experimental configurations are provided in Section 3, with full implementationdetails in Appendix D.</p>
<p>Table 4 :
4
Quality evaluation of retained and deleted documents across five languages.
Retained Documents (Kept by filter)Deleted Documents (Removed by filter)Language Good Borderline Bad Residual Noise (Bad %) Language Good BorderlineBadFalse Positives (Good %)English86%10%4%4%English5%14%81%5%Chinese82%13%5%5%Chinese6%18%76%6%German84%12%4%4%German5%16%79%5%Japanese81%12%7%7%Japanese6%17%77%6%French84%14%2%2%French4%15%81%4%Avg83.4%12.2%4.4%4.4%Avg5.2%16%78.8%5.2%</p>
<p>Table 5 :
5
Cost-Benefit Comparison of Filtering Methods (Per 1M Documents)
MetricHeuristic Filtering Anomaly Detection (DCAD)Cleaning Time10 minutes12-15 minutesMax Memory Usage (CPU)58 GB64 GBTraining Data Retained (%)88%77%Avg Model Accuracy (Global MMLU subset) -LLaMA-3.2-1B43.9%48.6%Accuracy Gain per CPU-hour+0.42%+0.60%Accuracy Gain per 1% data lost+0.16%+0.32%</p>
<p>Table 6 :
6
Ablation Study: Impact of Feature Subsets (Refer to Section 3.2.1)
Feature Subset UsedArabic TurkishAll 8 features (1-8)0.210.27w/o (8) Perplexity0.200.25w/o (7) LID score0.160.21w/o (6) Flagged word ratio0.200.24w/o (5) Stopword ratio0.210.26w/o (4) Special character ratio0.200.26w/o (3) Word repetition0.180.24w/o (2) Character repetition0.190.25w/o (1) Token count0.200.24</p>
<p>Table 7 :
7
Statisticals grouped by writing scripts (part I).Comparison of language count, document count, token count, disk size, and sources before and after data cleaning in DCAD-2000.
Script #LangsDocumentsTokensDisk SizeSourcekeepremovetotalkeepremovetotalkeepremovetotalLatn18305.50B 439.42M5.93B4.29T 327.39B4.61T21.13TB4.91TB26.12TB Fineweb-2, Fineweb, MaLA, New CCCyrl911.11B85.84M1.19B1.26T98.88B1.36T9.40TB2.43TB11.83TB Fineweb-2, MaLA, New CCHani12715.15M71.29M 786.45M 746.48B73.89B 820.36B2.90TB1.60TB4.50TB Fineweb-2, MaLA, New CCJpan1491.47M42.47M 533.93M 278.14B22.81B 300.95B2.00TB 504.87GB2.50TB Fineweb-2, New CCArab60198.64M20.36M 219.03M 122.36B13.12B 135.48B1.03TB 290.11GB1.31TB Fineweb-2, MaLA, New CCHang179.22M6.16M85.38M59.07B4.62B63.69B 336.56GB66.70GB 403.26GB Fineweb-2, MaLA, New CCGrek469.14M5.90M75.04M58.45B5.15B63.60B 432.64GB 120.10GB 552.76GB Fineweb-2, MaLA, New CCDeva4860.09M5.79M65.87M30.63B2.56B33.19B 342.83GB72.51GB 415.37GB Fineweb-2, MaLA, New CCThai1155.73M4.34M60.06M46.36B3.60B49.96B 526.40GB 110.69GB 637.11GB Fineweb-2, MaLA, New CCMlym639.16M3.89M43.05M7.00B 559.61M7.56B94.53GB18.86GB 113.40GB Fineweb-2, MaLA, New CCGujr238.91M4.55M43.46M5.07B 461.70M5.53B60.22GB13.54GB73.76GB Fineweb-2, MaLA, New CCKnda234.20M2.70M36.90M4.76B 359.45M5.12B68.85GB11.14GB79.99GB Fineweb-2, MaLA, New CCHebr626.99M1.83M28.82M21.15B1.38B22.53B 152.34GB30.80GB 183.18GB Fineweb-2, MaLA, New CCTaml226.65M2.92M29.56M5.88B 461.60M6.35B80.38GB19.44GB99.82GB Fineweb-2, MaLA, New CCGuru224.04M3.16M27.21M2.27B 227.69M2.50B26.71GB8.65GB35.36GB Fineweb-2, MaLA, New CCBeng621.91M1.51M23.42M12.67B 875.46M13.54B 148.42GB31.39GB 179.83GB Fineweb-2, MaLA, New CCGeor320.56M1.36M21.92M6.19B 419.61M6.61B83.04GB15.75GB98.81GB Fineweb-2, MaLA, New CCArmn417.24M1.46M18.70M4.74B 407.38M5.15B42.47GB11.43GB53.93GB Fineweb-2, MaLA, New CCTelu49.93M 821.21K10.75M3.91B 295.72M4.20B48.22GB9.65GB57.87GB Fineweb-2, MaLA, New CCSinh19.91M1.12M11.03M2.93B 251.40M3.18B32.73GB7.64GB40.37GB Fineweb-2, MaLA, New CCOrya66.57M 616.98K7.18M 464.57M37.89M 502.46M9.79GB2.20GB12.01GB Fineweb-2, MaLAEthi136.41M 429.99K6.85M1.38B91.75M1.46B12.66GB2.92GB15.59GB Fineweb-2, MaLA, New CCMymr96.04M 479.44K6.52M5.30B 406.67M5.72B40.57GB7.83GB48.39GB Fineweb-2, MaLA, New CCKana15.83M1.11M6.94M1.13B 219.26M1.35B16.90GB14.33GB31.23GB Fineweb-2, New CCKhmr74.96M 380.38K5.34M2.24B 160.29M2.40B30.95GB4.99GB35.95GB Fineweb-2, MaLA, New CCBamu14.71M1.00M5.71M 199.46M42.49M 241.95M79.67GB19.47GB99.14GB Fineweb-2, New CCCopt24.40M 361.99K4.76M 219.04M18.03M 237.09M8.97GB 864.17MB9.84GB Fineweb-2, New CCTang13.94M 741.81K4.68M 209.68M39.47M 249.15M22.70GB7.67GB30.36GB Fineweb-2, New CCXsux13.90M 694.59K4.59M 276.93M49.35M 326.28M13.84GB9.74GB23.58GB Fineweb-2, New CCLaoo53.46M 470.52K3.92M 840.28M87.36M 927.65M11.85GB3.95GB15.80GB Fineweb-2, MaLA, New CCYiii13.39M 417.38K3.81M 232.88M28.68M 261.56M25.82GB6.24GB32.05GB Fineweb-2, New CCHira12.78M 579.38K3.36M 361.77M75.28M 437.05M4.87GB4.04GB8.91GB Fineweb-2, New CCThaa22.51M 301.28K2.82M 425.90M45.08M 470.98M4.75GB1.28GB6.02GB Fineweb-2, MaLA, New CCKits11.86M 315.45K2.17M 269.54M45.75M 315.29M12.47GB17.12GB29.58GB Fineweb-2, New CCHluw11.71M 374.92K2.09M70.77M15.47M86.25M3.19GB3.45GB6.64GB Fineweb-2, New CCJapn11.60M 177.40K1.78M 148.77M17.99M 166.76M6.05GB2.16GB8.21GB MaLAShrd11.41M 216.59K1.62M 130.80M20.13M 150.93M6.06GB2.35GB8.40GB Fineweb-2, New CCLina11.37M 271.63K1.64M 130.39M25.87M 156.26M6.97GB3.85GB10.82GB Fineweb-2, New CCSamr11.35M 158.99K1.51M64.06M7.54M71.59M4.30GB1.72GB6.02GB Fineweb-2, New CCCans121.24M 248.84K1.49M 109.29M21.66M 130.96M3.55GB2.78GB6.33GB Fineweb-2, MaLASyrc41.12M 116.18K1.23M44.70M4.75M49.44M20.70GB4.35GB25.04GB Fineweb-2, MaLAAdlm11.12M 194.29K1.32M43.63M7.55M51.18M1.10GB 853.95MB1.95GB Fineweb-2, New CCEgyp11.12M 190.50K1.31M97.41M16.58M 113.99M2.54GB3.52GB6.05GB Fineweb-2, New CCMend11.03M 293.72K1.32M16.58M4.75M21.33M 893.39MB2.06GB2.95GB Fineweb-2, New CCLinb1735.07K 107.67K 842.75K52.97M7.76M60.73M6.30GB 997.90MB7.30GB Fineweb-2, New CCBrai1590.10K 125.33K 715.43K57.85M12.29M70.13M1.94GB1.30GB3.24GB Fineweb-2, New CCSgnw1567.29K 106.45K 673.74K37.34M7.01M44.34M1.40GB1.11GB2.50GB Fineweb-2, New CCTibt4544.99K70.33K 615.32K 288.24M33.57M 321.81M4.50GB1.53GB6.09GB Fineweb-2, MaLA, New CCHung1520.10K 155.23K 675.33K42.34M12.64M54.98M1.94GB2.32GB4.25GB Fineweb-2, New CCMong3435.35K61.47K 496.83K 119.66M16.95M 136.62M1.97GB1.04GB3.03GB Fineweb-2, MaLABali1422.49K77.08K 499.57K39.62M7.23M46.84M1.19GB 662.91MB1.85GB Fineweb-2, New CCNshu1419.71K89.40K 509.11K38.53M8.21M46.74M 993.06MB1.28GB2.27GB Fineweb-2, New CCModi1386.82K67.33K 454.15K52.58M9.15M61.73M16.45GB7.42GB23.87GB Fineweb-2, New CCLana1377.58K 110.80K 488.38K47.55M13.95M61.50M 688.16MB2.05GB2.74GB Fineweb-2, New CCSaur1315.78K73.82K 389.60K15.26M3.57M18.83M 398.55MB 489.07MB 887.62MB Fineweb-2, New CCDupl1258.90K53.06K 311.96K14.14M2.90M17.04M 752.58MB 502.95MB1.26GB Fineweb-2, New CCRunr2252.18K39.00K 291.19K 154.68M23.92M 178.61M1.25GB3.28GB4.52GB Fineweb-2, MaLAVaii1243.47K93.27K 336.73K71.28M27.31M98.59M 513.30MB1.88GB2.39GB Fineweb-2, New CCGlag1237.68K72.07K 309.75K20.38M6.18M26.56M 476.61MB 951.96MB1.43GB Fineweb-2, New CCDsrt1198.00K37.90K 235.90K4.47M 855.49K5.32M 248.83MB 562.92MB 811.75MB Fineweb-2, New CCMroo1186.14K22.85K 208.99K6.42M 788.69K7.21M2.43GB 335.38MB2.77GB Fineweb-2, New CCBopo1181.71K24.45K 206.16K30.63M4.12M34.75M3.45GB 890.68MB4.35GB Fineweb-2, New CCMtei2175.69K20.34K 196.03K49.11M5.76M54.87M 805.36MB 574.03MB1.38GB Fineweb-2, MaLAKhar1153.37K40.04K 193.41K6.75M1.76M8.52M 250.30MB 182.38MB 432.67MB Fineweb-2, New CCBrah1138.03K22.72K 160.75K7.85M1.29M9.15M 273.71MB 243.75MB 517.47MB Fineweb-2, New CCBhks1131.90K27.03K 158.93K3.93M 805.58K4.74M 190.96MB 154.63MB 345.59MB Fineweb-2, New CCHmnp1118.87K12.33K 131.20K6.83M 708.37K7.54M 436.28MB 151.81MB 588.09MB Fineweb-2, New CCPhag1107.75K17.58K 125.34K3.41M 556.36K3.97M 141.68MB93.31MB 234.99MB Fineweb-2, New CCMerc1107.52K38.04K 145.56K7.61M2.69M10.30M 215.43MB 472.23MB 687.66MB Fineweb-2, New CCKali2105.87K24.33K 130.20K1.39M 319.46K1.71M 105.24MB91.45MB 196.70MB Fineweb-2, New CCPlrd1104.31K21.07K 125.38K5.47M1.10M6.57M 214.53MB 225.25MB 439.77MB Fineweb-2, New CCLisu2101.48K20.06K 121.53K24.00M4.74M28.74M 204.24MB 527.21MB 731.45MB Fineweb-2, New CCHmng1101.02K23.34K 124.36K5.37M1.24M6.61M 153.20MB 196.99MB 350.19MB Fineweb-2, New CCNkoo298.77K25.89K 124.65K4.91M1.07M5.98M2.13GB 233.87MB2.36GB Fineweb-2, MaLAGran197.96K21.57K 119.53K3.57M 785.93K4.36M 135.27MB 243.90MB 379.18MB Fineweb-2, New CCGonm194.82K16.28K 111.10K2.83M 486.36K3.32M 106.89MB 142.16MB 249.05MB Fineweb-2, New CCCher294.19K25.99K 120.19K9.12M2.45M11.57M 245.29MB 689.18MB 934.47MB Fineweb-2, MaLATnsa189.55K17.93K 107.48K3.28M 656.33K3.93M98.49MB 204.04MB 302.53MB Fineweb-2, New CC</p>
<p>Table 8 :
8
Statisticals grouped by writing scripts (part II).Comparison of language count, document count, token count, disk size, and sources before and after data cleaning in DCAD-2000.
Script #LangsDocumentsTokensDisk SizeSourcekeep removetotalkeepremovetotalkeepremovetotalCprt188.19K 14.11K 102.30K7.87M1.26M9.13M 142.36MB85.91MB 228.27MB Fineweb-2, New CCCari177.73K 18.09K95.82K1.73M 401.78K2.13M89.37MB76.01MB 165.38MB Fineweb-2, New CCDiak168.42K 22.40K90.82K2.87M 938.52K3.81M58.40MB94.36MB 152.76MB Fineweb-2, New CCMarc167.80K 11.89K79.69K2.34M 410.50K2.75M66.51MB95.34MB 161.85MB Fineweb-2, New CCMani165.94K9.56K75.50K6.27M 908.84K7.17M 128.39MB 140.35MB 268.75MB Fineweb-2, New CCTalu265.77K 11.95K77.72K1.27M 231.55K1.50M78.51MB62.21MB 140.72MB Fineweb-2, MaLAVith165.14K 12.13K77.28K2.49M 464.49K2.96M 124.41MB95.26MB 219.66MB Fineweb-2, New CCNagm163.57K 11.94K75.51K1.03M 193.45K1.22M58.20MB73.87MB 132.08MB Fineweb-2, New CCAhom160.21K9.69K69.90K2.34M 376.34K2.72M 127.53MB70.68MB 198.21MB Fineweb-2, New CCJava158.52K 13.32K71.84K2.18M 496.30K2.68M66.55MB 116.13MB 182.68MB Fineweb-2, New CCPalm148.99K5.32K54.32K 424.13K46.09K 470.22K39.41MB43.82MB83.23MB Fineweb-2, New CCWara146.80K9.12K55.92K1.47M 286.76K1.76M58.48MB52.76MB 111.24MB Fineweb-2, New CCOlck245.80K4.06K49.86K6.69M 492.54K7.19M86.16MB38.55MB 124.71MB Fineweb-2, MaLAKhoj139.85K5.23K45.09K 892.46K 117.20K1.01M43.07MB40.20MB83.27MB Fineweb-2, New CCRohg135.21K5.32K40.53K 534.34K80.72K 615.05K36.76MB41.06MB77.82MB Fineweb-2, New CCSidd134.75K8.41K43.16K3.03M 732.80K3.76M46.06MB93.44MB 139.51MB Fineweb-2, New CCYezi133.92K3.35K37.27K96.61K9.53K 106.13K29.36MB14.31MB43.67MB Fineweb-2, New CCOugr132.34K6.13K38.47K 442.16K83.82K 525.98K31.03MB37.95MB68.98MB Fineweb-2, New CCAvst132.16K6.62K38.78K1.75M 360.09K2.11M51.64MB53.81MB 105.46MB Fineweb-2, New CCItal132.06K5.06K37.12K 519.27K81.93K 601.19K34.30MB29.24MB63.53MB Fineweb-2, New CCWcho131.94K6.51K38.45K1.48M 301.04K1.78M58.25MB74.54MB 132.79MB Fineweb-2, New CCKthi131.07K5.44K36.51K 763.52K 133.75K 897.27K30.79MB35.73MB66.52MB Fineweb-2, New CCTavt130.95K3.63K34.57K 670.82K78.65K 749.47K29.30MB14.97MB44.26MB Fineweb-2, New CCTakr130.70K5.29K35.99K1.73M 298.02K2.03M30.89MB45.59MB76.48MB Fineweb-2, New CCTfng429.84K3.34K33.18K1.42M 148.55K1.57M35.12MB24.87MB59.99MB Fineweb-2, New CCTale126.17K2.80K28.98K 220.84K23.64K 244.48K23.80MB16.84MB40.64MB Fineweb-2, New CCElba124.86K4.61K29.48K 394.51K73.22K 467.73K24.19MB19.19MB43.38MB Fineweb-2, New CCZanb124.46K4.76K29.21K 327.39K63.68K 391.07K26.07MB40.03MB66.10MB Fineweb-2, New CCSogo122.29K3.88K26.16K 146.13K25.41K 171.54K17.82MB20.07MB37.89MB Fineweb-2, New CCSoyo122.21K4.91K27.12K 598.89K 132.47K 731.36K25.04MB36.77MB61.81MB Fineweb-2, New CCDogr121.29K3.82K25.11K1.28M 229.94K1.51M29.94MB23.89MB53.84MB Fineweb-2, New CCKawi120.28K4.10K24.38K 396.57K80.26K 476.83K20.90MB24.30MB45.20MB Fineweb-2, New CCPhli119.16K2.88K22.04K41.16K6.19K47.35K17.52MB7.60MB25.13MB Fineweb-2, New CCCham117.92K3.60K21.52K 762.24K 153.32K 915.57K21.12MB39.91MB61.03MB Fineweb-2, New CCNbat117.61K3.19K20.80K 280.13K50.76K 330.89K18.90MB15.97MB34.87MB Fineweb-2, New CCNand117.39K3.36K20.75K 307.12K59.32K 366.44K17.76MB19.20MB36.96MB Fineweb-2, New CCOsma116.98K2.59K19.57K 495.54K75.61K 571.15K19.16MB15.11MB34.27MB Fineweb-2, New CCSind114.81K4.24K19.05K 315.61K90.31K 405.93K21.16MB18.70MB39.86MB Fineweb-2, New CCSogd114.52K2.73K17.24K 307.50K57.79K 365.30K14.67MB9.73MB24.40MB Fineweb-2, New CCPauc113.23K4.28K17.50K1.88M 609.43K2.49M13.65MB33.03MB46.67MB Fineweb-2, New CCSylo112.42K2.88K15.29K 922.71K 213.86K1.14M22.76MB22.23MB44.99MB Fineweb-2, New CCGoth211.84K1.24K13.08K 191.30K19.67K 210.97K11.59MB3.62MB15.22MB Fineweb-2, MaLARjng110.30K2.36K12.65K 595.51K 136.27K 731.78K9.43MB15.02MB24.45MB Fineweb-2, New CCChrs110.24K1.26K11.50K45.98K5.66K51.64K8.22MB5.45MB13.67MB Fineweb-2, New CCPhlp19.08K2.03K11.11K31.62K7.06K38.69K8.35MB5.61MB13.96MB Fineweb-2, New CCMand18.73K1.49K10.21K82.87K14.11K96.98K9.07MB5.24MB14.31MB Fineweb-2, New CCTglg18.58K1.88K10.46K 638.75K 140.15K 778.89K11.22MB10.89MB22.11MB Fineweb-2, New CCShaw18.41K1.28K9.69K 915.43K 139.72K1.06M13.65MB12.62MB26.27MB Fineweb-2, New CCHatr17.44K1.63K9.07K 371.48K81.61K 453.09K10.15MB13.53MB23.68MB Fineweb-2, New CCBugi27.03K1.33K8.36K95.81K18.11K 113.91K6.90MB6.18MB13.09MB Fineweb-2, MaLATagb16.58K1.14K7.72K30.92K5.37K36.30K5.84MB2.33MB8.17MB Fineweb-2, New CCPrti16.05K1.09K7.15K 225.93K40.79K 266.72K7.31MB4.57MB11.89MB Fineweb-2, New CCNarb15.22K8356.06K56.09K8.97K65.06K6.01MB7.12MB13.13MB Fineweb-2, New CCSarb14.99K8745.86K 170.46K29.86K 200.31K6.93MB15.95MB22.87MB Fineweb-2, New CCUgar14.85K6535.50K 133.05K17.92K 150.97K4.03MB2.47MB6.50MB Fineweb-2, New CCLydi14.59K1.03K5.62K28.08M6.29M34.37M77.22MB70.99MB 148.21MB Fineweb-2, New CCBuhd13.16K4483.61K7.77K1.10K8.87K2.73MB 623.88KB3.35MB Fineweb-2, New CCPerm12.87K6303.50K19.17K4.20K23.37K2.58MB1.36MB3.94MB Fineweb-2, New CCElym11.66K4962.16K61.25K18.28K79.53K1.88MB7.52MB9.40MB Fineweb-2, New CCLimb159157432.32K8.22K40.53K 754.75KB 229.80KB 984.54KB Fineweb-2, New CC</p>
<p>Table 9 :
9
Data Cleaning Statistics (part I): Comparison of document count, token count, disk size, and sources before and after data cleaning in DCAD-2000.
Lang CodeDocumentsTokensDisk SizeSourcekeepremovetotalkeepremovetotalkeepremovetotaleng_Latn1.31B 101.08M1.41B1.21T93.23B1.30T5.66TB1.49TB7.15TB Fineweb, MaLA, New CCrus_Cyrl858.53M67.21M 925.74M1.14T90.18B1.23T8.40TB2.22TB10.62TB Fineweb-2, MaLA, New CCcmn_Hani713.97M71.19M 785.16M 745.88B73.84B 819.71B2.90TB1.60TB4.50TB Fineweb-2, New CCdeu_Latn668.62M53.65M 722.27M 632.32B51.11B 683.44B2.85TB 664.79GB3.52TB Fineweb-2, MaLA, New CCspa_Latn604.45M43.33M 647.79M 483.75B34.79B 518.54B2.54TB 498.55GB3.03TB Fineweb-2, MaLA, New CCfra_Latn513.53M40.32M 553.85M 430.86B33.77B 464.64B2.15TB 491.23GB2.64TB Fineweb-2, MaLA, New CCjpn_Jpan491.47M42.47M 533.93M 278.14B22.81B 300.95B2.00TB 504.87GB2.50TB Fineweb-2, New CCita_Latn311.42M25.50M 336.93M 250.12B20.69B 270.81B1.29TB 292.75GB1.59TB Fineweb-2, MaLA, New CCpor_Latn271.48M18.67M 290.15M 204.64B14.12B 218.77B1.07TB 225.83GB1.30TB Fineweb-2, MaLA, New CCpol_Latn223.21M15.38M 238.59M 180.34B12.59B 192.93B 910.55GB 184.59GB1.10TB Fineweb-2, MaLA, New CCnld_Latn219.16M14.03M 233.19M 146.16B9.38B 155.54B 739.62GB 159.01GB 898.63GB Fineweb-2, MaLA, New CCind_Latn156.92M16.21M 173.12M60.97B5.15B66.11B 406.86GB64.84GB 471.70GB Fineweb-2, MaLAtur_Latn143.31M9.98M 153.30M 118.40B8.21B 126.61B 618.87GB 145.39GB 764.26GB Fineweb-2, MaLA, New CCvie_Latn87.77M6.19M93.96M 110.11B7.71B 117.82B 570.86GB 116.19GB 687.05GB Fineweb-2, MaLA, New CCfas_Arab82.80M9.49M92.29M67.58B7.91B75.49B 521.39GB 121.46GB 642.85GB Fineweb-2, MaLA, New CCkor_Hang79.22M6.16M85.38M59.07B4.62B63.69B 336.56GB66.70GB 403.26GB Fineweb-2, MaLA, New CCswe_Latn77.32M5.08M82.40M59.21B3.92B63.13B 269.37GB73.25GB 342.62GB Fineweb-2, MaLA, New CChun_Latn70.79M5.18M75.97M65.62B4.86B70.48B 319.58GB87.97GB 407.55GB Fineweb-2, MaLA, New CCukr_Cyrl67.87M4.31M72.18M53.79B3.41B57.20B 428.74GB82.51GB 511.25GB Fineweb-2, MaLA, New CCell_Grek67.48M5.67M73.15M57.63B5.03B62.66B 425.03GB 112.67GB 537.71GB Fineweb-2, MaLA, New CCtha_Thai55.47M4.29M59.76M46.31B3.59B49.90B 525.54GB 110.37GB 635.91GB Fineweb-2, MaLA, New CCarb_Arab53.70M4.06M57.76M25.21B1.92B27.13B 278.77GB72.25GB 351.02GB Fineweb-2, MaLAaze_Latn51.38M6.44M57.82M3.30B 392.00M3.70B41.90GB10.70GB52.60GB MaLAslv_Latn50.41M4.05M54.46M11.66B 836.48M12.50B69.22GB12.64GB81.87GB Fineweb-2, MaLA, New CCcat_Latn48.83M3.78M52.61M16.49B1.13B17.62B96.97GB14.24GB 111.21GB Fineweb-2, MaLA, New CCfin_Latn47.80M4.09M51.89M43.43B3.75B47.19B 202.14GB57.62GB 259.76GB Fineweb-2, MaLA, New CCces_Latn47.54M3.21M50.74M42.20B2.84B45.04B 195.62GB48.74GB 244.36GB MaLA, New CChbs_Latn42.98M8.05M51.04M1.53B 287.34M1.82B22.41GB6.41GB28.82GB MaLAfil_Latn40.15M6.32M46.47M3.47B 477.70M3.94B31.22GB9.20GB40.42GB Fineweb-2, MaLAmal_Mlym39.10M3.88M42.98M7.00B 558.83M7.56B94.47GB18.30GB 112.78GB Fineweb-2, MaLA, New CCnob_Latn38.88M4.33M43.21M24.13B2.81B26.94B 139.85GB66.29GB 206.15GB Fineweb-2, MaLAguj_Gujr38.82M4.54M43.36M5.07B 461.54M5.53B60.08GB13.49GB73.57GB Fineweb-2, MaLA, New CCbul_Cyrl37.11M2.56M39.67M32.29B2.23B34.51B 245.84GB55.86GB 301.69GB Fineweb-2, MaLA, New CCkan_Knda34.20M2.70M36.90M4.76B 359.21M5.12B68.82GB11.13GB79.95GB Fineweb-2, MaLA, New CChin_Deva29.15M2.47M31.62M22.08B1.81B23.89B 219.46GB46.45GB 265.91GB Fineweb-2, MaLA, New CCtam_Taml26.55M2.90M29.45M5.88B 460.75M6.34B80.26GB19.29GB99.55GB Fineweb-2, MaLA, New CCkaz_Cyrl25.78M1.67M27.45M6.37B 432.67M6.80B64.36GB12.99GB77.35GB Fineweb-2, MaLA, New CCheb_Hebr25.24M1.61M26.85M20.74B1.33B22.07B 147.85GB28.75GB 176.60GB Fineweb-2, MaLA, New CCara_Arab25.14M3.24M28.39M17.21B2.23B19.44B 152.73GB71.93GB 224.66GB MaLA, New CCsrp_Cyrl25.13M1.75M26.88M6.91B 496.07M7.41B60.34GB8.50GB68.84GB Fineweb-2, MaLA, New CCest_Latn24.18M2.86M27.04M2.89B 294.20M3.18B26.17GB8.91GB35.08GB MaLA, New CCsqi_Latn24.16M3.25M27.41M2.38B 237.81M2.61B21.08GB5.03GB26.11GB MaLA, New CCisl_Latn24.06M2.23M26.29M6.32B 561.74M6.89B34.88GB9.09GB43.97GB Fineweb-2, MaLA, New CCpan_Guru24.02M3.16M27.19M2.27B 227.60M2.50B26.69GB8.59GB35.28GB MaLA, New CCmlt_Latn23.37M2.08M25.45M3.24B 322.80M3.56B16.40GB4.96GB21.36GB Fineweb-2, MaLA, New CCmkd_Cyrl22.61M1.89M24.50M5.29B 396.98M5.68B51.37GB7.08GB58.45GB Fineweb-2, MaLA, New CCbos_Latn21.62M1.71M23.33M11.01B 831.59M11.84B59.71GB10.67GB70.38GB Fineweb-2, MaLA, New CCkat_Geor20.27M1.30M21.57M6.16B 413.36M6.57B82.54GB15.10GB97.65GB Fineweb-2, MaLA, New CClit_Latn20.09M1.51M21.60M17.47B1.33B18.80B91.29GB18.30GB 109.59GB Fineweb-2, MaLA, New CCben_Beng19.90M1.37M21.28M12.26B 848.75M13.11B 143.64GB30.36GB 174.00GB Fineweb-2, MaLA, New CChrv_Latn19.83M1.54M21.37M15.02B1.19B16.21B76.53GB16.65GB93.18GB Fineweb-2, MaLA, New CCglg_Latn19.31M1.58M20.89M4.45B 372.72M4.83B28.40GB4.50GB32.90GB Fineweb-2, MaLA, New CCron_Latn18.28M1.42M19.69M23.42B1.81B25.23B 110.94GB20.14GB 131.08GB MaLA, New CCceb_Latn18.14M1.82M19.97M1.91B 184.52M2.09B14.11GB2.06GB16.18GB Fineweb-2, MaLA, New CChye_Armn16.93M1.40M18.33M4.65B 392.68M5.04B41.29GB10.76GB52.05GB Fineweb-2, MaLA, New CCmsa_Latn16.90M1.51M18.40M12.27B1.05B13.32B67.19GB34.22GB 101.42GB MaLA, New CCtgk_Cyrl16.60M1.04M17.64M3.46B 241.47M3.70B29.00GB5.01GB34.01GB Fineweb-2, MaLA, New CCmar_Deva15.37M1.35M16.72M4.05B 287.28M4.34B52.49GB7.16GB59.65GB Fineweb-2, MaLA, New CCbel_Cyrl15.22M1.06M16.29M5.30B 353.85M5.65B45.23GB6.76GB51.99GB Fineweb-2, MaLA, New CCnep_Deva13.18M1.74M14.91M3.40B 354.95M3.75B57.72GB14.16GB71.88GB MaLA, New CCurd_Arab12.92M1.28M14.20M5.63B 463.49M6.09B43.36GB8.33GB51.69GB Fineweb-2, MaLA, New CCslk_Latn12.79M 850.42K13.64M10.71B 712.57M11.43B53.49GB10.01GB63.50GB MaLA, New CCmon_Cyrl11.46M1.37M12.83M2.05B 225.17M2.27B25.55GB7.89GB33.44GB MaLA, New CCdan_Latn11.33M 645.36K11.98M8.91B 506.75M9.42B42.48GB9.31GB51.78GB MaLA, New CCeus_Latn10.88M 720.92K11.60M2.86B 180.73M3.04B18.54GB2.98GB21.52GB Fineweb-2, MaLA, New CCazj_Latn10.37M 764.57K11.14M6.02B 427.97M6.45B54.46GB9.98GB64.44GB Fineweb-2, MaLA, New CCswa_Latn10.32M1.78M12.10M 968.63M 131.70M1.10B8.88GB2.59GB11.47GB MaLA, New CCals_Latn9.94M 695.21K10.64M7.84B 540.49M8.38B22.16GB3.80GB25.97GB Fineweb-2, MaLAsin_Sinh9.91M1.12M11.03M2.93B 251.40M3.18B32.73GB7.64GB40.37GB Fineweb-2, MaLA, New CClat_Latn9.86M 968.13K10.83M1.67B 209.54M1.88B8.93GB3.35GB12.27GB Fineweb-2, MaLA, New CCtel_Telu9.81M 790.37K10.60M3.90B 293.32M4.19B47.82GB9.23GB57.05GB Fineweb-2, MaLA, New CCafr_Latn9.38M 858.54K10.24M3.02B 252.81M3.27B16.05GB3.08GB19.13GB Fineweb-2, MaLA, New CC</p>
<p>Table 10 :
10
Data Cleaning Statistics (part II): Comparison of document count, token count, disk size, and sources before and after data cleaning in DCAD-2000.
Lang CodeDocumentsTokensDisk SizeSourcekeepremovetotalkeepremovetotalkeepremovetotalekk_Latn9.24M 772.47K 10.01M4.79B 401.83M5.19B 38.34GB11.83GB 50.16GB Fineweb-2, MaLAzsm_Latn8.67M 795.54K9.47M4.22B 365.48M4.59B 31.54GB8.93GB 40.48GB Fineweb-2, MaLAltz_Latn8.59M1.21M9.79M1.18B 146.26M1.33B6.77GB1.91GB8.68GB Fineweb-2, MaLA, New CCsom_Latn7.47M 716.70K8.19M2.20B 193.46M2.40B 10.27GB3.34GB 13.61GB Fineweb-2, MaLA, New CCkir_Cyrl6.47M 468.94K6.94M2.31B 183.29M2.49B 21.00GB3.63GB 24.63GB Fineweb-2, MaLA, New CCcym_Latn6.47M 515.43K6.99M2.01B 141.85M2.15B 10.29GB1.99GB 12.28GB Fineweb-2, MaLA, New CCnor_Latn6.13M 733.57K6.87M1.27B 150.12M1.42B8.91GB2.74GB 11.65GB MaLA, New CCuzb_Latn6.07M 715.37K6.78M 929.54M98.71M1.03B8.76GB2.73GB 11.49GB MaLA, New CCund_Kana5.83M1.11M6.94M1.13B 219.26M1.35B 16.90GB14.33GB 31.23GB Fineweb-2, New CCmya_Mymr 5.80M 449.02K6.25M5.28B 404.36M5.69B 40.05GB7.53GB 47.57GB Fineweb-2, MaLA, New CCepo_Latn5.77M 456.78K6.23M2.38B 177.31M2.56B 12.03GB2.25GB 14.27GB Fineweb-2, MaLA, New CCary_Arab5.67M 465.36K6.14M1.38B 114.17M1.50B 18.12GB4.32GB 22.44GB Fineweb-2, MaLAlvs_Latn5.51M 382.81K5.89M2.74B 185.99M2.92B 21.58GB6.85GB 28.43GB Fineweb-2, MaLAhau_Latn5.48M 662.28K6.15M 438.94M49.38M 488.32M3.22GB1.09GB4.32GB MaLAgle_Latn5.47M 428.92K5.90M1.65B 134.54M1.78B9.41GB1.55GB 10.96GB Fineweb-2, MaLA, New CCnno_Latn5.19M 553.48K5.75M1.35B 124.05M1.48B7.48GB1.85GB9.33GB Fineweb-2, MaLA, New CCory_Orya5.13M 444.55K5.57M 325.74M23.33M 349.07M7.34GB1.07GB8.41GB Fineweb-2, MaLAamh_Ethi4.86M 302.32K5.17M1.21B77.95M1.28B 10.27GB1.56GB 11.83GB Fineweb-2, MaLA, New CCkhm_Khmr 4.74M 344.10K5.09M2.23B 158.45M2.39B 30.49GB4.58GB 35.08GB Fineweb-2, MaLA, New CCtat_Cyrl4.72M 390.38K5.11M1.29B 103.35M1.39B 11.66GB2.16GB 13.82GB Fineweb-2, MaLA, New CCund_Bamu4.71M1.00M5.71M 199.46M42.49M 241.95M 79.67GB19.47GB 99.14GB Fineweb-2, New CCund_Copt4.40M 361.86K4.76M 218.11M17.95M 236.07M8.96GB 860.56MB9.82GB Fineweb-2, New CCarz_Arab4.19M 347.36K4.54M 794.23M62.86M 857.09M6.87GB1.16GB8.03GB Fineweb-2, MaLA, New CCund_Tang3.94M 741.81K4.68M 209.68M39.47M 249.15M 22.70GB7.67GB 30.36GB Fineweb-2, New CCund_Xsux3.90M 694.59K4.59M 276.93M49.35M 326.28M 13.84GB9.74GB 23.58GB Fineweb-2, New CClav_Latn3.76M 347.11K4.11M2.12B 196.45M2.31B 13.96GB7.36GB 21.32GB MaLA, New CCpus_Arab3.71M 493.24K4.21M 905.77M 106.28M1.01B7.66GB2.36GB 10.02GB MaLA, New CChbs_Cyrl3.47M 463.55K3.93M 131.15M17.53M 148.69M2.47GB 544.73MB3.02GB MaLA, New CCwar_Latn3.43M 283.72K3.71M 137.36M11.19M 148.55M1.84GB 161.55MB2.00GB Fineweb-2, MaLA, New CCund_Yiii3.39M 417.38K3.81M 232.88M28.68M 261.56M 25.82GB6.24GB 32.05GB Fineweb-2, New CCmulti_Latn3.11M 394.01K3.50M2.39B 303.45M2.70B 18.42GB7.60GB 26.02GB New CCmlg_Latn2.85M 437.74K3.29M 288.34M41.29M 329.63M2.74GB 765.89MB3.51GB MaLA, New CCund_Hira2.78M 579.38K3.36M 361.77M75.28M 437.05M4.87GB4.04GB8.91GB Fineweb-2, New CCuzn_Cyrl2.61M 304.12K2.91M 396.89M30.84M 427.73M6.39GB1.47GB7.86GB Fineweb-2, MaLAhat_Latn2.58M 226.91K2.81M 464.18M41.25M 505.43M2.60GB 548.40MB3.15GB Fineweb-2, MaLA, New CCzul_Latn2.47M 294.21K2.76M 333.05M38.27M 371.33M2.15GB 642.83MB2.79GB Fineweb-2, MaLAkur_Latn2.41M 327.93K2.74M 482.02M51.67M 533.69M3.40GB1.04GB4.44GB MaLAdiv_Thaa2.25M 263.72K2.52M 418.22M43.98M 462.20M4.37GB1.02GB5.38GB Fineweb-2, MaLA, New CCtgl_Latn2.24M 345.69K2.59M 369.19M35.56M 404.74M2.75GB 669.71MB3.42GB MaLA, New CCuzb_Cyrl2.22M 314.25K2.54M 194.02M27.60M 221.61M2.96GB1.14GB4.10GB MaLAfry_Latn2.14M 232.49K2.38M 605.32M65.90M 671.22M3.10GB 914.11MB4.01GB Fineweb-2, MaLA, New CCsna_Latn2.14M 181.61K2.32M 295.33M24.54M 319.87M1.84GB 428.76MB2.27GB Fineweb-2, MaLAfao_Latn2.09M 163.66K2.26M 199.43M14.19M 213.61M1.69GB 392.84MB2.08GB Fineweb-2, MaLAund_Laoo2.06M 364.70K2.42M 212.14M37.64M 249.78M4.20GB2.59GB6.79GB Fineweb-2, New CCsun_Latn1.99M 193.82K2.19M 275.24M25.28M 300.53M1.71GB 543.58MB2.25GB Fineweb-2, MaLA, New CCsnd_Arab1.91M 154.84K2.06M1.12B 105.00M1.22B5.27GB1.88GB7.15GB Fineweb-2, MaLA, New CCund_Cyrl1.86M 427.20K2.29M1.32B 302.88M1.62B5.09GB18.81GB 23.90GB Fineweb-2, New CCund_Kits1.86M 315.45K2.17M 269.54M45.75M 315.29M 12.47GB17.12GB 29.58GB Fineweb-2, New CCbak_Cyrl1.85M 132.43K1.99M 401.91M27.62M 429.53M3.87GB 733.50MB4.60GB Fineweb-2, MaLA, New CCasm_Beng1.82M 115.52K1.93M 380.78M23.67M 404.45M4.50GB 907.15MB5.40GB Fineweb-2, MaLA, New CCcos_Latn1.79M 274.66K2.06M 228.06M35.24M 263.31M1.10GB 580.00MB1.68GB MaLAckb_Arab1.78M 177.88K1.96M 841.60M76.59M 918.19M6.48GB1.52GB8.00GB Fineweb-2, MaLA, New CCund_Hluw1.71M 374.92K2.09M70.77M15.47M86.25M3.19GB3.45GB6.64GB Fineweb-2, New CCast_Latn1.63M 144.18K1.77M 213.12M19.08M 232.20M1.39GB 385.45MB1.78GB Fineweb-2, MaLA, New CCjpn_Japn1.60M 177.40K1.78M 148.77M17.99M 166.76M6.05GB2.16GB8.21GB MaLAibo_Latn1.59M 117.64K1.71M 233.50M16.65M 250.14M1.45GB 446.07MB1.89GB Fineweb-2, MaLAund_Grek1.57M 224.66K1.79M 755.84M 108.19M 864.02M6.94GB7.17GB 14.12GB Fineweb-2, New CCmri_Latn1.53M 133.72K1.67M 354.50M28.72M 383.22M1.71GB 472.53MB2.18GB Fineweb-2, MaLAars_Arab1.53M 108.78K1.64M 461.05M32.76M 493.81M4.88GB1.85GB6.73GB Fineweb-2, New CCanp_Deva1.44M 140.26K1.58M 805.49M78.54M 884.04M 10.69GB2.12GB 12.81GB Fineweb-2, MaLAkhk_Cyrl1.44M 128.14K1.57M 615.04M54.80M 669.84M8.17GB1.83GB 10.00GB Fineweb-2, New CCund_Shrd1.41M 216.59K1.62M 130.80M20.13M 150.93M6.06GB2.35GB8.40GB Fineweb-2, New CClao_Laoo1.40M 105.80K1.50M 628.08M49.71M 677.79M7.65GB1.36GB9.01GB Fineweb-2, MaLA, New CCund_Lina1.37M 271.63K1.64M 130.39M25.87M 156.26M6.97GB3.85GB 10.82GB Fineweb-2, New CCund_Samr1.35M 158.99K1.51M64.06M7.54M71.59M4.30GB1.72GB6.02GB Fineweb-2, New CCori_Orya1.34M 145.91K1.48M 128.69M11.97M 140.66M2.16GB 770.15MB2.93GB MaLAjav_Latn1.26M 122.51K1.38M 379.69M35.26M 414.95M1.96GB 587.75MB2.55GB Fineweb-2, MaLA, New CCyid_Hebr1.25M 160.66K1.41M 287.37M36.14M 323.51M2.84GB1.30GB4.14GB MaLA, New CCund_Cans1.23M 248.05K1.48M 106.39M21.43M 127.83M3.48GB2.77GB6.25GB Fineweb-2, New CC</p>
<p>Table 11 :
11
Data Cleaning Statistics (part III): Comparison of document count, token count, disk size, and sources before and after data cleaning in DCAD-2000.
Lang CodeDocumentsTokensDisk SizeSourcekeepremovetotalkeep removetotalkeepremovetotalnya_Latn1.21M 138.31K1.34M 230.59M 26.29M 256.88M1.34GB 437.81MB1.78GB Fineweb-2, MaLAhmn_Latn1.20M 195.18K1.40M 173.07M 28.59M 201.66M1.08GB 543.90MB1.63GB MaLAtir_Ethi1.20M78.32K1.28M 125.79M8.16M 133.96M1.15GB 290.56MB1.44GB Fineweb-2, MaLAuig_Arab1.19M78.60K1.27M 513.72M 37.42M 551.15M3.72GB 937.66MB4.65GB Fineweb-2, MaLA, New CCwln_Latn1.18M74.38K1.25M53.99M3.61M57.59M 520.40MB78.21MB 598.61MB Fineweb-2, MaLA, New CCund_Adlm1.12M 194.29K1.32M43.63M7.55M51.18M1.10GB 853.95MB1.95GB Fineweb-2, New CCund_Egyp1.12M 190.50K1.31M97.41M 16.58M 113.99M2.54GB3.52GB6.05GB Fineweb-2, New CCund_Syrc1.12M 115.88K1.23M42.71M4.43M47.14M20.68GB4.34GB25.01GB Fineweb-2, New CCswh_Latn1.12M82.67K1.20M 449.92M 32.71M 482.63M3.34GB 803.12MB4.15GB Fineweb-2, MaLAyor_Latn1.12M 108.67K1.22M 189.62M 18.77M 208.39M1.08GB 304.95MB1.38GB Fineweb-2, MaLA, New CCuzn_Latn1.03M68.06K1.10M 466.19M 30.78M 496.97M4.03GB1.03GB5.06GB Fineweb-2, New CCund_Mend1.03M 293.72K1.32M16.58M4.75M21.33M 893.39MB2.06GB2.95GB Fineweb-2, New CCxho_Latn1.02M88.44K1.11M 168.59M 13.93M 182.52M1.19GB 247.71MB1.44GB Fineweb-2, MaLAgla_Latn1.01M 115.44K1.13M 518.47M 76.34M 594.81M2.03GB 904.76MB2.94GB Fineweb-2, MaLA, New CCbre_Latn980.75K86.36K1.07M 134.68M 11.68M 146.37M 757.53MB 231.46MB 988.99MB Fineweb-2, MaLA, New CCsot_Latn917.37K78.48K 995.85K 223.24M 17.82M 241.06M1.09GB 283.15MB1.37GB Fineweb-2, MaLAnan_Latn905.48K86.68K 992.16K26.58M2.54M29.12M 483.99MB95.09MB 579.08MB Fineweb-2, MaLAtel_Latn898.42K92.51K 990.93K 204.17M 21.27M 225.44M 843.51MB 444.92MB1.29GB Fineweb-2, MaLAbew_Latn885.97K99.33K 985.30K 370.27M 41.51M 411.78M2.85GB 776.53MB3.62GB Fineweb-2, New CCsmo_Latn883.15K83.25K 966.41K 241.45M 21.17M 262.62M1.15GB 290.83MB1.44GB Fineweb-2, MaLAglk_Arab876.52K99.66K 976.18K44.95M5.30M50.24M 630.38MB 171.44MB 801.82MB Fineweb-2, MaLAche_Cyrl875.25K 117.29K 992.54K 118.78M 15.18M 133.96M1.05GB 346.83MB1.40GB Fineweb-2, MaLA, New CCorm_Latn859.55K77.40K 936.95K35.46M3.19M38.65M 476.68MB 150.02MB 626.69MB MaLAzho_Hani840.53K65.42K 905.95K 578.50M 46.68M 625.18M2.67GB 980.93MB3.65GB MaLAhaw_Latn808.97K88.12K 897.10K 227.68M 23.61M 251.29M 869.19MB 300.40MB1.17GB Fineweb-2, MaLApnb_Arab806.70K71.03K 877.73K 133.55M 11.76M 145.31M 881.83MB 493.40MB1.38GB Fineweb-2, MaLA, New CCoci_Latn760.65K59.16K 819.82K 123.30M 10.54M 133.84M 706.68MB 193.69MB 900.37MB Fineweb-2, MaLA, New CCund_Linb735.07K 107.67K 842.75K52.97M7.76M60.73M6.30GB 997.90MB7.30GB Fineweb-2, New CCchv_Cyrl731.68K60.72K 792.40K 188.93M 16.35M 205.28M1.10GB 361.84MB1.46GB Fineweb-2, MaLA, New CCkin_Latn701.70K67.29K 768.99K 197.65M 16.84M 214.49M1.43GB 160.27MB1.59GB Fineweb-2, MaLAsrp_Latn630.88K54.65K 685.53K 158.44M 13.19M 171.63M 775.01MB 209.48MB 984.49MB MaLAund_Brai590.10K 125.33K 715.43K57.85M 12.29M70.13M1.94GB1.30GB3.24GB Fineweb-2, New CCkaa_Cyrl588.71K48.01K 636.72K1.08B 86.21M1.16B3.58GB 620.59MB4.20GB Fineweb-2, MaLAlug_Latn570.88K40.31K 611.19K36.43M2.65M39.08M 344.92MB85.21MB 430.13MB Fineweb-2, MaLAund_Sgnw567.29K 106.45K 673.74K37.34M7.01M44.34M1.40GB1.11GB2.50GB Fineweb-2, New CCpcm_Latn563.55K80.45K 644.00K 135.97M 19.60M 155.57M1.45GB 231.26MB1.68GB Fineweb-2, MaLApbt_Arab556.45K36.70K 593.15K 273.04M 18.00M 291.04M2.40GB 481.43MB2.88GB Fineweb-2, MaLAmin_Latn548.22K32.98K 581.19K28.26M1.78M30.04M 326.92MB43.32MB 370.24MB Fineweb-2, MaLAtuk_Latn526.60K48.40K 575.00K 211.69M 23.04M 234.74M1.14GB 368.23MB1.51GB Fineweb-2, MaLAlim_Latn526.45K43.83K 570.28K49.16M4.85M54.01M 338.07MB70.26MB 408.33MB Fineweb-2, MaLA, New CCund_Hung520.10K 155.23K 675.33K42.34M 12.64M54.98M1.94GB2.32GB4.25GB Fineweb-2, New CCgsw_Latn519.60K64.76K 584.36K 171.13M 22.15M 193.28M2.02GB 248.45MB2.27GB Fineweb-2, MaLA, New CCaze_Arab481.85K 107.19K 589.05K16.65M3.70M20.35M 283.94MB 125.40MB 409.33MB MaLAkmr_Latn473.75K37.03K 510.79K 239.78M 19.24M 259.01M1.64GB 366.13MB2.01GB Fineweb-2, MaLA, New CCroh_Latn467.79K40.88K 508.66K59.96M5.00M64.96M 373.84MB 133.62MB 507.46MB Fineweb-2, MaLA, New CCvec_Latn451.53K28.94K 480.47K35.51M2.41M37.92M 248.96MB70.25MB 319.21MB Fineweb-2, MaLAsan_Deva426.60K30.30K 456.90K 186.19M 14.19M 200.38M1.37GB 884.42MB2.25GB Fineweb-2, MaLA, New CCund_Bali422.49K77.08K 499.57K39.62M7.23M46.84M1.19GB 662.91MB1.85GB Fineweb-2, New CCund_Nshu419.71K89.40K 509.11K38.53M8.21M46.74M 993.06MB1.28GB2.27GB Fineweb-2, New CCund_Modi386.82K67.33K 454.15K52.58M9.15M61.73M16.45GB7.42GB23.87GB Fineweb-2, New CCgmh_Latn383.58K47.47K 431.05K 769.12M 95.18M 864.30M5.51GB1.42GB6.93GB Fineweb-2, New CCsco_Latn382.19K37.49K 419.69K43.05M4.46M47.52M 357.63MB98.46MB 456.10MB Fineweb-2, MaLAnds_Latn379.54K44.24K 423.78K79.45M 11.68M91.13M 384.74MB 126.48MB 511.22MB Fineweb-2, MaLA, New CCund_Lana377.58K 110.80K 488.38K47.55M 13.95M61.50M 688.16MB2.05GB2.74GB Fineweb-2, New CCazb_Arab376.14K24.16K 400.30K81.10M6.51M87.61M 615.69MB 203.89MB 819.58MB Fineweb-2, MaLA, New CCtsn_Latn375.82K23.43K 399.25K24.79M1.54M26.33M 206.56MB41.32MB 247.88MB Fineweb-2, MaLAund_Mong364.92K51.36K 416.28K78.04M 10.98M89.03M1.32GB 827.40MB2.15GB Fineweb-2, New CCsah_Cyrl357.02K24.17K 381.19K 110.13M7.77M 117.89M1.05GB 202.76MB1.25GB MaLA, New CCund_Ethi351.77K49.20K 400.97K39.75M5.56M45.31M1.23GB1.08GB2.31GB Fineweb-2, New CCrus_Latn349.61K47.55K 397.17K77.31M 10.54M87.85M 755.00MB 485.49MB1.24GB MaLApri_Latn348.99K27.20K 376.20K 142.27M 11.09M 153.36M2.15GB 505.82MB2.66GB Fineweb-2, New CCund_Hebr345.20K46.87K 392.07K17.42M2.36M19.78M 548.23MB 461.10MB1.01GB Fineweb-2, New CCmon_Latn344.80K46.68K 391.48K31.56M4.27M35.84M 180.12MB 271.24MB 451.37MB MaLApap_Latn339.80K22.62K 362.42K 127.89M8.52M 136.41M 678.73MB 223.10MB 901.83MB Fineweb-2, MaLAtgk_Latn337.95K48.39K 386.35K26.44M3.79M30.22M 198.08MB 219.19MB 417.27MB MaLAplt_Latn330.57K28.23K 358.80K 118.46M8.02M 126.48M 951.31MB 189.98MB1.14GB Fineweb-2, MaLAlmo_Latn324.18K29.25K 353.43K41.37M4.09M45.46M 230.80MB58.92MB 289.72MB Fineweb-2, MaLA, New CCbod_Tibt318.52K34.22K 352.75K 252.06M 28.33M 280.39M3.37GB 998.77MB4.37GB MaLA, New CCund_Saur315.78K73.82K 389.60K15.26M3.57M18.83M 398.55MB 489.07MB 887.62MB Fineweb-2, New CCyue_Hani300.49K34.04K 334.53K9.02M1.03M10.06M 790.86MB 161.03MB 951.90MB Fineweb-2, MaLA, New CC
We follow the criteria from Flores-101[17] to categorize languages: High: &gt; 100M ; Medium: (1M, 100M ); Low: (100K, 1M ); Very Low: &lt; 100K.
Please refer to Appendix A for more details.
https://en.wikipedia.org/wiki/ISO_639-3
https://github.com/cisnlp/GlotScript
https://github.com/huggingface/datatrove
https://github.com/huggingface/fineweb-2
https://github.com/thisandagain/washyourmouthoutwithsoap
KenLM models are only trained for languages with sufficient clean Wikipedia data (minimum
,000 high-quality sentences). For other languages, we assign a default perplexity score of 500.
We also evaluate some other algorithms, please refer to Section 5 for more details.
We use the default settings of the specific anomaly detection algorithm in Scikit-learn, applying these settings globally rather than individually for each feature or language.
https://www.ksyun.com
https://kubernetes.io
Geographic data source: https://glottolog.org
https://huggingface.co/spaces/HuggingFaceFW/blogpost-fine-tasks
We use the implementation from https://github.com/bigscience-workshop/data-preparation
https://scikit-learn.org
https://github.com/huggingface/lighteval
https://scikit-learn.org
AcknowledgmentsThis work is supported by the Beijing Municipal Science and Technology Plan Project (Z241100001324025) and is also supported by the AI9Stars community.In addition, support was received from the European Research Council (ERC) under the Horizon Europe Research and Innovation Program of the European Union (Grant Agreement No. 101113091), as well as from the German Research Foundation (DFG; Grant FR 2829/7-1).250 500 750 1000 1250 1500 1750 2000 Number of Words 690.09 869.02 1368.97 1780.271929.60 1975.97 1642.051712.10 1522.16MaLA FineWeb-2 CC-MAIN-2024-22 CC-MAIN-2024-26 CC-MAIN-2024-30 CC-MAIN-2024-33 CC-MAIN-2024-38 CC-MAIN-2024-42 CC-MAIN-2024-46A Statistical Analysis of Multilingual DatasetsIn this section, we explore the statistical characteristics of the dataset through visual analysis, focusing on the distribution of data across different languages and the variations observed across different shards.We highlight the limitations of existing data cleaning methods that rely on fixed thresholds, particularly in the imbalanced data distribution scenarios.Specifically, when there are substantial discrepancies in word count distributions, these threshold-based cleaning methods are prone to errors, which fail to accurately distinguish between high-quality and low-quality data.
Large language models: A survey. Shervin Minaee, Tomas Mikolov, Narjes Nikzad, Meysam Chenaghlu, Richard Socher, Xavier Amatriain, Jianfeng Gao, arXiv:2402.061962024arXiv preprint</p>
<p>Multilingual large language models: A systematic survey. Shaolin Zhu, Shaoyang Xu, Haoran Sun, Leiyu Pan, Menglong Cui, Jiangcun Du, Renren Jin, António Branco, Deyi Xiong, arXiv:2411.110722024arXiv preprint</p>
<p>Kaiyu Huang, Fengran Mo, Hongliang Li, You Li, Yuanchi Zhang, Weijian Yi, Yulong Mao, Jinchen Liu, Yuzhuang Xu, Jinan Xu, arXiv:2405.10936A survey on large language models with multilingualism: Recent advances and new frontiers. 2024arXiv preprint</p>
<p>LLMs beyond English: Scaling the multilingual capability of LLMs with cross-lingual feedback. Wen Lai, Mohsen Mesgar, Alexander Fraser, Findings of the Association for Computational Linguistics: ACL 2024. August 2024</p>
<p>Aya model: An instruction finetuned open-access multilingual language model. Ahmet Üstün, Viraat Aryabumi, Zheng Yong, Wei-Yin Ko, D' Daniel, Gbemileke Souza, Neel Onilude, Shivalika Bhandari, Hui-Lee Singh, Amr Ooi, Freddie Kayid, Phil Vargus, Shayne Blunsom, Niklas Longpre, Marzieh Muennighoff, Julia Fadaee, Sara Kreutzer, Hooker, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. Long Papers. the 62nd Annual Meeting of the Association for Computational LinguisticsAugust 20241</p>
<p>Exploring the limits of transfer learning with a unified text-to-text transformer. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, Journal of machine learning research. 211402020</p>
<p>Towards a cleaner documentoriented multilingual crawled corpus. Julien Abadji, Pedro Ortiz Suarez, Laurent Romary, Benoît Sagot, arXiv:2201.066422022arXiv preprint</p>
<p>Glot500: Scaling multilingual corpora and language models to 500 languages. Ayyoob Imani, Peiqin Lin, Silvia Amir Hossein Kargaran, Severini, Jalili Masoud, Nora Sabet, Chunlan Kassner, Helmut Ma, André Schmid, François Martins, Hinrich Yvon, Schütze, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. the 61st Annual Meeting of the Association for Computational LinguisticsJuly 20231</p>
<p>CulturaX: A cleaned, enormous, and multilingual dataset for large language models in 167 languages. Thuat Nguyen, Chien Van Nguyen, Dac Viet, Hieu Lai, Man, Trung Nghia, Franck Ngo, Ryan A Dernoncourt, Thien Huu Rossi, Nguyen, Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024). the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)May 2024</p>
<p>Madlad-400: A multilingual and document-level large audited dataset. Sneha Kudugunta, Isaac Caswell, Biao Zhang, Xavier Garcia, Derrick Xin, Aditya Kusupati, Romi Stella, Ankur Bapna, Orhan Firat, Advances in Neural Information Processing Systems. 202436</p>
<p>Shaoxiong Ji, Zihao Li, Indraneil Paul, Jaakko Paavola, Peiqin Lin, Pinzhen Chen, O' Dayyán, Hengyu Brien, Hinrich Luo, Jörg Schütze, Tiedemann, arXiv:2409.17892Emma-500: Enhancing massively multilingual adaptation of large language models. 2024arXiv preprint</p>
<p>Glotcc: An open broad-coverage commoncrawl corpus and pipeline for minority languages. François Amir Hossein Kargaran, Hinrich Yvon, Schütze, arXiv:2410.238252024arXiv preprint</p>
<p>A new massive multilingual dataset for high-performance language technologies. Ona De Gibert, Graeme Nail, Nikolay Arefyev, Marta Bañón, Jelmer Van Der Linde, Shaoxiong Ji, Jaume Zaragoza-Bernabeu, Mikko Aulamo, Gema Ramírez-Sánchez, Andrey Kutuzov, Sampo Pyysalo, Stephan Oepen, Jörg Tiedemann, Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024). the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)May 2024</p>
<p>Fineweb2: One pipeline to scale them all-adapting pre-training data processing to every language. Guilherme Penedo, Hynek Kydlíček, Vinko Sabolčec, Bettina Messmer, Negar Foroutan, Colin Amir Hossein Kargaran, Martin Raffel, Leandro Jaggi, Thomas Von Werra, Wolf, arXiv:2506.209202025arXiv preprint</p>
<p>Peiqin Lin, Shaoxiong Ji, Jörg Tiedemann, André Ft Martins, Hinrich Schütze, arXiv:2401.13303Mala-500: Massive language adaptation of large language models. 2024arXiv preprint</p>
<p>A survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions. Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhangyin Feng, Haotian Wang, Qianglong Chen, Weihua Peng, Xiaocheng Feng, Bing Qin, arXiv:2311.052322023arXiv preprint</p>
<p>The Flores-101 evaluation benchmark for low-resource and multilingual machine translation. Naman Goyal, Cynthia Gao, Vishrav Chaudhary, Peng-Jen Chen, Guillaume Wenzek, Da Ju, Sanjana Krishnan, Marc ' , Aurelio Ranzato, Francisco Guzmán, Angela Fan, Transactions of the Association for Computational Linguistics. 102022</p>
<p>Sailor: Open language models for south-East Asia. Longxu Dou, Qian Liu, Guangtao Zeng, Jia Guo, Jiahui Zhou, Xin Mao, Ziqi Jin, Wei Lu, Min Lin, Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing: System Demonstrations. the 2024 Conference on Empirical Methods in Natural Language Processing: System DemonstrationsNovember 2024</p>
<p>MC 2 : Towards transparent and culturally-aware NLP for minority languages in China. Chen Zhang, Mingxu Tao, Quzhe Huang, Jiuheng Lin, Zhibin Chen, Yansong Feng, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. Long Papers. the 62nd Annual Meeting of the Association for Computational LinguisticsAugust 20241</p>
<p>A survey on data selection for language models. Alon Albalak, Yanai Elazar, Sang Michael Xie, Shayne Longpre, Nathan Lambert, Xinyi Wang, Niklas Muennighoff, Bairu Hou, Liangming Pan, Haewon Jeong, arXiv:2402.168272024arXiv preprint</p>
<p>GlotLID: Language identification for low-resource languages. Ayyoob Amir Hossein Kargaran, François Imani, Hinrich Yvon, Schuetze, Findings of the Association for Computational Linguistics: EMNLP 2023. December 2023</p>
<p>Large language models for forecasting and anomaly detection: A systematic literature review. Jing Su, Chufeng Jiang, Xin Jin, Yuxin Qiao, Tingsong Xiao, Hongda Ma, Rong Wei, Zhi Jing, Jiajun Xu, Junhong Lin, arXiv:2402.103502024arXiv preprint</p>
<p>The bigscience roots corpus: A 1.6 tb composite multilingual dataset. Lucile Hugo Laurençon, Thomas Saulnier, Christopher Wang, Le Akiki ; Teven, Leandro Scao, Chenghao Von Werra, Eduardo González Mou, Huu Ponferrada, Nguyen, Advances in Neural Information Processing Systems. 202235Albert Villanova del Moral</p>
<p>SIB-200: A simple, inclusive, and big evaluation dataset for topic classification in 200+ languages and dialects. David Ifeoluwa, Adelani , Hannah Liu, Xiaoyu Shen, Nikita Vassilyev, Jesujoba O Alabi, Yanke Mao, Haonan Gao, En-Shiun Annie Lee, Proceedings of the 18th Conference of the European Chapter. Long Papers. the 18th Conference of the European Chapterthe Association for Computational LinguisticsMarch 20241</p>
<p>No language left behind: Scaling human-centered machine translation. James Marta R Costa-Jussà, Onur Cross, Maha Çelebi, Kenneth Elbayad, Kevin Heafield, Elahe Heffernan, Janice Kalbassi, Daniel Lam, Jean Licht, Maillard, arXiv:2207.046722022arXiv preprint</p>
<p>Colin B Clement, Matthew Bierbaum, Kevin P O'keeffe, Alexander A Alemi, arXiv:1905.00075On the use of arxiv as a dataset. 2019arXiv preprint</p>
<p>Wikipedia as a data source for political scientists: Accuracy and completeness of coverage. Adam R Brown, PS: Political Science &amp; Politics. 4422011</p>
<p>Dbpedia-a large-scale, multilingual knowledge base extracted from wikipedia. Jens Lehmann, Robert Isele, Max Jakob, Anja Jentzsch, Dimitris Kontokostas, Pablo N Mendes, Sebastian Hellmann, Mohamed Morsey, Patrick Van Kleef, Sören Auer, Semantic web. 20156</p>
<p>Wikibench: Community-driven data curation for ai evaluation on wikipedia. Tzu-Sheng Kuo, Aaron Lee Halfaker, Zirui Cheng, Jiwoo Kim, Meng-Hsin Wu, Tongshuang Wu, Kenneth Holstein, Haiyi Zhu, Proceedings of the CHI Conference on Human Factors in Computing Systems. the CHI Conference on Human Factors in Computing Systems2024</p>
<p>Xuanyuan 2.0: A large chinese financial chat model with hundreds of billions parameters. Xuanyu Zhang, Qing Yang, Proceedings of the 32nd ACM international conference on information and knowledge management. the 32nd ACM international conference on information and knowledge management2023</p>
<p>Pmc-llama: toward building open-source language models for medicine. Chaoyi Wu, Weixiong Lin, Xiaoman Zhang, Ya Zhang, Weidi Xie, Yanfeng Wang, Journal of the American Medical Informatics Association. e0452024</p>
<p>Pierre Colombo, Pessoa Telmo, Malik Pires, Dominic Boudiaf, Rui Culver, Caio Melo, Andre Ft Corro, Fabrizio Martins, Vera Esposito, Sofia Lúcia Raposo, Morgado, arXiv:2403.03883Saullm-7b: A pioneering large language model for law. 2024arXiv preprint</p>
<p>Large language models for education: A survey. Hanyi Xu, Wensheng Gan, Zhenlian Qi, Jiayang Wu, Philip S Yu, arXiv:2405.130012024arXiv preprint</p>
<p>Fineweb-edu: the finest collection of educational content. Anton Lozhkov, Loubna Ben Allal, Leandro Von Werra, Thomas Wolf, 2024</p>
<p>Unsupervised cross-lingual representation learning at scale. Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzmán, Edouard Grave, Myle Ott, Luke Zettlemoyer, Veselin Stoyanov, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsJuly 2020</p>
<p>The fineweb datasets: Decanting the web for the finest text data at scale. Guilherme Penedo, Hynek Kydlíček, Anton Lozhkov, Margaret Mitchell, Colin Raffel, Leandro Von Werra, Thomas Wolf, arXiv:2406.175572024arXiv preprint</p>
<p>Yang Liu, Jiahuan Cao, Chongyu Liu, Kai Ding, Lianwen Jin, arXiv:2402.18041Datasets for large language models: A comprehensive survey. 2024arXiv preprint</p>
<p>SALAD-bench: A hierarchical and comprehensive safety benchmark for large language models. Lijun Li, Bowen Dong, Ruohui Wang, Xuhao Hu, Wangmeng Zuo, Dahua Lin, Yu Qiao, Jing Shao, Findings of the Association for Computational Linguistics: ACL 2024. August 2024</p>
<p>Which is more effective in label noise cleaning, correction or filtering?. Gaoxia Jiang, Jia Zhang, Xuefei Bai, Wenjian Wang, Deyu Meng, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202438</p>
<p>Can foundation models wrangle your. Avanika Narayan, Ines Chami, Laurel Orr, Simran Arora, Christopher Ré, arXiv:2205.099112022arXiv preprint</p>
<p>Zui Chen, Lei Cao, Sam Madden, Ju Fan, Nan Tang, Zihui Gu, Zeyuan Shang, Chunwei Liu, Michael Cafarella, Tim Kraska, arXiv:2310.00749Seed: Simple, efficient, and effective data management via large language models. 2023arXiv preprint</p>
<p>Iterclean: An iterative data cleaning framework with large language models. Wei Ni, Kaihang Zhang, Xiaoye Miao, Xiangyu Zhao, Yangyang Wu, Jianwei Yin, Proceedings of the ACM Turing Award Celebration Conference-China 2024. the ACM Turing Award Celebration Conference-China 20242024</p>
<p>Bloom library: Multimodal datasets in 300+ languages for a variety of downstream tasks. Colin Leong, Joshua Nemecek, Jacob Mansdorfer, Anna Filighera, Abraham Owodunni, Daniel Whitenack, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language ProcessingDecember 2022</p>
<p>Min-wise independent permutations. Andrei Z Broder, Moses Charikar, Alan M Frieze, Michael Mitzenmacher, Proceedings of the thirtieth annual ACM symposium on Theory of computing. the thirtieth annual ACM symposium on Theory of computing1998</p>
<p>KenLM: Faster and smaller language model queries. Kenneth Heafield, Proceedings of the Sixth Workshop on Statistical Machine Translation. the Sixth Workshop on Statistical Machine TranslationJuly 2011</p>
<p>Tony Fei, Kai Ming Liu, Zhi-Hua Ting, Zhou, Proceedings of the 2008 Eighth IEEE International Conference on Data Mining. the 2008 Eighth IEEE International Conference on Data Mining2008Isolation forest</p>
<p>The llama 3 herd of models. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, arXiv:2407.217832024arXiv preprint</p>
<p>. An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, arXiv:2412.1511520245 technical report. arXiv preprint</p>
<p>John Dang, Shivalika Singh, D' Daniel, Arash Souza, Alejandro Ahmadian, Madeline Salamanca, Aidan Smith, Sungjin Peppin, Manoj Hong, Terrence Govindassamy, Zhao, arXiv:2412.04261Aya expanse: Combining research breakthroughs for a new multilingual frontier. 2024arXiv preprint</p>
<p>One-class svms for document classification. M Larry, Malik Manevitz, Yousef, Journal of machine Learning research. 2Dec. 2001</p>
<p>Lof: identifying density-based local outliers. Markus M Breunig, Hans-Peter Kriegel, Raymond T Ng, Jörg Sander, Proceedings of the 2000 ACM SIGMOD international conference on Management of data. the 2000 ACM SIGMOD international conference on Management of data2000</p>
<p>A k-means clustering algorithm. Manchek A John A Hartigan, Wong, Applied statistics. 2811979</p>
<p>Language ID in the wild: Unexpected challenges on the path to a thousand-language web text corpus. Isaac Caswell, Theresa Breiner, Daan Van Esch, Ankur Bapna, Proceedings of the 28th International Conference on Computational Linguistics. the 28th International Conference on Computational LinguisticsDecember 2020</p>
<p>A comprehensive survey of anomaly detection algorithms. Durgesh Samariya, Amit Thakkar, Annals of Data Science. 1032023</p>
<p>J Edward, Yelong Hu, Phillip Shen, Zeyuan Wallis, Yuanzhi Allen-Zhu, Shean Li, Lu Wang, Weizhu Wang, Chen, arXiv:2106.09685Lora: Low-rank adaptation of large language models. 2021arXiv preprint</p>
<p>Anomaly detection with robust deep autoencoders. Chong Zhou, Randy C Paffenroth, Proceedings of the 23rd ACM SIGKDD international conference on knowledge discovery and data mining. the 23rd ACM SIGKDD international conference on knowledge discovery and data mining2017</p>
<p>Mean-shifted contrastive loss for anomaly detection. Tal Reiss, Yedid Hoshen, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202337</p>            </div>
        </div>

    </div>
</body>
</html>
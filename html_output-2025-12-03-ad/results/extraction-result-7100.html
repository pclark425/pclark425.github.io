<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7100 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7100</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7100</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-132.html">extraction-schema-132</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to synthesize novel chemical compounds for specific applications, including model details, generation methods, target applications, chemical representations, evaluation metrics, constraints, integration with external tools, datasets, reported performance, experimental validation, and reported challenges or limitations.</div>
                <p><strong>Paper ID:</strong> paper-fd17bd9a5dc24a081ad9743570f50dd6750f54b2</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/fd17bd9a5dc24a081ad9743570f50dd6750f54b2" target="_blank">Junction Tree Variational Autoencoder for Molecular Graph Generation</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Machine Learning</p>
                <p><strong>Paper TL;DR:</strong> The junction tree variational autoencoder generates molecular graphs in two phases, by first generating a tree-structured scaffold over chemical substructures, and then combining them into a molecule with a graph message passing network, which allows for incrementally expand molecules while maintaining chemical validity at every step.</p>
                <p><strong>Paper Abstract:</strong> We seek to automate the design of molecules based on specific chemical properties. In computational terms, this task involves continuous embedding and generation of molecular graphs. Our primary contribution is the direct realization of molecular graphs, a task previously approached by generating linear SMILES strings instead of graphs. Our junction tree variational autoencoder generates molecular graphs in two phases, by first generating a tree-structured scaffold over chemical substructures, and then combining them into a molecule with a graph message passing network. This approach allows us to incrementally expand molecules while maintaining chemical validity at every step. We evaluate our model on multiple tasks ranging from molecular generation to optimization. Across these tasks, our model outperforms previous state-of-the-art baselines by a significant margin.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7100.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7100.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to synthesize novel chemical compounds for specific applications, including model details, generation methods, target applications, chemical representations, evaluation metrics, constraints, integration with external tools, datasets, reported performance, experimental validation, and reported challenges or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>JT-VAE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Junction Tree Variational Autoencoder</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A two-stage variational autoencoder that generates molecular graphs by first producing a junction tree of chemically-valid substructures (rings, bonds, small clusters) and then assembling those clusters into a full molecular graph with graph message-passing; designed to maintain chemical validity during generation and enable property optimization in latent space.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Junction Tree VAE (JT-VAE)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Variational Autoencoder with specialized encoder/decoder: tree-structured encoder/decoder for junction tree + graph message-passing encoder/decoder for fine-grained connectivity</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>latent bottleneck dim = 56 (tree 28 + graph 28); hidden state dim = 450 for modules; total parameter count not reported</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Trained on ZINC dataset (~250K drug-like molecules). A cluster vocabulary of size |X| = 780 was extracted from training molecules (rings and non-ring bonds) via RDKit-based tree decomposition.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Coarse-to-fine generative procedure: (1) sample/decoding of junction tree (top-down depth-first tree decoder with chemical-compatibility masking); (2) structured assembly/selection of candidate subgraph merges per tree node scored by a learned scorer using graph message-passing and latent graph vector z_G; sampling from prior N(0,I) or decoding from latent vectors for optimization tasks. Bayesian optimization over latent space and gradient ascent for constrained optimization are demonstrated.</td>
                        </tr>
                        <tr>
                            <td><strong>chemical_representation</strong></td>
                            <td>Explicit molecular graphs plus a junction-tree of clusters (clusters are rings, bonds, or small atom clusters). Cluster labels form vocabulary; stereochemistry handled post hoc by enumerating stereoisomers via RDKit and ranking.</td>
                        </tr>
                        <tr>
                            <td><strong>target_application</strong></td>
                            <td>Drug-like molecule generation and optimization (e.g., maximize penalized logP: logP - SA - cycle penalty), constrained molecule optimization preserving similarity, and general molecular design.</td>
                        </tr>
                        <tr>
                            <td><strong>constraints_used</strong></td>
                            <td>During decoding: chemical-compatibility masks for cluster label selection; stereochemistry resolved by RDKit stereoisomer enumeration and ranking. In optimization tasks: synthetic accessibility (SA) and cycle penalties are included in scoring; constrained optimization applies Tanimoto similarity thresholds (δ = 0.0, 0.2, 0.4, 0.6).</td>
                        </tr>
                        <tr>
                            <td><strong>integration_with_external_tools</strong></td>
                            <td>RDKit used for: ring extraction (GetSymmSSSR), stereoisomer enumeration, validity checks, fingerprint calculation (Morgan fingerprints) and other cheminformatics operations; a sparse Gaussian process (SGP) used off-line for Bayesian optimization in latent space.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_used</strong></td>
                            <td>ZINC (~250K molecule subset used in Kusner et al. 2017 split).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Reconstruction accuracy (% identical reconstructions over Monte Carlo encoding/decoding), prior-sampled validity (% chemically valid by RDKit), novelty (distinctness from training set), Bayesian optimization property scores (top-k penalized logP), predictive performance of downstream SGP (log-likelihood and RMSE), constrained-optimization metrics: mean property improvement, molecular similarity (Tanimoto), and success rate.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Reconstruction accuracy 76.7%; prior validity 100% (1000 prior samples decoded 100 times each); all 5000 sampled molecules distinct from training set (reported). Bayesian optimization: top 3 property scores 5.30, 4.93, 4.49 (penalized logP). SGP predictive performance on JT-VAE embeddings: LL = -1.658 ± 0.023, RMSE = 1.290 ± 0.026. Constrained optimization (over 800 difficult test molecules): δ=0.0 improvement 1.91 ± 2.04 (similarity 0.28 ± 0.15, success 97.5%); δ=0.2 improvement 1.68 ± 1.85 (sim 0.33 ± 0.13, success 97.1%); δ=0.4 improvement 0.84 ± 1.45 (sim 0.51 ± 0.10, success 83.6%); δ=0.6 improvement 0.21 ± 0.71 (sim 0.69 ± 0.06, success 46.4%).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Junction tree depends on a precomputed vocabulary of clusters derived from training set, so arbitrary molecules outside vocabulary may be poorly represented; junction tree decomposition is not unique (introduces uncertainty); assembly step is ambiguous because multiple graphs can map to same tree (requires enumerating and scoring candidates); stereochemistry is not generated directly and is handled with post hoc enumeration and ranking; some optimization failures are due to inaccurate property predictor; generalization to arbitrary low-treewidth graphs is left for future work.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Junction Tree Variational Autoencoder for Molecular Graph Generation', 'publication_date_yy_mm': '2018-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7100.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7100.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to synthesize novel chemical compounds for specific applications, including model details, generation methods, target applications, chemical representations, evaluation metrics, constraints, integration with external tools, datasets, reported performance, experimental validation, and reported challenges or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CVAE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Character Variational Autoencoder</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A SMILES-based VAE that encodes and decodes molecules as character sequences (SMILES) using recurrent networks; used as a baseline for molecular generation tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Automatic chemical design using a data-driven continuous representation of molecules</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Character VAE (CVAE)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Variational Autoencoder with recurrent (character-level SMILES) encoder/decoder</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Applied to the same ZINC dataset split as JT-VAE for comparison in this study (original CVAE trained on molecular SMILES; the paper references Gómez-Bombarelli et al. 2016).</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Direct SMILES generation character-by-character by sampling from VAE decoder.</td>
                        </tr>
                        <tr>
                            <td><strong>chemical_representation</strong></td>
                            <td>SMILES strings (character-level).</td>
                        </tr>
                        <tr>
                            <td><strong>target_application</strong></td>
                            <td>Drug-like molecular generation and latent-space design for property optimization.</td>
                        </tr>
                        <tr>
                            <td><strong>constraints_used</strong></td>
                            <td>No explicit chemical-graph-level constraints; decoder may produce invalid SMILES (checked post hoc with RDKit for validity).</td>
                        </tr>
                        <tr>
                            <td><strong>integration_with_external_tools</strong></td>
                            <td>RDKit used to check validity of decoded SMILES in evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_used</strong></td>
                            <td>ZINC (same split used in paper for experiments/comparison).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Reconstruction accuracy; prior-sampled validity (RDKit); downstream SGP predictive metrics (LL, RMSE); BO top property scores.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Reconstruction 44.6%; prior validity 0.7% (Table 1). BO top 3 best scores (1st=1.98, 2nd=1.42, 3rd=1.19) in Table 2. SGP predictive LL = -1.812 ± 0.004, RMSE = 1.504 ± 0.006 (Table 3).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>SMILES representation is not designed to capture molecular similarity (string encodings of similar molecules can be very different); high rate of invalid outputs when sampling from prior (very low validity); lack of chemical-feasibility constraints in generation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Junction Tree Variational Autoencoder for Molecular Graph Generation', 'publication_date_yy_mm': '2018-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7100.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7100.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to synthesize novel chemical compounds for specific applications, including model details, generation methods, target applications, chemical representations, evaluation metrics, constraints, integration with external tools, datasets, reported performance, experimental validation, and reported challenges or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GVAE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Grammar Variational Autoencoder</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A SMILES-based VAE that uses a context-free grammar to constrain SMILES generation, improving syntactic correctness compared to character VAEs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Grammar variational autoencoder</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Grammar VAE (GVAE)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Variational Autoencoder using context-free grammar-constrained decoder for SMILES generation</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Evaluated on ZINC dataset in this paper for direct comparison; original method trained on SMILES corpora.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Generates SMILES according to a learned latent code but constrained by a context-free grammar to reduce syntactic SMILES errors.</td>
                        </tr>
                        <tr>
                            <td><strong>chemical_representation</strong></td>
                            <td>SMILES with CFG-constrained production rules.</td>
                        </tr>
                        <tr>
                            <td><strong>target_application</strong></td>
                            <td>Molecule generation and property optimization for drug-like molecules.</td>
                        </tr>
                        <tr>
                            <td><strong>constraints_used</strong></td>
                            <td>Context-free grammar ensures syntactic correctness of SMILES; does not fully enforce chemical validity beyond syntax.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_with_external_tools</strong></td>
                            <td>RDKit used for post-generation validity checking and parsing for evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_used</strong></td>
                            <td>ZINC (used for comparisons in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Reconstruction accuracy; prior validity; BO top property scores; SGP predictive LL and RMSE.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Reconstruction 53.7%; prior validity 7.2% (Table 1). BO top scores (1st=2.94, 2nd=2.89, 3rd=2.80) in Table 2. SGP LL = -1.739 ± 0.004, RMSE = 1.404 ± 0.006 (Table 3).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Context-free grammar enforces syntax but cannot fully capture chemical validity or semantics (e.g., valence constraints), leading to limited prior validity compared to graph-aware methods.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Junction Tree Variational Autoencoder for Molecular Graph Generation', 'publication_date_yy_mm': '2018-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7100.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7100.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to synthesize novel chemical compounds for specific applications, including model details, generation methods, target applications, chemical representations, evaluation metrics, constraints, integration with external tools, datasets, reported performance, experimental validation, and reported challenges or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SD-VAE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Syntax-Directed Variational Autoencoder</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A SMILES-based VAE that augments grammar constraints with additional semantic constraints (attribute grammars) to better capture chemical validity during SMILES generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Syntax-directed variational autoencoder for structured data</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Syntax-Directed VAE (SD-VAE)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Variational Autoencoder using attribute-grammar (syntax + semantic) constrained decoder for SMILES</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Evaluated on ZINC dataset for comparison; original SD-VAE trained to incorporate syntactic and semantic constraints for structured outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>SMILES generation constrained with attribute grammars to enforce additional semantic constraints beyond syntax (e.g., certain semantic checks relevant to SMILES).</td>
                        </tr>
                        <tr>
                            <td><strong>chemical_representation</strong></td>
                            <td>SMILES with attribute-grammar constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>target_application</strong></td>
                            <td>Molecule generation and optimization with improved validity over CFG-based methods.</td>
                        </tr>
                        <tr>
                            <td><strong>constraints_used</strong></td>
                            <td>Attribute grammar provides syntactic and semantic checks during generation to reduce invalid SMILES.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_with_external_tools</strong></td>
                            <td>RDKit used for validation and downstream evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_used</strong></td>
                            <td>ZINC (comparison dataset in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Reconstruction accuracy; prior validity; BO top property scores; SGP predictive LL and RMSE.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Reconstruction 76.2%; prior validity 43.5% (Table 1). BO top scores (1st=4.04, 2nd=3.50, 3rd=2.96) in Table 2. SGP LL = -1.697 ± 0.015, RMSE = 1.366 ± 0.023 (Table 3).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Attribute grammars improve validity but still do not capture full chemical feasibility (e.g., global valence and stereochemistry issues) and remain less effective than explicit graph-based approaches for enforcing chemical validity.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Junction Tree Variational Autoencoder for Molecular Graph Generation', 'publication_date_yy_mm': '2018-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7100.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e7100.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to synthesize novel chemical compounds for specific applications, including model details, generation methods, target applications, chemical representations, evaluation metrics, constraints, integration with external tools, datasets, reported performance, experimental validation, and reported challenges or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GraphVAE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GraphVAE</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A VAE that directly generates graph adjacency matrices and node labels for small graphs, applied as a baseline for molecule generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>GraphVAE: Towards generation of small graphs using variational autoencoders</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GraphVAE</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Variational Autoencoder that decodes adjacency matrices and node feature matrices (graph-level VAE)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Applied as baseline on ZINC for evaluation in this paper; original method targeted small graph generation.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Direct generation of atom labels and adjacency matrix representing bonds; decoding outputs are then interpreted as molecular graphs and validated.</td>
                        </tr>
                        <tr>
                            <td><strong>chemical_representation</strong></td>
                            <td>Explicit graph adjacency matrices and atom labels.</td>
                        </tr>
                        <tr>
                            <td><strong>target_application</strong></td>
                            <td>Small-molecule generation (benchmarking molecular graph generation).</td>
                        </tr>
                        <tr>
                            <td><strong>constraints_used</strong></td>
                            <td>No explicit chemical-clique/junction-tree constraints; relies on post hoc validation to filter chemically invalid graphs.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_with_external_tools</strong></td>
                            <td>RDKit used for validity checking in evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_used</strong></td>
                            <td>ZINC (used for comparison here).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Prior validity (%) reported.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Prior validity reported as 13.5% (Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Direct adjacency-matrix generation struggles to scale and produces low chemical validity; handling of chemical constraints (valence, rings) is nontrivial without specialized structure-aware decoding.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Junction Tree Variational Autoencoder for Molecular Graph Generation', 'publication_date_yy_mm': '2018-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7100.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e7100.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to synthesize novel chemical compounds for specific applications, including model details, generation methods, target applications, chemical representations, evaluation metrics, constraints, integration with external tools, datasets, reported performance, experimental validation, and reported challenges or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Atom-by-Atom LSTM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Atom-by-Atom LSTM autoregressive graph generator</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An autoregressive LSTM-based model that generates molecular graphs node-by-node (atom-by-atom) and edges sequentially, used as a baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Learning deep generative models of graphs</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Atom-by-Atom LSTM (autoregressive graph generator)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Autoregressive sequence model (LSTM) that generates graphs atom-by-atom and bond-by-bond</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Compared on ZINC dataset in this paper (model introduced in Li et al. 2018).</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Autoregressive generation of graph construction steps (nodes and edges) in sequence; operates on graph construction actions.</td>
                        </tr>
                        <tr>
                            <td><strong>chemical_representation</strong></td>
                            <td>Graph construction sequence (atoms and bonds) rather than SMILES.</td>
                        </tr>
                        <tr>
                            <td><strong>target_application</strong></td>
                            <td>Molecule generation for drug-like compounds.</td>
                        </tr>
                        <tr>
                            <td><strong>constraints_used</strong></td>
                            <td>No inherent guarantee of chemical validity at intermediate steps; validity enforced/checked only after full graph generation.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_with_external_tools</strong></td>
                            <td>RDKit used for final validity checking during evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_used</strong></td>
                            <td>ZINC (comparison dataset in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Prior validity (%) reported.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Prior validity 89.2% (Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Node-by-node generation produces chemically invalid intermediate states which complicate learning and can reduce final validity; less efficient than structure-by-structure/junction-tree approach for maintaining validity and for scaling.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Junction Tree Variational Autoencoder for Molecular Graph Generation', 'publication_date_yy_mm': '2018-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Automatic chemical design using a data-driven continuous representation of molecules <em>(Rating: 2)</em></li>
                <li>Grammar variational autoencoder <em>(Rating: 2)</em></li>
                <li>Syntax-directed variational autoencoder for structured data <em>(Rating: 2)</em></li>
                <li>GraphVAE: Towards generation of small graphs using variational autoencoders <em>(Rating: 2)</em></li>
                <li>Learning deep generative models of graphs <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7100",
    "paper_id": "paper-fd17bd9a5dc24a081ad9743570f50dd6750f54b2",
    "extraction_schema_id": "extraction-schema-132",
    "extracted_data": [
        {
            "name_short": "JT-VAE",
            "name_full": "Junction Tree Variational Autoencoder",
            "brief_description": "A two-stage variational autoencoder that generates molecular graphs by first producing a junction tree of chemically-valid substructures (rings, bonds, small clusters) and then assembling those clusters into a full molecular graph with graph message-passing; designed to maintain chemical validity during generation and enable property optimization in latent space.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Junction Tree VAE (JT-VAE)",
            "model_type": "Variational Autoencoder with specialized encoder/decoder: tree-structured encoder/decoder for junction tree + graph message-passing encoder/decoder for fine-grained connectivity",
            "model_size": "latent bottleneck dim = 56 (tree 28 + graph 28); hidden state dim = 450 for modules; total parameter count not reported",
            "training_data_description": "Trained on ZINC dataset (~250K drug-like molecules). A cluster vocabulary of size |X| = 780 was extracted from training molecules (rings and non-ring bonds) via RDKit-based tree decomposition.",
            "generation_method": "Coarse-to-fine generative procedure: (1) sample/decoding of junction tree (top-down depth-first tree decoder with chemical-compatibility masking); (2) structured assembly/selection of candidate subgraph merges per tree node scored by a learned scorer using graph message-passing and latent graph vector z_G; sampling from prior N(0,I) or decoding from latent vectors for optimization tasks. Bayesian optimization over latent space and gradient ascent for constrained optimization are demonstrated.",
            "chemical_representation": "Explicit molecular graphs plus a junction-tree of clusters (clusters are rings, bonds, or small atom clusters). Cluster labels form vocabulary; stereochemistry handled post hoc by enumerating stereoisomers via RDKit and ranking.",
            "target_application": "Drug-like molecule generation and optimization (e.g., maximize penalized logP: logP - SA - cycle penalty), constrained molecule optimization preserving similarity, and general molecular design.",
            "constraints_used": "During decoding: chemical-compatibility masks for cluster label selection; stereochemistry resolved by RDKit stereoisomer enumeration and ranking. In optimization tasks: synthetic accessibility (SA) and cycle penalties are included in scoring; constrained optimization applies Tanimoto similarity thresholds (δ = 0.0, 0.2, 0.4, 0.6).",
            "integration_with_external_tools": "RDKit used for: ring extraction (GetSymmSSSR), stereoisomer enumeration, validity checks, fingerprint calculation (Morgan fingerprints) and other cheminformatics operations; a sparse Gaussian process (SGP) used off-line for Bayesian optimization in latent space.",
            "dataset_used": "ZINC (~250K molecule subset used in Kusner et al. 2017 split).",
            "evaluation_metrics": "Reconstruction accuracy (% identical reconstructions over Monte Carlo encoding/decoding), prior-sampled validity (% chemically valid by RDKit), novelty (distinctness from training set), Bayesian optimization property scores (top-k penalized logP), predictive performance of downstream SGP (log-likelihood and RMSE), constrained-optimization metrics: mean property improvement, molecular similarity (Tanimoto), and success rate.",
            "reported_results": "Reconstruction accuracy 76.7%; prior validity 100% (1000 prior samples decoded 100 times each); all 5000 sampled molecules distinct from training set (reported). Bayesian optimization: top 3 property scores 5.30, 4.93, 4.49 (penalized logP). SGP predictive performance on JT-VAE embeddings: LL = -1.658 ± 0.023, RMSE = 1.290 ± 0.026. Constrained optimization (over 800 difficult test molecules): δ=0.0 improvement 1.91 ± 2.04 (similarity 0.28 ± 0.15, success 97.5%); δ=0.2 improvement 1.68 ± 1.85 (sim 0.33 ± 0.13, success 97.1%); δ=0.4 improvement 0.84 ± 1.45 (sim 0.51 ± 0.10, success 83.6%); δ=0.6 improvement 0.21 ± 0.71 (sim 0.69 ± 0.06, success 46.4%).",
            "experimental_validation": false,
            "challenges_or_limitations": "Junction tree depends on a precomputed vocabulary of clusters derived from training set, so arbitrary molecules outside vocabulary may be poorly represented; junction tree decomposition is not unique (introduces uncertainty); assembly step is ambiguous because multiple graphs can map to same tree (requires enumerating and scoring candidates); stereochemistry is not generated directly and is handled with post hoc enumeration and ranking; some optimization failures are due to inaccurate property predictor; generalization to arbitrary low-treewidth graphs is left for future work.",
            "uuid": "e7100.0",
            "source_info": {
                "paper_title": "Junction Tree Variational Autoencoder for Molecular Graph Generation",
                "publication_date_yy_mm": "2018-02"
            }
        },
        {
            "name_short": "CVAE",
            "name_full": "Character Variational Autoencoder",
            "brief_description": "A SMILES-based VAE that encodes and decodes molecules as character sequences (SMILES) using recurrent networks; used as a baseline for molecular generation tasks.",
            "citation_title": "Automatic chemical design using a data-driven continuous representation of molecules",
            "mention_or_use": "use",
            "model_name": "Character VAE (CVAE)",
            "model_type": "Variational Autoencoder with recurrent (character-level SMILES) encoder/decoder",
            "model_size": null,
            "training_data_description": "Applied to the same ZINC dataset split as JT-VAE for comparison in this study (original CVAE trained on molecular SMILES; the paper references Gómez-Bombarelli et al. 2016).",
            "generation_method": "Direct SMILES generation character-by-character by sampling from VAE decoder.",
            "chemical_representation": "SMILES strings (character-level).",
            "target_application": "Drug-like molecular generation and latent-space design for property optimization.",
            "constraints_used": "No explicit chemical-graph-level constraints; decoder may produce invalid SMILES (checked post hoc with RDKit for validity).",
            "integration_with_external_tools": "RDKit used to check validity of decoded SMILES in evaluation.",
            "dataset_used": "ZINC (same split used in paper for experiments/comparison).",
            "evaluation_metrics": "Reconstruction accuracy; prior-sampled validity (RDKit); downstream SGP predictive metrics (LL, RMSE); BO top property scores.",
            "reported_results": "Reconstruction 44.6%; prior validity 0.7% (Table 1). BO top 3 best scores (1st=1.98, 2nd=1.42, 3rd=1.19) in Table 2. SGP predictive LL = -1.812 ± 0.004, RMSE = 1.504 ± 0.006 (Table 3).",
            "experimental_validation": false,
            "challenges_or_limitations": "SMILES representation is not designed to capture molecular similarity (string encodings of similar molecules can be very different); high rate of invalid outputs when sampling from prior (very low validity); lack of chemical-feasibility constraints in generation.",
            "uuid": "e7100.1",
            "source_info": {
                "paper_title": "Junction Tree Variational Autoencoder for Molecular Graph Generation",
                "publication_date_yy_mm": "2018-02"
            }
        },
        {
            "name_short": "GVAE",
            "name_full": "Grammar Variational Autoencoder",
            "brief_description": "A SMILES-based VAE that uses a context-free grammar to constrain SMILES generation, improving syntactic correctness compared to character VAEs.",
            "citation_title": "Grammar variational autoencoder",
            "mention_or_use": "use",
            "model_name": "Grammar VAE (GVAE)",
            "model_type": "Variational Autoencoder using context-free grammar-constrained decoder for SMILES generation",
            "model_size": null,
            "training_data_description": "Evaluated on ZINC dataset in this paper for direct comparison; original method trained on SMILES corpora.",
            "generation_method": "Generates SMILES according to a learned latent code but constrained by a context-free grammar to reduce syntactic SMILES errors.",
            "chemical_representation": "SMILES with CFG-constrained production rules.",
            "target_application": "Molecule generation and property optimization for drug-like molecules.",
            "constraints_used": "Context-free grammar ensures syntactic correctness of SMILES; does not fully enforce chemical validity beyond syntax.",
            "integration_with_external_tools": "RDKit used for post-generation validity checking and parsing for evaluation.",
            "dataset_used": "ZINC (used for comparisons in this paper).",
            "evaluation_metrics": "Reconstruction accuracy; prior validity; BO top property scores; SGP predictive LL and RMSE.",
            "reported_results": "Reconstruction 53.7%; prior validity 7.2% (Table 1). BO top scores (1st=2.94, 2nd=2.89, 3rd=2.80) in Table 2. SGP LL = -1.739 ± 0.004, RMSE = 1.404 ± 0.006 (Table 3).",
            "experimental_validation": false,
            "challenges_or_limitations": "Context-free grammar enforces syntax but cannot fully capture chemical validity or semantics (e.g., valence constraints), leading to limited prior validity compared to graph-aware methods.",
            "uuid": "e7100.2",
            "source_info": {
                "paper_title": "Junction Tree Variational Autoencoder for Molecular Graph Generation",
                "publication_date_yy_mm": "2018-02"
            }
        },
        {
            "name_short": "SD-VAE",
            "name_full": "Syntax-Directed Variational Autoencoder",
            "brief_description": "A SMILES-based VAE that augments grammar constraints with additional semantic constraints (attribute grammars) to better capture chemical validity during SMILES generation.",
            "citation_title": "Syntax-directed variational autoencoder for structured data",
            "mention_or_use": "use",
            "model_name": "Syntax-Directed VAE (SD-VAE)",
            "model_type": "Variational Autoencoder using attribute-grammar (syntax + semantic) constrained decoder for SMILES",
            "model_size": null,
            "training_data_description": "Evaluated on ZINC dataset for comparison; original SD-VAE trained to incorporate syntactic and semantic constraints for structured outputs.",
            "generation_method": "SMILES generation constrained with attribute grammars to enforce additional semantic constraints beyond syntax (e.g., certain semantic checks relevant to SMILES).",
            "chemical_representation": "SMILES with attribute-grammar constraints.",
            "target_application": "Molecule generation and optimization with improved validity over CFG-based methods.",
            "constraints_used": "Attribute grammar provides syntactic and semantic checks during generation to reduce invalid SMILES.",
            "integration_with_external_tools": "RDKit used for validation and downstream evaluation.",
            "dataset_used": "ZINC (comparison dataset in this paper).",
            "evaluation_metrics": "Reconstruction accuracy; prior validity; BO top property scores; SGP predictive LL and RMSE.",
            "reported_results": "Reconstruction 76.2%; prior validity 43.5% (Table 1). BO top scores (1st=4.04, 2nd=3.50, 3rd=2.96) in Table 2. SGP LL = -1.697 ± 0.015, RMSE = 1.366 ± 0.023 (Table 3).",
            "experimental_validation": false,
            "challenges_or_limitations": "Attribute grammars improve validity but still do not capture full chemical feasibility (e.g., global valence and stereochemistry issues) and remain less effective than explicit graph-based approaches for enforcing chemical validity.",
            "uuid": "e7100.3",
            "source_info": {
                "paper_title": "Junction Tree Variational Autoencoder for Molecular Graph Generation",
                "publication_date_yy_mm": "2018-02"
            }
        },
        {
            "name_short": "GraphVAE",
            "name_full": "GraphVAE",
            "brief_description": "A VAE that directly generates graph adjacency matrices and node labels for small graphs, applied as a baseline for molecule generation.",
            "citation_title": "GraphVAE: Towards generation of small graphs using variational autoencoders",
            "mention_or_use": "use",
            "model_name": "GraphVAE",
            "model_type": "Variational Autoencoder that decodes adjacency matrices and node feature matrices (graph-level VAE)",
            "model_size": null,
            "training_data_description": "Applied as baseline on ZINC for evaluation in this paper; original method targeted small graph generation.",
            "generation_method": "Direct generation of atom labels and adjacency matrix representing bonds; decoding outputs are then interpreted as molecular graphs and validated.",
            "chemical_representation": "Explicit graph adjacency matrices and atom labels.",
            "target_application": "Small-molecule generation (benchmarking molecular graph generation).",
            "constraints_used": "No explicit chemical-clique/junction-tree constraints; relies on post hoc validation to filter chemically invalid graphs.",
            "integration_with_external_tools": "RDKit used for validity checking in evaluation.",
            "dataset_used": "ZINC (used for comparison here).",
            "evaluation_metrics": "Prior validity (%) reported.",
            "reported_results": "Prior validity reported as 13.5% (Table 1).",
            "experimental_validation": false,
            "challenges_or_limitations": "Direct adjacency-matrix generation struggles to scale and produces low chemical validity; handling of chemical constraints (valence, rings) is nontrivial without specialized structure-aware decoding.",
            "uuid": "e7100.4",
            "source_info": {
                "paper_title": "Junction Tree Variational Autoencoder for Molecular Graph Generation",
                "publication_date_yy_mm": "2018-02"
            }
        },
        {
            "name_short": "Atom-by-Atom LSTM",
            "name_full": "Atom-by-Atom LSTM autoregressive graph generator",
            "brief_description": "An autoregressive LSTM-based model that generates molecular graphs node-by-node (atom-by-atom) and edges sequentially, used as a baseline.",
            "citation_title": "Learning deep generative models of graphs",
            "mention_or_use": "use",
            "model_name": "Atom-by-Atom LSTM (autoregressive graph generator)",
            "model_type": "Autoregressive sequence model (LSTM) that generates graphs atom-by-atom and bond-by-bond",
            "model_size": null,
            "training_data_description": "Compared on ZINC dataset in this paper (model introduced in Li et al. 2018).",
            "generation_method": "Autoregressive generation of graph construction steps (nodes and edges) in sequence; operates on graph construction actions.",
            "chemical_representation": "Graph construction sequence (atoms and bonds) rather than SMILES.",
            "target_application": "Molecule generation for drug-like compounds.",
            "constraints_used": "No inherent guarantee of chemical validity at intermediate steps; validity enforced/checked only after full graph generation.",
            "integration_with_external_tools": "RDKit used for final validity checking during evaluation.",
            "dataset_used": "ZINC (comparison dataset in this paper).",
            "evaluation_metrics": "Prior validity (%) reported.",
            "reported_results": "Prior validity 89.2% (Table 1).",
            "experimental_validation": false,
            "challenges_or_limitations": "Node-by-node generation produces chemically invalid intermediate states which complicate learning and can reduce final validity; less efficient than structure-by-structure/junction-tree approach for maintaining validity and for scaling.",
            "uuid": "e7100.5",
            "source_info": {
                "paper_title": "Junction Tree Variational Autoencoder for Molecular Graph Generation",
                "publication_date_yy_mm": "2018-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Automatic chemical design using a data-driven continuous representation of molecules",
            "rating": 2
        },
        {
            "paper_title": "Grammar variational autoencoder",
            "rating": 2
        },
        {
            "paper_title": "Syntax-directed variational autoencoder for structured data",
            "rating": 2
        },
        {
            "paper_title": "GraphVAE: Towards generation of small graphs using variational autoencoders",
            "rating": 2
        },
        {
            "paper_title": "Learning deep generative models of graphs",
            "rating": 2
        }
    ],
    "cost": 0.01772975,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Junction Tree Variational Autoencoder for Molecular Graph Generation</h1>
<p>Wengong Jin ${ }^{1}$ Regina Barzilay ${ }^{1}$ Tommi Jaakkola ${ }^{1}$</p>
<h4>Abstract</h4>
<p>We seek to automate the design of molecules based on specific chemical properties. In computational terms, this task involves continuous embedding and generation of molecular graphs. Our primary contribution is the direct realization of molecular graphs, a task previously approached by generating linear SMILES strings instead of graphs. Our junction tree variational autoencoder generates molecular graphs in two phases, by first generating a tree-structured scaffold over chemical substructures, and then combining them into a molecule with a graph message passing network. This approach allows us to incrementally expand molecules while maintaining chemical validity at every step. We evaluate our model on multiple tasks ranging from molecular generation to optimization. Across these tasks, our model outperforms previous state-of-the-art baselines by a significant margin.</p>
<h2>1. Introduction</h2>
<p>The key challenge of drug discovery is to find target molecules with desired chemical properties. Currently, this task takes years of development and exploration by expert chemists and pharmacologists. Our ultimate goal is to automate this process. From a computational perspective, we decompose the challenge into two complementary subtasks: learning to represent molecules in a continuous manner that facilitates the prediction and optimization of their properties (encoding); and learning to map an optimized continuous representation back into a molecular graph with improved properties (decoding). While deep learning has been extensively investigated for molecular graph encoding (Duvenaud et al., 2015; Kearnes et al., 2016; Gilmer et al., 2017), the harder combinatorial task of molecular graph generation from latent representation remains under-explored.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1. Two almost identical molecules with markedly different canonical SMILES in RDKit. The edit distance between two strings is 22 ( $50.5 \%$ of the whole sequence).</p>
<p>Prior work on drug design formulated the graph generation task as a string generation problem (Gómez-Bombarelli et al., 2016; Kusner et al., 2017) in an attempt to side-step direct generation of graphs. Specifically, these models start by generating SMILES (Weininger, 1988), a linear string notation used in chemistry to describe molecular structures. SMILES strings can be translated into graphs via deterministic mappings (e.g., using RDKit (Landrum, 2006)). However, this design has two critical limitations. First, the SMILES representation is not designed to capture molecular similarity. For instance, two molecules with similar chemical structures may be encoded into markedly different SMILES strings (e.g., Figure 1). This prevents generative models like variational autoencoders from learning smooth molecular embeddings. Second, essential chemical properties such as molecule validity are easier to express on graphs rather than linear SMILES representations. We hypothesize that operating directly on graphs improves generative modeling of valid chemical structures.</p>
<p>Our primary contribution is a new generative model of molecular graphs. While one could imagine solving the problem in a standard manner - generating graphs node by node (Li et al., 2018) - the approach is not ideal for molecules. This is because creating molecules atom by atom would force the model to generate chemically invalid intermediaries (see, e.g., Figure 2), delaying validation until a complete graph is generated. Instead, we propose to generate molecular graphs in two phases by exploiting valid subgraphs as components. The overall generative approach, cast as a junction tree variational autoencoder ${ }^{1}$, first generates a tree structured object (a junction tree) whose role is to represent the scaffold of subgraph components and their coarse relative arrangements. The components are</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2. Comparison of two graph generation schemes: Structure by structure approach is preferred as it avoids invalid intermediate states (marked in red) encountered in node by node approach.
valid chemical substructures automatically extracted from the training set using tree decomposition and are used as building blocks. In the second phase, the subgraphs (nodes in the tree) are assembled together into a molecular graph.</p>
<p>We evaluate our model on multiple tasks ranging from molecular generation to optimization of a given molecule according to desired properties. As baselines, we utilize state-of-the-art SMILES-based generation approaches (Kusner et al., 2017; Dai et al., 2018). We demonstrate that our model produces $100 \%$ valid molecules when sampled from a prior distribution, outperforming the top performing baseline by a significant margin. In addition, we show that our model excels in discovering molecules with desired properties, yielding a $30 \%$ relative gain over the baselines.</p>
<h2>2. Junction Tree Variational Autoencoder</h2>
<p>Our approach extends the variational autoencoder (Kingma \&amp; Welling, 2013) to molecular graphs by introducing a suitable encoder and a matching decoder. Deviating from previous work (Gómez-Bombarelli et al., 2016; Kusner et al., 2017), we interpret each molecule as having been built from subgraphs chosen out of a vocabulary of valid components. These components are used as building blocks both when encoding a molecule into a vector representation as well as when decoding latent vectors back into valid molecular graphs. The key advantage of this view is that the decoder can realize a valid molecule piece by piece by utilizing the collection of valid components and how they interact, rather than trying to build the molecule atom by atom through chemically invalid intermediaries (Figure 2). An aromatic bond, for example, is chemically invalid on its own unless the entire aromatic ring is present. It would be therefore challenging to learn to build rings atom by atom rather than by introducing rings as part of the basic vocabulary.
Our vocabulary of components, such as rings, bonds and individual atoms, is chosen to be large enough so that a given molecule can be covered by overlapping components or clusters of atoms. The clusters serve the role analogous to cliques in graphical models, as they are expressive enough that a molecule can be covered by overlapping clusters without forming cluster cycles. In this sense, the clusters serve as cliques in a (non-optimal) triangulation of the molecular graph. We form a junction tree of such clusters and use it as the tree representation of the molecule. Since our choice
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3. Overview of our method: A molecular graph $G$ is first decomposed into its junction tree $\mathcal{T}<em _mathcal_T="\mathcal{T">{G}$, where each colored node in the tree represents a substructure in the molecule. We then encode both the tree and graph into their latent embeddings $\mathbf{z}</em>}}$ and $\mathbf{z<em _mathcal_T="\mathcal{T">{G}$. To decode the molecule, we first reconstruct junction tree from $\mathbf{z}</em>$, and then assemble nodes in the tree back to the original molecule.
of cliques is constrained a priori, we cannot guarantee that a junction tree exists with such clusters for an arbitrary molecule. However, our clusters are built on the basis of the molecules in the training set to ensure that a corresponding junction tree can be found. Empirically, our clusters cover most of the molecules in the test set.}</p>
<p>The original molecular graph and its associated junction tree offer two complementary representations of a molecule. We therefore encode the molecule into a two-part latent representation $\mathbf{z}=\left[\mathbf{z}<em G="G">{\mathcal{T}}, \mathbf{z}</em>}\right]$ where $\mathbf{z<em G="G">{\mathcal{T}}$ encodes the tree structure and what the clusters are in the tree without fully capturing how exactly the clusters are mutually connected. $\mathbf{z}</em>}$ encodes the graph to capture the fine-grained connectivity. Both parts are created by tree and graph encoders $q\left(\mathbf{z<em G="G">{\mathcal{T}} \mid \mathcal{T}\right)$ and $q\left(\mathbf{z}</em>} \mid G\right)$. The latent representation is then decoded back into a molecular graph in two stages. As illustrated in Figure 3, we first reproduce the junction tree using a tree decoder $p\left(\mathcal{T} \mid \mathbf{z<em _mathcal_T="\mathcal{T">{\mathcal{T}}\right)$ based on the information in $\mathbf{z}</em>)$ to realize the full molecular graph. The junction tree approach allows us to maintain chemical feasibility during generation.
Notation A molecular graph is defined as $G=(V, E)$ where $V$ is the set of atoms (vertices) and $E$ the set of bonds (edges). Let $N(x)$ be the neighbor of $x$. We denote sigmoid}}$. Second, we predict the fine grain connectivity between the clusters in the junction tree using a graph decoder $p(G \mid \mathcal{T}, \mathbf{z}_{G</p>
<p>function as $\sigma(\cdot)$ and ReLU function as $\tau(\cdot)$. We use $i, j, k$ for nodes in the tree and $u, v, w$ for nodes in the graph.</p>
<h3>2.1. Junction Tree</h3>
<p>A tree decomposition maps a graph $G$ into a junction tree by contracting certain vertices into a single node so that $G$ becomes cycle-free. Formally, given a graph $G$, a junction tree $\mathcal{T}<em 1="1">{G}=(\mathcal{V}, \mathcal{E}, \mathcal{X})$ is a connected labeled tree whose node set is $\mathcal{V}=\left{C</em>\right)$ is an induced subgraph of $G$, satisfying the following constraints:}, \cdots, C_{n}\right}$ and edge set is $\mathcal{E}$. Each node or cluster $C_{i}=\left(V_{i}, E_{i</p>
<ol>
<li>The union of all clusters equals $G$. That is, $\bigcup_{i} V_{i}=V$ and $\bigcup_{i} E_{i}=E$.</li>
<li>Running intersection: For all clusters $C_{i}, C_{j}$ and $C_{k}$, $V_{i} \cap V_{j} \subseteq V_{k}$ if $C_{k}$ is on the path from $C_{i}$ to $C_{j}$.</li>
</ol>
<p>Viewing induced subgraphs as cluster labels, junction trees are labeled trees with label vocabulary $\mathcal{X}$. By our molecule tree decomposition, $\mathcal{X}$ contains only cycles (rings) and single edges. Thus the vocabulary size is limited $(|\mathcal{X}|=780$ for a standard dataset with 250 K molecules).</p>
<p>Tree Decomposition of Molecules Here we present our tree decomposition algorithm tailored for molecules, which finds its root in chemistry (Rarey \&amp; Dixon, 1998). Our cluster vocabulary $\mathcal{X}$ includes chemical structures such as bonds and rings (Figure 3). Given a graph $G$, we first find all its simple cycles, and its edges not belonging to any cycles. Two simple rings are merged together if they have more than two overlapping atoms, as they constitute a specific structure called bridged compounds (Clayden et al., 2001). Each of those cycles or edges is considered as a cluster. Next, a cluster graph is constructed by adding edges between all intersecting clusters. Finally, we select one of its spanning trees as the junction tree of $G$ (Figure 3). As a result of ring merging, any two clusters in the junction tree have at most two atoms in common, facilitating efficient inference in the graph decoding phase. The detailed procedure is described in the supplementary.</p>
<h3>2.2. Graph Encoder</h3>
<p>We first encode the latent representation of $G$ by a graph message passing network (Dai et al., 2016; Gilmer et al., 2017). Each vertex $v$ has a feature vector $\mathbf{x}<em u="u" v="v">{v}$ indicating the atom type, valence, and other properties. Similarly, each edge $(u, v) \in E$ has a feature vector $\mathbf{x}</em>}$ indicating its bond type, and two hidden vectors $\boldsymbol{\nu<em u="u" v="v">{u v}$ and $\boldsymbol{\nu}</em>$ denoting the message from $u$ to $v$ and vice versa. Due to the loopy structure of the graph, messages are exchanged in a loopy belief propagation fashion:</p>
<p>$$
\boldsymbol{\nu}<em 1="1">{u v}^{(t)}=\tau\left(\mathbf{W}</em>}^{g} \mathbf{x<em 2="2">{u}+\mathbf{W}</em>}^{g} \mathbf{x<em 3="3">{u v}+\mathbf{W}</em>\right)
$$}^{g} \sum_{w \in N(u) \backslash v} \boldsymbol{\nu}_{w u}^{(t-1)</p>
<p>where $\boldsymbol{\nu}<em u="u" v="v">{u v}^{(t)}$ is the message computed in $t$-th iteration, initialized with $\boldsymbol{\nu}</em>$. After $T$ steps of iteration, we aggregate those messages as the latent vector of each vertex, which captures its local graphical structure:}^{(0)}=\mathbf{0</p>
<p>$$
\mathbf{h}<em 1="1">{u}=\tau\left(\mathbf{U}</em>}^{g} \mathbf{x<em N_u_="N(u)" _in="\in" v="v">{u}+\sum</em>} \mathbf{U<em u="u" v="v">{2}^{g} \boldsymbol{\nu}</em>\right)
$$}^{(T)</p>
<p>The final graph representation is $\mathbf{h}<em i="i">{G}=\sum</em>} \mathbf{h<em G="G">{i} /|V|$. The mean $\boldsymbol{\mu}</em>}$ and $\log$ variance $\log \boldsymbol{\sigma<em G="G">{G}$ of the variational posterior approximation are computed from $\mathbf{h}</em>}$ with two separate affine layers. $\mathbf{z<em G="G">{G}$ is sampled from a Gaussian $\mathcal{N}\left(\boldsymbol{\mu}</em>\right)$.}, \boldsymbol{\sigma}_{G</p>
<h3>2.3. Tree Encoder</h3>
<p>We similarly encode $\mathcal{T}<em i="i">{G}$ with a tree message passing network. Each cluster $C</em>}$ is represented by a one-hot encoding $\mathbf{x<em i="i">{i}$ representing its label type. Each edge $\left(C</em>}, C_{j}\right)$ is associated with two message vectors $\mathbf{m<em i="i" j="j">{i j}$ and $\mathbf{m}</em>$ is updated as:}$. We pick an arbitrary leaf node as the root and propagate messages in two phases. In the first bottom-up phase, messages are initiated from the leaf nodes and propagated iteratively towards root. In the top-down phase, messages are propagated from the root to all the leaf nodes. Message $\mathbf{m}_{i j</p>
<p>$$
\mathbf{m}<em i="i">{i j}=\operatorname{GRU}\left(\mathbf{x}</em>},\left{\mathbf{m<em N_i_="N(i)" _backslash="\backslash" _in="\in" j="j" k="k">{k i}\right}</em>\right)
$$</p>
<p>where GRU is a Gated Recurrent Unit (Chung et al., 2014; Li et al., 2015) adapted for tree message passing:</p>
<p>$$
\begin{aligned}
\mathbf{s}<em N_i_="N(i)" _backslash="\backslash" _in="\in" j="j" k="k">{i j} &amp; =\sum</em>} \mathbf{m<em i="i" j="j">{k i} \
\mathbf{z}</em>} &amp; =\sigma\left(\mathbf{W}^{z} \mathbf{x<em i="i" j="j">{i}+\mathbf{U}^{z} \mathbf{s}</em>\right) \
\mathbf{r}}+\mathbf{b}^{z<em i="i">{k i} &amp; =\sigma\left(\mathbf{W}^{r} \mathbf{x}</em>}+\mathbf{U}^{r} \mathbf{m<em i="i" j="j">{k i}+\mathbf{b}^{r}\right) \
\widehat{\mathbf{m}}</em>} &amp; =\tanh \left(\mathbf{W} \mathbf{x<em N_i_="N(i)" _backslash="\backslash" _in="\in" j="j" k="k">{i}+\mathbf{U} \sum</em>} \mathbf{r<em i="i" k="k">{k i} \odot \mathbf{m}</em>\right) \
\mathbf{m}<em i="i" j="j">{i j} &amp; =\left(1-\mathbf{z}</em>}\right) \odot \mathbf{s<em i="i" j="j">{i j}+\mathbf{z}</em>
\end{aligned}
$$} \odot \widehat{\mathbf{m}}_{i j</p>
<p>The message passing follows the schedule where $\mathbf{m}<em i="i" k="k">{i j}$ is computed only when all its precursors $\left{\mathbf{m}</em> \mid k \in N(i) \backslash j\right}$ have been computed. This architectural design is motivated by the belief propagation algorithm over trees and is thus different from the graph encoder.</p>
<p>After the message passing, we obtain the latent representation of each node $\mathbf{h}_{i}$ by aggregating its inward messages:</p>
<p>$$
\mathbf{h}<em i="i">{i}=\tau\left(\mathbf{W}^{o} \mathbf{x}</em>\right)
$$}+\sum_{k \in N(i)} \mathbf{U}^{o} \mathbf{m}_{k i</p>
<p>The final tree representation is $\mathbf{h}<em G="G">{\mathcal{T}</em>}}=\mathbf{h<em _mathcal_T="\mathcal{T">{\text {root }}$, which encodes a rooted tree $(\mathcal{T}$, root $)$. Unlike the graph encoder, we do not apply node average pooling because it confuses the tree decoder which node to generate first. $\mathbf{z}</em><em _mathcal_T="\mathcal{T">{G}}$ is sampled in a similar way as in the graph encoder. For simplicity, we abbreviate $\mathbf{z}</em><em _mathcal_T="\mathcal{T">{G}}$ as $\mathbf{z}</em>$ from now on.}</p>
<p>This tree encoder plays two roles in our framework. First, it is used to compute $\mathbf{z}_{\mathcal{T}}$, which only requires the bottom-up</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4. Illustration of the tree decoding process. Nodes are labeled in the order in which they are generated. 1) Node 2 expands child node 4 and predicts its label with message $\mathbf{h}<em 42="42">{24}$. 2) As node 4 is a leaf node, decoder backtracks and computes message $\mathbf{h}</em>$. 3) Decoder continues to backtrack as node 2 has no more children. 4) Node 1 expands node 5 and predicts its label.
phase of the network. Second, after a tree $\widehat{\mathcal{T}}$ is decoded from $\mathbf{z}<em i="i" j="j">{\mathcal{T}}$, it is used to compute messages $\widehat{\mathbf{m}}</em>$, to provide essential contexts of every node during graph decoding. This requires both top-down and bottom-up phases. We will elaborate this in section 2.5.}$ over the entire $\widehat{\mathcal{T}</p>
<h3>2.4. Tree Decoder</h3>
<p>We decode a junction tree $\mathcal{T}$ from its encoding $\mathbf{z}_{\mathcal{T}}$ with a tree structured decoder. The tree is constructed in a top-down fashion by generating one node at a time. As illustrated in Figure 4, our tree decoder traverses the entire tree from the root, and generates nodes in their depth-first order. For every visited node, the decoder first makes a topological prediction: whether this node has children to be generated. When a new child node is created, we predict its label and recurse this process. Recall that cluster labels represent subgraphs in a molecule. The decoder backtracks when a node has no more children to generate.</p>
<p>At each time step, a node receives information from other nodes in the current tree for making those predictions. The information is propagated through message vectors $\mathbf{h}<em 1="1">{i j}$ when trees are incrementally constructed. Formally, let $\hat{\mathcal{E}}=\left{\left(i</em>}, j_{1}\right), \cdots,\left(i_{m}, j_{m}\right)\right}$ be the edges traversed in a depth first traversal over $\mathcal{T}=(\mathcal{V}, \mathcal{E})$, where $m=2|\mathcal{E}|$ as each edge is traversed in both directions. The model visits node $i_{t}$ at time $t$. Let $\hat{\mathcal{E}<em i__t="i_{t">{t}$ be the first $t$ edges in $\hat{\mathcal{E}}$. The message $\mathbf{h}</em>$ is updated through previous messages:}, j_{t}</p>
<p>$$
\mathbf{h}<em t="t">{i</em>}, j_{t}}=\operatorname{GRU}\left(\mathbf{x<em t="t">{i</em>}},\left{\mathbf{h<em t="t">{k, i</em>\right}}<em t="t">{\left(k, i</em>}\right) \in \hat{\mathcal{E}<em t="t">{t}, k \neq j</em>\right)
$$}</p>
<p>where GRU is the same recurrent unit as in the tree encoder.
Topological Prediction When the model visits node $i_{t}$, it makes a binary prediction on whether it still has children to</p>
<div class="codehilite"><pre><span></span><code>Algorithm 1 Tree decoding at sampling time
Require: Latent representation \(\mathbf{z}_{\mathcal{T}}\)
    Initialize: Tree \(\widehat{\mathcal{T}} \leftarrow \emptyset\)
    function SampleTree \((i, t)\)
        Set \(\mathcal{X}_{i} \leftarrow\) all cluster labels that are chemically com-
        patible with node \(i\) and its current neighbors.
        Set \(d_{t} \leftarrow\) expand with probability \(p_{t} . \quad \triangleright\) Eq.(11)
        if \(d_{t}=\) expand and \(\mathcal{X}_{i} \neq \emptyset\) then
            Create a node \(j\) and add it to tree \(\widehat{\mathcal{T}}\).
            Sample the label of node \(j\) from \(\mathcal{X}_{i} \quad \triangleright . \mathrm{Eq} .(12)\)
            \(\operatorname{SampleTree}(j, t+1)\)
        end if
    end function
</code></pre></div>

<p>be generated. We compute this probability by combining $\mathbf{z}<em i__t="i_{t">{\mathcal{T}}$, node features $\mathbf{x}</em>}}$ and inward messages $\mathbf{h<em t="t">{k, i</em>$ via a one hidden layer network followed by a sigmoid function:}</p>
<p>$$
p_{t}=\sigma\left(\mathbf{u}^{d} \cdot \tau\left(\mathbf{W}<em i__t="i_{t">{1}^{d} \mathbf{x}</em>}}+\mathbf{W<em _mathcal_T="\mathcal{T">{2}^{d} \mathbf{z}</em>}}+\mathbf{W<em _left_k_="\left(k," i__t="i_{t">{3}^{d} \sum</em>}\right) \in \hat{\mathcal{E}<em i__t="i_{t" k_="k,">{t}} \mathbf{h}</em>\right)\right.
$$}</p>
<p>Label Prediction When a child node $j$ is generated from its parent $i$, we predict its node label with</p>
<p>$$
\mathbf{q}<em 1="1">{j}=\operatorname{softmax}\left(\mathbf{U}^{l} \tau\left(\mathbf{W}</em>}^{l} \mathbf{z<em 2="2">{\mathcal{T}}+\mathbf{W}</em>\right)\right)
$$}^{l} \mathbf{h}_{i j</p>
<p>where $\mathbf{q}<em i="i" j="j">{j}$ is a distribution over label vocabulary $\mathcal{X}$. When $j$ is a root node, its parent $i$ is a virtual node and $\mathbf{h}</em>$.}=\mathbf{0</p>
<p>Learning The tree decoder aims to maximize the likelihood $p\left(\mathcal{T} \mid \mathbf{z}<em t="t">{\mathcal{T}}\right)$. Let $\hat{p}</em>$} \in{0,1}$ and $\hat{\mathbf{q}}_{j}$ be the ground truth topological and label values, the decoder minimizes the following cross entropy loss: ${ }^{2</p>
<p>$$
\mathcal{L}<em t="t">{c}(\mathcal{T})=\sum</em>} \mathcal{L}^{d}\left(p_{t}, \hat{p<em j="j">{t}\right)+\sum</em>} \mathcal{L}^{l}\left(\mathbf{q<em j="j">{j}, \hat{\mathbf{q}}</em>\right)
$$</p>
<p>Similar to sequence generation, during training we perform teacher forcing: after topological and label prediction at each step, we replace them with their ground truth so that the model makes predictions given correct histories.</p>
<p>Decoding \&amp; Feasibility Check Algorithm 1 shows how a tree is sampled from $\mathbf{z}<em i="i">{\mathcal{T}}$. The tree is constructed recursively guided by topological predictions without any external guidance used in training. To ensure the sampled tree could be realized into a valid molecule, we define set $\mathcal{X}</em>}$ to be cluster labels that are chemically compatible with node $i$ and its current neighbors. When a child node $j$ is generated from node $i$, we sample its label from $\mathcal{X<em j="j">{i}$ with a renormalized distribution $\mathbf{q}</em>$ by masking out invalid labels.}$ over $\mathcal{X}_{i</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5. Decode a molecule from a junction tree. 1) Ground truth molecule $G$. 2) Predicted junction tree $\widetilde{\mathcal{T}}$. 3) We enumerate different combinations between red cluster $C$ and its neighbors. Crossed arrows indicate combinations that lead to chemically infeasible molecules. Note that if we discard tree structure during enumeration (i.e., ignoring subtree A), the last two candidates will collapse into the same molecule. 4) Rank subgraphs at each node. The final graph is decoded by putting together all the predicted subgraphs (dashed box).</p>
<h3>2.5. Graph Decoder</h3>
<p>The final step of our model is to reproduce a molecular graph $G$ that underlies the predicted junction tree $\widetilde{\mathcal{T}}=(\widetilde{\mathcal{V}}, \widetilde{\mathcal{E}})$. Note that this step is not deterministic since there are potentially many molecules that correspond to the same junction tree. The underlying degree of freedom pertains to how neighboring clusters $C_{i}$ and $C_{j}$ are attached to each other as subgraphs. Our goal here is to assemble the subgraphs (nodes in the tree) together into the correct molecular graph.
Let $\mathcal{G}(\mathcal{T})$ be the set of graphs whose junction tree is $\mathcal{T}$. Decoding graph $\hat{G}$ from $\widetilde{\mathcal{T}}=(\widetilde{\mathcal{V}}, \widetilde{\mathcal{E}})$ is a structured prediction:</p>
<p>$$
\hat{G}=\arg \max _{G^{\prime} \in \mathcal{G}(\widetilde{\mathcal{T}})} f^{a}\left(G^{\prime}\right)
$$</p>
<p>where $f^{a}$ is a scoring function over candidate graphs. We only consider scoring functions that decompose across the clusters and their neighbors. In other words, each term in the scoring function depends only on how a cluster $C_{i}$ is attached to its neighboring clusters $C_{j}, j \in N_{\widetilde{\mathcal{T}}}(i)$ in the tree $\widetilde{\mathcal{T}}$. The problem of finding the highest scoring graph $\hat{G}-$ the assembly task - could be cast as a graphical model infer-
ence task in a model induced by the junction tree. However, for efficiency reasons, we will assemble the molecular graph one neighborhood at a time, following the order in which the tree itself was decoded. In other words, we start by sampling the assembly of the root and its neighbors according to their scores. Then we proceed to assemble the neighbors and their associated clusters (removing the degrees of freedom set by the root assembly), and so on.</p>
<p>It remains to be specified how each neighborhood realization is scored. Let $G_{i}$ be the subgraph resulting from a particular merging of cluster $C_{i}$ in the tree with its neighbors $C_{j}, j \in N_{\widetilde{\mathcal{T}}}(i)$. We score $G_{i}$ as a candidate subgraph by first deriving a vector representation $\mathbf{h}<em i="i">{G</em>}}$ and then using $f_{i}^{a}\left(G_{i}\right)=\mathbf{h<em i="i">{G</em>}} \cdot \mathbf{z<em i="i">{G}$ as the subgraph score. To this end, let $u, v$ specify atoms in the candidate subgraph $G</em>}$ and let $\alpha_{v}=i$ if $v \in C_{i}$ and $\alpha_{v}=j$ if $v \in C_{j} \backslash C_{i}$. The indices $\alpha_{v}$ are used to mark the position of the atoms in the junction tree, and to retrieve messages $\widetilde{\mathbf{m}<em i="i">{i, j}$ summarizing the subtree under $i$ along the edge $(i, j)$ obtained by running the tree encoding algorithm. The neural messages pertaining to the atoms and bonds in subgraph $G</em>}$ are obtained and aggregated into $\mathbf{h<em i="i">{G</em>$, similarly to the encoding step, but with different (learned) parameters:}</p>
<p>$$
\begin{aligned}
\boldsymbol{\mu}<em 1="1">{u v}^{(t)} &amp; =\tau\left(\mathbf{W}</em>}^{a} \mathbf{x<em 2="2">{u}+\mathbf{W}</em>}^{a} \mathbf{x<em 3="3">{u v}+\mathbf{W}</em>}^{a} \tilde{\boldsymbol{\mu}<em u="u" v="v">{u v}^{(t-1)}\right) \
\widetilde{\boldsymbol{\mu}}</em>}^{(t-1)} &amp; = \begin{cases}\sum_{w \in N(u) \backslash v} \boldsymbol{\mu<em u="u">{w u}^{(t-1)} &amp; \alpha</em> \
\widetilde{\mathbf{m}}}=\alpha_{v<em u="u">{\alpha</em>}, \alpha_{v}}+\sum_{w \in N(u) \backslash v} \boldsymbol{\mu<em u="u">{w u}^{(t-1)} &amp; \alpha</em>
\end{aligned}
$$} \neq \alpha_{v}\end{cases</p>
<p>The major difference from Eq. (1) is that we augment the model with tree messages $\widetilde{\mathbf{m}}<em u="u">{\alpha</em>}, \alpha_{v}}$ derived by running the tree encoder over the predicted tree $\widetilde{\mathcal{T}}$. $\widetilde{\mathbf{m}<em u="u">{\alpha</em>$ provides a tree dependent positional context for bond $(u, v)$ (illustrated as subtree A in Figure 5).}, \alpha_{v}</p>
<p>Learning The graph decoder parameters are learned to maximize the log-likelihood of predicting correct subgraphs $G_{i}$ of the ground true graph $G$ at each tree node:</p>
<p>$$
\mathcal{L}<em i="i">{g}(G)=\sum</em>}\left[f^{a}\left(G_{i}\right)-\log \sum_{G_{i}^{\prime} \in \mathcal{G<em i="i">{i}} \exp \left(f^{a}\left(G</em>\right)\right)\right]
$$}^{\prime</p>
<p>where $\mathcal{G}_{i}$ is the set of possible candidate subgraphs at tree node $i$. During training, we again apply teacher forcing, i.e. we feed the graph decoder with ground truth trees as input.</p>
<p>Complexity By our tree decomposition, any two clusters share at most two atoms, so we only need to merge at most two atoms or one bond. By pruning chemically invalid subgraphs and merging isomorphic graphs, $\left|\mathcal{G}_{i}\right| \approx 4$ on average when tested on a standard ZINC drug dataset. The computational complexity of JT-VAE is therefore linear in the number of clusters, scaling nicely to large graphs.</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6. Left: Random molecules sampled from prior distribution $\mathcal{N}(\mathbf{0}, \mathbf{I})$. Right: Visualization of the local neighborhood of a molecule in the center. Three molecules highlighted in red dashed box have the same tree structure as the center molecule, but with different graph structure as their clusters are combined differently. The same phenomenon emerges in another group of molecules (blue dashed box).</p>
<h2>3. Experiments</h2>
<p>Our evaluation efforts measure various aspects of molecular generation. The first two evaluations follow previously proposed tasks (Kusner et al., 2017). We also introduce a third task - constrained molecule optimization.</p>
<ul>
<li>Molecule reconstruction and validity We test the VAE models on the task of reconstructing input molecules from their latent representations, and decoding valid molecules when sampling from prior distribution. (Section 3.1)</li>
<li>Bayesian optimization Moving beyond generating valid molecules, we test how the model can produce novel molecules with desired properties. To this end, we perform Bayesian optimization in the latent space to search molecules with specified properties. (Section 3.2)</li>
<li>Constrained molecule optimization The task is to modify given molecules to improve specified properties, while constraining the degree of deviation from the original molecule. This is a more realistic scenario in drug discovery, where development of new drugs usually starts with known molecules such as existing drugs (Besnard et al., 2012). Since it is a new task, we cannot compare to any existing baselines. (Section 3.3)</li>
</ul>
<p>Below we describe the data, baselines and model configuration that are shared across the tasks. Additional setup details are provided in the task-specific sections.</p>
<p>Data We use the ZINC molecule dataset from Kusner et al. (2017) for our experiments, with the same training/testing split. It contains about 250 K drug molecules extracted from
the ZINC database (Sterling \&amp; Irwin, 2015). We follow the same train/test split as in Kusner et al. (2017).</p>
<p>Baselines We compare our approach with SMILES-based baselines: 1) Character VAE (CVAE) (Gómez-Bombarelli et al., 2016) which generates SMILES strings character by character; 2) Grammar VAE (GVAE) (Kusner et al., 2017) that generates SMILES following syntactic constraints given by a context-free grammar; 3) Syntax-directed VAE (SDVAE) (Dai et al., 2018) that incorporates both syntactic and semantic constraints of SMILES via attribute grammar. For molecule generation task, we also compare with GraphVAE (Simonovsky \&amp; Komodakis, 2018) that directly generates atom labels and adjacency matrices of graphs, as well as an LSTM-based autoregressive model that generates molecular graphs atom by atom (Li et al., 2018).</p>
<p>Model Configuration To be comparable with the above baselines, we set the latent space dimension as 56, i.e., the tree and graph representation $\mathbf{h}<em G="G">{T}$ and $\mathbf{h}</em>$ have 28 dimensions each. Full training details and model configurations are provided in the appendix.</p>
<h3>3.1. Molecule Reconstruction and Validity</h3>
<p>Setup The first task is to reconstruct and sample molecules from latent space. Since both encoding and decoding process are stochastic, we estimate reconstruction accuracy by Monte Carlo method used in (Kusner et al., 2017): Each molecule is encoded 10 times and each encoding is decoded 10 times. We report the portion of the 100 decoded molecules that are identical to the input molecule.</p>
<p>Table 1. Reconstruction accuracy and prior validity results. Baseline results are copied from <em>Kusner et al. (2017); Dai et al. (2018); Simonovsky &amp; Komodakis (2018); Li et al. (2018)</em>.</p>
<table>
<thead>
<tr>
<th>Method</th>
<th>Reconstruction</th>
<th>Validity</th>
</tr>
</thead>
<tbody>
<tr>
<td>CVAE</td>
<td>44.6%</td>
<td>0.7%</td>
</tr>
<tr>
<td>GVAE</td>
<td>53.7%</td>
<td>7.2%</td>
</tr>
<tr>
<td>SD-VAE</td>
<td>76.2%</td>
<td>43.5%</td>
</tr>
<tr>
<td>GraphVAE</td>
<td>-</td>
<td>13.5%</td>
</tr>
<tr>
<td>Atom-by-Atom LSTM</td>
<td>-</td>
<td>89.2%</td>
</tr>
<tr>
<td>JT-VAE</td>
<td>76.7%</td>
<td>100.0%</td>
</tr>
</tbody>
</table>
<p>To compute validity, we sample 1000 latent vectors from the prior distribution $\mathcal{N}(\mathbf{0},\mathbf{I})$, and decode each of these vectors 100 times. We report the percentage of decoded molecules that are chemically valid (checked by RDKit). For ablation study, we also report the validity of our model without validity check in decoding phase.</p>
<p>Results Table 1 shows that JT-VAE outperforms previous models in molecule reconstruction, and always produces valid molecules when sampled from prior distribution. In contrast, the atom-by-atom based generation only achieves 89.2% validity as it needs to go through invalid intermediate states (Figure 2). Our model bypasses this issue by utilizing valid substructures as building blocks. As shown in Figure 6, the sampled molecules have non-trivial structures such as simple chains. We further sampled 5000 molecules from prior and found they are <em>all distinct</em> from the training set. Thus our model is not a simple memorization.</p>
<p>Analysis We qualitatively examine the latent space of JT-VAE by visualizing the neighborhood of molecules. Given a molecule, we follow the method in <em>Kusner et al. (2017)</em> to construct a grid visualization of its neighborhood. Figure 6 shows the local neighborhood of the same molecule visualized in <em>Dai et al. (2018)</em>. In comparison, our neighborhood does not contain molecules with huge rings (with more than 7 atoms), which rarely occur in the dataset. We also highlight two groups of closely resembling molecules that have identical tree structures but vary only in how clusters are attached together. This demonstrates the smoothness of learned molecular embeddings.</p>
<h3>3.2 Bayesian Optimization</h3>
<p>Setup The second task is to produce novel molecules with desired properties. Following <em>(Kusner et al., 2017)</em>, our target chemical property $y(\cdot)$ is octanol-water partition coefficients (logP) penalized by the synthetic accessibility (SA) score and number of long cycles. To perform Bayesian optimization (BO), we first train a VAE and associate each</p>
<p>Table 2. Best molecule property scores found by each method. Baseline results are from <em>Kusner et al. (2017); Dai et al. (2018)</em>.</p>
<table>
<thead>
<tr>
<th>Method</th>
<th>1st</th>
<th>2nd</th>
<th>3rd</th>
</tr>
</thead>
<tbody>
<tr>
<td>CVAE</td>
<td>1.98</td>
<td>1.42</td>
<td>1.19</td>
</tr>
<tr>
<td>GVAE</td>
<td>2.94</td>
<td>2.89</td>
<td>2.80</td>
</tr>
<tr>
<td>SD-VAE</td>
<td>4.04</td>
<td>3.50</td>
<td>2.96</td>
</tr>
<tr>
<td>JT-VAE</td>
<td>5.30</td>
<td>4.93</td>
<td>4.49</td>
</tr>
</tbody>
</table>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7. Best three molecules and their property scores found by JT-VAE using Bayesian optimization.</p>
<p>molecule with a latent vector, given by the mean of the variational encoding distribution. After the VAE is learned, we train a sparse Gaussian process (SGP) to predict $y(m)$ given its latent representation. Then we perform five iterations of batched BO using the expected improvement heuristic.</p>
<p>For comparison, we report 1) the predictive performance of SGP trained on latent encodings learned by different VAEs, measured by log-likelihood (LL) and root mean square error (RMSE) with 10-fold cross validation. 2) The top-3 molecules found by BO under different models.</p>
<p>Results As shown in Table 2, JT-VAE finds molecules with significantly better scores than previous methods. Figure 7 lists the top-3 best molecules found by JT-VAE. In fact, JT-VAE finds over 50 molecules with scores over 3.50 (the second best molecule proposed by SD-VAE). Moreover, the SGP yields better predictive performance when trained on JT-VAE embeddings (Table 3).</p>
<h3>3.3 Constrained Optimization</h3>
<p>Setup The third task is to perform molecule optimization in a constrained scenario. Given a molecule $m$, the task is to find a different molecule $m^{\prime}$ that has the highest property value with the molecular similarity $\operatorname{sim}(m, m^{\prime}) \geq \delta$ for some threshold $\delta$. We use Tanimoto similarity with Morgan fingerprint <em>(Rogers &amp; Hahn, 2010)</em> as the similarity metric, and penalized logP coefficient as our target chemical property. For this task, we jointly train a property predictor $F$ (parameterized by a feed-forward network) with JT-VAE to predict $y(m)$ from the latent embedding of $m$. To optimize a molecule $m$, we start from its latent representation, and apply gradient ascent in the latent space to improve the predicted score $F(\cdot)$, similar to <em>(Mueller et al., 2017)</em>. After</p>
<p>Table 3: Predictive performance of sparse Gaussian Processes trained on different VAEs. Baseline results are copied from <em>Kusner et al. (2017)</em> and <em>Dai et al. (2018)</em>.</p>
<table>
<thead>
<tr>
<th>Method</th>
<th>LL</th>
<th>RMSE</th>
</tr>
</thead>
<tbody>
<tr>
<td>CVAE</td>
<td>$-1.812\pm 0.004$</td>
<td>$1.504\pm 0.006$</td>
</tr>
<tr>
<td>GVAE</td>
<td>$-1.739\pm 0.004$</td>
<td>$1.404\pm 0.006$</td>
</tr>
<tr>
<td>SD-VAE</td>
<td>$-1.697\pm 0.015$</td>
<td>$1.366\pm 0.023$</td>
</tr>
<tr>
<td>JT-VAE</td>
<td>$\mathbf{- 1.6 5 8 \pm 0 . 0 2 3}$</td>
<td>$\mathbf{1 . 2 9 0 \pm 0 . 0 2 6}$</td>
</tr>
</tbody>
</table>
<p>Table 4: Constrained optimization result of JT-VAE: mean and standard deviation of property improvement, molecular similarity and success rate under constraints $\operatorname{sim}\left(m, m^{\prime}\right) \geq \delta$ with varied $\delta$.</p>
<table>
<thead>
<tr>
<th>$\delta$</th>
<th>Improvement</th>
<th>Similarity</th>
<th>Success</th>
</tr>
</thead>
<tbody>
<tr>
<td>0.0</td>
<td>$1.91 \pm 2.04$</td>
<td>$0.28 \pm 0.15$</td>
<td>97.5%</td>
</tr>
<tr>
<td>0.2</td>
<td>$1.68 \pm 1.85$</td>
<td>$0.33 \pm 0.13$</td>
<td>97.1%</td>
</tr>
<tr>
<td>0.4</td>
<td>$0.84 \pm 1.45$</td>
<td>$0.51 \pm 0.10$</td>
<td>83.6%</td>
</tr>
<tr>
<td>0.6</td>
<td>$0.21 \pm 0.71$</td>
<td>$0.69 \pm 0.06$</td>
<td>46.4%</td>
</tr>
</tbody>
</table>
<p>applying $K=80$ gradient steps, $K$ molecules are decoded from resulting latent trajectories, and we report the molecule with the highest $F(\cdot)$ that satisfies the similarity constraint. A modification succeeds if one of the decoded molecules satisfies the constraint and is distinct from the original.</p>
<p>To provide the greatest challenge, we selected 800 molecules with the lowest property score $y(\cdot)$ from the test set. We report the success rate (how often a modification succeeds), and among success cases the average improvement $y\left(m^{\prime}\right)-$ $y(m)$ and molecular similarity $\operatorname{sim}\left(m, m^{\prime}\right)$ between the original and modified molecules $m$ and $m^{\prime}$.</p>
<p>Results Our results are summarized in Table 4. The unconstrained scenario $(\delta=0)$ has the best average improvement, but often proposes dissimilar molecules. When we tighten the constraint to $\delta=0.4$, about $80 \%$ of the time our model finds similar molecules, with an average improvement 0.84 . This also demonstrates the smoothness of the learned latent space. Figure 8 illustrates an effective modification resulting in a similar molecule with great improvement.</p>
<h2>4 Related Work</h2>
<p>Molecule Generation Previous work on molecule generation mostly operates on SMILES strings. <em>GómezBombarelli et al. (2016); Segler et al. (2017)</em> built generative models of SMILES strings with recurrent decoders. Unfortunately, these models could generate invalid SMILES that do not result in any molecules. To remedy this issue, <em>Kusner et al. (2017); Dai et al. (2018)</em> complemented the decoder with syntactic and semantic constraints of SMILES by context free and attribute grammars, but these grammars do not fully capture chemical validity. Other techniques such as active learning <em>(Janz et al., 2017)</em> and reinforcement</p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 8. A molecule modification that yields an improvement of 4.0 with molecular similarity 0.617 (modified part is in red).
learning <em>(Guimaraes et al., 2017)</em> encourage the model to generate valid SMILES through additional training signal. Very recently, <em>Simonovsky &amp; Komodakis (2018)</em> proposed to generate molecular graphs by predicting their adjacency matrices, and <em>Li et al. (2018)</em> generated molecules node by node. In comparison, our method enforces chemical validity and is more efficient due to the coarse-to-fine generation.</p>
<p>Graph-structured Encoders The neural network formulation on graphs was first proposed by <em>Gori et al. (2005); Scarselli et al. (2009)</em>, and later enhanced by <em>Li et al. (2015)</em> with gated recurrent units. For recurrent architectures over graphs, <em>Lei et al. (2017)</em> designed Weisfeiler-Lehman kernel network inspired by graph kernels. <em>Dai et al. (2016)</em> considered a different architecture where graphs were viewed as latent variable graphical models, and derived their model from message passing algorithms. Our tree and graph encoder are closely related to this graphical model perspective, and to neural message passing networks <em>(Gilmer et al., 2017)</em>. For convolutional architectures, <em>Duvenaud et al. (2015)</em> introduced a convolution-like propagation on molecular graphs, which was generalized to other domains by <em>Niepert et al. (2016)</em>. <em>Bruna et al. (2013); Henaff et al. (2015)</em> developed graph convolution in spectral domain via graph Laplacian. For applications, graph neural networks are used in semisupervised classification <em>(Kipf &amp; Welling, 2016)</em>, computer vision <em>(Monti et al., 2016)</em>, and chemical domains <em>(Kearnes et al., 2016; Schütt et al., 2017; Jin et al., 2017)</em>.</p>
<p>Tree-structured Models Our tree encoder is related to recursive neural networks and tree-LSTM <em>(Socher et al., 2013; Tai et al., 2015; Zhu et al., 2015)</em>. These models encode tree structures where nodes in the tree are bottom-up transformed into vector representations. In contrast, our model propagates information both bottom-up and top-down.</p>
<p>On the decoding side, tree generation naturally arises in natural language parsing <em>(Dyer et al., 2016; Kiperwasser &amp; Goldberg, 2016)</em>. Different from our approach, natural language parsers have access to input words and only predict the topology of the tree. For general purpose tree generation, <em>Vinyals et al. (2015); Aharoni &amp; Goldberg (2017)</em> applied recurrent networks to generate linearized version of trees, but their architectures were entirely sequence-based. <em>Dong &amp; Lapata (2016); Alvarez-Melis &amp; Jaakkola (2016)</em> proposed tree-based architectures that construct trees top-down from the root. Our model is most closely related to <em>Alvarez-Melis &amp; Jaakkola (2016)</em> that disentangles topological prediction from label prediction, but we generate nodes in a depth-first</p>
<p>order and have additional steps that propagate information bottom-up. This forward-backward propagation also appears in Parisotto et al. (2016), but their model is node based whereas ours is based on message passing.</p>
<h2>5. Conclusion</h2>
<p>In this paper we present a junction tree variational autoencoder for generating molecular graphs. Our method significantly outperforms previous work in molecule generation and optimization. For future work, we attempt to generalize our method for general low-treewidth graphs.</p>
<h2>Acknowledgement</h2>
<p>We thank Jonas Mueller, Chengtao Li, Tao Lei and MIT NLP Group for their helpful comments. This work was supported by the DARPA Make-It program under contract ARO W911NF-16-2-0023.</p>
<h2>References</h2>
<p>Aharoni, R. and Goldberg, Y. Towards string-to-tree neural machine translation. arXiv preprint arXiv:1704.04743, 2017.</p>
<p>Alvarez-Melis, D. and Jaakkola, T. S. Tree-structured decoding with doubly-recurrent neural networks. 2016.</p>
<p>Besnard, J., Ruda, G. F., Setola, V., Abecassis, K., Rodriguiz, R. M., Huang, X.-P., Norval, S., Sassano, M. F., Shin, A. I., Webster, L. A., et al. Automated design of ligands to polypharmacological profiles. Nature, 492(7428): 215-220, 2012.</p>
<p>Bruna, J., Zaremba, W., Szlam, A., and LeCun, Y. Spectral networks and locally connected networks on graphs. arXiv preprint arXiv:1312.6203, 2013.</p>
<p>Chung, J., Gulcehre, C., Cho, K., and Bengio, Y. Empirical evaluation of gated recurrent neural networks on sequence modeling. arXiv preprint arXiv:1412.3555, 2014.</p>
<p>Clayden, J., Greeves, N., Warren, S., and Wothers, P. Organic Chemistry. Oxford University Press, 2001.</p>
<p>Dai, H., Dai, B., and Song, L. Discriminative embeddings of latent variable models for structured data. In International Conference on Machine Learning, pp. 2702-2711, 2016.</p>
<p>Dai, H., Tian, Y., Dai, B., Skiena, S., and Song, L. Syntaxdirected variational autoencoder for structured data. International Conference on Learning Representations, 2018. URL https://openreview.net/forum? id=SyqShMZRb.</p>
<p>Dong, L. and Lapata, M. Language to logical form with neural attention. arXiv preprint arXiv:1601.01280, 2016.</p>
<p>Duvenaud, D. K., Maclaurin, D., Iparraguirre, J., Bombarell, R., Hirzel, T., Aspuru-Guzik, A., and Adams, R. P. Convolutional networks on graphs for learning molecular fingerprints. In Advances in neural information processing systems, pp. 2224-2232, 2015.</p>
<p>Dyer, C., Kuncoro, A., Ballesteros, M., and Smith, N. A. Recurrent neural network grammars. arXiv preprint arXiv:1602.07776, 2016.</p>
<p>Gilmer, J., Schoenholz, S. S., Riley, P. F., Vinyals, O., and Dahl, G. E. Neural message passing for quantum chemistry. arXiv preprint arXiv:1704.01212, 2017.</p>
<p>Gómez-Bombarelli, R., Wei, J. N., Duvenaud, D., Hernández-Lobato, J. M., Sánchez-Lengeling, B., Sheberla, D., Aguilera-Iparraguirre, J., Hirzel, T. D., Adams, R. P., and Aspuru-Guzik, A. Automatic chemical design using a data-driven continuous representation of molecules. ACS Central Science, 2016. doi: 10.1021/ acscentsci.7b00572.</p>
<p>Gori, M., Monfardini, G., and Scarselli, F. A new model for learning in graph domains. In Neural Networks, 2005. IJCNN'05. Proceedings. 2005 IEEE International Joint Conference on, volume 2, pp. 729-734. IEEE, 2005.</p>
<p>Guimaraes, G. L., Sanchez-Lengeling, B., Farias, P. L. C., and Aspuru-Guzik, A. Objective-reinforced generative adversarial networks (organ) for sequence generation models. arXiv preprint arXiv:1705.10843, 2017.</p>
<p>Henaff, M., Bruna, J., and LeCun, Y. Deep convolutional networks on graph-structured data. arXiv preprint arXiv:1506.05163, 2015.</p>
<p>Janz, D., van der Westhuizen, J., and Hernández-Lobato, J. M. Actively learning what makes a discrete sequence valid. arXiv preprint arXiv:1708.04465, 2017.</p>
<p>Jin, W., Coley, C., Barzilay, R., and Jaakkola, T. Predicting organic reaction outcomes with weisfeiler-lehman network. In Advances in Neural Information Processing Systems, pp. 2604-2613, 2017.</p>
<p>Kearnes, S., McCloskey, K., Berndl, M., Pande, V., and Riley, P. Molecular graph convolutions: moving beyond fingerprints. Journal of computer-aided molecular design, 30(8):595-608, 2016.</p>
<p>Kingma, D. P. and Welling, M. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013.</p>
<p>Kiperwasser, E. and Goldberg, Y. Easy-first dependency parsing with hierarchical tree lstms. arXiv preprint arXiv:1603.00375, 2016.</p>
<p>Kipf, T. N. and Welling, M. Semi-supervised classification with graph convolutional networks. arXiv preprint arXiv:1609.02907, 2016.</p>
<p>Kusner, M. J., Paige, B., and Hernández-Lobato, J. M. Grammar variational autoencoder. arXiv preprint arXiv:1703.01925, 2017.</p>
<p>Landrum, G. Rdkit: Open-source cheminformatics. Online). http://www. rdkit. org. Accessed, 3(04):2012, 2006.</p>
<p>Lei, T., Jin, W., Barzilay, R., and Jaakkola, T. Deriving neural architectures from sequence and graph kernels. arXiv preprint arXiv:1705.09037, 2017.</p>
<p>Li, Y., Tarlow, D., Brockschmidt, M., and Zemel, R. Gated graph sequence neural networks. arXiv preprint arXiv:1511.05493, 2015.</p>
<p>Li, Y., Vinyals, O., Dyer, C., Pascanu, R., and Battaglia, P. Learning deep generative models of graphs. arXiv preprint arXiv:1803.03324, 2018.</p>
<p>Monti, F., Boscaini, D., Masci, J., Rodolà, E., Svoboda, J., and Bronstein, M. M. Geometric deep learning on graphs and manifolds using mixture model cnns. arXiv preprint arXiv:1611.08402, 2016.</p>
<p>Mueller, J., Gifford, D., and Jaakkola, T. Sequence to better sequence: continuous revision of combinatorial structures. In International Conference on Machine Learning, pp. 2536-2544, 2017.</p>
<p>Niepert, M., Ahmed, M., and Kutzkov, K. Learning convolutional neural networks for graphs. In International Conference on Machine Learning, pp. 2014-2023, 2016.</p>
<p>Parisotto, E., Mohamed, A.-r., Singh, R., Li, L., Zhou, D., and Kohli, P. Neuro-symbolic program synthesis. arXiv preprint arXiv:1611.01855, 2016.</p>
<p>Rarey, M. and Dixon, J. S. Feature trees: a new molecular similarity measure based on tree matching. Journal of computer-aided molecular design, 12(5):471-490, 1998.</p>
<p>Rogers, D. and Hahn, M. Extended-connectivity fingerprints. Journal of chemical information and modeling, 50 (5):742-754, 2010.</p>
<p>Scarselli, F., Gori, M., Tsoi, A. C., Hagenbuchner, M., and Monfardini, G. The graph neural network model. IEEE Transactions on Neural Networks, 20(1):61-80, 2009.</p>
<p>Schütt, K., Kindermans, P.-J., Felix, H. E. S., Chmiela, S., Tkatchenko, A., and Müller, K.-R. Schnet: A continuousfilter convolutional neural network for modeling quantum interactions. In Advances in Neural Information Processing Systems, pp. 992-1002, 2017.</p>
<p>Segler, M. H., Kogej, T., Tyrchan, C., and Waller, M. P. Generating focussed molecule libraries for drug discovery with recurrent neural networks. arXiv preprint arXiv:1701.01329, 2017.</p>
<p>Simonovsky, M. and Komodakis, N. Graphvae: Towards generation of small graphs using variational autoencoders. arXiv preprint arXiv:1802.03480, 2018.</p>
<p>Socher, R., Perelygin, A., Wu, J., Chuang, J., Manning, C. D., Ng, A., and Potts, C. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the 2013 conference on empirical methods in natural language processing, pp. 1631-1642, 2013.</p>
<p>Sterling, T. and Irwin, J. J. Zinc 15-ligand discovery for everyone. J. Chem. Inf. Model, 55(11):2324-2337, 2015.</p>
<p>Tai, K. S., Socher, R., and Manning, C. D. Improved semantic representations from tree-structured long short-term memory networks. arXiv preprint arXiv:1503.00075, 2015.</p>
<p>Vinyals, O., Kaiser, Ł., Koo, T., Petrov, S., Sutskever, I., and Hinton, G. Grammar as a foreign language. In Advances in Neural Information Processing Systems, pp. 2773-2781, 2015.</p>
<p>Weininger, D. Smiles, a chemical language and information system. 1. introduction to methodology and encoding rules. Journal of chemical information and computer sciences, 28(1):31-36, 1988.</p>
<p>Zhu, X., Sobihani, P., and Guo, H. Long short-term memory over recursive structures. In International Conference on Machine Learning, pp. 1604-1612, 2015.</p>
<h1>Supplementary Material</h1>
<h2>A. Tree Decomposition</h2>
<p>Algorithm 2 presents our tree decomposition of molecules. $V_{1}$ and $V_{2}$ contain non-ring bonds and simple rings respectively. Simple rings are extracted via RDKit's GetSymmSSSR function. We then merge rings that share three or more atoms as they form bridged compounds. We note that the junction tree of a molecule is not unique when its cluster graph contains cycles. This introduces additional uncertainty for our probabilistic modeling. To reduce such variation, for any of the three (or more) intersecting bonds, we add their intersecting atom as a cluster and remove the cycle connecting them in the cluster graph. Finally, we construct a junction tree as the maximum spanning tree of a cluster graph $(\mathcal{V}, \mathcal{E})$. Note that we assign an large weight over edges involving clusters in $V_{0}$ to ensure no edges in any cycles will be selected into the junction tree.</p>
<div class="codehilite"><pre><span></span><code><span class="nx">Algorithm</span><span class="w"> </span><span class="mi">2</span><span class="w"> </span><span class="nx">Tree</span><span class="w"> </span><span class="nx">decomposition</span><span class="w"> </span><span class="nx">of</span><span class="w"> </span><span class="nx">molecule</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">G</span><span class="p">=(</span><span class="nx">V</span><span class="p">,</span><span class="w"> </span><span class="nx">E</span><span class="p">)</span><span class="err">\</span><span class="p">)</span>
<span class="w">    </span><span class="err">\</span><span class="p">(</span><span class="nx">V_</span><span class="p">{</span><span class="mi">1</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nx">leftarrow</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">set</span><span class="w"> </span><span class="nx">of</span><span class="w"> </span><span class="nx">bonds</span><span class="w"> </span><span class="err">\</span><span class="p">((</span><span class="nx">u</span><span class="p">,</span><span class="w"> </span><span class="nx">v</span><span class="p">)</span><span class="w"> </span><span class="err">\</span><span class="k">in</span><span class="w"> </span><span class="nx">E</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">that</span><span class="w"> </span><span class="nx">do</span><span class="w"> </span><span class="k">not</span><span class="w"> </span><span class="nx">belong</span><span class="w"> </span><span class="nx">to</span><span class="w"> </span><span class="nx">any</span><span class="w"> </span><span class="nx">rings</span><span class="p">.</span>
<span class="w">    </span><span class="err">\</span><span class="p">(</span><span class="nx">V_</span><span class="p">{</span><span class="mi">2</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nx">leftarrow</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">set</span><span class="w"> </span><span class="nx">of</span><span class="w"> </span><span class="nx">simple</span><span class="w"> </span><span class="nx">rings</span><span class="w"> </span><span class="nx">of</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">G</span><span class="err">\</span><span class="p">).</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">r_</span><span class="p">{</span><span class="mi">1</span><span class="p">},</span><span class="w"> </span><span class="nx">r_</span><span class="p">{</span><span class="mi">2</span><span class="p">}</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">V_</span><span class="p">{</span><span class="mi">2</span><span class="p">}</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">do</span>
<span class="w">        </span><span class="nx">Merge</span><span class="w"> </span><span class="nx">rings</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">r_</span><span class="p">{</span><span class="mi">1</span><span class="p">},</span><span class="w"> </span><span class="nx">r_</span><span class="p">{</span><span class="mi">2</span><span class="p">}</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">into</span><span class="w"> </span><span class="nx">one</span><span class="w"> </span><span class="nx">ring</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="nx">they</span><span class="w"> </span><span class="nx">share</span><span class="w"> </span><span class="nx">more</span><span class="w"> </span><span class="nx">than</span><span class="w"> </span><span class="nx">two</span><span class="w"> </span><span class="nx">atoms</span><span class="w"> </span><span class="p">(</span><span class="nx">bridged</span><span class="w"> </span><span class="nx">rings</span><span class="p">).</span>
<span class="w">    </span><span class="nx">end</span><span class="w"> </span><span class="k">for</span>
<span class="w">    </span><span class="err">\</span><span class="p">(</span><span class="nx">V_</span><span class="p">{</span><span class="mi">0</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nx">leftarrow</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">atoms</span><span class="w"> </span><span class="nx">being</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">intersection</span><span class="w"> </span><span class="nx">of</span><span class="w"> </span><span class="nx">three</span><span class="w"> </span><span class="k">or</span><span class="w"> </span><span class="nx">more</span><span class="w"> </span><span class="nx">clusters</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">V_</span><span class="p">{</span><span class="mi">1</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nx">cup</span><span class="w"> </span><span class="nx">V_</span><span class="p">{</span><span class="mi">2</span><span class="p">}</span><span class="err">\</span><span class="p">).</span>
<span class="w">    </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">mathcal</span><span class="p">{</span><span class="nx">V</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nx">leftarrow</span><span class="w"> </span><span class="nx">V_</span><span class="p">{</span><span class="mi">0</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nx">cup</span><span class="w"> </span><span class="nx">V_</span><span class="p">{</span><span class="mi">1</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nx">cup</span><span class="w"> </span><span class="nx">V_</span><span class="p">{</span><span class="mi">2</span><span class="p">}</span><span class="err">\</span><span class="p">)</span>
<span class="w">    </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">mathcal</span><span class="p">{</span><span class="nx">E</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nx">leftarrow</span><span class="err">\</span><span class="p">{(</span><span class="nx">i</span><span class="p">,</span><span class="w"> </span><span class="nx">j</span><span class="p">,</span><span class="w"> </span><span class="nx">c</span><span class="p">)</span><span class="w"> </span><span class="err">\</span><span class="k">in</span><span class="w"> </span><span class="err">\</span><span class="nx">mathcal</span><span class="p">{</span><span class="nx">V</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nx">times</span><span class="w"> </span><span class="err">\</span><span class="nx">mathcal</span><span class="p">{</span><span class="nx">V</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nx">times</span><span class="w"> </span><span class="err">\</span><span class="nx">mathbb</span><span class="p">{</span><span class="nx">R</span><span class="p">}</span><span class="o">|</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="nx">i</span><span class="w"> </span><span class="err">\</span><span class="nx">cap</span><span class="w"> </span><span class="nx">j</span><span class="w"> </span><span class="err">\</span><span class="nx">mid</span><span class="p">&gt;</span><span class="mi">0</span><span class="err">\</span><span class="p">}</span><span class="err">\</span><span class="p">).</span><span class="w"> </span><span class="nx">Set</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">c</span><span class="p">=</span><span class="err">\</span><span class="nx">infty</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">i</span><span class="w"> </span><span class="err">\</span><span class="k">in</span><span class="w"> </span><span class="nx">V_</span><span class="p">{</span><span class="mi">0</span><span class="p">}</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="k">or</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">j</span><span class="w"> </span><span class="err">\</span><span class="k">in</span><span class="w"> </span><span class="nx">V_</span><span class="p">{</span><span class="mi">0</span><span class="p">}</span><span class="err">\</span><span class="p">),</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">c</span><span class="p">=</span><span class="mi">1</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">otherwise</span><span class="p">.</span>
<span class="w">    </span><span class="nx">Return</span><span class="w"> </span><span class="nx">The</span><span class="w"> </span><span class="nx">maximum</span><span class="w"> </span><span class="nx">spanning</span><span class="w"> </span><span class="nx">tree</span><span class="w"> </span><span class="nx">over</span><span class="w"> </span><span class="nx">cluster</span><span class="w"> </span><span class="nx">graph</span><span class="w"> </span><span class="err">\</span><span class="p">((</span><span class="err">\</span><span class="nx">mathcal</span><span class="p">{</span><span class="nx">V</span><span class="p">},</span><span class="w"> </span><span class="err">\</span><span class="nx">mathcal</span><span class="p">{</span><span class="nx">E</span><span class="p">})</span><span class="err">\</span><span class="p">).</span>
</code></pre></div>

<p><img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 9. Illustration of tree decomposition and sample of cluster label vocabulary.</p>
<h2>B. Stereochemistry</h2>
<p>Though usually presented as two-dimensional graphs, molecules are three-dimensional objects, i.e. molecules are defined not only by its atom types and bond connections, but also the spatial configuration between atoms (chiral atoms and cis-trans isomerism). Stereoisomers are molecules that have the same 2D structure, but differ in the 3D orientations of their atoms in space. We note that stereochemical feasibility could not be simply encoded as context free or attribute grammars.</p>
<p>Empirically, we found it more efficient to predict the stereochemical configuration separately from the molecule generation. Specifically, the JT-VAE first generates the 2D structure of a molecule $m$, following the same procedure described in section 2. Then we generate all its stereoisomers $\mathcal{S}<em m="m">{m}$ using RDKit's EnumerateStereoisomers function, which identifies atoms that could be chiral. For each isomer $m^{\prime} \in \mathcal{S}</em>}$, we encode its graph representation $\mathbf{h<em m_prime="m^{\prime">{m^{\prime}}$ with the graph encoder and compute their cosine similarity $f^{*}\left(m^{\prime}\right)=\cos \left(\mathbf{h}</em>}}, \mathbf{z<em m="m">{m}\right)$ (note that $\mathbf{z}</em>$ is stochastic). We reconstruct the</p>
<p>final 3D structure by picking the stereoisomer $\widehat{m}=\arg \max_{m^{\prime}} f^{s}\left(m^{\prime}\right)$. Since on average only few atoms could have stereochemical variations, this post ranking process is very efficient. Combining this with tree and graph generation, the molecule reconstruction loss $\mathcal{L}$ becomes</p>
<p>$$
\mathcal{L}=\mathcal{L}<em g="g">{c}+\mathcal{L}</em>}+\mathcal{L<em s="s">{s} ; \quad \mathcal{L}</em>\right)\right)
$$}=f^{s}(m)-\log \sum_{m^{\prime} \in \mathcal{S}_{m}} \exp \left(f^{s}\left(m^{\prime</p>
<h1>C. Training Details</h1>
<p>By applying tree decomposition over 240 K molecules in ZINC dataset, we collected our vocabulary set $\mathcal{X}$ of size $|\mathcal{X}|=780$. The hidden state dimension is 450 for all modules in JT-VAE and the latent bottleneck dimension is 56 . For the graph encoder, the initial atom features include its atom type, degree, its formal charge and its chiral configuration. Bond feature is a concatenation of its bond type, whether the bond is in a ring, and its cis-trans configuration. For our tree encoder, we represent each cluster with a neural embedding vector, similar to word embedding for words. The tree and graph decoder use the same feature setting as encoders. The graph encoder and decoder runs three iterations of neural message passing. For fair comparison to SMILES based method, we minimized feature engineering. We use PyTorch to implement all neural components and RDKit to process molecules.</p>
<h2>D. More Experimental Results</h2>
<p>Sampled Molecules Note that a degenerate model could also achieve 100\% prior validity by keep generating simple structures like chains. To prove that our model does not converge to such trivial solutions, we randomly sample and plot 250 molecules from prior distribution $\mathcal{N}(\mathbf{0}, \mathbf{I})$. As shown in Figure 10, our sampled molecules present rich variety and structural complexity. This demonstrates the soundness of the prior validity improvement of our model.</p>
<p>Neighborhood Visualization Given a molecule, we follow Kusner et al. (2017) to construct a grid visualization of its neighborhood. Specifically, we encode a molecule into the latent space and generate two random orthogonal unit vectors as two axis of a grid. Moving in combinations of these directions yields a set of latent vectors and we decode them into corresponding molecules. In Figure 11 and 12, we visualize the local neighborhood of two molecules presented in Dai et al. (2018). Figure 11 visualizes the same molecule in Figure 6, but with wider neighborhood ranges.</p>
<p>Bayesian Optimization We directly used open sourced implementation in Kusner et al. (2017) for Bayesian optimization (BO). Specifically, we train a sparse Gaussian process with 500 inducing points to predict properties of molecules. Five iterations of batch BO with expected improvement heuristic is used to propose new latent vectors. In each iteration, 50 latent vectors are proposed, from which molecules are decoded and added to the training set for next iteration. We perform 10 independent runs and aggregate results. In Figure 13, we present the top 50 molecules found among 10 runs using JT-VAE. Following Kusner et al.'s implementation, the scores reported are normalized to zero mean and unit variance by the mean and variance computed from training set.</p>
<p>Constrained Optimization For this task, a property predictor $F$ is trained jointly with VAE to predict $y(m)=\log P(m)-$ $S A(m)$ from the latent embedding of $m . F$ is a feed-forward network with one hidden layer of dimension 450 followed by $\tanh$ activation. To optimize a molecule $m$, we start with its mean encoding $\mathbf{z}<em m="m">{m}^{0}=\boldsymbol{\mu}</em>}$ and apply 80 gradient ascent steps: $\mathbf{z<em m="m">{m}^{t}=\mathbf{z}</em>\right)&lt;y(m)\right)$. This is caused by inaccurate property prediction. From Figure 14, we can see that tighter similarity constraint forces the model to preserve the original structure.}^{t-1}+\alpha \frac{\partial y}{\partial z}$ with $\alpha=2.0$. 80 molecules are decoded from latent vectors $\left{\mathbf{z}_{m}^{t}\right}$ and their property is calculated. Molecular similarity $\operatorname{sim}\left(m, m^{\prime}\right)$ is calculated via Morgan fingerprint of radius 2 with Tanimoto similarity. For each molecule $m$, we report the best modified molecule $m^{\prime}$ with $\operatorname{sim}\left(m, m^{\prime}\right)&gt;\delta$ for some threshold $\delta$. In Figure 14, we present three groups of modification examples with $\delta=0.2,0.4,0.6$. For each group, we present top three pairs that leads to best improvement $y\left(m^{\prime}\right)-y(m)$ as well as one pair decreased property $\left(y\left(m^{\prime</p>
<h1>Junction Tree Variational Autoencoder for Molecular Graph Generation</h1>
<table>
<thead>
<tr>
<th>1.1.1.1</th>
<th>1.1.1.2</th>
<th>1.1.1.3</th>
<th>1.1.1.4</th>
<th>1.1.1.5</th>
<th>1.1.1.6</th>
<th>1.1.1.7</th>
<th>1.1.1.8</th>
<th>1.1.1.9</th>
<th>1.1.1.10</th>
<th>1.1.1.11</th>
<th>1.1.1.12</th>
<th>1.1.1.13</th>
<th>1.1.1.14</th>
</tr>
</thead>
<tbody>
<tr>
<td>1.1.1.1</td>
<td>1.1.1.2</td>
<td>1.1.1.3</td>
<td>1.1.1.4</td>
<td>1.1.1.5</td>
<td>1.1.1.6</td>
<td>1.1.1.7</td>
<td>1.1.1.8</td>
<td>1.1.1.9</td>
<td>1.1.1.10</td>
<td>1.1.1.11</td>
<td>1.1.1.12</td>
<td>1.1.1.13</td>
<td>1.1.1.14</td>
</tr>
<tr>
<td>1.1.1.1</td>
<td>1.1.1.2</td>
<td>1.1.1.3</td>
<td>1.1.1.4</td>
<td>1.1.1.5</td>
<td>1.1.1.6</td>
<td>1.1.1.7</td>
<td>1.1.1.8</td>
<td>1.1.1.9</td>
<td>1.1.1.10</td>
<td>1.1.1.11</td>
<td>1.1.1.12</td>
<td>1.1.1.13</td>
<td>1.1.1.14</td>
</tr>
<tr>
<td>1.1.1.1</td>
<td>1.1.1.2</td>
<td>1.1.1.3</td>
<td>1.1.1.4</td>
<td>1.1.1.5</td>
<td>1.1.1.6</td>
<td>1.1.1.7</td>
<td>1.1.1.8</td>
<td>1.1.1.9</td>
<td>1.1.1.10</td>
<td>1.1.1.11</td>
<td>1.1.1.12</td>
<td>1.1.1.13</td>
<td>1.1.1.14</td>
</tr>
<tr>
<td>1.1.1.1</td>
<td>1.1.1.2</td>
<td>1.1.1.3</td>
<td>1.1.1.4</td>
<td>1.1.1.5</td>
<td>1.1.1.6</td>
<td>1.1.1.7</td>
<td>1.1.1.8</td>
<td>1.1.1.9</td>
<td>1.1.1.10</td>
<td>1.1.1.11</td>
<td>1.1.1.12</td>
<td>1.1.1.13</td>
<td>1.1.1.14</td>
</tr>
<tr>
<td>1.1.1.1</td>
<td>1.1.1.2</td>
<td>1.1.1.3</td>
<td>1.1.1.4</td>
<td>1.1.1.5</td>
<td>1.1.1.6</td>
<td>1.1.1.7</td>
<td>1.1.1.8</td>
<td>1.1.1.9</td>
<td>1.1.1.10</td>
<td>1.1.1.11</td>
<td>1.1.1.12</td>
<td>1.1.1.13</td>
<td>1.1.1.14</td>
</tr>
<tr>
<td>1.1.1.1</td>
<td>1.1.1.2</td>
<td>1.1.1.3</td>
<td>1.1.1.4</td>
<td>1.1.1.5</td>
<td>1.1.1.6</td>
<td>1.1.1.7</td>
<td>1.1.1.8</td>
<td>1.1.1.9</td>
<td>1.1.1.10</td>
<td>1.1.1.11</td>
<td>1.1.1.12</td>
<td>1.1.1.13</td>
<td>1.1.1.14</td>
</tr>
<tr>
<td>1.1.1.1</td>
<td>1.1.1.2</td>
<td>1.1.1.3</td>
<td>1.1.1.4</td>
<td>1.1.1.5</td>
<td>1.1.1.6</td>
<td>1.1.1.7</td>
<td>1.1.1.8</td>
<td>1.1.1.9</td>
<td>1.1.1.10</td>
<td>1.1.1.11</td>
<td>1.1.1.12</td>
<td>1.1.1.13</td>
<td>1.1.1.14</td>
</tr>
<tr>
<td>1.1.1.1</td>
<td>1.1.1.2</td>
<td>1.1.1.3</td>
<td>1.1.1.4</td>
<td>1.1.1.5</td>
<td>1.1.1.6</td>
<td>1.1.1.7</td>
<td>1.1.1.8</td>
<td>1.1.1.9</td>
<td>1.1.1.10</td>
<td>1.1.1.11</td>
<td>1.1.1.12</td>
<td>1.1.1.13</td>
<td>1.1.1.14</td>
</tr>
<tr>
<td>1.1.1.1</td>
<td>1.1.1.2</td>
<td>1.1.1.3</td>
<td>1.1.1.4</td>
<td>1.1.1.5</td>
<td>1.1.1.6</td>
<td>1.1.1.7</td>
<td>1.1.1.8</td>
<td>1.1.1.9</td>
<td>1.1.1.10</td>
<td>1.1.1.11</td>
<td>1.1.1.12</td>
<td>1.1.1.13</td>
<td>1.1.1.14</td>
</tr>
<tr>
<td>1.1.1.1</td>
<td>1.1.1.2</td>
<td>1.1.1.3</td>
<td>1.1.1.4</td>
<td>1.1.1.5</td>
<td>1.1.1.6</td>
<td>1.1.1.7</td>
<td>1.1.1.8</td>
<td>1.1.1.9</td>
<td>1.1.1.10</td>
<td>1.1.1.11</td>
<td>1.1.1.12</td>
<td>1.1.1.13</td>
<td>1.1.1.14</td>
</tr>
<tr>
<td>1.1.1.1</td>
<td>1.1.1.2</td>
<td>1.1.1.3</td>
<td>1.1.1.4</td>
<td>1.1.1.5</td>
<td>1.1.1.6</td>
<td>1.1.1.7</td>
<td>1.1.1.8</td>
<td>1.1.1.9</td>
<td>1.1.1.10</td>
<td>1.1.1.11</td>
<td>1.1.1.12</td>
<td>1.1.1.13</td>
<td>1.1.1.14</td>
</tr>
<tr>
<td>1.1.1.1</td>
<td>1.1.1.2</td>
<td>1.1.1.3</td>
<td>1.1.1.4</td>
<td>1.1.1.5</td>
<td>1.1.1.6</td>
<td>1.1.1.7</td>
<td>1.1.1.8</td>
<td>1.1.1.9</td>
<td>1.1.1.10</td>
<td>1.1.1.11</td>
<td>1.1.1.12</td>
<td>1.1.1.13</td>
<td>1.1.1.14</td>
</tr>
<tr>
<td>1.1.1.1</td>
<td>1.1.1.2</td>
<td>1.1.1.3</td>
<td>1.1.1.4</td>
<td>1.1.1.5</td>
<td>1.1.1.6</td>
<td>1.1.1.7</td>
<td>1.1.1.8</td>
<td>1.1.1.9</td>
<td>1.1.1.10</td>
<td>1.1.1.11</td>
<td>1.1.1.12</td>
<td>1.1.1.13</td>
<td>1.1.1.14</td>
</tr>
<tr>
<td>1.1.1.1</td>
<td>1.1.1.2</td>
<td>1.1.1.3</td>
<td>1.1.1.4</td>
<td>1.1.1.5</td>
<td>1.1.1.6</td>
<td>1.1.1.7</td>
<td>1.1.1.8</td>
<td>1.1.1.9</td>
<td>1.1.1.10</td>
<td>1.1.1.11</td>
<td>1.1.1.12</td>
<td>1.1.1.13</td>
<td>1.1.1.14</td>
</tr>
<tr>
<td>1.1.1.1</td>
<td>1.1.1.2</td>
<td>1.1.1.3</td>
<td>1.1.1.4</td>
<td>1.1.1.5</td>
<td>1.1.1.6</td>
<td>1.1.1.7</td>
<td>1.1.1.8</td>
<td>1.1.1.9</td>
<td>1.1.1.10</td>
<td>1.1.1.11</td>
<td>1.1.1.12</td>
<td>1.1.1.13</td>
<td>1.1.1.14</td>
</tr>
<tr>
<td>1.1.1.1</td>
<td>1.1.1.2</td>
<td>1.1.1.3</td>
<td>1.1.1.4</td>
<td>1.1.1.5</td>
<td>1.1.1.6</td>
<td>1.1.1.7</td>
<td>1.1.1.8</td>
<td>1.1.1.9</td>
<td>1.1.1.10</td>
<td>1.1.1.11</td>
<td>1.1.1.12</td>
<td>1.1.1.13</td>
<td>1.1.1.14</td>
</tr>
<tr>
<td>1.1.1.1</td>
<td>1.1.1.2</td>
<td>1.1.1.3</td>
<td>1.1.1.4</td>
<td>1.1.1.5</td>
<td>1.1.1.6</td>
<td>1.1.1.7</td>
<td>1.1.1.8</td>
<td>1.1.1.9</td>
<td>1.1.1.10</td>
<td>1.1.1.11</td>
<td>1.1.1.12</td>
<td>1.1.1.13</td>
<td>1.1.1.14</td>
</tr>
<tr>
<td>1.1.1.1</td>
<td>1.1.1.2</td>
<td>1.1.1.3</td>
<td>1.1.1.4</td>
<td>1.1.1.5</td>
<td>1.1.1.6</td>
<td>1.1.1.7</td>
<td>1.1.1.8</td>
<td>1.1.1.9</td>
<td>1.1.1.10</td>
<td>1.1.1.11</td>
<td>1.1.1.12</td>
<td>1.1.1.13</td>
<td>1.1.1.14</td>
</tr>
<tr>
<td>1.1.1.1</td>
<td>1.1.1.2</td>
<td>1.1.1.3</td>
<td>1.1.1.4</td>
<td>1.1.1.5</td>
<td>1.1.1.6</td>
<td>1.1.1.7</td>
<td>1.1.1.8</td>
<td>1.1.1.9</td>
<td>1.1.1.10</td>
<td>1.1.1.11</td>
<td>1.1.1.12</td>
<td>1.1.1.13</td>
<td>1.1.1.14</td>
</tr>
<tr>
<td>1.1.1.1</td>
<td>1.1.1.2</td>
<td>1.1.1.3</td>
<td>1.1.1.4</td>
<td>1.1.1.5</td>
<td>1.1.1.6</td>
<td>1.1.1.7</td>
<td>1.1.1.8</td>
<td>1.1.1.9</td>
<td>1.1.1.10</td>
<td>1.1.1.11</td>
<td>1.1.1.12</td>
<td>1.1.1.13</td>
<td>1.1.1.14</td>
</tr>
<tr>
<td>1.1.1.1</td>
<td>1.1.1.2</td>
<td>1.1.1.3</td>
<td>1.1.1.4</td>
<td>1.1.1.5</td>
<td>1.1.1.6</td>
<td>1.1.1.7</td>
<td>1.1.1.8</td>
<td>1.1.1.9</td>
<td>1.1.1.10</td>
<td>1.1.1.11</td>
<td>1.1.1.12</td>
<td>1.1.1.13</td>
<td>1.1.1.14</td>
</tr>
<tr>
<td>1.1.1.1</td>
<td>1.1.1.2</td>
<td>1.1.1.3</td>
<td>1.1.1.4</td>
<td>1.1.1.5</td>
<td>1.1.1.6</td>
<td>1.1.1.7</td>
<td>1.1.1.8</td>
<td>1.1.1.9</td>
<td>1.1.1.10</td>
<td>1.1.1.11</td>
<td>1.1.1.12</td>
<td>1.1.1.13</td>
<td>1.1.1.14</td>
</tr>
<tr>
<td>1.1.1.1</td>
<td>1.1.1.2</td>
<td>1.1.1.3</td>
<td>1.1.1.4</td>
<td>1.1.1.5</td>
<td>1.1.1.6</td>
<td>1.1.1.7</td>
<td>1.1.1.8</td>
<td>1.1.1.9</td>
<td>1.1.1.10</td>
<td>1.1.1.11</td>
<td>1.1.1.12</td>
<td>1.1.1.13</td>
<td>1.1.1.14</td>
</tr>
<tr>
<td>1.1.1.1</td>
<td>1.1.1.2</td>
<td>1.1.1.3</td>
<td>1.1.1.4</td>
<td>1.1.1.5</td>
<td>1.1.1.6</td>
<td>1.1.1.7</td>
<td>1.1.1.8</td>
<td>1.1.1.9</td>
<td>1.1.1.10</td>
<td>1.1.1.11</td>
<td>1.1.1.12</td>
<td>1.1.1.13</td>
<td>1.1.1.14</td>
</tr>
<tr>
<td>1.1.1.1</td>
<td>1.1.1.2</td>
<td>1.1.1.3</td>
<td>1.1.1.4</td>
<td>1.1.1.5</td>
<td>1.1.1.6</td>
<td>1.1.1.7</td>
<td>1.1.1.8</td>
<td>1.1.1.9</td>
<td>1.1.1.10</td>
<td>1.1.1.11</td>
<td>1.1.1.12</td>
<td>1.1.1.13</td>
<td>1.1.1.14</td>
</tr>
<tr>
<td>1.1.1.1</td>
<td>1.1.1.2</td>
<td>1.1.1.3</td>
<td>1.1.1.4</td>
<td>1.1.1.5</td>
<td>1.1.1.6</td>
<td>1.1.1.7</td>
<td>1.1.1.8</td>
<td>1.1.1.9</td>
<td>1.1.1.10</td>
<td>1.1.1.11</td>
<td>1.1.1.12</td>
<td>1.1.1.13</td>
<td>1.1.1.14</td>
</tr>
<tr>
<td>1.1.1.1</td>
<td>1.1.1.2</td>
<td>1.1.1.3</td>
<td>1.1.1.4</td>
<td>1.1.1.5</td>
<td>1.1.1.6</td>
<td>1.1.1.7</td>
<td>1.1.1.8</td>
<td>1.1.1.9</td>
<td>1.1.1.10</td>
<td>1.1.1.11</td>
<td>1.1.1.12</td>
<td>1.1.1.13</td>
<td>1.1.1.14</td>
</tr>
<tr>
<td>1.1.1.1</td>
<td>1.1.1.2</td>
<td>1.1.1.3</td>
<td>1.1.1.4</td>
<td>1.1.1.5</td>
<td>1.1.1.6</td>
<td>1.1.1.7</td>
<td>1.1.1.8</td>
<td>1.1.1.9</td>
<td>1.1.1.10</td>
<td>1.1.1.11</td>
<td>1.1.1.12</td>
<td>1.1.1.13</td>
<td>1.1.1.14</td>
</tr>
<tr>
<td>1.1.1.1</td>
<td>1.1.1.2</td>
<td>1.1.1.3</td>
<td>1.1.1.4</td>
<td>1.1.1.5</td>
<td>1.1.1.6</td>
<td>1.1.1.7</td>
<td>1.1.1.8</td>
<td>1.1.1.9</td>
<td>1.1.1.10</td>
<td>1.1.1.11</td>
<td>1.1.1.12</td>
<td>1.1.1.13</td>
<td>1.1.1.14</td>
</tr>
<tr>
<td>1.1.1.1</td>
<td>1.1.1.2</td>
<td>1.1.1.3</td>
<td>1.1.1.4</td>
<td>1.1.1.5</td>
<td>1.1.1.6</td>
<td>1.1.1.7</td>
<td>1.1.1.8</td>
<td>1.1.1.9</td>
<td>1.1.1.10</td>
<td>1.1.1.11</td>
<td>1.1.1.12</td>
<td>1.1.1.13</td>
<td>1.1.1.14</td>
</tr>
<tr>
<td>1.1.1.1</td>
<td>1.1.1.2</td>
<td>1.1.1.3</td>
<td>1.1.1.4</td>
<td>1.1.1.5</td>
<td>1.1.1.6</td>
<td>1.1.1.7</td>
<td>1.1.1.8</td>
<td>1.1.1.9</td>
<td>1.1.1.10</td>
<td>1.1.1.11</td>
<td>1.1.1.12</td>
<td>1.1.1.13</td>
<td>1.1.1.14</td>
</tr>
</tbody>
</table>
<h1>Junction Tree Variational Autoencoder for Molecular Graph Generation</h1>
<table>
<thead>
<tr>
<th>1.1.1.1</th>
<th>1.1.1.2</th>
<th>1.1.1.3</th>
<th>1.1.1.4</th>
<th>1.1.1.5</th>
<th>1.1.1.6</th>
<th>1.1.1.7</th>
<th>1.1.1.8</th>
<th>1.1.1.9</th>
<th>1.1.1.10</th>
<th>1.1.1.11</th>
<th>1.1.1.12</th>
<th>1.1.1.13</th>
<th>1.1.1.14</th>
<th>1.1.1.15</th>
<th>1.1.1.16</th>
</tr>
</thead>
<tbody>
<tr>
<td>1.1.1.1</td>
<td>1.1.1.2</td>
<td>1.1.1.3</td>
<td>1.1.1.4</td>
<td>1.1.1.5</td>
<td>1.1.1.6</td>
<td>1.1.1.7</td>
<td>1.1.1.8</td>
<td>1.1.1.9</td>
<td>1.1.1.10</td>
<td>1.1.1.11</td>
<td>1.1.1.12</td>
<td>1.1.1.13</td>
<td>1.1.1.14</td>
<td>1.1.1.15</td>
<td>1.1.1.16</td>
</tr>
<tr>
<td>1.1.1.2</td>
<td>1.1.1.3</td>
<td>1.1.1.4</td>
<td>1.1.1.5</td>
<td>1.1.1.6</td>
<td>1.1.1.7</td>
<td>1.1.1.8</td>
<td>1.1.1.9</td>
<td>1.1.1.10</td>
<td>1.1.1.11</td>
<td>1.1.1.12</td>
<td>1.1.1.13</td>
<td>1.1.1.14</td>
<td>1.1.1.15</td>
<td>1.1.1.16</td>
<td>1.1.1.17</td>
</tr>
<tr>
<td>1.1.1.2</td>
<td>1.1.1.3</td>
<td>1.1.1.4</td>
<td>1.1.1.5</td>
<td>1.1.1.6</td>
<td>1.1.1.7</td>
<td>1.1.1.8</td>
<td>1.1.1.9</td>
<td>1.1.1.10</td>
<td>1.1.1.11</td>
<td>1.1.1.12</td>
<td>1.1.1.13</td>
<td>1.1.1.14</td>
<td>1.1.1.15</td>
<td>1.1.1.16</td>
<td>1.1.1.17</td>
</tr>
<tr>
<td>1.1.1.3</td>
<td>1.1.1.4</td>
<td>1.1.1.5</td>
<td>1.1.1.6</td>
<td>1.1.1.7</td>
<td>1.1.1.8</td>
<td>1.1.1.9</td>
<td>1.1.1.10</td>
<td>1.1.1.11</td>
<td>1.1.1.12</td>
<td>1.1.1.13</td>
<td>1.1.1.14</td>
<td>1.1.1.15</td>
<td>1.1.1.16</td>
<td>1.1.1.17</td>
<td>1.1.1.18</td>
</tr>
<tr>
<td>1.1.1.2</td>
<td>1.1.1.3</td>
<td>1.1.1.4</td>
<td>1.1.1.5</td>
<td>1.1.1.6</td>
<td>1.1.1.7</td>
<td>1.1.1.8</td>
<td>1.1.1.9</td>
<td>1.1.1.10</td>
<td>1.1.1.11</td>
<td>1.1.1.12</td>
<td>1.1.1.13</td>
<td>1.1.1.14</td>
<td>1.1.1.15</td>
<td>1.1.1.16</td>
<td>1.1.1.17</td>
</tr>
<tr>
<td>1.1.1.3</td>
<td>1.1.1.4</td>
<td>1.1.1.5</td>
<td>1.1.1.6</td>
<td>1.1.1.7</td>
<td>1.1.1.8</td>
<td>1.1.1.9</td>
<td>1.1.1.10</td>
<td>1.1.1.11</td>
<td>1.1.1.12</td>
<td>1.1.1.13</td>
<td>1.1.1.14</td>
<td>1.1.1.15</td>
<td>1.1.1.16</td>
<td>1.1.1.17</td>
<td>1.1.1.18</td>
</tr>
<tr>
<td>1.1.1.2</td>
<td>1.1.1.3</td>
<td>1.1.1.4</td>
<td>1.1.1.5</td>
<td>1.1.1.6</td>
<td>1.1.1.7</td>
<td>1.1.1.8</td>
<td>1.1.1.9</td>
<td>1.1.1.10</td>
<td>1.1.1.11</td>
<td>1.1.1.12</td>
<td>1.1.1.13</td>
<td>1.1.1.14</td>
<td>1.1.1.15</td>
<td>1.1.1.16</td>
<td>1.1.1.17</td>
</tr>
<tr>
<td>1.1.1.3</td>
<td>1.1.1.4</td>
<td>1.1.1.5</td>
<td>1.1.1.6</td>
<td>1.1.1.7</td>
<td>1.1.1.8</td>
<td>1.1.1.9</td>
<td>1.1.1.10</td>
<td>1.1.1.11</td>
<td>1.1.1.12</td>
<td>1.1.1.13</td>
<td>1.1.1.14</td>
<td>1.1.1.15</td>
<td>1.1.1.16</td>
<td>1.1.1.17</td>
<td>1.1.1.18</td>
</tr>
<tr>
<td>1.1.1.2</td>
<td>1.1.1.3</td>
<td>1.1.1.4</td>
<td>1.1.1.5</td>
<td>1.1.1.6</td>
<td>1.1.1.7</td>
<td>1.1.1.8</td>
<td>1.1.1.9</td>
<td>1.1.1.10</td>
<td>1.1.1.11</td>
<td>1.1.1.12</td>
<td>1.1.1.13</td>
<td>1.1.1.14</td>
<td>1.1.1.15</td>
<td>1.1.1.16</td>
<td>1.1.1.17</td>
</tr>
<tr>
<td>1.1.1.3</td>
<td>1.1.1.4</td>
<td>1.1.1.5</td>
<td>1.1.1.6</td>
<td>1.1.1.7</td>
<td>1.1.1.8</td>
<td>1.1.1.9</td>
<td>1.1.1.10</td>
<td>1.1.1.11</td>
<td>1.1.1.12</td>
<td>1.1.1.13</td>
<td>1.1.1.14</td>
<td>1.1.1.15</td>
<td>1.1.1.16</td>
<td>1.1.1.17</td>
<td>1.1.1.18</td>
</tr>
<tr>
<td>1.1.1.3</td>
<td>1.1.1.4</td>
<td>1.1.1.5</td>
<td>1.1.1.6</td>
<td>1.1.1.7</td>
<td>1.1.1.8</td>
<td>1.1.1.9</td>
<td>1.1.1.10</td>
<td>1.1.1.11</td>
<td>1.1.1.12</td>
<td>1.1.1.13</td>
<td>1.1.1.14</td>
<td>1.1.1.15</td>
<td>1.1.1.16</td>
<td>1.1.1.17</td>
<td>1.1.1.18</td>
</tr>
<tr>
<td>1.1.1.3</td>
<td>1.1.1.4</td>
<td>1.1.1.5</td>
<td>1.1.1.6</td>
<td>1.1.1.7</td>
<td>1.1.1.8</td>
<td>1.1.1.9</td>
<td>1.1.1.10</td>
<td>1.1.1.11</td>
<td>1.1.1.12</td>
<td>1.1.1.13</td>
<td>1.1.1.14</td>
<td>1.1.1.15</td>
<td>1.1.1.16</td>
<td>1.1.1.17</td>
<td>1.1.1.18</td>
</tr>
<tr>
<td>1.1.1.3</td>
<td>1.1.1.4</td>
<td>1.1.1.5</td>
<td>1.1.1.6</td>
<td>1.1.1.7</td>
<td>1.1.1.8</td>
<td>1.1.1.9</td>
<td>1.1.1.10</td>
<td>1.1.1.11</td>
<td>1.1.1.12</td>
<td>1.1.1.13</td>
<td>1.1.1.14</td>
<td>1.1.1.15</td>
<td>1.1.1.16</td>
<td>1.1.1.17</td>
<td>1.1.1.18</td>
</tr>
<tr>
<td>1.1.1.3</td>
<td>1.1.1.4</td>
<td>1.1.1.5</td>
<td>1.1.1.6</td>
<td>1.1.1.7</td>
<td>1.1.1.8</td>
<td>1.1.1.9</td>
<td>1.1.1.10</td>
<td>1.1.1.11</td>
<td>1.1.1.12</td>
<td>1.1.1.13</td>
<td>1.1.1.14</td>
<td>1.1.1.15</td>
<td>1.1.1.16</td>
<td>1.1.1.17</td>
<td>1.1.1.18</td>
</tr>
<tr>
<td>1.1.1.3</td>
<td>1.1.1.4</td>
<td>1.1.1.5</td>
<td>1.1.1.6</td>
<td>1.1.1.7</td>
<td>1.1.1.8</td>
<td>1.1.1.9</td>
<td>1.1.1.10</td>
<td>1.1.1.11</td>
<td>1.1.1.12</td>
<td>1.1.1.13</td>
<td>1.1.1.14</td>
<td>1.1.1.15</td>
<td>1.1.1.16</td>
<td>1.1.1.17</td>
<td>1.1.1.18</td>
</tr>
<tr>
<td>1.1.1.3</td>
<td>1.1.1.4</td>
<td>1.1.1.5</td>
<td>1.1.1.6</td>
<td>1.1.1.7</td>
<td>1.1.1.8</td>
<td>1.1.1.9</td>
<td>1.1.1.10</td>
<td>1.1.1.11</td>
<td>1.1.1.12</td>
<td>1.1.1.13</td>
<td>1.1.1.14</td>
<td>1.1.1.15</td>
<td>1.1.1.16</td>
<td>1.1.1.17</td>
<td>1.1.1.18</td>
</tr>
<tr>
<td>1.1.1.3</td>
<td>1.1.1.4</td>
<td>1.1.1.5</td>
<td>1.1.1.6</td>
<td>1.1.1.7</td>
<td>1.1.1.8</td>
<td>1.1.1.9</td>
<td>1.1.1.10</td>
<td>1.1.1.11</td>
<td>1.1.1.12</td>
<td>1.1.1.13</td>
<td>1.1.1.14</td>
<td>1.1.1.15</td>
<td>1.1.1.16</td>
<td>1.1.1.17</td>
<td>1.1.1.18</td>
</tr>
<tr>
<td>1.1.1.3</td>
<td>1.1.1.4</td>
<td>1.1.1.5</td>
<td>1.1.1.6</td>
<td>1.1.1.7</td>
<td>1.1.1.8</td>
<td>1.1.1.9</td>
<td>1.1.1.10</td>
<td>1.1.1.11</td>
<td>1.1.1.12</td>
<td>1.1.1.13</td>
<td>1.1.1.14</td>
<td>1.1.1.15</td>
<td>1.1.1.16</td>
<td>1.1.1.17</td>
<td>1.1.1.18</td>
</tr>
<tr>
<td>1.1.1.3</td>
<td>1.1.1.4</td>
<td>1.1.1.5</td>
<td>1.1.1.6</td>
<td>1.1.1.7</td>
<td>1.1.1.8</td>
<td>1.1.1.9</td>
<td>1.1.1.10</td>
<td>1.1.1.11</td>
<td>1.1.1.12</td>
<td>1.1.1.13</td>
<td>1.1.1.14</td>
<td>1.1.1.15</td>
<td>1.1.1.16</td>
<td>1.1.1.17</td>
<td>1.1.1.18</td>
</tr>
<tr>
<td>1.1.1.3</td>
<td>1.1.1.4</td>
<td>1.1.1.5</td>
<td>1.1.1.6</td>
<td>1.1.1.7</td>
<td>1.1.1.8</td>
<td>1.1.1.9</td>
<td>1.1.1.10</td>
<td>1.1.1.11</td>
<td>1.1.1.12</td>
<td>1.1.1.13</td>
<td>1.1.1.14</td>
<td>1.1.1.15</td>
<td>1.1.1.16</td>
<td>1.1.1.17</td>
<td>1.1.1.18</td>
</tr>
<tr>
<td>1.1.1.3</td>
<td>1.1.1.4</td>
<td>1.1.1.5</td>
<td>1.1.1.6</td>
<td>1.1.1.7</td>
<td>1.1.1.8</td>
<td>1.1.1.9</td>
<td>1.1.1.10</td>
<td>1.1.1.11</td>
<td>1.1.1.12</td>
<td>1.1.1.13</td>
<td>1.1.1.14</td>
<td>1.1.1.15</td>
<td>1.1.1.16</td>
<td>1.1.1.17</td>
<td>1.1.1.18</td>
</tr>
<tr>
<td>1.1.1.3</td>
<td>1.1.1.4</td>
<td>1.1.1.5</td>
<td>1.1.1.6</td>
<td>1.1.1.7</td>
<td>1.1.1.8</td>
<td>1.1.1.9</td>
<td>1.1.1.10</td>
<td>1.1.1.11</td>
<td>1.1.1.12</td>
<td>1.1.1.13</td>
<td>1.1.1.14</td>
<td>1.1.1.15</td>
<td>1.1.1.16</td>
<td>1.1.1.17</td>
<td>1.1.1.18</td>
</tr>
<tr>
<td>1.1.1.3</td>
<td>1.1.1.4</td>
<td>1.1.1.5</td>
<td>1.1.1.6</td>
<td>1.1.1.7</td>
<td>1.1.1.8</td>
<td>1.1.1.9</td>
<td>1.1.1.10</td>
<td>1.1.1.11</td>
<td>1.1.1.12</td>
<td>1.1.1.13</td>
<td>1.1.1.14</td>
<td>1.1.1.15</td>
<td>1.1.1.16</td>
<td>1.1.1.17</td>
<td>1.1.1.18</td>
</tr>
<tr>
<td>1.1.1.3</td>
<td>1.1.1.4</td>
<td>1.1.1.5</td>
<td>1.1.1.6</td>
<td>1.1.1.7</td>
<td>1.1.1.8</td>
<td>1.1.1.9</td>
<td>1.1.1.10</td>
<td>1.1.1.11</td>
<td>1.1.1.12</td>
<td>1.1.1.13</td>
<td>1.1.1.14</td>
<td>1.1.1.15</td>
<td>1.1.1.16</td>
<td>1.1.1.17</td>
<td>1.1.1.18</td>
</tr>
<tr>
<td>1.1.1.3</td>
<td>1.1.1.4</td>
<td>1.1.1.5</td>
<td>1.1.1.6</td>
<td>1.1.1.7</td>
<td>1.1.1.8</td>
<td>1.1.1.9</td>
<td>1.1.1.10</td>
<td>1.1.1.11</td>
<td>1.1.1.12</td>
<td>1.1.1.13</td>
<td>1.1.1.14</td>
<td>1.1.1.15</td>
<td>1.1.1.16</td>
<td>1.1.1.17</td>
<td>1.1.1.18</td>
</tr>
<tr>
<td>1.1.1.3</td>
<td>1.1.1.4</td>
<td>1.1.1.5</td>
<td>1.1.1.6</td>
<td>1.1.1.7</td>
<td>1.1.1.8</td>
<td>1.1.1.9</td>
<td>1.1.1.10</td>
<td>1.1.1.11</td>
<td>1.1.1.12</td>
<td>1.1.1.13</td>
<td>1.1.1.14</td>
<td>1.1.1.15</td>
<td>1.1.1.16</td>
<td>1.1.1.17</td>
<td>1.1.1.18</td>
</tr>
<tr>
<td>1.1.1.3</td>
<td>1.1.1.4</td>
<td>1.1.1.5</td>
<td>1.1.1.6</td>
<td>1.1.1.7</td>
<td>1.1.1.8</td>
<td>1.1.1.9</td>
<td>1.1.1.10</td>
<td>1.1.1.11</td>
<td>1.1.1.12</td>
<td>1.1.1.13</td>
<td>1.1.1.14</td>
<td>1.1.1.15</td>
<td>1.1.1.16</td>
<td>1.1.1.17</td>
<td>1.1.1.18</td>
</tr>
<tr>
<td>1.1.1.3</td>
<td>1.1.1.4</td>
<td>1.1.1.5</td>
<td>1.1.1.6</td>
<td>1.1.1.7</td>
<td>1.1.1.8</td>
<td>1.1.1.9</td>
<td>1.1.1.10</td>
<td>1.1.1.11</td>
<td>1.1.1.12</td>
<td>1.1.1.13</td>
<td>1.1.1.14</td>
<td>1.1.1.15</td>
<td>1.1.1.16</td>
<td>1.1.1.17</td>
<td>1.1.1.18</td>
</tr>
<tr>
<td>1.1.1.3</td>
<td>1.1.1.4</td>
<td>1.1.1.5</td>
<td>1.1.1.6</td>
<td>1.1.1.7</td>
<td>1.1.1.8</td>
<td>1.1.1.9</td>
<td>1.1.1.10</td>
<td>1.1.1.11</td>
<td>1.1.1.12</td>
<td>1.1.1.13</td>
<td>1.1.1.14</td>
<td>1.1.1.15</td>
<td>1.1.1.16</td>
<td>1.1.1.17</td>
<td>1.1.1.18</td>
</tr>
<tr>
<td>1.1.1.3</td>
<td>1.1.1.4</td>
<td>1.1.1.5</td>
<td>1.1.1.6</td>
<td>1.1.1.7</td>
<td>1.1.1.8</td>
<td>1.1.1.9</td>
<td>1.1.1.10</td>
<td>1.1.1.11</td>
<td>1.1.1.12</td>
<td>1.1.1.13</td>
<td>1.1.1.14</td>
<td>1.1.1.15</td>
<td>1.1.1.16</td>
<td>1.1.1.17</td>
<td>1.1.1.18</td>
</tr>
<tr>
<td>1.1.1.3</td>
<td>1.1.1.4</td>
<td>1.1.1.5</td>
<td>1.1.1.6</td>
<td>1.1.1.7</td>
<td>1.1.1.8</td>
<td>1.1.1.9</td>
<td>1.1.1.10</td>
<td>1.1.1.11</td>
<td>1.1.1.12</td>
<td>1.1.1.13</td>
<td>1.1.1.14</td>
<td>1.1.1.15</td>
<td>1.1.1.16</td>
<td>1.1.1.17</td>
<td>1.1.1.18</td>
</tr>
<tr>
<td>1.1.1.3</td>
<td>1.1.1.4</td>
<td>1.1.1.5</td>
<td>1.1.1.6</td>
<td>1.1.1.7</td>
<td>1.1.1.8</td>
<td>1.1.1.9</td>
<td>1.1.1.10</td>
<td>1.1.1.11</td>
<td>1.1.1.12</td>
<td>1.1.1.13</td>
<td>1.1.1.14</td>
<td>1.1.1.15</td>
<td>1.1.1.16</td>
<td>1.1.1.17</td>
<td>1.1.1.18</td>
</tr>
<tr>
<td>1.1.1.3</td>
<td>1.1.1.4</td>
<td>1.1.1.5</td>
<td>1.1.1.6</td>
<td>1.1.1.7</td>
<td>1.1.1.8</td>
<td>1.1.1.9</td>
<td>1.1.1.10</td>
<td>1.1.1.11</td>
<td>1.1.1.12</td>
<td>1.1.1.13</td>
<td>1.1.1.14</td>
<td>1.1.1.15</td>
<td>1.1.1.16</td>
<td>1.1.1.17</td>
<td>1.1.1.18</td>
</tr>
<tr>
<td>1.1.1.3</td>
<td>1.1.1.4</td>
<td>1.1.1.5</td>
<td>1.1.1.6</td>
<td>1.1.1.7</td>
<td>1.1.1.8</td>
<td>1.1.1.9</td>
<td>1.1.1.10</td>
<td>1.1.1.11</td>
<td>1.1.1.12</td>
<td>1.1.1.13</td>
<td>1.1.1.14</td>
<td>1.1.1.15</td>
<td>1.1.1.16</td>
<td>1.1.1.17</td>
<td>1.1.1.18</td>
</tr>
<tr>
<td>1.1.1.3</td>
<td>1.1.1.4</td>
<td>1.1.1.5</td>
<td>1.1.1.6</td>
<td>1.1.1.7</td>
<td>1.1.1.8</td>
<td>1.1.1.9</td>
<td>1.1.1.10</td>
<td>1.1.1.11</td>
<td>1.1.1.12</td>
<td>1.1.1.13</td>
<td>1.1.1.14</td>
<td>1.1.1.15</td>
<td>1.1.1.16</td>
<td>1.1.1.17</td>
<td>1.1.1.18</td>
</tr>
<tr>
<td>1.1.1.3</td>
<td>1.1.1.4</td>
<td>1.1.1.5</td>
<td>1.1.1.6</td>
<td>1.1.1.7</td>
<td>1.1.1.8</td>
<td>1.1.1.9</td>
<td>1.1.1.10</td>
<td>1.1.1.11</td>
<td>1.1.1.12</td>
<td>1.1.1.13</td>
<td>1.1.1.14</td>
<td>1.1.1.15</td>
<td>1.1.1.16</td>
<td>1.1.1.17</td>
<td>1.1.1.18</td>
</tr>
<tr>
<td>1.1.1.3</td>
<td>1.1.1.4</td>
<td>1.1.1.5</td>
<td>1.1.1.6</td>
<td>1.1.1.7</td>
<td>1.1.1.8</td>
<td>1.1.1.9</td>
<td>1.1.1.10</td>
<td>1.1.1.11</td>
<td>1.1.1.12</td>
<td>1.1.1.13</td>
<td>1.1.1.14</td>
<td>1.1.1.15</td>
<td>1.1.1.16</td>
<td>1.1.1.17</td>
<td>1.1.1.18</td>
</tr>
<tr>
<td>1.1.1.3</td>
<td>1.1.1.4</td>
<td>1.1.1.5</td>
<td>1.1.1.6</td>
<td>1.1.1.7</td>
<td>1.1.1.8</td>
<td>1.1.1.9</td>
<td>1.1.1.10</td>
<td>1.1.1.11</td>
<td>1.1.1.12</td>
<td>1.1.1.13</td>
<td>1.1.1.14</td>
<td>1.1.1.15</td>
<td>1.1.1.16</td>
<td>1.1.1.17</td>
<td>1.1.1.18</td>
</tr>
<tr>
<td>1.1.1.3</td>
<td>1.1.1.4</td>
<td>1.1.1.5</td>
<td>1.1.1.6</td>
<td>1.1.1.7</td>
<td>1.1.1.8</td>
<td>1.1.1.9</td>
<td>1.1.1.10</td>
<td>1.1.1.11</td>
<td>1.1.1.12</td>
<td>1.1.1.13</td>
<td>1.1.1.14</td>
<td>1.1.1.15</td>
<td>1.1.1.16</td>
<td>1.1.1.17</td>
<td>1.1.1.18</td>
</tr>
<tr>
<td>1.1.1.3</td>
<td>1.1.1.4</td>
<td>1.1.1.5</td>
<td>1.1.1.6</td>
<td>1.1.1.7</td>
<td>1.1.1.8</td>
<td>1.1.1.9</td>
<td>1.1.1.10</td>
<td>1.1.1.11</td>
<td>1.1.1.12</td>
<td>1.1.1.13</td>
<td>1.1.1.14</td>
<td>1.1.1.15</td>
<td>1.1.1.16</td>
<td>1.1.1.17</td>
<td>1.1.1.18</td>
</tr>
</tbody>
</table>
<p><img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>Figure 12. Neighborhood visualization of molecule COc1cc(OC)cc([C@H]2CC<a href="CCC(F)(F)F">NH+</a>C2)c1.</p>
<p><img alt="img-10.jpeg" src="img-10.jpeg" /></p>
<p>Figure 13. Top 50 molecules found by Bayesian optimization using JT-VAE.</p>
<p><img alt="img-11.jpeg" src="img-11.jpeg" /></p>
<p>Figure 14. Row 1-3: Molecule modification results with similarity constraint $\operatorname{sim}\left(m, m^{\prime}\right) \geq 0.2,0.4,0.6$. For each group, we plot the top three pairs that leads to actual property improvement, and one pair with decreased property. We can see that tighter similarity constraint forces the model to preserve the original structure.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{2}$ The node ordering is not unique as the order within sibling nodes is ambiguous. In this paper we train our model with one ordering and leave this issue for future work.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{1}$ https://github.com/wengong-jin/icml18-jtnn&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8188 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8188</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8188</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-150.html">extraction-schema-150</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <p><strong>Paper ID:</strong> paper-262824840</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2309.14365v1.pdf" target="_blank">An In-depth Survey of Large Language Model-based Artificial Intelligence Agents</a></p>
                <p><strong>Paper Abstract:</strong> Due to the powerful capabilities demonstrated by large language model (LLM), there has been a recent surge in efforts to integrate them with AI agents to enhance their performance. In this paper, we have explored the core differences and characteristics between LLM-based AI agents and traditional AI agents. Specifically, we first compare the fundamental characteristics of these two types of agents, clarifying the significant advantages of LLM-based agents in handling natural language, knowledge storage, and reasoning capabilities. Subsequently, we conducted an in-depth analysis of the key components of AI agents, including planning, memory, and tool use. Particularly, for the crucial component of memory, this paper introduced an innovative classification scheme, not only departing from traditional classification methods but also providing a fresh perspective on the design of an AI agent's memory system. We firmly believe that in-depth research and understanding of these core components will lay a solid foundation for the future advancement of AI agent technology. At the end of the paper, we provide directional suggestions for further research in this field, with the hope of offering valuable insights to scholars and researchers in the field.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8188.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8188.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Reflexion</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reflexion: an autonomous agent with dynamic memory and self-reflection</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A framework that generates actions via an Actor module, evaluates outcomes, produces self-reflective feedback, and stores that feedback in an external memory to inform future decisions and error correction.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Reflexion: an autonomous agent with dynamic memory and self-reflection</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Reflexion</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Two-module agent: Actor (generates actions) and Self-reflection (produces feedback and stores it), designed to iteratively improve behavior by analyzing past failures and storing corrective knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>General interactive environment tasks / error correction</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Agents act in an environment, receive feedback about failed or successful actions, and must use stored feedback to avoid repeating errors and improve future planning/execution.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>interactive sequential decision / lifelong improvement</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>long-term / episodic-like feedback memory</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism</strong></td>
                            <td>Explicit external memory where generated feedback summaries are written and later consulted; memory acts as stored corrective summaries appended back to prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Self-reflection feedback (natural-language summaries of failures and corrective suggestions).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>Retrieval via prompt-augmentation (feedback stored is later appended to actor prompts) / dynamic reading when deciding next action.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>Survey describes that Reflexion uses feedback stored in memory to infer error-causing actions and correct them iteratively; no quantitative ablations reported in the survey.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Storing self-generated feedback in memory enables continuous improvement by allowing the agent to recall past mistakes and corrective plans, reducing repeated errors over time.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Survey notes mechanism but does not report quantitative gains; challenges include what to store, retrieval selection, and integration into actor prompts.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8188.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8188.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Voyager</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Voyager: An open-ended embodied agent with large language models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LLM-powered embodied agent that plays Minecraft using an LLM (GPT-4 access) together with a learned skill library to perform open-ended tasks and demonstrate in-context lifelong learning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Voyager: An open-ended embodied agent with large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Voyager</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Embodied agent that uses an LLM for planning and a learned skill library (a form of stored skills/memory) enabling reuse of previously learned behaviors to bootstrap new tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4 (access mentioned)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large pre-trained language model used for planning and task decomposition; survey notes Voyager has access to GPT-4.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Minecraft open-ended gameplay</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Open-ended embodied environment where the agent must explore, acquire resources, learn skills, and solve new tasks from scratch without human intervention.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>embodied open-ended exploration / lifelong learning</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>skill library (long-term episodic/semantic mix) and in-context episodic memories</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism</strong></td>
                            <td>Learned skill library that stores reusable procedures/skills; skills are invoked during planning and execution; in-context examples used as short-term working memory.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Stored skills (procedures), previous successful action traces, in-context exemplars.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>Selection of relevant skills from the skill library when decomposing/planning new tasks (skill lookup and in-context prompting).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>Survey states Voyager demonstrates stronger generalization and better data efficiency than traditional RL agents due to pre-trained knowledge and skill reuse; no numeric ablation reported in the survey.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>A learned skill library plus LLM planning enables few-shot lifelong learning in an embodied environment, improving generalization and data efficiency relative to RL-from-scratch baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Survey does not report quantitative memory ablations; open issues include how to best index, consolidate, and retrieve skills and how to avoid storing noisy or irrelevant skills.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8188.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8188.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Generative Agents</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generative Agents: Interactive simulacra of human behavior</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multi-agent system where each agent logs complete records of experiences in natural language which are periodically synthesized into higher-level reflections and dynamically retrieved to plan future actions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Generative agents: Interactive simulacra of human behavior</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Generative Agents</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Multiple LLM-based agents that maintain long-form natural language memories of experiences; memories are summarized and reflected upon to produce plans and simulate human-like behavior in a town simulation.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Social simulation / interactive multi-agent town</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Agents interact in a simulated town, recall past experiences, form higher-level reflections, and use retrieved memories to guide interpersonal behavior and decisions.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>multi-agent simulation / behavior planning</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>long-term episodic memory with synthesized reflections</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism</strong></td>
                            <td>Natural-language memory logs of experiences; periodic synthesis into higher-level reflections; dynamic retrieval of relevant memories during planning.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Raw experience entries (natural language), synthesized reflections and higher-level summaries.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>Dynamic retrieval of synthesized memories to inform planning and behavior (semantic selection from memory store).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>Survey highlights the architecture and that memory synthesis and dynamic retrieval are used for planning; no quantitative ablations reported.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Storing natural-language episodic experiences and synthesizing them into higher-level reflections enables richer, temporally-consistent agent behavior and planning in social simulations.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Open challenges include scalable memory indexing, deciding what to store/summarize, retrieval relevance, and evaluation of memory quality over long timescales.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8188.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8188.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MemoryBank</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MemoryBank: Enhancing large language models with long-term memory</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A proposed memory system for LLM-based agents that provides mechanisms for storing, retrieving, and updating long-term memories, including a forgetting/updating strategy based on human memory models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Memorybank: Enhancing large language models with long-term memory</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>MemoryBank (Memorybank)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A long-term memory augmentation for LLMs that manages stored memories and includes mechanisms for updating/forgetting, inspired by human forgetting curves.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>General long-term memory augmentation for LLM tasks</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Provide persistent, retrievable long-term facts or interaction histories to an LLM to improve future responses and decision-making.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>retrieval-augmented generation / memory-augmented reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>long-term external memory with forgetting mechanism</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism</strong></td>
                            <td>External memory store with update/forgetting policy (survey notes an Ebbinghaus-curve-inspired forgetting mechanism).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Summaries or structured memory entries derived from past interactions.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>Similarity-based selection of relevant memory entries for inclusion in prompts (semantic retrieval implied).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>Survey reports the design of a forgetting/updating mechanism but does not report numerical ablations in this survey.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Long-term memory with biologically-inspired forgetting/updating mechanisms is a promising direction to keep agent memories relevant and prevent unbounded growth.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Choosing appropriate forgetting schedules, storage formats, and retrieval criteria remains open; survey provides conceptual description without quantitative evaluation.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8188.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8188.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Liang Self-Controlled Memory</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Unleashing infinite-length input capacity for large-scale language models with self-controlled memory system</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A self-controlled memory system designed to extend LLM input capacity by managing a flash memory for recent interactions and an action memory for long-term storage, enabling effectively infinite-length context.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Unleashing infinite-length input capacity for large-scale language models with self-controlled memory system</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Self-controlled memory system (Liang et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Memory architecture splitting storage into fast-access flash memory for recent interactions and a larger action memory for long-term storage, with mechanisms to control what enters working context.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Extending context / long-horizon tasks</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Allow LLM-based agents to reason over and utilize information spanning far beyond the native context window by fetching relevant chunks from managed external memory.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>long-context retrieval / retrieval-augmented reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>hybrid (flash short-term + long-term action memory)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism</strong></td>
                            <td>Two-tier storage: recent interaction stored in flash memory for fast retrieval; other interactions moved to action memory for long-term storage; a controller selects what to bring into prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Recent interaction data in flash memory and longer-term action/history records in action memory.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>Recency prioritized for flash memory; retrieval controller selects relevant long-term records for prompt augmentation (implied semantic/recency hybrid).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>Survey describes architectural idea but does not present numeric ablations; claims allow effectively infinite input capacity via controlled retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Separating fast-access and long-term stores with a controller enables LLMs to leverage far more context than their base window, improving ability to handle long-horizon tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Designing the controller, determining retention policies, and ensuring retrieval relevance/scalability are open engineering challenges; no quantitative comparisons provided in survey.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8188.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8188.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Zhang et al. tabular memory</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A referenced approach that stores memory in a tabular key-value format where observations/states are keys and actions plus Q-values are stored as values to assist decision-making.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Tabular key-value memory (Zhang et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Memory design storing state/observation keys with action and Q-value values to allow retrieval of previously learned action-value mappings to guide LLM decisions.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Decision-making / reinforcement-like tasks</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Assist LLM decision-making by consulting a stored mapping of states to actions and their estimated values (Q-values) to select actions.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>policy augmentation / decision support</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>external structured key-value long-term memory</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism</strong></td>
                            <td>Tabular key-value store where similarity between current observation and stored keys is computed and top-k similar records are retrieved.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Observations/states as keys; actions and Q-values as values.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>Similarity computation between current input and stored keys; top-k selection of records to assist decision making.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>Survey notes the approach but does not provide ablation results; method intended to give LLMs access to learned action-value information.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Structured key-value memory enables recalling action-value pairs to guide decisions, bridging classic RL representations and LLM planning.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Scaling tabular memory to large state spaces, defining similarity metrics, and integrating Q-values into LLM prompting remain practical challenges.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8188.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e8188.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BlenderBot 3 memory example</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BlenderBot 3: a deployed conversational agent that continually learns to responsibly engage</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A conversational agent that generates summaries of recent interactions and conditionally stores them as persona memory to personalize future conversations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Blenderbot 3: a deployed conversational agent that continually learns to responsibly engage</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>BlenderBot 3 (memory persona summaries)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Chatbot that summarizes the last interaction; if the summary contains persona information it is stored in long-term memory to personalize future dialogues.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Open-domain conversational personalization</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Maintain and use a persona/interaction memory so the chatbot can recall user-specific facts or preferences across conversations.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>conversational memory / personalization</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>long-term persona memory</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism</strong></td>
                            <td>Generate summary of last interaction; conditional storage (store unless 'no persona'); retrieved later to personalize responses.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Natural-language persona summaries derived from past interactions.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>Concatenate relevant persona memory entries into prompt when generating replies (prompt-augmentation).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>Survey mentions the conditional-storage design but does not report quantitative ablations.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Simple summarization plus conditional storage is an effective design pattern to capture persistent user persona information for personalization.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Deciding what constitutes ‘persona’ vs transient information, privacy concerns, and memory growth control are open issues.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Reflexion: an autonomous agent with dynamic memory and self-reflection <em>(Rating: 2)</em></li>
                <li>Voyager: An open-ended embodied agent with large language models <em>(Rating: 2)</em></li>
                <li>Generative agents: Interactive simulacra of human behavior <em>(Rating: 2)</em></li>
                <li>Memorybank: Enhancing large language models with long-term memory <em>(Rating: 2)</em></li>
                <li>Unleashing infinite-length input capacity for large-scale language models with self-controlled memory system <em>(Rating: 2)</em></li>
                <li>Blenderbot 3: a deployed conversational agent that continually learns to responsibly engage <em>(Rating: 1)</em></li>
                <li>Think before you act: Decision transformers with internal working memory <em>(Rating: 1)</em></li>
                <li>Large language model is semi-parametric reinforcement learning agent <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8188",
    "paper_id": "paper-262824840",
    "extraction_schema_id": "extraction-schema-150",
    "extracted_data": [
        {
            "name_short": "Reflexion",
            "name_full": "Reflexion: an autonomous agent with dynamic memory and self-reflection",
            "brief_description": "A framework that generates actions via an Actor module, evaluates outcomes, produces self-reflective feedback, and stores that feedback in an external memory to inform future decisions and error correction.",
            "citation_title": "Reflexion: an autonomous agent with dynamic memory and self-reflection",
            "mention_or_use": "mention",
            "agent_name": "Reflexion",
            "agent_description": "Two-module agent: Actor (generates actions) and Self-reflection (produces feedback and stores it), designed to iteratively improve behavior by analyzing past failures and storing corrective knowledge.",
            "model_name": "",
            "model_description": "",
            "task_name": "General interactive environment tasks / error correction",
            "task_description": "Agents act in an environment, receive feedback about failed or successful actions, and must use stored feedback to avoid repeating errors and improve future planning/execution.",
            "task_type": "interactive sequential decision / lifelong improvement",
            "memory_used": true,
            "memory_type": "long-term / episodic-like feedback memory",
            "memory_mechanism": "Explicit external memory where generated feedback summaries are written and later consulted; memory acts as stored corrective summaries appended back to prompts.",
            "memory_representation": "Self-reflection feedback (natural-language summaries of failures and corrective suggestions).",
            "memory_retrieval_method": "Retrieval via prompt-augmentation (feedback stored is later appended to actor prompts) / dynamic reading when deciding next action.",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_with_without_memory": false,
            "ablation_or_comparison": "Survey describes that Reflexion uses feedback stored in memory to infer error-causing actions and correct them iteratively; no quantitative ablations reported in the survey.",
            "key_findings": "Storing self-generated feedback in memory enables continuous improvement by allowing the agent to recall past mistakes and corrective plans, reducing repeated errors over time.",
            "limitations_or_challenges": "Survey notes mechanism but does not report quantitative gains; challenges include what to store, retrieval selection, and integration into actor prompts.",
            "uuid": "e8188.0"
        },
        {
            "name_short": "Voyager",
            "name_full": "Voyager: An open-ended embodied agent with large language models",
            "brief_description": "An LLM-powered embodied agent that plays Minecraft using an LLM (GPT-4 access) together with a learned skill library to perform open-ended tasks and demonstrate in-context lifelong learning.",
            "citation_title": "Voyager: An open-ended embodied agent with large language models",
            "mention_or_use": "mention",
            "agent_name": "Voyager",
            "agent_description": "Embodied agent that uses an LLM for planning and a learned skill library (a form of stored skills/memory) enabling reuse of previously learned behaviors to bootstrap new tasks.",
            "model_name": "GPT-4 (access mentioned)",
            "model_description": "Large pre-trained language model used for planning and task decomposition; survey notes Voyager has access to GPT-4.",
            "task_name": "Minecraft open-ended gameplay",
            "task_description": "Open-ended embodied environment where the agent must explore, acquire resources, learn skills, and solve new tasks from scratch without human intervention.",
            "task_type": "embodied open-ended exploration / lifelong learning",
            "memory_used": true,
            "memory_type": "skill library (long-term episodic/semantic mix) and in-context episodic memories",
            "memory_mechanism": "Learned skill library that stores reusable procedures/skills; skills are invoked during planning and execution; in-context examples used as short-term working memory.",
            "memory_representation": "Stored skills (procedures), previous successful action traces, in-context exemplars.",
            "memory_retrieval_method": "Selection of relevant skills from the skill library when decomposing/planning new tasks (skill lookup and in-context prompting).",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_with_without_memory": false,
            "ablation_or_comparison": "Survey states Voyager demonstrates stronger generalization and better data efficiency than traditional RL agents due to pre-trained knowledge and skill reuse; no numeric ablation reported in the survey.",
            "key_findings": "A learned skill library plus LLM planning enables few-shot lifelong learning in an embodied environment, improving generalization and data efficiency relative to RL-from-scratch baselines.",
            "limitations_or_challenges": "Survey does not report quantitative memory ablations; open issues include how to best index, consolidate, and retrieve skills and how to avoid storing noisy or irrelevant skills.",
            "uuid": "e8188.1"
        },
        {
            "name_short": "Generative Agents",
            "name_full": "Generative Agents: Interactive simulacra of human behavior",
            "brief_description": "A multi-agent system where each agent logs complete records of experiences in natural language which are periodically synthesized into higher-level reflections and dynamically retrieved to plan future actions.",
            "citation_title": "Generative agents: Interactive simulacra of human behavior",
            "mention_or_use": "mention",
            "agent_name": "Generative Agents",
            "agent_description": "Multiple LLM-based agents that maintain long-form natural language memories of experiences; memories are summarized and reflected upon to produce plans and simulate human-like behavior in a town simulation.",
            "model_name": "",
            "model_description": "",
            "task_name": "Social simulation / interactive multi-agent town",
            "task_description": "Agents interact in a simulated town, recall past experiences, form higher-level reflections, and use retrieved memories to guide interpersonal behavior and decisions.",
            "task_type": "multi-agent simulation / behavior planning",
            "memory_used": true,
            "memory_type": "long-term episodic memory with synthesized reflections",
            "memory_mechanism": "Natural-language memory logs of experiences; periodic synthesis into higher-level reflections; dynamic retrieval of relevant memories during planning.",
            "memory_representation": "Raw experience entries (natural language), synthesized reflections and higher-level summaries.",
            "memory_retrieval_method": "Dynamic retrieval of synthesized memories to inform planning and behavior (semantic selection from memory store).",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_with_without_memory": false,
            "ablation_or_comparison": "Survey highlights the architecture and that memory synthesis and dynamic retrieval are used for planning; no quantitative ablations reported.",
            "key_findings": "Storing natural-language episodic experiences and synthesizing them into higher-level reflections enables richer, temporally-consistent agent behavior and planning in social simulations.",
            "limitations_or_challenges": "Open challenges include scalable memory indexing, deciding what to store/summarize, retrieval relevance, and evaluation of memory quality over long timescales.",
            "uuid": "e8188.2"
        },
        {
            "name_short": "MemoryBank",
            "name_full": "MemoryBank: Enhancing large language models with long-term memory",
            "brief_description": "A proposed memory system for LLM-based agents that provides mechanisms for storing, retrieving, and updating long-term memories, including a forgetting/updating strategy based on human memory models.",
            "citation_title": "Memorybank: Enhancing large language models with long-term memory",
            "mention_or_use": "mention",
            "agent_name": "MemoryBank (Memorybank)",
            "agent_description": "A long-term memory augmentation for LLMs that manages stored memories and includes mechanisms for updating/forgetting, inspired by human forgetting curves.",
            "model_name": "",
            "model_description": "",
            "task_name": "General long-term memory augmentation for LLM tasks",
            "task_description": "Provide persistent, retrievable long-term facts or interaction histories to an LLM to improve future responses and decision-making.",
            "task_type": "retrieval-augmented generation / memory-augmented reasoning",
            "memory_used": true,
            "memory_type": "long-term external memory with forgetting mechanism",
            "memory_mechanism": "External memory store with update/forgetting policy (survey notes an Ebbinghaus-curve-inspired forgetting mechanism).",
            "memory_representation": "Summaries or structured memory entries derived from past interactions.",
            "memory_retrieval_method": "Similarity-based selection of relevant memory entries for inclusion in prompts (semantic retrieval implied).",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_with_without_memory": false,
            "ablation_or_comparison": "Survey reports the design of a forgetting/updating mechanism but does not report numerical ablations in this survey.",
            "key_findings": "Long-term memory with biologically-inspired forgetting/updating mechanisms is a promising direction to keep agent memories relevant and prevent unbounded growth.",
            "limitations_or_challenges": "Choosing appropriate forgetting schedules, storage formats, and retrieval criteria remains open; survey provides conceptual description without quantitative evaluation.",
            "uuid": "e8188.3"
        },
        {
            "name_short": "Liang Self-Controlled Memory",
            "name_full": "Unleashing infinite-length input capacity for large-scale language models with self-controlled memory system",
            "brief_description": "A self-controlled memory system designed to extend LLM input capacity by managing a flash memory for recent interactions and an action memory for long-term storage, enabling effectively infinite-length context.",
            "citation_title": "Unleashing infinite-length input capacity for large-scale language models with self-controlled memory system",
            "mention_or_use": "mention",
            "agent_name": "Self-controlled memory system (Liang et al.)",
            "agent_description": "Memory architecture splitting storage into fast-access flash memory for recent interactions and a larger action memory for long-term storage, with mechanisms to control what enters working context.",
            "model_name": "",
            "model_description": "",
            "task_name": "Extending context / long-horizon tasks",
            "task_description": "Allow LLM-based agents to reason over and utilize information spanning far beyond the native context window by fetching relevant chunks from managed external memory.",
            "task_type": "long-context retrieval / retrieval-augmented reasoning",
            "memory_used": true,
            "memory_type": "hybrid (flash short-term + long-term action memory)",
            "memory_mechanism": "Two-tier storage: recent interaction stored in flash memory for fast retrieval; other interactions moved to action memory for long-term storage; a controller selects what to bring into prompt.",
            "memory_representation": "Recent interaction data in flash memory and longer-term action/history records in action memory.",
            "memory_retrieval_method": "Recency prioritized for flash memory; retrieval controller selects relevant long-term records for prompt augmentation (implied semantic/recency hybrid).",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_with_without_memory": false,
            "ablation_or_comparison": "Survey describes architectural idea but does not present numeric ablations; claims allow effectively infinite input capacity via controlled retrieval.",
            "key_findings": "Separating fast-access and long-term stores with a controller enables LLMs to leverage far more context than their base window, improving ability to handle long-horizon tasks.",
            "limitations_or_challenges": "Designing the controller, determining retention policies, and ensuring retrieval relevance/scalability are open engineering challenges; no quantitative comparisons provided in survey.",
            "uuid": "e8188.4"
        },
        {
            "name_short": "Zhang et al. tabular memory",
            "name_full": "",
            "brief_description": "A referenced approach that stores memory in a tabular key-value format where observations/states are keys and actions plus Q-values are stored as values to assist decision-making.",
            "citation_title": "",
            "mention_or_use": "mention",
            "agent_name": "Tabular key-value memory (Zhang et al.)",
            "agent_description": "Memory design storing state/observation keys with action and Q-value values to allow retrieval of previously learned action-value mappings to guide LLM decisions.",
            "model_name": "",
            "model_description": "",
            "task_name": "Decision-making / reinforcement-like tasks",
            "task_description": "Assist LLM decision-making by consulting a stored mapping of states to actions and their estimated values (Q-values) to select actions.",
            "task_type": "policy augmentation / decision support",
            "memory_used": true,
            "memory_type": "external structured key-value long-term memory",
            "memory_mechanism": "Tabular key-value store where similarity between current observation and stored keys is computed and top-k similar records are retrieved.",
            "memory_representation": "Observations/states as keys; actions and Q-values as values.",
            "memory_retrieval_method": "Similarity computation between current input and stored keys; top-k selection of records to assist decision making.",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_with_without_memory": false,
            "ablation_or_comparison": "Survey notes the approach but does not provide ablation results; method intended to give LLMs access to learned action-value information.",
            "key_findings": "Structured key-value memory enables recalling action-value pairs to guide decisions, bridging classic RL representations and LLM planning.",
            "limitations_or_challenges": "Scaling tabular memory to large state spaces, defining similarity metrics, and integrating Q-values into LLM prompting remain practical challenges.",
            "uuid": "e8188.5"
        },
        {
            "name_short": "BlenderBot 3 memory example",
            "name_full": "BlenderBot 3: a deployed conversational agent that continually learns to responsibly engage",
            "brief_description": "A conversational agent that generates summaries of recent interactions and conditionally stores them as persona memory to personalize future conversations.",
            "citation_title": "Blenderbot 3: a deployed conversational agent that continually learns to responsibly engage",
            "mention_or_use": "mention",
            "agent_name": "BlenderBot 3 (memory persona summaries)",
            "agent_description": "Chatbot that summarizes the last interaction; if the summary contains persona information it is stored in long-term memory to personalize future dialogues.",
            "model_name": "",
            "model_description": "",
            "task_name": "Open-domain conversational personalization",
            "task_description": "Maintain and use a persona/interaction memory so the chatbot can recall user-specific facts or preferences across conversations.",
            "task_type": "conversational memory / personalization",
            "memory_used": true,
            "memory_type": "long-term persona memory",
            "memory_mechanism": "Generate summary of last interaction; conditional storage (store unless 'no persona'); retrieved later to personalize responses.",
            "memory_representation": "Natural-language persona summaries derived from past interactions.",
            "memory_retrieval_method": "Concatenate relevant persona memory entries into prompt when generating replies (prompt-augmentation).",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_with_without_memory": false,
            "ablation_or_comparison": "Survey mentions the conditional-storage design but does not report quantitative ablations.",
            "key_findings": "Simple summarization plus conditional storage is an effective design pattern to capture persistent user persona information for personalization.",
            "limitations_or_challenges": "Deciding what constitutes ‘persona’ vs transient information, privacy concerns, and memory growth control are open issues.",
            "uuid": "e8188.6"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Reflexion: an autonomous agent with dynamic memory and self-reflection",
            "rating": 2,
            "sanitized_title": "reflexion_an_autonomous_agent_with_dynamic_memory_and_selfreflection"
        },
        {
            "paper_title": "Voyager: An open-ended embodied agent with large language models",
            "rating": 2,
            "sanitized_title": "voyager_an_openended_embodied_agent_with_large_language_models"
        },
        {
            "paper_title": "Generative agents: Interactive simulacra of human behavior",
            "rating": 2,
            "sanitized_title": "generative_agents_interactive_simulacra_of_human_behavior"
        },
        {
            "paper_title": "Memorybank: Enhancing large language models with long-term memory",
            "rating": 2,
            "sanitized_title": "memorybank_enhancing_large_language_models_with_longterm_memory"
        },
        {
            "paper_title": "Unleashing infinite-length input capacity for large-scale language models with self-controlled memory system",
            "rating": 2,
            "sanitized_title": "unleashing_infinitelength_input_capacity_for_largescale_language_models_with_selfcontrolled_memory_system"
        },
        {
            "paper_title": "Blenderbot 3: a deployed conversational agent that continually learns to responsibly engage",
            "rating": 1,
            "sanitized_title": "blenderbot_3_a_deployed_conversational_agent_that_continually_learns_to_responsibly_engage"
        },
        {
            "paper_title": "Think before you act: Decision transformers with internal working memory",
            "rating": 1,
            "sanitized_title": "think_before_you_act_decision_transformers_with_internal_working_memory"
        },
        {
            "paper_title": "Large language model is semi-parametric reinforcement learning agent",
            "rating": 1,
            "sanitized_title": "large_language_model_is_semiparametric_reinforcement_learning_agent"
        }
    ],
    "cost": 0.01668625,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>An In-depth Survey of Large Language Model-based Artificial Intelligence Agents
23 Sep 2023</p>
<p>Pengyu Zhao pengyuzhao@bjtu.edu.cn 
Beijing Jiaotong University
New York University</p>
<p>Zijian Jin 
Beijing Jiaotong University
New York University</p>
<p>Ning Cheng ningcheng@bjtu.edu.cn 
Beijing Jiaotong University
New York University</p>
<p>An In-depth Survey of Large Language Model-based Artificial Intelligence Agents
23 Sep 20237EA9BAAAF9B4FC0050740732B7E8544EarXiv:2309.14365v1[cs.CL]AI agentsSurveyLarge language model
Due to the powerful capabilities demonstrated by large language model (LLM), there has been a recent surge in efforts to integrate them with AI agents to enhance their performance.In this paper, we have explored the core differences and characteristics between LLM-based AI agents and traditional AI agents.Specifically, we first compare the fundamental characteristics of these two types of agents, clarifying the significant advantages of LLM-based agents in handling natural language, knowledge storage, and reasoning capabilities.Subsequently, we conducted an in-depth analysis of the key components of AI agents, including planning, memory, and tool use.Particularly, for the crucial component of memory, this paper introduced an innovative classification scheme, not only departing from traditional classification methods but also providing a fresh perspective on the design of an AI agent's memory system.We firmly believe that in-depth research and understanding of these core components will lay a solid foundation for the future advancement of AI agent technology.At the end of the paper, we provide directional suggestions for further research in this field, with the hope of offering valuable insights to scholars and researchers in the field.</p>
<p>Introduction</p>
<p>The notion of intelligent agents can trace its roots back to the research of the mid to late 20th century.Pioneering contributions in this realm encompass Hewitt's Actor model (Hewitt et al., 1973) and Minsky's innovative conceptualization in the 'Society of Mind' (Minsky, 1988) which still trigger some new ideas recently eg: "Mindstorms in Natural Language-Based Societies of Mind" (Zhuge and et al., 2023).In the 1990s, Russell introduced the framework for intelligent and rational agents (Russell and Norvig, 2010), which has since become a foundational theory in this field.The advent of deep neural networks post-2012 marked a significant shift in the AI landscape.Leveraging the power of backpropagation (Rumelhart et al., 1986) for training deep models, researchers began to explore more sophisticated agent behaviors, transcending beyond traditional rule-based methods.Among the emergent methodologies, Reinforcement Learning (RL) stood out as a paradigm where agents learn optimal behavior through interactions with the environment and receiving feedback in the form of rewards or penalties.In 2013, DeepMind (Mnih et al., 2013) used RL to play the Atair Game and win humans' performance which indicates that AI Agents are available to outperform human capabilities in specific areas.The incorporation of neural networks into RL, often referred to as Deep Reinforcement Learning (DRL) (Li, 2017), allowed for the tackling of previously in- * Equal contribution.</p>
<p>tractable problems, bridging the gap between highdimensional input spaces and complex decisionmaking processes (Arulkumaran et al., 2017).Despite the promising advancements offered by DRL, certain challenges persist.Chief among these is the issue of generalization.Many reinforcement learning agents, especially those trained in simulated environments, struggle to transfer their learned behavior to new or slightly altered scenarios, often termed as domain adaptation (Arndt et al., 2020).Training these agents can also be computationally intensive, often requiring vast amounts of interactions to achieve satisfactory performance.Furthermore, Reinforcement learning training struggles with convergence and the design of reward functions can be challenging, particularly in realworld scenarios, and can be a daunting and often unfeasible task.This hampers the rapid development and deployment of RL-based agents in diverse environments.</p>
<p>In 2020, OpenAI released GPT3 (Brown et al., 2020) with 175 billion parameters, making it the largest publicly available language model at the time.These models, characterized by their immense size and capacity, have shown exceptional prowess in generalization across a myriad of tasks.The ability of LLMs to understand and generate language allows them to act as a foundational model for a wide range of applications (Huang and Chang, 2022).Their inherent generalization capabilities make them ideal candidates to serve as base models for universal agents.By harness-ing the vast knowledge embedded within LLMs, researchers are now exploring hybrid models, integrating the strengths of reinforcement learning with the generalization capacities of LLMs (Hu et al., 2023).This symbiotic combination promises to pave the way for more robust, adaptable, and efficient intelligent agents in the future.In order to assist readers in quickly understanding the research history of AI agents and to further inspire research in AI agents, in this paper, we offer a comprehensive and systematic review of AI agents based on the components1 and applications.</p>
<p>LLM vs. Traditional Agents</p>
<p>Traditional agents were designed specifically to address certain problems.They primarily relied on predetermined algorithms or rule sets, excelling in tasks they were built for.However, they often struggled with generalization and reasoning when confronted with tasks outside their initial scope.The introduction of Large Language Models (LLMs) has brought significant changes to AI agent design.These agents, trained on the extensive corpus, are not only proficient in understanding and generating natural language but also display strong generalization abilities.This capability allows them to easily integrate with various tools, enhancing their versatility.On the other hand, the emergent abilities of Large Language Models (Wei et al., 2022a) shows that LLMs are also good at reasoning which can help them learn from fault behavior.Taking game exploration as an example, especially in the Minecraft setting, the differences between LLM-based agents like VOYAGER (Wang et al., 2023a) and traditional RL agents are evident.LLM agents, with their rich pre-trained knowledge, have an advantage in decision-making strategies even without task-specific training.On the other hand, traditional RL agents often need to start from scratch in new environments, relying heavily on interaction to learn.In this scenario, VOYAGER showcases better generalization and data efficiency.</p>
<p>Components of AI Agents</p>
<p>Overview</p>
<p>The LLM-powered AI agent system relies on LLM to function as its brain, which is supported by several crucial components that deploy various important functions.These functions, including planning, memory, and tool use, have been studied independently and thoughtfully in the past and have a well-established history.In this survey, we will introduce the research history of each individual functional model, mainstream methods, combination methods with the AI agent, and potential directions for the future.We hope that this historical information will serve as an inspiration for the future development of AI agents.It is worth noting that the integration of these three functional models is still a relatively new concept.</p>
<p>Planning</p>
<p>The goal of planning is to design a series of actions to facilitate state transitions and ultimately achieve the desired task.As shown in the left of Figure 1, this component, functioning as an individual module, has been integrated in various applications, such as robot manipulations (Chen et al., 2021), robot navigation (Lo et al., 2018), and service robots (Li and Ding, 2023).And the existing works, such as methods using the planning domain description language (PDDL) (Aeronautiques et al., 1998;Fox and Long, 2003;Jiang et al., 2019) and hierarchical planning frameworks (Erol et al., 1994;Suárez-Hernández et al., 2018;Guo et al., 2023), have greatly propelled the advancement of planning systems.Recently, with significant successes achieved by LLMs in various domains, numerous studies have been exploring the utilization of LLMs to enhance the planning and execution capabilities of AI agents.Benefiting from the powerful inference capabilities of LLM, LLM-based AI agents can efficiently decompose complex tasks or instructions into a series of subtasks or simpler instructions (i.e., planning).For instance, as shown in the top right of Figure 1, the LLM-based agent decomposes the complex instruction "Put the banana on the counter" into a series of simpler instructions which are easier for the agent to accomplish.Further, taking actions solely based on the initial plan formulated by the agent without considering external environmental feedback may limit the performance of the agent.For example, as shown in the bottom right of Figure 1, an agent creates a plan for the instruction "Put the bat on the bed", and the first step in the initial planning is "Pick up the baseball bat", which may fail to execute when there is no 'bat' nearby.However, if the agent can self-reflection based on the feedback, it can refine the first step to "Walk to the side of the baseball bat", and then progressively work towards achieving the goal.Therefore, during the execution process, reflecting on and analyzing past behaviors and feedback, and subsequently adjusting the plan, are equally pivotal for the successful execution of tasks by AI agents.Next, we will introduce relevant works that utilize LLM for task decomposition and self-reflection.</p>
<p>Put the banana on the counter</p>
<p>Step 2: Go to the counter.</p>
<p>Step 3: Put down the banana.</p>
<p>Put a bat on the bed</p>
<p>Step 1: Walk to the side of the baseball bat.</p>
<p>Step 2: Pick up the baseball bat.</p>
<p>Step 3: Walk to the bed.</p>
<p>Step 4: Lean the bat on bed.</p>
<p>Step 1: Pick up the baseball bat.</p>
<p>Refinement planning</p>
<p>Step 1: Pick up the banana.</p>
<p>Robot manipulations</p>
<p>Applications</p>
<p>Task Decomposition</p>
<p>Task decomposition aims to decompose the complex task or instruction into a series of simpler subgoals or sub-instructions for performing the task.For example, as shown in the top right of Figure 1, given a task instruction "Put the banana on the counter", the agent will split it into three steps: 1. Pick up the banana.2. Go to the counter.3. Put down the banana.The existing works mainly perform task decomposition by chain or tree of thought (Wei et al., 2022b;Kojima et al., 2022;Yao et al., 2023a) and PDDL with LLM (Liu et al., 2023a).Chain of thought can utilize a few examples or simple instructions to progressively guide LLM reasoning, in order to decompose complex tasks into a series of simpler tasks (Wei et al., 2022b;Zhang et al., 2022;Huang et al., 2022a;Wang et al., 2023b).Zhang et al. (Zhang et al., 2022) proposed a method for automatically generating chain of thought samples.They first clustered the problems and then, for each cluster, selected representative questions to generate chain of thought samples in a zero-shot manner.Huang et al. (Huang et al., 2022a) utilized high-level tasks related to the given task and their decomposed planning steps as examples, and combined these examples with input information to construct prompts.Then, they employed LLM to predict the next steps of planning and added the generated steps to the original prompts, continuing the prediction until the entire task was completed.Wang et al. (Wang et al., 2023b) proposed that by guiding LLM to first construct a series of plans and then progressively execute solutions, it can effectively alleviate the issue of intermediate plans disappearing during the reasoning process.Unlike linear thinking, the Tree of Thought (Long, 2023;Yao et al., 2023a) generates multiple branches of thoughts at each step to create a tree-like structure.Subsequently, searching on this tree of thought is conducted using methods like breadth-first search or depth-first search.</p>
<p>For evaluating each state, reasoning can be facilitated using a "value prompt" or assessment results can be generated through a voting mechanism.In addition, some research efforts consider combining LLM with PDDL for the purpose of planning target problems (Xie et al., 2023;Liu et al., 2023a;Guan et al., 2023).For example, Liu et al. (Liu et al., 2023a) first conveyed the task description in the form of natural language to LLM for translating to PDDL format by in-context learning, then they employed the classical planners to generate plans and converted them into natural language format by LLM again.</p>
<p>Self-Reflection</p>
<p>During the process of interacting with the environment, AI agents can enhance their planning ability by reflecting on past actions by receiving feedback.</p>
<p>There are many works attempt to combine LLMbased agents with the self-reflection (Yao et al., 2022;Huang et al., 2022b;Shinn et al., 2023;Liu et al., 2023b;Sun et al., 2023;Singh et al., 2023;Yao et al., 2023b;Chen and Chang, 2023).For example, Yao et al. (Yao et al., 2022) integrated actions with the chain of thought, leveraging thought to formulate planning that guides the agent's execution of acts.Simultaneously, interactive execution of actions in the environment further enhances the agent's planning ability.Shinn et al. (Shinn et al., 2023) introduced a framework named Reflexion, in which the approach first generates actions through the Actor module and evaluates them.</p>
<p>Then utilizes the self-reflection module to gener-ate feedback and store it in memory.When errors occur, this method can infer the actions that led to the errors and correct them, thereby continuously enhancing the agent's capabilities.Liu et al. (Liu et al., 2023b) first rated the various outputs of the model based on human feedback, then they used prompt templates to construct these ratings into natural language forms and combined them with the outputs for fine-tuning the model, thereby enabling it to learn self-reflection.Singh et al. (Singh et al., 2023) utilize Pythonic program and annotations to generate planning, wherein assertion functions are used to obtain feedback from the environment.When assertions are false, error recovery can be performed.Sun et al. (Sun et al., 2023) proposed a model named AdaPlanner, which utilizes two refiners to optimize and refine plans.One of the refiners collects information from the environment after executing an action, which is then utilized for subsequent actions.The other one adjusts the existing plan based on feedback obtained from the external environment when the executed action fails to achieve its intended outcome.Similarly, Yao et al (Yao et al., 2023b).first finetuned a small language model as a retrospective model to generate feedback for past failures, and then appended this feedback to the actor prompt as input of the large LLM for preventing the recurrence of similar errors and predicting the next action.</p>
<p>Memory</p>
<p>Memory can help individuals integrate past learned knowledge and experience events with their current state, thereby assisting in making more appropriate decisions.In general, human memory can be categorized into three primary types: sensory memory, short-term memory, and long-term memory (Camina and Güell, 2017).Sensory memory is the collection of information through the senses of touch, hearing, vision, and other senses, and it has an extremely brief lifespan (Wan et al., 2020;Jung et al., 2019).Short-term memory refers to the process of handling information within a brief period, and it is typically carried out by working memory (Hunter, 1957;Baddeley, 1983Baddeley, , 1997).In contrast, long-term memory refers to memories that can be stored for an extended period, which encompasses episodic memory and semantic memory.Episodic memory refers to the memory capacity for events that individuals have personally experienced, and it is often able to closely associate these events with contextual information (Tulving et al., 1972;Tulving, 1983).Semantic memory refers to the factual knowledge that individuals know, and this type of memory is unrelated to specific events and personal experiences (Tulving et al., 1972).Similarly, memory, as a key component of AI agents, can assist them in learning valuable knowl-edge from past information, thereby helping the agents perform tasks more effectively.To fully utilize the stored information in memory, some research has attempted to integrate AI agents with short-term memory (Kang et al., 2023;Peng et al., 2023), long-term memory (Vere and Bickmore, 1990;Kazemifard et al., 2014), and a combination of both (Nuxoll and Laird, 2007;Kim et al., 2023;Yao et al., 2023b;Shinn et al., 2023).In addition, since sensory memory can be regarded as the embedded representation of inputs such as text and images, similar to a sensory buffer, we consider sensory memory not to be part of the memory module of the AI agent.Short-term memory refers to the temporary information that AI agents process during task execution, such as the example information involved in the in-context learning process and the intermediate results generated during LLM inference.During the inference process, LLM temporarily stores and processes in-context information or intermediate results, using them to improve the ability of the model.This is similar to human working memory, which temporarily holds and processes information in the short-term to support complex cognitive tasks (Gong et al.).Some works utilize in-context learning to improve the performance of LLM.They first combine some examples with input information to construct a prompt and then send this prompt to LLM to utilize short-term memory (Li et al., 2023b;Logeswaran et al., 2022;Omidvar and An, 2023).For example, Li et al. (Li et al., 2023b) pointed out that when provided with a context that is relevant to the task, it is important to ensure that its working memory is controlled by the context.Otherwise, the model should rely on the world knowledge obtained during the pre-</p>
<p>Memory</p>
<p>Intelligent Agent with LLM</p>
<p>Input Embedding</p>
<p>Training Memory</p>
<p>Stored through model parameters.</p>
<p>Short-term Memory</p>
<p>Temporary information that LLM process during task execution</p>
<p>Long-term Memory</p>
<p>Stored in an external storage system</p>
<p>The knowledge and facts that LLM learns during the pre-training process.</p>
<p>Human's Memory Long-term memory refers to the information stored in an external storage system, and when AI agents use this memory, they can retrieve information relevant to the current context from the external storage.The utilization of long-term memory can be divided into three steps: information storage, in-formation retrieval, and information updating.Information storage aims to store essential information from the interactions between the agent and its environment.For example, Shuster et al. (Shuster et al., 2022) first generated a summary of the last interaction.If the generated summary is "no persona," it is not stored; otherwise, the summary information is stored in long-term memory.Zhang et al. (Zhang et al., 2023b) utilized a tabular format to store memory in the form of key-value pairs.In this format, the observations and states serve as the keys, and the actions and their corresponding Q-values are stored as values.Liang et al. (Liang et al., 2023a) stored the relevant information from the interactions between the agent and the environment.The information from the last interaction is stored in the flash memory for quick retrieval.The rest of the information is stored in the action memory as long-term memory.Information retrieval aims to retrieve information relevant to the current context from long-term memory to assist the agent in performing tasks.For example, Lee et al. (Lee et al., 2023) first clarified the input information, then they employed dense passage retrievers to select relevant information from long-term memory.Afterward, they combined the selected information with the input information and used methods like chain-of-thought or few-shot learning to choose the most relevant information for task execution.Zhang et al. (Zhang et al., 2023b) first computed the similarity between the received information and the keys stored in the long-term memory, and then selected the top k records with the highest similarity to assist the LLM's decisionmaking.Information updating aims to update the stored long-term memory.For example, Zhong et al. (Zhong et al., 2023) designed a forgetting mechanism based on the Ebbinghaus forgetting curve to simulate the updating process of human long-term memory.</p>
<p>Tool Use</p>
<p>Recent works have greatly propelled the development of LLMs, however, LLMs still fail to achieve satisfactory performance in certain scenarios involving up-to-date information, computational reasoning, and others.For example, when a user asks, 'Where is the global premiere of Oppenheimer?',ChatGPT is unable to answer this question because the movie 'Oppenheimer' is the latest information and is not included in the training corpus of the LLM.</p>
<p>To bridge these gaps, many efforts have been dedicated to integrating LLM with external tools to extend its capabilities.Some works aim to integrate LLM with specific tools such as web search (Nakano et al., 2021), translation (Thoppilan et al., 2022), calculators (Cobbe et al., 2021), and some plugins of ChatGPT 2 .Some other works consider teaching LLMs to choose suitable tools or combine various tools to accomplish tasks.For example, Karpas et al. (Karpas et al., 2022) implemented a system named MRKL, which mainly consists of a language model, an adapter, and multiple experts (e.g., model or tools), where the adapter is utilized to select the appropriate expert to assist the language model in processing input requests.Parisi et al. (Parisi et al., 2022) designed an iterative self-play algorithm to assist LM in learning how to utilize external APIs by fine-tuning LM.In self-play, they first fine-tuned LM with a few samples and then utilized it to generate the tool input for invoking the tool API to generate results, followed by an LM to infer an answer.If the referred answer is similar to the golden answer, the task input and predicted results (i.e., tool input, tool result, and predicted answer) are appended to the corpus sets for further fine-tuning and iteration in the next round.Patil et al. (Patil et al., 2023) first constructed a dataset with the format of instruct-API pairs, and then fine-tuned LLM based on the dataset for aiding LLM to employ tools with zero-shot and retriever-aware.Similarly, Schick et al. (Schick et al., 2023) fine-tuned the LLM on a dataset containing API calls to help the LLM learn the ability to invoke APIs.Paranjape et al. (Paranjape et al., 2023) first retrieved the related examples with the input task as a prompt and then employed the LLM to implement infer-2 https://openai.com/blog/chatgpt-pluginsence with chain reasoning.In this process, if the immediate step requires tools, the inference process is paused to execute the tools, and the output of the tools is inserted into the inference process.Li et al. (Li et al., 2023c) proposed the API bank to evaluate the LLM's ability to utilize tools and devised a tool-augmented LLM paradigm to alleviate the limitation of in-context length.Shen et al. (Shen et al., 2023) proposed a method to combine LLM with HuggingFace to enhance the performance of LLM.Specifically, the method first employs LLM to decompose complex tasks into a series of subtasks and then sequentially selects suitable models from HuggingFace to perform these sub-tasks.Lu et al. (Lu et al., 2023) designed a plug-and-play compositional reasoning method, which first plans the schedule of input tasks and then composes multiple tools to execute sub-tasks for achieving the original task.Liang et al. (Liang et al., 2023b) first applied a multi-model foundation model to understand and plan the given instructions for selecting suitable APIs from the API platform, and then utilized an action executor to generate results based on the selected APIs.Besides, they also exploited the feedback of humans to optimize the ability of planning and choose APIs of LLM, and the document of API in API platform.Different from the above approaches, Cai et al. (Cai et al., 2023) first employed an LLM to generate tool for input task, and then utilized an LLM to perform task based on the generated tool.Specifically, for an incoming task, if the tool required by the task has been generated, the tool will be invoked directly, otherwise, the LLM will first generates tool, and then uses it.An LLM chemistry agent designed to accomplish tasks across organic synthesis, drug discovery, and materials design Agent (Boiko et al., 2023) An intelligent agent system that combines multiple large language models for autonomous design, planning, and execution of scientific experiments</p>
<p>Application</p>
<p>Collaboration</p>
<p>DialOp (Lin et al., 2023a) AI assistants collaborating with one or more humans via natural language to help them make complex decisions MindOS An engine creating autonomous AI agents for users' professional tasks MetaGPT An multi-agent framework assigning different roles to GPTs to form a collaborative software entity for complex tasks Multi-GPT An experimental multi-agent system where multiple "expertG-PTs" collaborate to perform a task and each has their own short and long-term memory and the ability to communicate with each other.Generative Agents (Park et al., 2023) Multiple AI agents for the interactive simulacra of human behavior</p>
<p>General purpose</p>
<p>Auto-GPT An AI agent chaining LLM "thoughts" together to autonomously achieve whatever goal users set BabyAGI An task-driven autonomous agent leveraging GPT-4 language model, Pinecone vector search, and the LangChain framework to perform a wide range of tasks across diverse domains SuperAGI A developer-centric open-source framework to build, manage and run useful Autonomous AI Agents AgentGPT A framework allow users to configure and deploy Autonomous AI agents rapidly</p>
<p>Table 1: LLM-based AI Agent applications.</p>
<p>research, coding, collaboration, and general purpose, as shown in Tab. 1.</p>
<p>Chatbot</p>
<p>Pi3 is a typical LLM-based chatting AI agent released by Inflection.Like ChatGPT4 and Claude5 , users can talk directly with Pi, but Pi not only serves productivity needs such as searching or answering questions but also focuses on emotional companionship.Pi is known for its high emotional intelligence.Users can communicate with Pi as naturally as they would with a close friend.</p>
<p>Game</p>
<p>No other LLM-based gaming intelligence has recently received more attention than Voyager (Wang et al., 2023a).Voyager is an AI agent with access to GPT-4 (OpenAI, 2023).Voyager shows remarkable proficiency in playing the game of Minecraft and is able to utilize a learned skill library to solve new tasks from scratch without human intervention, demonstrating strong in-context lifelong learning capabilities.</p>
<p>Coding</p>
<p>Developers have always wanted to have a code generator to help improve programming efficiency.</p>
<p>LLM-based agents are naturally used in code generation.A very attractive coding agent is GPT Engineer6 , which can generate an entire codebase according to a prompt.GPT Engineer even learns the developer's coding style and lets the developer finish the coding project in just a few minutes.What makes GPT Engineer unique is that GPT Engineer asks many detailed questions to allow developers to clarify missing details instead of accepting these requests unconditionally made by developers.</p>
<p>Design</p>
<p>The idea of AI Agent has also been applied to design.Diagram</p>
<p>Research</p>
<p>A number of AI agents for autonomous scientific research have emerged.ChemCrow (Bran et al., 2023) is an LLM chemistry agent designed to accomplish various tasks such as organic synthesis, drug discovery, and materials design.It integrates 17 expert-designed chemistry tools and operates by prompting GPT-4 to provide specific instructions about the task and the format required.Specifically, a set of tools is created by using a variety of chemistry-related packages and software.These tools and user prompts are provided to GPT-4 and GPT-4 determines its behavioral path before arriving at the final answer through an automated, iterative chain-of-thought process.Throughout the process, ChemCrow serves as an assistant to expert chemists while simultaneously lowering the entry barrier for non-experts by offering a simple interface to access accurate chemical knowledge.Agent (Boiko et al., 2023) is an exploration of emerging autonomous scientific research capabilities of large language models.It binds multiple LLMs together for autonomous design, planning, and execution of scientific experiments (eg., the synthesis experiment of ibuprofen and the crosscoupling experiment of Suzuki and Sonogashira reaction).Specifically, autonomous scientific research is accomplished through a series of tools for surfing the Web, reading documents, executing code, etc., and several LLMs for well-timed calls.</p>
<p>Collaboration</p>
<p>Collaboration is one of the most significant applications of AI agents.Many researchers have already started to develop the application by allowing different AI agents to collaborate with each other, such as AI lawyers, AI programmers, and AI finance to form a team to complete complex tasks together.DialOp (Lin et al., 2023a) describes a simple collaborative morphology, in which AI assistants collaborate with one or more humans via natural language to help them make complex decisions.The autonomous AI agents currently created by MindOS8 are also used for simple human-agent collaboration to assist users with professional tasks.Compared to DialOp and Min-dOS, MetaGPT9 and Multi-GPT10 allow multiple agents can automatically divide up the work and collaborate with each other to accomplish a task, with MetaGPT focusing more on software industry tasks.</p>
<p>Additionally, Generative Agents (Park et al., 2023) are introduced to simulate human behavior.By extending LLMs, complete records of the experiences of the generative agents are stored using natural language, and over time these memories are synthesized to form higher-level reflections that are dynamically retrieved to plan behavior.End-users can interact with a town of 25 generative agents using natural language.The architecture behind these generative agents is expected to be applied in collaborative scenarios.</p>
<p>General purpose</p>
<p>In addition to specific applications, some AI agents are developed for general purposes.These AI agents generally perform a wide range of tasks across diverse domains and attempt to reach the goal by thinking of tasks to do, executing them, and learning from the results.Auto-GPT11 is one of the first examples of GPT-4 running fully autonomously.The feature of completing tasks autonomously without human intervention attracts people's attention.Similar to Auto-GPT, BabyAGI12 is a task-driven autonomous AI agent.BabyAGI constructs a task list dedicated to achieving the goal, derives further tasks based on the previous results, and executes these tasks in order of priority until the overall goal is achieved.Moreover, SuperAGI13 and AgentGPT14 support the building and deployment of autonomous AI agents, and have it embark on any goal imaginable.Although these AI agents are not so perfect and even have some deficiencies, their presentation is certainly an important step towards artificial general intelligence.</p>
<p>Vision-Language model-based agent application</p>
<p>LLM has already demonstrated outstanding capabilities in language-only scenarios.However, in some application scenarios, agents need to deal with multi-modal information, especially visionlanguage modalities.In such cases, modeling only the language information may not achieve satisfactory performance.Recent work considers equipping agents with the Vision-language model (VLM) to handle multi-modal information.In this subsection, we introduce some latest VLM-based agent applications.Some works attempt to apply VLM in the field of embodied AI and robotics that are based on visual and language modalities.For example, Khandelwal et al. (Khandelwal et al., 2022)</p>
<p>Benchmarking</p>
<p>Recently, LLM-based AI agents have attracted significant research interest.In order to evaluate the performance of the proposed agents, some works focus on designing more suitable benchmarks.For example, Valmeekam et al. (Valmeekam et al., 2023) focused on assessing the planning ability of LLMs, which is a key component of AI agents.Liu et al. (Liu et al., 2023d) designed a benchmark based on the WebShop and HotPotQA environment.Their goal is to compare the performance of multiple agent architectures equipped with different LLMs.Li et al. (Li et al., 2023c) constructed a benchmark, named API Bank, to evaluate the ability of LLMs to use tools.Fan et al. (Fan et al., 2022) proposed a simulator based on Minecraft to assess the performance of open-ended embod-ied agent.Xu et al. (Xu et al., 2023) designed a benchmark, named GentBench, which consists of public and private sections, with the aim of comprehensively evaluating the performance of agents.Specifically, GentBench includes a series of complex tasks that promote LLMs to employ external tools for addressing these challenges.Banerjee (Banerjee et al., 2023) introduced an end-toend benchmark that evaluates the performance of LLM-based chatbots by comparing generated answers with the gold answer.Lin et al. (Lin et al., 2023b) presented a task-based evaluation method, which assesses the capabilities of agents based on their task completion within the interactive environment.Liu et al. (Liu et al., 2023c) introduced a multi-dimensional benchmark, named AgentBench, which evaluates the performance of LLM across multiple environments.</p>
<p>Conclusion</p>
<p>In this paper, we presented a comprehensive and systematic survey of the LLM-based agents.</p>
<p>We first introduced the difference between agents based on LLM and traditional methods, then reviewed the related works from the perspectives of components and application of AI agents.Furthermore, we have explored some pressing issues that require solutions and valuable research directions.With the development of LLM, an increasing amount of research attention has been directed toward the field of AI agents, resulting in the emergence of numerous new technologies and methods.Through this review, we aim to assist readers in swiftly grasping the key information and applications of AI agents, and also provide insights into future research directions.</p>
<p>Mapping Structure of Memory: Left illustrates memory categories in human memory, while the right depicts memory categories in AI agents, which have been redefined based on the characteristics of LLM.
Sensory MemoryShort-term MemoryLong-term MemoryEpisodic MemorySemantic MemoryFigure 2: Subsequently, they employed a re-rank model to se-lect the most suitable plan from these candidates.Some works prompt LLM to output its thinkingprocess and results in the form of chain-of-thought,or to feed the intermediate results from LLM'sinference into LLM for further reasoning (Huanget al., 2022a; Akyurek et al., 2023; Chen et al.,2023b,a; Zhang et al., 2023a; Chen et al., 2023c).For example, Zhang et al. (Zhang et al., 2023a) firstguided the model to generate a chain of thoughtby engaging it in multi-turn dialogues based onthe given context. Subsequently, they combinedthe context with the generated chain of thoughtto form samples, which are then used to assist themodel in reasoning and prediction under new con-textual situations. Akyurek et al. (Akyurek et al.,2023) proposed a multi-agent collaborative systemthat includes two LLMs. One LLM is responsiblefor generating answers based on the input content,while the other LLM generates a textual critiquebased on the input and output of the first LLM toassist in error correction.
Logeswaran et al. (Logeswaran et al., 2022)n et al., 2022)first combined some examples with input instructions as a prompt, and then generated multiple candidate sub-goal plans using LLM.</p>
<p>Li et al.(Li et al., 2023a)., 2023)1)into Embodied Agents, and demonstrated that CLIP can effectively enhance the task performance of embodied AI.Driess et al. (Driess et al., 2023)combined ViT and PaLM to construct a multi-modal model named PaLM-E, which is applied in embodied reasoning.PaLM-E takes a multi-modal sequence (i.e., text and image) as input and converts it into text and image embeddings.Specifically, the image embedding is generated by the ViT and a projector encode images.Then, the text and image embeddings serve as input to PaLM for inferring the decisions that the robot needs to execute.Finally, the decisions are transformed into actions by a low-level policy or planner.Some works focus on the navigation task.For instance, Dorbala et al.(Dorbala et al., 2022)first used GPT-3 to break down navigation instructions into a series of sub-instructions.Then, at each time step, they utilized CLIP to select an image from the current panoramic view that corresponded to the subinstructions, serving as the direction for the next navigation step.This process continued until the agent reached its target location.ZSON(Majumdar et al., 2022)is an object-goal navigation agent designed to locate specific objects within an environment.Besides, some works consider applied LVM in the field of multi-model conversational.For example, Video-ChatGPT(Maaz et al., 2023)is a video-based conversational agent fine-tuned using video instruction data.It first employs the visual encoder from CLIP to encode video frames into temporal and spatial features.Then, it utilizes a trainable adapter to map these features into the language space and combines them with query representations as inputs of LLM to generate responses.Li et al.(Li et al., 2023a)introduce a conversational assistant for the biomedical field, named LLaVA-Med.It is continuously trained by LLaVA on multimodal biomedical datasets.</p>
<p>The key components of AI agents were originally defined at https://lilianweng.github.io/posts/2023-06-23-agent/
https://pi.ai/talk
https://chat.openai.com
https://www.anthropic.com/index/claude-2
https://github.com/AntonOsika/gpt-engineer
https://diagram.com/
https://mindos.com/marketplace
https://github.com/geekan/MetaGPT
https://github.com/sidhq/Multi-GPT
https://github.com/Significant-Gravitas/ Auto-GPT
https://github.com/yoheinakajima/babyagi
https://github.com/TransformerOptimus/ SuperAGI
https://github.com/reworkd/AgentGPT</p>
<p>Pddl-the planning domain definition language. Adele Aeronautiques, Craig Howe, Knoblock, Drew Isi, Ashwin Mcdermott, Manuela Ram, Daniel Veloso, David Weld, Sri Wilkins, Anthony Barrett, Dave Christianson, 1998Tech. RepBibliographical References Constructions</p>
<p>RL4F: Generating natural language feedback with reinforcement learning for repairing model outputs. Afra Feyza Akyurek, Ekin Akyurek, Ashwin Kalyan, Peter Clark, Derry Tanti Wijaya, Niket Tandon, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. the 61st Annual Meeting of the Association for Computational Linguistics2023</p>
<p>Meta reinforcement learning for sim-to-real domain adaptation. Karol Arndt, Murtaza Hazara, Ali Ghadirzadeh, Ville Kyrki, 2020 IEEE International Conference on Robotics and Automation (ICRA). IEEE2020</p>
<p>Deep reinforcement learning: A brief survey. Kai Arulkumaran, Marc Peter Deisenroth, Miles Brundage, Anil Anthony Bharath, IEEE Signal Processing Magazine. 3462017</p>
<p>Human memory: Theory and practice. Alan D Baddeley, 1997psychology press</p>
<p>Working memory. Alan David Baddeley, Philosophical Transactions of the Royal Society of London. B. 1983. 1110302</p>
<p>Debarag Banerjee, Pooja Singh, arXiv:2308.04624Arjun Avadhanam, and Saksham Srivastava. 2023. Benchmarking llm powered chatbots: Methods and metrics. arXiv preprint</p>
<p>Christopher Berner, Brockman, arXiv:1912.06680Dota 2 with large scale deep reinforcement learning. 2019arXiv preprint</p>
<p>Chatgpt is a knowledgeable but inexperienced solver: An investigation of commonsense problem in large language models. Xianpei Ning Bian, Le Han, Hongyu Sun, Yaojie Lin, Ben Lu, He, arXiv:2303.164212023arXiv preprint</p>
<p>Robert Daniil A Boiko, Gabe Macknight, Gomes, arXiv:2304.05332Emergent autonomous scientific research capabilities of large language models. 2023arXiv preprint</p>
<p>Sam Andres M Bran, Andrew D Cox, Philippe White, Schwaller, arXiv:2304.05376Chemcrow: Augmenting large-language models with chemistry tools. 2023arXiv preprint</p>
<p>Language models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Advances in neural information processing systems. 202033</p>
<p>Tianle Cai, Xuezhi Wang, Tengyu Ma, Xinyun Chen, Denny Zhou, arXiv:2305.17126Large language models as tool makers. 2023arXiv preprint</p>
<p>The neuroanatomical, neurophysiological and psychological basis of memory: Current models and their origins. Eduardo Camina, Francisco Güell, Frontiers in pharmacology. 84382017</p>
<p>Optimal mixed discrete-continuous planning for linear hybrid systems. Jingkai Chen, Brian C Williams, Chuchu Fan, Proceedings of the 24th International Conference on Hybrid Systems: Computation and Control. the 24th International Conference on Hybrid Systems: Computation and Control2021</p>
<p>When do you need chain-ofthought prompting for chatgpt?. Jiuhai Chen, Lichang Chen, Heng Huang, Tianyi Zhou, arXiv:2304.032622023aarXiv preprint</p>
<p>Liting Chen, Lu Wang, Hang Dong, Yali Du, Jie Yan, Fangkai Yang, Shuang Li, Pu Zhao, Si Qin, Saravan Rajmohan, arXiv:2305.11598Introspective tips: Large language model for in-context decision making. 2023barXiv preprint</p>
<p>Interact: Exploring the potentials of chatgpt as a cooperative agent. Po-Lin Chen, Cheng-Shang Chang, arXiv:2308.015522023arXiv preprint</p>
<p>Chatcot: Tool-augmented chain-ofthought reasoning on\chat-based large language models. Zhipeng Chen, Kun Zhou, Beichen Zhang, Zheng Gong, Wayne Xin Zhao, Ji-Rong Wen, arXiv:2305.14323Proceedings of the Conference on Empirical Methods in Natural Language Processing. the Conference on Empirical Methods in Natural Language Processing2023c. 2020arXiv preprintCheng-Han Chiang, Sung-Feng Huang, and Hung-Yi Lee</p>
<p>Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, arXiv:2110.14168Training verifiers to solve math word problems. 2021arXiv preprint</p>
<p>Analyzing commonsense emergence in few-shot knowledge models. Jeff Da, Ronan Le Bras, Ximing Lu, Yejin Choi, Antoine Bosselut, arXiv:2101.002972021arXiv preprint</p>
<p>Commonsense knowledge mining from pretrained models. Joe Davison, Joshua Feldman, Alexander M Rush, Proceedings of the conference on empirical methods in natural language processing and the 9th international joint conference on natural language processing. the conference on empirical methods in natural language processing and the 9th international joint conference on natural language processing2019</p>
<p>Clip-nav: Using clip for zero-shot vision-and-language navigation. Gunnar Vishnu Sashank Dorbala, Robinson Sigurdsson, Jesse Piramuthu, Thomason, Gaurav S Sukhatme, arXiv:2211.166492022arXiv preprint</p>
<p>Palm-e: An embodied multimodal language model. Danny Driess, Fei Xia, S M Mehdi, Corey Sajjadi, Aakanksha Lynch, Brian Chowdhery, Ayzaan Ichter, Jonathan Wahid, Quan Tompson, Tianhe Vuong, Wenlong Yu, Yevgen Huang, Pierre Chebotar, Daniel Sermanet, Sergey Duckworth, Vincent Levine, Karol Vanhoucke, Marc Hausman, Klaus Toussaint, Andy Greff, Igor Zeng, Pete Mordatch, Florence, Proceedings of the International Conference on Machine Learning. the International Conference on Machine Learning2023</p>
<p>Htn planning: complexity and expressivity. Kutluhan Erol, James Hendler, Dana S Nau, Proceedings of the Twelfth AAAI National Conference on Artificial Intelligence. the Twelfth AAAI National Conference on Artificial Intelligence1994</p>
<p>Andrew Tang, De-An Huang, Yuke Zhu, and Anima Anandkumar. 2022. Minedojo: Building open-ended embodied agents with internet-scale knowledge. Linxi Fan, Guanzhi Wang, Yunfan Jiang, Ajay Mandlekar, Yuncong Yang, Haoyi Zhu, Advances in Neural Information Processing Systems. 35</p>
<p>Pddl2. 1: An extension to pddl for expressing temporal planning domains. Maria Fox, Derek Long, Journal of artificial intelligence research. 202003</p>
<p>Working memory capacity of chatgpt: An empirstudy. Dongyu Gong, Xingchen Wan, Dingmin Wang, </p>
<p>Leveraging pre-trained large language models to construct and utilize world models for model-based task planning. Lin Guan, Karthik Valmeekam, arXiv:2305.14909Sarath Sreedharan, and Subbarao Kambhampati. 2023arXiv preprint</p>
<p>Recent trends in task and motion planning for robotics: A survey. Huihui Guo, Fan Wu, Yunchuan Qin, Ruihui Li, Keqin Li, Kenli Li, 2023ACM Computing Surveys</p>
<p>A universal modular actor formalism for artificial intelligence. Carl Hewitt, Peter Bishop, Richard Steiger, Proceedings of the 3rd international joint conference on Artificial intelligence. the 3rd international joint conference on Artificial intelligence1973</p>
<p>Enabling efficient interaction between an algorithm agent and an llm: A reinforcement learning approach. Bin Hu, Chenyang Zhao, Pu Zhang, Zihao Zhou, Yuanhang Yang, Zenglin Xu, Bin Liu, arXiv:2306.036042023arXiv preprint</p>
<p>Jie Huang, Kevin Chen, -Chuan Chang, arXiv:2212.10403Towards reasoning in large language models: A survey. 2022arXiv preprint</p>
<p>Language models as zero-shot planners: Extracting actionable knowledge for embodied agents. Wenlong Huang, Pieter Abbeel, Deepak Pathak, Igor Mordatch, International Conference on Machine Learning. 2022a</p>
<p>Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete Florence, Andy Zeng, Jonathan Tompson, Igor Mordatch, Yevgen Chebotar, arXiv:2207.05608Inner monologue: Embodied reasoning through planning with language models. 2022barXiv preprint</p>
<p>Task planning in robotics: an empirical comparison of pddl-and asp-based systems. Ian Ml Hunter, Yu-Qian Jiang, Shi-Qi Zhang, Piyush Khandelwal, Peter Stone, Frontiers of Information Technology &amp; Electronic Engineering. 201957. 2019Memory: Facts and fallacies</p>
<p>Bioinspired electronics for artificial sensory systems. Jung Yei Hwan, Byeonghak Park, Jong Uk Kim, Tae-Il Kim, Advanced Materials. 313418036372019</p>
<p>Think before you act: Decision transformers with internal working memory. Jikun Kang, Romain Laroche, Xindi Yuan, Adam Trischler, Xue Liu, Jie Fu, arXiv:2305.163382023arXiv preprint</p>
<p>Mrkl systems: A modular, neuro-symbolic architecture that combines large language models, external knowledge sources and discrete reasoning. Ehud Karpas, Omri Abend, Yonatan Belinkov, Barak Lenz, Opher Lieber, Nir Ratner, Yoav Shoham, Hofit Bata, Yoav Levine, Kevin Leyton-Brown, arXiv:2205.004452022arXiv preprint</p>
<p>An emotion understanding framework for intelligent agents based on episodic and semantic memories. Autonomous agents and multi-agent systems. Mohammad Kazemifard, Nasser Ghasem-Aghaee, Bryan L Koenig, Tuncer I Ören, 201428</p>
<p>Roozbeh Mottaghi, and Aniruddha Kembhavi. 2022. Simple but effective: Clip embeddings for embodied ai. Apoorv Khandelwal, Luca Weihs, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition</p>
<p>A machine with short-term, episodic, and semantic memory systems. Taewoon Kim, Michael Cochez, Vincent François-Lavet, Mark Neerincx, Piek Vossen, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202337</p>
<p>Large language models are zero-shot reasoners. Takeshi Kojima, Shane Shixiang, Machel Gu, Yutaka Reid, Yusuke Matsuo, Iwasawa, Advances in neural information processing systems. 202235</p>
<p>Prompted llms as chatbot modules for long open-domain conversation. Gibbeum Lee, Jongho Volker Hartmann, Dimitris Park, Kangwook Papailiopoulos, Lee, arXiv:2305.045332023arXiv preprint</p>
<p>Chunyuan Li, Cliff Wong, Sheng Zhang, Naoto Usuyama, Haotian Liu, Jianwei Yang, Tristan Naumann, Hoifung Poon, Jianfeng Gao, arXiv:2306.00890Llava-med: Training a large languageand-vision assistant for biomedicine in one day. 2023aarXiv preprint</p>
<p>Large language models with controllable working memory. Daliang Li, Ankit Singh Rawat, Manzil Zaheer, Xin Wang, Michal Lukasik, Andreas Veit, Felix Yu, Sanjiv Kumar, Findings of the Association for Computational Linguistics: ACL. 2023b</p>
<p>Adaptive and intelligent robot task planning for home service: A review. Haizhen Li, Xilun Ding, Engineering Applications of Artificial Intelligence. 1171056182023</p>
<p>Api-bank: A benchmark for tool-augmented llms. Minghao Li, Feifan Song, Bowen Yu, Haiyang Yu, Zhoujun Li, Fei Huang, Yongbin Li, arXiv:2304.082442023carXiv preprint</p>
<p>Yuxi Li, arXiv:1701.07274Deep reinforcement learning: An overview. 2017arXiv preprint</p>
<p>Unleashing infinite-length input capacity for large-scale language models with self-controlled memory system. Xinnian Liang, Bing Wang, Hui Huang, Shuangzhi Wu, Peihao Wu, Lu Lu, Zejun Ma, Zhoujun Li, arXiv:2304.133432023aarXiv preprint</p>
<p>Taskmatrix. ai: Completing tasks by connecting foundation models with millions of apis. Yaobo Liang, Chenfei Wu, Ting Song, Wenshan Wu, Yan Xia, Yu Liu, Yang Ou, Shuai Lu, Lei Ji, Shaoguang Mao, arXiv:2303.164342023barXiv preprint</p>
<p>Decision-oriented dialogue for human-ai collaboration. Jessy Lin, Nicholas Tomlin, Jacob Andreas, Jason Eisner, arXiv:2305.200762023aarXiv preprint</p>
<p>Agentsims: An open-source sandbox for large language model evaluation. Jiaju Lin, Haoran Zhao, Aochi Zhang, Yiting Wu, Huqiuyue Ping, Qin Chen, arXiv:2308.040262023barXiv preprint</p>
<p>Bo Liu, Yuqian Jiang, Xiaohan Zhang, Qiang Liu, Shiqi Zhang, Joydeep Biswas, Peter Stone, arXiv:2304.11477Llm+ p: Empowering large language models with optimal planning proficiency. 2023aarXiv preprint</p>
<p>Chain of hindsight aligns language models with feedback. Hao Liu, Carmelo Sferrazza, Pieter Abbeel, arXiv:2302.026762023b3arXiv preprint</p>
<p>Xiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu Lai, Yu Gu, Hangliang Ding, Kaiwen Men, Kejuan Yang, arXiv:2308.03688Agentbench: Evaluating llms as agents. 2023carXiv preprint</p>
<p>Zhiwei Liu, Weiran Yao, Jianguo Zhang, Le Xue, Shelby Heinecke, Rithesh Murthy, Yihao Feng, Zeyuan Chen, Juan Carlos Niebles, Devansh Arpit, arXiv:2308.05960Bolaa: Benchmarking and orchestrating llm-augmented autonomous agents. 2023darXiv preprint</p>
<p>Petlon: planning efficiently for tasklevel-optimal navigation. Shih-Yun Lo, Shiqi Zhang, Peter Stone, Proceedings of the 17th International Conference on Autonomous Agents and MultiAgent Systems. the 17th International Conference on Autonomous Agents and MultiAgent Systems2018</p>
<p>Few-shot subgoal planning with language models. Lajanugen Logeswaran, Yao Fu, Moontae Lee, Honglak Lee, arXiv:2205.142882022arXiv preprint</p>
<p>Large language model guided tree-of-thought. Jieyi Long, arXiv:2305.082912023arXiv preprint</p>
<p>Pan Lu, Baolin Peng, Hao Cheng, Michel Galley, Kai-Wei Chang, Ying Nian Wu, Song-Chun Zhu, Jianfeng Gao, arXiv:2304.09842Chameleon: Plugand-play compositional reasoning with large language models. 2023arXiv preprint</p>
<p>Videochatgpt: Towards detailed video understanding via large vision and language models. Muhammad Maaz, Hanoona Rasheed, Salman Khan, Fahad Shahbaz Khan, arXiv:2306.054242023arXiv preprint</p>
<p>Zson: Zero-shot object-goal navigation using multimodal goal embeddings. Arjun Majumdar, Gunjan Aggarwal, Bhavika Devnani, Judy Hoffman, Dhruv Batra, Advances in Neural Information Processing Systems. 2022</p>
<p>Programs with common sense. Mccarthy, Proc. Teddington Conference on the Mechanization of Thought Processes. Teddington Conference on the Mechanization of Thought esses1959. 1959</p>
<p>. Marvin L Minsky, The Society of Mind. Simon &amp; Schuster. 1988</p>
<p>Playing atari with deep reinforcement learning. Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, Martin A Riedmiller, CoRR, abs/1312.56022013</p>
<p>Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, arXiv:2112.09332Webgpt: Browser-assisted question-answering with human feedback. 2021arXiv preprint</p>
<p>Extending cognitive architecture with episodic memory. M Andrew, John E Nuxoll, Laird, Proceedings of the 22nd national conference on Artificial intelligence. the 22nd national conference on Artificial intelligence20072</p>
<p>Empowering conversational agents using semantic in-context learning. Amin Omidvar, Aijun An, Proceedings of the 18th Workshop on Innovative Use of NLP for Building Educational Applications (BEA 2023). the 18th Workshop on Innovative Use of NLP for Building Educational Applications (BEA 2023)2023</p>
<p>Gpt-4 technical Bhargavi. Scott Paranjape, Sameer Lundberg, Hannaneh Singh, Luke Hajishirzi, Marco Zettlemoyer, Ribeiro Tulio, arXiv:2303.09014Automatic multi-step reasoning and tool-use for large language models. Art2023. 2023OpenAIarXiv preprint</p>
<p>Aaron Parisi, Yao Zhao, Noah Fiedel, arXiv:2205.12255Talm: Tool augmented language models. 2022arXiv preprint</p>
<p>Sung Joon, Park, C Joseph, Carrie J O'brien, Meredith Ringel Cai, Percy Morris, Michael S Liang, Bernstein, arXiv:2304.03442Generative agents: Interactive simulacra of human behavior. 2023arXiv preprint</p>
<p>Tianjun Shishir G Patil, Xin Zhang, Joseph E Wang, Gonzalez, arXiv:2305.15334Gorilla: Large language model connected with massive apis. 2023arXiv preprint</p>
<p>Check your facts and try again: Improving large language models with external knowledge and automated feedback. Baolin Peng, Michel Galley, Pengcheng He, Hao Cheng, Yujia Xie, Yu Hu, Qiuyuan Huang, Lars Liden, Zhou Yu, Weizhu Chen, arXiv:2302.128132023arXiv preprint</p>
<p>Language models as knowledge bases?. Fabio Petroni, Tim Rocktäschel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, Alexander Miller, Proceedings of the Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing. the Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing2019</p>
<p>Learning transferable visual models from natural language supervision. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever, Proceedings of the 38th International Conference on Machine Learning. the 38th International Conference on Machine Learning2021</p>
<p>Anna Rogers, Olga Kovaleva, Anna Rumshisky, A primer in bertology: What we know about how bert works. Transactions of the Association for Computational Linguistics20218</p>
<p>Learning representations by back-propagating errors. Geoffrey E David E Rumelhart, Ronald J Hinton, Williams, nature. 32360881986</p>
<p>Stuart Russell, Peter Norvig, Artificial Intelligence: A Modern Approach. Prentice Hall20103 edition</p>
<p>Tara Safavi, Danai Koutra, arXiv:2104.05837Relational world knowledge representation in contextual language models: A review. 2021arXiv preprint</p>
<p>Toolformer: Language models can teach themselves to use tools. Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, Thomas Scialom, arXiv:2302.047612023arXiv preprint</p>
<p>Hugginggpt: Solving ai tasks with chatgpt and its friends in huggingface. Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, Yueting Zhuang, arXiv:2303.175802023arXiv preprint</p>
<p>Reflexion: an autonomous agent with dynamic memory and self-reflection. Noah Shinn, Beck Labash, Ashwin Gopinath, arXiv:2303.113662023arXiv preprint</p>
<p>Blenderbot 3: a deployed conversational agent that continually learns to responsibly engage. Kurt Shuster, Jing Xu, Mojtaba Komeili, Da Ju, Eric Michael Smith, Stephen Roller, Megan Ung, Moya Chen, Kushal Arora, Joshua Lane, arXiv:2208.031882022arXiv preprint</p>
<p>Mastering the game of go with deep neural networks and tree search. David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den, Julian Driessche, Schrittwieser, nature. 5297587Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot</p>
<p>Progprompt: Generating situated robot task plans using large language models. Ishika Singh, Valts Blukis, Arsalan Mousavian, Ankit Goyal, Danfei Xu, Jonathan Tremblay, Dieter Fox, Jesse Thomason, Animesh Garg, Proceedings of IEEE International Conference on Robotics and Automation. IEEE International Conference on Robotics and Automation2023</p>
<p>Interleaving hierarchical task planning and motion constraint testing for dual-arm manipulation. Alejandro Suárez-Hernández, Guillem Alenyà, Carme Torras, 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems. 2018</p>
<p>Haotian Sun, Yuchen Zhuang, Lingkai Kong, Bo Dai, Chao Zhang, arXiv:2305.16653Adaplanner: Adaptive planning from feedback with language models. 2023arXiv preprint</p>
<p>Chao Tang, Dehao Huang, Wenqi Ge, Weiyu Liu, Hong Zhang, arXiv:2307.13204Graspgpt: Leveraging semantic knowledge from a large language model for task-oriented grasping. 2023arXiv preprint</p>
<p>Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze, Alicia Cheng, Taylor Jin, Leslie Bos, Yu Baker, Du, arXiv:2201.08239Lamda: Language models for dialog applications. 2022arXiv preprint</p>
<p>Elements of episodic memory. Endel Tulving et al. 1972. Episodic and semantic memory. Endel Tulving, Organization of memory. 111983</p>
<p>On the planning abilities of large language models (a critical investigation with a proposed benchmark. Karthik Valmeekam, Sarath Sreedharan, Matthew Marquez, Alberto Olmo, Subbarao Kambhampati, arXiv:2302.067062023arXiv preprint</p>
<p>A basic agent. Steven Vere, Timothy Bickmore, Computational intelligence. 611990</p>
<p>Grandmaster level in starcraft ii using multi-agent reinforcement learning. Oriol Vinyals, Igor Babuschkin, Wojciech M Czarnecki, Michaël Mathieu, Andrew Dudzik, Junyoung Chung, David H Choi, Richard Powell, Timo Ewalds, Petko Georgiev, Nature. 57577822019</p>
<p>Artificial sensory memory. Changjin Wan, Pingqiang Cai, Ming Wang, Yan Qian, Wei Huang, Xiaodong Chen, Advanced Materials. 321519024342020</p>
<p>Voyager: An open-ended embodied agent with large language models. Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, Anima Anandkumar, arXiv:2305.162912023aarXiv preprint</p>
<p>Plan-and-solve prompting: Improving zero-shot chain-of-thought reasoning by large language models. Lei Wang, Wanyu Xu, Yihuai Lan, Zhiqiang Hu, Yunshi Lan, Roy , Ka-Wei Lee, Ee-Peng Lim, Proceedings of the 61st Annual Meeting of the Association for Computational. the 61st Annual Meeting of the Association for Computational2023b</p>
<p>Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, arXiv:2206.07682Emergent abilities of large language models. 2022aarXiv preprint</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Denny Quoc V Le, Zhou, Advances in Neural Information Processing Systems. 2022b35</p>
<p>Yaqi Xie, Chen Yu, Tongyao Zhu, Jinbin Bai, Ze Gong, Harold Soh, arXiv:2302.05128Translating natural language to planning goals with large-language models. 2023arXiv preprint</p>
<p>Binfeng Xu, Xukun Liu, Hua Shen, Zeyu Han, Yuhan Li, Murong Yue, Zhiyuan Peng, Yuchen Liu, Ziyu Yao, Dongkuan Xu, arXiv:2308.04030Gentopia: A collaborative platform for tool-augmented llms. 2023arXiv preprint</p>
<p>Tree of thoughts: Deliberate problem solving with large language models. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L Griffiths, Yuan Cao, Karthik Narasimhan, arXiv:2305.106012023aarXiv preprint</p>
<p>Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, Yuan Cao, arXiv:2210.03629React: Synergizing reasoning and acting in language models. 2022arXiv preprint</p>
<p>Weiran Yao, Shelby Heinecke, Juan Carlos Niebles, Zhiwei Liu, Yihao Feng, Le Xue, Rithesh Murthy, Zeyuan Chen, Jianguo Zhang, Devansh Arpit, arXiv:2308.02151Retroformer: Retrospective large language agents with policy gradient optimization. 2023barXiv preprint</p>
<p>Investigating chain-of-thought with chatgpt for stance detection on social media. Bowen Zhang, Xianghua Fu, Daijun Ding, Hu Huang, Yangyang Li, Liwen Jing, arXiv:2304.030872023aarXiv preprint</p>
<p>Large language model is semi-parametric reinforcement learning agent. Danyang Zhang, Lu Chen, Situo Zhang, Hongshen Xu, Zihan Zhao, Kai Yu, arXiv:2306.079292023barXiv preprint</p>
<p>Automatic chain of thought prompting in large language models. Zhuosheng Zhang, Aston Zhang, Mu Li, Alex Smola, Proceedings of the Eleventh International Conference on Learning Representations. the Eleventh International Conference on Learning Representations2022</p>
<p>Wanjun Zhong, Lianghong Guo, Qiqi Gao, Yanlin Wang, arXiv:2305.10250Memorybank: Enhancing large language models with long-term memory. 2023arXiv preprint</p>
<p>Mindstorms in natural language-based societies of mind. Mingchen Zhuge, Haozhe Liu, 2023</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4594 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4594</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4594</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-100.html">extraction-schema-100</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <p><strong>Paper ID:</strong> paper-2b37107e99568d7ece09c888d7e67b2d369ccc45</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/2b37107e99568d7ece09c888d7e67b2d369ccc45" target="_blank">ResearchTown: Simulator of Human Research Community</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> ResearchTown can provide a realistic simulation of collaborative research activities, including paper writing and review writing, and can generate interdisciplinary research ideas that potentially inspire pioneering research directions.</p>
                <p><strong>Paper Abstract:</strong> Large Language Models (LLMs) have demonstrated remarkable potential in scientific domains, yet a fundamental question remains unanswered: Can we simulate human research communities with LLMs? Addressing this question can deepen our understanding of the processes behind idea brainstorming and inspire the automatic discovery of novel scientific insights. In this work, we propose ResearchTown, a multi-agent framework for research community simulation. Within this framework, the human research community is simplified as an agent-data graph, where researchers and papers are represented as agent-type and data-type nodes, respectively, and connected based on their collaboration relationships. We also introduce TextGNN, a text-based inference framework that models various research activities (e.g., paper reading, paper writing, and review writing) as special forms of a unified message-passing process on the agent-data graph. To evaluate the quality of the research community simulation, we present ResearchBench, a benchmark that uses a node-masking prediction task for scalable and objective assessment based on similarity. Our experiments reveal three key findings: (1) ResearchTown can provide a realistic simulation of collaborative research activities, including paper writing and review writing; (2) ResearchTown can maintain robust simulation with multiple researchers and diverse papers; (3) ResearchTown can generate interdisciplinary research ideas that potentially inspire pioneering research directions.</p>
                <p><strong>Cost:</strong> 0.023</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4594.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4594.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RESEARCHTOWN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RESEARCHTOWN: Simulator of Human Research Community</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A graph-based multi-agent simulation framework that models a research community as an agent-data graph and implements multi-step research activities (paper reading, paper writing, review writing) via text-space message passing driven by LLM agent functions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>RESEARCHTOWN</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>RESEARCHTOWN represents a research community as a heterogeneous agent-data graph (researcher nodes as agent functions and paper nodes as data with text attributes) and simulates research workflows by running TextGNN-style text-space message passing. It implements three staged TextGNN layers: (1) paper reading — create agent profiles by aggregating nearby paper texts; (2) paper writing — generate a new paper node by aggregating outputs from agent functions and cited data nodes; (3) review writing — generate review bullet points and a score by aggregating reviewer agent outputs and cited papers. Aggregation settings include AGG-self, AGG-agent, AGG-data, and AGG-global (the latter is the full RESEARCHTOWN aggregation). The implementation parallelizes LLM calls (f_a and f_g prompts), controls output formats via prompt templates (bullet-point condensed hidden states), and evaluates outputs with embedding and LLM-based metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>GPT-4o-mini (primary), Qwen-2.5-7B-Instruct, Deepseek-v3; text-embedding-large-3 and voyage-3 for embeddings</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>LLM-driven text summarization and structured prompting (agent prompts f_a and global f_g) to extract condensed bullet-point hidden states from paper full texts and neighborhoods; parallelized prompting of agents over neighborhoods</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Text-space message passing (TextGNN): concatenation/aggregation of agent-produced texts and data node summaries into controlled-format prompts, followed by generation by a global agent function f_g; AGG-global merges agent and data contributions into final output</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Evaluated on RESEARCHBENCH: 1,000 paper-writing tasks and 200 review-writing tasks (neighborhood sizes vary; per-author collected publications limited to ~20)</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Computer science / Machine Learning (top-tier ML conference papers: NeurIPS 2024, ICLR 2024)</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Condensed bullet-point paper summaries (5-question format), generated preliminary research proposals, review strengths/weaknesses bullet points, numerical review scores</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Embedding-based cosine similarity (text-embedding-large-3, voyage-3), LLM-based fine-grained scoring across dimensions (topic/method/factual/claim/application/overall) via GPT-4o, human evaluation for subset, novelty & feasibility scoring (LLM & human)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Paper-writing (AGG-global) overall similarity 0.6751 (text-embedding-large-3); AGG-data 0.6530; AGG-agent 0.5524; AGG-self 0.4608. For reviews, strength similarity ~51, weakness similarity ~47 (embedding-percentage scores in paper's reported scale); LLM-based novelty/feasibility for simulated outputs: 7.39/6.82 vs real-world 7.85/7.13; human-based novelty/feasibility: 5.50/7.98 (simulation) vs 5.90/7.85 (real-world).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Aggregation ablations (AGG-self, AGG-agent, AGG-data), alternative LLMs (Qwen-2.5-7B-Instruct, Deepseek-v3), and human ground truth</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>AGG-global (RESEARCHTOWN) outperforms AGG-agent and AGG-self on paper writing (67.51 vs 55.24 vs 46.08 overall similarity) and slightly outperforms AGG-data (67.51 vs 65.30); Deepseek-v3 > GPT-4o-mini > Qwen-2.5 in aggregate performance; review metrics vary by aggregation: agent aggregation better for strengths/scores, data aggregation better for weaknesses.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>1) Text-space multi-agent message passing (AGG-global) produces realistic paper drafts that align semantically with real papers; 2) combining agent (researcher) context and cited papers yields the best results; 3) increasing the number of agents (collaborators/reviewers) improves output quality; 4) careful selection of cited papers (e.g., related-work citations) yields better synthesis than arbitrary references; 5) system can generate plausible interdisciplinary ideas.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Ethical concerns (plagiarism potential, misleading/low-quality claims, attribution/authorship); hallucination and superficial mixing when combining too many disparate domains; simulated outputs are less novel/feasible than best real-world papers; computational cost of many LLM calls (mitigated via parallelization); dependence on LLM evaluation biases.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Ablations show positive scaling with number of agents (e.g., AGG-agent similarity increases when agents go from 1 to 2: 49.0 -> 52.7 for a hard subset); quality depends on which cited papers are included (related-work citations help more than arbitrary citations); model choice scales performance (Deepseek-v3 > GPT-4o-mini > Qwen-2.5).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ResearchTown: Simulator of Human Research Community', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4594.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4594.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TextGNN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>TextGNN: Text-based message-passing framework for agent-data graphs</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A message-passing framework that defines node hidden states in the text space and uses LLM agent functions as message and aggregation operators to perform text-based GNN inference on an agent-data graph.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>TextGNN</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>TextGNN replaces classical numeric embeddings with textual hidden states and uses LLM-based agent functions (f_a for agent neighbors and f_g as a global aggregator) to implement MSG and AGG steps via prompt concatenation and LLM generation. It handles three interaction types (agent-agent, agent-data, data-data) by formatting neighborhood texts into prompts and producing controlled-format outputs (bullet-point summaries) for downstream aggregation.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Implemented with GPT-4o-mini in experiments (also compatible with Qwen-2.5-7B-Instruct, Deepseek-v3)</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Prompted LLM summarization of data node texts into bullet-point hidden states; agent-specific prompt templates to extract agent views from neighborhoods</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Concatenative prompt-based aggregation: feeds agent-produced summaries and cited-data summaries into a global LLM aggregator f_g to synthesize final paper or review outputs</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>General; applied in paper to ML research community simulation</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Textual hidden states (bullet point summaries), generated papers/proposals and reviews</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Same embedding- and LLM-based metrics as RESEARCHTOWN when TextGNN used within system</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared via aggregation ablations (AGG-self, AGG-agent, AGG-data, AGG-global) within RESEARCHTOWN experiments</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>AGG-global (TextGNN with full neighborhood aggregation) achieves the best overall performance in paper-writing tasks versus restricted aggregations</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Text-space message passing with LLMs enables multi-agent reasoning over textual data and supports simulation of collaborative research activities; controlling output format (bullet points) stabilizes multi-step text aggregation.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Potential computational cost (many LLM calls), need to control prompt/output length to avoid inflation, and risk of accumulating generation errors if intermediate outputs are imperfect.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>TextGNN is parallelizable by batching neighborhood prompts; performance improves with richer neighborhood information and additional agents but depends on prompt/control and selected neighborhood quality.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ResearchTown: Simulator of Human Research Community', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4594.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4594.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Agent-Data Graph</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Agent-Data Graph (community graph)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A heterogeneous graph abstraction with two node types—agent nodes (functions implemented by LLMs) and data nodes (textual artifacts like papers)—and three edge types (agent-agent, agent-data, data-data) used to model the research community.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Agent-Data Graph</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Defines agent nodes as functions (LLM prompts + profile) and data nodes as text attributes; edges encode authorship, reviewing expertise, and citations. Facilitates TextGNN message passing by providing structured neighborhoods for agent and data aggregation.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Neighborhood-based LLM summarization of connected data nodes and agent profiles</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Graph-structured aggregation implemented via LLM prompt concatenation following GNN-style message passing</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Graph-based modeling of research communities (general)</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Structured neighborhood-conditioned textual outputs (agent profiles, paper summaries, reviews)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Conceptually separates agents (as functions) from data (as textual nodes), enabling iterative read-write-update cycles and natural modeling of collaborative research processes.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Requires careful definition of agent functions and edge semantics; computational scaling with dense neighborhoods unless parallelized.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Design supports parallelization of agent prompts across neighborhoods; practical implementation reduces complexity from O(N*M) to O(M) LLM calls by batching data for each agent.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ResearchTown: Simulator of Human Research Community', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4594.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4594.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RESEARCHBENCH</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RESEARCHBENCH</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A benchmark introduced in this paper for evaluating research community simulation via a masked node prediction task, containing 1,000 paper-writing and 200 review-writing tasks drawn from recent top-tier ML conferences.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>RESEARCHBENCH</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Constructs masked-node prediction tasks on community graphs: mask a target paper node (or its review) and require the simulator to reconstruct the node from its neighborhood. Uses standardized 5-question alignment for papers and bullet-point format for reviews to enable embedding-based similarity evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Used voyage-3 and text-embedding-large-3 for retrieval/embedding; GPT-4o-mini used to generate simulation outputs during evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>LLM-based summarization into 5 aligned questions for papers and bullet-point conversion for reviews</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Neighborhood-conditioned generation via RESEARCHTOWN/TextGNN to produce masked-node predictions</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>1,000 paper-writing tasks (PAPERBENCH) and 200 review-writing tasks (REVIEWBENCH); additional HIGHIMPACTPAPERBENCH: 100 tasks</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Machine Learning (ICLR 2024, NeurIPS 2024)</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Generated paper summaries (5Q format) and review bullet points + scores</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Embedding cosine similarity (text-embedding-large-3, voyage-3), LLM-based scoring across fine-grained dimensions, human evaluation (subset)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Aggregation ablations within RESEARCHTOWN; inter-human similarity baselines provided (inter-reviewer similarity ~0.59)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Provides a scalable, objective, and reproducible evaluation setup for multi-agent research simulation using semantic similarity as ground-truth alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Relies on embedding models and LLMs for both generation and evaluation which can introduce biases; ground-truth (real papers/reviews) not perfect but treated as reference.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Designed for large-scale automated evaluation; can be extended to other domains and to larger numbers of tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ResearchTown: Simulator of Human Research Community', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4594.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e4594.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Researchagent</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Researchagent: Iterative research idea generation over scientific literature with large language models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A cited concurrent system that iteratively generates research ideas from scientific literature using LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Researchagent: Iterative research idea generation over scientific literature with large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Researchagent</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>As cited, an approach that uses LLMs to iteratively generate research ideas over scientific literature; details are referenced but not expanded within this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Described as iterative literature-conditioned idea generation (LLM-driven)</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Iterative idea generation over multiple documents (as cited)</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Research idea generation (general/innovation)</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Generated research ideas / proposals</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ResearchTown: Simulator of Human Research Community', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4594.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e4594.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AI Scientist</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>The AI Scientist: Towards fully automated open-ended scientific discovery</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A cited work that aims toward fully automated open-ended scientific discovery using autonomous systems; referenced as related research automation work.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>The ai scientist: Towards fully automated open-ended scientific discovery</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>The AI Scientist</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Cited as an approach toward automated scientific discovery; details not provided in this paper beyond the citation.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Automated scientific discovery</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ResearchTown: Simulator of Human Research Community', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4594.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e4594.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Nova</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Nova: An iterative planning and search approach to enhance novelty and diversity of LLM generated ideas</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A cited method that applies iterative planning and search to improve novelty/diversity of LLM-generated ideas, relevant to research idea generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Nova: An iterative planning and search approach to enhance novelty and diversity of llm generated ideas</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Nova</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Cited as an iterative planning and search approach to improve LLM-generated idea novelty and diversity; the RESEARCHTOWN paper refers to it in the context of related work on enhancing generated ideas.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Iterative planning & search over LLM outputs (as cited)</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Idea generation / creativity augmentation</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Novel/diverse ideas</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ResearchTown: Simulator of Human Research Community', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4594.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e4594.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPTSwarm</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPTSwarm: Language agents as optimizable graphs</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A cited multi-agent system that models language agents as an optimizable graph structure, relevant to multi-agent LLM graph modeling.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>GPTSwarm: Language agents as optimizable graphs</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>GPTSwarm</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Cited as work that treats language agents as nodes in an optimizable graph, aligning with the idea of graph-structured multi-agent systems; specific extraction/synthesis details are not provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Graph-structured multi-agent interactions (as cited)</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Multi-agent LLM simulation/optimization</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ResearchTown: Simulator of Human Research Community', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4594.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e4594.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SOTOPIA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SOTOPIA: Interactive evaluation for social intelligence in language agents</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A cited interactive evaluation framework for social intelligence in language-agent-based multi-agent simulations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>SOTOPIA: Interactive evaluation for social intelligence in language agents</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>SOTOPIA</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Referenced as a multi-agent LLM framework for social simulation and interactive evaluation; mentioned in related work contrasting social/game multi-agent frameworks with research-community simulation.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Social simulation / language agents</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ResearchTown: Simulator of Human Research Community', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4594.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e4594.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>WarAgent</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>War and Peace (WarAgent): Large language model-based multi-agent simulation of world wars</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A cited multi-agent simulation system using LLMs to model complex historical/game environments, used as an example of LLM multi-agent frameworks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>War and peace (waragent): Large language model-based multi-agent simulation of world wars</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>WarAgent</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Cited as an LLM-based multi-agent simulation applied to historical war simulation; used to motivate that existing multi-agent frameworks (social/game) are not directly suited for research-community simulation.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Multi-agent simulation / historical/game domains</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ResearchTown: Simulator of Human Research Community', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4594.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e4594.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MLAgentBench</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MLAgentBench: Evaluating language agents on machine learning experimentation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A cited benchmark/framework that evaluates language agents on ML experimentation tasks, referenced among research automation efforts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Mlagentbench: Evaluating language agents on machine learning experimentation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>MLAgentBench</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Cited as an effort evaluating language agents for ML experimentation workflows; described as focused on single-agent workflows rather than collaborative research simulation.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Machine learning experimentation / agent evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ResearchTown: Simulator of Human Research Community', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4594.11">
                <h3 class="extraction-instance">Extracted Data Instance 11 (e4594.11)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Label-free Node Classification (Chen et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Label-free node classification on graphs with large language models (LLMs)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A cited approach using LLMs for graph node classification without traditional labels, part of related work on LLMs applied to text-attributed graphs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Label-free node classification on graphs with large language models (llms)</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Label-free node classification (Chen et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Cited as work that treats LLMs as tools for understanding text attributes on graphs and co-training with GNNs; referenced to contrast prior TAG research with RESEARCHTOWN's agent nodes-as-functions concept.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>LLM-based understanding of text attributes for graph tasks (as cited)</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Text-attributed graphs / node classification</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Node labels/classifications</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ResearchTown: Simulator of Human Research Community', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4594.12">
                <h3 class="extraction-instance">Extracted Data Instance 12 (e4594.12)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Explanations-as-Features</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Explanations as features: LLM-based features for text-attributed graphs</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A cited work that uses LLMs to generate explanation-style features for text-attributed graphs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Explanations as features: Llm-based features for text-attributed graphs</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Explanations-as-Features</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Cited in related work as an example of using LLMs to extract richer textual features for graph nodes, informing the backdrop for TextGNN and agent-data graphs.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>LLM-based explanation/feature generation for textual graph nodes (as cited)</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Text-attributed graphs</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>LLM-generated explanatory features</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ResearchTown: Simulator of Human Research Community', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Researchagent: Iterative research idea generation over scientific literature with large language models <em>(Rating: 2)</em></li>
                <li>The ai scientist: Towards fully automated open-ended scientific discovery <em>(Rating: 2)</em></li>
                <li>GPTSwarm: Language agents as optimizable graphs <em>(Rating: 2)</em></li>
                <li>SOTOPIA: Interactive evaluation for social intelligence in language agents <em>(Rating: 1)</em></li>
                <li>Nova: An iterative planning and search approach to enhance novelty and diversity of llm generated ideas <em>(Rating: 1)</em></li>
                <li>Label-free node classification on graphs with large language models (llms) <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4594",
    "paper_id": "paper-2b37107e99568d7ece09c888d7e67b2d369ccc45",
    "extraction_schema_id": "extraction-schema-100",
    "extracted_data": [
        {
            "name_short": "RESEARCHTOWN",
            "name_full": "RESEARCHTOWN: Simulator of Human Research Community",
            "brief_description": "A graph-based multi-agent simulation framework that models a research community as an agent-data graph and implements multi-step research activities (paper reading, paper writing, review writing) via text-space message passing driven by LLM agent functions.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "RESEARCHTOWN",
            "system_description": "RESEARCHTOWN represents a research community as a heterogeneous agent-data graph (researcher nodes as agent functions and paper nodes as data with text attributes) and simulates research workflows by running TextGNN-style text-space message passing. It implements three staged TextGNN layers: (1) paper reading — create agent profiles by aggregating nearby paper texts; (2) paper writing — generate a new paper node by aggregating outputs from agent functions and cited data nodes; (3) review writing — generate review bullet points and a score by aggregating reviewer agent outputs and cited papers. Aggregation settings include AGG-self, AGG-agent, AGG-data, and AGG-global (the latter is the full RESEARCHTOWN aggregation). The implementation parallelizes LLM calls (f_a and f_g prompts), controls output formats via prompt templates (bullet-point condensed hidden states), and evaluates outputs with embedding and LLM-based metrics.",
            "llm_model_used": "GPT-4o-mini (primary), Qwen-2.5-7B-Instruct, Deepseek-v3; text-embedding-large-3 and voyage-3 for embeddings",
            "extraction_technique": "LLM-driven text summarization and structured prompting (agent prompts f_a and global f_g) to extract condensed bullet-point hidden states from paper full texts and neighborhoods; parallelized prompting of agents over neighborhoods",
            "synthesis_technique": "Text-space message passing (TextGNN): concatenation/aggregation of agent-produced texts and data node summaries into controlled-format prompts, followed by generation by a global agent function f_g; AGG-global merges agent and data contributions into final output",
            "number_of_papers": "Evaluated on RESEARCHBENCH: 1,000 paper-writing tasks and 200 review-writing tasks (neighborhood sizes vary; per-author collected publications limited to ~20)",
            "domain_or_topic": "Computer science / Machine Learning (top-tier ML conference papers: NeurIPS 2024, ICLR 2024)",
            "output_type": "Condensed bullet-point paper summaries (5-question format), generated preliminary research proposals, review strengths/weaknesses bullet points, numerical review scores",
            "evaluation_metrics": "Embedding-based cosine similarity (text-embedding-large-3, voyage-3), LLM-based fine-grained scoring across dimensions (topic/method/factual/claim/application/overall) via GPT-4o, human evaluation for subset, novelty & feasibility scoring (LLM & human)",
            "performance_results": "Paper-writing (AGG-global) overall similarity 0.6751 (text-embedding-large-3); AGG-data 0.6530; AGG-agent 0.5524; AGG-self 0.4608. For reviews, strength similarity ~51, weakness similarity ~47 (embedding-percentage scores in paper's reported scale); LLM-based novelty/feasibility for simulated outputs: 7.39/6.82 vs real-world 7.85/7.13; human-based novelty/feasibility: 5.50/7.98 (simulation) vs 5.90/7.85 (real-world).",
            "comparison_baseline": "Aggregation ablations (AGG-self, AGG-agent, AGG-data), alternative LLMs (Qwen-2.5-7B-Instruct, Deepseek-v3), and human ground truth",
            "performance_vs_baseline": "AGG-global (RESEARCHTOWN) outperforms AGG-agent and AGG-self on paper writing (67.51 vs 55.24 vs 46.08 overall similarity) and slightly outperforms AGG-data (67.51 vs 65.30); Deepseek-v3 &gt; GPT-4o-mini &gt; Qwen-2.5 in aggregate performance; review metrics vary by aggregation: agent aggregation better for strengths/scores, data aggregation better for weaknesses.",
            "key_findings": "1) Text-space multi-agent message passing (AGG-global) produces realistic paper drafts that align semantically with real papers; 2) combining agent (researcher) context and cited papers yields the best results; 3) increasing the number of agents (collaborators/reviewers) improves output quality; 4) careful selection of cited papers (e.g., related-work citations) yields better synthesis than arbitrary references; 5) system can generate plausible interdisciplinary ideas.",
            "limitations_challenges": "Ethical concerns (plagiarism potential, misleading/low-quality claims, attribution/authorship); hallucination and superficial mixing when combining too many disparate domains; simulated outputs are less novel/feasible than best real-world papers; computational cost of many LLM calls (mitigated via parallelization); dependence on LLM evaluation biases.",
            "scaling_behavior": "Ablations show positive scaling with number of agents (e.g., AGG-agent similarity increases when agents go from 1 to 2: 49.0 -&gt; 52.7 for a hard subset); quality depends on which cited papers are included (related-work citations help more than arbitrary citations); model choice scales performance (Deepseek-v3 &gt; GPT-4o-mini &gt; Qwen-2.5).",
            "uuid": "e4594.0",
            "source_info": {
                "paper_title": "ResearchTown: Simulator of Human Research Community",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "TextGNN",
            "name_full": "TextGNN: Text-based message-passing framework for agent-data graphs",
            "brief_description": "A message-passing framework that defines node hidden states in the text space and uses LLM agent functions as message and aggregation operators to perform text-based GNN inference on an agent-data graph.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "TextGNN",
            "system_description": "TextGNN replaces classical numeric embeddings with textual hidden states and uses LLM-based agent functions (f_a for agent neighbors and f_g as a global aggregator) to implement MSG and AGG steps via prompt concatenation and LLM generation. It handles three interaction types (agent-agent, agent-data, data-data) by formatting neighborhood texts into prompts and producing controlled-format outputs (bullet-point summaries) for downstream aggregation.",
            "llm_model_used": "Implemented with GPT-4o-mini in experiments (also compatible with Qwen-2.5-7B-Instruct, Deepseek-v3)",
            "extraction_technique": "Prompted LLM summarization of data node texts into bullet-point hidden states; agent-specific prompt templates to extract agent views from neighborhoods",
            "synthesis_technique": "Concatenative prompt-based aggregation: feeds agent-produced summaries and cited-data summaries into a global LLM aggregator f_g to synthesize final paper or review outputs",
            "number_of_papers": null,
            "domain_or_topic": "General; applied in paper to ML research community simulation",
            "output_type": "Textual hidden states (bullet point summaries), generated papers/proposals and reviews",
            "evaluation_metrics": "Same embedding- and LLM-based metrics as RESEARCHTOWN when TextGNN used within system",
            "performance_results": null,
            "comparison_baseline": "Compared via aggregation ablations (AGG-self, AGG-agent, AGG-data, AGG-global) within RESEARCHTOWN experiments",
            "performance_vs_baseline": "AGG-global (TextGNN with full neighborhood aggregation) achieves the best overall performance in paper-writing tasks versus restricted aggregations",
            "key_findings": "Text-space message passing with LLMs enables multi-agent reasoning over textual data and supports simulation of collaborative research activities; controlling output format (bullet points) stabilizes multi-step text aggregation.",
            "limitations_challenges": "Potential computational cost (many LLM calls), need to control prompt/output length to avoid inflation, and risk of accumulating generation errors if intermediate outputs are imperfect.",
            "scaling_behavior": "TextGNN is parallelizable by batching neighborhood prompts; performance improves with richer neighborhood information and additional agents but depends on prompt/control and selected neighborhood quality.",
            "uuid": "e4594.1",
            "source_info": {
                "paper_title": "ResearchTown: Simulator of Human Research Community",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "Agent-Data Graph",
            "name_full": "Agent-Data Graph (community graph)",
            "brief_description": "A heterogeneous graph abstraction with two node types—agent nodes (functions implemented by LLMs) and data nodes (textual artifacts like papers)—and three edge types (agent-agent, agent-data, data-data) used to model the research community.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Agent-Data Graph",
            "system_description": "Defines agent nodes as functions (LLM prompts + profile) and data nodes as text attributes; edges encode authorship, reviewing expertise, and citations. Facilitates TextGNN message passing by providing structured neighborhoods for agent and data aggregation.",
            "llm_model_used": null,
            "extraction_technique": "Neighborhood-based LLM summarization of connected data nodes and agent profiles",
            "synthesis_technique": "Graph-structured aggregation implemented via LLM prompt concatenation following GNN-style message passing",
            "number_of_papers": null,
            "domain_or_topic": "Graph-based modeling of research communities (general)",
            "output_type": "Structured neighborhood-conditioned textual outputs (agent profiles, paper summaries, reviews)",
            "evaluation_metrics": null,
            "performance_results": null,
            "comparison_baseline": null,
            "performance_vs_baseline": null,
            "key_findings": "Conceptually separates agents (as functions) from data (as textual nodes), enabling iterative read-write-update cycles and natural modeling of collaborative research processes.",
            "limitations_challenges": "Requires careful definition of agent functions and edge semantics; computational scaling with dense neighborhoods unless parallelized.",
            "scaling_behavior": "Design supports parallelization of agent prompts across neighborhoods; practical implementation reduces complexity from O(N*M) to O(M) LLM calls by batching data for each agent.",
            "uuid": "e4594.2",
            "source_info": {
                "paper_title": "ResearchTown: Simulator of Human Research Community",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "RESEARCHBENCH",
            "name_full": "RESEARCHBENCH",
            "brief_description": "A benchmark introduced in this paper for evaluating research community simulation via a masked node prediction task, containing 1,000 paper-writing and 200 review-writing tasks drawn from recent top-tier ML conferences.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "RESEARCHBENCH",
            "system_description": "Constructs masked-node prediction tasks on community graphs: mask a target paper node (or its review) and require the simulator to reconstruct the node from its neighborhood. Uses standardized 5-question alignment for papers and bullet-point format for reviews to enable embedding-based similarity evaluation.",
            "llm_model_used": "Used voyage-3 and text-embedding-large-3 for retrieval/embedding; GPT-4o-mini used to generate simulation outputs during evaluation",
            "extraction_technique": "LLM-based summarization into 5 aligned questions for papers and bullet-point conversion for reviews",
            "synthesis_technique": "Neighborhood-conditioned generation via RESEARCHTOWN/TextGNN to produce masked-node predictions",
            "number_of_papers": "1,000 paper-writing tasks (PAPERBENCH) and 200 review-writing tasks (REVIEWBENCH); additional HIGHIMPACTPAPERBENCH: 100 tasks",
            "domain_or_topic": "Machine Learning (ICLR 2024, NeurIPS 2024)",
            "output_type": "Generated paper summaries (5Q format) and review bullet points + scores",
            "evaluation_metrics": "Embedding cosine similarity (text-embedding-large-3, voyage-3), LLM-based scoring across fine-grained dimensions, human evaluation (subset)",
            "performance_results": null,
            "comparison_baseline": "Aggregation ablations within RESEARCHTOWN; inter-human similarity baselines provided (inter-reviewer similarity ~0.59)",
            "performance_vs_baseline": null,
            "key_findings": "Provides a scalable, objective, and reproducible evaluation setup for multi-agent research simulation using semantic similarity as ground-truth alignment.",
            "limitations_challenges": "Relies on embedding models and LLMs for both generation and evaluation which can introduce biases; ground-truth (real papers/reviews) not perfect but treated as reference.",
            "scaling_behavior": "Designed for large-scale automated evaluation; can be extended to other domains and to larger numbers of tasks.",
            "uuid": "e4594.3",
            "source_info": {
                "paper_title": "ResearchTown: Simulator of Human Research Community",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "Researchagent",
            "name_full": "Researchagent: Iterative research idea generation over scientific literature with large language models",
            "brief_description": "A cited concurrent system that iteratively generates research ideas from scientific literature using LLMs.",
            "citation_title": "Researchagent: Iterative research idea generation over scientific literature with large language models",
            "mention_or_use": "mention",
            "system_name": "Researchagent",
            "system_description": "As cited, an approach that uses LLMs to iteratively generate research ideas over scientific literature; details are referenced but not expanded within this paper.",
            "llm_model_used": null,
            "extraction_technique": "Described as iterative literature-conditioned idea generation (LLM-driven)",
            "synthesis_technique": "Iterative idea generation over multiple documents (as cited)",
            "number_of_papers": null,
            "domain_or_topic": "Research idea generation (general/innovation)",
            "output_type": "Generated research ideas / proposals",
            "evaluation_metrics": null,
            "performance_results": null,
            "comparison_baseline": null,
            "performance_vs_baseline": null,
            "key_findings": null,
            "limitations_challenges": null,
            "scaling_behavior": null,
            "uuid": "e4594.4",
            "source_info": {
                "paper_title": "ResearchTown: Simulator of Human Research Community",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "AI Scientist",
            "name_full": "The AI Scientist: Towards fully automated open-ended scientific discovery",
            "brief_description": "A cited work that aims toward fully automated open-ended scientific discovery using autonomous systems; referenced as related research automation work.",
            "citation_title": "The ai scientist: Towards fully automated open-ended scientific discovery",
            "mention_or_use": "mention",
            "system_name": "The AI Scientist",
            "system_description": "Cited as an approach toward automated scientific discovery; details not provided in this paper beyond the citation.",
            "llm_model_used": null,
            "extraction_technique": null,
            "synthesis_technique": null,
            "number_of_papers": null,
            "domain_or_topic": "Automated scientific discovery",
            "output_type": null,
            "evaluation_metrics": null,
            "performance_results": null,
            "comparison_baseline": null,
            "performance_vs_baseline": null,
            "key_findings": null,
            "limitations_challenges": null,
            "scaling_behavior": null,
            "uuid": "e4594.5",
            "source_info": {
                "paper_title": "ResearchTown: Simulator of Human Research Community",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "Nova",
            "name_full": "Nova: An iterative planning and search approach to enhance novelty and diversity of LLM generated ideas",
            "brief_description": "A cited method that applies iterative planning and search to improve novelty/diversity of LLM-generated ideas, relevant to research idea generation.",
            "citation_title": "Nova: An iterative planning and search approach to enhance novelty and diversity of llm generated ideas",
            "mention_or_use": "mention",
            "system_name": "Nova",
            "system_description": "Cited as an iterative planning and search approach to improve LLM-generated idea novelty and diversity; the RESEARCHTOWN paper refers to it in the context of related work on enhancing generated ideas.",
            "llm_model_used": null,
            "extraction_technique": null,
            "synthesis_technique": "Iterative planning & search over LLM outputs (as cited)",
            "number_of_papers": null,
            "domain_or_topic": "Idea generation / creativity augmentation",
            "output_type": "Novel/diverse ideas",
            "evaluation_metrics": null,
            "performance_results": null,
            "comparison_baseline": null,
            "performance_vs_baseline": null,
            "key_findings": null,
            "limitations_challenges": null,
            "scaling_behavior": null,
            "uuid": "e4594.6",
            "source_info": {
                "paper_title": "ResearchTown: Simulator of Human Research Community",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "GPTSwarm",
            "name_full": "GPTSwarm: Language agents as optimizable graphs",
            "brief_description": "A cited multi-agent system that models language agents as an optimizable graph structure, relevant to multi-agent LLM graph modeling.",
            "citation_title": "GPTSwarm: Language agents as optimizable graphs",
            "mention_or_use": "mention",
            "system_name": "GPTSwarm",
            "system_description": "Cited as work that treats language agents as nodes in an optimizable graph, aligning with the idea of graph-structured multi-agent systems; specific extraction/synthesis details are not provided in this paper.",
            "llm_model_used": null,
            "extraction_technique": null,
            "synthesis_technique": "Graph-structured multi-agent interactions (as cited)",
            "number_of_papers": null,
            "domain_or_topic": "Multi-agent LLM simulation/optimization",
            "output_type": null,
            "evaluation_metrics": null,
            "performance_results": null,
            "comparison_baseline": null,
            "performance_vs_baseline": null,
            "key_findings": null,
            "limitations_challenges": null,
            "scaling_behavior": null,
            "uuid": "e4594.7",
            "source_info": {
                "paper_title": "ResearchTown: Simulator of Human Research Community",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "SOTOPIA",
            "name_full": "SOTOPIA: Interactive evaluation for social intelligence in language agents",
            "brief_description": "A cited interactive evaluation framework for social intelligence in language-agent-based multi-agent simulations.",
            "citation_title": "SOTOPIA: Interactive evaluation for social intelligence in language agents",
            "mention_or_use": "mention",
            "system_name": "SOTOPIA",
            "system_description": "Referenced as a multi-agent LLM framework for social simulation and interactive evaluation; mentioned in related work contrasting social/game multi-agent frameworks with research-community simulation.",
            "llm_model_used": null,
            "extraction_technique": null,
            "synthesis_technique": null,
            "number_of_papers": null,
            "domain_or_topic": "Social simulation / language agents",
            "output_type": null,
            "evaluation_metrics": null,
            "performance_results": null,
            "comparison_baseline": null,
            "performance_vs_baseline": null,
            "key_findings": null,
            "limitations_challenges": null,
            "scaling_behavior": null,
            "uuid": "e4594.8",
            "source_info": {
                "paper_title": "ResearchTown: Simulator of Human Research Community",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "WarAgent",
            "name_full": "War and Peace (WarAgent): Large language model-based multi-agent simulation of world wars",
            "brief_description": "A cited multi-agent simulation system using LLMs to model complex historical/game environments, used as an example of LLM multi-agent frameworks.",
            "citation_title": "War and peace (waragent): Large language model-based multi-agent simulation of world wars",
            "mention_or_use": "mention",
            "system_name": "WarAgent",
            "system_description": "Cited as an LLM-based multi-agent simulation applied to historical war simulation; used to motivate that existing multi-agent frameworks (social/game) are not directly suited for research-community simulation.",
            "llm_model_used": null,
            "extraction_technique": null,
            "synthesis_technique": null,
            "number_of_papers": null,
            "domain_or_topic": "Multi-agent simulation / historical/game domains",
            "output_type": null,
            "evaluation_metrics": null,
            "performance_results": null,
            "comparison_baseline": null,
            "performance_vs_baseline": null,
            "key_findings": null,
            "limitations_challenges": null,
            "scaling_behavior": null,
            "uuid": "e4594.9",
            "source_info": {
                "paper_title": "ResearchTown: Simulator of Human Research Community",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "MLAgentBench",
            "name_full": "MLAgentBench: Evaluating language agents on machine learning experimentation",
            "brief_description": "A cited benchmark/framework that evaluates language agents on ML experimentation tasks, referenced among research automation efforts.",
            "citation_title": "Mlagentbench: Evaluating language agents on machine learning experimentation",
            "mention_or_use": "mention",
            "system_name": "MLAgentBench",
            "system_description": "Cited as an effort evaluating language agents for ML experimentation workflows; described as focused on single-agent workflows rather than collaborative research simulation.",
            "llm_model_used": null,
            "extraction_technique": null,
            "synthesis_technique": null,
            "number_of_papers": null,
            "domain_or_topic": "Machine learning experimentation / agent evaluation",
            "output_type": null,
            "evaluation_metrics": null,
            "performance_results": null,
            "comparison_baseline": null,
            "performance_vs_baseline": null,
            "key_findings": null,
            "limitations_challenges": null,
            "scaling_behavior": null,
            "uuid": "e4594.10",
            "source_info": {
                "paper_title": "ResearchTown: Simulator of Human Research Community",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "Label-free Node Classification (Chen et al.)",
            "name_full": "Label-free node classification on graphs with large language models (LLMs)",
            "brief_description": "A cited approach using LLMs for graph node classification without traditional labels, part of related work on LLMs applied to text-attributed graphs.",
            "citation_title": "Label-free node classification on graphs with large language models (llms)",
            "mention_or_use": "mention",
            "system_name": "Label-free node classification (Chen et al.)",
            "system_description": "Cited as work that treats LLMs as tools for understanding text attributes on graphs and co-training with GNNs; referenced to contrast prior TAG research with RESEARCHTOWN's agent nodes-as-functions concept.",
            "llm_model_used": null,
            "extraction_technique": "LLM-based understanding of text attributes for graph tasks (as cited)",
            "synthesis_technique": null,
            "number_of_papers": null,
            "domain_or_topic": "Text-attributed graphs / node classification",
            "output_type": "Node labels/classifications",
            "evaluation_metrics": null,
            "performance_results": null,
            "comparison_baseline": null,
            "performance_vs_baseline": null,
            "key_findings": null,
            "limitations_challenges": null,
            "scaling_behavior": null,
            "uuid": "e4594.11",
            "source_info": {
                "paper_title": "ResearchTown: Simulator of Human Research Community",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "Explanations-as-Features",
            "name_full": "Explanations as features: LLM-based features for text-attributed graphs",
            "brief_description": "A cited work that uses LLMs to generate explanation-style features for text-attributed graphs.",
            "citation_title": "Explanations as features: Llm-based features for text-attributed graphs",
            "mention_or_use": "mention",
            "system_name": "Explanations-as-Features",
            "system_description": "Cited in related work as an example of using LLMs to extract richer textual features for graph nodes, informing the backdrop for TextGNN and agent-data graphs.",
            "llm_model_used": null,
            "extraction_technique": "LLM-based explanation/feature generation for textual graph nodes (as cited)",
            "synthesis_technique": null,
            "number_of_papers": null,
            "domain_or_topic": "Text-attributed graphs",
            "output_type": "LLM-generated explanatory features",
            "evaluation_metrics": null,
            "performance_results": null,
            "comparison_baseline": null,
            "performance_vs_baseline": null,
            "key_findings": null,
            "limitations_challenges": null,
            "scaling_behavior": null,
            "uuid": "e4594.12",
            "source_info": {
                "paper_title": "ResearchTown: Simulator of Human Research Community",
                "publication_date_yy_mm": "2024-12"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Researchagent: Iterative research idea generation over scientific literature with large language models",
            "rating": 2
        },
        {
            "paper_title": "The ai scientist: Towards fully automated open-ended scientific discovery",
            "rating": 2
        },
        {
            "paper_title": "GPTSwarm: Language agents as optimizable graphs",
            "rating": 2
        },
        {
            "paper_title": "SOTOPIA: Interactive evaluation for social intelligence in language agents",
            "rating": 1
        },
        {
            "paper_title": "Nova: An iterative planning and search approach to enhance novelty and diversity of llm generated ideas",
            "rating": 1
        },
        {
            "paper_title": "Label-free node classification on graphs with large language models (llms)",
            "rating": 1
        }
    ],
    "cost": 0.023212499999999997,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>RESEARCHTOWN: Simulator of Human Research Community</h1>
<p>Haofei Yu ${ }^{1 <em>}$ Zhaochen Hong ${ }^{1 </em>}$ Zirui Cheng ${ }^{1 <em>}$ Kunlun Zhu ${ }^{1 </em>}$<br>Keyang Xuan ${ }^{1}$ Jinwei Yao ${ }^{1}$ Tao Feng ${ }^{1}$ Jiaxuan You ${ }^{1}$<br>Code: https://github.com/ulab-uiuc/research-town<br>Data: https://huggingface.co/datasets/ulab-ai/research-bench</p>
<h4>Abstract</h4>
<p>Large Language Models (LLMs) have demonstrated remarkable potential in scientific domains, yet a fundamental question remains unanswered: Can we simulate human research communities with LLMs? Addressing this question can deepen our understanding of the processes behind idea brainstorming and inspire the automatic discovery of novel scientific insights. In this work, we propose RESEARCHTOWN, a multi-agent framework for research community simulation. Within this framework, the human research community is simplified as an agent-data graph, where researchers and papers are represented as agent-type and datatype nodes, respectively, and connected based on their collaboration relationships. We also introduce TextGNN, a text-based inference framework that models various research activities (e.g., paper reading, paper writing, and review writing) as special forms of a unified message-passing process on the agent-data graph. To evaluate the quality of the research community simulation, we present RESEARCHBENCH, a benchmark that uses a nodemasking prediction task for scalable and objective assessment based on similarity. Our experiments reveal three key findings: (1) RESEARCHTOWN can provide a realistic simulation of collaborative research activities, including paper writing and review writing; (2) RESEARCHTOWN can maintain robust simulation with multiple researchers and diverse papers; (3) RESEARCHTOWN can generate interdisciplinary research ideas that potentially inspire pioneering research directions.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h2>1. Introduction</h2>
<p>LLMs have proved to be powerful copilots in scientific research (AI4Science \&amp; Quantum, 2023), demonstrating their great potential for accelerating scientific discovery. Despite the promising finding, a more ambitious question remains: Can we simulate the human research community with LLMs? Answering such a question has multiple benefits: (1) simulating the human research community helps understand the underlying process behind the discovery of existing research ideas; (2) it can further help democratize and accelerate the discovery process of new research ideas.</p>
<p>However, simulating the human research community is challenging, as it involves leveraging multiple LLM agents to interact with complex research data. While existing multiagent LLM frameworks have been successfully applied to areas like social simulation (Zhou et al., 2023; Gao et al., 2023) and game simulation (Hua et al., 2023; Xu et al., 2023), they are not well-suited for simulating research communities due to the complexity of collaborative research activities like paper writing and review writing. Although recent efforts have explored research automation using LLMs, these frameworks are typically limited to specific research tasks, such as idea generation (Girotra et al., 2023; Baek et al., 2024) or code experimentation (Huang et al., 2024), or focus on simulating single-agent workflows (Lu et al., 2024). These frameworks cannot simulate collaborative research activities where researchers with diverse backgrounds work together to brainstorm ideas, review papers, etc-processes that are fundamental to modern human research.</p>
<p>Research community as graph. Our key observation is that the deeply interconnected research community can be naturally represented as graphs. Indeed, similar graph structures like citation networks (Newman, 2001) and academic social networks (Tang et al., 2008) have been extensively studied within data mining research, with proven values in applications such as citation prediction (Holm et al., 2020), recommendation (West et al., 2016), and community detection (Yang \&amp; Leskovec, 2012). However, introducing LLMs to a graph-structured research community can extend these previous works from prediction and analysis with existing</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Abstracting and simplifying human research community as an agent-data graph, i.e., community graph. An agent-data graph has researchers as agent nodes and blogs, codebases, and papers as data nodes. Without losing generality, we abstract it into a simplified version with only researcher and paper nodes and focus on critical research tasks, including paper reading, paper writing, and review writing. Each data node has a hidden state $h_{u}$, and each agent node is paired with an agent function $f_{v}(\cdot)$ and a hidden state $h_{v}$.
data to dynamic simulation and real-time forecasting.
Novel framework for research simulation. In this work, we propose RESEARCHTOWN, a simulator of the human research community. To bridge the gap between existing multi-agent simulation frameworks and the complexity of research activities, we propose a graph-based framework, inspired by the message-passing mechanism in Graph Neural Networks (GNNs), for multi-agent simulation. Concretely, as shown in Figure 1, we propose a new concept of agentdata graph with 2 generic types of nodes: (1) agent nodes, suitable for entities like agents; (2) data nodes, suitable for entities such as papers, reviews, and blogs. Agent-data graphs are unique from standard heterogeneous graphs; here, the key conceptual difference between agent and data nodes is that an agent node can be considered a function over data nodes. To inference on agent-data graphs, we propose a TextGNN framework where message-passing processes are defined based on text-form information processing with LLMs, thanks to their strong in-context learning (Wei et al., 2023) and reasoning (Lee et al., 2024) ability. We apply the proposed agent-data graph and TextGNN to the research simulation. Here, a research community can be regarded as a special form of agent-data graph, called community graph, with research agents and research papers as two types of nodes, and we consider three types of edges (review, author, and cite) in the graph. Different community activities, such as paper writing and review writing, can be modeled as special message-passing processes on the community graph.</p>
<p>Novel evaluation for research simulation. With RESEARCHTOWN for research simulation, a further research question is to evaluate the quality of that. Prior works primarily use human evaluation with breakdown metrics such as novelty, excitement, feasibility, and expected effectiveness (Si et al., 2024; Hu et al., 2024b). These approaches
inevitably suffer from subjectiveness and high costs. In our work, since RESEARCHTOWN functions as a simulator, our primary focus is on measuring how closely its outputs align with those of the real-world research community. Community graphs naturally provide a similarity-based evaluation method by masking a given paper node in the community graph and evaluating whether a simulator can reconstruct the masked nodes. This definition focuses on simulation similarity, making it scalable and objective. Based on such a node masking prediction task, we build a benchmark called RESEARCHBENCH with 1,000 paper writing tasks and 200 review writing tasks requiring multi-agent collaboration.</p>
<p>Main discoveries. Based on the evaluation results from RESEARCHBENCH, we highlight three key findings: (1) RESEARCHTOWN effectively simulates collaborative research activities, achieving an average similarity score of 0.68 for paper writing and 0.49 for review writing, as measured by the state-of-the-art text embedding model; (2) RESEARCHTOWN demonstrates robustness and effectiveness in research simulation, showing improvement when more agents are added and maintaining performance when including unrelated papers; (3) RESEARCHTOWN inspires interdisciplinary research, generating innovative ideas that combine insights from NLP, criminology, and astronomy and does not exist in the real-world research.</p>
<p>Stressing ethical concerns. As our work targets simulating the human research community, multiple ethical concerns, including facilitating research plagiarism and producing low-quality or misleading claims, appear. These ethical concerns are addressed in detail in Appendix §B.</p>
<h2>2. Additional Related Work</h2>
<p>Graphs with text attributes. In real-world graph tasks, nodes often have textual attributes to carry richer informa-</p>
<p>tion, forming text-attributed graphs (TAGs) (Yang et al., 2021; He et al., 2023). Previous work on TAGs mainly treats LLMs as tools for understanding text attributes and aims at achieving co-training LLMs and GNNs (Zhao et al., 2023; Chen et al., 2024). In contrast, our approach incorporates agent nodes into the graph, enabling text-based message passing between agent nodes and data nodes. Furthermore, while previous TAG research mainly focuses on node prediction and link prediction tasks (Yan et al., 2023), RESEARCHTOWN extends it to both the reconstruction of existing nodes and the prediction of new, non-existent nodes.</p>
<p>Graphs for multi-agent modeling. Recent works model multi-agent communication using graphs and develop learnable methods to optimize the communication process (Zhuge et al., 2024; Martinkus et al., 2023; Hu et al., 2024a). However, these works often neglect the interactive nature of data, where agents can read, write, and update shared data iteratively. Currently, few works include a welldefined framework to represent graphs that integrate both agents and their associated data.</p>
<h2>3. Agent-Data Graph for Multi-agent LLMs</h2>
<p>Definition of agent-data graphs. To initiate our discussion, we formally define the proposed agent-data graph. An agentdata graph is a special type of heterogeneous graph $\mathcal{G}=$ $(\mathcal{V}, \mathcal{E})$, where $\mathcal{V}=\mathcal{V}<em d="d">{a} \cup \mathcal{V}</em>}$ is the node set consisting of two types of nodes, agent nodes and data nodes, and $\mathcal{E}=$ $\mathcal{E<em a="a" d="d">{a a} \cup \mathcal{E}</em>} \cup \mathcal{E<em d="d">{d d}$ is the edge set consisting of three types of relations, agent-agent, data-data, and agent-data interactions. Here, each data node $v \in \mathcal{V}</em>}$ comes with attributes, e.g., a piece of text, $\mathbf{x<em u="u">{v}$; each agent node $u$ is accompanied with an agent function, e.g., an LLM $f</em>(\cdot)$ with its prompt template and the profile. Each agent function is responsible for two types of tasks: message generation and message aggregation. More details about agent functions are in Appendix §D.3. Without loss of generality, we assume that the data nodes have text attributes, and leave the multi-modal extension of our work, e.g., images, audio, and videos, to future works.</p>
<p>Uniqueness of agent-data graphs. Unlike standard heterogeneous graphs, the uniqueness of an agent-data graph is that the agent nodes take functions as their attributes, rather than embeddings. Concretely, each agent node could take a piece of text, e.g., $\mathbf{x}<em u="u">{v}$ from one data node, as the input and output new data based on its profile prompt $\mathbf{x}</em>}$, e.g., $\mathbf{x<em u="u">{u v}=f</em>}\left(\left[\mathbf{x<em v="v">{u}, \mathbf{x}</em>}\right]\right)$ where $[\cdot]$ indicates filling the prompt template with $\mathbf{x<em v="v">{u}$ and $\mathbf{x}</em>}$. Such definition greatly facilitates the multi-agent scenarios where agents could communicate among themselves, with edge type $\mathcal{E<em a="a" d="d">{a a}$; interacting with the environment, with edge type $\mathcal{E}</em>$.}$; representing the inherent data relationships within an environment $\mathcal{E}_{d d</p>
<p>Example of agent-data graphs. Figure 1 shows an example of the agent-data graph. Its definition could be extended
to more node types (e.g., codebase, blogs) and edge types (e.g., attend, post, commit). Typically, one blog post can be directly connected to multiple researchers, papers, and other blog posts if they are related to each other.</p>
<h2>4. Building TextGNN on Agent-Data Graphs</h2>
<p>TextGNN motivations. The agent-data graph $\mathcal{G}$ provides a platform for expressing a complex multi-agent scenario, e.g., a human research community. To further simulate based on a given real-world agent-data graph, we need agentic models, e.g., LLMs, to generate new data and interactions on the agent-data graph. To this end, motivated by the message-passing algorithm in GNNs, we proposed a textbased message-passing mechanism on an agent-data graph, called TextGNN, where all hidden states are defined in the text space instead of the embedding space.</p>
<p>Recap: message passing in standard GNN. In standard GNNs, input features $\mathbf{x}<em v="v">{v}$ are used to initialize the initial states $\mathbf{x}</em>}=\mathbf{h<em v="v">{v}^{(0)}$. Afterward, the goal is to learn useful node embeddings $\mathbf{h}</em>$ by iteratively aggregating information from local neighborhoods. Hidden states, message functions, and aggregation functions are the three main components in one GNN layer. The $k$-th iteration of message passing (or the $k$-th GNN layer) is typically defined as:</p>
<p>$$
\begin{gathered}
\mathbf{m}<em u="u">{u}^{(k)}=\operatorname{MSG}^{(k)}\left(\mathbf{h}</em>\right) \
\mathbf{h}}^{(k-1)<em v="v">{v}^{(k)}=\operatorname{AGG}^{(k)}\left(\mathbf{h}</em>(v)\right}\right)
\end{gathered}
$$}^{(k-1)},\left{\mathbf{m}_{u}^{(k)} \mid u \in \mathcal{N</p>
<p>where $\mathbf{h}<em v="v">{v}^{(k)}$ is the node embedding at the $k$-th layer, $\mathbf{h}</em>(\cdot)$ is defined to update the hidden states of a node based on neighborhood messages. More generally, we can broadly consider the $k$-th layer of GNN to be an aggregation function that implicitly includes message functions inside:}^{(0)}=$ $\mathbf{x}_{v}$ is the initial node feature, and $\mathcal{N}(v)$ is the set of neighbors of node $v . \operatorname{MSG}^{(k)}(\cdot)$ is a transformative function to convert the hidden states of one node into a message for aggregation. $\operatorname{AGG}^{(k)</p>
<p>$$
\mathbf{h}<em v="v">{v}^{(k)}=\operatorname{AGG}^{(k)}\left(\mathbf{h}</em>(v)\right}\right)
$$}^{(k-1)},\left{\mathbf{h}_{u}^{(k-1)} \mid u \in \mathcal{N</p>
<p>Message passing in TextGNN. Following the messagepassing process in the standard GNN, we now define a general form of the aggregation function to describe the textbased message-passing process on an agent-data graph $\mathcal{G}$. The key difference between a standard GNN and a TextGNN is that all hidden states in the standard GNN are defined in the embedding space $\left(\mathbf{h}<em v="v">{v} \in \mathbb{R}^{d}\right)$ while those in TextGNN are defined in the text space $\left(\mathbf{h}</em>} \in \Sigma^{*}\right)$. In a TextGNN, we first set the initial hidden states for data nodes $\mathbf{h<em v="v">{v}^{(0)}=\mathbf{x}</em>}$, where $\mathbf{x<em u="u">{v}$ are text attributes, and the initial hidden states for agent nodes is empty $\mathbf{h}</em>=\emptyset$. Next, we design a general form of message passing function that handles three}^{(0)</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: RESEARCHTOWn simulation as TextGNN inference on the community graph. The simulation proceeds in three stages: (1) paper reading, where new agent nodes are added based on existing data; (2) paper writing, where data nodes are created; (3) review writing, where the community evaluates and selectively removes (or retains) generated nodes.
distinctive types of interaction, including agent-agent $\mathcal{E}<em a="a" d="d">{a a}$, agent-data $\mathcal{E}</em>}$, and data-data $\mathcal{E<em a="a">{d d}$.
Specifically, the $k$-th TextGNN layer for an agent node $u \in \mathcal{V}</em>$ can be written as:</p>
<p>$$
\begin{aligned}
\mathbf{h}<em u="u">{u}^{(k)}= &amp; \operatorname{AGG}^{(k)}\left(f</em>}(\cdot), \mathbf{h<em d="d">{u}^{(k-1)},\left{\mathbf{h}</em>}^{(k-1)} \mid(u, d) \in \mathcal{E<em a="a">{a d}\right},\right. \
&amp; \left.\left{f</em>}(\cdot), \mathbf{h<em a="a">{u}^{(k-1)} \mid(u, a) \in \mathcal{E}</em>\right}\right) \
= &amp; f_{u}\left(\left[\mathbf{h}<em a="a">{u}^{(k-1)},\left{f</em>}\left(\left[\mathbf{h<em u="u">{a}^{(k-1)}, \mathbf{h}</em>}^{(k-1)}, \mathbf{h<em a="a">{d}^{(k-1)}\right]\right)\right.\right.\right. \
&amp; \left.\left.(u, a) \in \mathcal{E}</em>\right}\right]\right)
\end{aligned}
$$},(u, d) \in \mathcal{E}_{a d</p>
<p>where $[\cdot]$ is the concatenation function between texts to fill in the prompt template, $\mathbf{h}<em a="a">{u}^{(k)}$ represents the hidden states of the $k$-th layer of $v \in \mathcal{V}, f</em>(\cdot)$ represents the agent function paired with the agent node.
Similarly, the forwarding process of the $k$-th TextGNN layer for a data node $v \in \mathcal{V}_{d}$ can be written as:}(\cdot)$ represents the agent function paired with the agent node in the neighborhood and $f_{u</p>
<p>$$
\begin{aligned}
\mathbf{h}<em v="v">{u}^{(k)}= &amp; \operatorname{AGG}^{(k)}\left(\mathbf{h}</em>}^{(k-1)},\left{\mathbf{h<em d="d">{d}^{(k-1)} \mid(v, d) \in \mathcal{E}</em>\right},\right. \
&amp; \left.\left.\left{f_{a}(\cdot), \mathbf{h}<em a="a" d="d">{u}^{(k-1)} \mid(v, a) \in \mathcal{E}</em>\right}\right)\right. \
= &amp; f_{g}\left(\left[\mathbf{h}<em a="a">{v}^{(k-1)},\left{f</em>}\left(\left[\mathbf{h<em v="v">{a}^{(k-1)}, \mathbf{h}</em>}^{(k-1)}, \mathbf{h<em a="a" d="d">{d}^{(k-1)}\right]\right)\right.\right.\right. \
&amp; \left.\left.(v, a) \in \mathcal{E}</em>\right}\right])
\end{aligned}
$$},(v, d) \in \mathcal{E}_{d d</p>
<p>where $f_{g}(\cdot)$ is defined as a global agent function without a specialized profile, and $f_{a}(\cdot)$ is the agent function paired with the agent node in the neighborhood.</p>
<h2>5. RESEARCHTOWN: Applying TextGNN to Community Graph</h2>
<p>Inputs and outputs of RESEARCHTOWN. Building on the definitions of TextGNN and the agent-data graph in Section $\S 3$ and Section $\S 4$, we simulate different research activities by modeling each as a specific instantiation of a TextGNN layer. RESEARCHTOWn processes diverse research materials and produces structured outputs. The input
varies by task: only paper abstracts are used for paper reading and writing, while full papers are provided for review writing. The output format is also task-specific: paper reading generates profile descriptions, paper writing generates bullet-point summaries, and review writing produces bulletpoint critiques along with a numerical review score. These standardized output formats-described in more detail in Appendix $\S \mathrm{F}$-facilitate evaluation over long-context inputs and enable fine-grained, sub-component similarity scoring.</p>
<p>Hidden states of RESEARCHTOWN. In RESEARCHTOWN, the hidden state of each node represents a condensed version of research materials, such as papers or reviews. Initially, paper nodes are initialized with the full text of papers. Through iterative message passing, these nodes gradually evolve into a standardized bullet-point format, distilling key information for easier downstream evaluation. Similarly, review attributes associated with paper nodes are also represented using bullet points to make it in a compact form. Bulletpoint compact form with limited length allows TextGNN to conduct message passing multiple times efficiently.</p>
<p>Agent-data graph for research community modeling community graph. We adopt the agent-data graph $\mathcal{G}=$ $(\mathcal{V}, \mathcal{E})$ to research community simulation, which we named as community graph. As is shown in Figure 2, each agent node $\mathcal{V}<em d="d">{a}$ represents one researcher, and each data node $\mathcal{V}</em>}$ represents a paper. The edge set $\mathcal{E<em a="a" d="d">{d d}$ captures paper citations, the edge set $\mathcal{E}</em>}$ captures authorship (a researcher writes a paper) and reviewing expertise (a researcher is qualified to review a paper). We omit the edge set $\mathcal{E<em a="a" d="d">{a a}$ to simplify the framework, as a collaboration between authors can typically be inferred through 2-hop paths via $\mathcal{E}</em>$ edges.</p>
<p>TextGNN for research activity simulation. Based on the constructed community graph, we further identify the key types of research activities where TextGNN can be used for simulation. Specifically, as shown in Figure 2, we split the research simulation process into three critical stages: (1) paper reading, (2) paper writing, and (3) review writing. We</p>
<p>believe these stages are crucial in the research community, and each stage relies on the output of the previous stage as input. We provide a detailed description for each stage and the corresponding TextGNN layer definition below.
$\triangleright$ Stage 1: Paper reading. Reading papers to collect insights is a necessary process for initializing a research project. In the community graph, the paper reading process can be described as inserting a new agent node to the community graph and aggregating its neighborhood information based on Equation 4. Here, the new agent profile is non-existent before reading a collection of papers, and the profile is created after the paper reading process, making the TextGNN layer unique. Concretely, by adapting Equation 4, the TextGNN layer for paper reading can be written as:</p>
<p>$$
\begin{aligned}
\mathbf{h}<em u="u">{u} &amp; =\operatorname{AGG}\left(f</em>}(\cdot),\left{\mathbf{h<em a="a" d="d">{d} \mid(u, d) \in \mathcal{E}</em>\right}\right) \
&amp; =f_{u}\left(\left[\left{\mathbf{h}<em a="a" d="d">{d} \mid(u, d) \in \mathcal{E}</em>\right}\right]\right)
\end{aligned}
$$</p>
<p>where $\mathbf{h}<em u="u">{u},\left{f</em>}(\cdot), \mathbf{h<em a="a">{a} \mid(u, a) \in \mathcal{E}</em>$ specifically refers to the authorship relation between agent and data nodes. Equation 4 degrades to an aggregation of papers based on the researcher agent without the profile, illustrated in Figure 2 "Stage 1".
$\triangleright$ Stage 2: Paper writing. After paper reading, the next important research stage is paper writing. Different from paper reading, the paper writing process can be understood as inserting a new data node into the community graph. Here, the new data node is non-existent before writing the paper, and the data node is created after the paper writing process. Concretely, by adapting Equation 5, the TextGNN layer for paper writing can be written as:}\right}$ in Equation 4 are empty since the agent node is initialized as empty and is not directly connected with any agents, and $\mathcal{E}_{a d</p>
<p>$$
\begin{aligned}
\mathbf{h}<em a="a">{v} &amp; =\operatorname{AGG}\left(\left{f</em>}(\cdot),\left{\mathbf{h<em d="d">{d} \mid(v, d) \in \mathcal{E}</em>}\right}, \mathbf{h<em a="a" d="d">{a} \mid(v, a) \in \mathcal{E}</em>\right}\right) \
&amp; =f_{g}\left(\left[\left{f_{a}\left(\left[\mathbf{h}<em d="d">{a}, \mathbf{h}</em>}\right]\right) \mid(v, a) \in \mathcal{E<em d="d">{a d},(v, d) \in \mathcal{E}</em>\right}\right]\right)
\end{aligned}
$$</p>
<p>where $\mathbf{h}<em a="a" d="d">{v}$ in Equation 5 is empty since paper node contents are non-existent before paper writing; $\mathcal{E}</em>}$ specifically refers to authorship relations between agent and data nodes, and $\mathcal{E<em v="v">{d d}$ refers to citation relations within data nodes. A visualization of Equation 7 is shown in Figure 2 "Stage 2".
$\triangleright$ Stage 3: Review writing. The review writing task is the final stage of the automatic research simulation, serving as a reflection stage in the multi-agent research simulator. The difference between the previous 2 stages is that, first, the researchers involved during review writing are not the authors but the reviewers of the paper. Additionally, review writing is based on a written paper where $\mathbf{h}</em>$ is no longer empty. Concretely, by adapting Equation 5, the TextGNN layer for review writing can be written as:</p>
<p>$$
\begin{aligned}
\mathbf{r}<em v="v">{v} &amp; =\operatorname{AGG}\left(\mathbf{h}</em>},,\left{\mathbf{h<em d="d">{d} \mid(v, d) \in \mathcal{E}</em>}\right}\left{f_{a}(\cdot), \mathbf{h<em a="a" d="d">{a} \mid(v, a) \in \mathcal{E}</em>\right}\right) \
&amp; =f_{g}\left(\left[\mathbf{h}<em a="a">{v},\left{f</em>}\left(\left[\mathbf{h<em v="v">{a}, \mathbf{h}</em>}, \mathbf{h<em a="a" d="d">{d}\right]\right) \mid(v, a) \in \mathcal{E}</em>\right}\right]\right)
\end{aligned}
$$},(v, d) \in \mathcal{E}_{d d</p>
<p>$\triangleright$ Summary: RESEARCHTOWN simulation algorithm. Utilizing the community graph $\mathcal{G}$, we propose a simulation algorithm named as RESEARCHTOWN. Overall, the simulation algorithm can be considered as a 2-layer GNN where the paper reading is the first layer of information aggregation. Both paper writing and review writing are the second layer of the GNN to generate the final simulation outputs. We formally summarize the research community simulation in Algorithm 1. To achieve better efficiency, the modified version for implementation is in Appendix §D.2.</p>
<div class="codehilite"><pre><span></span><code>Algorithm 1 RESEARCHTOWN simulation algorithm
Require: community graph \(\mathcal{G}(\mathcal{V}, \mathcal{E})\)
        paper contents \(\mathbf{x}_{v}\) for all paper nodes,
        target paper node \(v\)
Ensure: paper content \(\mathbf{h}_{v}\) and review content \(\mathbf{r}_{v}\) for paper node \(v\)
    for each \(u \in \mathcal{N}(v)\) do
        if \(u \in \mathcal{V}_{d}\) then
            \(\mathbf{h}_{u} \leftarrow \mathbf{x}_{u}\)
        else
            \(\mathbf{h}_{v} \leftarrow f_{u}\left(\left[\left\{\mathbf{x}_{d} \mid(u, d) \in \mathcal{E}_{a d}\right\}\right]\right) \quad \triangleright\) Eq. (6)
        end if
    end for
    \(\mathbf{h}_{v} \leftarrow f_{g}\left(\left[\left\{f_{a}\left(\left[\mathbf{h}_{a}, \mathbf{h}_{d}\right]\right)\right.\right.\right.\)
    ( \(v, a) \in \mathcal{E}_{a d},(v, d) \in \mathcal{E}_{d d}\right\}\right] \quad \triangleright\) Eq. (7)
    \(\mathbf{r}_{v} \leftarrow f_{g}\left(\left[\mathbf{h}_{v},\left\{f_{a}\left(\left[\mathbf{h}_{a}, \mathbf{h}_{v}, \mathbf{h}_{d}\right]\right)\right.\right.\right.\)
    ( \(\left.\left.v, a\right) \in \mathcal{E}_{a d},(v, d) \in \mathcal{E}_{d d}\right\}\right] \quad \triangleright\) Eq. (8)
    return \(\mathbf{h}_{v}, \mathbf{r}_{v}\)
</code></pre></div>

<h2>6. Evaluating RESEARCHTOWN via Masked Node Prediction Task</h2>
<p>Utilizing graph structures not only enables the design of the research simulation algorithm but also provides a natural way to evaluate it. As we show next, we propose to view research evaluation as a masked node prediction task, including evaluation for both paper writing and review writing.
Evaluation by masked node prediction. A masked node prediction task in the community graph $\mathcal{G}$ can be defined as first masking a specific node $v \in \mathcal{V}$ in the community graph by setting its hidden states $\mathbf{h}<em v="v">{v}=\emptyset$, where the original hidden state is saved as $\mathbf{h}</em>^{<em>}$; then an ideal model should be able to predict the hidden states $\mathbf{h}_{v}^{</em>}$ of the masked node from its neighborhood $\mathcal{N}(v)$. Concretely, in Equation 7, the output $\mathbf{h}<em v="v">{v}$ can be regarded as the masked node prediction for evaluation of paper writing, suppose that the node $v$ is a masked version of a ground truth data node. Similarly, in Equation 8, the output $\mathbf{r}</em>$ can be regarded</p>
<p>as the predicted node attributes for review writing, where the original review is represented as $\mathbf{r}_{v}^{*}$. In general, we have:</p>
<p>$$
\mathbf{h}<em v="v">{v}, \mathbf{r}</em>(v)\right} ; v)
$$}=\operatorname{RESEARCHTOWN}(\mathcal{G}(\mathcal{V}, \mathcal{E}) ;\left{\mathbf{x}_{u} \mid u \in \mathcal{N</p>
<p>where $\mathbf{h}<em v="v">{v}$ is the text-form hidden states of a masked node $v$ and $\mathbf{r}</em>}$ is the text-form prediction output of a masked node $v$. Since we have real-world results for both paper writing and review, we treat them as ground truth even though they are not perfect because the goal of RESEARCHTOWN is to simulate the human research community rather than to find optimal solutions for papers and reviews ( $\mathbf{h<em v="v">{v}^{<em>}$ for paper ground-truth and $\mathbf{r}_{v}^{</em>}$ for review ground-truth) and we can systematically evaluate both processes to check the effectiveness of our simulation algorithm. More specifically, since we have access to ground-truth papers $\mathbf{h}</em>^{<em>}$ when evaluating the review writing simulation, to avoid accumulated errors, we update Equation 8 during evaluation so that reviews $\mathbf{r}<em v="v">{v}$ are generated based on $\mathbf{h}</em>^{</em>}$, instead of $\mathbf{h}<em v="v">{v}$ :
$\mathbf{r}</em>}=\operatorname{AGG}\left(\mathbf{h<em r="r">{v}^{<em>},\left{\mathbf{h}<em d="d">{d} \mid(v, d) \in \mathcal{E}</em>}\right},\left{f_{a}(\cdot), \mathbf{h<em a="a" d="d">{a} \mid(v, a) \in \mathcal{E}</em>\right}\right)$
Evaluation metric. We utilize state-of-the-art embedding models like text-embedding-large-3 ${ }^{1}$ to build distance function for $d_{p}\left(\mathbf{h}<em v="v">{v}, \mathbf{h}</em>^{</em>}\right)$ and $d</em>}\left(\mathbf{r<em v="v">{v}, \mathbf{r}</em>\right)$. More details related to formal embedding-based metric definitions for paper writing and review writing tasks are available in Appendix §F.}^{*</p>
<h2>7. Experimental Settings</h2>
<p>RESEARCHTOWN setting. We utilize GPT-4o-mini ${ }^{2}$ as the LLM backbone for implementing the agent functions, with the decoding temperature set to 0 to ensure reproducibility. To evaluate different aggregation strategies, we conduct experiments using specific types of nodes connected to the target node: (1) $A G G$-self, where the aggregation relies solely on the target node; (2) $A G G$-agent, which includes the target node and its neighboring agent nodes; (3) $A G G$ data, which involves the target node and its neighboring data nodes; and (4) $A G G$-global, which incorporates the target node and all its neighboring nodes, including agent and data nodes. We specifically refer to $A G G$-global as our proposed RESEARCHTOWN method for simulation, while the others serve as baselines. This experimental design enables a systematic comparison of the effects of different neighborhood information on the aggregation process. More details about different settings are available in Appendix §D.1.</p>
<p>RESEARCHBENCH setting. To evaluate RESEARCHTOWN for research simulation, we introduce RESEARCHBENCH,</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Table 1: Evaluation results for paper writing simulation. Hard, Medium, and Easy correspond to three subsets of the paper writing tasks with different difficulties, while Overall refers to the performance across all parts. Text-embedding-large-3 is used to build embedding-based similarity metrics. Comprehensive results are available in Appendix §J.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">AGG Type</th>
<th style="text-align: center;">Easy $\uparrow$</th>
<th style="text-align: center;">Medium $\uparrow$</th>
<th style="text-align: center;">Hard $\uparrow$</th>
<th style="text-align: center;">Overall $\uparrow$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">AGG-self</td>
<td style="text-align: center;">46.42</td>
<td style="text-align: center;">45.92</td>
<td style="text-align: center;">45.90</td>
<td style="text-align: center;">46.08</td>
</tr>
<tr>
<td style="text-align: left;">AGG-agent</td>
<td style="text-align: center;">56.90</td>
<td style="text-align: center;">55.55</td>
<td style="text-align: center;">53.26</td>
<td style="text-align: center;">55.24</td>
</tr>
<tr>
<td style="text-align: left;">AGG-data</td>
<td style="text-align: center;">$\underline{74.36}$</td>
<td style="text-align: center;">66.42</td>
<td style="text-align: center;">56.02</td>
<td style="text-align: center;">65.30</td>
</tr>
<tr>
<td style="text-align: left;">AGG-global</td>
<td style="text-align: center;">73.79</td>
<td style="text-align: center;">$\underline{67.85}$</td>
<td style="text-align: center;">$\underline{60.89}$</td>
<td style="text-align: center;">$\underline{67.51}$</td>
</tr>
</tbody>
</table>
<p>Table 2: Evaluation results for review writing simulation. For strength and weakness, it shows embedding-based similarity results. We use text-embedding-large-3 as embedding models and select 5 reviewers for running $A G G$-agent and $A G G$-global. $\Delta \mathbf{S}$ refers to the average difference of review scores between real-world ones and generated ones. $\overline{\mathbf{S}}$ refers to the average scores of generated ones. Comprehensive results are available in Appendix §J.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">AGG Type</th>
<th style="text-align: center;">Strength $\uparrow$</th>
<th style="text-align: center;">Weakness $\uparrow$</th>
<th style="text-align: center;">$\Delta \mathbf{S} \downarrow$</th>
<th style="text-align: center;">$\overline{\mathbf{S}}$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">AGG-self</td>
<td style="text-align: center;">51.23</td>
<td style="text-align: center;">47.16</td>
<td style="text-align: center;">1.27</td>
<td style="text-align: center;">5.33</td>
</tr>
<tr>
<td style="text-align: left;">AGG-agent</td>
<td style="text-align: center;">$\underline{51.66}$</td>
<td style="text-align: center;">46.75</td>
<td style="text-align: center;">$\underline{1.19}$</td>
<td style="text-align: center;">5.40</td>
</tr>
<tr>
<td style="text-align: left;">AGG-data</td>
<td style="text-align: center;">51.45</td>
<td style="text-align: center;">$\underline{47.62}$</td>
<td style="text-align: center;">1.26</td>
<td style="text-align: center;">5.30</td>
</tr>
<tr>
<td style="text-align: left;">AGG-global</td>
<td style="text-align: center;">51.51</td>
<td style="text-align: center;">47.17</td>
<td style="text-align: center;">1.55</td>
<td style="text-align: center;">5.00</td>
</tr>
</tbody>
</table>
<p>which consists of 1,000 paper writing tasks and 200 review writing tasks. All tasks are sourced from recent top-tier machine learning conferences such as NeurIPS $2024^{3}$ and ICLR $2024^{4}$. Since most papers are released after the cutoff date of GPT-4o-mini, information leakage is not considered an issue. For paper writing tasks, we categorize them into three difficulty levels-hard (333 tasks), medium (334 tasks), and easy ( 333 tasks) —based on the similarity results of data-only aggregation. Specifically, for review writing tasks, the reviewers prepared for each paper are selected from the top 5 researchers most related to the paper, as reviewer information is not publicly available in the real world. More details about the data collection and prevention of information leakage during simulation are in Appendix §E.</p>
<h2>8. Core Results: In-distribution Evaluation</h2>
<p>In this section, we present the main results of our research simulation on RESEARCHBENCH, including 1,000 paper writing tasks and 200 review writing tasks. We evaluate existing paper nodes that have fully known their content and their neighborhoods within the community graph. We refer to these scenarios as in-distribution cases.</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Ablation study on paper number. We select different sub-parts of cited papers in paper writing tasks.
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Ablation study on agent number. We select different numbers of agents for paper writing tasks.
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Ablation study on agent number. We select different numbers of agents for review writing tasks.</p>
<p>Overall: RESEARCHTOWN can provide a realistic simulation of research activity. To evaluate research simulation, we utilize state-of-the-art embedding models (text-embedding-3-large) to compare the semantic similarity between simulated results and real-world results. For paper writing, as shown in Table 1, the overall similarity score obtained using text-embedding-3-large across 1,000 papers is 67.51 . Notably, the score increases to 73.79 for an easy subset of the benchmark. These results demonstrate that paper writing with RESEARCHTOWN can produce realistic outputs compared to real-world ones. Moreover, it indicates that some ideas in top-tier conference papers are not hard to think of and can be imagined by LLMs. For review writing, as shown in Table 2, the similarity scores are generally lower compared with paper writing, with strength-related scores averaging around 51 and weakness-related scores averaging around 47. This suggests that review writing is more challenging to generate with RESEARCHTOWN, particularly for weakness identification. A possible explanation is that real-world review data is often noisier and more diverse, making it harder to simulate accurately.</p>
<p>Paper writing: participation of multi-researchers improves paper quality. As shown in Table 1, cited papers contribute more effectively than authors in the paper writing simulation, with data-aggregation achieving a score of 65.30 compared to 55.24 for agent-aggregation. The best results are obtained by combining both, surpassing data aggregation by 2.21 points. Researchers are particularly beneficial under difficult scenarios, improving the text-embedding-large-3 score from 56.02 to 60.89 , likely due to the inclusion of multi-hop paper information from researchers.</p>
<p>Review writing: participation of multi-reviewers improves review quality. Unlike paper writing, review writing mainly relies on the paper that needs to be reviewed, making reviewers and cited papers less impactful, with differences limited to within 1 point. However, as shown in Table 2, adding additional information consistently improves performance over the self-aggregation baseline. Agent aggregation performs best for writing strengths and assigning scores, while data aggregation achieves the best results for
writing weaknesses. This pattern likely reflects the role of related work comparisons in highlighting weaknesses, while multiple reviewers help provide a more balanced assessment of strengths. Interestingly, global aggregation leads to larger differences in scores. We consider it an exception since GPT-4o-mini tends to apply stricter novelty judgments under global aggregation-its average assigned score drops from 5.3 to 5.0. As shown in Table 3, this effect is not observed for Qwen-2.5-7B-Instruct or Deepseek-v3, which gain better results with global aggregation.</p>
<h2>9. Ablation Study:RESEARCHTOWN is Robust</h2>
<p>We conduct ablation studies on both hyperparameters and model selection. The results show that RESEARCHTOWN consistently produces high-quality simulations across a range of settings, demonstrating strong robustness. Detailed experimental configurations are provided in Appendix §I.</p>
<p>Ablation on paper number. In paper writing tasks, users can freely assign papers to simulate non-existent work, making robustness to the number of papers essential. As shown in Figure 3, papers cited in the related work section have the greatest positive impact, increasing the similarity score from 66.4 to 66.7 compared to using all papers. In contrast, using only papers cited in the introduction lowers the score to 65.2 , while including papers from other sections reduces it further to 58.4 . These results highlight the importance of selecting informative references when generating papers. In review writing, the number of papers is fixed, so no ablation study on the paper number is applicable.
Ablation on agent number. For RESEARCHTOWN simulation, users can assign different numbers of agents, making robustness to agent number critical for RESEARCHTown. In Figure 4, in the paper writing task, increasing the agent number improves simulation quality under the agent-aggregation setting. The most notable gain occurs when increasing from 1 to 2 , boosting the similarity score from 49.0 to 52.7. Similar trends hold in review writing (Figure 5), where increasing the agent number consistently enhances output quality. The strength score improves from 50.8 to 51.5 when increasing the reviewer from 1 to 5 .</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: Fine-grained similarity evaluation using LLM-as-a-judge for paper writing simulation. We use GPT-4o as the evaluator, prompting it to score each dimension on a scale from 0 to 10. The first five dimensions assess specific aspects of similarity, while the final score (overall similarity) represents an overall score as judged by the LLM.</p>
<p>Ablation on generation models. The choice of LLMs significantly impacts simulation quality. In addition to GPT-4o-mini, we evaluate two models from different families: Qwen-2.5-7B-Instruct ${ }^{1}$ and Deepseek-v3 ${ }^{2}$. In Table 3, for both paper writing tasks, global aggregation (RESEARCHTOWN) consistently yields the highest similarity scores across all models. It also achieves the best review difference scores for Qwen-2.5-7B-Instruct and Deepseekv3. The only exception is GPT-4o-mini, which shows an unexpected increase in review difference under AGG-global. Overall, Deepseek-v3 outperforms GPT-4o-mini, which in turn outperforms Qwen-2.5-7B-Instruct—consistent with their relative performance on other tasks.</p>
<p>Ablation on embedding models. Similarity scores can be computed using different models, and voyage-3 ${ }^{3}$ serves as an alternative to the text-embedding-3-large used in our main experiments. As shown in Figures 3, 4, and 5, voyage3 produces consistent trends in ablation studies involving the number of papers and agents. This consistency suggests that RESEARCHTOWN is robust to the choice of embedding model, and different models lead to the same conclusions.</p>
<h2>10. Discussion: RESEARCHTOWN is Effective</h2>
<p>Besides computing embedding-based similarities, we provide more types of evaluations here. First, we prompt LLMs to calculate fine-grained similarity scores that assess consistency between real-world data and simulated ones across various dimensions. Next, we evaluate the intrinsic quality of the simulated outputs themselves and compare them with real-world data. Finally, we report results from human evaluations to validate the alignment between LLM-based evaluation and human judgments. More details about LLMbased evaluation are available in Appendix §G, and details about human evaluation are available in Appendix §H.</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Table 3: Comparison of simulation results with different generation models. For Qwen, we refer to Qwen-2.5-7BInstruct. For GPT, we refer to GPT-4o-mini. For DS, we refer to Deepseek-v3. For paper writing metrics, we utilize the overall similarity. For review writing metrics, we use $\Delta \mathbf{S}$ to represent its review alignment with the real world.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">AGG Type</th>
<th style="text-align: center;">Paper Writing</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Review Writing</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Qwen</td>
<td style="text-align: center;">GPT</td>
<td style="text-align: center;">DS</td>
<td style="text-align: center;">Qwen</td>
<td style="text-align: center;">GPT</td>
<td style="text-align: center;">DS</td>
</tr>
<tr>
<td style="text-align: center;">AGG-self</td>
<td style="text-align: center;">46.45</td>
<td style="text-align: center;">46.08</td>
<td style="text-align: center;">48.62</td>
<td style="text-align: center;">1.36</td>
<td style="text-align: center;">1.27</td>
<td style="text-align: center;">1.11</td>
</tr>
<tr>
<td style="text-align: center;">AGG-agent</td>
<td style="text-align: center;">53.91</td>
<td style="text-align: center;">55.24</td>
<td style="text-align: center;">56.19</td>
<td style="text-align: center;">1.41</td>
<td style="text-align: center;">1.19</td>
<td style="text-align: center;">1.05</td>
</tr>
<tr>
<td style="text-align: center;">AGG-data</td>
<td style="text-align: center;">65.03</td>
<td style="text-align: center;">65.30</td>
<td style="text-align: center;">65.05</td>
<td style="text-align: center;">1.28</td>
<td style="text-align: center;">1.26</td>
<td style="text-align: center;">1.07</td>
</tr>
<tr>
<td style="text-align: center;">AGG-global</td>
<td style="text-align: center;">65.30</td>
<td style="text-align: center;">67.51</td>
<td style="text-align: center;">65.33</td>
<td style="text-align: center;">0.79</td>
<td style="text-align: center;">1.51</td>
<td style="text-align: center;">0.81</td>
</tr>
</tbody>
</table>
<p>Automatic evaluation on fine-grained similarity. A high cosine similarity score alone can mask important issues in simulated results. To capture a more complete picture of similarity, we move beyond a single score and instead evaluate across five fine-grained dimensions: topic consistency, method consistency, factual consistency, claim consistency, and application context consistency. These dimensions collectively reflect subcomponents of overall semantic similarity. For evaluation, we use GPT-4o to assign scores from 0 to 10 for each dimension for each paper. As shown in Figure 6, our proposed global aggregation method (RESEARCHTOWN) consistently outperforms all other aggregation baselines across these dimensions. This demonstrates that RESEARCHTOWN provides a more effective simulation of research activities compared to baselines.</p>
<p>Automatic evaluation on intrinsic quality. In addition to evaluating semantic similarity between simulated and real-world data, we also assess the intrinsic quality of the generated content. Specifically, we focus on two key dimensions: novelty and feasibility, which we consider the two most critical aspects of a research proposal. As shown in Table 4, the simulated outputs still do not match the novelty and feasibility levels of real-world articles but are close to those. This gap indicates that RESEARCHTOWN would benefit from a more coordinated agentic workflow to enhance the quality of the generated research outputs.</p>
<h2>NLP + Astronomy</h2>
<p>What is the problem?
How can kinematic modeling techniques from astrophysics be applied to analyze and understand the evolution of writing styles in large-scale linguistic datasets?</p>
<p>What are the key components of my approach and results?
...We will utilize large-scale corpora from diverse language families, focusing on historical texts to capture changes in syntax, vocabulary, and stylistic elements over time. Specifically, we will adapt kinematic models by defining linguistic "velocity" metrics that quantify shifts in writing style, analogous to how velocities are analyzed in astrophysics...</p>
<h2>NLP + Criminology</h2>
<p>What is the problem?
How can a multimodal LLM that integrates qualitative narrative analysis with real-time speech translation effectively address the communication needs of communities affected by mass incarceration?</p>
<p>What are the key components of my approach and results?
...I will employ a mixed-methods approach, combining qualitative data from interviews with impacted individuals and quantitative data from existing linguistic resources. The model will be trained on a diverse dataset that captures a wide range of narratives related to mass incarceration, ensuring representation of various socio-cultural contexts...</p>
<p>Figure 7: Examples of generated interdisciplinary research papers from RESEARCHTOWN. For each example, we include RESEARCHTOWN's responses to two questions: "What is the problem?" and "What are the key components of my approach and results?" as these are the most critical among the five questions mentioned in Appendix §F. Appendix §K provides the full contents of the above two and more examples for interdisciplinary research.</p>
<p>Table 4: Evaluation results on novelty and feasibility. Each paper is assigned scores from 0 to 10 for novelty and feasibility. Both LLM-based evaluation and human evaluations are conducted to evaluate the quality of simulated papers. LLM-based evaluation includes results on 1,000 papers, and human evaluation includes results on 40 of them. Simulation represents the outputs of RESEARCHTOWN, realworld represents the existing papers.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Evaluation</th>
<th style="text-align: center;">Simulation</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Real-world</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">Novelty</td>
<td style="text-align: center;">Feasibility</td>
<td style="text-align: center;">Novelty</td>
<td style="text-align: center;">Feasibility</td>
</tr>
<tr>
<td style="text-align: left;">LLM-based</td>
<td style="text-align: center;">7.39</td>
<td style="text-align: center;">6.82</td>
<td style="text-align: center;">$\underline{7.85}$</td>
<td style="text-align: center;">$\underline{7.13}$</td>
</tr>
<tr>
<td style="text-align: left;">Human-based</td>
<td style="text-align: center;">5.50</td>
<td style="text-align: center;">$\underline{7.98}$</td>
<td style="text-align: center;">$\underline{5.90}$</td>
<td style="text-align: center;">7.85</td>
</tr>
</tbody>
</table>
<p>Human evaluation. Evaluation based on LLMs may introduce bias into the results. To validate the reliability of LLMbased evaluations, we conduct additional human evaluations. For similarity-based assessments, human judgments correlate well with LLM scores, achieving a Pearson correlation of 0.61 , indicating reasonable agreement. However, for intrinsic quality evaluations, the correlation between human and LLM scores is low. This is likely due to the inherent ambiguity of such tasks and the need for domain-specific expertise. Despite this, both human and LLM evaluations consistently indicate that simulated papers are slightly less novel than real-world ones-though the gap is relatively small ( 5.50 vs 5.90 for humans and 7.39 vs 7.85 for LLMs).</p>
<h2>11. Case Study: Out-of-distribution Use</h2>
<p>As discussed in Section §8, the node masking evaluation in RESEARCHTOWN targets in-distribution settings with predefined neighborhoods. In real-world use, however, RESEARCHTOWN must generate non-existing papers and reviews without such neighborhoods, requiring automatic construction via paper-researcher matching. This leads to out-of-distribution cases, such as interdisciplinary research, where unrelated papers and researchers form unconventional neighborhoods without prior related works.</p>
<p>RESEARCHTOWN can inspire interdisciplinary research. Interdisciplinary research is often challenging due to limited collaboration across fields. RESEARCHTOWN addresses this by enabling agents with diverse expertise to read, interact, and co-create novel ideas. For example, as shown in Figure 7, combining NLP and astronomy papers leads to using kinematic models to analyze language evolution, while linking NLP and criminology inspires the use of LLMs to support communities affected by mass incarceration. These domain pairings are rarely explored in existing literature, demonstrating RESEARCHTOWN 's ability to generate innovative, cross-disciplinary research directions.</p>
<h2>RESEARCHTOWN-written contents might have limited</h2>
<p>use in the real world. RESEARCHTOWN exhibits failure modes when combining too many disparate domains, often producing incoherent or superficial outputs. For example, combining researchers and papers from LLM, biology, criminology, and astronomy, RESEARCHTOWN generates a research question of "How does coded language in political discourse influence societal biases, and how can a Bayesian hierarchical model be employed to analyze this effect while simultaneously addressing observational biases in white dwarf population studies?" It simply strings together terminology from different domains without presenting a clear research direction. Such vagueness might hinder the real use of the papers simulated from RESEARCHTOWN.</p>
<h2>12. Conclusion</h2>
<p>We introduce RESEARCHTOWN, a graph-based multi-agent framework that simulates research communities by modeling them as heterogeneous graphs. RESEARCHTOWN integrates key research activities-paper reading, writing, and reviewing-into a unified TextGNN-driven inference process. It enables realistic and robust simulations through agent collaboration and facilitates rare interdisciplinary interactions. RESEARCHTOWN offers a valuable platform for studying research dynamics and developing algorithms to support automated scientific discovery.</p>
<h2>Impact Statement</h2>
<p>RESEARCHTOWN presents an LLM-based simulation framework that models human research communities as graph-based multi-agent systems, enabling the study of collaboration, knowledge diffusion, and institutional dynamics. By formalizing how agents create, refine, and evaluate academic papers, the simulator can inform the design of autonomous research systems that assist, rather than replace, human researchers. Potential applications include optimizing collaboration structures, identifying systemic bottlenecks in peer review or discovery, and stress-testing scientific workflows under various incentive and communication settings. While the framework is primarily a research tool, we acknowledge that future extensions involving autonomous agents could raise ethical considerations around authorship, influence, and epistemic trust. Our work highlights the imperative for adaptive ethical frameworks that keep pace with technological capabilities while protecting scholarly values.</p>
<h2>Acknowledgments</h2>
<p>We sincerely appreciate the support from Amazon grant funding project #120359, "GRAG: Enhance RAG Applications with Graph-structured Knowledge", and Meta gift funding project "PERM: Toward Parameter Efficient Foundation Models for Recommenders".</p>
<h2>References</h2>
<p>AI4Science, M. R. and Quantum, M. A. The impact of large language models on scientific discovery: a preliminary study using gpt-4. arXiv preprint arXiv:2311.07361, 2023.</p>
<p>Ba, J. L. Layer normalization. arXiv preprint arXiv:1607.06450, 2016.</p>
<p>Baek, J., Jauhar, S. K., Cucerzan, S., and Hwang, S. J. Researchagent: Iterative research idea generation over scientific literature with large language models. arXiv preprint arXiv:2404.07738, 2024.</p>
<p>Cao, B., Cai, D., Zhang, Z., Zou, Y., and Lam, W. On the worst prompt performance of large language models. arXiv preprint arXiv:2406.10248, 2024.</p>
<p>Chen, M., Li, T., Sun, H., Zhou, Y., Zhu, C., Wang, H., Pan, J. Z., Zhang, W., Chen, H., Yang, F., et al. Learning to reason with search for llms via reinforcement learning. arXiv preprint arXiv:2503.19470, 2025.</p>
<p>Chen, Z., Mao, H., Wen, H., Han, H., Jin, W., Zhang, H., Liu, H., and Tang, J. Label-free node classification on graphs with large language models (llms). In The Twelfth</p>
<p>International Conference on Learning Representations, 2024.</p>
<p>Domingo-Enrich, C., Han, J., Amos, B., Bruna, J., and Chen, R. T. Stochastic optimal control matching. arXiv preprint arXiv:2312.02027, 2023.</p>
<p>Elali, F. R. and Rachid, L. N. Ai-generated research paper fabrication and plagiarism in the scientific community. Patterns, 2023.</p>
<p>Gao, C., Lan, X., Lu, Z., Mao, J., Piao, J., Wang, H., Jin, D., and Li, Y. S ${ }^{3}$ : Social-network simulation system with large language model-empowered agents. arXiv preprint arXiv:2307.14984, 2023.</p>
<p>Girotra, K., Meincke, L., Terwiesch, C., and Ulrich, K. T. Ideas are dimes a dozen: Large language models for idea generation in innovation. Available at SSRN 4526071, 2023.</p>
<p>Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A., and Bengio, Y. Generative adversarial nets. Advances in neural information processing systems, 2014.</p>
<p>Han, X. N., Zickler, T., and Nishino, K. Multistable shape from shading emerges from patch diffusion. arXiv preprint arXiv:2405.14530, 2024.</p>
<p>He, X., Bresson, X., Laurent, T., Hooi, B., et al. Explanations as features: Llm-based features for text-attributed graphs. arXiv preprint arXiv:2305.19523, 2023.</p>
<p>Holm, A. N., Plank, B., Wright, D., and Augenstein, I. Longitudinal citation prediction using temporal graph neural networks. arXiv preprint arXiv:2012.05742, 2020.</p>
<p>Hu, S., Shen, L., Zhang, Y., and Tao, D. Learning multiagent communication from graph modeling perspective. In The Twelfth International Conference on Learning Representations, 2024a.</p>
<p>Hu, X., Fu, H., Wang, J., Wang, Y., Li, Z., Xu, R., Lu, Y., Jin, Y., Pan, L., and Lan, Z. Nova: An iterative planning and search approach to enhance novelty and diversity of llm generated ideas. arXiv preprint arXiv:2410.14255, 2024b.</p>
<p>Hua, W., Fan, L., Li, L., Mei, K., Ji, J., Ge, Y., Hemphill, L., and Zhang, Y. War and peace (waragent): Large language model-based multi-agent simulation of world wars. arXiv preprint arXiv:2311.17227, 2023.</p>
<p>Huang, Q., Vora, J., Liang, P., and Leskovec, J. Mlagentbench: Evaluating language agents on machine learning experimentation. In Forty-first International Conference on Machine Learning, 2024.</p>
<p>Jin, B., Zeng, H., Yue, Z., Yoon, J., Arik, S., Wang, D., Zamani, H., and Han, J. Search-r1: Training llms to reason and leverage search engines with reinforcement learning. arXiv preprint arXiv:2503.09516, 2025.</p>
<p>Jin, D., Jin, Z., Zhou, J. T., and Szolovits, P. Is bert really robust? a strong baseline for natural language attack on text classification and entailment. In Proceedings of the AAAI conference on artificial intelligence, 2020.</p>
<p>Kingma, D. P. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013.</p>
<p>Kingma, D. P. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.</p>
<p>Langley, P. Crafting papers on machine learning. In Langley, P. (ed.), Proceedings of the 17th International Conference on Machine Learning (ICML 2000), pp. 1207-1216, Stanford, CA, 2000. Morgan Kaufmann.</p>
<p>Lee, S., Sim, W., Shin, D., Kim, S., and Kim, S. Reasoning abilities of large language models through the lens of abstraction and reasoning. In The First Workshop on System-2 Reasoning at Scale, NeurIPS'24, 2024.</p>
<p>Lu, C., Lu, C., Lange, R. T., Foerster, J., Clune, J., and Ha, D. The ai scientist: Towards fully automated open-ended scientific discovery. arXiv preprint arXiv:2408.06292, 2024.</p>
<p>Martinkus, K., Papp, P. A., Schesch, B., and Wattenhofer, R. Agent-based graph neural networks. In The Eleventh International Conference on Learning Representations, 2023.</p>
<p>Newman, M. E. The structure of scientific collaboration networks. Proceedings of the national academy of sciences, 2001.</p>
<p>Qi, P., Zhang, Y., Zhang, Y., Bolton, J., and Manning, C. D. Stanza: A python natural language processing toolkit for many human languages. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations, 2020.</p>
<p>Si, C., Yang, D., and Hashimoto, T. Can llms generate novel research ideas? a large-scale human study with 100+ nlp researchers. arXiv preprint arXiv:2409.04109, 2024.</p>
<p>Strubell, E., Ganesh, A., and McCallum, A. Energy and policy considerations for deep learning in NLP. In Korhonen, A., Traum, D., and Màrquez, L. (eds.), Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, Florence, Italy, 2019. Association for Computational Linguistics.</p>
<p>Tang, J., Zhang, J., Yao, L., Li, J.-Z., Zhang, L., and Su, Z. Arnetminer: extraction and mining of academic social networks. In Knowledge Discovery and Data Mining, 2008.</p>
<p>Tasse, G. N., Jarvis, D., James, S., and Rosman, B. Skill machines: Temporal logic skill composition in reinforcement learning. In The Twelfth International Conference on Learning Representations, 2024.</p>
<p>Tu, Z., Aranguri, S., and Jacot, A. Mixed dynamics in linear networks: Unifying the lazy and active regimes. arXiv preprint arXiv:2405.17580, 2024.</p>
<p>Wei, J., Wei, J., Tay, Y., Tran, D., Webson, A., Lu, Y., Chen, X., Liu, H., Huang, D., Zhou, D., et al. Larger language models do in-context learning differently. arXiv preprint arXiv:2303.03846, 2023.</p>
<p>West, J. D., Wesley-Smith, I., and Bergstrom, C. T. A recommendation system based on hierarchical clustering of an article-level citation network. IEEE Transactions on Big Data, 2016.</p>
<p>Xu, Z., Yu, C., Fang, F., Wang, Y., and Wu, Y. Language agents with reinforcement learning for strategic play in the werewolf game. arXiv preprint arXiv:2310.18940, 2023.</p>
<p>Yan, H., Li, C., Long, R., Yan, C., Zhao, J., Zhuang, W., Yin, J., Zhang, P., Han, W., Sun, H., Deng, W., Zhang, Q., Sun, L., Xie, X., and Wang, S. A comprehensive study on text-attributed graphs: Benchmarking and rethinking. In Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2023.</p>
<p>Yang, J. and Leskovec, J. Defining and evaluating network communities based on ground-truth. Knowledge and Information Systems, 2012.</p>
<p>Yang, J., Liu, Z., Xiao, S., Li, C., Lian, D., Agrawal, S., Singh, A., Sun, G., and Xie, X. Graphformers: Gnnnested transformers for representation learning on textual graph. Advances in Neural Information Processing Systems, 2021.</p>
<p>Yu, H., Wang, C., Zhuang, P., Menapace, W., Siarohin, A., Cao, J., Jeni, L. A., Tulyakov, S., and Lee, H.-Y. 4real: Towards photorealistic 4d scene generation via video diffusion models. arXiv preprint arXiv:2406.07472, 2024.</p>
<p>Zhao, J., Qu, M., Li, C., Yan, H., Liu, Q., Li, R., Xie, X., and Tang, J. Learning on large-scale text-attributed graphs via variational inference. In The Eleventh International Conference on Learning Representations, 2023.</p>
<p>Zhou, X., Zhu, H., Mathur, L., Zhang, R., Yu, H., Qi, Z., Morency, L.-P., Bisk, Y., Fried, D., Neubig, G., et al. SOTOPIA: Interactive evaluation for social intelligence in language agents. In The Twelfth International Conference on Learning Representations, 2023.</p>
<p>Zhuge, M., Wang, W., Kirsch, L., Faccio, F., Khizbullin, D., and Schmidhuber, J. GPTSwarm: Language agents as optimizable graphs. In Forty-first International Conference on Machine Learning, 2024.</p>
<h1>A. Individual Contribution</h1>
<p>Haofei Yu Overall project leader<br>Zhaochen Hong Co-lead, code writing, benchmark collection, review writing experiment<br>Zirui Cheng Co-lead, paper writing, code writing, system design<br>Kunlun Zhu Co-lead, benchmark collection, code writing, paper writing, experiment<br>Keyang Xuan Participant, code writing, benchmark collection, case study<br>Jinwei Yao Participant, code writing, evaluation experiment in early versions<br>Tao Feng Co-lead in early versions, paper writing, code writing in early versions<br>Jiaxuan You Overall project advisor</p>
<h2>B. Ethical Concern</h2>
<p>The development and deployment of RESEARCHTOWN raises several important ethical considerations that we have carefully addressed in our work. We first discuss how RESEARCHTOWN prevents dangerous use, including facilitating plagiarism, producing misleading or low-quality claims, and role-playing human researchers. Furthermore, we discuss the attribution and authorship issues for generated content and discuss the model and data license in our work.</p>
<h2>B.1. Potential to facilitate plagiarism</h2>
<p>Generative AI's capabilities for image and text generation can potentially lead to plagiarism in research (Elali \&amp; Rachid, 2023). To address this, we have implemented safeguards to ensure responsible usage. RESEARCHTOWN is designed as an assistive tool to help researchers gather inspiration for papers and review writing, rather than generating complete, ready-to-use content. By design, RESEARCHTOWN ensures that its outputs serve as a starting point for further intellectual effort, rather than a replacement for human researchers.</p>
<p>For generated papers, RESEARCHTOWN provides only preliminary answers to five key research questions. These outputs are intentionally incomplete and generic, requiring significant refinement and further development by the user. Critical sections such as the introduction, background, methodology, discussion, and conclusion are not included, placing the responsibility for completing and validating the content on the researcher.</p>
<p>For generated reviews, RESEARCHTOWN provides general guidance on potential strengths and weaknesses, accompanied by an indicative score for reference. However, these reviews are intentionally non-definitive and generic, only as a supplementary aid to help reviewers organize their thoughts. Generated reviews do not replace human judgment in determining the acceptance or rejection of a paper. The final evaluation, including critical reasoning, detailed feedback, and the ultimate decision, remains the sole responsibility of the reviewer. Reviewers must ensure fairness, accuracy, and rigor, using AI outputs only as a starting point to enhance their assessment process.</p>
<h2>B.2. Potential to produce misleading or low-quality claims</h2>
<p>The motivation of our paper is to simulate research activities and generate preliminary research progress (e.g., papers and reviews that are in their condensed bullet-point summarized format) that can be scrutinized and validated by human researchers, ultimately contributing to the acceleration of the research process. We acknowledge that AI-generated ideas may vary in quality, and therefore, these outputs are not intended for direct dissemination. Instead, they serve as initial, unofficial suggestions that require further experimental validation by human researchers. This approach ensures that only rigorously tested and verified research is presented as final, high-quality work.</p>
<h2>B.3. Potential to role-play human researchers</h2>
<p>The primary objective of our work is to leverage existing academic literature to simulate research activities. In this paper, our research agents are designed to act as research domain experts, generating informative and relevant content based on a given and limited research domain. Importantly, we do not aim to simulate human-like interactive dialogues between research agents, nor do we attempt to mimic the specific research styles of individual human researchers. Instead, we focus on using related academic papers as conditions for generating more related research content.</p>
<p>The research agents are built using publicly available, properly cited academic papers, which eliminates the need for additional consent. We utilize the LLM-based research agents, each with one or more specific research domains, modeling</p>
<p>the typical academic process, where researchers read, synthesize, and build upon available public academic data. By focusing on publicly available research papers, we align with the papers' intended purpose: contributing to the collective advancement of knowledge and fostering academic growth.</p>
<h1>B.4. Attribution and authorship</h1>
<p>The AI-generated content, such as papers, reviews, or other research outputs, is meant for internal discussion and as a reference to assist human researchers. These outputs are not intended for direct publication. Our proposed methods serve as tools to accelerate the research process by offering starting points that require further elaboration, critical analysis, and human refinement to reach a publishable standard. The final authority to complete and submit research lies solely with human authors, ensuring that full responsibility and ownership remain with them. Since the AI-generated content is not considered complete or officially authored, it does not raise issues of authorship or attribution.</p>
<h2>C. Artifact</h2>
<p>We list all licenses for the data and models used in our paper in this section.</p>
<h2>C.1. Data license</h2>
<p>All papers in RESEARCHBENCH come from top-tier machine learning conferences (ICLR 2024 and NeurIPS 2024). These papers are publicly available and under the license of CC-BY 4.0, allowing for redistribution and sharing. For the evaluation results of RESEARCHBENCH, all inputs and outputs are logged and open for access. Additionally, we keep an accessible record of all supplementary papers referenced during RESEARCHTOWN's inference process. All outputs from RESEARCHTOWN are released under the licenses of the papers used for generation.</p>
<h2>C.2. Model license</h2>
<p>Our work relies on multiple foundation models, including GPT-4o-mini, Qwen-2.5-7B-Instruct, Deepseek-v3, text-embedding-3-large, and voyage-3. Specifically, we use gpt-4o-mini-2024-07-18 accessed via the OpenAI API. We use Qwen-2.5-7B-Instruct-Turbo and Deepseek-v3-0324 via the together.ai ${ }^{1}$ inference API. We utilize the official inference API provided by OpenAI and VoyageAI to use text-embedding-3-large and voyage-3 separately.</p>
<p>The GPT-4o-mini, text-embedding-3-large, and voyage-3 models are closed-source and operate under proprietary licenses. We use them only for academic, and non-commercial purposes and ensure all inputs come from publicly available data, complying with their usage restrictions. By contrast, Qwen-2.5-7B-Instruct is released under the permissive Apache 2.0 license, and Deepseek-v3-0324 is available under the MIT License, allowing for broad academic and research use. We make no modifications to these models and use them as-is via their public APIs.</p>
<h2>D. RESEARCHTOWN Details</h2>
<p>In this section, we provide more explanation and implementation details for RESEARCHTOWN simulation algorithm. To achieve better performance and efficiency, we design different prompts for each agent function and make the aggregation process run in parallel.</p>
<h2>D.1. RESEARCHTOWN aggregation setting implementation</h2>
<p>We provide more information about the 4 aggregation experimental settings mentioned in Section $\S 7$ (i.e. self-agg, agent-agg, data-agg, global-agg). The main difference lies in the neighborhood nodes that participated during the message-passing process.</p>
<p>AGG-self. We do not rely on any neighborhood information.
$\triangleright$ Paper writing: The LLM agent without profiles brainstorms independently without referencing any external data or other</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>agents' ideas. This setting extends Equation 7 to</p>
<p>$$
\mathbf{h}<em g="g">{v}=\operatorname{AGG}(\emptyset, \emptyset, \emptyset)=f</em>(\emptyset)
$$</p>
<p>$\triangleright$ Review writing: The LLM agent without profiles writes the review based solely on the paper itself, without considering additional references or other agents. This setting extends Equation 8 to</p>
<p>$$
\mathbf{r}<em v="v">{v}=\operatorname{AGG}\left(\mathbf{h}</em>\right)
$$}, \emptyset, \emptyset\right)=f_{g}\left(\mathbf{h}_{v</p>
<p>AGG-agent. We rely only on agent nodes and exclude data nodes.
$\triangleright$ Paper writing: Multiple LLM agents collaborate by sharing their content and insights to produce the final paper's content. This setting extends Equation 7 to</p>
<p>$$
\begin{aligned}
\mathbf{h}<em v="v">{v} &amp; =\operatorname{AGG}\left(\mathbf{h}</em>},\left{f_{a}(\cdot), \mathbf{h<em a="a" d="d">{a} \mid(v, a) \in \mathcal{E}</em>\right}, \emptyset\right) \
&amp; =f_{g}\left(\left[f_{a}\left(\mathbf{h}<em a="a" d="d">{a}\right) \mid(v, a) \in \mathcal{E}</em>\right]\right)
\end{aligned}
$$</p>
<p>$\triangleright$ Review writing: Multiple LLM agents collectively review the paper, sharing their input and critiques to form the final review. This setting extends Equation 8 to</p>
<p>$$
\begin{aligned}
\mathbf{r}<em v="v">{v} &amp; =\operatorname{AGG}\left(\mathbf{h}</em>},\left{f_{a}(\cdot), \mathbf{h<em a="a" d="d">{a} \mid(v, a) \in \mathcal{E}</em>\right}, \emptyset\right) \
&amp; =f_{g}\left(\left[\mathbf{h}<em a="a">{v}, f</em>}\left(\left[\mathbf{h<em v="v">{a}, \mathbf{h}</em>\right]\right)
\end{aligned}
$$}\right]\right) \mid(v, a) \in \mathcal{E}_{a d</p>
<p>AGG-data. We rely only on data nodes and exclude agent nodes.
$\triangleright$ Paper writing: A single LLM agent without profiles reads and synthesizes information from related data sources to write a paper. This setting extends Equation 7 to</p>
<p>$$
\begin{aligned}
\mathbf{h}<em v="v">{v} &amp; =\operatorname{AGG}\left(\mathbf{h}</em>}, \emptyset,\left{\mathbf{h<em d="d">{d} \mid(v, d) \in \mathcal{E}</em>\right}\right) \
&amp; =f_{g}\left(\left{\mathbf{h}<em d="d">{d} \mid(v, d) \in \mathcal{E}</em>\right}\right)
\end{aligned}
$$</p>
<p>$\triangleright$ Review writing: A single LLM agent without profiles produces a review by reading both the paper and its related data sources, integrating the information to form a comprehensive critique. This setting extends Equation 8 to</p>
<p>$$
\begin{aligned}
\mathbf{r}<em v="v">{v} &amp; =\operatorname{AGG}\left(\mathbf{h}</em>}, \emptyset,\left{\mathbf{h<em d="d">{d} \mid(v, d) \in \mathcal{E}</em>\right}\right) \
&amp; =f_{g}\left(\left[\mathbf{h}<em d="d">{v},\left{\mathbf{h}</em>\right}\right]\right)
\end{aligned}
$$} \mid(v, d) \in \mathcal{E}_{d d</p>
<p>AGG-global. We include all neighborhood nodes (both agent and data) during aggregation.
$\triangleright$ Paper writing: Multiple LLM agents produce content in parallel while referencing various data sources. Their aggregated outputs, which incorporate insights from both other agents and data, form the final paper. This setting extends Equation 7 to</p>
<p>$$
\begin{aligned}
\mathbf{h}<em v="v">{v} &amp; =\operatorname{AGG}\left(\mathbf{h}</em>},\left{f_{a}(\cdot), \mathbf{h<em a="a" d="d">{a} \mid(v, a) \in \mathcal{E}</em>}\right},\left{\mathbf{h<em d="d">{d} \mid(v, d) \in \mathcal{E}</em>\right}\right) \
&amp; =f_{g}\left(\left[\left{f_{a}\left(\left[\mathbf{h}<em d="d">{a}, \mathbf{h}</em>}\right]\right) \mid(v, a) \in \mathcal{E<em d="d">{a d},(v, d) \in \mathcal{E}</em>\right}\right]\right)
\end{aligned}
$$</p>
<p>$\triangleright$ Review writing: Multiple LLM agents each consider the paper and its related works to write their review. The final review is a combination of these integrated perspectives. This setting extends Equation 8 to</p>
<p>$$
\begin{aligned}
\mathbf{r}<em v="v">{v} &amp; =\operatorname{AGG}\left(\mathbf{h}</em>},\left{f_{a}(\cdot), \mathbf{h<em a="a" d="d">{a} \mid(v, a) \in \mathcal{E}</em>}\right},\left{\mathbf{h<em d="d">{d} \mid(v, d) \in \mathcal{E}</em>\right}\right) \
&amp; =f_{g}\left(\left[\mathbf{h}<em a="a">{v},\left{f</em>}\left(\left[\mathbf{h<em v="v">{a}, \mathbf{h}</em>}, \mathbf{h<em a="a" d="d">{d}\right]\right) \mid(v, a) \in \mathcal{E}</em>\right}\right]\right)
\end{aligned}
$$},(v, d) \in \mathcal{E}_{d d</p>
<h1>D.2. RESEARCHTOWN simulation algorithm implementation</h1>
<p>One practical issue with Equation 4 and Equation 5 is that $f_{a}(\cdot)$ must be computed for every combination of agent and data nodes, leading to a significant computational burden if implemented directly as defined. We introduce the definition of $f_{a}(\cdot)$ to maintain scalability and alignment with traditional Graph Neural Network definitions. In practice, we can easily</p>
<p>parallelize $f_{a}(\cdot)$ over all data nodes, which means we can prompt once and put all data nodes' information in one prompt to get the results instead of repeating the prompting process on each data node. Thus, we provide a modified version of the original aggregation process below.</p>
<p>While the paper reading process remains the same for implementation, the paper writing process can be alternatively calculated as: This setting extends Equation 7 to</p>
<p>$$
\begin{aligned}
\mathbf{h}<em a="a">{v} &amp; =\operatorname{AGG}\left(\emptyset,\left{f</em>}(\cdot), \mathbf{h<em a="a" d="d">{a} \mid(v, a) \in \mathcal{E}</em>}\right},\left{\mathbf{h<em d="d">{d} \mid(v, d) \in \mathcal{E}</em>\right}\right) \
&amp; =f_{g}\left(\left[\left{f_{a}\left(\left[\mathbf{h}<em d="d">{a},\left{\mathbf{h}</em>} \mid(v, a) \in \mathcal{E<em a="a" d="d">{d d}\right}\right]\right) \mid(v, a) \in \mathcal{E}</em>\right}\right]\right)
\end{aligned}
$$</p>
<p>Similarly, the review writing process can be calculated as: This setting extends Equation 8 to</p>
<p>$$
\begin{aligned}
\mathbf{r}<em v="v">{v} &amp; =\operatorname{AGG}\left(\mathbf{h}</em>},\left{f_{a}(\cdot), \mathbf{h<em a="a" d="d">{a} \mid(v, a) \in \mathcal{E}</em>}\right},\left{\mathbf{h<em d="d">{d} \mid(v, d) \in \mathcal{E}</em>\right}\right) \
&amp; =f_{g}\left(\left[\mathbf{h}<em a="a">{v},\left{f</em>}\left(\left[\mathbf{h<em v="v">{a}, \mathbf{h}</em>},\left{\mathbf{h<em d="d">{d} \mid(v, d) \in \mathcal{E}</em>\right}\right]\right)
\end{aligned}
$$}\right}\right]\right) \mid(v, a) \in \mathcal{E}_{a d</p>
<p>Therefore, we reduce the calling of $f_{a}(\cdot)$ from $N \times M$ to $M$ where $M$ represents the number of agent nodes in the neighborhoods and $N$ represents the number of data nodes in the neighborhoods.</p>
<h1>D.3. RESEARCHTOWN agent function implementation</h1>
<p>For each $f_{a}(\cdot)$ and $f_{g}(\cdot)$ in Algorithm 1, these functions represent LLMs equipped with task-specific prompt templates. In the global-agg setting, we provide examples of the prompt templates for each agent function. Other settings follow a similar style but use fewer details.</p>
<p>For the paper writing stage, Table 7 presents the $f_{u}(\cdot)$ prompt template used in it. During the paper writing stage, Table 8 and Table 9 show the prompt templates for $f_{a}(\cdot)$ and $f_{g}(\cdot)$ respectively.</p>
<p>For the review writing stage, since we need to separately generate strengths, weaknesses, and scores, $f_{a}(\cdot)$ combines the prompt templates from Table 10, Table 11, and Table 12. Similarly, $f_{g}(\cdot)$ is formed by combining the prompt templates from Table 13 and Table 14.</p>
<p>The aggregation function for classical GNN in Eq 1 and Eq 2, which is often a pooling or mean operation, is used to condense all neighborhood information into one embedding with the same size as the input. Similarly, our TextGNN layers in Eq 4 and Eq 5, act as an aggregation function similar to classical GNN, producing outputs with controlled textual formats and similar lengths with updated information in the neighborhood nodes by summarizing with LLMs. Therefore, the output length of multiple layers of TextGNN would not increase but would remain approximately the same. We achieve such length control in TextGNN via format control in prompting. We specifically designed prompts to ensure each output adheres to pre-defined constraints. These prompt-controlled constraints ensure stable output lengths at every TextGNN layer, avoiding text length inflation with increasing depth. Each aggregation step condenses and prioritizes critical information, effectively filtering less relevant details.</p>
<h2>D.4. RESEARCHTOWN future application</h2>
<p>Any research-related content-e.g., images, codebases, models, or social media posts-can be represented as nodes in the agent-data graph, with edge types like "cite the paper," "release model," or "comment on X post" (examples in Figure 1) defining interactions. By specifying appropriate edge types and agent functions, the framework can be extended to simulate tasks such as code writing, model release, panel discussions, or lectures. While we focus on paper and review writing due to their importance, available real-world data, and simplicity, the framework supports broader applications.</p>
<p>Additionally, RESEARCHTOWN can be extended to model social dynamics such as peer pressure, collaborations, and institutional roles via agent-agent relationship edges. Our current implementation already includes role-based dynamics (e.g., leader vs. participant), and we plan to support richer simulations of institutional and reputational factors in future work.</p>
<h2>E. RESEARCHBENCH Details</h2>
<p>In this section, we provide the technical details included in the construction process of RESEARCHBENCH. We describe the methodologies used for data collection across its three main components, and we name them as: (1) PAPERBENCH, (2)</p>
<p>HighIMPACTPAPERBENCH, and (3) REVIEWBENCH. Statistically, PAPERBENCH and HighIMPACTPAPERBENCH focus on a paper writing simulation, which contains 1,000 and 100 tasks, respectively. REVIEWBENCH focuses on review writing simulation and includes 200 tasks.</p>
<h1>E.1. Data collection details</h1>
<p>We first include technical details related to how we collect paper, author, and review data from publicly available platforms as a source to build RESEARCHBENCH.</p>
<p>Paper data collection. We begin by recording the titles of all papers that we plan to crawl. Then, using the arxiv Python package ${ }^{1}$, we query the arXiv API to check for any papers with identical titles. If a match is found, we note the corresponding arXiv ID and use the API to retrieve the paper's metadata, including its title, arXiv ID, author list, abstract, and citation information.</p>
<p>Author data collection. A primary challenge in collecting author data is that there might be multiple human researchers with the same name, and some human researchers may not have any publicly available publication records on public platforms, including arXiv, Google Scholar, or Semantic Scholar. As a result, at the paper collection stage, we only have each author's name. We use the semanticscholar Python package ${ }^{2}$ to search for the author by name, verify that they have contributed to the specific target paper, and obtain a unique author ID from Semantic Scholar. This ID then allows us to retrieve their available publication information. To prevent information leakage when simulating paper writing and review scenarios, we exclude any of the author's publications released after the target paper's publication year. For example, if we aim to simulate a paper published in 2022, we ignore all of the author's publications appearing after 2022. We also exclude the target paper itself to avoid leaking information. Generally, we limit the maximum number of collected publications to around 20, focusing on those most relevant to the target time frame. Additionally, we gather each author's co-author network and their top publications to enrich the dataset with useful relational information.</p>
<p>Review data collection. In addition to paper and author data, we also leverage OpenReview to extract public review information. Since fully public review data is predominantly available for ICLR, we focus on collecting reviews from ICLR2024. Using the openreview Python package ${ }^{3}$, we first verify the arXiv ID to ensure that we are retrieving the correct paper and its corresponding reviews. The collected review data aligns with ICLR's criteria, including detailed feedback on soundness, presentation, contributions, reviewer scores, and commentary on strengths and weaknesses. We adopt this review structure when generating our reviews, incorporating strengths, weaknesses, and ratings for the paper.</p>
<h2>E.2. PAPERBENCH details</h2>
<p>PAPERBENCH is designed to evaluate the effectiveness of paper-writing simulations by gathering high-quality paper metadata from top-tier ML conferences, such as NeurIPS 2024 and ICLR 2024. Both NeurIPS 2024 and ICLR 2024 post-date beyond GPT-4o-mini's October 2023 knowledge cutoff. Thus, data leakage is not a concern. We also mask the full text during the simulation to avoid accidental exposure. Based on the collected author and paper data, we perform the following two post-processing steps:</p>
<p>First, we address cases where authors have no accessible publications beyond the current paper or where citation data extraction fails due to API issues. In such cases, we exclude these papers. We only retain those with full author publication information, as well as complete metadata including introduction, abstract, title, and citations. After this filtering step, we end up with approximately 1,200 papers, and then randomly sample 1,000 from them.</p>
<p>Second, to allow more fine-grained analysis, we split these 1,000 paper-writing tasks into three subgroups based on their difficulty level. We use the data-agg settings described in Section $\S 7$ to obtain results and compute similarity scores for our simulations. We then divide the dataset into three equal subsets: the worst 333 data points (hard), the middle 334 data points (medium), and the top 333 data points (easy). This results in a more granular categorization of the dataset's difficulty.</p>
<p>Intuitively, papers in the hard sub-part tend to be more theoretical and math-focused, while those in the easy sub-part are more application-oriented. Examples for hard sub-parts of the dataset include "Stochastic Optimal Control Matching" (DomingoEnrich et al., 2023), "Mixed Dynamics In Linear Networks: Unifying the Lazy and Active Regimes" (Tu et al., 2024), and</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>"Multistable Shape from Shading Emerges from Patch Diffusion" (Han et al., 2024). Examples for easy sub-parts of the dataset include "4Real: Towards Photorealistic 4D Scene Generation via Video Diffusion Models" (Yu et al., 2024), "Skill Machines: Temporal Logic Skill Composition in Reinforcement Learning" (Tasse et al., 2024), and "On the Worst Prompt Performance of Large Language Models" (Cao et al., 2024).</p>
<h1>E.3. REVIEWBENCH details</h1>
<p>Since public review data is only fully accessible from ICLR, we focus on collecting review data for the ICLR 2024 papers included in PAPERBENCH. All reviews are anonymous, so no direct reviewer information is available. To address this, we identify suitable reviewers by first summarizing each researcher's publications. We then use the abstract of the target paper as a query and the researcher profiles as documents for a ranking task with the voyage-3 model. All authors included in RESEARCHBENCH serve as the corpus for retrieval. The top 20 most relevant authors, excluding the paper's authors, become the suitable reviewer candidates.</p>
<p>After obtaining the reviewer, paper, and author data, we filter out any papers lacking valid reviews during crawling. From the remaining set, we randomly select 200 reviews, each corresponding to one paper as REVIEWBENCH.</p>
<h2>E.4. HighIMPACTPAPERBENCH details</h2>
<p>HighIMPACTPAPERBENCH serves as an extreme benchmark for RESEARCHTOWN, focused on simulating impactful research. We begin by collecting the 20 most-cited papers from each of 10 leading AI-related conferences-CVPR, ECCV, NeurIPS, ICLR, ICML, AAAI, IJCAI, ACL, EMNLP, and NAACL—based on Google Scholar citation rankings. ${ }^{1}$ Additionally, we include classic machine learning algorithm papers such as those introducing VAE (Kingma, 2013), GAN (Goodfellow et al., 2014), and Adam (Kingma, 2014), each with over 1,000 citations, even if they are no longer listed in the current Google Scholar citation rankings.</p>
<p>For these impactful papers, it is crucial to prevent the inclusion of publications released after their publication year when gathering authors' publication data. Later works such as these could significantly alter the trajectory of the researcher, misrepresent the historical context of these influential contributions, and leak information for simulation. After collecting paper and author data, we remove any papers with incomplete information due to crawling errors. From the remaining set, we randomly sample 100 papers to form the final benchmark. These selected papers have averaged over 100 citations in the past five years, ensuring that HIGHIMPACTPAPERBENCH represents a collection of influential and well-established research.</p>
<p>The motivation for using impactful papers as evaluation is to use them as an extreme-case test for idea simulation. While some may exist in the LLM's training data, this benchmark is separate from our main results and serves to explore how LLMs handle well-known concepts. Our similarity analysis shows that $55 \%$ of generated papers score between $0.65-0.75$, and $18 \%$ exceed 0.75 , indicating moderate to high alignment. Only $1 \%$ scored below 0.45 . These scores are comparable to PAPERBENCH, suggesting no abnormal inflation. Even famous papers like VAE, GAN, and LayerNorm do not receive notably high scores, implying that semantic similarity, not memorization based on citation relationships, drives the results, especially for tool/benchmark papers, which naturally resemble their references more.</p>
<h2>F. Embedding-based Evaluation Details</h2>
<p>In this section, we first explain the motivation for our designed multi-component embedding-based evaluation, then we provide a more formal definition and implementation details related to our evaluation process.</p>
<p>Decompositionality. A single idea or a review can manifest through diverse descriptions or implementation strategies. Therefore, directly applying a cosine similarity-based metric is inadequate for capturing conceptual equivalence. To solve this, we design point-wise evaluation metrics to paraphrase the paper and review it into aligned key points with the same LLM-based prompting. This structure enables alignment between papers that differ methodologically but share similar motivations and problem framings. For instance, in Chen et al. (2025) and Jin et al. (2025), despite distinct methods and settings, experts would find strong alignment on the motivations and core concepts in these papers.</p>
<p>Scalability. To address the challenge that a single idea can take many concrete forms, we complement decomposition with scalability. LLMs can generate hundreds of semantically distinct research questions from a single prompt, but evaluating</p>
<p><sup id="fnref6:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>these outputs traditionally requires domain experts-a process that is costly, slow, unscalable, and hard to reproduce. For example, Si et al. (2024) spent thousands hiring top-tier researchers solely for annotation and review, which is infeasible for evaluating large-scale, automated research generation. Our approach replaces this bottleneck with semantic similarity over 5Q-decomposed representations. We can select the best among the samples and make the score the final result.</p>
<p>Extensibility. While we acknowledge the importance of elements like mathematical formulations or algorithmic workflows, our framework is inherently extensible-the original format can be expanded with more key points by adding domain-specific dimensions such as algorithmic structure or key theoretical results. This is especially valuable in systems and theory papers, enabling more fine-grained and domain-aware similarity analysis. As demonstrated in [Fine-Grained Evaluation with LLM and Human], our approach also supports the integration of non-semantic metrics like logical consistency and factual accuracy, making it extensible from an evaluation metric perspective.</p>
<p>Reliability. Our embedding-based / LLM-based similarity metric builds on state-of-the-art models optimized for knowledgeintensive tasks. Voyage AI embeddings, widely adopted in real-world RAG systems, are designed to reduce hallucination and excel in high-precision semantic retrieval, making them ideal for evaluating research content. Additionally, state-of-the-art LLMs are highly effective at semantic comparison.</p>
<p>Baselines for evaluation. To check whether RESEARCHTOWN provides a realistic simulation, we benchmark similarity in real-world research activity. For paper writing, we reference two concurrent papers (Chen et al., 2025; Jin et al., 2025) recognized for presenting nearly identical ideas, yet with different writing styles and experiments, which yield a VoyageAI similarity of 0.8244 . This suggests that scores above 0.82 can potentially indicate strong idea overlap. For review writing, we analyze the data of reviewers evaluating the same paper. The average inter-reviewer similarity is 0.5900 (strengths) and 0.5904 (weaknesses), reflecting natural variance in human judgment. These inter-similarity scores in the real world confirm that similarity scores represent realistic simulation.</p>
<p>More details on paper evaluation. To evaluate the paper writing stage, we define a distance function $d_{p}(\cdot, \cdot)$ to measure the similarity between the generated paper $\mathbf{h}<em v="v">{v}$ and the ground-truth paper $\mathbf{h}</em>^{<em>}$. Since directly comparing full papers in different formats can be challenging and inaccurate, we align $\mathbf{h}<em v="v">{v}$ and $\mathbf{h}</em>^{</em>}$ into a unified format using a well-recognized framework ${ }^{1}$ that summarizes the core components of a paper through five questions: (1) What is the problem? (2) Why is it interesting and important? (3) Why is it hard? (4) Why hasn't it been solved before? (5) What are the key components of my approach and results? We mark these questions as Q1-Q5 for short. By using an LLM-based summarization function $f_{\text {sum }}(\cdot)$, we convert the input papers into an aligned text-based list $\mathbf{a}<em _sum="{sum" _text="\text">{v}=f</em>}}\left(\mathbf{h<em v="v">{v}\right)$ and $\mathbf{a}</em>^{<em>}=f_{\text {sum }}\left(\mathbf{h}_{v}^{</em>}\right)$, where each element in $\mathbf{a}<em v="v">{v}$ and $\mathbf{a}</em>$ corresponds to the answer of one question mentioned above. The distance function for paper writing is formally defined as:}^{*</p>
<p>$$
d_{p}\left(\mathbf{h}<em v="v">{v}, \mathbf{h}</em>^{<em>}\right)=\frac{1}{5} \sum_{i=1}^{5} \operatorname{SIM}\left(\mathbf{a}<em i="i" v_="v,">{v, i}, \mathbf{a}</em>^{</em>}\right)
$$</p>
<p>where $\operatorname{SIM}(\cdot, \cdot)$ represents an embedding-based similarity metric, such as voyage-3 ${ }^{2}$ and text-embedding-large-3 ${ }^{3}$. By leveraging the LLM to generate structured embeddings for each question, this approach ensures a meaningful and consistent comparison of the generated and ground-truth papers.</p>
<p>More details on review evaluation. Another research activity we aim to evaluate is review writing. Similar to paper writing evaluation, we project both real-world and generated reviews into a unified format for comparison. For this purpose, we adopt a bullet point-based format to represent weaknesses and advantages in the review, as it effectively captures the key aspects of a review. Using an LLM-based summarization function $f_{\text {sum }}(\cdot)$, we convert the input reviews $\mathbf{r}<em v="v">{v}$ and $\mathbf{r}</em>^{<em>}$ into a bullet point list $\mathbf{b}<em _sum="{sum" _text="\text">{v}=f</em>}}\left(\mathbf{r<em v="v">{v}\right)$ and $\mathbf{b}</em>^{</em>}=f_{\text {sum }}\left(\mathbf{r}_{v}^{<em>}\right)$, where each element of $\mathbf{b}<em v="v">{v}$ and $\mathbf{b}</em>^{</em>}$ corresponds to a bullet point of the review. Formally, the distance function for review writing is computed as:</p>
<p>$$
d_{r}\left(\mathbf{r}<em v="v">{v}, \mathbf{r}</em>^{<em>}\right)=\frac{1}{n} \sum_{j=1}^{n} \max <em i="i" v_="v,">{i} \operatorname{SIM}\left(\mathbf{b}</em>^{}, \mathbf{b}_{v, j</em>}\right)
$$</p>
<p>where $\operatorname{SIM}(\cdot, \cdot)$ refers to the same similarity metric in paper writing evaluation. This metric emphasizes the recall rate of the generated review by measuring whether each point in the real-world review is potentially included in the generated</p>
<p><sup id="fnref7:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>review. Since each review consists of both strengths and weaknesses, we compute separate similarity scores for strengths and weaknesses. Additionally, since both $\mathbf{r}<em v="v">{v}$ and $\mathbf{r}</em>^{<em>}$ include a final score $\mathbf{S}<em v="v">{v}$ and $\mathbf{S}</em>^{</em>}$ as attributes, we calculate $\Delta \mathbf{S}<em v="v">{v}=\left|\mathbf{S}</em>\right|$ to quantify the difference between the generated and real-world review scores.}-\mathbf{S}_{v}^{*</p>
<p>Prompt. Table 15 presents the prompt used to convert any existing paper into responses to the five critical research questions. Similarly, Table 16 shows the prompt used to transform any existing review into a bullet-point format. Both prompts ensure that the transformed papers and reviews are aligned with the generated ones, facilitating consistent evaluation in the same format. The transformed format for the paper is considered as $\mathbf{a}_{v}^{<em>}$ and the concatenation of all ground-truth reviews is considered as $\mathbf{b}_{v}^{</em>}$, as mentioned in Section $\S 6$.</p>
<p>Metric. For our embedding-based similarity calculations, we use the text-embedding-large-3 model via the litellm Python package by calling litellm.embedding(). For the voyage-3 model, we rely on the voyageai Python package by calling voyageai. Client ( ). embed ( ). We then compute the cosine similarity between the resulting embeddings to measure their similarity.</p>
<h1>G. LLM-based Evaluation Details</h1>
<p>In this section, we provide more technical details about using LLM prompting for evaluation.
Prompting for similarity. For prompting-based evaluation, we decompose overall similarity into six fine-grained dimensions: (1) topic consistency, (2) method consistency, (3) factual consistency, (4) claim consistency, (5) application context consistency, and (6) overall semantic similarity. These dimensions are designed to capture distinct yet complementary aspects of alignment between the generated and reference proposals, ranging from high-level research focus (such as topic and application context) to specific technical content (such as methods, facts, and claims). Importantly, they are intended to capture nuances that may not be easily detected by embedding-based models, enabling a more comprehensive and interpretable assessment than relying on a single similarity score. Each dimension is rated on a scale from 0 to 10.</p>
<p>Prompting for novelty and feasibility. In addition to measuring similarity, we prompt LLMs to assess two intrinsic quality dimensions: (1) novelty and (2) feasibility, which we consider essential characteristics of a strong research proposal. While similarity captures how well the generated content aligns with a reference, it does not fully reflect the proposal's originality or practicality. These intrinsic dimensions address that gap by evaluating the creativity of the proposed idea and its potential for real-world implementation. Each dimension is scored on a scale from 0 to 10 , complementing similarity-based metrics for a more holistic evaluation.</p>
<p>Prompt. To enable efficient evaluation, we adopt parallel prompting, where both the reference and generated proposals are input to the LLM in a single prompt, along with all evaluation criteria. This allows the model to produce scores for all dimensions simultaneously in one forward pass. The detailed descriptions of these evaluation criteria and the full prompts are provided in Table 17.</p>
<h2>H. Human Evaluation Details</h2>
<p>Annotator Information. We recruit two graduate-level students with backgrounds in computer science and artificial intelligence. Both annotators have prior experience publishing in top-tier machine-learning conferences.</p>
<p>Annotated Data. We randomly sample 40 reference proposals and their corresponding generated proposals from PAPERBENCH for human evaluation. We ask annotators to annotate on overall similarity, novelty, and feasibility.</p>
<p>Annotation Process. The annotation process consists of three stages: (1) preliminary annotation, (2) discussion, and (3) final annotation. In the preliminary stage, each annotator independently labels 10 examples. They then meet to discuss discrepancies, align their understanding, and refine the annotation criteria. Based on this discussion, they proceed to annotate the official 40 examples using the agreed-upon guidelines as the final results.</p>
<p>Annotation Instructions. At the start, annotators receive the same input information as used in the LLM-based prompting setup. During the discussion phase, they collaboratively develop more detailed and consistent annotation guidelines to ensure alignment in their final evaluations.</p>
<h1>I. Ablation Study Details</h1>
<p>Due to the experimental setting, the ablation study on paper writing simulation tasks does not include all the 1,000 tasks that existed in RESEARCHBENCH. Therefore, we provide detailed explanations and technical details for this.</p>
<p>Data for paper-writing researcher number ablation.. Not all papers in RESEARCHBENCH have more than five authors. To ablate the effect of the number of researchers ( 1 to 5), we select a subset from the hard part of PAPERBENCH within RESEARCHBENCH, including 333 paper writing tasks, ensuring each paper has more than five authors. This filtering results in a subset of 172 paper-writing tasks. We focus on the hard subset because we believe that involving multiple research agents in more challenging scenarios yields a more significant difference in performance.</p>
<p>Data for paper-writing paper number ablation.. In this ablation, we vary the number of cited papers included in different sections of the target paper. Specifically, we examine citations in the related work, introduction, and other sections. To do this, we retrieve the raw $\mathrm{ET}_{\mathrm{E}} \mathrm{X}$ source from arXiv and extract references at the section level. Due to varying data availability, we finalize a subset of RESEARCHBENCH that includes 296 paper-writing tasks for this study.</p>
<p>Data for review-writing researcher number ablation.. Since the reviewer construction does not depend on any complex data preprocessing, we do not encounter data issues for the review-writing ablation. Consequently, the ablation results are based on all 200 review-writing tasks in RESEARCHBENCH.</p>
<h2>J. Additional Experimental Results</h2>
<p>We provide more comprehensive experimental results on each sub-part of RESEARCHBENCH (PAPERBENCH, REVIEWBENCH, HIGHIMPACTPAPERBENCH) in this section.</p>
<p>Additional Results on PAPERBENCH. Table 5 shows that all models-Qwen-2.5-7B-Instruct, GPT-4o-mini, and Deepseekv3-consistently achieve better performance with richer reference contexts (AGG-data and AGG-global) compared to narrower ones (AGG-self and AGG-agent), highlighting the importance of contextual information in similarity evaluation.</p>
<p>Additional Results on REVIEWBENCH. As shown in Table 6, voyage-3 embeddings yield higher strength scores and larger $\Delta \mathbf{S}$ values than text-embedding-3, indicating greater discriminative power. While Qwen-2.5-7B-Instruct maintains strong similarity scores across all aggregation types, it exhibits larger deviations from human scores, suggesting potential scoring bias or overconfidence in its own outputs.</p>
<p>Additional results on HighIMPACTPAPERBENCH. Besides the full results on PAPERBENCH, we also evaluate RESEARCHTOWN under extreme conditions by attempting to simulate 100 of the most-cited machine learning papers from the past decade. RESEARCHTOWN achieves low similarity scores for papers introducing groundbreaking methods, such as "Layer Normalization" (Ba, 2016), or novel topics, such as "Energy and Policy Considerations for Deep Learning in NLP" (Strubell et al., 2019). However, the framework performs notably better on impactful papers focused on analysis or tool development. For instance, it achieves a similarity score exceeding 0.8 for papers like "Is BERT Really Robust? A Strong Baseline for Natural Language Attack on Text Classification and Entailment" (Jin et al., 2020), which provides adversarial analysis, and "Stanza: A Python Natural Language Processing Toolkit for Many Human Languages" (Qi et al., 2020), which offers a practical toolkit. These results suggest that high-impact research ideas may be more feasible than commonly perceived, and RESEARCHTOWN could potentially serve as a tool to inspire future impactful research.</p>
<h2>K. Additional Case Study</h2>
<p>Beyond the examples included in Case Study Section §11, we provide additional examples to show the generation results of our work and provide further insights about the strengths and weaknesses of RESEARCHTOWN.</p>
<p>Additional case study for in-distribution evaluation. Tables 18, 19, 20, 21, and 22 present examples of tasks and their corresponding outputs for the in-distribution evaluation of RESEARCHTOWN. These examples illustrate the evaluation process defined in this work.</p>
<p>Additional case study for out-of-distribution application. In Table 23, 24, 25, 26, 27, 28, 29, 30, 31, and 32, we show examples of the inputs and outputs of the out-of-distribution application of RESEARCHTOWN. Additionally, each table caption includes a brief comment on the quality of the generated papers for reference.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ https://cs.stanford.edu/people/widom/paper-writing.html
${ }^{2}$ https://blog.voyageai.com/2024/09/18/voyage-3/
${ }^{3}$ https://openai.com/index/new-embedding-models-and-api-updates/&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref6:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref7:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{3}$ https://neurips.cc/Conferences/2024
${ }^{4}$ https://openreview.net/group?id=ICLR.cc/ 2024/Conference&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-617 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-617</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-617</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-19.html">extraction-schema-19</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of variability, reproducibility, or stochasticity in language model-driven scientific experiments, including sources of variability, reproducibility metrics, and methods to improve reproducibility.</div>
                <p><strong>Paper ID:</strong> paper-186d26390779f7c54930e05812cfe85e6973961f</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/186d26390779f7c54930e05812cfe85e6973961f" target="_blank">With Little Power Comes Great Responsibility</a></p>
                <p><strong>Paper Venue:</strong> Conference on Empirical Methods in Natural Language Processing</p>
                <p><strong>Paper TL;DR:</strong> It is concluded that underpowered experiments are common in the NLP literature and an overview of best practices for power analysis in NLP is given and a series of notebooks are released to assist with future power analyses.</p>
                <p><strong>Paper Abstract:</strong> Despite its importance to experimental design, statistical power (the probability that, given a real effect, an experiment will reject the null hypothesis) has largely been ignored by the NLP community. Underpowered experiments make it more difficult to discern the difference between statistical noise and meaningful model improvements, and increase the chances of exaggerated findings. By meta-analyzing a set of existing NLP papers and datasets, we characterize typical power for a variety of settings and conclude that underpowered experiments are common in the NLP literature. In particular, for several tasks in the popular GLUE benchmark, small test sets mean that most attempted comparisons to state of the art models will not be adequately powered. Similarly, based on reasonable assumptions, we find that the most typical experimental design for human rating studies will be underpowered to detect small model differences, of the sort that are frequently studied. For machine translation, we find that typical test sets of 2000 sentences have approximately 75% power to detect differences of 1 BLEU point. To improve the situation going forward, we give an overview of best practices for power analysis in NLP and release a series of notebooks to assist with future power analyses.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e617.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e617.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of variability, reproducibility, or stochasticity in language model-driven scientific experiments, including sources of variability, reproducibility metrics, and methods to improve reproducibility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Simulation-based power analysis</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Simulation-based power analysis for NLP experiments</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A general, modular procedure that estimates statistical power by simulating datasets from an explicit generative process G(n, e*, h), running a chosen significance test T on many repetitions, and reporting the proportion of simulated datasets that yield significant results under assumed effect sizes and variances.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>With Little Power Comes Great Responsibility</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>unspecified / any NLP model comparisons</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Natural Language Processing (NLP)</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_task</strong></td>
                            <td>Estimating statistical power for comparisons between models, human evaluations, and corpus-level metrics via simulated datasets</td>
                        </tr>
                        <tr>
                            <td><strong>variability_sources</strong></td>
                            <td>assumed true effect size (e*), sample size (n), variance (components in h: variance by item, variance by rater, residual variance), choice of statistical test, significance threshold (alpha), agreement rate between models (P_a), corpus-level dependencies for metrics like BLEU (per-instance delta effects), and randomness in simulation/randomization test.</td>
                        </tr>
                        <tr>
                            <td><strong>variability_measured</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>variability_metrics</strong></td>
                            <td>power (proportion of simulated tests with p <= alpha and correct sign), Type-M and Type-S error rates (discussed/estimated in appendices), distributional parameters used in generative models (e.g., agreement rate P_a, P0 and b0 for BLEU per-sentence effects), standard deviation of observed effects in meta-analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>variability_results</strong></td>
                            <td>Simulations used thousands of repetitions; power curves produced showing dependence on n, effect size, and variance. Example quantitative findings from simulations: for MT, ~75% power to detect 1 BLEU point with n=2000 (using average fitted P0 and b0). For classifier comparisons and human evals, power varied widely depending on agreement and variance parameters (see other entries for specifics).</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_metrics</strong></td>
                            <td>Predicted power, predicted significance rate, and sensitivity analyses across assumed parameter regimes; ability to reproduce expected power using validation vs test (correlation measures); simulation replicability via repeated iterations.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_results</strong></td>
                            <td>Validation-test paired metrics showed very high correlation in SQuAD 2.0 (r = 0.99) supporting the use of dev predictions for power estimates; regressions for predicting typical improvements achieved high R^2 (e.g., R^2=0.69 for Delta_acc regression, R^2=0.97 for Pa regression on GLUE). Leave-one-out predictions estimated that 46% of past reported comparisons would have had adequate power and ~51% would have been significant.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_challenges</strong></td>
                            <td>Need for correct parameter estimation (effect sizes, variances); post-hoc power traps; domain shift between dev and test; scarcity of held-out evaluation data (esp. MT); missing reporting (raw data, ratings, model predictions) that prevents reproducing generative assumptions.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_methods</strong></td>
                            <td>Make explicit generative assumptions and run pre-data-collection power simulations across a range of plausible parameters; share model predictions and checkpoints; provide raw human ratings and code; conduct sensitivity analyses across parameter choices; choose larger test sets or increase numbers of raters/items.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Simulation methodology demonstrates how increasing n or reducing variance raises power (examples: MT 2k sentences → ~75% power for 1 BLEU; increasing annotators per item from 3 to 10 can enable detection of much smaller effects in human evals). No single uniform numeric reduction in variance reported because effects are scenario-specific.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_without_controls</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>number_of_runs</strong></td>
                            <td>Simulations performed with thousands of repetitions (paper generically refers to repeating thousands of times; exact r varies by experiment / simulation).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>A transparent simulation-based power analysis is practical for NLP tasks, can quantify how sample size, effect size, and variance affect power, and reveals many standard NLP experimental designs are underpowered unless assumptions (effect, agreement, variance) are favorable.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'With Little Power Comes Great Responsibility', 'publication_date_yy_mm': '2020-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e617.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e617.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of variability, reproducibility, or stochasticity in language model-driven scientific experiments, including sources of variability, reproducibility metrics, and methods to improve reproducibility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Classifier paired comparison (McNemar)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Paired classifier comparison using McNemar's test with agreement-rate modeling</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Comparing two classifiers on paired data using McNemar's test; power depends on sample size n, expected difference in accuracy Delta_acc, and expected agreement proportion P_a between models, and can be estimated by simulating contingency-table outcomes.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>With Little Power Comes Great Responsibility</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>unspecified classifiers / leaderboard models (e.g., GLUE/SQuAD competitors)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Natural Language Processing (model evaluation / benchmark comparisons)</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_task</strong></td>
                            <td>Testing whether a new model improves accuracy over a baseline on benchmark tasks (paired evaluation on same test items)</td>
                        </tr>
                        <tr>
                            <td><strong>variability_sources</strong></td>
                            <td>difference in accuracy between models (Delta_acc), expected agreement between models (P_a), test set size (n), variance arising from distribution of disagreements (off-diagonal counts), reliance on dev vs test estimation (domain shift), historical variability across leaderboards.</td>
                        </tr>
                        <tr>
                            <td><strong>variability_measured</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>variability_metrics</strong></td>
                            <td>agreement rate P_a, Delta_acc (difference in accuracy), regression R^2 when predicting these quantities from historical data, minimum detectable effect (MDE) for 80% power, proportion of reported effect sizes vs MDE, Type-M/Type-S considerations.</td>
                        </tr>
                        <tr>
                            <td><strong>variability_results</strong></td>
                            <td>Regression fits: predicting Delta_acc on GLUE had R^2=0.69; predicting P_a on GLUE (from model test predictions) gave R^2=0.97 (SQuAD P_a R^2=0.94). Many GLUE tasks (e.g., WNLI, MRPC, SST-2) have test sets where mean reported improvements are well below estimated MDEs; leave-one-out estimates suggest only ~46% of past comparisons would have had adequate power and ~51% would have been significant.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_metrics</strong></td>
                            <td>Leave-one-out regression predictions to estimate power/significance of previously reported improvements; dev-test correlation (SQuAD r=0.99) used to support reproducibility of parameter estimates across splits.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_results</strong></td>
                            <td>High dev-test correlation (r = 0.99) indicates paired dev predictions can predict test behavior; regression-based estimation suggests many past reported improvements were underpowered and at risk of Type-M exaggeration.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_challenges</strong></td>
                            <td>Small test sets limiting power (minimum detectable effects large), lack of significance testing reported in papers, possible inflation of validation performance, and the danger of Type-M/Type-S errors when underpowered.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_methods</strong></td>
                            <td>Use pre-experiment power analyses to assess MDE and required n; increase test set sizes or aggregate across tasks; release validation/test predictions and model checkpoints so paired agreements can be estimated; retire or expand datasets that no longer support detecting typical SOTA improvements.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Quantified via MDE calculations: for many GLUE tasks current MDEs exceed typical reported improvements, indicating expanding test sets or larger datasets would be effective; specific numeric improvements depend on dataset/task parameters.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_without_controls</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_runs</strong></td>
                            <td>Simulations for power estimation used many repetitions (thousands) per parameter setting; exact counts vary by analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Power for paired classifier comparisons depends critically on agreement P_a and Delta_acc; many popular benchmarks have test sets too small to detect the scale of improvements commonly reported, so underpowered comparisons and exaggerated significant findings are widespread.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'With Little Power Comes Great Responsibility', 'publication_date_yy_mm': '2020-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e617.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e617.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of variability, reproducibility, or stochasticity in language model-driven scientific experiments, including sources of variability, reproducibility metrics, and methods to improve reproducibility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MT BLEU randomization model</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Machine translation BLEU power analysis via Delta–Laplace per-sentence effect generative model and randomization test</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A generative model for MT evaluation that models the per-sentence effect of swapping outputs as a mixture of a point mass at zero (P0) and a Laplace distribution (parameter b0 scaled by n), enabling simulation-based power estimates for corpus-level BLEU with a paired randomization significance test.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>With Little Power Comes Great Responsibility</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>FAIRSEQ Transformer and Convolutional models (various WMT submissions: TF19, TF18, TF16, Conv17, Conv14)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Machine Translation (NLP evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_task</strong></td>
                            <td>Estimating power to detect differences in corpus-level BLEU between two MT models using a randomization test and a Delta-Laplace generative model of per-sentence BLEU swap effects</td>
                        </tr>
                        <tr>
                            <td><strong>variability_sources</strong></td>
                            <td>corpus-level dependence of BLEU (per-sentence delta_i effects), proportion of zero-effect sentences (P0), per-sentence effect dispersion controlled by b0 (and b0/n scaling), test set size n, BLEU difference Delta_B, scarcity of untouched evaluation data, variability across language pairs and test sets.</td>
                        </tr>
                        <tr>
                            <td><strong>variability_measured</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>variability_metrics</strong></td>
                            <td>fitted P0 and b0 parameters for Delta–Laplace mixture, observed Delta_B (BLEU differences), power as function of n and Delta_B, empirical goodness-of-fit of per-sentence effect mixture (visual/appendix), randomization-test p-values from permuted outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>variability_results</strong></td>
                            <td>Estimated parameters (Table 3) for four En-De comparisons: P0 = {0.19, 0.09, 0.12, 0.10} and b0 = {23.7, 29.4, 22.5, 27.6} respectively; using average fitted values, power curves indicate ~75% power to detect a 1 BLEU point difference with n = 2000; power increases strongly with n and Delta_B.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_metrics</strong></td>
                            <td>Fitting Delta–Laplace to several model/test pairs; sensitivity analyses across fitted P0 and b0 values; using randomization test repeated thousands of times to form null distribution.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_results</strong></td>
                            <td>Consistent parameter estimates across diverse MT model comparisons suggest the generative model generalizes for En–De; simulations show predictable power dependence on n and Delta_B (e.g., 2k→~75% for 1 BLEU).</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_challenges</strong></td>
                            <td>Limited untouched parallel evaluation data (most parallel corpora used in training), corpus-level metric dependence making per-instance independence invalid, potential domain/language-pair differences requiring refitting, and scarcity of large test sets.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_methods</strong></td>
                            <td>Fit per-sentence effect distributions to available held-out data to inform simulations; use randomization tests for significance; curate larger held-out test sets not used in training; run sensitivity analyses across P0/b0; aggregate results across larger evaluation sets.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Demonstrated numerically via simulations: increasing n increases power (e.g., power rises above 75% for 1 BLEU with larger n), and fitting the Delta–Laplace model allows realistic power estimation; exact variance reduction numbers depend on n and fitted parameters.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_without_controls</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>number_of_runs</strong></td>
                            <td>Randomization tests and simulations were repeated thousands of times per setting; exact counts not specified but described as 'thousands'.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>A Delta–Laplace mixture of per-sentence swap effects provides a practical generative model for BLEU power analysis; for typical MT test sets (~2000 sentences) the ability to detect a 1 BLEU improvement is limited (~75% power), and larger held-out sets would substantially improve detection power.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'With Little Power Comes Great Responsibility', 'publication_date_yy_mm': '2020-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e617.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e617.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of variability, reproducibility, or stochasticity in language model-driven scientific experiments, including sources of variability, reproducibility metrics, and methods to improve reproducibility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Human Likert-scale evaluation (mixed-effects)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Human evaluation of generated text using hierarchical mixed-effects models for power analysis</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Analysis of human Likert-scale evaluations using hierarchical mixed-effects models to estimate variance components (by item, by worker, item×model, worker×model, residual), enabling realistic simulation-based power curves for typical annotation designs and quantification of underpowered common practices.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>With Little Power Comes Great Responsibility</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>unspecified NLG systems / evaluated generation models (various contemporary models in convenience datasets)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Natural Language Generation / Human evaluation in NLP</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_task</strong></td>
                            <td>Detecting mean differences in Likert-scale human ratings between generation models using mixed-effects regression and power simulations</td>
                        </tr>
                        <tr>
                            <td><strong>variability_sources</strong></td>
                            <td>variance by item (different generated sentences), variance by worker (rater bias/scale), variance in worker sensitivity to model differences, variance in item sensitivity to model differences, residual measurement error, number of annotators per item, number of items rated, and missing reporting of annotation details.</td>
                        </tr>
                        <tr>
                            <td><strong>variability_measured</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>variability_metrics</strong></td>
                            <td>standard deviation of reported effects across experiments (meta-analysis s.d. = 0.12 on [0,1] scale), hierarchical mixed-effects model variance components (7 parameters: baseline, effect size, var_by_worker, var_by_worker×model, var_by_item, var_by_item×model, residual), power as function of items and annotators, and proportions of studies reporting key metadata.</td>
                        </tr>
                        <tr>
                            <td><strong>variability_results</strong></td>
                            <td>Meta-analysis: reported effect sizes s.d. = 0.12 on [0,1]. Reporting gaps: 69% of studies used ≤100 items; 57% collected 3 annotations per item; 34% omitted number of ratings per item; 28% omitted total number of workers. Power simulations: 'typical' EMNLP 2019 design (3 workers, 100 items) is underpowered unless effect ≥ 0.2 (on [0,1]); in 'low-variance' scenarios, detecting small effects (0.05) for 100 items requires ≥10 annotations/item.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_metrics</strong></td>
                            <td>Meta-analysis of 117 comparisons from 41 papers; mixed-effects model fits to several large convenience datasets; simulation-based power estimation across high/low variance parameter sets.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_results</strong></td>
                            <td>High variability across experiments and widespread missing reporting hinder reproducibility; simulations using empirically estimated variance components show many existing human evaluations lack adequate power and would benefit from more items/raters; sharing raw data and model details recommended to enable replication.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_challenges</strong></td>
                            <td>Frequently missing methodological details (ratings per item, number of workers), low numbers of items and raters in typical designs, heterogeneity in annotator behavior, and absence of shared raw rating data or code to replicate analyses.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_methods</strong></td>
                            <td>Fit hierarchical mixed-effects models for analysis and power estimation; collect more raters per item and/or more items; run pre-data-collection power analyses to choose sample sizes; share anonymized raw human data, parameters, and code; use mixed-effects regression for significance testing.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Simulations quantify effectiveness: e.g., increasing annotators per item from 3 to 10 can enable detection of small (0.05) effects with 100 items in low-variance settings; typical 3×100 designs detect only larger effects (≥0.2).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_without_controls</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_runs</strong></td>
                            <td>Simulations used many iterations (thousands) per parameter configuration; some convenience datasets provided up to ~20 ratings/item used to fit models.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Many published human evaluation designs in NLP are underpowered—especially typical 3-raters×~100-items setups—so researchers should collect more raters and/or items, use mixed-effects models for analysis, and share raw data to improve reproducibility.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'With Little Power Comes Great Responsibility', 'publication_date_yy_mm': '2020-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Beyond power calculations: Assessing type S (sign) and type M (magnitude) errors <em>(Rating: 2)</em></li>
                <li>Power failure: Why small sample size undermines the reliability of neuroscience <em>(Rating: 2)</em></li>
                <li>The hitchhiker's guide to testing statistical significance in natural language processing <em>(Rating: 2)</em></li>
                <li>Best practices for the human evaluation of automatically generated text <em>(Rating: 2)</em></li>
                <li>BERTs of a feather do not generalize together: Large variability in generalization across models with similar test set performance <em>(Rating: 1)</em></li>
                <li>Show your work: Improved reporting of experimental results <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-617",
    "paper_id": "paper-186d26390779f7c54930e05812cfe85e6973961f",
    "extraction_schema_id": "extraction-schema-19",
    "extracted_data": [
        {
            "name_short": "Simulation-based power analysis",
            "name_full": "Simulation-based power analysis for NLP experiments",
            "brief_description": "A general, modular procedure that estimates statistical power by simulating datasets from an explicit generative process G(n, e*, h), running a chosen significance test T on many repetitions, and reporting the proportion of simulated datasets that yield significant results under assumed effect sizes and variances.",
            "citation_title": "With Little Power Comes Great Responsibility",
            "mention_or_use": "use",
            "model_name": "unspecified / any NLP model comparisons",
            "model_size": null,
            "scientific_domain": "Natural Language Processing (NLP)",
            "experimental_task": "Estimating statistical power for comparisons between models, human evaluations, and corpus-level metrics via simulated datasets",
            "variability_sources": "assumed true effect size (e*), sample size (n), variance (components in h: variance by item, variance by rater, residual variance), choice of statistical test, significance threshold (alpha), agreement rate between models (P_a), corpus-level dependencies for metrics like BLEU (per-instance delta effects), and randomness in simulation/randomization test.",
            "variability_measured": true,
            "variability_metrics": "power (proportion of simulated tests with p &lt;= alpha and correct sign), Type-M and Type-S error rates (discussed/estimated in appendices), distributional parameters used in generative models (e.g., agreement rate P_a, P0 and b0 for BLEU per-sentence effects), standard deviation of observed effects in meta-analysis.",
            "variability_results": "Simulations used thousands of repetitions; power curves produced showing dependence on n, effect size, and variance. Example quantitative findings from simulations: for MT, ~75% power to detect 1 BLEU point with n=2000 (using average fitted P0 and b0). For classifier comparisons and human evals, power varied widely depending on agreement and variance parameters (see other entries for specifics).",
            "reproducibility_assessed": true,
            "reproducibility_metrics": "Predicted power, predicted significance rate, and sensitivity analyses across assumed parameter regimes; ability to reproduce expected power using validation vs test (correlation measures); simulation replicability via repeated iterations.",
            "reproducibility_results": "Validation-test paired metrics showed very high correlation in SQuAD 2.0 (r = 0.99) supporting the use of dev predictions for power estimates; regressions for predicting typical improvements achieved high R^2 (e.g., R^2=0.69 for Delta_acc regression, R^2=0.97 for Pa regression on GLUE). Leave-one-out predictions estimated that 46% of past reported comparisons would have had adequate power and ~51% would have been significant.",
            "reproducibility_challenges": "Need for correct parameter estimation (effect sizes, variances); post-hoc power traps; domain shift between dev and test; scarcity of held-out evaluation data (esp. MT); missing reporting (raw data, ratings, model predictions) that prevents reproducing generative assumptions.",
            "mitigation_methods": "Make explicit generative assumptions and run pre-data-collection power simulations across a range of plausible parameters; share model predictions and checkpoints; provide raw human ratings and code; conduct sensitivity analyses across parameter choices; choose larger test sets or increase numbers of raters/items.",
            "mitigation_effectiveness": "Simulation methodology demonstrates how increasing n or reducing variance raises power (examples: MT 2k sentences → ~75% power for 1 BLEU; increasing annotators per item from 3 to 10 can enable detection of much smaller effects in human evals). No single uniform numeric reduction in variance reported because effects are scenario-specific.",
            "comparison_with_without_controls": null,
            "number_of_runs": "Simulations performed with thousands of repetitions (paper generically refers to repeating thousands of times; exact r varies by experiment / simulation).",
            "key_findings": "A transparent simulation-based power analysis is practical for NLP tasks, can quantify how sample size, effect size, and variance affect power, and reveals many standard NLP experimental designs are underpowered unless assumptions (effect, agreement, variance) are favorable.",
            "uuid": "e617.0",
            "source_info": {
                "paper_title": "With Little Power Comes Great Responsibility",
                "publication_date_yy_mm": "2020-10"
            }
        },
        {
            "name_short": "Classifier paired comparison (McNemar)",
            "name_full": "Paired classifier comparison using McNemar's test with agreement-rate modeling",
            "brief_description": "Comparing two classifiers on paired data using McNemar's test; power depends on sample size n, expected difference in accuracy Delta_acc, and expected agreement proportion P_a between models, and can be estimated by simulating contingency-table outcomes.",
            "citation_title": "With Little Power Comes Great Responsibility",
            "mention_or_use": "use",
            "model_name": "unspecified classifiers / leaderboard models (e.g., GLUE/SQuAD competitors)",
            "model_size": null,
            "scientific_domain": "Natural Language Processing (model evaluation / benchmark comparisons)",
            "experimental_task": "Testing whether a new model improves accuracy over a baseline on benchmark tasks (paired evaluation on same test items)",
            "variability_sources": "difference in accuracy between models (Delta_acc), expected agreement between models (P_a), test set size (n), variance arising from distribution of disagreements (off-diagonal counts), reliance on dev vs test estimation (domain shift), historical variability across leaderboards.",
            "variability_measured": true,
            "variability_metrics": "agreement rate P_a, Delta_acc (difference in accuracy), regression R^2 when predicting these quantities from historical data, minimum detectable effect (MDE) for 80% power, proportion of reported effect sizes vs MDE, Type-M/Type-S considerations.",
            "variability_results": "Regression fits: predicting Delta_acc on GLUE had R^2=0.69; predicting P_a on GLUE (from model test predictions) gave R^2=0.97 (SQuAD P_a R^2=0.94). Many GLUE tasks (e.g., WNLI, MRPC, SST-2) have test sets where mean reported improvements are well below estimated MDEs; leave-one-out estimates suggest only ~46% of past comparisons would have had adequate power and ~51% would have been significant.",
            "reproducibility_assessed": true,
            "reproducibility_metrics": "Leave-one-out regression predictions to estimate power/significance of previously reported improvements; dev-test correlation (SQuAD r=0.99) used to support reproducibility of parameter estimates across splits.",
            "reproducibility_results": "High dev-test correlation (r = 0.99) indicates paired dev predictions can predict test behavior; regression-based estimation suggests many past reported improvements were underpowered and at risk of Type-M exaggeration.",
            "reproducibility_challenges": "Small test sets limiting power (minimum detectable effects large), lack of significance testing reported in papers, possible inflation of validation performance, and the danger of Type-M/Type-S errors when underpowered.",
            "mitigation_methods": "Use pre-experiment power analyses to assess MDE and required n; increase test set sizes or aggregate across tasks; release validation/test predictions and model checkpoints so paired agreements can be estimated; retire or expand datasets that no longer support detecting typical SOTA improvements.",
            "mitigation_effectiveness": "Quantified via MDE calculations: for many GLUE tasks current MDEs exceed typical reported improvements, indicating expanding test sets or larger datasets would be effective; specific numeric improvements depend on dataset/task parameters.",
            "comparison_with_without_controls": false,
            "number_of_runs": "Simulations for power estimation used many repetitions (thousands) per parameter setting; exact counts vary by analysis.",
            "key_findings": "Power for paired classifier comparisons depends critically on agreement P_a and Delta_acc; many popular benchmarks have test sets too small to detect the scale of improvements commonly reported, so underpowered comparisons and exaggerated significant findings are widespread.",
            "uuid": "e617.1",
            "source_info": {
                "paper_title": "With Little Power Comes Great Responsibility",
                "publication_date_yy_mm": "2020-10"
            }
        },
        {
            "name_short": "MT BLEU randomization model",
            "name_full": "Machine translation BLEU power analysis via Delta–Laplace per-sentence effect generative model and randomization test",
            "brief_description": "A generative model for MT evaluation that models the per-sentence effect of swapping outputs as a mixture of a point mass at zero (P0) and a Laplace distribution (parameter b0 scaled by n), enabling simulation-based power estimates for corpus-level BLEU with a paired randomization significance test.",
            "citation_title": "With Little Power Comes Great Responsibility",
            "mention_or_use": "use",
            "model_name": "FAIRSEQ Transformer and Convolutional models (various WMT submissions: TF19, TF18, TF16, Conv17, Conv14)",
            "model_size": null,
            "scientific_domain": "Machine Translation (NLP evaluation)",
            "experimental_task": "Estimating power to detect differences in corpus-level BLEU between two MT models using a randomization test and a Delta-Laplace generative model of per-sentence BLEU swap effects",
            "variability_sources": "corpus-level dependence of BLEU (per-sentence delta_i effects), proportion of zero-effect sentences (P0), per-sentence effect dispersion controlled by b0 (and b0/n scaling), test set size n, BLEU difference Delta_B, scarcity of untouched evaluation data, variability across language pairs and test sets.",
            "variability_measured": true,
            "variability_metrics": "fitted P0 and b0 parameters for Delta–Laplace mixture, observed Delta_B (BLEU differences), power as function of n and Delta_B, empirical goodness-of-fit of per-sentence effect mixture (visual/appendix), randomization-test p-values from permuted outputs.",
            "variability_results": "Estimated parameters (Table 3) for four En-De comparisons: P0 = {0.19, 0.09, 0.12, 0.10} and b0 = {23.7, 29.4, 22.5, 27.6} respectively; using average fitted values, power curves indicate ~75% power to detect a 1 BLEU point difference with n = 2000; power increases strongly with n and Delta_B.",
            "reproducibility_assessed": true,
            "reproducibility_metrics": "Fitting Delta–Laplace to several model/test pairs; sensitivity analyses across fitted P0 and b0 values; using randomization test repeated thousands of times to form null distribution.",
            "reproducibility_results": "Consistent parameter estimates across diverse MT model comparisons suggest the generative model generalizes for En–De; simulations show predictable power dependence on n and Delta_B (e.g., 2k→~75% for 1 BLEU).",
            "reproducibility_challenges": "Limited untouched parallel evaluation data (most parallel corpora used in training), corpus-level metric dependence making per-instance independence invalid, potential domain/language-pair differences requiring refitting, and scarcity of large test sets.",
            "mitigation_methods": "Fit per-sentence effect distributions to available held-out data to inform simulations; use randomization tests for significance; curate larger held-out test sets not used in training; run sensitivity analyses across P0/b0; aggregate results across larger evaluation sets.",
            "mitigation_effectiveness": "Demonstrated numerically via simulations: increasing n increases power (e.g., power rises above 75% for 1 BLEU with larger n), and fitting the Delta–Laplace model allows realistic power estimation; exact variance reduction numbers depend on n and fitted parameters.",
            "comparison_with_without_controls": null,
            "number_of_runs": "Randomization tests and simulations were repeated thousands of times per setting; exact counts not specified but described as 'thousands'.",
            "key_findings": "A Delta–Laplace mixture of per-sentence swap effects provides a practical generative model for BLEU power analysis; for typical MT test sets (~2000 sentences) the ability to detect a 1 BLEU improvement is limited (~75% power), and larger held-out sets would substantially improve detection power.",
            "uuid": "e617.2",
            "source_info": {
                "paper_title": "With Little Power Comes Great Responsibility",
                "publication_date_yy_mm": "2020-10"
            }
        },
        {
            "name_short": "Human Likert-scale evaluation (mixed-effects)",
            "name_full": "Human evaluation of generated text using hierarchical mixed-effects models for power analysis",
            "brief_description": "Analysis of human Likert-scale evaluations using hierarchical mixed-effects models to estimate variance components (by item, by worker, item×model, worker×model, residual), enabling realistic simulation-based power curves for typical annotation designs and quantification of underpowered common practices.",
            "citation_title": "With Little Power Comes Great Responsibility",
            "mention_or_use": "use",
            "model_name": "unspecified NLG systems / evaluated generation models (various contemporary models in convenience datasets)",
            "model_size": null,
            "scientific_domain": "Natural Language Generation / Human evaluation in NLP",
            "experimental_task": "Detecting mean differences in Likert-scale human ratings between generation models using mixed-effects regression and power simulations",
            "variability_sources": "variance by item (different generated sentences), variance by worker (rater bias/scale), variance in worker sensitivity to model differences, variance in item sensitivity to model differences, residual measurement error, number of annotators per item, number of items rated, and missing reporting of annotation details.",
            "variability_measured": true,
            "variability_metrics": "standard deviation of reported effects across experiments (meta-analysis s.d. = 0.12 on [0,1] scale), hierarchical mixed-effects model variance components (7 parameters: baseline, effect size, var_by_worker, var_by_worker×model, var_by_item, var_by_item×model, residual), power as function of items and annotators, and proportions of studies reporting key metadata.",
            "variability_results": "Meta-analysis: reported effect sizes s.d. = 0.12 on [0,1]. Reporting gaps: 69% of studies used ≤100 items; 57% collected 3 annotations per item; 34% omitted number of ratings per item; 28% omitted total number of workers. Power simulations: 'typical' EMNLP 2019 design (3 workers, 100 items) is underpowered unless effect ≥ 0.2 (on [0,1]); in 'low-variance' scenarios, detecting small effects (0.05) for 100 items requires ≥10 annotations/item.",
            "reproducibility_assessed": true,
            "reproducibility_metrics": "Meta-analysis of 117 comparisons from 41 papers; mixed-effects model fits to several large convenience datasets; simulation-based power estimation across high/low variance parameter sets.",
            "reproducibility_results": "High variability across experiments and widespread missing reporting hinder reproducibility; simulations using empirically estimated variance components show many existing human evaluations lack adequate power and would benefit from more items/raters; sharing raw data and model details recommended to enable replication.",
            "reproducibility_challenges": "Frequently missing methodological details (ratings per item, number of workers), low numbers of items and raters in typical designs, heterogeneity in annotator behavior, and absence of shared raw rating data or code to replicate analyses.",
            "mitigation_methods": "Fit hierarchical mixed-effects models for analysis and power estimation; collect more raters per item and/or more items; run pre-data-collection power analyses to choose sample sizes; share anonymized raw human data, parameters, and code; use mixed-effects regression for significance testing.",
            "mitigation_effectiveness": "Simulations quantify effectiveness: e.g., increasing annotators per item from 3 to 10 can enable detection of small (0.05) effects with 100 items in low-variance settings; typical 3×100 designs detect only larger effects (≥0.2).",
            "comparison_with_without_controls": false,
            "number_of_runs": "Simulations used many iterations (thousands) per parameter configuration; some convenience datasets provided up to ~20 ratings/item used to fit models.",
            "key_findings": "Many published human evaluation designs in NLP are underpowered—especially typical 3-raters×~100-items setups—so researchers should collect more raters and/or items, use mixed-effects models for analysis, and share raw data to improve reproducibility.",
            "uuid": "e617.3",
            "source_info": {
                "paper_title": "With Little Power Comes Great Responsibility",
                "publication_date_yy_mm": "2020-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Beyond power calculations: Assessing type S (sign) and type M (magnitude) errors",
            "rating": 2
        },
        {
            "paper_title": "Power failure: Why small sample size undermines the reliability of neuroscience",
            "rating": 2
        },
        {
            "paper_title": "The hitchhiker's guide to testing statistical significance in natural language processing",
            "rating": 2
        },
        {
            "paper_title": "Best practices for the human evaluation of automatically generated text",
            "rating": 2
        },
        {
            "paper_title": "BERTs of a feather do not generalize together: Large variability in generalization across models with similar test set performance",
            "rating": 1
        },
        {
            "paper_title": "Show your work: Improved reporting of experimental results",
            "rating": 1
        }
    ],
    "cost": 0.015279,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>With Little Power Comes Great Responsibility</h1>
<p>Dallas Card ${ }^{1}$ Peter Henderson ${ }^{1}$ Urvashi Khandelwal ${ }^{1}$ Robin Jia ${ }^{1}$<br>Kyle Mahowald ${ }^{2}$ Dan Jurafsky ${ }^{1}$<br>${ }^{1}$ Stanford University, Stanford, CA<br>${ }^{2}$ University of California Santa Barbara, Santa Barbara, CA<br>dcard@stanford.edu, phend@stanford.edu, urvashik@stanford.edu, robinjia@stanford.edu, mahowald@ucsb.edu, jurafsky@stanford.edu</p>
<h4>Abstract</h4>
<p>Despite its importance to experimental design, statistical power (the probability that, given a real effect, an experiment will reject the null hypothesis) has largely been ignored by the NLP community. Underpowered experiments make it more difficult to discern the difference between statistical noise and meaningful model improvements, and increase the chances of exaggerated findings. By metaanalyzing a set of existing NLP papers and datasets, we characterize typical power for a variety of settings and conclude that underpowered experiments are common in the NLP literature. In particular, for several tasks in the popular GLUE benchmark, small test sets mean that most attempted comparisons to state of the art models will not be adequately powered. Similarly, based on reasonable assumptions, we find that the most typical experimental design for human rating studies will be underpowered to detect small model differences, of the sort that are frequently studied. For machine translation, we find that typical test sets of 2000 sentences have approximately 75\% power to detect differences of 1 BLEU point. To improve the situation going forward, we give an overview of best practices for power analysis in NLP and release a series of notebooks to assist with future power analyses. ${ }^{1}$</p>
<h2>1 Introduction</h2>
<p>Despite its importance to empirical evaluation, relatively little attention has been paid to statistical power in NLP. In particular, if it is the case that typical experiments in NLP are underpowered, not only would we expect many meaningful improvements to go undetected, we would also expect many apparently significant differences to be exaggerated (Gelman and Carlin, 2014). In this paper, we build on past work calling for greater rigor</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Cartoon example of statistical power in comparing two models: $65 \%$ of all people in the population always prefer system B (left). A comparison using a sample of 100 people would be well-powered (middle): over $80 \%$ of such samples will show a significant difference (plotted in red) from the null hypothesis that the models are equally good (dashed line). In samples of 25 people (right), far fewer tests will be significant (power $\approx 30 \%$ ). Note that the observed mean of significant findings (dotted line) slightly overestimates the true proportion that prefer system B when $n=100$ and more severely overestimates it when $n=25$.
in evaluation (McCoy et al., 2019; Azer et al., 2020), including the need for careful hypothesis testing (Koehn, 2004; Berg-Kirkpatrick et al., 2012; Søgaard et al., 2014; Dror et al., 2018), and show why and how power matters to NLP, addressing challenges unique to this domain.</p>
<p>Roughly speaking, power is the probability that a statistical test will successfully detect a true effect. As an illustrative example, imagine comparing two dialog systems (see Figure 1). We want to know if people tend to prefer one system over the other. To test this, we will need multiple people to evaluate the systems. But how many? Once we have collected data, a statistical test will tell us if we can reject the null hypothesis the systems are equally good. Assuming the systems are not identical, statistical power is the probability that the experiment will return a significant result (or equivalently, it is one minus the probability of failing to detect the difference as significant). Although we don't know the magnitude of this difference, power analysis helps to estimate how much power an experiment</p>
<p>will have under various assumptions.
Power depends on multiple factors, including the statistical test used, the significance threshold, true effect size, variance, and sample size. All else being equal, experiments with larger samples will have greater power than smaller samples, as shown in Figure 1. Similarly, larger effects and those with less variance are easier to detect, and therefore require fewer samples for equivalent power. Importantly, note that if we do find a significant difference, this does not imply that the experiment had high power. ${ }^{2}$</p>
<p>Proceeding with a test that is underpowered (i.e., too few subjects or items; often taken to mean less than $80 \%$ power; Cohen, 1962) means that one is less likely to be able to draw any useful statistical conclusion from the experiment, and has contributed, in part, to the replication crisis in other fields (Button et al., 2013; Szucs and Ioannidis, 2017; Ioannidis et al., 2017). Routinely running experiments with low statistical power undermines the scientific enterprise. Not only will true effects go undetected; when significant effects are found, they are likely to be noisier and have lower positive predictive value (Button et al., 2013).</p>
<p>Moreover, significant findings from underpowered experiments are more likely to exaggerate or reverse the true effect - so-called Type-M (magnitude) and Type-S (sign) errors, respectively (Gelman and Carlin, 2014). This problem can lead to systematic distortions in the literature if only significant findings are published, especially if these results are based on underpowered experiments (Scargle, 1999). The effect of Type-M error can be seen in Figure 1; significant differences are less likely to be found in smaller samples (right), but among those tests that are significant, the observed difference will tend to exaggerate the true difference (left) by more than a larger sample (middle). For further discussion of Type-M and Type-S errors, please refer to Appendix B.</p>
<p>Here, we investigate how these issues affect NLP. Although retrospective analysis of power involves challenges, we present evidence that underpowered experiments are widespread in NLP research. Among human evaluations, we find most experimental designs involve too few items and/or raters</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>to detect small effects (§5). For comparing models in terms of accuracy, we find that some widely used benchmark datasets, including MRPC and SST-2, are now too small to be able to properly measure future progress against top performing models (§3). We also introduce a novel approach to power analysis for machine translation and characterize power in experiments testing for differences in BLEU (§4). Finally, a survey of recent papers reveals a general lack of statistical evaluation and a dearth of detailed reporting (§5.1).</p>
<p>To improve future practice, we suggest broader adoption of power analyses prior to evaluation, provide guidance on running power analyses in NLP, and release a series of notebooks for this purpose.</p>
<h2>2 Power Analysis for NLP</h2>
<p>Because most NLP tasks do not take the form of standard experiments in other sciences (Kraemer and Blasey, 2015; Westfall et al., 2014), it is nontrivial to run power analyses for many tasks of interest. While we cannot cover every scenario, we present here a generalizable, simulation-based approach to power analysis, along with three sample applications, which can be extended as necessary. Such an approach is modular, reusable, and transparent, and encourages planning of analyses in advance of data collection.</p>
<p>Every power analysis requires assumptions, and there is not likely to be a single correct approach. Rather, the point is to make one's assumptions explicit, and include enough detail so as to account for whatever is likely to be observed. By using reasonable assumptions, one can help to ensure that one's experiment is sufficiently well-powered, In the case of NLP, this means that one recruits enough subjects, collects enough ratings, or uses a large enough test set.</p>
<p>The general procedure we suggest for power analysis is described in detail in Figure 2. At a high level, the idea is to estimate power by running simulations. Recall that power is the probability of detecting a true effect, conditional on the experimental setting (effect size, variance, etc.) and significance threshold. Thus, if one can translate these assumptions into a process for generating simulated data, we can estimate power by generating many simulated datasets using assumed or estimated parameter values, running each sample through a significance test, and reporting the proportion that are found to be significant.</p>
<p>Define a generative process $G\left(n, e^{<em>}, \mathbf{h}\right)$ parameterized by number of items, $n$, hypothesized effect $e^{</em>}$ for the statistic of interest $E$, and other relevant parameters $\mathbf{h}$ (e.g., variance). Also choose a statistical test $T(\mathcal{D})$, which returns a p-value $p$ when performed on data $\mathcal{D}$ sampled from $G\left(n, e^{*}, \mathbf{h}\right)$. Finally, choose the size of the dataset to be sampled, $n$, significance threshold, $\alpha$, and number of repetitions, $r$.</p>
<ol>
<li>
<p>For $i$ in range( $r$ ):</p>
</li>
<li>
<p>sample a dataset of size $n, \mathcal{D}_{i} \sim G\left(n, e^{*}, \mathbf{h}\right)$</p>
</li>
<li>compute the effect of interest on this sample, $e_{i}=E\left(\mathcal{D}_{i}\right)$</li>
<li>
<p>also compute a p-value according to the test of interest: $p_{i}=T\left(D_{i}\right)$</p>
</li>
<li>
<p>Power $\approx \frac{1}{r} \sum\left(\mathbb{I}\left[p_{i} \leq \alpha\right] \cdot \mathbb{I}\left[\operatorname{sign}\left(e_{i}\right)=\operatorname{sign}\left(e^{*}\right)\right]\right)$</p>
</li>
</ol>
<p>Figure 2: An algorithm for power analysis by simulation. For the example of comparing two systems presented in Figure 1, $e^{<em>}$ is the assumed overall proportion of people who prefer system B, relative to the null hypothesis, $p=0.5, G\left(n, e^{</em>}, \mathbf{h}\right)$ is simply $\operatorname{Binomial}\left(n, 0.5+e^{*}\right)$, while $e_{i}$ is the observed proportion of people who prefer system B in sample $i$, again relative to 0.5 . For extensions to estimate Type-M and Type-S error, see Appendix B.</p>
<p>The key to generalizing this approach is to begin with the end in mind. In particular, if one plans to test for a difference between models, one needs to choose the statistical test that will be used. That test will determine the level of detail required in the generative process for simulating data.</p>
<p>To return to the opening example of evaluating dialog systems, we want to test if people prefer one system over the other (Ai et al., 2007). If we ignore the nuances of human preference for now (but see $\S 5$ for a more nuanced approach), and simply assume that each person either prefers system A or system B , the only assumption we need to make for a power analysis in this setting is the proportion of people in the population who prefer system B. We can then simulate samples of $n$ people (each of whom independently has the same probability of preferring system B) as a draw from a binomial distribution, and repeat this thousands of times. ${ }^{3}$ For each sample, we then test whether the proportion of people who prefer system B is significantly different from 0.5 . The estimated power of this experiment would thus be the proportion of simulated differences that are found to be significant. ${ }^{4}$</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup>The most difficult part of power analyses is estimating the relevant quantities, such as the true proportion of people that prefer system B. Note, however, that one can always compute what power would be for a range of possible values, and indeed, this is the recommended procedure. For estimating the relevant parameters within an NLP context, we will primarily rely on data from the literature, measurements on validation data, and estimates from external datasets (see $\S 3.2$ ). However, where appropriate, pilot studies may also be informative.</p>
<p>In the remainder of this paper, we consider three scenarios of interest in depth, and assess the state of power in the NLP literature for each.</p>
<h2>3 Comparing Models on Accuracy</h2>
<p>It is common in NLP research to look for models which improve over state of the art (SOTA) on various benchmarks. However, an important but rarely asked question is, can these benchmarks support the kinds of comparisons we want to make? Many have emphasized the need for proper significance testing to avoid spurious findings, but if an experiment's test set is small, the minimum detectable effect (MDE) size may be large: only large improvements will yield sufficiently powered comparisons (i.e., $\geq 80 \%$ power). If an experiment is badly underpowered, it cannot provide useful evidence that one model achieves slightly better performance than another for the underlying data distribution. Reliance on such evidence risks leading to over-confidence about the relative ranking of various models. As we show in $\S 3.3$, there is legitimate reason to be concerned about this in the case of certain widely used benchmarks.</p>
<h3>3.1 Significance test for comparing classifiers</h3>
<p>The standard statistical test for comparing classifiers on paired data is McNemar's test (Dietterich, 1998; Dror et al., 2018), which uses the numbers of items where the models disagree (i.e., the offdiagonal elements in Table 1). ${ }^{5}$ McNemar's test assesses whether $\chi^{2}=\frac{\left(p_{10}-p_{01}\right)^{2}}{p_{10}+p_{01}}$ is significant, and if so, rejects the null hypothesis that the distributions are the same.</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">M1 correct</th>
<th style="text-align: center;">M1 incorrect</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">M2 correct</td>
<td style="text-align: center;">both correct</td>
<td style="text-align: center;">only M2 correct</td>
</tr>
<tr>
<td style="text-align: left;">M2 incorrect</td>
<td style="text-align: center;">only M1 correct</td>
<td style="text-align: center;">both incorrect</td>
</tr>
</tbody>
</table>
<p>Table 1: A contingency table representing the distribution of possible outcomes for two models (M1 and M2).</p>
<p>Thus, for McNemar's test, the relevant data generating process for simulations can be specified in terms of the expected difference in accuracy between the models, $\Delta_{a c c}$, and $P_{a}$, the expected proportion of examples for which the models will have the same outcome (i.e., both correct or both incorrect). From these we can compute the expected proportions of examples on which only one model is correct (i.e., the off-diagonals in Table 1), and estimate power via the algorithm in Figure 2. Figure 3 illustrates how power increases with increased sample size, effect size, and agreement rate. ${ }^{6}$
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 3: Power for comparing two classifiers on accuracy using paired data depends on the size of the test set $(n)$, the expected agreement $\left(P_{a}\right)$, and the expected difference in accuracy $\left(\Delta_{a c c}\right)$. The dashed line shows $80 \%$ power, often taken to be a minimal requirement.</p>
<h3>3.2 Estimating parameters</h3>
<p>In order to estimate the required parameters ( $P_{a}$ and $\Delta_{a c c}$ ), we consider three options: (1) use results on validation (dev) data; (2) fit a regression based on historical data; (3) use middle-of-the-road assumptions when lacking other information. Using these methods, we can then estimate power or calculate the smallest effect that can be detected with $80 \%$ power at $\alpha=0.05$ (or other thresholds). Both to illustrate this process, and to provide guidance for future work, we demonstrate these approaches below using data from two widelyused datasets for evaluating NLP models: SQuAD 2.0 (Rajpurkar et al., 2016, 2018) and the GLUE benchmark (Wang et al., 2018).</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Using validation results: To the extent that we expect performance on test data to match performance on validation data (i.e., in the absence of domain shift), paired performance on validation data (i.e., difference in accuracy and agreement rate) provides one method for estimating power when comparing against a baseline model.</p>
<p>To illustrate this, from the authors of SQuAD 2.0, we obtain the pairwise agreement rates between all models submitted to the leaderboard on both validation and test data. We find a very strong correlation between validation and test for both pairwise accuracy differences $\left(\Delta_{a c c}\right)$ and agreement rates $\left(P_{a}\right)$ ( $r=0.99$ for both, as shown in Figure 9 in Appendix D, with results on validation data included in the accompanying online materials), suggesting we can use paired predictions on validation data for power calculations when we have access to the predictions from both models. Note that this approach assumes that the dev and test data have been drawn from the same distribution, and that dev performance has not been artificially inflated (such as by training on validation data directly).</p>
<p>Using historical data: When one does not have access to the baseline model or an informative prior, one can make use of historical trends. That is, we can try to estimate what a typical improvement will look like, given the current state of the art (SOTA). To illustrate this approach, we collect reported results for both SQuAD 2.0 and GLUE, and fit regressions to estimate $\Delta_{a c c}$ and $P_{a}$. Given these parameters, we can assess the likely power and MDE for a typical model improvement against a given baseline accuracy level.</p>
<p>To fit a regression to predict typical improvements to SOTA, we gather data from GLUE papers and manually label 119 accuracy comparisons and 57 claims of improvement (as denoted by bolding of a result and a claim of SOTA in text) across 14 papers (selected as being at or above the BERT score on the GLUE leaderboard with an accompanying paper). In regressing $\Delta_{a c c}$ on baseline accuracy and task, we achieve an $R^{2}=0.69$, which is not a perfect fit, but still provides a prior on likely effect size. Similarly, we achieve an $R^{2}=0.67$ when fitting a regression to SOTA improvements on the SQuAD 2.0 leaderboard (selected as being a significant improvement in time-ordered submissions). See Appendix E.2.1 for more details.</p>
<p>To assess power for McNemar's test, we must also fit a regression predicting the expected overlap</p>
<p>between the models $\left(P_{a}\right)$. To fit such a regression, from GLUE authors we obtain the model test set predictions on all tasks from a set of 10 highperforming models, which allows us to measure the extent to which their predictions overlap with each other. Using GLUE tasks which measure accuracy, we regress $P_{a}$ on baseline accuracy and $\Delta_{a c c}$, and achieve an $R^{2}$ of $0.97 .{ }^{7}$ Repeating this for SQuAD 2.0, we get an $R^{2}$ of 0.94 . See Appendix E. 2 for regression coefficients and additional details.</p>
<p>Typical improvements on popular tasks tend to be small (see mean improvements in Table 2). Except for rare transformative work, such as BERT (Devlin et al., 2019), it is generally difficult to do much better than a previous SOTA and thus improvements are likely to follow a trend, which is why we are able to use historical data as a guide. In cases where such data is not available or cannot be trusted, other methods are necessary.</p>
<p>No prior: If no informative prior is available and the baseline model or can't be used for comparison on a validation set, then we must fall back on middle of the road assumptions. Lachenbruch (1992) provides a suggested default prior, and we find that MDEs using this method are very similar to those found by using the regression based approach. Appendix E. 3 provides more details, and Table 9 in the appendix presents the comparison.</p>
<h3>3.3 Assessing power in the literature</h3>
<p>Using the regression-based approach of estimating $\Delta_{a c c}$ and $P_{a}$ described above, we estimate the MDE for each individual accuracy-based GLUE task in comparison to current SOTA, and report the average effect size of results which claimed improvements. Table 2 summarizes these results, showing for each dataset the size of the test set, the accuracy of the best performing model on each task at the time of writing, the estimated MDE to have $80 \%$ power using our regression to predict overlap $\left(P_{a}\right)$, and the average reported difference from their respective baselines.</p>
<p>As can be seen in Table 2, the mean reported effect size $\left(\left|\Delta_{a c c}\right|\right)$ is well below the estimated MDE for the three smallest test sets - WNLI, MRPC, and SST-2. Because this mean is based</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Table 2: Estimated minimum detectable effect (MDE) using a regression-based estimate of likely agreement with leaderboard SOTA as of May 6th, 2020. $\left|\Delta_{a c c}\right|$ is the average improvement over baseline per task among surveyed papers that claimed SOTA. For future comparisons, unless the expected improvement is larger than the estimated MDE, an experiment is unlikely to be adequately powered, and researchers should instead choose a different (larger) dataset. Note that this likely applies to the vast majority of experiments on WNLI, MRPC, and SST-2, based on recent trends. $\dagger$ indicates that the SQuAD 2.0 average was based on leaderboard improvements, which weren't necessarily reported in a publication. See Appendix E for full table and details.
on models comparing to even weaker baselines, we would expect most future improvements to be even smaller. Thus, most future experiments involving these three datasets will not have adequate power to test for improvements over the current SOTA in the way that they are routinely used. Moreover, alternative analyses give even more pessimistic estimates of likely improvements relative to MDE, as described in Appendix E.4. If an experiment does show significant improvement on a dataset such as MRPC, the potential for Type-M error should make us skeptical that this improvement will generalize to new data from the same domain.</p>
<p>While the above results are informative about future experiments, we would also ideally like to know about the power of past experiments. Most of the papers from which we collected results did not report a significance test on the test set. Here we estimate the expected power and predicted result of such a test using leave-one-out regressions, where we make a prediction for each reported improvement using all other reported model comparisons. This procedure reveals that only $\mathbf{4 6 \%}$ would have predicted adequate power (using estimates for expected improvement and agreement), and approximately $\mathbf{5 1 \%}$ would have been significant (based on estimated agreement and reported improvement). Approximately $80 \%$ of experiments with at least $80 \%$ power would also have been</p>
<p>found to be significant ( $37 \%$ of all comparisons).
In part because performance on many of these tasks is now so good, a large expected improvement is required in order for a new experiment to have $80 \%$ power, suggesting that larger test set sizes may be necessary to continue making wellpowered claims of SOTA improvement on individual tasks. For any comparisons which are likely to be underpowered, we should refrain from placing much emphasis on obtaining small improvements over the previously reported best model. In extreme cases, such as MRPC and SST-2, it is worth considering whether it is time to retire these datasets as the basis for model comparison. ${ }^{8}$</p>
<h2>4 Machine Translation</h2>
<p>To show how our approach to power analysis can be applied to a more difficult setting, we consider automated evaluation of machine translation using BLEU scores (Papineni et al., 2002). As with accuracy, we would like to know what scale of improvements can be detected with reasonable power on typical test sets. This setting is more complicated because (1) BLEU is a corpus-level metric, rather than being averaged across instances, and (2) typical models are trained on vast amounts of parallel data, with little data available that has not been used in training, making it difficult to estimate variation in performance.</p>
<p>Significance testing for BLEU: To test for a significant difference between two MT models we use the randomization test, as recommended in Dror et al. (2018): given the paired output translations from both models, swap the outputs for a random subset of test examples and compute the resulting difference in BLEU. Repeating this thousands of times gives us a null distribution, which can be used to test the observed difference between models.</p>
<p>Generative process for simulations: If large amounts of untouched evaluation data were available, we could approach power analysis by simply evaluating BLEU score on many random subsets of $n$ sentences, and computing the mean and variance of each system. Unfortunately, because MT depends on parallel text (most of which is used in training), evaluation data tends to be scarce. In-</p>
<p><sup id="fnref6:0"><a class="footnote-ref" href="#fn:0">1</a></sup>stead, we introduce a generative process that can produce the necessary inputs for power analysis.</p>
<p>For intuition, note that if we swap the $i^{\text {th }}$ pair of model outputs (as is done in the randomization test), leaving rest as they are, we change the difference in BLEU between models by a specific amount, $\delta_{i}$, which we call the effect of making that swap. While these individual effects are not independent of each other due to the corpus-level nature of the metric, in practice, the sum of individual effects closely approximates the net effect of swapping entire subsets (see Figure 15 in Appendix G).</p>
<p>Based on analyzing several models and datasets, we find the typical distribution of these individual effects can be approximated using a mixture of a Delta distribution at zero, and a Laplace distribution (see Appendix G for details). Concretely, if we assume $\Delta_{B}$ is the expected difference in BLEU between two models on a dataset of $n$ examples, and $P_{0}$ is the expected proportion of examples for which $\delta_{i}=0$, we can simulate a dataset $\left{\delta_{i}\right}<em 0="0">{i=1}^{n}$ of $n$ individual effects using the following process: with probability $P</em>$}, \delta_{i}=0$. With probability $1-P_{0}, \delta_{i} \sim \operatorname{Laplace}(\mu, b)$, where $\mu=\frac{-2 \cdot \Delta_{B}}{n\left(1-P_{0}\right)}$, $b=b_{0} / n$, and $b_{0}$ is a user-specified parameter that controls the variance, independent of the sample size. By construction, $\mathbb{E}\left[\sum_{i=1}^{n} \delta_{i}\right]=-2 \cdot \Delta_{B} .{ }^{9</p>
<p>Given this generative process, we can then estimate power using the Algorithm in Figure 2. On each iteration, draw a simulated dataset from the generative process, compute the observed difference between models as $\hat{\Delta}<em i="1">{B}=-\frac{1}{2} \sum</em>$ 's in the subset. (Please see online materials for an interactive example).}^{n} \delta_{i}$, and test if this is significantly different from zero using a modified randomization test, in which we assume that the net effect of swapping a subset of instances is simply the sum of the $\delta_{i</p>
<p>Empirical estimates: In order to estimate reasonable values for the required parameters, we use several pretrained models from the FAIRSEQ library (Ott et al., 2019) for the WMT English-German translation task. We evaluate these models on the shared task test sets from 2016-2019 and compute BLEU scores using SACREBLEU (Post, 2018). Fitting a Delta-Laplace mixture to the effects of swapping individual output pairs, we estimate values for $\hat{P}<em 0="0">{0}$ and $\hat{b}</em>$, reported in Table 3. (See also Figure 16 in Appendix G; code for computing estimates is provided in the online materials).</p>
<p><sup id="fnref2:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<table>
<thead>
<tr>
<th>M1</th>
<th>M2</th>
<th>Test set</th>
<th>$n$</th>
<th>$\Delta_{\mathrm{B}}$</th>
<th>$\hat{P}_{0}$</th>
<th>$\hat{b}_{0}$</th>
</tr>
</thead>
<tbody>
<tr>
<td>TF19^{∗}</td>
<td>TF18</td>
<td>2019</td>
<td>2K</td>
<td>4.3</td>
<td>0.19</td>
<td>23.7</td>
</tr>
<tr>
<td>TF18</td>
<td>TF16</td>
<td>2018</td>
<td>3K</td>
<td>4.2</td>
<td>0.09</td>
<td>29.4</td>
</tr>
<tr>
<td>TF16</td>
<td>Conv17</td>
<td>2017</td>
<td>3K</td>
<td>1.3</td>
<td>0.12</td>
<td>22.5</td>
</tr>
<tr>
<td>TF16</td>
<td>Conv14</td>
<td>2016</td>
<td>3K</td>
<td>7.6</td>
<td>0.10</td>
<td>27.6</td>
</tr>
</tbody>
</table>
<p>Table 3: Relevant parameters from four MT evaluations. TF are Transformer-based (Ott et al., 2018; Edunov et al., 2018; Ng et al., 2019) and Conv are Convolutional models (Gehring et al., 2017) from FAIRSEQ. Test sets are from WMT shared tasks for En-De translation. $\Delta_{B}$ is the reported difference in BLEU, whereas $\hat{P}<em 0="0">{0}$ and $\hat{b}</em>$ are estimated. * indicates ensembles.
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 4: Power analysis for MT, showing how power increases with $n$ and $\Delta_{B}$, using an average of fitted values for $P_{0}$ and $b_{0}$. Based on this analysis, we expect that an experiment with a test set of 2000 sentences would have approximately $75 \%$ power to detect a difference of 1 BLEU point as significant. For additional plots, refer to Figure 17 in Appendix G.</p>
<p>While far from identical, the four comparisons, each representing different stages of model evolution, all produce similar estimates. Although these estimates are only based on a single language pair, the models and test sets are relatively diverse, and we expect that these estimates will generalize, though better estimates could be obtained by fitting this distribution to a new domain of interest.</p>
<p>Using these estimates, we can now characterize how much power test sets of different test set sizes $(n)$ would have for a range of possible differences in BLEU $\left(\Delta_{B}\right)$. Figure 4 shows this for $P_{0}$ and $b_{0}$ set to the average of the observed values. ${ }^{10}$ Based on this estimate, we conclude that for typical MT test sets of around 2,000 examples, an improvement of 1 BLEU point can likely be detected with approximately 75\% power. As shown in Figure 4 this power level increases dramatically with sample size and effect size.</p>
<p>This analysis has served, in part, to show how a simulation-based approach to power analysis can</p>
<p><sup id="fnref7:0"><a class="footnote-ref" href="#fn:0">1</a></sup>be adapted to virtually any task. Additional work is required to test how well these specific parameter estimates will generalize, but the same process can easily be adapted to new language pairs. More generally, there would be great value in the MT community curating larger held-out test sets, both to validate this analysis, and for better powered future comparison.</p>
<h2>5 Likert-Scale Human Evaluations</h2>
<p>Tasks such as natural language generation are difficult to evaluate using automated methods; as such, human evaluations are central to NLP. Past work has reported great variation in how human evaluations are done (van der Lee et al., 2019). Therefore, we begin with a meta-analysis of a subset of human evaluation experiments from EMNLP 2019, which we then use as the basis for claims about the power of human evaluations in NLP more generally.</p>
<h3>5.1 Meta-analysis</h3>
<p>To characterize the state of human evaluation in NLP, we identified papers from the main session of EMNLP 2019 that made use of human evaluations (details in Appendix H.2). To generalize across studies, we restrict our analysis to Likertscale comparisons, which was the most commonly reported type of evaluation. We extracted all cases where a new model was being compared to the best-performing baseline on one more metrics (117 comparisons from 41 papers) and normalized all ratings to be on a $0-1$ scale.</p>
<p>One takeaway from this meta-analysis is that the reported effect sizes (that is, difference between the novel model and the best-performing baseline) vary widely (s.d. $=.12$ on a $[0,1]$ scale). Number of items tested is more consistent: $69 \%$ used 100 or fewer, and only $18 \%$ used over 200. But, as similarly found by van der Lee et al. (2019), many key details were not reported in this sample of experiments. Most commonly missing was number of ratings per item ( $34 \%$ of all experiments), followed by total number of workers (28\%). For 7\% of experiments, we could not determine the number of items tested. $57 \%$ of experiments collected 3 annotations per item, which was also the modal number of unique annotators. Thus, it is often difficult to ascertain, for any particular experiment, the details of the experimental setting that are necessary to evaluate the validity of the results.</p>
<p>Because the number of items rated was the most</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 5: Scaled effect size vs. number of items from our EMNLP 2019 survey, showing higher variance in the smallest samples. There is a slight negative correlation, though it is not significant. As can be seen, most experiments are small (n ≤ 100).</p>
<p>commonly reported, we use that as our proxy for sample size. Figure 5 shows scaled mean difference between models as a function of number of items. As expected, we see greater variance in effects with smaller samples since, with smaller samples, we expect greater noise. We also observe a slight negative correlation between effect size and sample size. That is, as sample size gets larger (and, thus, as estimates get more precise), the estimated effect size gets smaller. This trend is sometimes used as an indication of publication bias (censoring of null and opposite-direction effects) since, in a sample with no publication bias, the effect size should be independent of the sample size (Begg and Mazumdar, 1994). However, in our case, this correlation is not significant (Kendall's τ = -.07, p = .32) and so it is difficult to draw strong conclusions.11</p>
<h3>5.2 Power analysis for human Likert ratings</h3>
<p>What kind of effect sizes can typical human evaluation experimental designs detect? As in previous sections, we can use simulations to explore how many annotators and/or instances should be used to have sufficient power.</p>
<p>Simulating human experiments is conceptually simple (e.g., m raters each rate n generated sentence on overall quality), but for realistic simulations, we need to consider variation in items (some generated sentences are better than others), and variation by rater (some raters use higher ratings and/or respond to different aspects of quality), as well as the overall difference in quality between models. A simulation which treated all workers as identical would fail to capture this variation, and hence might overestimate power (Barr et al., 2013).</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 6: Using parameters estimated with mixed effects models from a high variance setting (top) and a low variance setting (bottom), the left panel shows simulated experiments with 3 workers annotating each item, the right panel shows an unusually high number of annotators per item (10 workers). Under typical assumptions, many common experimental settings (e.g., 3 workers and 100 items) are underpowered.</p>
<p>Unfortunately, details such as worker variance are rarely reported in published papers. To better characterize the typical variation in human evaluations, we rely on a convenience sample of several large datasets to estimate these parameters and use them in our simulations as a proxy for what we might observe in practice. Although focused on different tasks, all use a similar methodology, namely, getting many Likert-scale annotations per instance from many annotators and models (in some cases as many as 20 ratings per item).12</p>
<p>In order to extract estimates of these parameters for our simulations, we use hierarchical mixed-effects models, as used in psychology and other behavioral fields (Barr et al., 2013; Gelman and Hill, 2006). Such models incorporate variation in the quality of generated instances, annotator responses, and annotator sensitivity, and are recommended by van der Lee et al. (2019) for analyzing human evaluations. (We provide details in Appendix H.3 and include code for fitting such models as part of the online materials). Using this approach, we obtain an estimate of the relevant parameters from each of the large datasets. From these, we choose sets of parameters to be representative of experiments with high or low variance, with full results in Appendix H.3 (see Table 16 for parameter estimates).</p>
<p>As before, we then use these estimates to simulate data, assess significance on the simulated data (here using mixed effect regression), and compute power as a function of mean difference and sample</p>
<p><sup>11</sup>We exclude from this analysis two large negative effects with N = 500 which would exaggerate this correlation.</p>
<p><sup>12</sup>We use publicly available or author-provided data from Hashimoto et al. (2019); Dathathri et al. (2020); Holtzman et al. (2020), and WMT19 (links in Appendix H.2).</p>
<p>size. ${ }^{13}$ The resulting power estimates are shown in Figure 6, plotted in terms of effect size, sample size, and numbers of workers and items, for both the high and low variance scenarios. From this analysis, we highlight a few key takeaways:</p>
<ul>
<li>Many human evaluation studies are likely underpowered: Using the "high variance" parameters (which are typical of most of the datasets we used), the most common design at EMNLP 2019 (3 workers, 100 items) is underpowered unless the effect size is quite large ( 0.2 or higher on the $[0,1]$ scale).</li>
<li>Even with low variance, typical designs are underpowered to detect small effects: Using our estimated parameters for the low variance setting, experiments will be underpowered to detect small effects ( 0.05 on the $[0,1]$ scale), unless an unusually large number of ratings per item are collected ( $10+$ for 100 items).</li>
<li>Need for improved reporting: Most human evaluations do not report enough detail to interpret the results. This could be drastically improved through basic power analyses, significance testing using mixed-effects models, and sharing of raw data.</li>
</ul>
<p>Given our model estimates and simulations, we conclude that, in aggregate, many human evaluations are underpowered and would benefit from larger sample sizes, particularly by using more workers per item. Increased adoption of even approximate power calculations within the NLP community will promote thoughtful consideration of appropriate sample sizes and improve the reliability and replicability of results.</p>
<h2>6 Overall Recommendations</h2>
<ul>
<li>Power analyses should be done prior to evaluation when comparing against a baseline. If a comparison is likely to be underpowered, the pros and cons of running that evaluation should be carefully considered. Underpowered experiments do not provide convincing evidence of progress.</li>
<li>For new datasets and shared tasks, the number of instances in the test will determine the</li>
</ul>
<p><sup id="fnref8:0"><a class="footnote-ref" href="#fn:0">1</a></sup>minimum detectable effect size, and should be chosen accordingly.</p>
<ul>
<li>For tasks which no longer have adequate power to detect typical improvements (e.g., MRPC and SST-2), authors should consider expanding the test set or retiring the task.</li>
<li>To facilitate future power calculation and significance tests, model owners should release final fine-tuned model checkpoints. Alternatively, leaderboard owners may wish to make validation set predictions from all submitted models publicly available.</li>
<li>For human evaluations, (anonymized) raw data should be shared, along with parameters and code to replicate the analysis, including proper significance testing. Prior to collecting human evaluation data, researchers should create an analysis plan and run power analyses to determine an appropriate sample size (likely requiring more workers and items than is currently typical in NLP).</li>
</ul>
<h2>7 Conclusion</h2>
<p>Recent progress in NLP has been extraordinarily rapid, sometimes at the cost of experimental rigor. In this paper, we have presented evidence that underpowered experiments are widespread in NLP. For comparisons based on small samples, there is little reason to think that such an evaluation could reliably provide evidence of a significant improvement, and good reason to believe that improvements found to be significant will exaggerate or reverse the true effect. Going forward, a combination of larger test sets, simple power analyses, and wider sharing of code, data, and experimental details will help to build the foundation for a higher standard of experimental methodology in NLP.</p>
<h2>Acknowledgments</h2>
<p>Toyota Research Institute ("TRI") provided funds to assist the authors with their research but this article solely reflects the opinions and conclusions of its authors and not TRI or any other Toyota entity. Thanks to Sam Bowman, Amanpreet Singh, Kevin Clark, Naman Goyal, and Colin Raffel for providing data from submissions to the GLUE leaderboard, as well as Taylor Berg-Kirkpatrick, Sumanth Dathathri, Ari Holtzman, Hannah Rashkin, and Nikita Srivatsan for providing raw human evaluation data, not all of which made it into the paper.</p>
<h2>References</h2>
<p>Hua Ai, Antoine Raux, Dan Bohus, Maxine Eskenazi, and Diane Litman. 2007. Comparing spoken dialog corpora collected with recruited subjects versus real users. In Proceedings SIGdial.</p>
<p>Frank J. Anscombe. 1954. Fixed-sample-size analysis of sequential observations. Biometrics, 10:89-100.</p>
<p>Matthias G. Arend and Thomas Schäfer. 2019. Statistical power in two-level models: A tutorial based on Monte Carlo simulation. Psychological methods, 24(1):1-19.</p>
<p>Erfan Sadeqi Azer, Daniel Khashabi, Ashish Sabharwal, and Dan Roth. 2020. Not all claims are created equal: Choosing the right statistical approach to assess hypotheses. In Proceedings of ACL.</p>
<p>Roy Bar-Haim, Ido Dagan, Bill Dolan, Lisa Ferro, Danilo Giampiccolo, Bernardo Magnini, and Idan Szpektor. 2006. The second PASCAL recognising textual entailment challenge. In Proceedings of the second PASCAL challenges workshop on recognising textual entailment.</p>
<p>Dale J. Barr, Roger Levy, Christoph Scheepers, and Harry J. Tily. 2013. Random effects structure for confirmatory hypothesis testing: Keep it maximal. Journal of Memory and Language, 68(3):255-278.</p>
<p>Colin B. Begg and Madhuchhanda Mazumdar. 1994. Operating characteristics of a rank correlation test for publication bias. Biometrics, 50(4):1088-1101.</p>
<p>Luisa Bentivogli, Peter Clark, Ido Dagan, and Danilo Giampiccolo. 2009. The fifth PASCAL recognizing textual entailment challenge. In Proceedings of TAC.</p>
<p>Taylor Berg-Kirkpatrick, David Burkett, and Dan Klein. 2012. An empirical investigation of statistical significance in NLP. In Proceedings of EMNLP.</p>
<p>Katherine S. Button, John P. A. Ioannidis, Claire Mokrysz, Brian A. Nosek, Jonathan Flint, Emma S. J. Robinson, and Marcus R. Munafò. 2013. Power failure: Why small sample size undermines the reliability of neuroscience. Nature Reviews Neuroscience, 14(5):365-376.</p>
<p>Boxing Chen and Colin Cherry. 2014. A systematic comparison of smoothing techniques for sentencelevel BLEU. In Proceedings of WMT.</p>
<p>Kevin Clark, Minh-Thang Luong, Urvashi Khandelwal, Christopher D. Manning, and Quoc V. Le. 2019a. BAM! Born-again multi-task networks for natural language understanding. In Proceedings of ACL.</p>
<p>Kevin Clark, Minh-Thang Luong, Quoc V. Le, and Christopher D. Manning. 2019b. ELECTRA: Pretraining text encoders as discriminators rather than generators. In Proceedings of ICLR.</p>
<p>Jacob Cohen. 1962. The statistical power of abnormalsocial psychological research: A review. Journal of Abnormal and Social Psychology, 65(3):145-153.</p>
<p>John E. Connett, Judith A. Smith, and Richard B. McHugh. 1987. Sample size and power for pairmatched case-control studies. Statistics in Medicine, $6(1): 53-59$.</p>
<p>Ido Dagan, Oren Glickman, and Bernardo Magnini. 2005. The PASCAL recognising textual entailment challenge. In Proceedings of the Machine Learning Challenges Workshop.</p>
<p>Sumanth Dathathri, Andrea Madotto, Janice Lan, Jane Hung, Eric Frank, Piero Molino, Jason Yosinski, and Rosanne Liu. 2020. Plug and play language models: A simple approach to controlled text generation. In Proceedings of ICLR.</p>
<p>Janez Demšar. 2006. Statistical comparisons of classifiers over multiple data sets. J. Mach. Learn. Res., 7:1-30.</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of NAACL.</p>
<p>Thomas G. Dietterich. 1998. Approximate statistical tests for comparing supervised classification learning algorithms. Neural computation, 10(7):18951923.</p>
<p>Jesse Dodge, Suchin Gururangan, Dallas Card, Roy Schwartz, and Noah A. Smith. 2019. Show your work: Improved reporting of experimental results. In Proceedings of EMNLP.</p>
<p>William B. Dolan and Chris Brockett. 2005. Automatically constructing a corpus of sentential paraphrases. In Proceedings of the Third International Workshop on Paraphrasing (IWP2005).</p>
<p>Rotem Dror, Gili Baumer, Segev Shlomov, and Roi Reichart. 2018. The hitchhiker's guide to testing statistical significance in natural language processing. In Proceedings of $A C L$.</p>
<p>Stephen W. Duffy. 1984. Asymptotic and exact power for the McNemar test and its analogue with R controls per case. Biometrics, 40:1005-1015.</p>
<p>Sergey Edunov, Myle Ott, Michael Auli, and David Grangier. 2018. Understanding back-translation at scale. In Proceedings of EMNLP.</p>
<p>Morten W. Fagerland, Stian Lydersen, and Petter Laake. 2013. The McNemar test for binary matched-pairs data: mid- $p$ and asymptotic are better than exact conditional. BMC Medical Research Methodology, 13.</p>
<p>Cristina Garbacea, Samuel Carton, Shiyan Yan, and Qiaozhu Mei. 2019. Judge the judges: A large-scale evaluation study of neural language models for online review generation. In Proceedings of EMNLP.</p>
<p>Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. 2017. Convolutional sequence to sequence learning. In Proceedings of ICML.</p>
<p>Andrew Gelman. 2019. Don't calculate post-hoc power using observed estimate of effect size. Annals of Surgery, 269(1):e9-e10.</p>
<p>Andrew Gelman and John Carlin. 2014. Beyond power calculations: Assessing type S (sign) and type M (magnitude) errors. Perspectives on Psychological Science, 9(6):641-651.</p>
<p>Andrew Gelman and Jennifer Hill. 2006. Data Analysis Using Regression and Multilevel/Hierarchical Models. Cambridge University Press.</p>
<p>Andrew Gelman and Eric Loken. 2013. The garden of forking paths: Why multiple comparisons can be a problem, even when there is no "fishing expedition" or "p-hacking" and the research hypothesis was posited ahead of time.</p>
<p>Danilo Giampiccolo, Bernardo Magnini, Ido Dagan, and Bill Dolan. 2007. The third PASCAL recognizing textual entailment challenge. In Proceedings of the ACL-PASCAL Workshop on Textual Entailment and Paraphrasing.</p>
<p>Yvette Graham, Nitika Mathur, and Timothy Baldwin. 2014. Randomized significance tests in machine translation. In Proceedings of WMT.</p>
<p>Tatsunori B Hashimoto, Hugh Zhang, and Percy Liang. 2019. Unifying human and statistical evaluation for natural language generation. In Proceedings of NAACL.</p>
<p>John M. Hoenig and Dennis M. Heisey. 2001. The abuse of power: The pervasive fallacy of power calculations for data analysis. The American Statistician, 55(1):19-24.</p>
<p>Ari Holtzman, Jan Buys, Maxwell Forbes, and Yejin Choi. 2020. The curious case of neural text degeneration. In Proceedings of ICLR.</p>
<p>John P. A. Ioannidis. 2019. What have we (not) learnt from millions of scientific papers with $P$ values? The American Statistician, 73(sup1):20-25.</p>
<p>John P. A. Ioannidis, T. D. Stanley, and Hristos Doucouliagos. 2017. The power of bias in economics research. The Economic Journal, 127(605):F236F265.</p>
<p>Shankar Iyer, Nikhil Dandekar, and Kornél Csernai. 2017. First Quora dataset release: Question pairs.</p>
<p>Philipp Koehn. 2004. Statistical significance tests for machine translation evaluation. In Proceedings of EMNLP.</p>
<p>Helena C. Kraemer and Christine Blasey. 2015. How Many Subjects?: Statistical Power Analysis in Research. SAGE.</p>
<p>Peter A Lachenbruch. 1992. On the sample size for studies based upon McNemar's test. Statistics in Medicine, 11(11):1521-1525.</p>
<p>Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut. 2020. ALBERT: A lite BERT for self-supervised learning of language representations. In Proceedings of ICLR.</p>
<p>Chris van der Lee, Albert Gatt, Emiel van Miltenburg, Sander Wubben, and Emiel Krahmer. 2019. Best practices for the human evaluation of automatically generated text. In Proceedings of INLG.</p>
<p>Hector J. Levesque, Ernest Davis, and Leora Morgenstern. 2012. The Winograd schema challenge. In Proceedings of $K R$.</p>
<p>Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. ROBERTA: A robustly optimized BERT pretraining approach. Computing Research Repository, arXiv:1907.11692.
R. Thomas McCoy, Junghyun Min, and Tal Linzen. 2019. BERTs of a feather do not generalize together: Large variability in generalization across models with similar test set performance. Computing Research Repository, arXiv:1911.02969.</p>
<p>Blakeley B. McShane, David Gal, Andrew Gelman, Christian Robert, and Jennifer L. Tackett. 2019. Abandon statistical significance. The American Statistician, 73(sup1):235-245.</p>
<p>Nathan Ng, Kyra Yee, Alexei Baevski, Myle Ott, Michael Auli, and Sergey Edunov. 2019. Facebook FAIR's WMT19 news translation task submission. In Proceedings of WMT.</p>
<p>Daniel J. O'Keefe. 2007. Brief report: Post hoc power, observed power, a priori power, retrospective power, prospective power, achieved power: Sorting out appropriate uses of statistical power analyses. Communication Methods and Measures, 1(4):291-299.</p>
<p>Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier, and Michael Auli. 2019. FAIRSEQ: A fast, extensible toolkit for sequence modeling. In Proceedings of NAACL.</p>
<p>Myle Ott, Sergey Edunov, David Grangier, and Michael Auli. 2018. Scaling neural machine translation. In Proceedings of WMT.</p>
<p>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. BLEU: A method for automatic evaluation of machine translation. In Proceedings of $A C L$.</p>
<p>Jason Phang, Thibault Févry, and Samuel R Bowman. 2018. Sentence encoders on STILTs: Supplementary training on intermediate labeled-data tasks. Computing Research Repository, arXiv:1811.01088.</p>
<p>Matt Post. 2018. A call for clarity in reporting BLEU scores. In Proceedings of WMT.</p>
<p>Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2019. Exploring the limits of transfer learning with a unified text-totext transformer. Computing Research Repository, arXiv:1910.10683.</p>
<p>Pranav Rajpurkar, Robin Jia, and Percy Liang. 2018. Know what you don't know: Unanswerable questions for SQuAD. In Proceedings of ACL.</p>
<p>Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. SQuAD: 100,000+ questions for machine comprehension of text. In Proceedings of EMNLP.</p>
<p>Stefan Riezler and John T. Maxwell. 2005. On some pitfalls in automatic evaluation and significance testing for MT. In Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization.</p>
<p>Jeffrey D. Scargle. 1999. Publication bias: The "filedrawer" problem in scientific inference. arXiv, arXiv:physics/9909033.</p>
<p>James J. Schlesselman. 1982. Case-control studies: Design, conduct, analysis. Oxford University Press.</p>
<p>Roy Schwartz, Jesse Dodge, Noah A. Smith, and Oren Etzioni. 2019. Green AI. Computing Research Repository, arXiv:1907.10597.</p>
<p>Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Y. Ng, and Christopher Potts. 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of EMNLP.</p>
<p>Anders Søgaard, Anders Johannsen, Barbara Plank, Dirk Hovy, and Hector Martínez Alonso. 2014. What's in a $p$-value in NLP? In Proceedings CoNLL.</p>
<p>Samy Suissa and Jonathan J. Shuster. 1991. The $2 \times 2$ matched-pairs trial: Exact unconditional design and analysis. Biometrics, 47(2):361-372.</p>
<p>Denes Szucs and John P. A. Ioannidis. 2017. Empirical assessment of published effect sizes and power in the recent cognitive neuroscience and psychology literature. PLoS biology, 15(3).</p>
<p>Eric-Jan Wagenmakers. 2007. A practical solution to the pervasive problems of $p$ values. Psychonomic Bulletin \&amp; Review, 14:779-804.</p>
<p>Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. 2018. GLUE: A multi-task benchmark and analysis platform for natural language understanding. In Proceedings of the Workshop on BlackboxNLP.</p>
<p>Jacob Westfall, David A. Kenny, and Charles M. Judd. 2014. Statistical power and optimal design in experiments in which samples of participants respond to samples of stimuli. Journal of Experimental Pyschology: General, 143(5):2020-2045.</p>
<p>Adina Williams, Nikita Nangia, and Samuel R. Bowman. 2018. A broad-coverage challenge corpus for sentence understanding through inference. In Proceedings of NAACL.</p>
<p>Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R. Salakhutdinov, and Quoc V. Le. 2019. XLNet: Generalized autoregressive pretraining for language understanding. In Proceedings of NeurIPS.</p>
<p>Georgios N. Yannakakis and Héctor P. Martínez. 2015. Ratings are overrated! Frontiers in ICT, 2.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{13}$ These simulations require estimates for 7 parameters: the baseline, the effect size, variance by worker, variance by worker as a function of model, variance by item, variance by item as a function of model, and residual variance.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref6:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref7:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref8:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{9}$ Note that swapping all $n$ examples would reverse the model scores, equivalent to a net effect of $-2 \cdot \Delta_{B}$.&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
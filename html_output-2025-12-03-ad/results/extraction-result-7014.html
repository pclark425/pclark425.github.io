<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7014 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7014</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7014</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-135.html">extraction-schema-135</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <p><strong>Paper ID:</strong> paper-267413180</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2402.02130v5.pdf" target="_blank">GITA: Graph to Visual and Textual Integration for Vision-Language Graph Reasoning</a></p>
                <p><strong>Paper Abstract:</strong> Large Language Models (LLMs) are increasingly used for various tasks with graph structures. Though LLMs can process graph information in a textual format, they overlook the rich vision modality, which is an intuitive way for humans to comprehend structural information and conduct general graph reasoning. The potential benefits and capabilities of representing graph structures as visual images (i.e., $\textit{visual graph}$) are still unexplored. To fill the gap, we innovatively propose an end-to-end framework, called $\textbf{G}$raph to v$\textbf{I}$sual and $\textbf{T}$extual Integr$\textbf{A}$tion (GITA), which firstly incorporates visual graphs into general graph reasoning. Besides, we establish $\textbf{G}$raph-based $\textbf{V}$ision-$\textbf{L}$anguage $\textbf{Q}$uestion $\textbf{A}$nswering (GVLQA) dataset from existing graph data, which is the first vision-language dataset for general graph reasoning purposes. Extensive experiments on the GVLQA dataset and five real-world datasets show that GITA outperforms mainstream LLMs in terms of general graph reasoning capabilities. Moreover, We highlight the effectiveness of the layout augmentation on visual graphs and pretraining on the GVLQA dataset.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7014.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7014.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GITA Graph Describer</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GITA Task-agnostic Graph Describer (template-based)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A template-driven graph-to-text component in the GITA pipeline that generates task-agnostic textual descriptions of input graphs by filling curated templates with graph statistics (node/edge counts), explicit edge endpoint lists, directionality and optional weights/capacities.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Template-based edge-list textual description</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Encodes a graph as a natural-language, template-filled description that lists the graph type (directed/undirected), number of nodes/edges, and then enumerates edges (with endpoints) and optionally node/edge attributes (weights, capacities). Placeholders in templates are replaced with concrete values; templates cover directed/undirected graphs and presence/absence of weights.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>sequential, token-based (structured natural language); effectively lossless for the information enumerated</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>edge-list serialization via curated natural-language templates (explicit listing of nodes and edges, plus attributes); subgraphs are relabeled (nodes renumbered to 0..|C_u|-1) before serialization</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>GVLQA (constructed from NLGraph-full)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Instruction-based graph reasoning (Connectivity, Cycle, Topological Sort, Shortest Path, MaxFlow, Bipartite Matching, Hamilton Path)</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Used with LLM/VLM reasoners (e.g., GPT-4 Turbo, GPT-4V, LLaVA-7B/13B, LLaMA2-7B/13B, Vicuna-7B/13B; GITA VLM reasoner variants GITA-7B/13B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Applied with a variety of closed-source and open-source VLM/LLM backbones; experiments reported for 7B and 13B parameter LLaVA/LLaMA/Vicuna style models and GPT-4V/GPT-4 Turbo as representatives.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy (exact matching) on GVLQA tasks; downstream node-classification and link-prediction accuracy on real-world datasets</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Reported: GITA (which uses these textual descriptions together with visual graphs) achieves e.g. fine-tuned GITA-7B average 58.76% and GITA-13B average 60.00% on GVLQA-BASE (accuracy exact-match across seven tasks); comparisons in paper tables contrast these numbers with LLM-only baselines (LLaMA2-7B ~53.66% fine-tuned, Vicuna-7B ~54.15% fine-tuned) and GNN baselines (GCN 58.02%, SAGE 59.78%). Note: these numbers reflect the full GITA pipeline (text+visual) using the described textual representation in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>Enables instruction-based graph reasoning by converting structured graphs into human-readable prompts; used together with visual graphs to improve downstream accuracy and to create pretraining data (GVLQA) — pretraining on GVLQA-BASE produced modest gains reported (e.g., +0.67% zero-shot and +0.92% fine-tuning on certain real-world tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Verbose token usage (potentially large token cost for big subgraphs), reliance on k-hop subgraph sampling (may lose global structural information), template-based descriptions can be rigid and require careful template design; scaling to very large graphs limited by LLM context/token length.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>Compared to graph-to-token approaches (special token sequences), template-based natural-language descriptions are more interpretable and flexible for instruction-style querying but are less token-efficient and may be slower; the paper highlights that text modality excels on sequence-related tasks while vision complements by capturing local visual substructures.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GITA: Graph to Visual and Textual Integration for Vision-Language Graph Reasoning', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7014.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7014.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GVLQA Task-specific Query</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GVLQA Task-specific Query (questioner output)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Task-specific textual queries produced by the GITA questioner that enrich the graph description with node/edge semantics, task responsibilities, and precise input/output specifications (generated either from manual templates or LLM-agent bootstrapping).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Task-specific natural-language query (template- or agent-generated)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Takes the task-agnostic description and injects task context: clarifies node/edge meaning, states task responsibilities, and provides input/output format constraints; formatted as a natural-language prompt for LLM/VLM reasoners (examples include explicit 'Q:' lines and expected answer format).</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>sequential natural-language prompt (hierarchical in that it augments the base description with task metadata)</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>template-based augmentation or LLM-agent bootstrapped synthesis; templates fill placeholders with graph data and task specifications; bootstrapped agents produce queries by interpreting the graph description and task info.</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>GVLQA (all subsets: BASE and augmented variants)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Graph reasoning tasks used in GVLQA (Connectivity, Cycle, TS, SP, MaxFlow, BGM, HP)</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Fed as textual input to VLM reasoners (GPT-4V, LLaVA variants, LLaMA/Vicuna baselines in text-only mode)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Used with both closed-source GPT-4 family and open-source LLaVA/LLaMA/Vicuna models; fine-tuning experiments typically update LoRA adapters on text decoders while freezing vision encoders unless otherwise noted.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy (exact matching) per task on GVLQA; impact of query-generation style (manual templates vs bootstrapped) not separately quantified beyond dataset construction choices.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>GVLQA queries used to train/evaluate models in the paper; pretraining on GVLQA improves downstream real-world task averages by reported +0.67% (zero-shot) and +0.92% (fine-tuning) in one comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>Provides clear I/O and task semantics which helps LLMs/VLMs follow instructions for varied graph tasks; manual templates ensure precision for well-defined problems (e.g., TSP), while LLM-agent bootstrapping offers flexibility for dynamic tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Quality depends on template design or bootstrapped LLM agent reliability; bootstrapped generation may introduce variability or imprecision if not carefully constrained.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>More task-focused and constrained than plain graph-descriptions; provides necessary structure to translate graph descriptions into well-posed prompts for LLMs, contrasted with raw template descriptions or compact token encodings which may lack explicit task I/O formatting.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GITA: Graph to Visual and Textual Integration for Vision-Language Graph Reasoning', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7014.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7014.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>InstructGLM (related work)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>InstructGLM (graph-to-text approach cited)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prior work that uses natural-language descriptions of graphs combined with instruction prompts to fine-tune language models for graph-related tasks; cited as an example of graph-to-text conversion.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Natural-language graph descriptions (InstructGLM style)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Converts graphs into natural-language descriptions paired with instruction prompts used to fine-tune LLMs; exact template or encoding specifics are not detailed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>sequential natural-language</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>natural-language description + instruction prompt (unspecified encoding details in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Graph reasoning via instruction-finetuned LLMs (general)</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>InstructGLM (as reported in cited work)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A model fine-tuned with instruction-formatted graph descriptions (details not given here)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>Reported in cited literature to enable LLMs to handle graph queries via language prompts; cited as motivating graph-to-text approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Not detailed in this paper; cited as lacking vision modality and thus potentially missing visual structural cues.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>Positioned in the paper as a representative graph-to-text approach; contrasted with GITA which additionally integrates visual (image) representations of graphs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GITA: Graph to Visual and Textual Integration for Vision-Language Graph Reasoning', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7014.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7014.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT4Graph / LLMtoGraph (related work)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT4Graph and LLMtoGraph (graph-to-text/code conversion approaches cited)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Works that convert graphs into code-like or natural language formats using powerful LLM agents (e.g., ChatGPT) so that LLMs can answer graph reasoning queries; referenced in related work as graph-to-text examples.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Code- or natural-language formatted graph serializations (GPT4Graph/LLMtoGraph)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Transforms graphs into either domain-specific code snippets or structured natural language that a general LLM can ingest; in paper described at high-level (conversion performed by ChatGPT in cited works).</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>sequential natural-language or code-like serialization</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>unspecified conversion to code or NL via LLM agent</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Graph reasoning via LLMs (as per cited empirical evaluations)</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT/ChatGPT-based pipelines (per cited works)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>LLM-agent-driven graph conversion and query-answering pipelines (details in cited work)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>Shows feasibility of leveraging LLMs by converting graphs into NL/code; cited as prior art but without multimodal visual graph representations.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Paper notes these methods do not exploit vision modality; specific limitations are in cited works rather than described here.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>Compared qualitatively to GITA: while GPT4Graph/LLMtoGraph convert solely to text/code, GITA augments with visual-graph images to leverage VLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GITA: Graph to Visual and Textual Integration for Vision-Language Graph Reasoning', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7014.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e7014.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Graph-to-token methods (GraphGPT/GraphToken/LLaGA)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Graph-to-token methods (examples: GraphGPT, GraphToken, LLaGA)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Approaches that design special token sequences to represent graphs, which are projected or merged into the LLM token space for reasoning, cited as an alternative to graph-to-text.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Special token-sequence graph encoding (graph-to-token)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Encodes graph structure as a compact, specially-designed token sequence that is mapped into the LLM's token embedding space (rather than being plain natural-language text); specifics are not given in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>token-based, sequential (compact encoding)</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>custom token serialization / projection into token embedding space (unspecified here)</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Graph reasoning via tokenized graph encodings</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GraphGPT, GraphToken, LLaGA (as referenced works)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Methods that integrate graph tokens into LLM inputs, sometimes requiring architectural adapters; details in cited works.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>Described as an alternate approach that can be more compact than natural-language descriptions; not used in this paper's experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Paper notes these approaches differ from GITA (which focuses on visual + text); specific trade-offs (e.g., interpretability vs compactness) are implied but not experimentally evaluated here.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>Contrasted with graph-to-text approaches: graph-to-token may be more token-efficient and direct for LLM internalization, whereas graph-to-text is interpretable and amenable to instruction prompting; GITA instead explores visual graph representations combined with textual prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GITA: Graph to Visual and Textual Integration for Vision-Language Graph Reasoning', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7014.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e7014.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>NLGraph-full (source dataset)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>NLGraph-full (open-source graph dataset used to build GVLQA)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An existing graph dataset containing graph structures and ground-truth outputs which the authors used as the source to generate GVLQA samples (visual graphs + textual descriptions + queries + answers) via the GITA pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Structured graph instances with ground-truth textual outputs (original dataset)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Contains structural graphs and corresponding ground-truth outputs which are serializable into textual answers/labels; used as the provenance for converting to multimodal GVLQA examples.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>structured graph dataset (not a textual encoding per se)</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>source graphs were rendered into visual graphs and textual descriptions using Graphviz and GITA templates</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>NLGraph-full</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Source tasks for GVLQA construction (same seven graph reasoning tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>Served as the ground-truth source to construct the GVLQA pretraining dataset which in turn improved VLM/LLM performance modestly on downstream tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>The paper does not detail NLGraph-full encoding properties beyond its role as source data.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>Used as a base to generate GVLQA; contrasted with creating datasets from scratch because NLGraph-full provided structured graph instances and ground-truth answers.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GITA: Graph to Visual and Textual Integration for Vision-Language Graph Reasoning', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>GPT4Graph: Can large language models understand graph structured data? an empirical evaluation and benchmarking <em>(Rating: 2)</em></li>
                <li>Talk like a graph: Encoding graphs for large language models <em>(Rating: 2)</em></li>
                <li>Let your graph do the talking: Encoding structured data for llms <em>(Rating: 2)</em></li>
                <li>Graphgpt: Graph instruction tuning for large language models <em>(Rating: 2)</em></li>
                <li>InstructGLM <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7014",
    "paper_id": "paper-267413180",
    "extraction_schema_id": "extraction-schema-135",
    "extracted_data": [
        {
            "name_short": "GITA Graph Describer",
            "name_full": "GITA Task-agnostic Graph Describer (template-based)",
            "brief_description": "A template-driven graph-to-text component in the GITA pipeline that generates task-agnostic textual descriptions of input graphs by filling curated templates with graph statistics (node/edge counts), explicit edge endpoint lists, directionality and optional weights/capacities.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "Template-based edge-list textual description",
            "representation_description": "Encodes a graph as a natural-language, template-filled description that lists the graph type (directed/undirected), number of nodes/edges, and then enumerates edges (with endpoints) and optionally node/edge attributes (weights, capacities). Placeholders in templates are replaced with concrete values; templates cover directed/undirected graphs and presence/absence of weights.",
            "representation_type": "sequential, token-based (structured natural language); effectively lossless for the information enumerated",
            "encoding_method": "edge-list serialization via curated natural-language templates (explicit listing of nodes and edges, plus attributes); subgraphs are relabeled (nodes renumbered to 0..|C_u|-1) before serialization",
            "canonicalization": true,
            "average_token_length": null,
            "dataset_name": "GVLQA (constructed from NLGraph-full)",
            "task_name": "Instruction-based graph reasoning (Connectivity, Cycle, Topological Sort, Shortest Path, MaxFlow, Bipartite Matching, Hamilton Path)",
            "model_name": "Used with LLM/VLM reasoners (e.g., GPT-4 Turbo, GPT-4V, LLaVA-7B/13B, LLaMA2-7B/13B, Vicuna-7B/13B; GITA VLM reasoner variants GITA-7B/13B)",
            "model_description": "Applied with a variety of closed-source and open-source VLM/LLM backbones; experiments reported for 7B and 13B parameter LLaVA/LLaMA/Vicuna style models and GPT-4V/GPT-4 Turbo as representatives.",
            "performance_metric": "Accuracy (exact matching) on GVLQA tasks; downstream node-classification and link-prediction accuracy on real-world datasets",
            "performance_value": "Reported: GITA (which uses these textual descriptions together with visual graphs) achieves e.g. fine-tuned GITA-7B average 58.76% and GITA-13B average 60.00% on GVLQA-BASE (accuracy exact-match across seven tasks); comparisons in paper tables contrast these numbers with LLM-only baselines (LLaMA2-7B ~53.66% fine-tuned, Vicuna-7B ~54.15% fine-tuned) and GNN baselines (GCN 58.02%, SAGE 59.78%). Note: these numbers reflect the full GITA pipeline (text+visual) using the described textual representation in experiments.",
            "impact_on_training": "Enables instruction-based graph reasoning by converting structured graphs into human-readable prompts; used together with visual graphs to improve downstream accuracy and to create pretraining data (GVLQA) — pretraining on GVLQA-BASE produced modest gains reported (e.g., +0.67% zero-shot and +0.92% fine-tuning on certain real-world tasks).",
            "limitations": "Verbose token usage (potentially large token cost for big subgraphs), reliance on k-hop subgraph sampling (may lose global structural information), template-based descriptions can be rigid and require careful template design; scaling to very large graphs limited by LLM context/token length.",
            "comparison_with_other": "Compared to graph-to-token approaches (special token sequences), template-based natural-language descriptions are more interpretable and flexible for instruction-style querying but are less token-efficient and may be slower; the paper highlights that text modality excels on sequence-related tasks while vision complements by capturing local visual substructures.",
            "uuid": "e7014.0",
            "source_info": {
                "paper_title": "GITA: Graph to Visual and Textual Integration for Vision-Language Graph Reasoning",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "GVLQA Task-specific Query",
            "name_full": "GVLQA Task-specific Query (questioner output)",
            "brief_description": "Task-specific textual queries produced by the GITA questioner that enrich the graph description with node/edge semantics, task responsibilities, and precise input/output specifications (generated either from manual templates or LLM-agent bootstrapping).",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "Task-specific natural-language query (template- or agent-generated)",
            "representation_description": "Takes the task-agnostic description and injects task context: clarifies node/edge meaning, states task responsibilities, and provides input/output format constraints; formatted as a natural-language prompt for LLM/VLM reasoners (examples include explicit 'Q:' lines and expected answer format).",
            "representation_type": "sequential natural-language prompt (hierarchical in that it augments the base description with task metadata)",
            "encoding_method": "template-based augmentation or LLM-agent bootstrapped synthesis; templates fill placeholders with graph data and task specifications; bootstrapped agents produce queries by interpreting the graph description and task info.",
            "canonicalization": null,
            "average_token_length": null,
            "dataset_name": "GVLQA (all subsets: BASE and augmented variants)",
            "task_name": "Graph reasoning tasks used in GVLQA (Connectivity, Cycle, TS, SP, MaxFlow, BGM, HP)",
            "model_name": "Fed as textual input to VLM reasoners (GPT-4V, LLaVA variants, LLaMA/Vicuna baselines in text-only mode)",
            "model_description": "Used with both closed-source GPT-4 family and open-source LLaVA/LLaMA/Vicuna models; fine-tuning experiments typically update LoRA adapters on text decoders while freezing vision encoders unless otherwise noted.",
            "performance_metric": "Accuracy (exact matching) per task on GVLQA; impact of query-generation style (manual templates vs bootstrapped) not separately quantified beyond dataset construction choices.",
            "performance_value": "GVLQA queries used to train/evaluate models in the paper; pretraining on GVLQA improves downstream real-world task averages by reported +0.67% (zero-shot) and +0.92% (fine-tuning) in one comparison.",
            "impact_on_training": "Provides clear I/O and task semantics which helps LLMs/VLMs follow instructions for varied graph tasks; manual templates ensure precision for well-defined problems (e.g., TSP), while LLM-agent bootstrapping offers flexibility for dynamic tasks.",
            "limitations": "Quality depends on template design or bootstrapped LLM agent reliability; bootstrapped generation may introduce variability or imprecision if not carefully constrained.",
            "comparison_with_other": "More task-focused and constrained than plain graph-descriptions; provides necessary structure to translate graph descriptions into well-posed prompts for LLMs, contrasted with raw template descriptions or compact token encodings which may lack explicit task I/O formatting.",
            "uuid": "e7014.1",
            "source_info": {
                "paper_title": "GITA: Graph to Visual and Textual Integration for Vision-Language Graph Reasoning",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "InstructGLM (related work)",
            "name_full": "InstructGLM (graph-to-text approach cited)",
            "brief_description": "Prior work that uses natural-language descriptions of graphs combined with instruction prompts to fine-tune language models for graph-related tasks; cited as an example of graph-to-text conversion.",
            "citation_title": "",
            "mention_or_use": "mention",
            "representation_name": "Natural-language graph descriptions (InstructGLM style)",
            "representation_description": "Converts graphs into natural-language descriptions paired with instruction prompts used to fine-tune LLMs; exact template or encoding specifics are not detailed in this paper.",
            "representation_type": "sequential natural-language",
            "encoding_method": "natural-language description + instruction prompt (unspecified encoding details in this paper)",
            "canonicalization": null,
            "average_token_length": null,
            "dataset_name": null,
            "task_name": "Graph reasoning via instruction-finetuned LLMs (general)",
            "model_name": "InstructGLM (as reported in cited work)",
            "model_description": "A model fine-tuned with instruction-formatted graph descriptions (details not given here)",
            "performance_metric": null,
            "performance_value": null,
            "impact_on_training": "Reported in cited literature to enable LLMs to handle graph queries via language prompts; cited as motivating graph-to-text approaches.",
            "limitations": "Not detailed in this paper; cited as lacking vision modality and thus potentially missing visual structural cues.",
            "comparison_with_other": "Positioned in the paper as a representative graph-to-text approach; contrasted with GITA which additionally integrates visual (image) representations of graphs.",
            "uuid": "e7014.2",
            "source_info": {
                "paper_title": "GITA: Graph to Visual and Textual Integration for Vision-Language Graph Reasoning",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "GPT4Graph / LLMtoGraph (related work)",
            "name_full": "GPT4Graph and LLMtoGraph (graph-to-text/code conversion approaches cited)",
            "brief_description": "Works that convert graphs into code-like or natural language formats using powerful LLM agents (e.g., ChatGPT) so that LLMs can answer graph reasoning queries; referenced in related work as graph-to-text examples.",
            "citation_title": "",
            "mention_or_use": "mention",
            "representation_name": "Code- or natural-language formatted graph serializations (GPT4Graph/LLMtoGraph)",
            "representation_description": "Transforms graphs into either domain-specific code snippets or structured natural language that a general LLM can ingest; in paper described at high-level (conversion performed by ChatGPT in cited works).",
            "representation_type": "sequential natural-language or code-like serialization",
            "encoding_method": "unspecified conversion to code or NL via LLM agent",
            "canonicalization": null,
            "average_token_length": null,
            "dataset_name": null,
            "task_name": "Graph reasoning via LLMs (as per cited empirical evaluations)",
            "model_name": "GPT/ChatGPT-based pipelines (per cited works)",
            "model_description": "LLM-agent-driven graph conversion and query-answering pipelines (details in cited work)",
            "performance_metric": null,
            "performance_value": null,
            "impact_on_training": "Shows feasibility of leveraging LLMs by converting graphs into NL/code; cited as prior art but without multimodal visual graph representations.",
            "limitations": "Paper notes these methods do not exploit vision modality; specific limitations are in cited works rather than described here.",
            "comparison_with_other": "Compared qualitatively to GITA: while GPT4Graph/LLMtoGraph convert solely to text/code, GITA augments with visual-graph images to leverage VLMs.",
            "uuid": "e7014.3",
            "source_info": {
                "paper_title": "GITA: Graph to Visual and Textual Integration for Vision-Language Graph Reasoning",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Graph-to-token methods (GraphGPT/GraphToken/LLaGA)",
            "name_full": "Graph-to-token methods (examples: GraphGPT, GraphToken, LLaGA)",
            "brief_description": "Approaches that design special token sequences to represent graphs, which are projected or merged into the LLM token space for reasoning, cited as an alternative to graph-to-text.",
            "citation_title": "",
            "mention_or_use": "mention",
            "representation_name": "Special token-sequence graph encoding (graph-to-token)",
            "representation_description": "Encodes graph structure as a compact, specially-designed token sequence that is mapped into the LLM's token embedding space (rather than being plain natural-language text); specifics are not given in this paper.",
            "representation_type": "token-based, sequential (compact encoding)",
            "encoding_method": "custom token serialization / projection into token embedding space (unspecified here)",
            "canonicalization": null,
            "average_token_length": null,
            "dataset_name": null,
            "task_name": "Graph reasoning via tokenized graph encodings",
            "model_name": "GraphGPT, GraphToken, LLaGA (as referenced works)",
            "model_description": "Methods that integrate graph tokens into LLM inputs, sometimes requiring architectural adapters; details in cited works.",
            "performance_metric": null,
            "performance_value": null,
            "impact_on_training": "Described as an alternate approach that can be more compact than natural-language descriptions; not used in this paper's experiments.",
            "limitations": "Paper notes these approaches differ from GITA (which focuses on visual + text); specific trade-offs (e.g., interpretability vs compactness) are implied but not experimentally evaluated here.",
            "comparison_with_other": "Contrasted with graph-to-text approaches: graph-to-token may be more token-efficient and direct for LLM internalization, whereas graph-to-text is interpretable and amenable to instruction prompting; GITA instead explores visual graph representations combined with textual prompts.",
            "uuid": "e7014.4",
            "source_info": {
                "paper_title": "GITA: Graph to Visual and Textual Integration for Vision-Language Graph Reasoning",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "NLGraph-full (source dataset)",
            "name_full": "NLGraph-full (open-source graph dataset used to build GVLQA)",
            "brief_description": "An existing graph dataset containing graph structures and ground-truth outputs which the authors used as the source to generate GVLQA samples (visual graphs + textual descriptions + queries + answers) via the GITA pipeline.",
            "citation_title": "",
            "mention_or_use": "use",
            "representation_name": "Structured graph instances with ground-truth textual outputs (original dataset)",
            "representation_description": "Contains structural graphs and corresponding ground-truth outputs which are serializable into textual answers/labels; used as the provenance for converting to multimodal GVLQA examples.",
            "representation_type": "structured graph dataset (not a textual encoding per se)",
            "encoding_method": "source graphs were rendered into visual graphs and textual descriptions using Graphviz and GITA templates",
            "canonicalization": null,
            "average_token_length": null,
            "dataset_name": "NLGraph-full",
            "task_name": "Source tasks for GVLQA construction (same seven graph reasoning tasks)",
            "model_name": null,
            "model_description": null,
            "performance_metric": null,
            "performance_value": null,
            "impact_on_training": "Served as the ground-truth source to construct the GVLQA pretraining dataset which in turn improved VLM/LLM performance modestly on downstream tasks.",
            "limitations": "The paper does not detail NLGraph-full encoding properties beyond its role as source data.",
            "comparison_with_other": "Used as a base to generate GVLQA; contrasted with creating datasets from scratch because NLGraph-full provided structured graph instances and ground-truth answers.",
            "uuid": "e7014.5",
            "source_info": {
                "paper_title": "GITA: Graph to Visual and Textual Integration for Vision-Language Graph Reasoning",
                "publication_date_yy_mm": "2024-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "GPT4Graph: Can large language models understand graph structured data? an empirical evaluation and benchmarking",
            "rating": 2,
            "sanitized_title": "gpt4graph_can_large_language_models_understand_graph_structured_data_an_empirical_evaluation_and_benchmarking"
        },
        {
            "paper_title": "Talk like a graph: Encoding graphs for large language models",
            "rating": 2,
            "sanitized_title": "talk_like_a_graph_encoding_graphs_for_large_language_models"
        },
        {
            "paper_title": "Let your graph do the talking: Encoding structured data for llms",
            "rating": 2,
            "sanitized_title": "let_your_graph_do_the_talking_encoding_structured_data_for_llms"
        },
        {
            "paper_title": "Graphgpt: Graph instruction tuning for large language models",
            "rating": 2,
            "sanitized_title": "graphgpt_graph_instruction_tuning_for_large_language_models"
        },
        {
            "paper_title": "InstructGLM",
            "rating": 1,
            "sanitized_title": "instructglm"
        }
    ],
    "cost": 0.019050499999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>GITA: Graph to Visual and Textual Integration for Vision-Language Graph Reasoning
31 Oct 2024</p>
<p>Yanbin Wei yanbin.ust@gmail.com 
Department of Computer Science and Engineering
Southern University of Science and Technology</p>
<p>Department of Computer Science and Engineering
Hong Kong University of Science and Technology</p>
<p>Shuai Fu 
Department of Computer Science and Engineering
Southern University of Science and Technology</p>
<p>Institute for Machine Learning
University of Adelaide
Australia</p>
<p>Weisen Jiang 
Department of Computer Science and Engineering
Southern University of Science and Technology</p>
<p>Department of Computer Science and Engineering
Hong Kong University of Science and Technology</p>
<p>Zejian Zhang zejianzhang33@gmail.com 
Tencent</p>
<p>Zhixiong Zeng 
Tencent</p>
<p>Qi Wu 
Institute for Machine Learning
University of Adelaide
Australia</p>
<p>James T Kwok jamesk@cse.ust.hk 
Department of Computer Science and Engineering
Hong Kong University of Science and Technology</p>
<p>Yu Zhang yu.zhang.ust@gmail.com 
Department of Computer Science and Engineering
Southern University of Science and Technology</p>
<p>GITA: Graph to Visual and Textual Integration for Vision-Language Graph Reasoning
31 Oct 2024BD0CDD8C490D8581F582B10D5B2F370DarXiv:2402.02130v5[cs.CL]
Large Language Models (LLMs) are increasingly used for various tasks with graph structures.Though LLMs can process graph information in the textual format, they overlook the rich vision modality, which is an intuitive way for humans to comprehend structural information and conduct general graph reasoning.The potential benefits and capabilities of representing graph structures as visual images (i.e., visual graph) are still unexplored.To fill the gap, we innovatively propose an end-to-end framework, called Graph to vIsual and Textual IntegrAtion (GITA), which incorporates visual graphs into general graph reasoning.Besides, we construct the Graph-based Vision-Language Question Answering (GVLQA) dataset from existing graph data, which is the first vision-language dataset for general graph reasoning.Extensive experiments on the GVLQA dataset and five real-world datasets show that GITA outperforms mainstream LLMs on general graph reasoning.Moreover, experimental results demonstrate the effectiveness of the layout augmentation on visual graphs and pretraining on the GVLQA dataset.</p>
<p>Introduction</p>
<p>Graph reasoning tasks are pivotal in domains such as recommendation systems [25; 60], social network analysis [7; 29], and knowledge graph reasoning [72; 46; 62].Various architectures have been developed, from shallow embedding methods [6; 53] to advanced Graph Neural Networks (GNNs) [37; 64] and graph Transformers [71; 40; 8].While these models excel in graph reasoning tasks, they often lack generalizability, flexibility, and user-friendliness.Achieving good performance with these models typically requires domain-specific tuning, which limits their abilities to generalize across different domains.Additionally, these models struggle to handle diverse tasks with the same architecture.Each task often requires a specialized design, including task-specific data processing and decoder, leading to limited flexibility.Lastly, unlike the Large Language Models (LLMs) that can engage in conversations with users, these models are less explainable and user-friendly.</p>
<p>In contrast, LLMs have shown great generalization capabilities across a wide variety of reasoning tasks [61; 67; 74; 32; 33] by encapsulating task-specific demands within a cohesive and interpretable mechanism -text prompts, under a unified architecture with minimal domain-specific adjustments.These advantages have sparked investigations into the potential of LLMs for graph reasoning.Recent developments lend credence to the notion that LLMs can indeed interpret and manipulate graph-structured data through textual representations.For example, InstructGLM [66], GPT4Graph [21], and LLMtoGraph [44] convert graphs into textual descriptions and then use these descriptions paired with queries to enable LLMs to generate accurate responses for graph reasoning tasks.Furthermore, the introduction of benchmarks such as GraphQA [14] and NLGraph [59] is a testament to the growing interest in evaluating LLMs' effectiveness on graph reasoning tasks framed in natural languages.</p>
<p>Despite the development of numerous methods and benchmarks for graph reasoning on LLMs, they often overlook the valuable vision modality, which is a natural means for humans to comprehend structural information and has demonstrated its success in various visual reasoning scenarios [30; 68; 69; 34; 3; 54].Consequently, the following questions arise: (1) Can incorporating visual information be beneficial for general graph reasoning scenarios?(2) If so, how can we effectively integrate the vision modality into graph reasoning?To the best of our knowledge, these questions remain unexplored.</p>
<p>To answer them, we first propose an end-to-end framework called Graph to vIsual and Textual IntegrAtion (GITA) 34 that systematically integrates visual information into instruction-based graph reasoning, by rendering graph structures to customized visual images which are called visual graph.Specifically, the GITA framework has four components: a graph visualizer for generating visual graphs, a graph describer for producing textual descriptions of the graph structure, a task-based questioner that organizes the description and requirements of the current task into prompt instruction, and a Vision-Language Model (VLM) to perform vision-language graph reasoning.In the proposed GITA framework, visual information can be incorporated into many tasks with explicit or implicit graph structures, without sacrificing its versatility, flexibility, or user-friendliness.Besides, since there is no dataset for vision-supported general graph reasoning capabilities, we construct the first vision-language dataset for general graph reasoning purposes called Graph-based Vision-Language Question Answering (GVLQA) 5 based on the proposed GITA framework.The GVLQA dataset consists of 526K instances covering seven representative graph reasoning tasks, aiming to thoroughly evaluate the structure-based graph reasoning abilities of VLMs and LLMs.Extensive experiments on the GVLQA dataset and five real-world datasets demonstrate the effectiveness of the proposed GITA model.Furthermore, we delve into the effects of visual graph augmentation strategies and find that layout augmentation can dramatically boost vision-based graph reasoning performance.</p>
<p>Our main contributions are summarized as follows.</p>
<p>• We introduce an end-to-end GITA framework, innovatively integrating vision modality to boost the graph reasoning abilities of language models.</p>
<p>• We establish GVLQA, the first vision-language question-answering dataset for general graph reasoning purposes.It can be used to thoroughly evaluate the structure-based graph reasoning abilities of LLMs/VLMs and can also be used as pretraining data to boost the performance of downstream tasks.</p>
<p>• Extensive experiments on benchmark datasets across various graph reasoning tasks demonstrate the effectiveness of the proposed GITA framework and the benefits of layout augmentation on visual graphs.</p>
<p>Related Work</p>
<p>Graph Reasoning.Graph reasoning [5; 63] aims to answer questions based on graphs, which involves utilizing graph structures to guide the reasoning process to generate answers.Graph reasoning has a wide variety of applications in social network analysis [47; 41], bioinformatics [31; 18], chemistry [19], physics [4], knowledge graph reasoning [6], and recommendation systems [39; 26].Many graph reasoning methods have been proposed.Early attempts [6; 53] learn node and edge representations through shallow modules, which may have only limited expressive power.Graph Neural Networks (GNNs) such as GCN [37], GAT [58], GraphSAGE [23], MPNN [19], and GIN [64] use messagepassing paradigm [19] to model graph dependencies and update node features.Transformer-based graph models [71; 40; 8] further propose to use self-attention to increase the expressiveness and longrange dependency.However, as discussed in Sec 1, these models may exhibit limited generalizability, flexibility, and user-friendliness.</p>
<p>LLMs on Graph</p>
<p>Reasoning.There have been many attempts to use LLMs in graph reasoning.Depending on how they align the input spaces of graphs and LLMs, we categorize them into two types: Graph-to-text and Graph-to-token.Graph-to-text methods transform a graph into textual descriptions, which are concatenated with the instructions and fed to the LLM for querying.For example, InstructGLM [66] uses natural language to describe the graph and proposes instruction prompts to fine-tune the LLM.He et.al [27] applies LLMs to explain graphs for training GNNs, while Chen et.al [10] treat LLMs as enhancers to exploit text attributes or as predictors for node classification on text-attributed graphs.GPT4Graph [21] and LLMtoGraph [44] convert graphs into specific code or natural language formats by the powerful ChatGPT [48;49].On the other hand, Graph-to-token methods include GraphGPT [55], GraphToken [50] and LLaGA [9].For these methods, the graph is represented as a specially designed token sequence, which is projected or merged into the LLM's token space for text-based reasoning.However, none of the aforementioned methods represent the graph structure information as images, highlighting the uniqueness of the proposed GITA framework and GVLQA dataset.</p>
<p>Large Vision-Language Models.Large VLMs have significantly expanded the cognitive abilities of LLMs by integrating the vision modality to address vision-language tasks.Many methods have been proposed.Some early explorations like Flamingo [2], CLIP [51], and BLIP-2 [43] use a visual encoder for processing images and align the visual and textual embeddings.Subsequent models like LLaVA [45] and MiniGPT-4 [75] combine visual and textual inputs in a single LLM for solving multimodal tasks.InstructBlip [11] proposes an instruction-aware query transformer and trains a vision-language model by instruction tuning.However, despite progress in a wide range of visionlanguage tasks [70; 16], using visual information in graph reasoning remains overlooked.We take the first step in this field, pushing the boundaries of VLMs in graph reasoning.(ii) the task requirement T , encompassing specific operations or questions pertaining to the graph.</p>
<p>Based on the information provided in G and a specific task requirement T , models are expected to output a reasonable answer A. On the other hand, in the context of instruction-based graph reasoning methods, it is necessary to convert these inputs into textual form.This transformation facilities graph reasoning within natural language, allowing for improved interpretation and harnessing the formidable reasoning capabilities of large language models.</p>
<p>Architecture</p>
<p>Overview.Different from the above graph reasoning methods, we propose a Graph to Image-Txt Assistant (GITA), which is the first attempt to perform graph reasoning in a vision-text-based manner.GITA comprises four pivotal components: a task-agnostic graph visualizer V , a graph describer D, a task-specific questioner Q, and a VLM reasoner R ϕ , as illustrated in Figure 1.Firstly, V and G are designed to produce visual depictions (i.e., visual graphs) and textual descriptions of the graph structure inputs, respectively.Then, given the task requirement T and the textual description produced by D, Q is designed to form a task-specific query.Finally, R ϕ receives the visual input I G from V based on the visual graph and the textual input Q T G from Q, then generates answers A in natural language.In the following, we introduce the four components in detail.</p>
<p>Graph Visualizer.The role of the graph visualizer is to generate visual graphs from structural graphs.The image representation of a structural graph is not unique, as there can be variations in many aspects, such as backdrop colors, layouts, and node shapes.These variations may enhance the robustness of models through effective training but simultaneously increase the learning difficulty for models.Therefore, balancing consistency and variety is necessary during the graph visualization process.This trade-off is reflected in our design of graph visualizer, by maintaining consistency in</p>
<p>GITA Framewrok</p>
<p>Graph Data</p>
<p>Graph Visualizer</p>
<p>Configurations</p>
<p>Base Image Styles</p>
<p>Graph-related Image Styles</p>
<p>Textual Description</p>
<p>In a directed graph, (i,j) means that node i and node j are connected with an undirected edge.</p>
<p>The nodes are numbered from 0 to 6, and the edges are: (0,2) (2,6) ...</p>
<p>Graph Describer</p>
<p>Task-agnostic Graph Describing Templates</p>
<p>Visual Graph</p>
<p>Traditional LLM Solutions</p>
<p>Graph Data</p>
<p>Large Language Model Instructions Q: Is there a cycle in this graph?</p>
<p>Q: Is there a path between node 0 and node 4?</p>
<p>...</p>
<p>Response:</p>
<p>Yes.</p>
<p>Task Info</p>
<p>...</p>
<p>Task-based Questioner</p>
<p>Task Info</p>
<p>Task-specific Query</p>
<p>In a directed graph with 7 nodes numbered from 0 to 6: node 0 should be visited before node 2, ... This task is to find a valid topological sorting for this directed graph.Please provide a possible topological ordering path, for example: 0 -&gt; 1 -&gt; 2 -&gt; 3 -&gt; 4. Q: The topological order of the directed graph is:</p>
<p>VLM Reasoner</p>
<p>Task-agnostic descriptions</p>
<p>Response: 0 -&gt; 2 -&gt; 6 -&gt; 3 -&gt; 1 -&gt; 4 -&gt; 5.</p>
<p>Frozen Image Styles Customizable Image Styles</p>
<p>Task Responsibility</p>
<p>Output Specification</p>
<p>Refined descriptions (w/ the meanings of nodes/edges) basic image styles common to general images (i.e., size, resolution, backdrop) and only introducing customizable variations in four graph-related image styles unique to visual graphs (i.e., layout, node shapes, node outline styles, and edge thickness).Graph visualization in V can be formulated by the following equation:
I G = V (G, Γ, ∆),(1)
where I G denotes the visual graph derived from graph G, while Γ and ∆ are the fixed basic image styles and customizable graph-related image styles, respectively.</p>
<p>Visualizing the entire graph can be challenging when the number of nodes or edges is very large, affecting the clarity of the images.To address this, our graph visualizer adopts the standard strategy of k-hop subgraph sampling.Specifically, k-hop subgraph sampling for a node u in the set of vertices C involves selecting a subgraph G u = {C u ⊆ N k (u), E u ⊆ E}, where N k (u) includes nodes within k steps from u and each edge (i, j) in E u connects nodes within C u .To generate the visual graph of the k-hop subgraph G u centered on u, the nodes within G u are relabeled from 0 to |C u | − 1 to facilitate the generalization of visual graphs.Subsequently, this relabeled subgraph G u is fed to the graph visualizer to generate its visual graph I Gu by Eq. (1).</p>
<p>In practice, the graph visualizer can be implemented by a variety of graphic visualization tools, such as Graphviz [17], Matplotlib [56], and NetworkX [22].Among them, Graphviz can automatically design the layouts of visual graphs, and is especially suitable for building large-scale datasets.Matplotlib is excellent for customizable plots with fine-grained control, and NetworkX excels in complex network analysis.We have implemented various graph visualizers using modular, plug-in architecture in GITA.Specific examples of the visual graphs generated with these tools can be found in Appendix D.</p>
<p>Graph Describer.The graph describer D is tasked with generating task-agnostic textual descriptions of a given graph G.To ensure clarity and fidelity of these descriptions, we meticulously craft a curated set of graph-describing templates.The graph description templates outlined in Appendix E are designed to cover a broad spectrum of scenarios, accommodating various graph configurations including directed or undirected graphs and those with or without node or edge weights.To generate the description for a given graph, the graph describer initially selects an appropriate template based on the graph's characteristics, such as its directionality and whether it includes node attributes or edge weights.Subsequently, this template is used by replacing placeholders with actual data, such as the number of nodes, the number of edges, and the endpoints of each edge, to craft detailed descriptions tailored to the specific graph in question.The process for D to generate textual descriptions can be formulated as follows:
D G = D(G, P ),(2)
where D G denotes the textual description generated by graph describer, and P is the graph-describing template of the graph G.</p>
<p>By introducing these unified and structured graph-describing templates, the graph describer is empowered to generate coherent and informative descriptions that focus on the inherent characteristics of the graph itself, independent of specific task requirements.</p>
<p>Questioner.The questioner Q is tailored to capture the intricate requirements of specific tasks and reflect them in its output task-specific query.In detail, Q receives the task-agnostic textual descriptions from the graph describer and refines them to align with the task context by elucidating the concrete meanings of nodes and edges.These refined descriptions are then enriched with task responsibilities and input/output specifications to form task-specific queries.The formulation of the questioner to generate the task-specific queries can be represented as follows:
Q T G = Q(T, D G ),(3)
where Q T G represents the task-specific query generated by the questioner with given the task requirement T and the textual description D G .The construction of task-specific queries can be approached in two main ways: manual template-based construction and bootstrapping LLM agents.Manual template-based construction enriches D G with task-specific manual templates, which is preferred for tasks with precise requirements, such as the Traveling Salesman Problem (TSP) [12], where accuracy is critical and the task definitions are well-understood.This is because it can ensure clarity and reduce the risk of errors due to its meticulous attention to details.On the other hand, bootstrapping LLM agents for automated synthesis is more economical and suitable for dynamic or bespoke tasks, such as robotic planning or complex gaming scenarios, as it can take advantage of the speed and adaptability of LLM agents to interpret context and generate appropriate queries, minimizing manual effort and enhancing responsiveness to changing conditions.Both methods are illustrated with examples in Appendix F, showcasing their applications and benefits in different scenarios.</p>
<p>VLM Reasoner.The VLM reasoner R ϕ performs final graph reasoning with visual inputs I G from V and textual inputs Q T G from Q, and outputs responses in natural language.This reasoning process can be represented as the following:
A = R(I G , Q T G ),(4)
where A is the answer generated by the vision-language model R.In this work, we adopt GPT-4V and LLaVA-7B/13B as VLM reasoners.These models are regarded as representatives in the realm of closed-source and open-source VLMs, respectively.In summary, GITA systematically incorporates the vision modality into instruction-based graph reasoning.In Appendix B, we discuss the characteristics of GITA, in aspects of generalizability, flexibility and user-friendliness.</p>
<p>Visual Graph Augmentation</p>
<p>Visual graphs generated for the same graph G can be considered as an unique data augmentation technique.Building on the four graph-related image styles introduced in the graph visualizer part of Sec 3.2, we propose the following augmentation strategies: layout augmentation, node shape augmentation, node outline style augmentation, and edge thickness augmentation.Specifically, layout augmentation involves altering the layout styles while keeping all the other settings constant.Similarly, by changing only the respective attributes, we can implement node shape augmentation, node outline style augmentation, and edge thickness augmentation.These four proposed augmentation strategies facilitate studies on the importance of each in enhancing the graph reasoning abilities of VLM reasoners.</p>
<p>Training</p>
<p>Given a visual graph I G and a text-specific query Q T G , along with the target answer A t , the VLM reasoner of GITA is trained to generate answers A. Specifically, I G is input into the vision encoder of the VLM reasoner, resulting in a set of visual features F .If there is a dimension difference between F v and the pretrained word embeddings, these F will be aligned with the pretrained word embedding space of the text decoder by a vision-to-text projector.Finally, the aligned visual features F aligned and Q T G are concatenated as input sequences of the text decoder.Formally, given I G , Q T G , and A t , the VLM reasoner is trained by minimizing the following negative log-likelihood:
L ϕ = − |A| i=1 log p ϕ (A i | F aligned , Q T G , A &lt;i ),(5)
where ϕ is the trainable parameter and A i denotes the prediction token at the i-th position.Besides, A &lt;i represents the first i − 1 predicted tokens.During the inference process, GITA is capable of accepting structure graphs as inputs and performing graph reasoning in an end-to-end manner.</p>
<p>GVLQA Dataset</p>
<p>In this section, we introduce the GVLQA dataset to fill the absence of a vision-language-based general graph reasoning dataset.It is designed to: 1) evaluate the graph reasoning capabilities of VLMs or LLMs; 2) help models acquire fundamental graph comprehension and reasoning abilities as a pretraining dataset.</p>
<p>Construction</p>
<p>The GVLQA dataset is created by utilizing the graph visualizerthe graph describer, and questioner in GITA to generate vision-language-based question-answer pairs for graph reasoning on an open-source graph dataset.Specifically, we first extract both the original graph structures and the ground-truth outputs from the NLGraph-full dataset [59].Then the graph visualizer (detailed in Sec 3.2) and the graph describer (outlined in Sec 3.2) are used to generate visual graphs and textual descriptions for these original graph structures, respectively.Afterwards, the questioner (described in Sec 3.2) further improves and enriches the textual descriptions by converting them into textual queries.At the same time, it transforms the ground-truth output into text-based answers, following specific output requirements.By combining these visual graphs, textual queries, and text-based answers, we obtain the Graph-based Vision-Language Question Answering (GVLQA) dataset.</p>
<p>In the process of establishing GVLQA, we employed graphviz [17] to instantiate the graph visualizer.This choice is made due to its multitude of pre-defined layout algorithms, which enable convenient adjustment of visual graph layouts.Additionally, manual template-based constructed queries are utilized as the questioner because these tasks are famous with well-defined requirements.</p>
<p>Structure</p>
<p>The GVLQA dataset comprises 526K samples, each consisting of a visual graph, a textual query, and its corresponding answer.It is divided into five subsets: GVLQA-BASE, and four augmentation subsets GVLQA-AUGLY, GVLQA-AUGNS, GVLQA-AUGNO, and GVLQA-AUGET.In GVLQA-BASE, the visual graphs are uniformly styled.The remaining four augmentation subsets are derived from GVLQA-BASE through the four visual graph augmentations (Sec 3.3), varying in six different layouts, three node shapes, four node outline styles, and four degrees of edge thickness, respectively.Detailed statistics of the four subsets are shown in Table 6 of Appendix C.</p>
<p>Each GVLQA subset undergoes evaluation across seven graph reasoning tasks, outlined as follows.</p>
<p>• Connectivity [52] (denoted Connect): Determine whether two randomly selected nodes u and v in an undirected graph are connected.</p>
<p>• Cycle [52]: Identify whether a cycle exists in an undirected graph.</p>
<p>• Topological Sort [35] (denoted TS): Find a valid topological sort for a directed acyclic graph.Here, topological sort outputs a linear ordering of the nodes such that for every directed edge u ← v, node u comes before v in the ordering.</p>
<p>• Shortest Path [13] (denoted SP): Find the shortest path between two nodes in a weighted undirected graph.The shortest path between two nodes is the path connecting the two nodes with the minimum sum of edge weights along the path.</p>
<p>• Maximum Flow [15] (denoted MaxFlow): Calculate the maximum flow from a source node to a sink node in a network graph.</p>
<p>• Bipartite Graph Matching [36] (denoted BGM): Find a matching set in a bipartite graph with the largest number of edges.A matching set is a collection of edges in which no two edges share any common node.</p>
<p>• Hamilton Path [20] (denoted HP): Find a valid Hamilton path in an undirected graph.A Hamiltonian path is a path that traverses each node in a graph exactly once.</p>
<p>Figure 6 offers illustrations for these tasks in the GVLQA-BASE dataset.Illustrations of all the GVLQA subsets are provided in Appendix H.</p>
<p>Experiments</p>
<p>In this section, we extensively evaluate the performance of LLM baselines and the proposed GITA on the GVLQA-BASE and five real-world datasets.To better clarify the reasoning capabilities of solely visual graphs, we also test GITA without the textual descriptions of graphs, which can be considered as a variant of GITA and denoted as vision-only (VO).In this case, the visual graph is the only information source for graph reasoning.Additionally, we investigate the importance of visual graph augmentation (Sec 3.3) strategies, by comparison GITA-7B trained on GVLQA-BASE and on the other augmentation subsets of GVLQA (Sec 4.2).Lastly, we investigate the effectiveness of using GVLQA as the pretrained dataset on real-world datasets.The evaluation metrics for all experiments are accuracy by exact matching.For the fine-tuning setting, we fine-tune the LoRA adapters [28] for all weight matrices in the text decoder of the VLM reasoner, while keeping the vision encoder in the VLM reasoner frozen.More detailed experimental settings are in Appendix G.</p>
<p>Evaluation on the GVLQA-BASE Dataset</p>
<p>In this subsection, we perform experiments on the GVLQA-BASE dataset to compare GITA with popular LLMs including GPT-4 Turbo [49], LLaMA2-7B/13B [57], and Vicuna-7B/13B [73], under both zero-shot and fine-tuning settings.The experimental results are shown in Table 1.Based on these results, we can obtain the following observations.</p>
<p>Observation 1: GITA Outperforms LLM Baselines.As can be seen in Table 1, GITA consistently outperforms the LLM baselines under the same setting.This underscores its SOTA effectiveness in instruction-based graph reasoning tasks, showing robust capabilities across different parameter scales under both fine-tuning and zero-shot settings.Moreover, under the fine-tuning setting, incorporating the vision modality consistently benefits 7B models.But for the 13B models, the performance of some tasks may degrade.This could be attributed to the greater challenge of aligning representations of the visual and textual modalities in the larger 13B models compared to the 7B models, in the case of only fine-tuning LoRA adapters in the text decoder.We speculate that full training could potentially address this issue.However, we leave this as future work due to resource constraints.the benefits of increasing the model size for graph reasoning capabilities.In this regard, GITA-13B outperforms its counterpart with 7B parameters (GITA-7B) both on average and across four of the seven tasks.However, it is worth noting that GITA-13B does not outperform GITA-7B on the other three tasks.We hypothesize that this discrepancy may be attributed to insufficient modality alignment due to LoRA fine-tuning.</p>
<p>Observation 4: Vision and Text Modalities Proficient in Different Types of Graph Reasoning Tasks.We explore the individual capabilities of the visual and textual modalities within the GITA framework.The results indicate that the text and vision modalities can complement each other and contribute to better performance than individual ones, as removing either modality leads to performance drops in most cases (Vicuna &amp; GITA (VO) and GPT-4 Turbo &amp; GITA (VO) in Table 1).While the graph reasoning capability provided by the vision modality may not be as strong as that of the text modality in most cases, relying solely on vision still enables the model to possess basic graph reasoning abilities.Specifically, the model outperforms text-based LLMs in 2 of the 7 tasks (Cycle and BGM) when relying solely on vision.This consistent improvement across all comparison groups demonstrates the potential of the vision modality to excel in certain graph reasoning tasks, leveraging its ability to capture visual patterns like cycles and graph properties such as bipartition.In contrast, text exhibits a higher proficiency than vision modality in sequence-related graph reasoning problems, particularly on tasks such as TS, SP, and HP, which require constructing ordered node sequences.</p>
<p>Evaluation for the Visual Graph Augmentations</p>
<p>To assess the impact of the proposed visual graph augmentation strategies (including layout, node shape, node outline style, and edge thickness augmentations), we compare the performance of visiononly GITA-7B models trained on the four augmented subsets of GVLQA and on GVLQA-BASE (without augmentation).The results are presented in Table 2. To fully utilize the visual information in visual graphs, we fine-tune the visual encoder of VLMs in addition to the vision-to-text projector and the LoRA adapters within the text decoder in this experiment.</p>
<p>As can be seen from the results, a significant enhancement in overall performance is observed with the introduction of layout augmentation (GVLQA-AUGLY).The average performance improves remarkably from 38.91% to 63.36%.Notably, significant improvements are observed on SP (5.72% to 76.55%), HP (1.11% to 70.74%), and Connect (59.97% to 87.18%).These findings highlight the critical role of layout augmentation in generating visual graphs.In other words, this observation suggests the potential for creating larger-scale datasets for vision-language-based graph reasoning, which could significantly contribute to advancing this field.Conversely, the other three augmentations do not yield such substantial performance improvements, further emphasizing the importance of layout augmentation in vision-language-based reasoning.</p>
<p>Evaluation on Real-World Datasets</p>
<p>In this section, we study the effectiveness of GITA on the ca-GrQC [42] and ca-HepTh [42] datasets for the link prediction task, and on the PolBlog [1], Cora [65] and CiteSeer [65] datasets for the node classification task.The experimental results are presented in Table 3.It is evident that GITA consistently outperforms the LLM baselines, and its performance progressively improves with the addition of layout augmentation and the use of the GVLQA checkpoint.Notably, we emphasize the advantages of using GVLQA-BASE as a pretrained dataset by comparing it with GITA-7b.Performance improvements of 0.67% and 0.92% are observed in the zero-shot and fine-tuning settings, respectively.This highlights the potential application value of the proposed GVLQA dataset.Though GITA is designed for language-based general graph reasoning settings, which are much more user-friendly (by user-readable natural language) and general (unique model architecture for various scenarios) than the typical application of dedicated GNNs, it remains essential to conduct a comprehensive comparison with specialized GNNs to elucidate the strengths and limitations of GITA's applicability and capabilities.To this end, we assess the graph reasoning abilities of GITA against dedicated GNNs, including GCN [38] and SAGE [24], using the GVLQA-Base dataset, as detailed in Table 4.In addition, we explore and compare the effects of k-hop subgraph sampling on the proposed GITA and GNN baselines.Using the ca-Hepth dataset, we analyze the impact of increasing the number of hops k on the reasoning time and performance of both GITA and GNNs.</p>
<p>Comparison of GITA with Dedicated Graph Baselines</p>
<p>The results are in Table 5.</p>
<p>Overall Graph Reasoning Ability Comparison.As shown in Table 4, compared to the dedicated GNNs, the fine-tuned GITA-7B models have comparable average graph reasoning performance, with the larger GATA-13B model performs slightly better.In particular, compared to GNNs, the GITA model shows a stronger ability in recognizing local structures in the graphs (Connect and Cycle) and to accomplish tasks with obvious layout heuristics (BGM).We believe that this advantage comes from GITA's visual perception.For SP and MaxFlow, GITA's performance is inferior to GNNs.This may be because GNNs process edge weight more effectively through the message-passing mechanism.</p>
<p>Scalability and Performance Variation with Different Numbers of Hops k.The inference time results are shown in Table 5.As can be seen, GITA demonstrates inferior scalability compared to the GNN baselines.Its scalability remains stable as the sampled graph size (i.e., k) increases.From the accuracy results in Table 5, GITA, GCN, and SAGE achieve their peak performance at k = 2, suggesting that a small sampled graph size suffices for optimal performance.Though the dedicated GNNs attain higher peak performance than GITA, they exhibit performance declines as k increases (e.g., 3 or 4), while GITA's performance is more stable w.r.t.k.</p>
<p>Case Study</p>
<p>Textual Query</p>
<p>In an undirected graph, (i,j) means that node i and node j are connected with an undirected edge.</p>
<p>The nodes are numbered from 0 to 9, and the edges are:  On the other hand, the fixed layout of visual graphs presented in GVLQA-BASE may impede the visual encoder in identifying the shortest path between two nodes, although we have verified layout augmentation can greatly improve the graph reasoning abilities of models, as shown in Sec.5.2.This limitation might arise from the confusion caused by the visual distance within an image, without considering the weights between the nodes.For instance, in Figure 2(b), the correct answer is '4-&gt;6-&gt;0', which visually appears as a more convoluted path but numerically has a shorter path length of 3 = 1 + 2. In contrast, the incorrect answer given by GITA-7B (vision-only) is '4-&gt;2-&gt;0', which has a higher path length cost of 4 = 1 + 3 but visually seems like a more direct shortcut.This observation further validates the effectiveness of employing layout augmentation to enhance performance in this task.Layout variations of visual graphs play a crucial role in mitigating the visual confusion caused by the spatial arrangement within a visual graph.However, it seems more effective for text-based LLMs to handle explicitly separated nodes and weights, as illustrated by the text (in red) in Figure 2(b).</p>
<p>Conclusion</p>
<p>In this paper, we propose an end-to-end framework called GITA for vision-language-based graph reasoning.Extensive experiments validate the superiority of incorporating visual information into instruction-based graph reasoning.Furthermore, we conduct comparative analysis of the four proposed visual graph augmentations and identify layout augmentation as the most effective approach for enhancing visual graphs.This finding offers valuable insights for the development of largerscale datasets aimed at facilitating vision-language-based graph reasoning.Lastly, we highlight the potential application value of the proposed GVLQA dataset as a pretrained dataset.</p>
<p>A Visual Modality Enhances Effectiveness by Uncovering Critical Substructures</p>
<p>In this section, we present a case study to highlight the complementary role of the visual modality in graph reasoning tasks.The visual modality excels at recognizing beneficial substructures or local patterns, which are crucial for effective graph reasoning.For instance, identifying the "hop number" is essential for shortest path calculations, recognizing "leaf nodes" is vital for topological sorting, and detecting "cycles" is necessary to avoid in Hamilton path construction.We extracted these substructures from the GVLQA-Base dataset and manually labeled them.By employing a frozen Vision Transformer (ViT) in the LLaVA framework with a trainable Multi-Layer Perceptron (MLP) decoder, we achieved identification accuracies of 89.92%, 95.16%, and 92.39% for hop number counting, leaf node identification, and cycle detection, respectively.In contrast, using a pre-trained BERT model with the same trainable MLP decoder resulted in significantly lower accuracies of 55.47%, 26.33%, and 60.32% for the same tasks.Therefore, the enhanced effectiveness of integrating visual and textual modalities can be attributed to the additional structural information provided by the visual modality, which facilitates the identification of these critical substructures.</p>
<p>B Advantages of GITA Over Traditional Graph Neural Networks</p>
<p>GITA offers several advantages over traditional Graph Neural Networks (GNNs) in terms of generalizability, flexibility, and user-friendliness:</p>
<p>Unlike GNNs, which require task-specific feature engineering and architecture adjustments, GITA employs a unified model architecture for all tasks, demonstrating its generalizability.By separating task specifications from graph structures, GITA can handle various graph reasoning tasks seamlessly.Additionally, it exhibits strong zero-shot capabilities, allowing it to perform well on tasks it has not been explicitly trained on, which is a feature not commonly found in traditional GNNs.</p>
<p>Besides, traditional GNNs often demand specialized knowledge in model architectures and coding to accommodate diverse tasks, posing a challenge for non-experts.In contrast, GITA overcomes this barrier by employing language-based templates for task adaptation, enhancing its flexibility.This flexibility enables GITA to effectively handle a broad spectrum of tasks, offering a framework that can be customized to specific requirements using daily language, without the necessity of profound expertise in graph neural networks.</p>
<p>Moreover, by leveraging existing VLMs, GITA can respond in natural language, allowing for intuitive graph reasoning with simple queries like "Is there a cycle in this graph?"This stands in contrast to the unreadable vector representations typically used in GNNs, significantly enhancing GITA's user-friendliness.</p>
<p>C Datasets Statistics</p>
<p>D Illustrations for Visualization Tools in Graph Visualizer</p>
<p>The GITA graph visualizer incorporates a variety of implementations for existing visualization tools such as Graphviz, Matplotlib with NetworkX, and igraph, each selected for their unique capabilities  Figure 3 showcases some visual graphs produced by these different graph visualizer implementations.</p>
<p>E Graph-describing Templates</p>
<p>The graph describer relies on a set of unified structured templates designed to generate coherent and informative descriptions that emphasize the inherent characteristics of the graph itself, regardless of specific task requirements.These graph-describing templates cover various scenarios, including directed graphs, undirected graphs, graphs with node identities or features, and graphs with edge weights or capacities.Table 9 provides an illustration of these templates, where [P] denotes placeholders required to be filled by corresponding graph information.</p>
<p>F Examples of Manual-template-based and LLM-agent-bootstrapped Query Generation</p>
<p>Manual-template-based Query Generation.The queries Q T G can be generated by task-specific manual templates.These templates are manually crafted by human to supplement descriptions/instructions about 1) concrete meanings of nodes and edges, 2) task responsibilities and 3) input/output specifications into the task-agnostic graph description D G .Therefore, the precision and faith of generated task-specific queries Q T G are guaranteed by human calibrations.An example of manual-templatebased query generation for topological sorting is illustrated in Figure 4.In this example, placeholders [P] are used to represent information that scripts will automatically fill in.</p>
<p>Graph categories Undirected</p>
<p>Directed</p>
<p>Prototype</p>
<p>In an undirected graph, (i,j) means that node i and node j are connected with an undirected edge.</p>
<p>Textual Description</p>
<p>In a directed graph, (i,j) means that node i and node j are connected with an undirected edge.</p>
<p>The nodes are numbered from 0 to 6, and the edges are: (0,2) (2,6) ...</p>
<p>Task-specific Query</p>
<p>In a directed graph with 7 nodes numbered from 0 to 6: node 0 should be visited before node 2, ... This task is to find a valid topological sorting for this directed graph.Please provide a possible topological ordering path, for example: 0 -&gt; 1 -&gt; 2 -&gt; 3 -&gt; 4. Q:</p>
<p>The topological order of the directed graph is:  LLM-agent-bootstrapped Query Generation. Figure 5 presents an example of employing a bootstrapped LLM agent, such as ChatGPT[48], for monster-hunting gaming.By incorporating task-specific information into the prompt, including node/edge meanings and task responsibilities, the LLM agent automatically generates a response that serves as the desired task-specific query.Compared to using manual templates, bootstrapping LLM agents for automated synthesis is more flexible and economic as it can take advantage of LLM agents to automatically interpret context and generate appropriate queries for various scenarios and minimize manual effort with changing conditions.Such properties make it suitable for dynamic or bespoke tasks, such as robotic planning or complex gaming scenarios.</p>
<p>Manual-template-based Questioner</p>
<p>Graph description: In a directed graph, (i,j) means that node i and node j are connected with an undirected edge.The nodes are numbered from 0 to 3, and the edges are: (0,1) (1,2) (2,3) ...You are required to help refine the graph description with 1) concrete nodes/edges meaning: Node -&gt; Monsters Edge-&gt; Two monsters coupled and hunting one of them will irritate another.2) task responsibility: hunting monster each by each without irritating more than 1. 3) A short sentence to rule input/output specification, for example, a sentence like "The output should be only a number" or "The output should be only Yes. or No.".Directly give me the refined graph description:</p>
<p>Task-specific Query</p>
<p>In a directed graph, each node represents a monster, and an undirected edge between two nodes signifies that the corresponding monsters are coupled.If one monster is hunted, it will irritate the other monster it is coupled with.The nodes are numbered from 0 to 3, and the edges are defined as follows: (0,1) -Monster 0 and Monster 1 are coupled.</p>
<p>(1,2) -Monster 1 and Monster 2 are coupled.</p>
<p>(2,3) -Monster 2 and Monster 3 are coupled.... The task responsibility is to hunt each monster one by one without irritating more than one monster at a time.</p>
<p>Input/Output Specification: The output should be a sequence of numbers representing the order in which monsters can be hunted without irritating more than one monster at a time.</p>
<p>Task Responsibility meanings of nodes/edges</p>
<p>Output Specification</p>
<p>Prompt</p>
<p>G Experiment Settings</p>
<p>For all fine-tuning experiments, we use a batch size of 128 and adopt the AdamW optimizer (with a learning rate of 0.0002 and 0.00002 for the LoRA adapters within the text decoder and vision-to-text projector, respectively).</p>
<p>Detailed Settings for GVLQA Dataset During the evaluation, the temperature is set to 0 for all baselines.All fine-tuning experiments are conducted on an NVIDIA DGX station with 8×A100 GPUs.We split the GVLQA dataset in the ratio of 7:3 for training and testing, respectively.The accuracy (%) metrics are computed by comparing the prediction and ground truths with exact matching.We use the next-token-prediction loss to fine-tune the LoRA [28] adapters of LLMs and the vision-to-text projector.Visual graphs are encoded as visual embeddings by a visual encoder.Visual embeddings are concatenated with the embeddings of textual descriptions and instructions (i.e., questions), then fed to the text decoder to generate the answer.</p>
<p>Real-world Datasets Here we provide more details about the five real-world datasets used in Sec 5.3.The datasets ca-GrQC and ca-HepTh represent collaboration networks from the arXiv sections of General Relativity and Quantum Cosmology, and High Energy Physics -Theory, respectively, featuring nodes as authors and edges as co-authorships.They can be downloaded from Stanford Network Analysis Project (SNAP) website 6 .PolBlogs is a network of U.S. political blogs from February 2005, categorized by political alignment and linked by blog references.Cora and CiteSeer are both citation networks, where nodes correspond to scientific papers and edges to citations, utilized for tasks such as document classification and citation analysis, with papers categorized into various research fields.Statistics of the datasets are shown in Table 8.For each dataset, 80%/10%/10% of the edges are randomly used for training/validation/testing, respectively.</p>
<p>Detailed Settings for Real-world Benchmarks In the conventional semi-supervised node classification setting, class labels are available for some nodes, which is reflected in the visual graph by coloring the nodes with a unique random color for each class.To focus on evaluating the model's ability to capture structural information, GITA filters out the influence of node features in Cora and CiteSeer datasets.For link prediction tasks on ca-GrQC and ca-HepTh datasets, GITA treats the graphs as undirected.In the test split, both the original links and their reverse links do not appear in the train and valid splits.During training and evaluation, an equal number of negative sampled links are used alongside the positive links.These negative links are sampled at each training epoch but remain fixed during evaluation.For the GVLQA pretrain checkpoint, GITA adopts the 7B cycle checkpoint finetuned on GVLQA-BASE, where the performance is nearly mature.Hyperparameter combinations for each model are determined through grid search, and the specific combinations can be found in the provided code.</p>
<p>H Illustrations of GVLQA subsets</p>
<p>In this section, we present the illustrations of the GVLQA subsets.Figure 6 provides an overview of GVLQA-BASE.Subsequently, from Figure 7 to Figure 10, we showcase the augmented visual graphs in GVLQA-AUGLY (augment layouts), GVLQA-AUGNS (augment node styles), GVLQA-AUGNO (augment node outline styles), and GVLQA-AUGET (augment edge thickness), respectively.</p>
<p>In this undirected graph, (i,j) means that node i and node j are connected with an undirected edge.The nodes are numbered from 0 to 5, and the edges are: (0,5) (0,1) (0,3) (1,5) (1,3) (2,4) (3,5).</p>
<p>Q: Is there a path between node 1 and node 5 in this undirected graph?A: Yes.</p>
<p>Visual graph</p>
<p>Textual description</p>
<p>Connectivity</p>
<p>In this undirected graph, (i,j) means that node i and node j are connected with an undirected edge.</p>
<p>Q:</p>
<p>The topological order of the directed graph is: A: 1,4,3,0,2.</p>
<p>Visual graph</p>
<p>Visual graph</p>
<p>Textual description</p>
<p>Maximum Flow</p>
<p>There are 4 hosts numbered from 0 to 3, and 3 tasks numbered from 0 to 2. Each host has a set of tasks that it is interested in: Host 2 is interested in task 2. Host 2 is interested in task 1 ...However, each host is capable of solving only one task, and similarly, each task can be resolved by just one host.</p>
<p>Q: What is the maximum number of hosts that can be assigned a task they are interested in?A: 2.</p>
<p>Visual graph</p>
<p>Textual description</p>
<p>Bipartite Graph Matching</p>
<p>In this undirected graph, (i,j) means that node i and node j are connected with an undirected edge.The nodes are numbered from 0 to 10, and the edges are:</p>
<p>I Limitation</p>
<p>The GITA framework proposed in the paper, along with its experimental results, exhibits certain limitations that should be acknowledged.Firstly, when dealing with large-scale graphs, the conventional subgraph sampling strategy employed by GITA may result in imbalanced and insufficient sampling, leading to the loss of critical graph structural information.This compromise is necessary to accommodate the limited contextual length of the text-based LLM and the restricted scope of the visual graph.Secondly, due to computational constraints, the fine-tuning procedures in the paper were restricted to the LoRA framework.While this approach offers advantages, a more comprehensive fine-tuning process that considers both visual and text modalities is expected to better align the two and potentially enhance performance.Addressing these limitations should be considered as a future research direction in this field.</p>
<p>• The answer NA means that the paper does not include experiments.</p>
<p>• If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.• If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.• Depending on the contribution, reproducibility can be accomplished in various ways.</p>
<p>For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model.In general.releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.• While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution., with an open-source dataset or instructions for how to construct the dataset).(d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility.</p>
<p>In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.</p>
<p>Open access to data and code</p>
<p>Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?</p>
<p>Answer: [Yes] Justification: The complete codes are included, and the proposed GVLQA dataset is released with common access.</p>
<p>Guidelines:</p>
<p>• The answer NA means that the paper does not include experiments requiring code.• At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).• Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.</p>
<p>Experimental Setting/Details</p>
<p>Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results?</p>
<p>Answer: [Yes]</p>
<p>Justification: Experiment details are given in both Section 5 and Appendix G.</p>
<p>Guidelines:</p>
<p>• The answer NA means that the paper does not include experiments.</p>
<p>• The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them.• The full details can be provided either with the code, in appendix, or as supplemental material.</p>
<p>Experiment Statistical Significance</p>
<p>Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments?</p>
<p>Answer: [No] Justification: The paper does not include error bars.</p>
<p>Guidelines:</p>
<p>• The answer NA means that the paper does not include experiments.</p>
<p>• The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.• The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).• The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) • The assumptions made should be given (e.g., Normally distributed errors).• It should be clear whether the error bar is the standard deviation or the standard error of the mean.• It is OK to report 1-sigma error bars, but one should state it.The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.• For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g.negative error rates).• If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.</p>
<p>Experiments Compute Resources</p>
<p>Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments?</p>
<p>Answer: [Yes] Justification: We report the machine (type and storage) requirements in Appendix G.</p>
<p>Guidelines:</p>
<p>• The answer NA means that the paper does not include experiments.</p>
<p>• The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.• The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.• The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).</p>
<p>Code Of Ethics</p>
<p>Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?</p>
<p>Answer: [Yes]</p>
<p>Justification: We make sure the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics.</p>
<p>Guidelines:</p>
<p>• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.</p>
<p>• If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.• The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).</p>
<p>Broader Impacts</p>
<p>Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed?</p>
<p>Answer: [NA] Justification: The research does not have concerns about societal impacts because it is designed for general-purpose graph reasoning.</p>
<p>Guidelines:</p>
<p>• The answer NA means that there is no societal impact of the work performed.</p>
<p>• If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.• Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.• The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments.However, if there is a direct path to any negative applications, the authors should point it out.For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation.On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.• The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.• If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).</p>
<p>Safeguards</p>
<p>Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)?</p>
<p>In traditional graph reasoning settings, models typically rely on two main inputs: (i) the graph structure G = {C, E}, where C and E are the set of vertices and edges, respectively;</p>
<dl>
<dt>Figure 1 :</dt>
<dt>1</dt>
<dt>Figure 1: The architecture of the GITA framework with comparison to existing LLM solution.</dt>
<dd>
<p>Yes.GITA-7B (VO): No. GITA-7B: No.</p>
</dd>
</dl>
<p>Figure 2 :
2
Figure 2: A comparative case study of graph representation in vision and text modalities.All methods are trained on the GVLQA-BASE dataset.</p>
<p>Figure 3 :
3
Figure 3: Examples of the visual graph generated by various visualization tools.</p>
<p>Figure 4 :
4
Figure 4: Examples of the manual-template-based query generation, where [P] denotes the placeholders.</p>
<p>Figure 5 :
5
Figure 5: Examples of the LLM-agent-bootstrapped query generation.</p>
<p>, with each edge's distance or length indicated by a numerical label in close proximity.In a undirected graph, the nodes are numbered from 0 to 4, and the edges are: an edge between node 1 and node 0 with weight 1, an edge ...Q:Give the shortest path from node 3 to node 1: A: 3-&gt;2-&gt;0-&gt;1.directed graph, with each edge's capacity indicated by a numerical label in close proximity.In this directed graph, the nodes are numbered from 0 to 5, and the edges are: an edge from node 1 to node 3 with capacity 9, an edge from node 2 to node 0 with capacity 9, an edge from node 2 to node 3 with capacity 7, an edge from node 4 to node 3 with capacity 1, an edge from node 4 to node 2 with ... Q: What is the maximum flow from node 4 to node 3: A: 14.</p>
<p>Figure 6 :
6
Figure 6: An overview of the GVLQA-BASE.Each figure depicts the tasks involving graph-based reasoning, showcasing a visual graph, a textual question, and the corresponding answer.</p>
<p>For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm.(b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully.(c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g.</p>
<p>Table 2 :
2
Accuracy (%) comparisons across GVLQA subsets using GITA-7B (VO).↑ denotes dramatic performance improvement.
ModelsConnect CycleTSSPMaxFlow BGMHPAvgZero-shotLLaMA2-7B50.0649.43 0.000.000.000.000.00 14.21Vicuna-7B50.0649.43 0.000.000.000.000.00 14.21GITA-7B (VO)50.0650.33 0.000.000.000.000.00 14.34GITA-7B50.0649.43 0.000.000.000.000.00 14.21GPT-4 Turbo76.7049.51 19.59 35.356.8942.11 47.04 39.60GITA-ZS (VO)57.7663.34 5.344.881.5946.60 10.74 27.18GITA-ZS82.5851.46 19.71 37.696.0052.21 50.00 42.81Fine-tuningLLaMA2-7B97.3394.63 33.26 26.019.5690.86 23.95 53.66Vicuna-7B97.5895.04 34.46 25.989.3391.04 25.55 54.15GITA-7B (VO)59.9796.34 13.30 5.722.8993.01 1.11 38.91GITA-7B98.9596.67 41.12 32.1520.0093.19 29.26 58.76LLaMA2-13B98.7993.36 33.83 27.9312.2291.34 33.46 55.85Vicuna-13B99.3594.39 36.73 28.5311.3492.65 34.81 56.83GITA-13B (VO)58.0096.91 14.45 5.724.8993.19 1.85 39.29GITA-13B99.1495.60 38.69 40.4720.6692.12 33.33 60.00
Observation 2: Mainstream Open-source VLM/LLMs Lack Fundamental Graph Reasoning Abilities.The zero-shot results illustrate that prominent open-source LLMs or VLMs, including LLaMA2, Vicuna, and LLaVA, exhibit minimal graph reasoning capabilities on the GVLQA-BASE dataset.Specifically, these models produce random answers, i.e., randomly responding with either "Yes." or "No." for tasks involving Connect and Cycle, resulting in a performance close to 50%.Cur-Table1: Accuracy (%) comparisons on GVLQA-BASE under zero-shot and fine-tuning settings, where "VO" denotes a variant of GITA using only the vision modality.</p>
<p>Table 3 :
3
Accuracy (%) comparisons on real-world datasets under zero-shot and fine-tuning settings, where ‡ indicates the usage of a checkpoint pretrained in the Cycle task of GVLQA-BASE.
Modelsca-GrQc ca-HepTh PolBlogs Cora CiteSeerAvgZero-shotLLaMA2-7B40.5948.8910.7424.3530.3330.98Vicuna-7B41.3550.008.7226.9429.1331.22GITA-7B71.9586.0646.9831.3730.6353.40GITA-7B  ‡72.0286.0848.3232.1031.8354.07Fine-tuningLLaMA2-7B76.5789.0680.5483.7673.2780.64Vicuna-7B78.9589.8580.5484.8774.1781.68GITA-7B79.7091.1384.5685.2475.0783.14GITA-7B (w/ AUGLY)79.7791.2185.2385.2475.6883.43GITA-7B  ‡80.4691.6885.2386.3576.5784.06
rent SOTA closed-source LLMs or VLMs, including GPT-4 Turbo and GPT-4V, demonstrate superior zero-shot performance compared with the aforementioned open-source models.This observation implies that current open-source LLMs and VLMs lack basic graph reasoning ability, which may be attributed to the insufficient availability of relevant training data.Such observation also enhances our motivation to propose the GVLQA dataset, with the aim of improving the graph reasoning capabilities of VLMs/LLMs.Observation 3: Increasing Model Size Leads to Better Graph Reasoning Capabilities.The comparison of VLMs/LLMs with different parameter sizes, specifically 7B and 13B models, verify</p>
<p>Table 8 in the appendix C presents the statistics of these datasets.The graph can have thousands of nodes/edges, making it infeasible to feed the entire graph into the model.Consequently, we employ k-hop subgraph sampling (with k = 2) discussed in Sec 3.2 to satisfy the token length restriction of LLMs and visual graph scope effectively.</p>
<p>Table 4 :
4
Accuracy (%) comparisons among dedicated GNNs and GITAs on GVLQA-Base.
Connect CycleTSSPMaxFlow BGMHPAvgGCN79.6570.89 45.71 44.5656.4476.70 32.22 58.02SAGE82.7273.58 44.51 49.2550.6781.00 36.67 59.78GITA-7B98.9596.67 41.12 32.1520.0093.19 29.26 58.76GITA-13B99.1495.60 38.69 40.4720.6692.12 33.33 60.00</p>
<p>Table 5 :
5
Accuracy (%) and Inference Time (in parentheses) for GNNs and GITA on ca-Hepth Dataset with different subgraph sampling hop number k ∈ {1, 2, 3, 4}.
GCNSAGEGITA-7Bk=1 93.27 (0.02s) 94.40 (0.03s) 90.33 (17.23min)k=2 94.49 (0.04s) 94.43 (0.04s) 91.13 (17.66min)k=3 91.10 (0.04s) 90.95 (0.18s) 90.31 (17.22min)k=4 81.92 (0.05s) 83.60 (0.22s) 86.10 (17.01min)</p>
<p>Table 6 :
6
Statistics of the GVLQA dataset.
SubsetConnect CycleTSSPMaxFlow BMGHPTotalBASE16,4104,1002,9101,5601,5001,86090029,240AUGLY98,460 24,600 17,460 9,3609,00011,160 5,400 175,440AUGNS49,230 12,300 8,7304,6804,5005,5802,70087,720AUGNO 65,640 16,400 11,640 6,2406,0007,4403,600 116,960AUGET65,640 16,400 11,640 6,2406,0007,4403,600 116,960Total295,380 73,800 52,380 28,08027,00033,480 16,200 526,320</p>
<p>Table 7 :
7
Average numbers of nodes and edges for each task in GVLQA.These tools are implemented in our code as interchangeable modules, enhancing flexibility based on the requirements of different projects.
Average / Task Connect CycleTSSPMaxFlow BGMHP#nodes25.0123.42 21.86 13.6513.9021.13 13.24#edges95.4623.66 114.10 23.9949.1651.03 45.05Table 8: Statistics of real-world datasetsca-GrQCca-HepThPolBlogsCoraCiteSeer# Nodes5,2429,8771,4902,7083,327# Edges14,49625,99819,0255,2784,676domaincollaboration collaborationsocialcitation citationaverage degree5.535.2625.543.92.74in graph rendering.</p>
<p>Table 9 :
9
Graph-describing Templates for various categories.</p>
<p>• Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.• While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer.Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).• The instructions should contain the exact command and environment needed to run to reproduce the results.See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy)for more details.• The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. • The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines.If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.</p>
<p>https://snap.stanford.edu/index.html
AcknowledgementsThis work was supported by NSFC key grant 62136005 and NSFC general grant 62076118, and in part by the Research Grants Council of the Hong Kong Special Administrative Region (Grants 16200021 and 16202523).v-graph.github.io.4Code Repository: https://github.com/WEIYanbin1999/GITA/.In this undirected graph, (i,j) means that node i and node j are connected with an undirected edge.The nodes are numbered from 0 to 5, and the edges are: (0,5) (0,1) (0,3) (1,5) (1,3) (2,4)(3,5).Q: Is there a path between node 1 and node 5 in this undirected graph?A: Yes.Visual graphTextual description 1. Connectivity Q: Is there a cycle in this undirected graph?A: No.Visual graph Textual description 2. CycleThis diagram depicts a directed graph, in which each directed edge from node A to node B signifies that, according to the topological order, node A must precede node B. In this directed graph with 5 nodes numbered from 0 to 4: node 1 should be visited before node 0 ...Q:The topological order of the directed graph is: A: 1,4,3,0,2.Visual graph Textual descriptionTopology SortThis graphic illustrates an undirected graph, with each edge's distance or length indicated by a numerical label in close proximity.In a undirected graph, the nodes are numbered from 0 to 4, and the edges are: an edge between node 1 and node 0 with weight 1, an edge ...Q:Give the shortest path from node 3 to node 1: A: 3-&gt;2-&gt;0-&gt;1.Visual graph Textual description 4. Shortest PathThis graphic illustrates a directed graph, with each edge's capacity indicated by a numerical label in close proximity.In this directed graph, the nodes are numbered from 0 to 5, and the edges are: an edge from node 1 to node 3 with capacity 9, an edge from node 2 to node 0 with capacity 9, an edge from node 2 to node 3 with capacity 7, an edge from node 4 to node 3 with capacity 1, an edge from node 4 to node 2 with ...Q:What is the maximum flow from node 4 to node 3: A: 14.Visual graph Textual description 5. Maximum FlowThere are 4 hosts numbered from 0 to 3, and 3 tasks numbered from 0 to 2. Each host has a set of tasks that it is interested in: Host 2 is interested in task 2. Host 2 is interested in task 1 ...However, each host is capable of solving only one task, and similarly, each task can be resolved by just one host.Q: What is the maximum number of hosts that can be assigned a task they are interested in?A: 2.Visual graphTextual descriptionBipartite Graph MatchingIn this undirected graph, (i,j) means that node i and node j are connected with an undirected edge.The nodes are numbered from 0 to 10, and the edges are: Q: Begin with node 0, Is there a path in this graph that visits every node exactly once?If yes, give the path.Note that in a path, adjacent nodes must be connected with edges.A: 0-&gt;6-&gt;1-&gt;7-&gt;3-&gt;10-&gt;5-&gt;9-&gt;8-&gt;2-&gt;4.In this undirected graph, (i,j) means that node i and node j are connected with an undirected edge.The nodes are numbered from 0 to 5, and the edges are: (0,5) (0,1) (0,3) (1,5) (1,3) (2,4)(3,5).Visual graphQ: Is there a path between node 1 and node 5 in this undirected graph?A: Yes.Visual graphVisual graph Textual descriptionCycleThis diagram depicts a directed graph, in which each directed edge from node A to node B signifies that, according to the topological order, node A must precede node B. In this directed graph with 5 nodes numbered from 0 to 4: node 1 should be visited before node 0 ...Q:The topological order of the directed graph is: A: 1,4,3,0,2.Visual graph Textual descriptionTopology SortThis graphic illustrates an undirected graph, with each edge's distance or length indicated by a numerical label in close proximity.In a undirected graph, the nodes are numbered from 0 to 4, and the edges are: an edge between node 1 and node 0 with weight 1, an edge ...Q:Give the shortest path from node 3 to node 1: A: 3-&gt;2-&gt;0-&gt;1.Visual graph Textual descriptionShortest PathQ: What is the maximum flow from node 4 to node 3: A: 14.Visual graph Textual description 5. Maximum FlowThere are 4 hosts numbered from 0 to 3, and 3 tasks numbered from 0 to 2. Each host has a set of tasks that it is interested in: Host 2 is interested in task 2. Host 2 is interested in task 1 ...However, each host is capable of solving only one task, and similarly, each task can be resolved by just one host.Q: What is the maximum number of hosts that can be assigned a task they are interested in?A: 2.Visual graph Textual description 6. Bipartite Graph MatchingIn this undirected graph, (i,j) means that node i and node j are connected with an undirected edge.The nodes are numbered from 0 to 10, and the edges are: Q: Begin with node 0, Is there a path in this graph that visits every node exactly once?If yes, give the path.Note that in a path, adjacent nodes must be connected with edges.A: 0-&gt;6-&gt;1-&gt;7-&gt;3-&gt;10-&gt;5-&gt;9-&gt;8-&gt;2-&gt;4.Visual graph Textual description 7. Hamilton PathIn this undirected graph, (i,j) means that node i and node j are connected with an undirected edge.The nodes are numbered from 0 to 4, and the edges are: (1,0) (1,4) (3,1) (2,0).This graphic illustrates a directed graph, with each edge's capacity indicated by a numerical label in close proximity.In this directed graph, the nodes are numbered from 0 to 5, and the edges are: an edge from node 1 to node 3 with capacity 9, an edge from node 2 to node 0 with capacity 9, an edge from node 2 to node 3 with capacity 7, an edge from node 4 to node 3 with capacity 1, an edge from node 4 to node 2 with ... In this undirected graph, (i,j) means that node i and node j are connected with an undirected edge.The nodes are numbered from 0 to 5, and the edges are: (0,5) (0,1) (0,3) (1,5) (1,3) (2,4)(3,5).Q: Is there a path between node 1 and node 5 in this undirected graph?A: Yes.Visual graph Textual description1. Connectivity Q: Is there a cycle in this undirected graph?A: No.Visual graph Textual description 2. CycleThis diagram depicts a directed graph, in which each directed edge from node A to node B signifies that, according to the topological order, node A must precede node B. In this directed graph with 5 nodes numbered from 0 to 4: node 1 should be visited before node 0 ...Q:The topological order of the directed graph is:Visual graph Textual descriptionTopology SortThis graphic illustrates an undirected graph, with each edge's distance or length indicated by a numerical label in close proximity.In a undirected graph, the nodes are numbered from 0 to 4, and the edges are: an edge between node 1 and node 0 with weight 1, an edge ...Q:Give the shortest path from node 3 to node 1:Visual graph Textual description 4. Shortest PathQ: What is the maximum flow from node 4 to node 3: A: 14.Visual graph Textual description 5. Maximum FlowThere are 4 hosts numbered from 0 to 3, and 3 tasks numbered from 0 to 2. Each host has a set of tasks that it is interested in: Host 2 is interested in task 2. Host 2 is interested in task 1 ...However, each host is capable of solving only task, and similarly, each task can be resolved by just one host.Q: What is the maximum number of hosts that can be assigned a task they are interested in?A: 2.Visual graph Textual description 6. Bipartite Graph Matchingthis undirected graph, (i,j) means that node i and node j are connected with an undirected edge.The nodes are numbered from 0 to 10, and the edges are: (7,1) (10,3) (0,6) (5,10) (5,3) (9,8) (7,3) (1,10) (7,4) (1,5) (4,10) (5,9) (7,5) (6,1) (5,2) (2,8) (8,5) (4,2) (0,2) (5,6) (1,4).Q: Begin with node 0, Is there a path in this graph that visits every node exactly once?If yes, give the path.Note that in a path, adjacent nodes must be connected with edges.A: 0-&gt;6-&gt;1-&gt;7-&gt;3-&gt;10-&gt;5-&gt;9-&gt;8-&gt;2-&gt;4.Visual graphTextual descriptionHamilton PathIn this undirected graph, (i,j) means that node i and node j are connected with an undirected edge.The nodes are numbered from 0 to 4, and the edges are:This graphic illustrates a directed graph, with each edge's capacity indicated by a numerical label in close proximity.In this directed graph, the nodes are numbered from 0 to 5, and the edges are: an edge from node 1 to node 3 with capacity 9, an edge from node 2 to node 0 with capacity 9, an edge from node 2 to node 3 with capacity 7, an edge from node 4 to node 3 with capacity 1, an edge from node 4 to node 2 with ... In this undirected graph, (i,j) means that node i and node j are connected with an undirected edge.The nodes are numbered from 0 to 5, and the edges are: (0,5) (0,1) (0,3) (1,5) (1,3) (2,4) (3,5).Q: Is there a path between node 1 and node 5 in this undirected graph?A: Yes.Visual graphVisual graph Textual description 5. Maximum FlowThere are 4 hosts numbered from 0 to 3, and 3 tasks numbered from 0 to 2. Each host has a set of tasks that it is interested in: Host 2 is interested in task 2. Host 2 is interested in task 1 ...However, each host is capable of solving only one task, and similarly, each task can be resolved by just one host.Q: What is the maximum number of hosts that can be assigned a task they are interested in?A: 2.Visual graph Textual description 6. Bipartite Graph MatchingIn this undirected graph, (i,j) means that node i and node j are connected with an undirected edge.The nodes are numbered from 0 to 10, and the edges are:NeurIPS Paper ChecklistThe checklist is designed to encourage best practices for responsible machine learning research, addressing issues of reproducibility, transparency, research ethics, and societal impact.Do not remove the checklist: The papers not including the checklist will be desk rejected.The checklist should follow the references and follow the (optional) supplemental material.The checklist does NOT count towards the page limit.Please read the checklist guidelines carefully for information on how to answer these questions.For each question in the checklist:• You should answer [Yes] , [No] , or [NA] .• [NA] means either that the question is Not Applicable for that particular paper or the relevant information is Not Available.• Please provide a short (1-2 sentence) justification right after your answer (even for NA).The checklist answers are an integral part of your paper submission.They are visible to the reviewers, area chairs, senior area chairs, and ethics reviewers.You will be asked to also include it (after eventual revisions) with the final version of your paper, and its final version will be published with the paper.The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation.While "[Yes] " is generally preferable to "[No] ", it is perfectly acceptable to answer "[No] " provided a proper justification is given (e.g., "error bars are not reported because it would be too computationally expensive" or "we were unable to find the license for the dataset we used").In general, answering "[No] " or "[NA] " is not grounds for rejection.While the questions are phrased in a binary way, we acknowledge that the true answer is often more nuanced, so please just use your best judgment and write a justification to elaborate.All supporting evidence can appear either in the main paper or the supplemental material, provided in appendix.If you answer [Yes] to a question, in the justification please point to the section(s) where related material for the question can be found.IMPORTANT, please:• Delete this instruction block, but keep the section heading "NeurIPS paper checklist",• Keep the checklist subsection headings, questions/answers and guidelines below.• Do not modify the questions and only use the provided macros for your answers.ClaimsQuestion: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope?Answer:[Yes]Justification: Please refer to the abstract part and the contribution enumeration at the tail of the introduction part.Guidelines:• The answer NA means that the abstract and introduction do not include the claims made in the paper.• The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations.A No or NA answer to this question will not be perceived well by the reviewers.• The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.• It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.LimitationsQuestion: Does the paper discuss the limitations of the work performed by the authors?Answer:[Yes]Justification: Please refer to Appendix I Guidelines:• The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.• The authors are encouraged to create a separate "Limitations" section in their paper.• The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally).The authors should reflect on how these assumptions might be violated in practice and what the implications would be.• The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs.In general, empirical results often depend on implicit assumptions, which should be articulated.• The authors should reflect on the factors that influence the performance of the approach.For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting.Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.• The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.• If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.• While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper.The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community.Reviewers will be specifically instructed to not penalize honesty concerning limitations.Theory Assumptions and ProofsQuestion: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?Answer: [NA] Justification: The paper does not involve any theoretical result.Guidelines:• The answer NA means that the paper does not include theoretical results.• All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.• All assumptions should be clearly stated or referenced in the statement of any theorems.• The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.• Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.• Theorems and Lemmas that the proof relies upon should be properly referenced.Experimental Result ReproducibilityQuestion: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)?Answer: [Yes] Justification: We show fundamental experiment settings in Section 5, and more details for experiments settings in Appendix G. Besides, we provide the complete codes as supplementary materials.Guidelines:Answer: [Yes] Justification: The paper includes using an graph visualizer to generate abstract graph images, however, these images are focus on graph structure, without any sensitive information.Guidelines:• The answer NA means that the paper poses no such risks.• Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.• Datasets that have been scraped from the Internet could pose safety risks.The authors should describe how they avoided releasing unsafe images.• We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.12. Licenses for existing assets Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected?Answer: [Yes] Justification: We have cited necessary assets and conduct CC-BY 4.0 for our codes and datasets.Guidelines:• The answer NA means that the paper does not use existing assets.• The authors should cite the original paper that produced the code package or dataset.• The authors should state which version of the asset is used and, if possible, include a URL.• The name of the license (e.g., CC-BY 4.0) should be included for each asset.• For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.• If assets are released, the license, copyright information, and terms of use in the package should be provided.For popular datasets, paperswithcode.com/datasetshas curated licenses for some datasets.Their licensing guide can help determine the license of a dataset.• For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.• If this information is not available online, the authors are encouraged to reach out to the asset's creators.• We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.• For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.
The political blogosphere and the 2004 us election: divided they blog. A Lada, Natalie Adamic, Glance, Proceedings of the 3rd international workshop on Link discovery. the 3rd international workshop on Link discovery2005</p>
<p>Flamingo: a visual language model for few-shot learning. Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, Advances in Neural Information Processing Systems. 202235</p>
<p>Neural module networks. Jacob Andreas, Marcus Rohrbach, Trevor Darrell, Dan Klein, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern Recognition2016</p>
<p>Interaction networks for learning about objects, relations and physics. Peter Battaglia, Razvan Pascanu, Matthew Lai, Danilo Jimenez Rezende, Advances in neural information processing systems. 292016</p>
<p>Relational inductive biases, deep learning, and graph networks. Jessica B Peter W Battaglia, Victor Hamrick, Alvaro Bapst, Vinicius Sanchez-Gonzalez, Mateusz Zambaldi, Andrea Malinowski, David Tacchetti, Adam Raposo, Ryan Santoro, Faulkner, arXiv:1806.012612018arXiv preprint</p>
<p>Translating embeddings for modeling multi-relational data. Antoine Bordes, Nicolas Usunier, Alberto Garcia-Duran, Jason Weston, Oksana Yakhnenko, Advances in neural information processing systems. 262013</p>
<p>Popularity prediction on social platforms with coupled graph neural networks. Qi Cao, Huawei Shen, Jinhua Gao, Bingzheng Wei, Xueqi Cheng, Proceedings of the 13th International Conference on Web Search and Data Mining. the 13th International Conference on Web Search and Data Mining2020</p>
<p>Structure-aware transformer for graph representation learning. Dexiong Chen, O' Leslie, Karsten Bray, Borgwardt, International Conference on Machine Learning. PMLR2022</p>
<p>Runjin Chen, Tong Zhao, Ajay Jaiswal, Neil Shah, Zhangyang Wang, arXiv:2402.08170Llaga: Large language and graph assistant. 2024arXiv preprint</p>
<p>Exploring the potential of large language models (LLMs) in learning on graphs. Zhikai Chen, Haitao Mao, Hang Li, Wei Jin, Hongzhi Wen, Xiaochi Wei, Shuaiqiang Wang, Dawei Yin, Wenqi Fan, Hui Liu, arXiv:2307.033932023arXiv preprint</p>
<p>Instructblip: Towards general-purpose vision-language models with instruction tuning. Dai, Li, Li, Tiong, Zhao, Wang, Li, Fung, Hoi, Advances in Neural Information Processing Systems. 2023</p>
<p>Solution of a large-scale travelingsalesman problem. George Dantzig, Ray Fulkerson, Selmer Johnson, Journal of the operations research society of America. 241954</p>
<p>A note on two problems in connexion with graphs. W Edsger, Dijkstra, 1959Numerische Mathematik</p>
<p>Talk like a graph: Encoding graphs for large language models. Bahare Fatemi, Jonathan Halcrow, Bryan Perozzi, Proceedings of International Conference on Learning Representations. International Conference on Learning Representations2024</p>
<p>Maximal flow through a network. Lester Randolph, Ford , Delbert R Fulkerson, Canadian Journal of Mathematics. 1956</p>
<p>Nemesis: Normalizing the soft-prompt vectors of vision-language models. Shuai Fu, Xiequn Wang, Qiushi Huang, Yu Zhang, Proceedings of International Conference on Learning Representations. International Conference on Learning Representations2024</p>
<p>An open graph visualization system and its applications to software engineering. Software: practice and experience. R Emden, Stephen C Gansner, North, 200030</p>
<p>Proteome survey reveals modularity of the yeast cell machinery. Anne-Claude Gavin, Patrick Aloy, Paola Grandi, Roland Krause, Markus Boesche, Martina Marzioch, Christina Rau, Lars Juhl Jensen, Sonja Bastuck, Birgit Dümpelfeld, Nature. 44070842006</p>
<p>Neural message passing for quantum chemistry. Justin Gilmer, Patrick F Samuel S Schoenholz, Oriol Riley, George E Vinyals, Dahl, International conference on machine learning. PMLR2017</p>
<p>Advances on the hamiltonian problem-a survey. Graphs and Combinatorics. J Ronald, Gould, 2003</p>
<p>GPT4Graph: Can large language models understand graph structured data? an empirical evaluation and benchmarking. Jiayan Guo, Lun Du, Hengyu Liu, arXiv:2305.150662023arXiv preprint</p>
<p>Exploring network structure, dynamics, and function using networkx. Aric Hagberg, Pieter Swart, Daniel S Chult, 2008Los Alamos, NM (United StatesLos Alamos National Lab.(LANL)Technical report</p>
<p>Inductive representation learning on large graphs. Will Hamilton, Zhitao Ying, Jure Leskovec, 201730Advances in neural information processing systems</p>
<p>Inductive representation learning on large graphs. Will Hamilton, Zhitao Ying, Jure Leskovec, 201730Advances in neural information processing systems</p>
<p>Lightgcn: Simplifying and powering graph convolution network for recommendation. Xiangnan He, Kuan Deng, Xiang Wang, Yan Li, Yongdong Zhang, Meng Wang, Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval. the 43rd International ACM SIGIR conference on research and development in Information Retrieval2020</p>
<p>Neural collaborative filtering. Xiangnan He, Lizi Liao, Hanwang Zhang, Liqiang Nie, Xia Hu, Tat-Seng Chua, Proceedings of the 26th international conference on world wide web. the 26th international conference on world wide web2017</p>
<p>Harnessing explanations: LLM-to-LM interpreter for enhanced text-attributed graph representation learning. Xiaoxin He, Xavier Bresson, Thomas Laurent, Adam Perold, Yann Lecun, Bryan Hooi, Proceedings of International Conference on Learning Representations. International Conference on Learning Representations2024</p>
<p>Low-rank adaptation of large language models. J Edward, Phillip Hu, Zeyuan Wallis, Yuanzhi Allen-Zhu, Shean Li, Lu Wang, Weizhu Wang, Chen, International Conference on Learning Representations. 2021</p>
<p>Knowledge-aware coupled graph neural network for social recommendation. Chao Huang, Huance Xu, Yong Xu, Peng Dai, Lianghao Xia, Mengyin Lu, Liefeng Bo, Hao Xing, Xiaoping Lai, Yanfang Ye, Proceedings of the AAAI conference on artificial intelligence. the AAAI conference on artificial intelligence202135</p>
<p>Gqa: A new dataset for real-world visual reasoning and compositional question answering. A Drew, Christopher D Hudson, Manning, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2019</p>
<p>Lethality and centrality in protein networks. Hawoong Jeong, Sean P Mason, A-L Barabási, Zoltan N Oltvai, Nature. 41168332001</p>
<p>Forward-backward reasoning in large language models for mathematical verification. Weisen Jiang, Han Shi, Longhui Yu, Zhengying Liu, Yu Zhang, Zhenguo Li, James Kwok, Findings of the. Association for Computational Linguistics2024</p>
<p>Effective structured-prompting by meta-learning and representitive verbalizer. Weisen Jiang, Yu Zhang, James Kwok, International Conference on Machine Learning. 2023</p>
<p>Clevr: A diagnostic dataset for compositional language and elementary visual reasoning. Justin Johnson, Bharath Hariharan, Laurens Van Der Maaten, Li Fei-Fei, Lawrence Zitnick, Ross Girshick, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern Recognition2017</p>
<p>Topological sorting of large networks. Arthur B Kahn, Communications of ACM. 1962</p>
<p>An optimal algorithm for on-line bipartite matching. Umesh V Richard M Karp, Vijay V Vazirani, Vazirani, Proceedings of the twenty-second annual ACM symposium on Theory of computing. the twenty-second annual ACM symposium on Theory of computing1990</p>
<p>Semi-supervised classification with graph convolutional networks. N Thomas, Max Kipf, Welling, arXiv:1609.029072016arXiv preprint</p>
<p>Semi-supervised classification with graph convolutional networks. N Thomas, Max Kipf, Welling, arXiv:1609.029072016arXiv preprint</p>
<p>Matrix factorization techniques for recommender systems. Yehuda Koren, Robert Bell, Chris Volinsky, Computer. 4282009</p>
<p>Rethinking graph transformers with spectral attention. Devin Kreuzer, Dominique Beaini, Will Hamilton, Vincent Létourneau, Prudencio Tossou, Advances in Neural Information Processing Systems. 202134</p>
<p>Microscopic evolution of social networks. Jure Leskovec, Lars Backstrom, Ravi Kumar, Andrew Tomkins, Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining. the 14th ACM SIGKDD international conference on Knowledge discovery and data mining2008</p>
<p>Graph evolution: Densification and shrinking diameters. Jure Leskovec, Jon Kleinberg, Christos Faloutsos, ACM transactions on Knowledge Discovery from Data (TKDD). 1122007</p>
<p>BLIP-2: bootstrapping language-image pre-training with frozen image encoders and large language models. Junnan Li, Dongxu Li, Silvio Savarese, Steven Hoi, International conference on machine learning. 2023</p>
<p>Evaluating large language models on graphs: Performance insights and comparative analysis. Chang Liu, Bo Wu, arXiv:2308.112242023arXiv preprint</p>
<p>Visual instruction tuning. Haotian Liu, Chunyuan Li, Qingyang Wu, Yong Jae Lee, Advances in Neural Information Processing Systems. 2023</p>
<p>Mask and reason: Pre-training knowledge graph transformers for complex logical queries. Xiao Liu, Shiyu Zhao, Kai Su, Yukuo Cen, Jiezhong Qiu, Mengdi Zhang, Wei Wu, Yuxiao Dong, Jie Tang, Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining2022</p>
<p>The structure and function of complex networks. Mark Ej Newman, SIAM review. 4522003</p>
<p>GPT-4 Turbo. 2023OpenAITechnical report</p>
<p>Let your graph do the talking: Encoding structured data for llms. Bryan Perozzi, Bahare Fatemi, Dustin Zelle, Anton Tsitsulin, Mehran Kazemi, Rami Al-Rfou, Jonathan Halcrow, arXiv:2402.058622024arXiv preprint</p>
<p>Learning transferable visual models from natural language supervision. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, International conference on machine learning. PMLR2021</p>
<p>Algorithms in C, part 5: graph algorithms. Robert Sedgewick, 2001Pearson Education</p>
<p>Reasoning with neural tensor networks for knowledge base completion. Richard Socher, Danqi Chen, Christopher D Manning, Andrew Ng, Advances in neural information processing systems. 262013</p>
<p>A corpus for reasoning about natural language grounded in photographs. Alane Suhr, Stephanie Zhou, Ally Zhang, Iris Zhang, Huajun Bai, Yoav Artzi, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. the 57th Annual Meeting of the Association for Computational Linguistics2019</p>
<p>Graphgpt: Graph instruction tuning for large language models. Jiabin Tang, Yuhao Yang, Wei Wei, Lei Shi, Lixin Su, Suqi Cheng, Dawei Yin, Chao Huang, arXiv:2310.130232023arXiv preprint</p>
<p>Matplotlib for Python developers. Sandro Tosi, 2009Packt Publishing Ltd</p>
<p>Llama 2: Open foundation and fine-tuned chat models. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, arXiv:2307.092882023arXiv preprint</p>
<p>Graph attention networks. Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, Yoshua Bengio, stat. 1050202017</p>
<p>Can language models solve graph problems in natural language?. Heng Wang, Shangbin Feng, Tianxing He, Zhaoxuan Tan, Xiaochuang Han, Yulia Tsvetkov, NeurIPS. 2023</p>
<p>Learning intents behind interactions with knowledge graph for recommendation. Xiang Wang, Tinglin Huang, Dingxian Wang, Yancheng Yuan, Zhenguang Liu, Xiangnan He, Tat-Seng Chua, Proceedings of the web conference 2021. the web conference 20212021</p>
<p>Chain of thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H Chi, Denny Quoc V Le, Zhou, Proceedings of Neural Information Processing Systems. Neural Information Processing Systems2022</p>
<p>Kicgpt: Large language model with knowledge in context for knowledge graph completion. Yanbin Wei, Qiushi Huang, Yu Zhang, James Kwok, Findings of the Association for Computational Linguistics: EMNLP 2023. 2023</p>
<p>A comprehensive survey on graph neural networks. Zonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi Zhang, Philip S Yu, IEEE Transactions on Neural Networks and Learning Systems. 2020</p>
<p>Keyulu Xu, Weihua Hu, Jure Leskovec, Stefanie Jegelka, arXiv:1810.00826How powerful are graph neural networks?. 2018arXiv preprint</p>
<p>Revisiting semi-supervised learning with graph embeddings. Zhilin Yang, William Cohen, Ruslan Salakhudinov, International conference on machine learning. PMLR2016</p>
<p>Natural language is all a graph needs. Ruosong Ye, Caiqi Zhang, Runhui Wang, Shuyuan Xu, Yongfeng Zhang, arXiv:2308.071342023arXiv preprint</p>
<p>MetaMath: Bootstrap your own mathematical questions for large language models. Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengying Liu, Yu Zhang, James T Kwok, Zhenguo Li, Adrian Weller, Weiyang Liu, Proceedings of International Conference on Learning Representations. International Conference on Learning Representations2024</p>
<p>From recognition to cognition: Visual commonsense reasoning. Rowan Zellers, Yonatan Bisk, Ali Farhadi, Yejin Choi, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2019</p>
<p>Raven: A dataset for relational and analogical visual reasoning. Chi Zhang, Feng Gao, Baoxiong Jia, Yixin Zhu, Song-Chun Zhu, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2019</p>
<p>Duzhen Zhang, Yahan Yu, Chenxing Li, Jiahua Dong, Dan Su, Chenhui Chu, Dong Yu, arXiv:2401.13601Mmllms: Recent advances in multimodal large language models. 2024arXiv preprint</p>
<p>Graph-bert: Only attention is needed for learning graph representations. Jiawei Zhang, Haopeng Zhang, Congying Xia, Li Sun, arXiv:2001.051402020arXiv preprint</p>
<p>Cone: Cone embeddings for multi-hop reasoning over knowledge graphs. Zhanqiu Zhang, Jie Wang, Jiajun Chen, Shuiwang Ji, Feng Wu, Advances in Neural Information Processing Systems. 202134</p>
<p>and Ion Stoica. Judging LLM-as-a-judge with MT-bench and chatbot arena. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, Hao Zhang, Joseph E Gonzalez, 2023In NeurIPS (Datasets and Benchmarks Track</p>
<p>Least-to-most prompting enables complex reasoning in large language models. Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Ed H Quoc V Le, Chi, Proceedings of International Conference on Learning Representations. International Conference on Learning Representations2023</p>
<p>Minigpt-4: Enhancing vision-language understanding with advanced large language models. Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, Mohamed Elhoseiny, arXiv:2304.105922023arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3067 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3067</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3067</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-76.html">extraction-schema-76</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <p><strong>Paper ID:</strong> paper-254854344</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2212.09146v2.pdf" target="_blank">Can Retriever-Augmented Language Models Reason? The Blame Game Between the Retriever and the Language Model</a></p>
                <p><strong>Paper Abstract:</strong> Augmenting pretrained language models with retrievers to select the supporting documents has shown promise in effectively solving common NLP problems, including language modeling and question answering, in an interpretable way. In this paper, we first study the strengths and weaknesses of different retriever-augmented language models (REALM, $k$NN-LM, FiD coupled with DPR, and ATLAS and Flan-T5 coupled with Contriever) in reasoning over the retrieved statements in different tasks. We show how the retrieve-then-read models' limitations in reasoning are rooted both in the retriever module as well as the language model. Our experimental results demonstrate that the similarity metric used by the retrievers is generally insufficient for reasoning tasks. Additionally, we show that the language models in retriever-augmented models do not take the complicated relations between the statements into account, which leads to poor reasoning performance even when using the larger models. Moreover, we analyze the reasoning performance of large language models using multihop retrieval but we only observe minor improvements. Overall, this shows great room for further research in this area.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3067.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3067.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>REALM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Retrieval-Augmented Language Model (REALM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A BERT-based masked language model augmented with a latent dense retriever that selects documents by similarity and feeds retrieved passages to the model for masked-token prediction or extractive QA.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>REALM</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Masked language model (BERT-based) with a latent knowledge retriever that encodes queries and passages with a BERT encoder and selects top passages by dense similarity (e.g., inner product); for QA a BERT reader extracts answer spans from retrieved passages.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['retrieval-augmented reasoning (similarity-based retrieval)', 'extractive-span reasoning (selecting answer spans from retrieved passages)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>REALM relies on a dense retriever to return the most similar statements to the query (inner-product on BERT encodings). For LM it appends retrieved statements to the query and predicts masked tokens; for QA it extracts an answer span from among retrieved passages rather than composing across multiple statements.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>similar style (single retrieval-based approach focused on query–document similarity). The paper finds REALM uses a uniform similarity-retrieval heuristic and an extractive reader, so it does not perform diverse multi-step reasoning over combinations of statements.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>EntailmentBank (EB-1/2/3) and StrategyQA (converted to declarative) — Language Modeling and Question Answering</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>EntailmentBank: multi-step entailment trees where answers require composing facts; StrategyQA: yes/no questions requiring implicit multi-step reasoning (converted to declarative form for LM experiments). Evaluation includes next-token preference/accuracy and token-overlap F1 for QA.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>On LM tasks REALM is among the stronger models (paper: 'ATLAS and REALM perform well in language modeling'). In QA REALM performs poorly, tending to extract answers from the first retrieved statements; its QA score plateaus after ~3 retrieved statements. Exact numeric metrics are not provided in-text.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>Compared against FiD, ATLAS, kNN-LM, and Flan-T5: REALM is relatively strong at LM but weak at QA where reasoning across multiple statements is required. The paper attributes this to REALM's extractive approach and retriever similarity heuristic which does not capture inter-statement relations.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>REALM's similarity-based retriever and extractive reader lead to failures when correct answers require composing multiple retrieved statements; it often returns answers from early retrieved passages instead of integrating information across multiple statements.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>REALM performs well on LM next-token prediction but fails on multi-statement QA even when gold statements are retrieved, because the reader extracts single spans rather than composing facts.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can Retriever-Augmented Language Models Reason? The Blame Game Between the Retriever and the Language Model', 'publication_date_yy_mm': '2022-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3067.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3067.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>kNN-LM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>k-Nearest Neighbor Language Model (kNN-LM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An autoregressive decoder-only Transformer that interpolates its next-token distribution with a distribution estimated from nearest-neighbor token sequences retrieved from an external datastore.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>kNN-LM</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Decoder-only Transformer augmented with a k-nearest-neighbor module: at generation time, nearest token-sequence neighbors are found by L2 similarity in representation space and their next-token statistics are interpolated with the model's own distribution (interpolation weight λ).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['retrieval-augmented statistical interpolation (kNN over token sequences)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>kNN-LM finds nearest neighbor token sequences (spans) by L2 similarity to the current context representation and forms a distribution over next tokens from those neighbors; final prediction interpolates the model's distribution with the kNN distribution (paper used λ=0.65). It does not perform explicit multi-statement logical composition.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>single/similar style (nearest-neighbor interpolation). The method leverages token-sequence similarity and frequency of next tokens — it does not implement diverse compositional reasoning strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>EntailmentBank and StrategyQA (LM and QA setups as used in the paper)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>LM preference and next-token prediction tasks requiring reasoning across statements; QA token-overlap evaluation for answer generation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>kNN-LM performs substantially worse than REALM, FiD, and ATLAS on LM preference tasks and shows poorer ability to prefer gold targets over alternatives; it also underperforms in QA. Increasing the number of retrieved sequences still fails to cover all gold statements (tends to retrieve multiple spans from the same statement). Exact numeric metrics are not provided in-text.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>Compared to other retrieval-augmented models, kNN-LM's nearest-neighbor interpolation does not enable composing multiple facts and achieves lower reasoning performance even when gold statements are available.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>kNN-LM's retrieval over token sequences and interpolation affects next-token distributions but does not support reasoning over combinations of statements; retrieving more sequences often retrieves repeated content from the same statement and fails to retrieve needed distinct facts.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Even when allowed many (e.g., 100) retrieved sequences, kNN-LM does not reliably cover all gold statements and remains worse on the LM preference proxy; thus simply increasing retrieval does not overcome the lack of compositional reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can Retriever-Augmented Language Models Reason? The Blame Game Between the Retriever and the Language Model', 'publication_date_yy_mm': '2022-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3067.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3067.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>FiD</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Fusion-in-Decoder (FiD)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A sequence-to-sequence T5-based architecture that encodes each retrieved passage separately and lets the decoder attend over all encoded representations to generate answers.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>FiD (T5-based Fusion-in-Decoder)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Seq2seq model using T5 encoder(s) to encode query+each retrieved passage independently; the decoder attends jointly over all encodings to produce outputs. In the paper FiD is paired with DPR retriever (dense inner-product).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['retrieval-augmented generative decoding (Fusion-in-Decoder)', 'encoder-decoder attention over multiple retrieved statements']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>FiD encodes each retrieved statement separately and the decoder can attend to all encodings, enabling implicit combination of multiple passages during generation. The model is not, however, explicitly trained to perform symbolic or structured compositional reasoning over relations between statements.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>similar style within retrieval-augmented architectures but using a decoder-attention fusion mechanism; it does not explicitly implement distinct reasoning paradigms beyond aggregation via attention.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>EntailmentBank and QA tasks (LM and QA evaluations)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Tasks require composing multiple supporting statements (EntailmentBank) and answering questions by integrating facts; evaluation via LM preference and QA token-overlap F1.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>FiD is among the superior models for QA (paper: 'FiD and Flan-T5 are the superior models in QA tasks'); it also performs reasonably when hypothesis-like statement is provided. Exact numeric scores are not reproduced in-text.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>FiD outperforms some retriever-augmented alternatives (REALM, ATLAS) on QA in the experiments; however, FiD still fails frequently when reasoning across retrieved statements is required and when retriever returns distracting statements.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Decoder fusion across encodings helps FiD in QA relative to extractive readers, but FiD is not explicitly trained to perform logical composition across statements and thus still exhibits reasoning failures when the retriever is imperfect or information must be combined in nontrivial ways.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>FiD benefits when a conclusion/hypothesis statement (one-step form of the answer) is present among inputs, indicating it is better at direct matching than multi-step compositional entailment; it still struggles when needed relations are not explicit.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can Retriever-Augmented Language Models Reason? The Blame Game Between the Retriever and the Language Model', 'publication_date_yy_mm': '2022-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3067.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e3067.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ATLAS</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ATLAS (pre-trained retrieval-augmented language model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A retrieval-augmented sequence-to-sequence model that jointly fine-tunes a Contriever-style retriever and a T5-like reader to better integrate retrieval with generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ATLAS</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Pretrained retrieval-augmented seq2seq model where retriever (Contriever) and reader are fine-tuned jointly; retrieval is based on inner-products of averaged encoder hidden states (Contriever). The reader architecture is similar to FiD (T5-style).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['retrieval-augmented seq2seq reasoning with joint retriever-reader fine-tuning', 'encoder-decoder attention fusion (FiD-like)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>ATLAS fine-tunes retriever and reader jointly on downstream tasks to improve retrieval quality in an end-to-end fashion; the reader then attends over multiple retrieved passages to produce answers. Retrieval still uses dense similarity (inner product) but the joint training aims to improve relevance.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>similar style (retrieval + decoder fusion) with some architectural diversity because retriever and reader are co-finetuned; overall still follows similarity-based retrieval heuristics rather than heterogeneous reasoning strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>EntailmentBank and QA (LM and QA tasks in the paper)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Multi-step entailment and question answering tasks requiring composing facts; evaluated with LM preference/accuracy and QA token-overlap F1.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>ATLAS performs well in LM tasks (paper: 'ATLAS and REALM perform well in language modeling') but worse in QA (does not deeply reason; tends to copy basic answers). Exact numbers are not quoted in-text.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>Compared with FiD and Flan-T5, ATLAS is stronger on LM but weaker on QA reasoning. Joint retriever-reader fine-tuning improves retrieval quality relative to some retrievers, but ATLAS still fails to perform deep multi-statement reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>ATLAS's joint training improves some retrieval robustness, leading to good LM behavior, but the model still does not perform deep compositional reasoning over multiple retrieved facts and often copies simple answers.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>ATLAS fails when reasoning across statements is needed and is observed to copy basic answers rather than integrate multiple facts for entailment-style reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can Retriever-Augmented Language Models Reason? The Blame Game Between the Retriever and the Language Model', 'publication_date_yy_mm': '2022-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3067.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e3067.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Flan-T5</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Flan-T5 (instruction-finetuned T5)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An instruction-finetuned T5 variant shown to improve reasoning abilities when prompted with chain-of-thought style finetuning; in this paper it is evaluated as a retriever-augmented model when paired with DPR.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Flan-T5 (instruction-finetuned T5)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>T5 model finetuned with instruction tuning and chain-of-thought style datasets to elicit reasoning; in experiments Flan-T5-base is paired with DPR retriever and consumes concatenated retrieved statements plus query as input to generate answers.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>base (flan-t5-base checkpoint used)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['instruction finetuning / chain-of-thought prompting', 'retrieval-augmented concatenation (input includes retrieved statements concatenated with query)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Flan-T5 has been finetuned on instruction-following and chain-of-thought reasoning datasets, which helps generation that includes intermediate reasoning steps; when paired with a retriever (DPR) it consumes retrieved statements concatenated with the query, relying on its finetuned reasoning style to combine facts.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>more diverse than purely similarity-based readers: combines instruction/chain-of-thought training with retrieval; however, when retrieval returns distracting statements Flan-T5 can be very sensitive and fails to ignore distractors.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>EntailmentBank and StrategyQA (LM / QA evaluations in the paper)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Multi-step entailment (EntailmentBank) and implicit-strategy QA (StrategyQA converted to declarative) requiring composition of supporting facts; measured via LM preference and QA token-overlap F1.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>Flan-T5 is among the superior models for QA in the paper (FiD and Flan-T5 best in QA). With gold statements Flan-T5 achieves the best QA performance among studied models but absolute QA F1 remains modest (paper reports best QA performance around ~0.4 on EntailmentBank with gold statements). Flan-T5 is sensitive to distracting (irrelevant) retrieved statements, which degrades performance.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>When compared to other retrieval-augmented models Flan-T5 benefits from instruction/chain-of-thought finetuning and outperforms them on QA when gold facts are provided; however, Flan-T5's sensitivity to distracting evidence means it can degrade more than retriever-trained models when retriever introduces irrelevant statements.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Instruction finetuning and chain-of-thought-style training improve Flan-T5's ability to reason over multiple facts when the required supporting facts are present, but Flan-T5 is also more harmed by distracting retrieved statements compared to models trained with imperfect retrievers.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Even with gold statements Flan-T5's QA scores are limited (best ~0.4 F1), and when retrievers introduce distractors Flan-T5's performance drops substantially; sometimes Flan-T5 generates answers ignoring retrieved statements entirely.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can Retriever-Augmented Language Models Reason? The Blame Game Between the Retriever and the Language Model', 'publication_date_yy_mm': '2022-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3067.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e3067.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DPR</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Dense Passage Retriever (DPR)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A dense dual-encoder passage retriever that encodes queries and passages with separate encoders and retrieves passages by inner-product similarity in embedding space.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DPR</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Dual BERT-based encoders produce embeddings for queries and passages (e.g., [CLS] token); retrieval is performed by inner-product similarity to select top-k passages. In the paper DPR is used with FiD and Flan-T5 experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['similarity-based dense retrieval']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>DPR returns the passages most similar to the query as measured by inner-product of embeddings; it does not consider relations between candidate passages, so it often fails to return combinations of statements needed for multi-step reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>single/similarity-focused retrieval style; does not provide diverse retrieval heuristics that consider inter-passage relations.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>EntailmentBank-2 retrieval evaluation (and other tasks where retriever supplies statements to models)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Retrieve supporting statements necessary for entailment-style reasoning in EntailmentBank; evaluated by retrieval F1 of ground-truth statements.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>DPR retrieves a gold top-1 statement only ~15% of the time on EntailmentBank-2 according to the paper; overall DPR coverage of gold statements is low especially at small k.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>Compared with Contriever and other retrievers, DPR is weaker in retrieving necessary statements for reasoning; Contriever is reported as superior among the studied retrievers but still insufficient overall.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Similarity-based dense retrieval (DPR) often misses crucial supporting statements required for multi-step reasoning; relying solely on query–document similarity is insufficient.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Even with increased k DPR's top retrieved items often do not include needed statements; DPR's failures harm downstream reasoning models substantially (e.g., Flan-T5 is sensitive to distractors DPR may introduce).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can Retriever-Augmented Language Models Reason? The Blame Game Between the Retriever and the Language Model', 'publication_date_yy_mm': '2022-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3067.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e3067.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Contriever</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Contriever (unsupervised dense retriever)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An unsupervised dense retrieval encoder trained with contrastive learning to produce passage embeddings; used as the retriever for ATLAS in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Contriever</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Dual-encoder retriever that produces averaged last-layer encoder representations for passages and uses inner-product similarity for retrieval; trained with unsupervised contrastive objectives to improve general retrieval quality.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['similarity-based dense retrieval (contrastive pretraining)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Contriever ranks passages by inner-product similarity of learned embeddings; while it improves retrieval quality relative to some alternatives, it still does not model relations among passages needed for compositional reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>single retrieval style; marginally more robust due to contrastive pretraining but not a diversity of reasoning strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>EntailmentBank-2 retrieval evaluation and downstream LM/QA tasks</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Retrieve ground-truth supporting statements for multi-step entailment; used to assess retriever contribution to reasoning performance.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>Contriever is reported as superior among the studied retrievers (better retrieval F1 than DPR/kNN/REALM retrievers) but still fails to reliably retrieve all required statements; overall retriever F1 remains low for small k.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>While Contriever outperforms DPR and other retrievers in this study, all studied retrievers suffer from the same limitation of relying on similarity and failing to account for inter-statement relations, which limits downstream reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Even the best retriever examined (Contriever) does not retrieve all gold statements at small k; retriever-level improvements alone are insufficient without modeling relations/composition across retrieved statements.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Contriever improves retrieval but not enough to enable reliable multi-statement reasoning in downstream models; retriever failures remain a bottleneck.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can Retriever-Augmented Language Models Reason? The Blame Game Between the Retriever and the Language Model', 'publication_date_yy_mm': '2022-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3067.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e3067.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Chain-of-Thought / Instruction FT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Thought reasoning (instruction finetuning)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting/finetuning style that elicits intermediate step-by-step reasoning (chain-of-thought) from sequence models; used to strengthen Flan-T5's reasoning abilities via instruction finetuning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Chain-of-Thought / Instruction Finetuning</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Training or prompting approaches that expose models to intermediate reasoning steps (chain-of-thought) and instruction-following data to improve multi-step reasoning and explanation generation.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['chain-of-thought (CoT)', 'instruction finetuning']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Chain-of-thought encourages models to generate intermediate reasoning steps rather than only final answers; instruction finetuning exposes models to many instruction–response pairs (including CoT style examples) to improve generalization on reasoning tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>diverse style (introduces stepwise reasoning behavior absent from simple retrieval-and-match pipelines). In the paper Flan-T5 (instruction finetuned) is an example of a model with this additional reasoning style.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>EntailmentBank and other reasoning benchmarks discussed in related work</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Tasks requiring step-by-step reasoning where intermediate inferences matter; CoT is meant to help models chain facts across multiple steps.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>Flan-T5 benefits from this style and achieves the best QA performance among models in this study when given gold statements, but absolute performance remains limited (best QA ~0.4 F1).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>Chain-of-thought/instruction finetuning helps Flan-T5 outperform other models on QA when supporting facts are present, but it is more sensitive to distracting retrieved statements than models trained with noisy retrievers.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>CoT/instruction finetuning improves ability to reason across facts, but without robust retrieval that returns the right pieces of evidence and without mechanisms to ignore distractors, CoT finetuning alone does not solve the reasoning deficits.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Despite CoT finetuning, Flan-T5 still fails frequently on EntailmentBank and is harmed by distracting retrievals; CoT did not yield large absolute QA scores in this setup.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can Retriever-Augmented Language Models Reason? The Blame Game Between the Retriever and the Language Model', 'publication_date_yy_mm': '2022-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Realm: Retrievalaugmented language model pre-training <em>(Rating: 2)</em></li>
                <li>Leveraging passage retrieval with generative models for open domain question answering <em>(Rating: 2)</em></li>
                <li>Unsupervised dense information retrieval with contrastive learning <em>(Rating: 2)</em></li>
                <li>Scaling instruction-finetuned language models <em>(Rating: 2)</em></li>
                <li>Explaining answers with entailment trees <em>(Rating: 2)</em></li>
                <li>Did Aristotle Use a Laptop? A Question Answering Benchmark with Implicit Reasoning Strategies <em>(Rating: 2)</em></li>
                <li>Faithful reasoning using large language models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3067",
    "paper_id": "paper-254854344",
    "extraction_schema_id": "extraction-schema-76",
    "extracted_data": [
        {
            "name_short": "REALM",
            "name_full": "Retrieval-Augmented Language Model (REALM)",
            "brief_description": "A BERT-based masked language model augmented with a latent dense retriever that selects documents by similarity and feeds retrieved passages to the model for masked-token prediction or extractive QA.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "REALM",
            "model_description": "Masked language model (BERT-based) with a latent knowledge retriever that encodes queries and passages with a BERT encoder and selects top passages by dense similarity (e.g., inner product); for QA a BERT reader extracts answer spans from retrieved passages.",
            "model_size": null,
            "reasoning_methods": [
                "retrieval-augmented reasoning (similarity-based retrieval)",
                "extractive-span reasoning (selecting answer spans from retrieved passages)"
            ],
            "reasoning_methods_description": "REALM relies on a dense retriever to return the most similar statements to the query (inner-product on BERT encodings). For LM it appends retrieved statements to the query and predicts masked tokens; for QA it extracts an answer span from among retrieved passages rather than composing across multiple statements.",
            "diversity_of_methods": "similar style (single retrieval-based approach focused on query–document similarity). The paper finds REALM uses a uniform similarity-retrieval heuristic and an extractive reader, so it does not perform diverse multi-step reasoning over combinations of statements.",
            "reasoning_task_name": "EntailmentBank (EB-1/2/3) and StrategyQA (converted to declarative) — Language Modeling and Question Answering",
            "reasoning_task_description": "EntailmentBank: multi-step entailment trees where answers require composing facts; StrategyQA: yes/no questions requiring implicit multi-step reasoning (converted to declarative form for LM experiments). Evaluation includes next-token preference/accuracy and token-overlap F1 for QA.",
            "performance_by_method": "On LM tasks REALM is among the stronger models (paper: 'ATLAS and REALM perform well in language modeling'). In QA REALM performs poorly, tending to extract answers from the first retrieved statements; its QA score plateaus after ~3 retrieved statements. Exact numeric metrics are not provided in-text.",
            "comparison_of_methods": "Compared against FiD, ATLAS, kNN-LM, and Flan-T5: REALM is relatively strong at LM but weak at QA where reasoning across multiple statements is required. The paper attributes this to REALM's extractive approach and retriever similarity heuristic which does not capture inter-statement relations.",
            "key_findings": "REALM's similarity-based retriever and extractive reader lead to failures when correct answers require composing multiple retrieved statements; it often returns answers from early retrieved passages instead of integrating information across multiple statements.",
            "counter_examples_or_negative_results": "REALM performs well on LM next-token prediction but fails on multi-statement QA even when gold statements are retrieved, because the reader extracts single spans rather than composing facts.",
            "uuid": "e3067.0",
            "source_info": {
                "paper_title": "Can Retriever-Augmented Language Models Reason? The Blame Game Between the Retriever and the Language Model",
                "publication_date_yy_mm": "2022-12"
            }
        },
        {
            "name_short": "kNN-LM",
            "name_full": "k-Nearest Neighbor Language Model (kNN-LM)",
            "brief_description": "An autoregressive decoder-only Transformer that interpolates its next-token distribution with a distribution estimated from nearest-neighbor token sequences retrieved from an external datastore.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "kNN-LM",
            "model_description": "Decoder-only Transformer augmented with a k-nearest-neighbor module: at generation time, nearest token-sequence neighbors are found by L2 similarity in representation space and their next-token statistics are interpolated with the model's own distribution (interpolation weight λ).",
            "model_size": null,
            "reasoning_methods": [
                "retrieval-augmented statistical interpolation (kNN over token sequences)"
            ],
            "reasoning_methods_description": "kNN-LM finds nearest neighbor token sequences (spans) by L2 similarity to the current context representation and forms a distribution over next tokens from those neighbors; final prediction interpolates the model's distribution with the kNN distribution (paper used λ=0.65). It does not perform explicit multi-statement logical composition.",
            "diversity_of_methods": "single/similar style (nearest-neighbor interpolation). The method leverages token-sequence similarity and frequency of next tokens — it does not implement diverse compositional reasoning strategies.",
            "reasoning_task_name": "EntailmentBank and StrategyQA (LM and QA setups as used in the paper)",
            "reasoning_task_description": "LM preference and next-token prediction tasks requiring reasoning across statements; QA token-overlap evaluation for answer generation.",
            "performance_by_method": "kNN-LM performs substantially worse than REALM, FiD, and ATLAS on LM preference tasks and shows poorer ability to prefer gold targets over alternatives; it also underperforms in QA. Increasing the number of retrieved sequences still fails to cover all gold statements (tends to retrieve multiple spans from the same statement). Exact numeric metrics are not provided in-text.",
            "comparison_of_methods": "Compared to other retrieval-augmented models, kNN-LM's nearest-neighbor interpolation does not enable composing multiple facts and achieves lower reasoning performance even when gold statements are available.",
            "key_findings": "kNN-LM's retrieval over token sequences and interpolation affects next-token distributions but does not support reasoning over combinations of statements; retrieving more sequences often retrieves repeated content from the same statement and fails to retrieve needed distinct facts.",
            "counter_examples_or_negative_results": "Even when allowed many (e.g., 100) retrieved sequences, kNN-LM does not reliably cover all gold statements and remains worse on the LM preference proxy; thus simply increasing retrieval does not overcome the lack of compositional reasoning.",
            "uuid": "e3067.1",
            "source_info": {
                "paper_title": "Can Retriever-Augmented Language Models Reason? The Blame Game Between the Retriever and the Language Model",
                "publication_date_yy_mm": "2022-12"
            }
        },
        {
            "name_short": "FiD",
            "name_full": "Fusion-in-Decoder (FiD)",
            "brief_description": "A sequence-to-sequence T5-based architecture that encodes each retrieved passage separately and lets the decoder attend over all encoded representations to generate answers.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "FiD (T5-based Fusion-in-Decoder)",
            "model_description": "Seq2seq model using T5 encoder(s) to encode query+each retrieved passage independently; the decoder attends jointly over all encodings to produce outputs. In the paper FiD is paired with DPR retriever (dense inner-product).",
            "model_size": null,
            "reasoning_methods": [
                "retrieval-augmented generative decoding (Fusion-in-Decoder)",
                "encoder-decoder attention over multiple retrieved statements"
            ],
            "reasoning_methods_description": "FiD encodes each retrieved statement separately and the decoder can attend to all encodings, enabling implicit combination of multiple passages during generation. The model is not, however, explicitly trained to perform symbolic or structured compositional reasoning over relations between statements.",
            "diversity_of_methods": "similar style within retrieval-augmented architectures but using a decoder-attention fusion mechanism; it does not explicitly implement distinct reasoning paradigms beyond aggregation via attention.",
            "reasoning_task_name": "EntailmentBank and QA tasks (LM and QA evaluations)",
            "reasoning_task_description": "Tasks require composing multiple supporting statements (EntailmentBank) and answering questions by integrating facts; evaluation via LM preference and QA token-overlap F1.",
            "performance_by_method": "FiD is among the superior models for QA (paper: 'FiD and Flan-T5 are the superior models in QA tasks'); it also performs reasonably when hypothesis-like statement is provided. Exact numeric scores are not reproduced in-text.",
            "comparison_of_methods": "FiD outperforms some retriever-augmented alternatives (REALM, ATLAS) on QA in the experiments; however, FiD still fails frequently when reasoning across retrieved statements is required and when retriever returns distracting statements.",
            "key_findings": "Decoder fusion across encodings helps FiD in QA relative to extractive readers, but FiD is not explicitly trained to perform logical composition across statements and thus still exhibits reasoning failures when the retriever is imperfect or information must be combined in nontrivial ways.",
            "counter_examples_or_negative_results": "FiD benefits when a conclusion/hypothesis statement (one-step form of the answer) is present among inputs, indicating it is better at direct matching than multi-step compositional entailment; it still struggles when needed relations are not explicit.",
            "uuid": "e3067.2",
            "source_info": {
                "paper_title": "Can Retriever-Augmented Language Models Reason? The Blame Game Between the Retriever and the Language Model",
                "publication_date_yy_mm": "2022-12"
            }
        },
        {
            "name_short": "ATLAS",
            "name_full": "ATLAS (pre-trained retrieval-augmented language model)",
            "brief_description": "A retrieval-augmented sequence-to-sequence model that jointly fine-tunes a Contriever-style retriever and a T5-like reader to better integrate retrieval with generation.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "ATLAS",
            "model_description": "Pretrained retrieval-augmented seq2seq model where retriever (Contriever) and reader are fine-tuned jointly; retrieval is based on inner-products of averaged encoder hidden states (Contriever). The reader architecture is similar to FiD (T5-style).",
            "model_size": null,
            "reasoning_methods": [
                "retrieval-augmented seq2seq reasoning with joint retriever-reader fine-tuning",
                "encoder-decoder attention fusion (FiD-like)"
            ],
            "reasoning_methods_description": "ATLAS fine-tunes retriever and reader jointly on downstream tasks to improve retrieval quality in an end-to-end fashion; the reader then attends over multiple retrieved passages to produce answers. Retrieval still uses dense similarity (inner product) but the joint training aims to improve relevance.",
            "diversity_of_methods": "similar style (retrieval + decoder fusion) with some architectural diversity because retriever and reader are co-finetuned; overall still follows similarity-based retrieval heuristics rather than heterogeneous reasoning strategies.",
            "reasoning_task_name": "EntailmentBank and QA (LM and QA tasks in the paper)",
            "reasoning_task_description": "Multi-step entailment and question answering tasks requiring composing facts; evaluated with LM preference/accuracy and QA token-overlap F1.",
            "performance_by_method": "ATLAS performs well in LM tasks (paper: 'ATLAS and REALM perform well in language modeling') but worse in QA (does not deeply reason; tends to copy basic answers). Exact numbers are not quoted in-text.",
            "comparison_of_methods": "Compared with FiD and Flan-T5, ATLAS is stronger on LM but weaker on QA reasoning. Joint retriever-reader fine-tuning improves retrieval quality relative to some retrievers, but ATLAS still fails to perform deep multi-statement reasoning.",
            "key_findings": "ATLAS's joint training improves some retrieval robustness, leading to good LM behavior, but the model still does not perform deep compositional reasoning over multiple retrieved facts and often copies simple answers.",
            "counter_examples_or_negative_results": "ATLAS fails when reasoning across statements is needed and is observed to copy basic answers rather than integrate multiple facts for entailment-style reasoning.",
            "uuid": "e3067.3",
            "source_info": {
                "paper_title": "Can Retriever-Augmented Language Models Reason? The Blame Game Between the Retriever and the Language Model",
                "publication_date_yy_mm": "2022-12"
            }
        },
        {
            "name_short": "Flan-T5",
            "name_full": "Flan-T5 (instruction-finetuned T5)",
            "brief_description": "An instruction-finetuned T5 variant shown to improve reasoning abilities when prompted with chain-of-thought style finetuning; in this paper it is evaluated as a retriever-augmented model when paired with DPR.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Flan-T5 (instruction-finetuned T5)",
            "model_description": "T5 model finetuned with instruction tuning and chain-of-thought style datasets to elicit reasoning; in experiments Flan-T5-base is paired with DPR retriever and consumes concatenated retrieved statements plus query as input to generate answers.",
            "model_size": "base (flan-t5-base checkpoint used)",
            "reasoning_methods": [
                "instruction finetuning / chain-of-thought prompting",
                "retrieval-augmented concatenation (input includes retrieved statements concatenated with query)"
            ],
            "reasoning_methods_description": "Flan-T5 has been finetuned on instruction-following and chain-of-thought reasoning datasets, which helps generation that includes intermediate reasoning steps; when paired with a retriever (DPR) it consumes retrieved statements concatenated with the query, relying on its finetuned reasoning style to combine facts.",
            "diversity_of_methods": "more diverse than purely similarity-based readers: combines instruction/chain-of-thought training with retrieval; however, when retrieval returns distracting statements Flan-T5 can be very sensitive and fails to ignore distractors.",
            "reasoning_task_name": "EntailmentBank and StrategyQA (LM / QA evaluations in the paper)",
            "reasoning_task_description": "Multi-step entailment (EntailmentBank) and implicit-strategy QA (StrategyQA converted to declarative) requiring composition of supporting facts; measured via LM preference and QA token-overlap F1.",
            "performance_by_method": "Flan-T5 is among the superior models for QA in the paper (FiD and Flan-T5 best in QA). With gold statements Flan-T5 achieves the best QA performance among studied models but absolute QA F1 remains modest (paper reports best QA performance around ~0.4 on EntailmentBank with gold statements). Flan-T5 is sensitive to distracting (irrelevant) retrieved statements, which degrades performance.",
            "comparison_of_methods": "When compared to other retrieval-augmented models Flan-T5 benefits from instruction/chain-of-thought finetuning and outperforms them on QA when gold facts are provided; however, Flan-T5's sensitivity to distracting evidence means it can degrade more than retriever-trained models when retriever introduces irrelevant statements.",
            "key_findings": "Instruction finetuning and chain-of-thought-style training improve Flan-T5's ability to reason over multiple facts when the required supporting facts are present, but Flan-T5 is also more harmed by distracting retrieved statements compared to models trained with imperfect retrievers.",
            "counter_examples_or_negative_results": "Even with gold statements Flan-T5's QA scores are limited (best ~0.4 F1), and when retrievers introduce distractors Flan-T5's performance drops substantially; sometimes Flan-T5 generates answers ignoring retrieved statements entirely.",
            "uuid": "e3067.4",
            "source_info": {
                "paper_title": "Can Retriever-Augmented Language Models Reason? The Blame Game Between the Retriever and the Language Model",
                "publication_date_yy_mm": "2022-12"
            }
        },
        {
            "name_short": "DPR",
            "name_full": "Dense Passage Retriever (DPR)",
            "brief_description": "A dense dual-encoder passage retriever that encodes queries and passages with separate encoders and retrieves passages by inner-product similarity in embedding space.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "DPR",
            "model_description": "Dual BERT-based encoders produce embeddings for queries and passages (e.g., [CLS] token); retrieval is performed by inner-product similarity to select top-k passages. In the paper DPR is used with FiD and Flan-T5 experiments.",
            "model_size": null,
            "reasoning_methods": [
                "similarity-based dense retrieval"
            ],
            "reasoning_methods_description": "DPR returns the passages most similar to the query as measured by inner-product of embeddings; it does not consider relations between candidate passages, so it often fails to return combinations of statements needed for multi-step reasoning.",
            "diversity_of_methods": "single/similarity-focused retrieval style; does not provide diverse retrieval heuristics that consider inter-passage relations.",
            "reasoning_task_name": "EntailmentBank-2 retrieval evaluation (and other tasks where retriever supplies statements to models)",
            "reasoning_task_description": "Retrieve supporting statements necessary for entailment-style reasoning in EntailmentBank; evaluated by retrieval F1 of ground-truth statements.",
            "performance_by_method": "DPR retrieves a gold top-1 statement only ~15% of the time on EntailmentBank-2 according to the paper; overall DPR coverage of gold statements is low especially at small k.",
            "comparison_of_methods": "Compared with Contriever and other retrievers, DPR is weaker in retrieving necessary statements for reasoning; Contriever is reported as superior among the studied retrievers but still insufficient overall.",
            "key_findings": "Similarity-based dense retrieval (DPR) often misses crucial supporting statements required for multi-step reasoning; relying solely on query–document similarity is insufficient.",
            "counter_examples_or_negative_results": "Even with increased k DPR's top retrieved items often do not include needed statements; DPR's failures harm downstream reasoning models substantially (e.g., Flan-T5 is sensitive to distractors DPR may introduce).",
            "uuid": "e3067.5",
            "source_info": {
                "paper_title": "Can Retriever-Augmented Language Models Reason? The Blame Game Between the Retriever and the Language Model",
                "publication_date_yy_mm": "2022-12"
            }
        },
        {
            "name_short": "Contriever",
            "name_full": "Contriever (unsupervised dense retriever)",
            "brief_description": "An unsupervised dense retrieval encoder trained with contrastive learning to produce passage embeddings; used as the retriever for ATLAS in the paper.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Contriever",
            "model_description": "Dual-encoder retriever that produces averaged last-layer encoder representations for passages and uses inner-product similarity for retrieval; trained with unsupervised contrastive objectives to improve general retrieval quality.",
            "model_size": null,
            "reasoning_methods": [
                "similarity-based dense retrieval (contrastive pretraining)"
            ],
            "reasoning_methods_description": "Contriever ranks passages by inner-product similarity of learned embeddings; while it improves retrieval quality relative to some alternatives, it still does not model relations among passages needed for compositional reasoning.",
            "diversity_of_methods": "single retrieval style; marginally more robust due to contrastive pretraining but not a diversity of reasoning strategies.",
            "reasoning_task_name": "EntailmentBank-2 retrieval evaluation and downstream LM/QA tasks",
            "reasoning_task_description": "Retrieve ground-truth supporting statements for multi-step entailment; used to assess retriever contribution to reasoning performance.",
            "performance_by_method": "Contriever is reported as superior among the studied retrievers (better retrieval F1 than DPR/kNN/REALM retrievers) but still fails to reliably retrieve all required statements; overall retriever F1 remains low for small k.",
            "comparison_of_methods": "While Contriever outperforms DPR and other retrievers in this study, all studied retrievers suffer from the same limitation of relying on similarity and failing to account for inter-statement relations, which limits downstream reasoning.",
            "key_findings": "Even the best retriever examined (Contriever) does not retrieve all gold statements at small k; retriever-level improvements alone are insufficient without modeling relations/composition across retrieved statements.",
            "counter_examples_or_negative_results": "Contriever improves retrieval but not enough to enable reliable multi-statement reasoning in downstream models; retriever failures remain a bottleneck.",
            "uuid": "e3067.6",
            "source_info": {
                "paper_title": "Can Retriever-Augmented Language Models Reason? The Blame Game Between the Retriever and the Language Model",
                "publication_date_yy_mm": "2022-12"
            }
        },
        {
            "name_short": "Chain-of-Thought / Instruction FT",
            "name_full": "Chain-of-Thought reasoning (instruction finetuning)",
            "brief_description": "A prompting/finetuning style that elicits intermediate step-by-step reasoning (chain-of-thought) from sequence models; used to strengthen Flan-T5's reasoning abilities via instruction finetuning.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "Chain-of-Thought / Instruction Finetuning",
            "model_description": "Training or prompting approaches that expose models to intermediate reasoning steps (chain-of-thought) and instruction-following data to improve multi-step reasoning and explanation generation.",
            "model_size": null,
            "reasoning_methods": [
                "chain-of-thought (CoT)",
                "instruction finetuning"
            ],
            "reasoning_methods_description": "Chain-of-thought encourages models to generate intermediate reasoning steps rather than only final answers; instruction finetuning exposes models to many instruction–response pairs (including CoT style examples) to improve generalization on reasoning tasks.",
            "diversity_of_methods": "diverse style (introduces stepwise reasoning behavior absent from simple retrieval-and-match pipelines). In the paper Flan-T5 (instruction finetuned) is an example of a model with this additional reasoning style.",
            "reasoning_task_name": "EntailmentBank and other reasoning benchmarks discussed in related work",
            "reasoning_task_description": "Tasks requiring step-by-step reasoning where intermediate inferences matter; CoT is meant to help models chain facts across multiple steps.",
            "performance_by_method": "Flan-T5 benefits from this style and achieves the best QA performance among models in this study when given gold statements, but absolute performance remains limited (best QA ~0.4 F1).",
            "comparison_of_methods": "Chain-of-thought/instruction finetuning helps Flan-T5 outperform other models on QA when supporting facts are present, but it is more sensitive to distracting retrieved statements than models trained with noisy retrievers.",
            "key_findings": "CoT/instruction finetuning improves ability to reason across facts, but without robust retrieval that returns the right pieces of evidence and without mechanisms to ignore distractors, CoT finetuning alone does not solve the reasoning deficits.",
            "counter_examples_or_negative_results": "Despite CoT finetuning, Flan-T5 still fails frequently on EntailmentBank and is harmed by distracting retrievals; CoT did not yield large absolute QA scores in this setup.",
            "uuid": "e3067.7",
            "source_info": {
                "paper_title": "Can Retriever-Augmented Language Models Reason? The Blame Game Between the Retriever and the Language Model",
                "publication_date_yy_mm": "2022-12"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Realm: Retrievalaugmented language model pre-training",
            "rating": 2,
            "sanitized_title": "realm_retrievalaugmented_language_model_pretraining"
        },
        {
            "paper_title": "Leveraging passage retrieval with generative models for open domain question answering",
            "rating": 2,
            "sanitized_title": "leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering"
        },
        {
            "paper_title": "Unsupervised dense information retrieval with contrastive learning",
            "rating": 2,
            "sanitized_title": "unsupervised_dense_information_retrieval_with_contrastive_learning"
        },
        {
            "paper_title": "Scaling instruction-finetuned language models",
            "rating": 2,
            "sanitized_title": "scaling_instructionfinetuned_language_models"
        },
        {
            "paper_title": "Explaining answers with entailment trees",
            "rating": 2,
            "sanitized_title": "explaining_answers_with_entailment_trees"
        },
        {
            "paper_title": "Did Aristotle Use a Laptop? A Question Answering Benchmark with Implicit Reasoning Strategies",
            "rating": 2,
            "sanitized_title": "did_aristotle_use_a_laptop_a_question_answering_benchmark_with_implicit_reasoning_strategies"
        },
        {
            "paper_title": "Faithful reasoning using large language models",
            "rating": 1,
            "sanitized_title": "faithful_reasoning_using_large_language_models"
        }
    ],
    "cost": 0.01540075,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Can Retriever-Augmented Language Models Reason? The Blame Game Between the Retriever and the Language Model</p>
<p>Parishad Behnamghader 
McGill University / Mila -Quebec AI</p>
<p>Santiago Miret 
Intel Labs</p>
<p>Siva Reddy siva.reddy@mila.quebecsantiago.miret@intel.com 
McGill University / Mila -Quebec AI</p>
<p>Can Retriever-Augmented Language Models Reason? The Blame Game Between the Retriever and the Language Model</p>
<p>The emergence of large pretrained models has enabled language models to achieve superior performance in common NLP tasks, including language modeling and question answering, compared to previous static word representation methods. Augmenting these models with a retriever to retrieve the related text and documents as supporting information has shown promise in effectively solving NLP problems in a more interpretable way given that the additional knowledge is injected explicitly rather than being captured in the models' parameters. In spite of the recent progress, our analysis on retriever-augmented language models shows that this class of language models still lack reasoning over the retrieved documents. In this paper, we study the strengths and weaknesses of different retriever-augmented language models such as REALM, kNN-LM, FiD, ATLAS, and Flan-T5 in reasoning over the selected documents in different tasks. In particular, we analyze the reasoning failures of each of these models and study how the models' failures in reasoning are rooted in the retriever module as well as the language model. 1</p>
<p>Introduction</p>
<p>Pretrained language models, such as decoderonly transformers, BERT, and T5 are being used in a wide range of tasks with outstanding results (Vaswani et al., 2017;Devlin et al., 2019;Raffel et al., 2020). Additionally, many methods have been proposed to improve language models, including augmenting the models with knowledge retrievers (Guu et al., 2020;Izacard and Grave, 2021;Izacard et al., 2022b) or memory components (Zhong et al., 2022;Khandelwal et al., 2020;Verga et al., 2021). The primary goal of using a latent knowledge retriever is to allow the model to capture information from external knowledge Phobos is one of two large objects that orbit the planet mars. Because Phobos orbits mars, Phobos should be classified as which type of body?</p>
<p>Mars is a kind of planet. Phobos orbits mars. Moons orbit planets.</p>
<p>FiD moon kNN-LM orbits mars.</p>
<p>Flan-T5 a moon ATLAS Moons orbit planets.</p>
<p>REALM Phobos</p>
<p>In New York State, the shortest period of daylight occurs during which month?</p>
<p>December is during the winter in the northern hemisphere. New york state is a state located in the united states of america.</p>
<p>Winter has the least sunlight. United states is located in the northern hemisphere.</p>
<p>FiD winter kNN-LM is during the winter in the northern hemisphere.</p>
<p>Flan-T5 january ATLAS Winter REALM December Figure 1: Example of language model failures in question answering tasks when reasoning is needed. Here, the correct answer is in bold with the majority of retriever-augmented language models failing to answer correctly.</p>
<p>rather than relying only on the implicit knowledge hidden in the model's parameters. In other words, models perform inference using the provided extra knowledge from the retrieved documents without having to retrain larger models and thereby circumvent the limitations of memorized knowledge based on the size of the model. Although retriever-augmented models perform well on knowledge-intensive NLP problems, such as question answering, theoretically they can also perform better than parametric, non-retrieval based models when it comes to reasoning given that they have access to some explicit tangible knowledge. Reasoning in language modeling and question answering means to complete a [masked or incomplete] sentence query or answer a question correctly by reasoning over the retrieved statements. As shown in Figure 1, models sometimes fail in solving the task when some reasoning is needed over the given statements.</p>
<p>The reasoning abilities of large language models have been well studied in the literature, but</p>
<p>The sun is the star at the center of the Solar System Retriever</p>
<p>Language Model moon</p>
<p>Phobos is a kind of [MASK].</p>
<p>retrieved statements</p>
<p>Mars is a kind of planet.</p>
<p>Phobos orbits mars.</p>
<p>Moons orbit planets.</p>
<p>Triton is the largest moon of neptune.</p>
<p>set of statements query query Phobos should be classified as which type of body? Figure 2: A visualization of the architecture of retriever-augmented language models. The model is given a set of statements S for each query. The retriever retrieves some of the statements as related and necessary to solve the task and the language model predicts the answer using the query and the retrieved documents.</p>
<p>many of them target large language models in arithmetic, commonsense, and symbolic reasoning problems (Wei et al., 2022;Zelikman et al., 2022), while others focus on targeted models for reasoning over facts and rules in order to check whether a hypothesis can be entailed (Dalvi et al., 2021;Tafjord et al., 2021;Bostrom et al., 2022;Creswell and Shanahan, 2022). Reasoning over supporting information with retriever-augmented language models, however, has not been extensively studied.</p>
<p>Intuitively, one might assume that retrieveraugmented pretrained language models should also possess the ability to reason in a scenario where sentence completion or question answering may require a few reasoning steps over the retrieved supporting statements. The results presented in this work, however, suggest that retriever-based language models perform poorly in the aforementioned settings and that the shortcomings of retriever-augmented language models in reasoning are rooted in two distinct parts of their design. On one hand, the knowledge retriever is not trained to retrieve the required statements for completing the task when reasoning. In most cases, the retriever model selects the most similar documents based on a similarity metric between each candidate statement and the input query (Guu et al., 2020;Karpukhin et al., 2020;Khandelwal et al., 2020;Izacard et al., 2022a). However, in the case of reasoning, it would likely be more fruitful to consider other heuristics, such as the relations between the documents themselves and the input query, thereby enabling reasoning over the combination of the retrieved statements leading to informative pieces of knowledge.</p>
<p>On the other hand, current language model architectures are not intentionally designed to enhance reasoning performance. As a general rule, language models integrate different retrieved statements by combining the statements, their encoded representations, or interpolating between the language model and the retrieved statements distributions (Guu et al., 2020;Izacard and Grave, 2021;Khandelwal et al., 2020;Izacard et al., 2022b). These models show promising performance when a similar form of the query's answer exists among the retrieved statements, but perform worse when some reasoning is required over them.</p>
<p>In this study, we demonstrate how retrieveraugmented language models fail in reasoning from different perspectives by evaluating them in language modeling and question answering tasks using different variations of EntailmentBank (Dalvi et al., 2021) and StrategyQA (Geva et al., 2021) datasets. In order to perform well in the tasks of these datasets, the models need to: 1) retrieve the best statements as supporting evidence leading to a promising result; 2) aggregate knowledge from retrieved statements by reasoning for a correct answer. Concretely, we analyze the performance of pretrained REALM, FiD, ATLAS, and k-NN-LM as retriever-based language models and the instruction-finetuned Flan-T5 as a recent strong model in reasoning. In this paper, we address the following questions:</p>
<p>Q1 Can retriever-augmented language models generally reason? We investigate the reasoning ability of retriever-augmented language models by analyzing their overall performance in both question answering (QA) and language modeling (LM) tasks. The experimental results show that these models lack reasoning in solving many QA and LM examples.</p>
<p>Q2 How much do the retrievers contribute to the reasoning performance? We investigate the role of retrievers in models' failures by assessing their shortcomings in retrieving the required statements for reasoning. We conclude that the heuristic of retrievers selecting the statements based on their similarity with the queries is not sufficient for the reasoning nature.</p>
<p>Q3 How much do the language models contribute to the reasoning performance? We investigate the role of language models in the failures of retriever-augmented models by measuring the language models' lack of reasoning over the retrieved statements. The experimental results demonstrate that language models cannot consider the relations between the ground-truth statements, which is necessary for reasoning. Interestingly, we also observe that language models perform much better when a conclusion containing all relevant facts from those statements is given as the input.</p>
<p>Related Work</p>
<p>Augmenting language models with external corpora or memory has been well studied in literature (Guu et al., 2020;Khandelwal et al., 2020;Izacard and Grave, 2021;Izacard et al., 2022b). Additionally, researchers have recently been interested in eliciting the reasoning abilities of the large language models to perform better in the tasks where answering the questions involves multiple computational steps. (Wei et al., 2022;Chung et al., 2022). Next, we discuss some of the current research in these areas.</p>
<p>Retriever-augmented Language Models</p>
<p>Retriever-based language modeling or question answering has been well studied in the literature, and different mechanisms have been proposed for integrating language models with retrieved statements. For instance, REALM (Retrieval-Augmented Language Model) is a masked language model that is augmented with a latent knowledge retriever, which allows the model to retrieve and attend over statements (Guu et al., 2020). The knowledge retriever employs a BERT encoder to achieve a dense representation for both the query and statements. The knowledge retriever selects the most similar statements among all based on a similarity metric (e.g. dense inner product) between the transformed query's and the statements' representations. In the language modeling task, the retrieved statements are appended to the query and passed to a trained BERT-based language model to solve the task. In the question answering task, however, a trained BERT-based reader extracts the more promising span from the statements as the answer.</p>
<p>kNN-LM is another proposed model based on a decoder-only Transformer, integrated with a knearest neighbor module (Khandelwal et al., 2020). In this model, the most related token sequences from the statements are selected based on an L2 similarity metric between the representation of the query and all token sequences. The distribution over the next token in generation in this autoregressive model is subsequently computed as the interpolation between the Transformer's final distribution and the distribution of the next tokens over the retrieved statements.</p>
<p>FiD (Fusion-in-Decoder) is a sequence-tosequence T5-based neural network which can work with any retriever (Izacard and Grave, 2021). Given the retrieved statements by the retriever, the encoder encodes the query and each retrieved statement separately. Afterwards, the decoder attends over the resulting representations of all the retrieved statements. In this paper, we investigate DPR (Dense Passage Retriever) as the retriever for FiD (Karpukhin et al., 2020). DPR retrieves the most similar documents based on the inner product of the representations of the query and the documents (i.e. embeddings of the [CLS] token) from two independent trained BERT encoder models.</p>
<p>Finally, ATLAS is a pre-trained retrieval augmented language model that finetunes the retriever and the language model jointly using very few training examples (Izacard et al., 2022b). The language model is a sequence-to-sequence model similar to the FiD's architecture, and the retriever is based on Contriever, consisting of a dual BERT-based encoder architecture (Izacard et al., 2022a). The retrieved documents are selected based on the inner product of the representations of the query and the documents, which are computed as the average hidden representations of the encoder's last layer.</p>
<p>Reasoning of Language Models</p>
<p>Eliciting the reasoning ability of language models has recently attracted the attention of many NLP researchers (Wei et al., 2022;Zelikman et al., 2022;Chung et al., 2022). For instance, Wei et al., 2022 investigates how reasoning abilities emerge in large language models when they are prompted with a few intermediate reasoning steps known as chain of thoughts.</p>
<p>Moreoever, Flan-T5 is an instruction-finetuned T5 model which is shown to have strong reasoning abilities, outperforming the T5 model (Chung et al., 2022;Raffel et al., 2020). Although this finetuned model was not initially constructed for retrieverbased language modeling, it can be coupled with DPR to complete the language modeling and question answering task using the retrieved statements.</p>
<p>In this paper, we study the reasoning ability of REALM, kNN-LM, FiD with DPR, and ATLAS with Contriever as retriever-based language models and Flan-T5 as a reasoning language model coupled with DPR as a retriever. While retrievers generally select statements from a huge common corpus in the literature, as illustrated in Figure 2, we accompany each query with a data-specific collection of statements since we want to have more control over the statements and the reasoning abilities of the models. Although many recent papers focus on commonsense or arithmetic reasoning, we evaluate the models on reasoning via entailment and logical reasoning (Wei et al., 2022;Lewkowycz et al., 2022;Mishra et al., 2022).</p>
<p>Problem Definition</p>
<p>In our retriever-augmented language model reasoning setting, we provide the model with a complete set of statements S = {s 1 , s 2 , . . . , s m } for each sample. In some cases, only some of these statements are necessary to predict the answer with the others contain distracting information. For a fixed number of retrieved statements k, the model should retrieve the set of statements S r = {r 1 , r 2 , . . . , r k } ⊆ S which finds more related and necessary and solve the target task by reasoning over them. A visualization of the general task is represented in Figure 2. We study REALM, kNN-LM, FiD, ATLAS, and Flan-T5 in this paper. Based on the implementation details stated in Appendix A, most of these models have almost the same size as presented in Table 1, and hence, the model size does not play an important role in the results.  Language Modeling (LM). In the language modeling task setup, we measure the performance of the retriever-augmented language models with two metrics: 1) Predicting the desired token correctly; 2) Assigning a higher likelihood to the gold sentence over a similar but incorrect sentence. We use the latter task as a proxy to compare between different autoregressive or masked language models. To this end, we consider two alternative sentences by defining an alternative target for the true target entity mention and compare the score of the model corresponding to each sentence by attending to the retrieved statements by the retriever. A perfect reasoning model should allocate a higher score to the sentence including the true target 100% of the times. An example of this task is illustrated in Table 3. The score of a target entity mention T = t 1 t 2 . . . t M for input query Q = q 1 q 2 . . . q N in each model is computed as follows:
• REALM: 1 M M i=1 log p ([MASK] i = t i |Q, S r ) • kNN-LM: 1 N log p (Q T |S r ), where Q T is the query Q with [MASK] tokens substituted with target tokens T .
• FiD, ATLAS, and Flan-T5: −loss(T ), where loss is the output loss of the models corresponding to target tokens T .</p>
<p>More detailed information about query formats for each model is described in Appendix B.</p>
<p>Question Answering (QA). In the question answering task's setup, we study the token overlapping F1 score of the predicted answer of the models on the reasoning question answering datasets.</p>
<p>Exploring Weaknesses in Reasoning</p>
<p>The shortcomings of retriever-augmented language models are embedded in different parts of their architectures. In this section, we explain the weaknesses of these models caused by the retriever as well as the accompanying language model.</p>
<p>Retriever Shortcomings</p>
<p>The purpose of the retriever is to select k statements as the "retrieved statements" using a relevance score between the query and each statement as explained in detail in Section 2.1. These similarity metrics could be the inner product or the L2 distance between query and statements (or a span in statements) representations derived by one or two different encoders. This naive way of selecting the related statements does not take into account the relation between the statements. Particularly, there may be some data samples and their corresponding sets of statements in which the relation between statements themselves can lead to meaningful knowledge more related to the query compared to each candidate statement itself. Moreover, similarity is not a good representative of carrying a lot of information that can be used for reasoning over statements to solve the language modeling or question answering task.</p>
<p>Language Model Shortcomings</p>
<p>In this subsection, we discuss why current retrieveraugmented language models do not consider the complicated relations between statements resulting in poor reasoning ability. Suppose we have a perfect retriever that can retrieve necessary and sufficient statements for solving the target task, which we call the "gold statements". As described in greater detail below, the language models we study in this paper cannot solve the task perfectly regarding reasoning over the retrieved gold statements S r due to inherent design weaknesses.</p>
<p>REALM. In the language modeling task, one way to predict a token in REALM is to predict the token using each statement in S r separately, and then perform majority voting to select the final output as the predicted token. This approach does not take reasoning into account at all given that the relation between statements is a key factor for the model to be able to reason effectively, while here, the statements of S r are disconnected from each other. Another perspective in using REALM in masked language modeling is to pass all the retrieved statements S r to the language model at once. In this case, as opposed to the previous option, the connections between statements can be considered. However, reasoning over sentences stacked together is not currently considered in the Transformer-based nature of BERT. Similarly, in question answering, REALM does not consider the complicated relations between statements. The model outputs the most probable span among the retrieved statements without combining information from multiple statements.</p>
<p>kNN-LM. According to Section 2.1, given the gold statements S r , the language model predictions will change based on the next tokens of similar token sequences in S r . Once again the model does not consider the relations between the statements and instead, takes the frequency (i.e. probability) of the next tokens of the nearest sequence candidates into account.</p>
<p>FiD and ATLAS. In FiD and ATLAS, the decoder takes all the encoded representations of the retrieved statements as input. However, while the model performs well in simple frameworks, it is not trained to reason over the representations.</p>
<p>Flan-T5. Flan-T5, similar to REALM when concatenating all the retrieved external textual information, generates an answer using all the retrieved facts as well as the query as the inputs. Although this model is finetuned on the chain of thought reasoning datasets, still it cannot perfectly solve the tasks by reasoning, especially when the retriever is not perfect and retrieves some wrong statements.</p>
<p>Evaluation and Results</p>
<p>In this section, we first introduce the datasets and metrics used in the experiments for both language modeling and question answering tasks as explained in Section 2.1. Afterwards, we demonstrate the weakness of the retriever-augmented language models in detail.</p>
<p>Datasets</p>
<p>We compare the reasoning ability of the models based on their performance on the following datasets in different formats with detailed dataset preparation mechanism explained in Appendix C and results shown in Figure 3 and Figure 4.</p>
<p>• EntailmentBank (EB, Dalvi et al., 2021) contains a multi-step entailment tree for a given hypothesis. This dataset consists of three parts, each with a specific characteristic. In EB-1, only the required statements are provided in data samples. In EB-2, 25 statements have been given for each data samples, including relevant and irrelevant ones. In EB-3, similar to EB-2, 25 relevant and irrelevant statements have been given by sampling from a large corpus with no ground-truth entailment tree. Each data sample in this dataset consists of some statements, a question, an answer, and a hypothesis rephrasing the question and answer in a declarative form.</p>
<p>• StrategyQA (Geva et al., 2021) contains yes/no question answering samples, where the required reasoning steps are implicit in the question, as well as supporting and further relevant evidence from Wikipedia. For evaluating the models in language modeling, we convert each question to a declarative format.</p>
<p>Evaluation Metrics</p>
<p>We evaluate the performance of the different models by measuring the model preference accuracy as well as the accuracy in predicting the first target token and present the result in Figure 3 and Figure 8. Note that in kNN-LM, a decoder-only model, the model does not see the input tokens appearing after the masked tokens.</p>
<p>In question answering, we take the token overlapping F1 score of the generated answer with the results shown in Figure 4.</p>
<p>When analyzing the retrievers' performance, we take the ground-truth statement retrieval F1 score to see how well retrievers find the relevant statements.</p>
<p>Can Retriever-Augmented Language Models Generally Reason? (Q1)</p>
<p>In this section, we show the weaknesses of retrieveraugmented language models in reasoning from different perspectives. We present the overall behavior of the models in model preference on the development sets in Figure 3 and Figure 4. Results show that although ATLAS and REALM perform well in language modeling, they perform badly in question answering. One reason is that REALM is an extractive QA model, but another more important reason is that REALM tends to extract the answer from the first few retrieved statements, leading to inferior performance. This phenomenon is illustrated in Figure 1 and Table 3, and can also be seen in Figure 4, where REALM's performance stays the same after retrieving about 3 statements. The experiments on ATLAS show that this model does not deeply reason over the statements, while generally copying only basic answers. This phenomenon is illustrated in the qualitative examples in Appendix E.</p>
<p>Additionally, Flan-T5, an originally nonretriever-based model, shows much sensitivity to distracting statements in both EntailmentBank-  Table 2: Experimental results of the best retriever-augmented models in LM and QA on test sets. The two best models are highlighted in green. ATLAS is coupled with Contriever, and FiD and Flan are augmented with DPR as retriever, respectively. The results show that ATLAS and REALM are superior in LM, and FiD and Flan-T5 are the superior models in QA tasks.</p>
<p>2 and EntailmentBank-3 datasets after providing more and more distracting statements. On the contrary, other models seem more robust in both language modeling and question answering experiments, as represented in Figure 3 and Figure 4. Our hypothesis is that the supporting evidence used in Flan-T5's finetuning procedure has been necessary for solving the tasks, and hence, Flan-T5 has not been finetuned in a way to learn to ignore related but distracting information. On the other hand, the other models have been trained with retrievers, which are imperfect, retrieving incorrect statements in some cases. As a result, they are more robust to the distracting information.</p>
<p>It is worth mentioning that the models have almost the same number of parameters as shown in Table 1, and hence, the model size does not have a meaningful impact on the models' performance. We show some of the models' failures in LM or QA experiments in Table 3. These examples demonstrate the faulty module in each case as described in Section 4. The more comprehensive qualitative results are available in Appendix E. Table 2 demonstrates the performance of the best retriever-augmented models (based on the performance on the dev sets) on the test sets in both language modeling and question answering tasks. It can be observed that the same discussion about the behavior of the models holds on the test set.</p>
<p>Note that all the data samples have a set of at most 25 statements, however, kNN-LM's retriever does not retrieve one statement at a time. Instead, it might retrieve different sequences of tokens from a specific statement as the relevant sequences of tokens. This is why the performance of all other three models stays the same after k = 25, but this is not the case in kNN-LM.</p>
<p>How Much Do The Retrievers Contribute</p>
<p>To The Performance? (Q2)</p>
<p>In this subsection, we study the efficacy of the retrievers in reasoning by evaluating the models on the EntailmentBank-2 dataset that includes both gold and distracting statements. Figure 5 demonstrates that although Contriever is superior among the studied retrievers, all four REALM, kNN-LM, DPR, and Contriever retrievers lack in retrieving the most relevant and necessary statements for rea- </p>
<p>kNN-LM</p>
<p>The robot will weigh less on mars than earth but will have the same [MASK]. mass vs mars -As the force of gravity decreases, the weight of the object will decrease. -The gravitational force of a planet does not change the mass of an object on that planet or celestial body. ...  Table 3: Some examples of models' failures rooted in the retriever or language model modules. The correctly retrieved statement and the one that had to be retrieved in order for the model to solve the task correctly are highlighted in green and red, respectively. The true answer (or sequence of tokens leading to the true answer) for each data sample's statements is marked in bold.  soning. For instance, The top retrieved statement by DPR is among the gold statements only 15% of the times. Note that in the language modeling setting, REALM, DPR and Contriever have access to all the tokens of the query, while kNN-LM is autoregressive and only sees the tokens before masking tokens. Another observation is that even with letting kNN-LM retrieve 100 sequences, it does not cover all the gold statements and tends to retrieve sequences from the same statement. Some failures of the retrievers are demonstrated in Table 3. These examples show the weaknesses of retrievers in selecting necessary statements for reasoning. Although these missed statements do not seem similar to the query, they carry important information required for reasoning. Overall, retrievers perform poorly in selecting relevant statements required to perform well in reasoning tasks given that they do not account for the relationships between statements.</p>
<p>mars</p>
<p>How Much Do The Language Models contribute To The Performance? (Q3)</p>
<p>This subsection pictures the shortcomings of the language models by providing the language models exclusively with gold statements. Figure 6 demonstrates the performance of the language models given all or ground-truth, gold statements. According to the solid lines in this figure, we conclude that even with the ground-truth statements, language models do not perform perfectly, at least in question answering, even with having all the required statements in hand. For instance, the best performance in question answering dataset is around 0.4 for Flan-T5 which is instruction finetuned on the chain of thought data. Furthermore in language modeling, results show that kNN-LM, which is trained for language modeling, is performing substantially worse than REALM, FiD, and ATLAS, and its performance does not differ much even with only the ground-truth statements. Experimental results in Figure 7 show how the language models perform much better when no reasoning is needed for completing the task. In this visualization, the solid lines refer to the performance of retriever-augmented language models given ground-truth statements and the dotted lines refer to the performance of the model when only one statement is given with no need to reason. This given statement is in fact the hypothesis sentence in EntailmentBank-2 data samples which can be inferred from the gold statements using reasoning and is sufficient to answer the question. From this figure, we can infer that language models perform better when reasoning is not required and a similar form of the answer is mentioned in the statements. Additionally, we observe that Flan-T5 performs the best in question answering with reasoning over ground-truth statements, while FiD performs much The dotted lines and the solid lines refer to experiments given single statements and gold statements (when reasoning is required), respectively. Results illustrate that the language models are not good at answering questions by reasoning. more promising when the hypothesis is given as a statement.</p>
<p>Some failures of the language models are demonstrated in Table 3. These examples show that even when the query's answer exists in the retrieved statements, the models sometimes have difficulty finding the correct answer. For instance, results show that the QA version of REALM tends to select the answer span from the first or second retrieved statements which can also be observed in Figure 4 and Figure 7. Furthermore, we sometimes observe that Flan-T5 answers the queries regardless of the provided retrieved statements, as demonstrated in Table 3.</p>
<p>Conclusion</p>
<p>In this paper, we analyzed to what extent the retriever-augmented language models, including REALM, kNN-LM, ATLAS coupled with Contriever, and FiD and Flan-T5 coupled with DPR as retriever are capable of solving downstream tasks where reasoning is required. Our analysis is rooted in investigating three leading questions outlined in Section 1 that characterize reasoning ability in retriever-based language models. With respect to Q1, the results show that these models have difficulties in completing both language modeling tasks and question answering tasks. Regarding Q2, experimental results demonstrate that retrievers do not retrieve the statements necessary for reasoning, and instead, select statements based on query similarity which is not often desired in reasoning settings. These incorrect retrieved statements are shown to be harmful to some of the models, especially Flan-T5. With regards to Q3, we conclude that although most of these models perform reasonably well when the answer is mentioned in one of the statements in a similar format, they generally cannot reason over different statements even when all the given statements are among the ground-truth, "gold statements". Instead, these models exhibit different kinds of behavior, such as copying a piece of information from the statement with the highest score and generating an answer regardless of the retrieved statements in question answering tasks. We also observed some deficiencies in preferring the true targets over alternative ones in language modeling tasks. Overall, our qualitative results demonstrate that language models do not take the relations between the statements into account, and therefore, cannot reason over them properly.</p>
<p>These results suggest opportunities for improving the reasoning ability of the retriever-augmented language models by first, improving the retrievers so that they select statements using a more focused heuristic compared to a simple similarity score, and second, enhancing the language models so that they combine the information from retrieved statements by considering the statements' relations more effectively.</p>
<p>B Model and Task-Specific Query Format</p>
<p>This section includes the model-specific query formats in each target task. As stated in Section 3, we aim to study the reasoning abilities of retrieveraugmented language models in language modeling and question answering tasks. A sample of what the queries to each model in language modeling looks like is presented in Table 4. These examples are all specified for next token prediction. From these examples, it can be observed that unlike REALM, FiD, ATLAS, and Flan-T5, kNN-LM cannot see the tokens appearing after the target tokens in the sentence. We resolve this problem using the model preference proxy task.</p>
<p>In the question answering setting, on the other hand, we give the whole question into the model,  and <extra_id_0> as the special masking tokens in BERT-based and T5-based models, respectively. and take the generated output as the generated answer.</p>
<p>C Dataset Preparation Details</p>
<p>In order to prepare the datasets for the language modeling experiments, we keep the data samples that include at least one entity mention and mask out the last entity mention in the hypothesis sentences of StrategyQA and different Entailment-Bank variants using Spacy (Honnibal and Montani, 2017). Also, we randomly pick another entity mentioned in data sample's statements as the alternative target (as described in Section 3) and compare the models' score for each target. Regarding the question answering experiments, we use En-tailmentBank's question and answer formats. The reason we don't use StrategyQA in the experiments is that comparing FiD, ATLAS, and Flan-T5 with REALM as an extractive QA model and kNN-LM as a memory-augmented model on a yes/no question answering dataset like StrategyQA is not fair. We evaluate the retriever-augmented language models based on the number of retrieved statements on the development set of the datasets, and in each model, we report the result of the best settings on test sets. Regarding the EntailmentBank dataset, we run the experiments on the same development and test sets as the original data. However, in the StrategyQA dataset, since we don't have access to the answers in the test split, we cannot change the samples' formats to declarative form. Therefore, we pick 25% and almost 35% of the train data as the development and test sets, respectively.</p>
<p>D Quantitative Results</p>
<p>This section includes more visualizations and detailed results. Figure 8 demonstrates the performance of retriever-augmented language models on the development sets in language modeling based on next token prediction accuracy. As expected, results show that REALM is still performing reasonably well in language modeling in predicting the first masked token. By comparing Figure 3 and Figure 8, we observe that although kNN-LM takes the third place among the models in the next token prediction in EntailmentBank datasets, it can not prefer the ground-truth target over the alternative one correctly a lot of times.</p>
<p>E Qualitative Results</p>
<p>We demonstrate a failure example in each of the retrievers and language models in Table 5. In this table, the true retrieved statement and the one that had to be retrieved in order for the model to solve the task correctly are highlighted in green and red, respectively. The true answer (or sequence of tokens leading to the true answer) for each data sample's statements is marked in bold. These examples explain how not retrieving the necessary statements for reasoning or not reasoning over true statements can lead to incorrect answers. The robot will weigh less on mars than earth but will have the same [MASK]. mass vs mars -As the force of gravity decreases, the weight of the object will decrease. -The gravitational force of a planet does not change the mass of an object on that planet or celestial body. ... What allows two students standing ten feet apart to hear each other talk?</p>
<p>-Talking is when a human produces sound to communicate.</p>
<p>-Sound can travel through air by vibrating air. ...</p>
<p>a microphone</p>
<p>FiD +DPR Which energy conversion happens when a person shivers and the energy is transferred to make the muscles and joints move?</p>
<p>-A person is a kind of animal.</p>
<p>-When an animal moves, chemical energy is converted to mechanical energy.</p>
<p>-Shivering is a kind of shaking.</p>
<p>-Shaking is a kind of moving. ...</p>
<p>shaking</p>
<p>ATLAS + Contriever</p>
<p>Wave energy from the ocean can be harnessed to power generators to make electricity. Energy from ocean tides can also be used to make electricity. How would you categorize these two sources of energy? -Tidal energy means energy from ocean tides. -Tidal energy is a renewable resource. -Wave energy is a renewable resource.</p>
<p>Wave energy</p>
<p>Which changes will most likely have a negative effect on an ecosystem? -Humans changing ecosystems usually has a negative impact on an ecosystem / organisms living in an ecosystem.</p>
<p>-Humans building roads in an ecosystem causes that ecosystem to change. ...</p>
<p>Humans changing ecosystems</p>
<p>kNN-LM</p>
<p>The mass of earth causes the pull of gravity on [MASK]. earth vs newton -The mass of a planet causes the pull of gravity on that planet.  </p>
<p>Figure 3 :
3A visualization of the model preference accuracy of the retrieval-augmented models in language modeling on development sets based on the number of retrieved statements. It can be observed that ATLAS, REALM, and FiD perform reasonably well in the reasoning datasets while kNN-LM and Flan-T5 perform poorly.</p>
<p>Figure 4 :
4A visualization of the token overlap F1 score of the retrieval-augmented models in question answering on development sets based on the number of retrieved statements. These results show that the entire set of studied models perform poorly at reasoning when answering questions.</p>
<p>Figure 5 :
5Retrieved statements F1 score of retrievers on EntailmentBank-2 test set in both LM and QA based on the number of retrieved statements (k). Results demonstrate that retrievers do not select required statements properly especially in experiments with small k.</p>
<p>Figure 6 :
6Performance of language models on EntailmentBank-2 test set in both LM and QA. The solid lines and the dotted lines refer to the experiments using the gold statements and all of the statements (including gold and distracting ones), respectively. It can be observed that even when all gold statements are given, some models still have difficulty in solving the tasks.</p>
<p>Figure 7 :
7Token overlap F1 score of language models on EntailmentBank-2 test set in question answering.</p>
<p>Figure 8 :
8Next token prediction accuracy of the retrieval-augmented models in language modeling on development sets based on the number of retrieved statements. The results show that models do not predict the first target token a lot of the times.</p>
<p>-
If an animal lives a certain environment then that animal usually requires that kind of environment.-Polar bears live in cold environments.</p>
<p>orbit of mercury around the sun takes [MASK]. around 88 earth days vs between 1 and 365 -A complete revolution / orbit of a planet around its star takes 1 / one planetary year. -One mercury year is about 88 earth days. moon occurred on june 2. -A moon phase occurs 28 days after the last time it occurred. -2 plus 28 equals 30.</p>
<p>Table 1 :
1The number of parameters in the studied lan-
guage models. Most of the language models have al-
most the same size, implying that model size does not 
play an important role in our analysis on reasoning abil-
ities. </p>
<p>In a zoo located in a warm region, what should be included in the polar bear exhibit?-If an animal lives a certain environment then that animal usually requires that kind of environment.-Polar bears live in cold environments. ...-The moon 's orbit is elliptical.-Gravity causes orbits. ...model </p>
<p>query 
statements 
answer </p>
<p>Retriever's Failures </p>
<p>FiD 
+ DPR </p>
<p>warm </p>
<p>ATLAS 
+ Contriever </p>
<p>What keeps the Moon orbit-
ing Earth? </p>
<p>elliptical </p>
<p>Table 4 :
4A sample of the query formats for each model in the language modeling task. We use[MASK]   </p>
<p>Table 5 :
5A complete set of models' failure examples rooted in the retriever or language model modules.
We release our code at https://github.com/ McGill-NLP/retriever-lm-reasoning .
A Implementation DetailsWe present the implementation details of the analyzed models in this section. All the experiments are conducted using PyTorch(Paszke et al., 2019). Note that we have changed the retriever in each model to retrieve statements from a sample-specific set of statements instead of a large common corpus.In REALM's experiments, we use the Huggingface's transformers implementation for both masked language modeling and question answering.We load the realm-cc-news-pretrained-encoder checkpoint as a knowledge encoder for masked language modeling and realm-orqa-nq-openqa checkpoint for question answering(Wolf et al., 2020). For kNN-LM experiments, we use the best checkpoint available in the original papers' github repository, and we find λ = 0.65 the best value as the interpolation hyperparameter based on the experiments on development sets. In FiD experiments, we use nq_reader_base checkpoint available in the papers' github repository, and for experimenting ATLAS, we use the trained atlas_data/models/atlas_nq/base checkpoint of both language model and retriever.Also, as the Flan-T5 model, we load the flan-t5-base model from Huggingface's transformers.Finally, we do the FiD and Flan-T5 experiments using the nq.bert-base-encoder's checkpoint of the DPR retriever.
Swarat Chaudhuri, and Greg Durrett. 2022. Natural language deduction through search over statement compositions. Kaj Bostrom, Zayne Sprague, abs/2201.06028CoRRKaj Bostrom, Zayne Sprague, Swarat Chaudhuri, and Greg Durrett. 2022. Natural language deduction through search over statement compositions. CoRR, abs/2201.06028.</p>
<p>. Hyung Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, 10.48550/arXiv.2210.11416Mostafa dehghani, Siddhartha Brahma, Albert Webson, Shixiang Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Sharan Narang, Gaurav Mishra, Adams Yu, and Jason Wei. 2022Scaling instruction-finetuned language modelsHyung Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa dehghani, Siddhartha Brahma, Al- bert Webson, Shixiang Gu, Zhuyun Dai, Mirac Suz- gun, Xinyun Chen, Aakanksha Chowdhery, Sharan Narang, Gaurav Mishra, Adams Yu, and Jason Wei. 2022. Scaling instruction-finetuned language mod- els.</p>
<p>Faithful reasoning using large language models. Antonia Creswell, Murray Shanahan, 10.48550/ARXIV.2208.14271Antonia Creswell and Murray Shanahan. 2022. Faith- ful reasoning using large language models.</p>
<p>Explaining answers with entailment trees. Bhavana Dalvi, Peter Jansen, Oyvind Tafjord, Zhengnan Xie, Hannah Smith, Leighanna Pipatanangkura, Peter Clark, 10.18653/v1/2021.emnlp-main.585Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. the 2021 Conference on Empirical Methods in Natural Language ProcessingOnline and Punta Cana, Dominican RepublicAssociation for Computational LinguisticsBhavana Dalvi, Peter Jansen, Oyvind Tafjord, Zheng- nan Xie, Hannah Smith, Leighanna Pipatanangkura, and Peter Clark. 2021. Explaining answers with en- tailment trees. In Proceedings of the 2021 Confer- ence on Empirical Methods in Natural Language Processing, pages 7358-7370, Online and Punta Cana, Dominican Republic. Association for Compu- tational Linguistics.</p>
<p>BERT: Pre-training of deep bidirectional transformers for language understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, 10.18653/v1/N19-1423Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMinneapolis, MinnesotaLong and Short Papers1Association for Computational LinguisticsJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language under- standing. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171-4186, Minneapolis, Minnesota. Associ- ation for Computational Linguistics.</p>
<p>Did Aristotle Use a Laptop? A Question Answering Benchmark with Implicit Reasoning Strategies. Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, Dan Roth, Jonathan Berant, 10.1162/tacl_a_00370Transactions of the Association for Computational Linguistics. 9Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, Dan Roth, and Jonathan Berant. 2021. Did Aristo- tle Use a Laptop? A Question Answering Bench- mark with Implicit Reasoning Strategies. Transac- tions of the Association for Computational Linguis- tics, 9:346-361.</p>
<p>Realm: Retrievalaugmented language model pre-training. Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, Ming-Wei Chang, Proceedings of the 37th International Conference on Machine Learning. the 37th International Conference on Machine LearningICML'20. JMLR.orgKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasu- pat, and Ming-Wei Chang. 2020. Realm: Retrieval- augmented language model pre-training. In Pro- ceedings of the 37th International Conference on Machine Learning, ICML'20. JMLR.org.</p>
<ol>
<li>spaCy 2: Natural language understanding with Bloom embeddings, convolutional neural networks and incremental parsing. Matthew Honnibal, Ines Montani, Matthew Honnibal and Ines Montani. 2017. spaCy 2: Natural language understanding with Bloom embed- dings, convolutional neural networks and incremen- tal parsing.</li>
</ol>
<p>Unsupervised dense information retrieval with contrastive learning. Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Riedel, Piotr Bojanowski, Armand Joulin, Edouard Grave, Transactions on Machine Learning Research. Gautier Izacard, Mathilde Caron, Lucas Hosseini, Se- bastian Riedel, Piotr Bojanowski, Armand Joulin, and Edouard Grave. 2022a. Unsupervised dense in- formation retrieval with contrastive learning. Trans- actions on Machine Learning Research.</p>
<p>Leveraging passage retrieval with generative models for open domain question answering. Gautier Izacard, Edouard Grave, 10.18653/v1/2021.eacl-main.74Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume. the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main VolumeGautier Izacard and Edouard Grave. 2021. Leveraging passage retrieval with generative models for open domain question answering. In Proceedings of the 16th Conference of the European Chapter of the As- sociation for Computational Linguistics: Main Vol- ume, pages 874-880, Online. Association for Com- putational Linguistics.</p>            </div>
        </div>

    </div>
</body>
</html>
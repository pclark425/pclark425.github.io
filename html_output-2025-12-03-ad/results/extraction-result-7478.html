<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7478 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7478</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7478</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-136.html">extraction-schema-136</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <p><strong>Paper ID:</strong> paper-3d85460139dd4a49e3d601daebfce86c50577bca</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/3d85460139dd4a49e3d601daebfce86c50577bca" target="_blank">Cognitive Effects in Large Language Models</a></p>
                <p><strong>Paper Venue:</strong> European Conference on Artificial Intelligence</p>
                <p><strong>Paper TL;DR:</strong> It is found that LLMs are indeed prone to several human cognitive effects, and it is shown that the priming, distance, SNARC, and size congruity effects were presented with GPT-3, while the anchoring effect is absent.</p>
                <p><strong>Paper Abstract:</strong> Large Language Models (LLMs) such as ChatGPT have received enormous attention over the past year and are now used by hundreds of millions of people every day. The rapid adoption of this technology naturally raises questions about the possible biases such models might exhibit. In this work, we tested one of these models (GPT-3) on a range of cognitive effects, which are systematic patterns that are usually found in human cognitive tasks. We found that LLMs are indeed prone to several human cognitive effects. Specifically, we show that the priming, distance, SNARC, and size congruity effects were presented with GPT-3, while the anchoring effect is absent. We describe our methodology, and specifically the way we converted real-world experiments to text-based experiments. Finally, we speculate on the possible reasons why GPT-3 exhibits these effects and discuss whether they are imitated or reinvented.</p>
                <p><strong>Cost:</strong> 0.01</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7478.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7478.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3 Priming</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generative Pre-trained Transformer 3 — priming effect evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-3 was evaluated on classic semantic/behavioral priming paradigms converted to text prompts; the paper reports that GPT-3 exhibited priming effects qualitatively.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Autoregressive transformer-based large language model developed by OpenAI (decoder-only Transformer).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>175B</td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>Priming effect (semantic priming)</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>Memory / Semantic priming</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Measures facilitation (usually faster reaction times or higher accuracy) for a target when preceded by a semantically related prime versus an unrelated prime; here classical behavioral priming experiments were converted into text-based prompts for the LLM.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Qualitative presence reported: GPT-3 exhibited priming effects in the text-based experiments (no numeric metric reported in the provided text).</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Text-based conversion of real-world cognitive psychology experiments into prompts; exact prompt templates not specified in the provided text.</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>The paper's abstract states the priming effect was present for GPT-3, but the provided document fragment does not include numeric metrics, human baseline values, or detailed prompt/analysis descriptions. No statistical comparisons or p-values are included in the visible text.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Cognitive Effects in Large Language Models', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7478.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7478.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3 Distance Effect</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generative Pre-trained Transformer 3 — numerical distance effect evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-3 was tested on tasks assessing the numerical distance effect (how performance varies with numerical distance between quantities); the paper reports GPT-3 shows the effect qualitatively.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Autoregressive transformer-based large language model developed by OpenAI (decoder-only Transformer).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>175B</td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>Numerical distance effect</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>Numerical cognition / magnitude comparison</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Classical effect where comparisons between numbers are faster and/or more accurate when the numerical distance is larger (e.g., comparing 2 vs 9 is easier than 7 vs 8); administered here by converting numeric-comparison experiments into text prompts for the model.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Qualitative presence reported: GPT-3 exhibited a distance effect in the authors' text-based experiments (no numeric metric reported in the provided text).</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Text-based conversion of numeric-comparison experiments; exact prompt details not provided in the available excerpt.</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>The provided abstract indicates the distance effect appeared with GPT-3, but no numeric results or human baselines are present in the supplied document fragment.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Cognitive Effects in Large Language Models', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7478.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7478.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3 SNARC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generative Pre-trained Transformer 3 — SNARC effect evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-3 was evaluated for the SNARC effect (spatial–numerical associations) using text-adapted versions of standard tasks; the paper reports the SNARC effect appeared in GPT-3.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Autoregressive transformer-based large language model developed by OpenAI (decoder-only Transformer).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>175B</td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>SNARC effect (spatial–numerical association of response codes)</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>Numerical cognition / spatial-numerical association</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>The SNARC effect describes a tendency for smaller numbers to be associated with left space and larger numbers with right space, typically measured via response mappings or reaction times; here adapted to text-based queries for the LLM.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Qualitative presence reported: GPT-3 exhibited the SNARC effect in the converted text experiments (no numeric metric reported in the provided text).</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Text-based conversion of spatial-numerical tasks; specific prompt format not reported in the provided fragment.</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>The abstract explicitly lists SNARC as present for GPT-3; the excerpt does not include quantitative comparisons to human data or statistical tests.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Cognitive Effects in Large Language Models', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7478.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7478.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3 Size Congruity</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generative Pre-trained Transformer 3 — size congruity effect evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper evaluated GPT-3 on size congruity paradigms (magnitude vs physical size interference) after converting tasks to text; GPT-3 showed the effect according to the abstract.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Autoregressive transformer-based large language model developed by OpenAI (decoder-only Transformer).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>175B</td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>Size congruity effect</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>Perceptual interference / magnitude comparison</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Size congruity paradigms test interference between numerical magnitude and physical size (e.g., faster responses when numerical and physical sizes are congruent); adapted here into text prompts for model evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Qualitative presence reported: GPT-3 exhibited the size congruity effect in the authors' text-based experiments (no numeric metric reported in the provided text).</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Text-based conversion of size-congruity tasks; exact prompt details are not present in the provided document fragment.</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>Abstract reports presence of the effect; the supplied HTML contains no numeric LLM scores or human baseline statistics.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Cognitive Effects in Large Language Models', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7478.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e7478.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3 Anchoring</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generative Pre-trained Transformer 3 — anchoring effect evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-3 was tested for the anchoring effect (bias in numerical/judgment estimates due to arbitrary anchors) and the paper reports that anchoring was absent for GPT-3 in their experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Autoregressive transformer-based large language model developed by OpenAI (decoder-only Transformer).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>175B</td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>Anchoring effect</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>Judgment and decision-making / cognitive bias</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Anchoring refers to the influence of an initial numeric anchor on subsequent estimates or judgments; experiments typically compare estimates following high vs low anchors and measure mean differences.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Qualitative absence reported: the paper's abstract states the anchoring effect was absent for GPT-3 in their text-based experiments (no numeric metric reported in the provided text).</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Text-based conversion of anchoring tasks into prompts; specific prompt wording and conditions are not provided in the available excerpt.</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>The abstract explicitly notes anchoring was absent, contrasting with the other effects; again, no numeric comparisons or formal statistical tests are present in the provided fragment of the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Cognitive Effects in Large Language Models', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Time required for judgments of numerical inequality <em>(Rating: 2)</em></li>
                <li>The mental representation of parity and number <em>(Rating: 2)</em></li>
                <li>Judgment under Uncertainty: Heuristics and Biases <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7478",
    "paper_id": "paper-3d85460139dd4a49e3d601daebfce86c50577bca",
    "extraction_schema_id": "extraction-schema-136",
    "extracted_data": [
        {
            "name_short": "GPT-3 Priming",
            "name_full": "Generative Pre-trained Transformer 3 — priming effect evaluation",
            "brief_description": "GPT-3 was evaluated on classic semantic/behavioral priming paradigms converted to text prompts; the paper reports that GPT-3 exhibited priming effects qualitatively.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3",
            "model_description": "Autoregressive transformer-based large language model developed by OpenAI (decoder-only Transformer).",
            "model_size": "175B",
            "test_name": "Priming effect (semantic priming)",
            "test_category": "Memory / Semantic priming",
            "test_description": "Measures facilitation (usually faster reaction times or higher accuracy) for a target when preceded by a semantically related prime versus an unrelated prime; here classical behavioral priming experiments were converted into text-based prompts for the LLM.",
            "evaluation_metric": null,
            "human_performance": null,
            "llm_performance": "Qualitative presence reported: GPT-3 exhibited priming effects in the text-based experiments (no numeric metric reported in the provided text).",
            "prompting_method": "Text-based conversion of real-world cognitive psychology experiments into prompts; exact prompt templates not specified in the provided text.",
            "fine_tuned": false,
            "human_data_source": null,
            "statistical_significance": null,
            "notes": "The paper's abstract states the priming effect was present for GPT-3, but the provided document fragment does not include numeric metrics, human baseline values, or detailed prompt/analysis descriptions. No statistical comparisons or p-values are included in the visible text.",
            "uuid": "e7478.0",
            "source_info": {
                "paper_title": "Cognitive Effects in Large Language Models",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "GPT-3 Distance Effect",
            "name_full": "Generative Pre-trained Transformer 3 — numerical distance effect evaluation",
            "brief_description": "GPT-3 was tested on tasks assessing the numerical distance effect (how performance varies with numerical distance between quantities); the paper reports GPT-3 shows the effect qualitatively.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3",
            "model_description": "Autoregressive transformer-based large language model developed by OpenAI (decoder-only Transformer).",
            "model_size": "175B",
            "test_name": "Numerical distance effect",
            "test_category": "Numerical cognition / magnitude comparison",
            "test_description": "Classical effect where comparisons between numbers are faster and/or more accurate when the numerical distance is larger (e.g., comparing 2 vs 9 is easier than 7 vs 8); administered here by converting numeric-comparison experiments into text prompts for the model.",
            "evaluation_metric": null,
            "human_performance": null,
            "llm_performance": "Qualitative presence reported: GPT-3 exhibited a distance effect in the authors' text-based experiments (no numeric metric reported in the provided text).",
            "prompting_method": "Text-based conversion of numeric-comparison experiments; exact prompt details not provided in the available excerpt.",
            "fine_tuned": false,
            "human_data_source": null,
            "statistical_significance": null,
            "notes": "The provided abstract indicates the distance effect appeared with GPT-3, but no numeric results or human baselines are present in the supplied document fragment.",
            "uuid": "e7478.1",
            "source_info": {
                "paper_title": "Cognitive Effects in Large Language Models",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "GPT-3 SNARC",
            "name_full": "Generative Pre-trained Transformer 3 — SNARC effect evaluation",
            "brief_description": "GPT-3 was evaluated for the SNARC effect (spatial–numerical associations) using text-adapted versions of standard tasks; the paper reports the SNARC effect appeared in GPT-3.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3",
            "model_description": "Autoregressive transformer-based large language model developed by OpenAI (decoder-only Transformer).",
            "model_size": "175B",
            "test_name": "SNARC effect (spatial–numerical association of response codes)",
            "test_category": "Numerical cognition / spatial-numerical association",
            "test_description": "The SNARC effect describes a tendency for smaller numbers to be associated with left space and larger numbers with right space, typically measured via response mappings or reaction times; here adapted to text-based queries for the LLM.",
            "evaluation_metric": null,
            "human_performance": null,
            "llm_performance": "Qualitative presence reported: GPT-3 exhibited the SNARC effect in the converted text experiments (no numeric metric reported in the provided text).",
            "prompting_method": "Text-based conversion of spatial-numerical tasks; specific prompt format not reported in the provided fragment.",
            "fine_tuned": false,
            "human_data_source": null,
            "statistical_significance": null,
            "notes": "The abstract explicitly lists SNARC as present for GPT-3; the excerpt does not include quantitative comparisons to human data or statistical tests.",
            "uuid": "e7478.2",
            "source_info": {
                "paper_title": "Cognitive Effects in Large Language Models",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "GPT-3 Size Congruity",
            "name_full": "Generative Pre-trained Transformer 3 — size congruity effect evaluation",
            "brief_description": "The paper evaluated GPT-3 on size congruity paradigms (magnitude vs physical size interference) after converting tasks to text; GPT-3 showed the effect according to the abstract.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3",
            "model_description": "Autoregressive transformer-based large language model developed by OpenAI (decoder-only Transformer).",
            "model_size": "175B",
            "test_name": "Size congruity effect",
            "test_category": "Perceptual interference / magnitude comparison",
            "test_description": "Size congruity paradigms test interference between numerical magnitude and physical size (e.g., faster responses when numerical and physical sizes are congruent); adapted here into text prompts for model evaluation.",
            "evaluation_metric": null,
            "human_performance": null,
            "llm_performance": "Qualitative presence reported: GPT-3 exhibited the size congruity effect in the authors' text-based experiments (no numeric metric reported in the provided text).",
            "prompting_method": "Text-based conversion of size-congruity tasks; exact prompt details are not present in the provided document fragment.",
            "fine_tuned": false,
            "human_data_source": null,
            "statistical_significance": null,
            "notes": "Abstract reports presence of the effect; the supplied HTML contains no numeric LLM scores or human baseline statistics.",
            "uuid": "e7478.3",
            "source_info": {
                "paper_title": "Cognitive Effects in Large Language Models",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "GPT-3 Anchoring",
            "name_full": "Generative Pre-trained Transformer 3 — anchoring effect evaluation",
            "brief_description": "GPT-3 was tested for the anchoring effect (bias in numerical/judgment estimates due to arbitrary anchors) and the paper reports that anchoring was absent for GPT-3 in their experiments.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3",
            "model_description": "Autoregressive transformer-based large language model developed by OpenAI (decoder-only Transformer).",
            "model_size": "175B",
            "test_name": "Anchoring effect",
            "test_category": "Judgment and decision-making / cognitive bias",
            "test_description": "Anchoring refers to the influence of an initial numeric anchor on subsequent estimates or judgments; experiments typically compare estimates following high vs low anchors and measure mean differences.",
            "evaluation_metric": null,
            "human_performance": null,
            "llm_performance": "Qualitative absence reported: the paper's abstract states the anchoring effect was absent for GPT-3 in their text-based experiments (no numeric metric reported in the provided text).",
            "prompting_method": "Text-based conversion of anchoring tasks into prompts; specific prompt wording and conditions are not provided in the available excerpt.",
            "fine_tuned": false,
            "human_data_source": null,
            "statistical_significance": null,
            "notes": "The abstract explicitly notes anchoring was absent, contrasting with the other effects; again, no numeric comparisons or formal statistical tests are present in the provided fragment of the paper.",
            "uuid": "e7478.4",
            "source_info": {
                "paper_title": "Cognitive Effects in Large Language Models",
                "publication_date_yy_mm": "2023-08"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Time required for judgments of numerical inequality",
            "rating": 2
        },
        {
            "paper_title": "The mental representation of parity and number",
            "rating": 2
        },
        {
            "paper_title": "Judgment under Uncertainty: Heuristics and Biases",
            "rating": 2
        }
    ],
    "cost": 0.010035,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><!DOCTYPE html>
<html lang='en'>
<head>

<!--Google consent mode V2-->
<script>
    window.dataLayer = window.dataLayer || [];
    function gtag() { dataLayer.push(arguments); }
    if (localStorage.getItem('consentMode') === null) {
        gtag('consent', 'default', {
            'ad_storage': 'denied',
            'ad_user_data': 'denied',
            'ad_personalization': 'denied',
            'analytics_storage': 'denied',
            'personalization_storage': 'granted',
            'functionality_storage': 'granted',
            'security_storage': 'granted',
        });
    } else {
        gtag('consent', 'default', JSON.parse(localStorage.getItem('consentMode')));
    }
    gtag('set', 'ads_data_redaction', true);
    gtag('set', 'url_passthrough', true);
</script>

<!-- Google Tag Manager -->
<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
})(window,document,'script','dataLayer','GTM-W7N3WL5P');</script>
<!-- End Google Tag Manager -->


    <link href="/favicon.ico" rel="shortcut icon" type="image/x-icon" />
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width" />
        <title>IOS Press Ebooks - Cognitive Effects in Large Language Models</title>


    <meta name="DC.type" content="Text" />
    <!--Publisher-->
    <meta name="citation_publisher" content="IOS Press" />
    <!--Volume-->
    <meta name="citation_inbook_title" content="ECAI 2023" />
    <!--Article-->
    <meta name="citation_firstpage" content="2105" />
    <meta name="citation_lastpage" content="2112" />
    <meta name="citation_title" content="Cognitive Effects in Large Language Models" />
        <meta name="citation_author" content="Shaki, Jonathan" />
        <meta name="citation_author" content="Kraus, Sarit" />
        <meta name="citation_author" content="Wooldridge, Michael" />

        <meta name="citation_publication_date" content="2023" /><!-- year only is allowed, full date possible-->

        <meta name="citation_online_date" content="2023/09/29" />

        <meta name="citation_doi" content="10.3233/FAIA230505" />
        <meta name="citation_abstract_html_url" content="https://ebooks.iospress.nl/doi/10.3233/FAIA230505" />
        <meta name="citation_fulltext_html_url" content="https://ebooks.iospress.nl/doi/10.3233/FAIA230505" />

        <meta name="citation_fulltext_world_readable" content="">


        <link href="/Content/themes/iospress/css?v=ljnAn2YK9VAuWQ6yMHuVpt0PyO6URwXlRsslkMHliAM1" rel="stylesheet"/>

    <script src="https://ajax.aspnetcdn.com/ajax/jQuery/jquery-3.5.1.min.js"></script>

    <script src="/bundles/jqueryval?v=S0zjY_C9SrWh_jAKguTKrt0UAVoLX4jD0DmtC1N35rA1"></script>

</head>
<body>
    <!-- HEADER -->
    <header id="header">
        <div id="default_header">
    <div id="logo"><img alt="IOS Press Logo" src="/Content/themes/iospress/images/ioslogo.png" /></div>
    <div id="divider"></div>
    <div id="title">IOS Press Ebooks</div>
    <div style="position:relative; float: right; width: 300px;">


    <!-- NOT LOGGED IN -->
    <div id="header_accountbox" class="loggedout">
        <div class="accountbox_welcome">Guest Access</div>
        <div id="header_accountbox_qmark">?</div>
        <div class="accountbox_actions">


            <a data-dialog-title="Identification" href="/Account/Login?returnUrl=%2FDOI%2F10.3233%2FFAIA230505" id="loginLink">Log in</a>
        </div>
        <div id="header_accountbox_infobox">
            <div id="header_accountbox_infobox_arrow"></div>
            <div id="header_accountbox_infobox_content">
                As a guest user you are not logged in or recognized by your IP address. You have
                access to the Front Matter, Abstracts, Author Index, Subject Index and the full
                text of Open Access publications.
            </div>
        </div>
    </div>

<script type="text/javascript">
    $(document).ready(function () {

        $("#header_accountbox_qmark").mouseenter(function () {
            $("#header_accountbox_infobox").show();
        });
        $("#header_accountbox_qmark").mouseleave(function () {
            $("#header_accountbox_infobox").hide();
        });

    });
</script>



    </div>
<div id="menu_wrapper">
    <div id="menu-primary" class="menu-container">
        <div class="menu" role="navigation" aria-label="Main Menu">
            <ul id="menu-primary-items" role="menubar">
                    <li role="presentation" class="menu-item"><a role="menuitem" href="/"><span>Home</span></a></li>                    
                    <li role="presentation" class="menu-item current-menu-item"><a role="menuitem" href="/Publication/Books"><span>Ebooks</span></a></li>                    
                    <li role="presentation" class="menu-item"><a role="menuitem" href="/Publication/OpenAccess"><span>Open Access</span></a></li>                    
                    <li role="presentation" class="menu-item"><a role="menuitem" href="/Publisher/About"><span>About IOS Press</span></a></li>                    
                    <li role="presentation" class="menu-item"><a role="menuitem" href="/Publisher/Offices"><span>Contact</span></a></li>                    
                    <li role="presentation" class="menu-item"><a role="menuitem" href="/Publisher/Faq"><span>FAQ</span></a></li>                    
            </ul>
        </div>
    </div>
</div>
</div>
    </header>

    <!-- END HEADER -->
    <div id="content" class="clearfix">
        <div id="leftcolumn">

    <div class="searchtitle" id="searchlabel">Search</div>
<div id="sidesearchbox" style="font-size: 11px; width: 100%;" class="clearfix">
<form action="/Search" method="post">        <div style="float: left; width: 100px;"><input aria-labelledby="searchlabel" class="searchterm" id="SearchTerm" name="SearchTerm" title="Search ebooks" type="text" value="" /></div>
<input data-val="true" data-val-required="The SearchInAuthors field is required." id="SearchInAuthors" name="SearchInAuthors" type="hidden" value="True" /><input data-val="true" data-val-required="The SearchInTitle field is required." id="SearchInTitle" name="SearchInTitle" type="hidden" value="True" /><input data-val="true" data-val-required="The SearchInKeywords field is required." id="SearchInKeywords" name="SearchInKeywords" type="hidden" value="True" /><input data-val="true" data-val-required="The SearchInISSNISBN field is required." id="SearchInISSNISBN" name="SearchInISSNISBN" type="hidden" value="True" />        <div style="clear: both; padding-top: 12px;" align="right">
            <input type="image" value="SideSearch" src="/Content/themes/iospress/images/searchbutton.png" alt="Submit Search" class="submit" />
        </div>
</form></div>


            <div class="partialContents" data-url="/Subject/BrowserAsync">
                <img alt="loader" src="/Content/Images/indicator.white.gif" /> loading subjects...
            </div>


        </div>
        <main id="contentcolumn">




<div class="bookseriesvolumearticleheader">
    <div class="content">
        <div class="cover"><img alt="cover" class="volume" src="/Content/themes/iospress/images/article.gif" /></div>
        <div class="metadata">

            <div class="value title">Cognitive Effects in Large Language Models</div>

                <span class="metadata_label">Authors</span>
                <div class="value authors">Jonathan Shaki, Sarit Kraus, Michael Wooldridge</div>

                <span class="metadata_label">Pages</span>
                <div class="value pages">2105 - 2112</div>

                <span class="metadata_label">DOI</span>
                <div class="value doi">10.3233/FAIA230505</div>

                <span class="metadata_label">Category</span>
                <div class="value category">Research Article</div>

            <span class="metadata_label">Series</span>
            <div class="value book"><a href="/bookseries/frontiers-in-artificial-intelligence-and-applications">Frontiers in Artificial Intelligence and Applications</a></div>

            <span class="metadata_label">Ebook</span>
                <div class="value book"><a href="/volume/ecai-2023-26th-european-conference-on-artificial-intelligence-including-12th-conference-on-prestigious-applications-of-intelligent-systems-pais-2023">Volume 372: ECAI 2023</a></div>

                <div class="abstract">
                    <b>Abstract</b><br />
                    <section>
  <p>Large Language Models (LLMs) such as ChatGPT have received enormous attention over the past year and are now used by hundreds of millions of people every day. The rapid adoption of this technology naturally raises questions about the possible biases such models might exhibit. In this work, we tested one of these models (GPT-3) on a range of cognitive effects, which are systematic patterns that are usually found in human cognitive tasks. We found that LLMs are indeed prone to several human cognitive effects. Specifically, we show that the priming, distance, SNARC, and size congruity effects were presented with GPT-3, while the anchoring effect is absent. We describe our methodology, and specifically the way we converted real-world experiments to text-based experiments. Finally, we speculate on the possible reasons why GPT-3 exhibits these effects and discuss whether they are imitated or reinvented.</p>
</section>
                </div>
        </div>
        <div class="actions">


            <form action="/Download/Pdf" id="downloadform64437" method="post">        <input type="hidden" name="id" value="64437" />
        <div id='downloadlink64437' class="button getpdf">Download PDF</div>
</form>    <script type="text/javascript">
    $(function () {
        var $link = $('div#downloadlink64437');
        var $form = $('form#downloadform64437');

        $link.on('click', function (e) {
            if ($link.hasClass('busy')) return;   // already clicked once

            $link.addClass('busy');               // mark as locked
            $form.submit();                       // original behaviour

            setTimeout(function () {              // unlock after 2 s
                $link.removeClass('busy');
            }, 2000);
        });
    });
    </script>







<div class="button openaccesslicense">
    <a rel="license" target="_blank" title="This work is licensed under a Creative Commons License"
       href="https://creativecommons.org/licenses/by-nc/4.0/deed.en_US">
        <img alt="Creative Commons License" style="border-width: 0" src="https://i.creativecommons.org/l/by-nc/4.0/80x15.png" />
    </a>
</div>


        </div>
    </div>
</div>



        </main>
    </div>
    <footer id="footer" class="clearfix">
        <script type="text/javascript">
    $(document).ready(function () {

        $('[id^="tip_"]').mouseenter(function () {
            $(this).stop(true);
            $(this).hide().show();
        });
        $('.tip').mouseleave(function () {
            $('[id^="tip_"]').fadeOut(600, function () {
                $('[id^="tip_"]').hide()
            });
        });

        $('[name^="atip_"]').mouseenter(function () {
            var idtoshow = $(this).attr('name').substring(1);
            $('#' + idtoshow).hide().fadeIn(600, function () {
                $('#' + idtoshow).show()
            });
        });

        $('[name^="atip_"]').mouseleave(function () {
            var idtoshow = $(this).attr('name').substring(1);
            $('#' + idtoshow).fadeOut(600, function () {
                $('#' + idtoshow).hide()
            });
        });
    });
</script>
<div id="default_footer">
    <div id="footer_contact">
        <h2>
            Contact
        </h2>

        <ul class="list" style="float: left">
            <li>&nbsp;</li>
            <li><a name="atip_europe" href="/Publisher/Offices">IOS Press / Sage Publishing</a></li>
            <li>&nbsp;</li>
        </ul>

        <a name="atip_usa" id="usa" class="usa" href="#" title="North America"></a>
        <a name="atip_europe" id="europe" class="europe" title="Europe" href="/Publisher/Offices"></a>
        <a name="atip_asia" id="asia" class="asia" title="Asia" href="#"></a>

        <div id="tip_europe" style="display: none;">
            <div class="tip">
                <strong>IOS Press / Sage Publishing</strong><br />
                Teleportboulevard 120<br />
                1043 EJ Amsterdam<br />
                The Netherlands<br /><br />

                <strong>Sage Publishing</strong><br />
                <a href="https://www.sagepub.com/contact-and-support/contact-us" target="_blank">Contact Sage</a>
            </div>
        </div>

    </div>
    <div id="footer_divider">
    </div>
    <div id="footer_links">
        <h2>
            Copyright 2025 © IOS Press
        </h2>
        <a href="/publisher/Disclaimer/">Disclaimer</a> &nbsp;&nbsp;&nbsp; <a href="/publisher/TermsOfUse/">
            Terms of use
        </a>&nbsp;&nbsp;&nbsp;<a href="/publisher/PrivacyPolicy/">
            Privacy Policy
        </a>&nbsp;&nbsp;&nbsp;<a href="/publisher/Contact/">
            Contact
        </a>&nbsp;&nbsp;&nbsp;<a href="/publisher/Faq/">FAQ</a>
    </div>
    <div id="footer_twitter">

        <a href="https://twitter.com/IOSPress_STM" class="twitter-follow-button" data-show-screen-name="false" data-show-count="false">Follow @twitter</a>
        <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

    </div>
</div>

    </footer>
    <script type="text/javascript">
        if (typeof jQuery == 'undefined') {
            var e = document.createElement('script');
            e.src = '/Scripts/jquery-3.5.1.min.js';
            e.type = 'text/javascript';
            document.getElementsByTagName("head")[0].appendChild(e);
        }
    </script>
    <script src="/bundles/subjectmenu?v=jieOsy0lZdhfbuK00Z1qohST722ZF4yIVuWCjPrX6ZE1"></script>

    <script src="/bundles/pubility?v=meEJdaSXIsVeUXAotcBmWyMjARi2G38fFL4j3SAMa9c1"></script>



    <script type="text/javascript">
        $(document).ready(function () {
            initExpandBoxes(10);
        })
    </script>



<style>
    .cookie-consent-banner {
        display: none;
        position: fixed;
        bottom: 0;
        left: 0;
        right: 0;
        background-color: #f8f9fa;
        box-shadow: 0 -2px 10px rgba(0, 0, 0, 0.1);
        color: black;
        padding: 15px;
        font-size: 14px;
        text-align: center;
        z-index: 1000;
    }

        .cookie-consent-banner h3 {
            padding: 0px 0px;
            font-size: 14px;
            font-weight: bolder;
        }

    .cookie-consent-button {
        border: none;
        padding: 8px 16px;
        text-align: center;
        text-decoration: none;
        display: inline-block;
        font-size: 14px;
        margin: 4px 2px;
        cursor: pointer;
        border-radius: 4px;
    }

        .cookie-consent-button:hover {
            box-shadow: 0 -2px 5px rgba(0, 0, 0, 0.2);
        }

        .cookie-consent-button:active {
            opacity: .5;
        }

        .cookie-consent-button.btn-success {
            background-color: #34a853;
            color: white;
        }

        .cookie-consent-button.btn-grayscale {
            background-color: #dfe1e5;
            color: black;
        }

        .cookie-consent-button.btn-outline {
            background-color: #e6f4ea;
            color: #34a853;
        }

    .cookie-consent-options {
        display: flex;
        justify-content: center;
        flex-wrap: wrap;
        margin-bottom: 10px;
    }

        .cookie-consent-options label {
            margin: 4px 5px;
            font-size: 12px;
            font-weight: normal;
        }

        .cookie-consent-options input {
            margin-right: 5px;
        }
</style>

<div id="cookie-consent-banner-basic" class="cookie-consent-banner">
    <h3>This website uses cookies</h3>
    <p>
        We use cookies to provide you with the best possible experience. They also allow us to analyze user behavior in order to constantly improve the website for you. Info about the <a href="/publisher/PrivacyPolicy/">privacy policy</a> of IOS Press.
    </p>
    <button id="btn-advanced-basic" class="cookie-consent-button btn-grayscale">Customize</button>
    <button id="btn-accept-all-basic" class="cookie-consent-button btn-success">Accept</button>
</div>

<div id="cookie-consent-banner-advanced" class="cookie-consent-banner">
    <h3>This website uses cookies</h3>
    <p>
        We use cookies to provide you with the best possible experience. They also allow us to analyze user behavior in order to constantly improve the website for you. Info about the <a href="/publisher/PrivacyPolicy2">privacy policy</a> of IOS Press.
    </p>
    <button id="btn-reject-all-advanced" class="cookie-consent-button btn-grayscale" style="display:none">Reject All</button>
    <button id="btn-accept-some-advanced" class="cookie-consent-button btn-outline">Accept Selection</button>
    <button id="btn-accept-all-advanced" class="cookie-consent-button btn-success">Accept</button>
    <div class="cookie-consent-options">
        <label><input id="consent-necessary" type="checkbox" value="Necessary" checked disabled>Necessary</label>
        <label><input id="consent-preferences" type="checkbox" value="Preferences" checked>Preferences</label>
        <label><input id="consent-analytics" type="checkbox" value="Analytics" checked>Analytics</label>
        <label><input id="consent-marketing" type="checkbox" value="Marketing" checked>Marketing</label>
    </div>
</div>

<script>

    function hideBannerAdvanced() {
        document.getElementById('cookie-consent-banner-advanced').style.display = 'none';
    }
    function hideBannerBasic() {
        document.getElementById('cookie-consent-banner-basic').style.display = 'none';
    }
    function hideAll() {
        hideBannerBasic();
        hideBannerAdvanced();
    }

    function showBannerAdvanced() {
        if (localStorage.getItem('consentMode') !== null) {
            var consentmode = JSON.parse(localStorage.getItem('consentMode'));
            document.getElementById('consent-analytics').checked = (consentmode.analytics_storage === 'granted');
            document.getElementById('consent-preferences').checked = (consentmode.personalization_storage === 'granted');
            document.getElementById('consent-marketing').checked = (consentmode.ad_storage === 'granted');
        }
        document.getElementById('cookie-consent-banner-advanced').style.display = 'block';
    }
    function showBannerBasic() {
        if (localStorage.getItem('consentMode') !== null) {
            var consentmode = JSON.parse(localStorage.getItem('consentMode'));
            document.getElementById('consent-analytics').checked = (consentmode.analytics_storage === 'granted');
            document.getElementById('consent-preferences').checked = (consentmode.personalization_storage === 'granted');
            document.getElementById('consent-marketing').checked = (consentmode.ad_storage === 'granted');
        }
        document.getElementById('cookie-consent-banner-basic').style.display = 'block';
    }

    document.getElementById('btn-accept-all-basic').addEventListener('click', function () {
        setConsent({
            necessary: true,
            analytics: true,
            preferences: true,
            marketing: true
        });
        hideAll();
    });
    document.getElementById('btn-accept-all-advanced').addEventListener('click', function () {
        setConsent({
            necessary: true,
            analytics: true,
            preferences: true,
            marketing: true
        });
        hideAll();
    });
    document.getElementById('btn-advanced-basic').addEventListener('click', function () {
        hideBannerBasic();
        showBannerAdvanced();
    });
    document.getElementById('btn-accept-some-advanced').addEventListener('click', function () {
        setConsent({
            necessary: true,
            analytics: document.getElementById('consent-analytics').checked,
            preferences: document.getElementById('consent-preferences').checked,
            marketing: document.getElementById('consent-marketing').checked
        });
        hideAll();
    });
    document.getElementById('btn-reject-all-advanced').addEventListener('click', function () {
        setConsent({
            necessary: false,
            analytics: false,
            preferences: false,
            marketing: false
        });
        hideAll()
    });

    if (localStorage.getItem('consentMode') === null) {

        showBannerBasic();
    }

    function setConsent(consent) {
        const consentMode = {
            'ad_storage': consent.marketing ? 'granted' : 'denied',
            'ad_user_data': consent.marketing ? 'granted' : 'denied',
            'ad_personalization': consent.marketing ? 'granted' : 'denied',
            'analytics_storage': consent.analytics ? 'granted' : 'denied',
            'personalization_storage': consent.preferences ? 'granted' : 'denied',
            'functionality_storage': consent.necessary ? 'granted' : 'denied',
            'security_storage': consent.necessary ? 'granted' : 'denied',
        };
        gtag('consent', 'update', consentMode);
        localStorage.setItem('consentMode', JSON.stringify(consentMode));
    }

</script>


</body>
</html>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5798 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5798</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5798</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-116.html">extraction-schema-116</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <p><strong>Paper ID:</strong> paper-8dce168f723158b771b526401113064c36fc875e</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/8dce168f723158b771b526401113064c36fc875e" target="_blank">State of What Art? A Call for Multi-Prompt LLM Evaluation</a></p>
                <p><strong>Paper Venue:</strong> Transactions of the Association for Computational Linguistics</p>
                <p><strong>Paper TL;DR:</strong> A large-scale collection of instruction paraphrases is created and a set of diverse metrics on multiple instruction paraphrases are proposed, specifically tailored for different use cases (e.g., LLM vs. downstream development), ensuring a more reliable and meaningful assessment of LLM capabilities.</p>
                <p><strong>Paper Abstract:</strong> Abstract Recent advances in LLMs have led to an abundance of evaluation benchmarks, which typically rely on a single instruction template per task. We create a large-scale collection of instruction paraphrases and comprehensively analyze the brittleness introduced by single-prompt evaluations across 6.5M instances, involving 20 different LLMs and 39 tasks from 3 benchmarks. We find that different instruction templates lead to very different performance, both absolute and relative. Instead, we propose a set of diverse metrics on multiple instruction paraphrases, specifically tailored for different use cases (e.g., LLM vs. downstream development), ensuring a more reliable and meaningful assessment of LLM capabilities. We show that our metrics provide new insights into the strengths and limitations of current LLMs.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5798.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5798.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Multi-prompt instability</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Performance instability across instruction paraphrases (Multi-prompt evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper demonstrates that small changes in instruction templates (prompt wording, punctuation, minor rephrasing) produce large changes in both absolute accuracy and relative model ranking across many LLMs and tasks, using a large-scale evaluation over millions of instances.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>multiple models (16 open-source models; 4 OpenAI models in small-scale eval)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>39 tasks from LMENTRY, BIG-bench Lite (BBL), BIG-bench Hard (BBH)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Diverse set of classification, linguistic and reasoning tasks drawn from three established benchmarks; evaluation mostly zero-shot (single instruction template per trial) using exact-match/benchmark scripts.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Zero-shot natural-language instruction templates; the study generates and uses many paraphrased instruction templates per task (on average ~240 validated paraphrases per LMENTRY task and ~175 per BBH task). Formats include simple rephrasings, chain-of-thought-based generation of prompts, and gradual template generation; some evaluations include adding an example of the prediction format.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Original benchmark instruction template (single prompt) vs. many paraphrased instruction templates (automatically generated and manually verified) and vs. human-crafted paraphrases for BBL.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Aggregate finding: large variance across prompts — rankings and absolute scores vary widely; across evaluation (≈6.5M instances) many prompt-induced differences observed (no single numeric accuracy since varies by model/task/prompt).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Qualitative: many cases of >1 standard deviation divergence from average when using original prompt; 15/25 tasks had paraphrase pairs with negative Kendall's τ (mostly disagreeing rankings); Friedman test shows statistically significant differences for 21/25 tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>varied (some paraphrases improve, others degrade performance; both absolute and relative rankings change)</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>The paper hypothesizes that LLM sensitivity arises from how instructions are phrased and formatted (including punctuation and minimal wording changes), potential exposure of some prompts in training (prompt leakage) for some models, and differences in how models internalize instruction tokens; they rule out prompt-generation noise as a major factor (see separate result). The authors argue different evaluation needs (max vs average) depending on downstream use.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'State of What Art? A Call for Multi-Prompt LLM Evaluation', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5798.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5798.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>KendallW instability</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Ranking instability measured by Kendall's W and Kendall's τ</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper quantifies how different instruction templates induce different model rankings using nonparametric rank-correlation statistics (Kendall's W and Kendall's τ), showing weak-to-moderate agreement for most tasks and many cases of near-complete reordering.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>multiple models (ranking across evaluated LLMs)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>25 tasks (subset analyzed for ranking stability across paraphrases) from LMENTRY, BBH, BBL</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Tasks for which multiple paraphrased instruction templates were evaluated to assess ranking stability.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Compare rankings induced by different paraphrased instruction templates; compute Kendall's W across all templates for each task and Kendall's τ for pairs of templates.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Different paraphrased instruction templates compared pairwise and collectively vs. single original template.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Reported Kendall's W per task (examples): LMENTRY 'not containing' W=0.271 (weak), 'first alphabet' W=0.436 (weak); many BBH tasks W in 0.628–0.873 (medium to strong); overall most W < 0.85 indicating weak-to-moderate agreement.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>15 out of 25 tasks have instruction-template paraphrase pairs with negative Kendall's τ (indicating mostly disagreeing LLM rankings); Friedman test significant for 21/25 tasks (different instructions lead to statistically significant ranking differences).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Quantified via Kendall's W and τ; many W values are low (e.g., 0.271) and 15/25 tasks with negative τ for some pairs.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>reordered rankings (i.e., format changed relative ordering of models)</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Different instruction templates act as different 'judges' producing inconsistent rankings; therefore a single template is insufficient to claim robust model superiority. Authors propose aggregate metrics (MaxP, AvgP, CPS) to accommodate this.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'State of What Art? A Call for Multi-Prompt LLM Evaluation', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5798.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5798.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Flan-T5 format edits</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Flan-T5 sensitivity to minor lexical edits in instruction wording</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Specific minor wording edits in prompts (e.g., replacing 'excludes' with 'lacks' or punctuation/quote additions) produced large accuracy swings for Flan-T5 variants, sometimes flipping performance by tens of percentage points.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Flan-T5-large</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>780M</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>LMENTRY (word/letter containment / rhyming sub-tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Elementary linguistic tasks from LMENTRY such as 'write a word that does not contain the letter [l]' and rhyming/length/comparison tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Zero-shot instruction templates; minimal lexical edits in the templates (e.g., 'excludes' -> 'lacks', 'omits' -> 'lacks', punctuation and quoting changes).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Original prompt wording vs. minimally edited prompt (examples: 'Write a word that excludes the letter "[letter]".'  vs 'Write a word that lacks the letter "[letter]".'; or addition of quotes around placeholders).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Example: Flan-T5-large accuracy on one LMENTRY prompt: 0.54 (with 'excludes')</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Same model with 'lacks' phrasing: 0.12 (difference -0.42, i.e., -42 percentage points) as reported in Table 5; elsewhere paper reports Flan-T5-large average degradation ~28% for that same edit across paraphrases.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>-0.42 (absolute accuracy change) in the shown example; average reported ~-28% for that lexical substitution for Flan-T5-large; Flan-T5-XL showed +0.62 (+62 percentage points) for a specific pair and +46% average improvement in another mention.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>could reduce or improve performance depending on exact model size and prompt: example degraded for Flan-T5-large, improved for Flan-T5-XL</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Authors note that very minor surface edits influence tokenization/interpretation and model-conditioned behavior differently across model sizes/architectures; highlights that prompt wording interacts with model internals and training exposure.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'State of What Art? A Call for Multi-Prompt LLM Evaluation', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5798.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e5798.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Alpaca divergence</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Alpaca-13B divergence from prompt-average performance</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Alpaca-13B often shows strong positive divergence when evaluated on some original benchmark prompts compared to its average over many paraphrases, indicating that single-prompt scores can be misleadingly high.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Alpaca-13B</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>13B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>LMENTRY (10 selected challenging tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Subset of LMENTRY tasks chosen for low original scores to highlight differences across paraphrases.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Original benchmark instruction templates vs. many paraphrased templates (~240 validated paraphrases per task used to compute average).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Original single instructions compared to average over paraphrases.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Using original LMENTRY instruction templates Alpaca-13B sometimes achieved substantially higher scores than its paraphrase average (text notes: outperformed its average by >1 std in 7 out of 10 LMENTRY tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Average performance across paraphrases is lower; divergence defined as number of standard deviations from the paraphrase average — Alpaca-13B exceeded +1 std on 7/10 tasks for original templates.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>>1 standard deviation divergence (in 7/10 tasks) — paper reports this as a noteworthy effect (exact accuracy per task not enumerated in text excerpt).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>original prompts often overestimate performance (i.e., original prompt performance higher than prompt-average)</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Original benchmark templates can be 'lucky' for some models, perhaps matching phrasing seen by the model during training or otherwise being especially compatible; hence single-prompt evaluation frequently overstates a model's general robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'State of What Art? A Call for Multi-Prompt LLM Evaluation', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5798.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e5798.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>OpenAI prompt sensitivity</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OpenAI models' sensitivity to near-identical prompt formatting (td002, td003, davinci, gpt-3.5-turbo)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Small-scale evaluation of OpenAI API models shows that trivial formatting changes (adding quotes, changing punctuation, swapping 'which'/'what', adding brackets) produce large changes in accuracy; original prompts often perform better than paraphrase-average and may underperform the paraphrase-max.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>text-davinci-002 (td002), text-davinci-003 (td003), davinci, GPT-3.5-Turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>LMENTRY subset (4 tasks in Figures and 10 tasks in small-scale eval)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Elementary linguistic tasks from LMENTRY used to probe prompt formatting sensitivity; average and maximal performance estimated using randomized sampling and a greedy search over paraphrases.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Original LMENTRY instruction templates vs. paraphrases (randomly sampled to estimate AvgP, greedy search to estimate MaxP); minimal changes include adding quotes around placeholders, commas->semicolons, bracket formatting, swapping determiners.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Original prompt vs paraphrased prompts (many paraphrases) — both average and maximal metrics estimated.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Representative examples (Table 7): td002: 'Which word has a greater number of letters, [word1] or [word2]?' acc .50 -> with quotes .23 (Δ -0.27); td003 similar example .60 -> .14 (Δ -0.46). Comma->semicolon change: td002 .08 -> .85 (Δ +0.77); td003 .48 -> .90 (Δ +0.42).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Across small-scale eval: in 72.5% of cases original instructions had higher performance than paraphrase-average; davinci original prompts added on average +21 accuracy points relative to the paraphrase-average. For text-davinci-002, some paraphrases improved maximal accuracy above 90% for 8/10 tasks. 26/40 differences across four models were statistically significant by McNemar test.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Examples range from -0.46 to +0.77 absolute accuracy change; davinci average +21 percentage points for original vs paraphrase-average; 72.5% of original prompts outperform average.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>both improved and reduced performance depending on specific edit and model; overall original prompts often higher than paraphrase-average but many paraphrases can exceed original in MaxP</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Authors suggest minimal punctuation and quoting differences change how the model interprets placeholders and instruction structure; some original prompts may be specially effective (possibly due to training exposure), while other paraphrases unlock better model behavior for MaxP.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'State of What Art? A Call for Multi-Prompt LLM Evaluation', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5798.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e5798.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Auto-paraphrase noise null</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Automatic paraphrase noise has little impact on metric-based rankings</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The authors find that noise from automatically generated paraphrases (i.e., occasional incorrect paraphrases) has virtually no impact on model rankings computed by their proposed aggregate metrics (MaxP, AvgP, CPS).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>multiple models (rankings compared before/after filtering paraphrases)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>All studied tasks (LMENTRY, BBH, BBL)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Evaluation of effect of removing incorrect paraphrases on final model rankings using Kendall's τ.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Compare model rankings computed using automatically-generated paraphrase pool vs. manually filtered paraphrase pool.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Unfiltered auto-generated paraphrases vs. filtered (human-verified) paraphrases.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Kendall's τ comparing rankings before and after manual filtering is near-perfect to perfect for almost all tasks and metrics (Table 6); one exception due to a buggy LMENTRY script ('ends with word').</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Table 6 averaged Kendall's Tau values for Max/Avg/Combined perf ranking comparisons were high (e.g., LMENTRY .963/.978/.948 and BBH .991/.983/.966 as averaged Kendall's Tau across tasks after filtering — indicating strong agreement).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Near-perfect agreement in rankings (Kendall's τ ~0.96–0.99 averaged across tasks/metrics) before vs after paraphrase filtering.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>no effect on metric-based model rankings (i.e., automatic paraphrase noise negligible)</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Most automatically-generated paraphrases are valid and the small fraction of incorrect paraphrases does not change aggregate metric-based rankings; thus automated paraphrase generation is sufficient for multi-prompt evaluation without exhaustive manual verification.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td>One task ('ends with word' in LMENTRY) showed disagreement attributable to an error in the evaluation script, not paraphrase noise.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'State of What Art? A Call for Multi-Prompt LLM Evaluation', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5798.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e5798.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Human paraphrase sensitivity (BBL)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sensitivity to human-crafted paraphrases (BIG-bench Lite)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Using human-written paraphrases produced after model training (from Sun et al. 2023) the paper observes similar sensitivity to paraphrasing, showing that effects are not solely due to leaked prompts in training data.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>subset of 11 larger-parameter models evaluated on BBH/BBL</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>BIG-bench Lite (BBL) tasks (human-crafted paraphrases 7–12 per task)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>BBL tasks covering multiple knowledge domains sampled from BIG-bench, with human-written paraphrases produced after model training.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Original single BBL instruction template vs. multiple manual paraphrases authored post-training.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Original BBL prompt vs human paraphrases (no training leakage possible).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Paper reports similar inconsistencies in both absolute scores and rankings when using human paraphrases, with examples of paraphrase pairs producing minimal wording differences but notable ranking changes (illustrated in figures/tables).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Qualitative: notable score and ranking changes comparable to those seen with auto-generated paraphrases; exact numeric deltas depend on model/task pair (paper shows examples and minimal-distance pairs with large deltas).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>varied (both improvements and degradations observed across paraphrases)</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Because the human paraphrases were authored after model training, this indicates that sensitivity is not explained solely by prompt leakage; rather models are intrinsically sensitive to phrasing and formatting.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'State of What Art? A Call for Multi-Prompt LLM Evaluation', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Evaluating the zero-shot robustness of instruction-tuned language models <em>(Rating: 2)</em></li>
                <li>Quantifying language models' sensitivity to spurious features in prompt design or: How i learned to start worrying about prompt formatting <em>(Rating: 2)</em></li>
                <li>Mind your format: Towards consistent evaluation of in-context learning improvements <em>(Rating: 2)</em></li>
                <li>PromptBench: Towards evaluating the robustness of large language models on adversarial prompts <em>(Rating: 1)</em></li>
                <li>Is prompt all you need? no. a comprehensive and broader view of instruction learning <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5798",
    "paper_id": "paper-8dce168f723158b771b526401113064c36fc875e",
    "extraction_schema_id": "extraction-schema-116",
    "extracted_data": [
        {
            "name_short": "Multi-prompt instability",
            "name_full": "Performance instability across instruction paraphrases (Multi-prompt evaluation)",
            "brief_description": "The paper demonstrates that small changes in instruction templates (prompt wording, punctuation, minor rephrasing) produce large changes in both absolute accuracy and relative model ranking across many LLMs and tasks, using a large-scale evaluation over millions of instances.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "multiple models (16 open-source models; 4 OpenAI models in small-scale eval)",
            "model_size": null,
            "task_name": "39 tasks from LMENTRY, BIG-bench Lite (BBL), BIG-bench Hard (BBH)",
            "task_description": "Diverse set of classification, linguistic and reasoning tasks drawn from three established benchmarks; evaluation mostly zero-shot (single instruction template per trial) using exact-match/benchmark scripts.",
            "problem_format": "Zero-shot natural-language instruction templates; the study generates and uses many paraphrased instruction templates per task (on average ~240 validated paraphrases per LMENTRY task and ~175 per BBH task). Formats include simple rephrasings, chain-of-thought-based generation of prompts, and gradual template generation; some evaluations include adding an example of the prediction format.",
            "comparison_format": "Original benchmark instruction template (single prompt) vs. many paraphrased instruction templates (automatically generated and manually verified) and vs. human-crafted paraphrases for BBL.",
            "performance": "Aggregate finding: large variance across prompts — rankings and absolute scores vary widely; across evaluation (≈6.5M instances) many prompt-induced differences observed (no single numeric accuracy since varies by model/task/prompt).",
            "performance_comparison": null,
            "format_effect_size": "Qualitative: many cases of &gt;1 standard deviation divergence from average when using original prompt; 15/25 tasks had paraphrase pairs with negative Kendall's τ (mostly disagreeing rankings); Friedman test shows statistically significant differences for 21/25 tasks.",
            "format_effect_direction": "varied (some paraphrases improve, others degrade performance; both absolute and relative rankings change)",
            "explanation_or_hypothesis": "The paper hypothesizes that LLM sensitivity arises from how instructions are phrased and formatted (including punctuation and minimal wording changes), potential exposure of some prompts in training (prompt leakage) for some models, and differences in how models internalize instruction tokens; they rule out prompt-generation noise as a major factor (see separate result). The authors argue different evaluation needs (max vs average) depending on downstream use.",
            "counterexample_or_null_result": null,
            "uuid": "e5798.0",
            "source_info": {
                "paper_title": "State of What Art? A Call for Multi-Prompt LLM Evaluation",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "KendallW instability",
            "name_full": "Ranking instability measured by Kendall's W and Kendall's τ",
            "brief_description": "The paper quantifies how different instruction templates induce different model rankings using nonparametric rank-correlation statistics (Kendall's W and Kendall's τ), showing weak-to-moderate agreement for most tasks and many cases of near-complete reordering.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "multiple models (ranking across evaluated LLMs)",
            "model_size": null,
            "task_name": "25 tasks (subset analyzed for ranking stability across paraphrases) from LMENTRY, BBH, BBL",
            "task_description": "Tasks for which multiple paraphrased instruction templates were evaluated to assess ranking stability.",
            "problem_format": "Compare rankings induced by different paraphrased instruction templates; compute Kendall's W across all templates for each task and Kendall's τ for pairs of templates.",
            "comparison_format": "Different paraphrased instruction templates compared pairwise and collectively vs. single original template.",
            "performance": "Reported Kendall's W per task (examples): LMENTRY 'not containing' W=0.271 (weak), 'first alphabet' W=0.436 (weak); many BBH tasks W in 0.628–0.873 (medium to strong); overall most W &lt; 0.85 indicating weak-to-moderate agreement.",
            "performance_comparison": "15 out of 25 tasks have instruction-template paraphrase pairs with negative Kendall's τ (indicating mostly disagreeing LLM rankings); Friedman test significant for 21/25 tasks (different instructions lead to statistically significant ranking differences).",
            "format_effect_size": "Quantified via Kendall's W and τ; many W values are low (e.g., 0.271) and 15/25 tasks with negative τ for some pairs.",
            "format_effect_direction": "reordered rankings (i.e., format changed relative ordering of models)",
            "explanation_or_hypothesis": "Different instruction templates act as different 'judges' producing inconsistent rankings; therefore a single template is insufficient to claim robust model superiority. Authors propose aggregate metrics (MaxP, AvgP, CPS) to accommodate this.",
            "counterexample_or_null_result": null,
            "uuid": "e5798.1",
            "source_info": {
                "paper_title": "State of What Art? A Call for Multi-Prompt LLM Evaluation",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "Flan-T5 format edits",
            "name_full": "Flan-T5 sensitivity to minor lexical edits in instruction wording",
            "brief_description": "Specific minor wording edits in prompts (e.g., replacing 'excludes' with 'lacks' or punctuation/quote additions) produced large accuracy swings for Flan-T5 variants, sometimes flipping performance by tens of percentage points.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Flan-T5-large",
            "model_size": "780M",
            "task_name": "LMENTRY (word/letter containment / rhyming sub-tasks)",
            "task_description": "Elementary linguistic tasks from LMENTRY such as 'write a word that does not contain the letter [l]' and rhyming/length/comparison tasks.",
            "problem_format": "Zero-shot instruction templates; minimal lexical edits in the templates (e.g., 'excludes' -&gt; 'lacks', 'omits' -&gt; 'lacks', punctuation and quoting changes).",
            "comparison_format": "Original prompt wording vs. minimally edited prompt (examples: 'Write a word that excludes the letter \"[letter]\".'  vs 'Write a word that lacks the letter \"[letter]\".'; or addition of quotes around placeholders).",
            "performance": "Example: Flan-T5-large accuracy on one LMENTRY prompt: 0.54 (with 'excludes')",
            "performance_comparison": "Same model with 'lacks' phrasing: 0.12 (difference -0.42, i.e., -42 percentage points) as reported in Table 5; elsewhere paper reports Flan-T5-large average degradation ~28% for that same edit across paraphrases.",
            "format_effect_size": "-0.42 (absolute accuracy change) in the shown example; average reported ~-28% for that lexical substitution for Flan-T5-large; Flan-T5-XL showed +0.62 (+62 percentage points) for a specific pair and +46% average improvement in another mention.",
            "format_effect_direction": "could reduce or improve performance depending on exact model size and prompt: example degraded for Flan-T5-large, improved for Flan-T5-XL",
            "explanation_or_hypothesis": "Authors note that very minor surface edits influence tokenization/interpretation and model-conditioned behavior differently across model sizes/architectures; highlights that prompt wording interacts with model internals and training exposure.",
            "counterexample_or_null_result": null,
            "uuid": "e5798.2",
            "source_info": {
                "paper_title": "State of What Art? A Call for Multi-Prompt LLM Evaluation",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "Alpaca divergence",
            "name_full": "Alpaca-13B divergence from prompt-average performance",
            "brief_description": "Alpaca-13B often shows strong positive divergence when evaluated on some original benchmark prompts compared to its average over many paraphrases, indicating that single-prompt scores can be misleadingly high.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Alpaca-13B",
            "model_size": "13B",
            "task_name": "LMENTRY (10 selected challenging tasks)",
            "task_description": "Subset of LMENTRY tasks chosen for low original scores to highlight differences across paraphrases.",
            "problem_format": "Original benchmark instruction templates vs. many paraphrased templates (~240 validated paraphrases per task used to compute average).",
            "comparison_format": "Original single instructions compared to average over paraphrases.",
            "performance": "Using original LMENTRY instruction templates Alpaca-13B sometimes achieved substantially higher scores than its paraphrase average (text notes: outperformed its average by &gt;1 std in 7 out of 10 LMENTRY tasks).",
            "performance_comparison": "Average performance across paraphrases is lower; divergence defined as number of standard deviations from the paraphrase average — Alpaca-13B exceeded +1 std on 7/10 tasks for original templates.",
            "format_effect_size": "&gt;1 standard deviation divergence (in 7/10 tasks) — paper reports this as a noteworthy effect (exact accuracy per task not enumerated in text excerpt).",
            "format_effect_direction": "original prompts often overestimate performance (i.e., original prompt performance higher than prompt-average)",
            "explanation_or_hypothesis": "Original benchmark templates can be 'lucky' for some models, perhaps matching phrasing seen by the model during training or otherwise being especially compatible; hence single-prompt evaluation frequently overstates a model's general robustness.",
            "counterexample_or_null_result": null,
            "uuid": "e5798.3",
            "source_info": {
                "paper_title": "State of What Art? A Call for Multi-Prompt LLM Evaluation",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "OpenAI prompt sensitivity",
            "name_full": "OpenAI models' sensitivity to near-identical prompt formatting (td002, td003, davinci, gpt-3.5-turbo)",
            "brief_description": "Small-scale evaluation of OpenAI API models shows that trivial formatting changes (adding quotes, changing punctuation, swapping 'which'/'what', adding brackets) produce large changes in accuracy; original prompts often perform better than paraphrase-average and may underperform the paraphrase-max.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "text-davinci-002 (td002), text-davinci-003 (td003), davinci, GPT-3.5-Turbo",
            "model_size": null,
            "task_name": "LMENTRY subset (4 tasks in Figures and 10 tasks in small-scale eval)",
            "task_description": "Elementary linguistic tasks from LMENTRY used to probe prompt formatting sensitivity; average and maximal performance estimated using randomized sampling and a greedy search over paraphrases.",
            "problem_format": "Original LMENTRY instruction templates vs. paraphrases (randomly sampled to estimate AvgP, greedy search to estimate MaxP); minimal changes include adding quotes around placeholders, commas-&gt;semicolons, bracket formatting, swapping determiners.",
            "comparison_format": "Original prompt vs paraphrased prompts (many paraphrases) — both average and maximal metrics estimated.",
            "performance": "Representative examples (Table 7): td002: 'Which word has a greater number of letters, [word1] or [word2]?' acc .50 -&gt; with quotes .23 (Δ -0.27); td003 similar example .60 -&gt; .14 (Δ -0.46). Comma-&gt;semicolon change: td002 .08 -&gt; .85 (Δ +0.77); td003 .48 -&gt; .90 (Δ +0.42).",
            "performance_comparison": "Across small-scale eval: in 72.5% of cases original instructions had higher performance than paraphrase-average; davinci original prompts added on average +21 accuracy points relative to the paraphrase-average. For text-davinci-002, some paraphrases improved maximal accuracy above 90% for 8/10 tasks. 26/40 differences across four models were statistically significant by McNemar test.",
            "format_effect_size": "Examples range from -0.46 to +0.77 absolute accuracy change; davinci average +21 percentage points for original vs paraphrase-average; 72.5% of original prompts outperform average.",
            "format_effect_direction": "both improved and reduced performance depending on specific edit and model; overall original prompts often higher than paraphrase-average but many paraphrases can exceed original in MaxP",
            "explanation_or_hypothesis": "Authors suggest minimal punctuation and quoting differences change how the model interprets placeholders and instruction structure; some original prompts may be specially effective (possibly due to training exposure), while other paraphrases unlock better model behavior for MaxP.",
            "counterexample_or_null_result": null,
            "uuid": "e5798.4",
            "source_info": {
                "paper_title": "State of What Art? A Call for Multi-Prompt LLM Evaluation",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "Auto-paraphrase noise null",
            "name_full": "Automatic paraphrase noise has little impact on metric-based rankings",
            "brief_description": "The authors find that noise from automatically generated paraphrases (i.e., occasional incorrect paraphrases) has virtually no impact on model rankings computed by their proposed aggregate metrics (MaxP, AvgP, CPS).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "multiple models (rankings compared before/after filtering paraphrases)",
            "model_size": null,
            "task_name": "All studied tasks (LMENTRY, BBH, BBL)",
            "task_description": "Evaluation of effect of removing incorrect paraphrases on final model rankings using Kendall's τ.",
            "problem_format": "Compare model rankings computed using automatically-generated paraphrase pool vs. manually filtered paraphrase pool.",
            "comparison_format": "Unfiltered auto-generated paraphrases vs. filtered (human-verified) paraphrases.",
            "performance": "Kendall's τ comparing rankings before and after manual filtering is near-perfect to perfect for almost all tasks and metrics (Table 6); one exception due to a buggy LMENTRY script ('ends with word').",
            "performance_comparison": "Table 6 averaged Kendall's Tau values for Max/Avg/Combined perf ranking comparisons were high (e.g., LMENTRY .963/.978/.948 and BBH .991/.983/.966 as averaged Kendall's Tau across tasks after filtering — indicating strong agreement).",
            "format_effect_size": "Near-perfect agreement in rankings (Kendall's τ ~0.96–0.99 averaged across tasks/metrics) before vs after paraphrase filtering.",
            "format_effect_direction": "no effect on metric-based model rankings (i.e., automatic paraphrase noise negligible)",
            "explanation_or_hypothesis": "Most automatically-generated paraphrases are valid and the small fraction of incorrect paraphrases does not change aggregate metric-based rankings; thus automated paraphrase generation is sufficient for multi-prompt evaluation without exhaustive manual verification.",
            "counterexample_or_null_result": "One task ('ends with word' in LMENTRY) showed disagreement attributable to an error in the evaluation script, not paraphrase noise.",
            "uuid": "e5798.5",
            "source_info": {
                "paper_title": "State of What Art? A Call for Multi-Prompt LLM Evaluation",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "Human paraphrase sensitivity (BBL)",
            "name_full": "Sensitivity to human-crafted paraphrases (BIG-bench Lite)",
            "brief_description": "Using human-written paraphrases produced after model training (from Sun et al. 2023) the paper observes similar sensitivity to paraphrasing, showing that effects are not solely due to leaked prompts in training data.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "subset of 11 larger-parameter models evaluated on BBH/BBL",
            "model_size": null,
            "task_name": "BIG-bench Lite (BBL) tasks (human-crafted paraphrases 7–12 per task)",
            "task_description": "BBL tasks covering multiple knowledge domains sampled from BIG-bench, with human-written paraphrases produced after model training.",
            "problem_format": "Original single BBL instruction template vs. multiple manual paraphrases authored post-training.",
            "comparison_format": "Original BBL prompt vs human paraphrases (no training leakage possible).",
            "performance": "Paper reports similar inconsistencies in both absolute scores and rankings when using human paraphrases, with examples of paraphrase pairs producing minimal wording differences but notable ranking changes (illustrated in figures/tables).",
            "performance_comparison": null,
            "format_effect_size": "Qualitative: notable score and ranking changes comparable to those seen with auto-generated paraphrases; exact numeric deltas depend on model/task pair (paper shows examples and minimal-distance pairs with large deltas).",
            "format_effect_direction": "varied (both improvements and degradations observed across paraphrases)",
            "explanation_or_hypothesis": "Because the human paraphrases were authored after model training, this indicates that sensitivity is not explained solely by prompt leakage; rather models are intrinsically sensitive to phrasing and formatting.",
            "counterexample_or_null_result": null,
            "uuid": "e5798.6",
            "source_info": {
                "paper_title": "State of What Art? A Call for Multi-Prompt LLM Evaluation",
                "publication_date_yy_mm": "2023-12"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Evaluating the zero-shot robustness of instruction-tuned language models",
            "rating": 2
        },
        {
            "paper_title": "Quantifying language models' sensitivity to spurious features in prompt design or: How i learned to start worrying about prompt formatting",
            "rating": 2
        },
        {
            "paper_title": "Mind your format: Towards consistent evaluation of in-context learning improvements",
            "rating": 2
        },
        {
            "paper_title": "PromptBench: Towards evaluating the robustness of large language models on adversarial prompts",
            "rating": 1
        },
        {
            "paper_title": "Is prompt all you need? no. a comprehensive and broader view of instruction learning",
            "rating": 1
        }
    ],
    "cost": 0.015747249999999997,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>State of What Art? <br> A Call for Multi-Prompt LLM Evaluation</h1>
<p>Moran Mizrahi ${ }^{\dagger}$ Guy Kaplan ${ }^{\dagger}$ Dan Malkin ${ }^{\dagger}$ Rotem Dror<em> Dafna Shahaf ${ }^{\dagger}$ Gabriel Stanovsky ${ }^{\dagger}$<br>${ }^{\dagger}$ School of Computer Science, The Hebrew University of Jerusalem<br></em>Department of Information Systems, University of Haifa<br>{moran.mizrahi, guy.kaplan2, dan.malkinhueb, gabriel.stanovsky}@mail.huji.ac.il<br>rdror@is.haifa.ac.il, dshahaf@cs.huji.ac.il</p>
<h4>Abstract</h4>
<p>Recent advances in LLMs have led to an abundance of evaluation benchmarks, which typically rely on a single instruction template per task. We create a largescale collection of instruction paraphrases and comprehensively analyze the brittleness introduced by single-prompt evaluations across 6.5 M instances, involving 20 different LLMs and 39 tasks from 3 benchmarks. We find that different instruction templates lead to very different performance, both absolute and relative. Instead, we propose a set of diverse metrics on multiple instruction paraphrases, specifically tailored for different use cases (e.g., LLM vs. downstream development), ensuring a more reliable and meaningful assessment of LLM capabilities. We show that our metrics provide new insights into the strengths and limitations of current LLMs.</p>
<h2>1 Introduction</h2>
<p>Recent years have seen an explosion of large language models (LLMs), which generalize to unseen tasks via natural language instructions. Various LLM evaluation benchmarks, such as BIG-bench and HELM, use a single instruction template per task, evaluating all models against it (Srivastava et al., 2023a; Liang et al., 2023). However, there could be a myriad of ways to phrase an instruction template for a given task; see Figure 1 for examples of different templates for the task of recognizing homophones. Naturally, LLM performance depends on the chosen template.</p>
<p>We explore the question of robustly comparing different models on a given task. We first create a dataset of paraphrased instructions, employing three automatic paraphrasing methods based on recent techniques such as chain-of-thought. We manually verify and filter a large collection of more than 175 paraphrases for different tasks ( 5 K
instruction paraphrases in total), which we make publicly available for future research. ${ }^{1}$</p>
<p>Next, we use our dataset to perform a largescale statistical evaluation of over 6.5 M instances, involving 20 different LLMs and 39 tasks from 3 benchmarks. We find that models perform very differently on different instruction paraphrases. For example, Figure 1 shows four models evaluated on four semantically equivalent prompts, with both absolute and relative performance varying widely; one can even observe cases where the same model performs the best on one instruction and the worst on a semantically equivalent instruction (e.g., GPT-3.5-Turbo on $P_{1}$ vs. $P_{4}$ ). Subsequently, we argue that very little can be said on either absolute or relative performance based on single-instruction evaluation. This may also partially explain why some models seem less accurate in practice than their formal evaluation suggests.</p>
<p>Note that while the claim that evaluating against a single instruction template leads to brittle results is not surprising per se, to the best of our knowledge it has never been subjected to rigorous empirical testing before.</p>
<p>To address the limitations of single-instruction evaluation, we propose to take a step back and consider multi-prompt LLM evaluation - a set of metrics which measure aggregated performance over a set of instruction template paraphrases.</p>
<p>We argue that different use cases should entail different evaluation metrics. For example, LLM developers may be interested in measuring the robustness of performance across multiple instruction templates. In contrast, developers aiming to integrate an LLM into a specific downstream task may be interested in comparing models according to their corresponding top-performing instruction.</p>
<p>We evaluate 20 LLMs with our metrics, finding that their absolute and relative performance differ from results obtained with the benchmarks'</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Evaluation of different OpenAI models on the homophones task from LMENTRY over four paraphrases. Each cluster of columns corresponds to a distinct paraphrased <em>instruction template</em> (see respective texts below; words in bold indicate an instantiation). Despite all instructions being semantically equivalent, both absolute performance and relative ranking vary widely.</p>
<p>Original instructions. We demonstrate that different models excel in different metrics: For instance, in the LMENTRY benchmark, LLaMA-based models are comparable to T5-based models when looking at top-performing instructions, but lag behind when average performance is considered, due to poor performance on a large number of paraphrases. We also show that our automatic paraphrasing method is effective, and there is no need to manually verify the paraphrases.</p>
<p>Our results suggest that future work should use multi-prompt LLM evaluations and choose a metric for aggregating the results according to the <em>extrinsic needs</em> of the evaluators. We hope that our work will help spur more consistency and comparability in LLM evaluation, which is strongly tied to real-world usage of LLMs.</p>
<h2>2 Background and Definitions</h2>
<p>Below we survey how generalization to a new task format is evaluated and compared between LLMs, finding that the common practice involves a single (or very few) task instruction templates. In the rest of the paper, we will argue that such practice leads to brittle, unreliable results.</p>
<p><strong>Task instruction templates.</strong> Following <em>Mishra et al. (2022); Chung et al. (2024)</em>, we separate between task instruction, samples, and input-output exemplars which may be provided during in-context learning. We define an <em>instruction template</em> for a given task as a string with placeholders where the input samples are to be inserted. As seen in Figure 1, the same task can be described using different task instruction templates.</p>
<p><strong>Evaluation benchmarks.</strong> Several recent efforts aim to standardize LLM evaluation. Notable examples include MMLU <em>Hendrycks et al. (2020)</em>, BIG-bench <em>Srivastava et al. (2023a); Suzgun et al. (2023)</em>, and HELM <em>Liang et al. (2023)</em>. In all of these, each task has a single instruction template, against which all models are evaluated. Another benchmark, LMENTRY <em>Efrat et al. (2023)</em>, reports models' average performance on three instruction templates. The instruction templates are provided with these benchmarks, allowing new models to be tested against the same template.</p>
<p>We note that many notable works do not disclose the instruction templates used for evaluation (e.g., LLaMA <em>Touvron et al. (2023)</em>, PALM <em>Chowdhery et al. (2023)</em>, GPT-4 <em>Achiam et al. (2023)</em>, Gemini <em>Team et al. (2023)</em>). While there are reasons to withhold instructions (e.g., avoid potential leakage), this practice exacerbates the challenge of meaningful comparative evaluation.</p>
<p><strong>Prompt robustness.</strong> Related to this study is a line of work measuring LLM's robustness to prompt (or instruction template) modifications. Unlike our work, these typically aim to measure model performance against <em>adversarial</em> paraphrasing approaches. PromptBench <em>Zhu et al. (2023)</em> measures performance on erroneous instructions (e.g., instructions written by non-native English speakers). They then compare performance on perturbed instructions vs. the benchmark's original instructions, which are considered the gold-standard reference. <em>Gu et al. (2023)</em> examined a single LLM's robustness under various instruction perturbations, including word-, sentence-, and instruction-level changes. <em>Sun et al. (2023)</em> show that LLMs perform better on instructions they have seen in training compared to manual paraphrases. We later incorporate their manual paraphrases in our evaluation of BIG-bench Lite.</p>
<p>In contrast to works on prompt robustness, we</p>
<p>analyze the impact of the choice of prompt in terms of both absolute and relative model performance, covering a wide range of models and several different metrics.</p>
<h2>3 Experimental Setup</h2>
<h3>3.1 Tasks</h3>
<p>We evaluate 39 diverse tasks from three evaluation benchmarks, as itemized below.</p>
<p>10 tasks from LMentry (Efrat et al., 2023). LMentry consists of simple linguistic tasks (e.g., "write a word that doesn't contain the letter $l$ "), each accompanied by three associated instruction templates. The tasks are designed to capture explainable and controllable linguistic phenomena. We choose the 10 tasks that received the lowest scores in the original paper, as these more challenging tasks are likely to better highlight the differences between models.</p>
<p>14 tasks from BIG-bench Lite (BBL; Srivastava et al., 2023a). These cover multiple knowledge domains, sampled from the larger BIGBench benchmark (Srivastava et al., 2023b). We focus on a set of 14 tasks studied recently by Sun et al. (2023). Each task in BBL is associated with a single instruction template.</p>
<p>15 tasks from BIG-bench Hard (BBH; Suzgun et al., 2023). This is another curated subset of BIG-bench, containing particularly challenging tasks on which LLM underperform the average human score. We focused on a set of 15 classification and multiple choice tasks to streamline the evaluation process. Each task in BBH is associated with a single instruction template.</p>
<p>Measuring performance. In LMentry we measure performance using the official evaluation script, while in Big-Bench we perform exact string matching. We note that while exact matching is somewhat strict, we believe it is also fair and straightforward.</p>
<h3>3.2 Models</h3>
<p>We evaluate 16 instruction-tuned LLMs from 11 diverse model families (Chung et al., 2024; Sanh et al., 2021; Taori et al., 2023; Zheng et al., 2024; Durbin, 2023; Ding et al., 2023; NousResearch, 2023; Almazrouei et al., 2023; Team, 2023; Collective, 2023) (see Table 1). We refrain from including closed, API-based models (e.g., OpenAI</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">Model size</th>
<th style="text-align: center;">Base model</th>
<th style="text-align: center;"># Params</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Flan-T5</td>
<td style="text-align: center;">Small</td>
<td style="text-align: center;">T5</td>
<td style="text-align: center;">80M</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Base</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">250M</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Large</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">780M</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">XL</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">3B</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">XXL</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">11B</td>
</tr>
<tr>
<td style="text-align: center;">T0</td>
<td style="text-align: center;">Small</td>
<td style="text-align: center;">T5</td>
<td style="text-align: center;">3B</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">T0pp</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">11B</td>
</tr>
<tr>
<td style="text-align: center;">Alpaca</td>
<td style="text-align: center;">Small</td>
<td style="text-align: center;">LLaMA</td>
<td style="text-align: center;">7B</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Big</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">13B</td>
</tr>
<tr>
<td style="text-align: center;">Vicuna</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">LLaMA</td>
<td style="text-align: center;">13B</td>
</tr>
<tr>
<td style="text-align: center;">Airoboros</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">LLaMA</td>
<td style="text-align: center;">13B</td>
</tr>
<tr>
<td style="text-align: center;">UltraLM</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">LLaMA</td>
<td style="text-align: center;">13B</td>
</tr>
<tr>
<td style="text-align: center;">Nous-Hermes</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">LLaMA</td>
<td style="text-align: center;">13B</td>
</tr>
<tr>
<td style="text-align: center;">Falcon-Instruct</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Falcon</td>
<td style="text-align: center;">7B</td>
</tr>
<tr>
<td style="text-align: center;">MPT</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">MPT</td>
<td style="text-align: center;">7B</td>
</tr>
<tr>
<td style="text-align: center;">Minotaur</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">StarCoder Plus</td>
<td style="text-align: center;">15B</td>
</tr>
</tbody>
</table>
<p>Table 1: The different LLMs evaluated in this work, grouped by model family, along with their size, in number of parameters. All models were instruction-tuned.
models) in our main evaluation for two reasons. First, using them at scale is an expensive prospect. For example, running our entire evaluation suite on GPT-4 will cost thousands of dollars. Second, and more importantly, the closed API for these models reportedly manipulates the input prompts in an undisclosed manner (e.g., wrapping them with meta-prompts, or rerouting to other models) (Rao et al., 2023) which interferes with our evaluation. We do however perform a small-scale evaluation of OpenAI models in Section 7 to show that they are also sensitive to prompt paraphrasing.</p>
<h2>4 Evaluating against a Single Prompt Leads to Instability in Results</h2>
<p>As discussed in the previous section, LLMs are usually evaluated against a single instruction template. In this section, we will show that this approach is quite brittle. Indeed, a simple rephrasing of the instruction template can lead to drastic changes in both absolute and relative model performance.</p>
<p>In Section 4.1 we create a large number of automatically-generated instruction paraphrases for tasks from the LMentry and BBH benchmarks. Paraphrases are created using an LLM and verified by human annotators. In Section 4.2, we statistically analyze the performance of various LLMs against these instruction templates and</p>
<table>
<thead>
<tr>
<th>Benchmark</th>
<th>Method</th>
<th>#Automatic</th>
<th>#Correct</th>
<th>Correct</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td>Paraphrases</td>
<td>Paraphrases</td>
<td>Ratio</td>
</tr>
<tr>
<td>LMENTRY</td>
<td>All</td>
<td>2429</td>
<td>2186</td>
<td>90.00%</td>
</tr>
<tr>
<td></td>
<td>Rephrase</td>
<td>461</td>
<td>408</td>
<td>88.50%</td>
</tr>
<tr>
<td></td>
<td>CoT</td>
<td>1286</td>
<td>1234</td>
<td>95.96%</td>
</tr>
<tr>
<td></td>
<td>Gradual</td>
<td>652</td>
<td>514</td>
<td>78.83%</td>
</tr>
<tr>
<td>BBH</td>
<td>All</td>
<td>2615</td>
<td>2209</td>
<td>84.47%</td>
</tr>
<tr>
<td></td>
<td>Rephrase</td>
<td>734</td>
<td>627</td>
<td>85.42%</td>
</tr>
<tr>
<td></td>
<td>CoT</td>
<td>775</td>
<td>630</td>
<td>81.29%</td>
</tr>
<tr>
<td></td>
<td>Gradual</td>
<td>1091</td>
<td>937</td>
<td>85.88%</td>
</tr>
</tbody>
</table>
<p>Table 2: Manual validation and filtering of automatic instruction paraphrases generated for LMENTRY and BBH, showing percentages of valid paraphrases.
quantify the variation in model performance. Finally, in Section 4.3, we show that models exhibit similar brittleness with manually-written paraphrases for tasks from the BBL benchmark.</p>
<h3>4.1 Paraphrasing Instruction Templates</h3>
<p>We use three prompting methods which were found useful in previous works: (1) instruction template rephrasing: asking an LLM to rephrase a seed prompt (Lester et al., 2021; Gonen et al., 2023; Honovich et al., 2023a); (2) Chain-ofThought prompting (Wei et al., 2022): we provided the model with a sequence of steps in which the model is asked first to produce a task description, and then to generate various instruction templates for the task; and (3) Gradual template generation: inspired by Honovich et al. (2023b), we split the COT approach into three LLM calls. The first for generating a task description from a seed instruction template, the second for generating instruction provided by input-output examples, and the third for processing the instruction and examples into an instruction template.</p>
<p>In all of the above, we use GPT-3.5-Turbo for generation, and the original instruction templates for each of our tasks to seed these three generation methods, resulting on average in more than 200 automatically-generated instruction template paraphrases for each of our tasks (see Table 2). We make this collection, as well as the code used to generate it, publicly available for reproducibility and to enable future work.</p>
<p>Manual validation and filtering of automatic instruction paraphrases. All automatically generated paraphrases were manually verified and filtered by an annotator from our group to ensure their coherence and relevance to the task. A</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Benchmark</th>
<th style="text-align: center;">Correct (\%)</th>
<th style="text-align: center;">Agreement <br> (accuracy)</th>
<th style="text-align: center;">Agreement <br> (Cohen's $\kappa$ )</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">LMENTRY</td>
<td style="text-align: center;">86.0</td>
<td style="text-align: center;">.953</td>
<td style="text-align: center;">.774</td>
</tr>
<tr>
<td style="text-align: left;">BBH</td>
<td style="text-align: center;">86.7</td>
<td style="text-align: center;">.916</td>
<td style="text-align: center;">.491</td>
</tr>
</tbody>
</table>
<p>Table 3: Human evaluation of doubly annotated paraphrases. Out of 375 automatically generated instructions, more than $85 \%$ were found to be correct by both annotators. Both Cohen's $\kappa$ and the agreement accuracy indicate varying, yet generally high levels of agreement given pronounced label imbalance.
portion of the data involving 15 randomly selected templates from each task, totaling in 375 instructions, was also given to a second annotator; results show reliable agreement (Table 3), indicating our evaluation process is calibrated.</p>
<p>See Table 2 for a fine-grained distribution across the different generation metrics. Overall, we found that $90 \%$ of the generated paraphrases created for LMENTRY were correct, and roughly $84 \%$ of the paraphrases for BBH were correct.</p>
<p>On average, the validation process yields 240 validated instruction paraphrases per task for LMENTRY and 175 paraphrases per task for BBH. Next, we use these paraphrases to quantify performance variability due to instruction template paraphrasing across $\sim 6.5 M$ instances. ${ }^{2}$</p>
<h3>4.2 Quantifying Performance Variance due to Instruction Paraphrasing</h3>
<p>We leverage the collection of validated paraphrases to assess how model performance varies with paraphrasing. Our main finding is that the common approach of evaluating against a single propmt is unstable, leading to unreliable results.</p>
<p>Instance sampling and prompt construction. Our study involves a large number of tasks, models, and instruction paraphrases. However, evaluating LLMs can become prohibitively expensive with the increase of the number of samples, datasets, models, and instruction templates (Perlitz et al., 2023). To make our evaluation feasible, we chose to evaluate each instruction template on a randomly selected subset of 100 task samples. Furthermore, we found that all models struggle on</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>BBH, beyond the point of meaningful comparison. To address this, we evaluate 11 out of the 16 models on it (the ones with the largest number of parameters), and add an example of the prediction format to all instruction template paraphrases.</p>
<p>Examining the effect of few-shot learning is beyond the scope of this paper, however, sclar2023, weber2023 and voronov2024 recently observed similar performance sensibility when introducing varying number of incontext examples.</p>
<p>Using a single-instruction template leads to brittle ranking. We compute Kendall's $W$ : $\mathbb{N}^{m \times n} \mapsto[0,1]$ Kendall1939, a nonparametric statistic which measures the ranking correlation between $m$ judges (instruction templates, in our case) ranking $n$ objects (LLMs, in our case) by calculating the squared deviation between the sum of ranks of different judges ( $R_{i}=$ $\sum_{j=1}^{m} r_{i j}$ ) and their mean value:</p>
<p>$$
W=\frac{12 \sum_{i=1}^{n}\left(R_{i}-\bar{R}\right)^{2}}{m^{2}\left(n^{3}-n\right)}
$$</p>
<p>Kendall's $W$ would be 1 for all tasks if model ranking were the same among all instruction templates (in other words, they are interchangeable for the sake of evaluation). In contrast, the more $W$ approaches 0 , the lesser the rankings induced by different instructions agree.</p>
<p>The results (Table 4) demonstrate that a single instruction template leads to unreliable rankings for many of the tasks, with 10 of the tasks exhibiting only slight to moderate ranking agreement, and only two exhibiting strong agreement. To complement the analysis, we performed Friedman test with tied data corder2011, showing that different instructions lead to statistically significant differences in performance for 21 out of the 25 tasks.</p>
<p>Examples of differences in model ranking. We illustrate the implications of ranking differences in Figure 2. In all three cases, $P_{1}$ and $P_{2}$ are valid paraphrases, yet they lead to vastly different rankings. For example, T0pp ranks first on the BBH task (center) according to $P_{1}$ and only 9th according to $P_{2}$. Similarly, Alpaca-13B and Alpaca-7B are in the top-performing models on the LMENTRY task $P_{2}$, while they rank last for $P_{1}$.</p>
<p>We quantify the difference between two rankings with Kendall's $\tau: \mathbb{N}^{n} \times \mathbb{N}^{n} \mapsto[-1,1]$, which</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Tasks</th>
<th style="text-align: center;">Kendall's W</th>
<th style="text-align: center;">Friedman p-val</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">LMENTRY</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">not containing</td>
<td style="text-align: center;">.271 (weak)</td>
<td style="text-align: center;">$0.0^{*}$</td>
</tr>
<tr>
<td style="text-align: center;">word before</td>
<td style="text-align: center;">.367 (weak)</td>
<td style="text-align: center;">$0.0^{*}$</td>
</tr>
<tr>
<td style="text-align: center;">first alphabet</td>
<td style="text-align: center;">.436 (weak)</td>
<td style="text-align: center;">$0.0^{*}$</td>
</tr>
<tr>
<td style="text-align: center;">less letters</td>
<td style="text-align: center;">.485 (weak)</td>
<td style="text-align: center;">$0.0^{*}$</td>
</tr>
<tr>
<td style="text-align: center;">rhyming word</td>
<td style="text-align: center;">.496 (weak)</td>
<td style="text-align: center;">$0.0^{*}$</td>
</tr>
<tr>
<td style="text-align: center;">ends with word</td>
<td style="text-align: center;">.518 (weak)</td>
<td style="text-align: center;">$0.0^{*}$</td>
</tr>
<tr>
<td style="text-align: center;">homophones</td>
<td style="text-align: center;">.518 (weak)</td>
<td style="text-align: center;">$0.0^{*}$</td>
</tr>
<tr>
<td style="text-align: center;">all words</td>
<td style="text-align: center;">.522 (weak)</td>
<td style="text-align: center;">$0.0^{*}$</td>
</tr>
<tr>
<td style="text-align: center;">any words</td>
<td style="text-align: center;">.527 (weak)</td>
<td style="text-align: center;">$0.0^{*}$</td>
</tr>
<tr>
<td style="text-align: center;">more letters</td>
<td style="text-align: center;">.540 (weak)</td>
<td style="text-align: center;">$0.0^{*}$</td>
</tr>
<tr>
<td style="text-align: center;">BIG-bench Hard</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">recommendations</td>
<td style="text-align: center;">.628 (medium)</td>
<td style="text-align: center;">.897</td>
</tr>
<tr>
<td style="text-align: center;">formal fallacies</td>
<td style="text-align: center;">.704 (medium)</td>
<td style="text-align: center;">5.6E-13</td>
</tr>
<tr>
<td style="text-align: center;">geometric shapes</td>
<td style="text-align: center;">.710 (medium)</td>
<td style="text-align: center;">.167</td>
</tr>
<tr>
<td style="text-align: center;">hyperbaton</td>
<td style="text-align: center;">.730 (medium)</td>
<td style="text-align: center;">1.0E-4</td>
</tr>
<tr>
<td style="text-align: center;">logical deduction 3</td>
<td style="text-align: center;">.740 (medium)</td>
<td style="text-align: center;">4.9E-16</td>
</tr>
<tr>
<td style="text-align: center;">disambiguation qa</td>
<td style="text-align: center;">.764 (medium)</td>
<td style="text-align: center;">2.1E-17</td>
</tr>
<tr>
<td style="text-align: center;">ruin names</td>
<td style="text-align: center;">.776 (medium)</td>
<td style="text-align: center;">.366</td>
</tr>
<tr>
<td style="text-align: center;">logical deduction 7</td>
<td style="text-align: center;">.778 (medium)</td>
<td style="text-align: center;">1.4E-13</td>
</tr>
<tr>
<td style="text-align: center;">translation error</td>
<td style="text-align: center;">.800 (medium)</td>
<td style="text-align: center;">6.9E-9</td>
</tr>
<tr>
<td style="text-align: center;">logical deduction 5</td>
<td style="text-align: center;">.818 (medium)</td>
<td style="text-align: center;">3.0E-9</td>
</tr>
<tr>
<td style="text-align: center;">snarks</td>
<td style="text-align: center;">.823 (medium)</td>
<td style="text-align: center;">.604</td>
</tr>
<tr>
<td style="text-align: center;">penguins in a table</td>
<td style="text-align: center;">.830 (medium)</td>
<td style="text-align: center;">7.3E-15</td>
</tr>
<tr>
<td style="text-align: center;">navigate</td>
<td style="text-align: center;">.838 (medium)</td>
<td style="text-align: center;">5.6E-10</td>
</tr>
<tr>
<td style="text-align: center;">causal judgement</td>
<td style="text-align: center;">.851 (strong)</td>
<td style="text-align: center;">4.9E-7</td>
</tr>
<tr>
<td style="text-align: center;">sports</td>
<td style="text-align: center;">.873 (strong)</td>
<td style="text-align: center;">8.0E-13</td>
</tr>
<tr>
<td style="text-align: center;">BIG-bench Lite</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">known unknown</td>
<td style="text-align: center;">.316 (weak)</td>
<td style="text-align: center;">4.4E-5</td>
</tr>
<tr>
<td style="text-align: center;">play dialog</td>
<td style="text-align: center;">.355 (weak)</td>
<td style="text-align: center;">4.3E-5</td>
</tr>
<tr>
<td style="text-align: center;">winowhy</td>
<td style="text-align: center;">.520 (weak)</td>
<td style="text-align: center;">6.0E-4</td>
</tr>
<tr>
<td style="text-align: center;">strategic qa</td>
<td style="text-align: center;">.529 (weak)</td>
<td style="text-align: center;">.191</td>
</tr>
<tr>
<td style="text-align: center;">hindu knowledge</td>
<td style="text-align: center;">.560 (weak)</td>
<td style="text-align: center;">.569</td>
</tr>
<tr>
<td style="text-align: center;">conceptual</td>
<td style="text-align: center;">.731 (medium)</td>
<td style="text-align: center;">.132</td>
</tr>
<tr>
<td style="text-align: center;">strange stories</td>
<td style="text-align: center;">.731 (medium)</td>
<td style="text-align: center;">.431</td>
</tr>
<tr>
<td style="text-align: center;">code desc</td>
<td style="text-align: center;">.756 (medium)</td>
<td style="text-align: center;">.002</td>
</tr>
<tr>
<td style="text-align: center;">novel concepts</td>
<td style="text-align: center;">.787 (medium)</td>
<td style="text-align: center;">.620</td>
</tr>
<tr>
<td style="text-align: center;">logic grid puzzle</td>
<td style="text-align: center;">.796 (medium)</td>
<td style="text-align: center;">.010</td>
</tr>
<tr>
<td style="text-align: center;">lang. identification</td>
<td style="text-align: center;">.811 (medium)</td>
<td style="text-align: center;">.002</td>
</tr>
<tr>
<td style="text-align: center;">vitamins</td>
<td style="text-align: center;">.888 (strong)</td>
<td style="text-align: center;">.772</td>
</tr>
<tr>
<td style="text-align: center;">bbq lite</td>
<td style="text-align: center;">.890 (strong)</td>
<td style="text-align: center;">.023</td>
</tr>
<tr>
<td style="text-align: center;">logical deduction</td>
<td style="text-align: center;">.913 (strong)</td>
<td style="text-align: center;">.895</td>
</tr>
</tbody>
</table>
<p>Table 4: Kendall's $W \in[0,1]$ values for all tasks sorted in ascending order. The smaller the value of $W$ the more that the ranking on different prompts is de-correlated. Most $W$ are smaller than 0.85 , indicating weak to moderate agreement. The pvalues from Friedman test indicate significant differences between rankings of models when using different prompts. ${ }^{*} \mathrm{p}$-values of 0 represent statistical significance levels that are smaller than 1E-50.
estimates the agreement between two specific instruction templates which induce rankings $R_{1}, R_{2}$ over $n$ LLMs, formally defined as kendall1945:</p>
<p>$$
\tau_{b}=\frac{P-Q}{\sqrt{(P+Q+T) \cdot(P+Q+U)}}
$$</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Model performance and ranking induced by pairs of paraphrases that exhibit the minimal Kendall τ correlation on three different tasks (one for each benchmark). For each template pair, models are ordered according to their performance against the first instruction template P₁, enabling straightforward comparisons of ranking changes. In other words, if the bars of P₂ appear scattered rather than follow a clear descending order, this indicates a significant reshuffling of rankings.</p>
<p>Where P is the number of concordant pairs, Q is the number of discordant pairs, T is the number of ties in the first ranking, and U is the number of ties in the second ranking. Therefore, τ &gt; 0 indicates that most pairs are concordant (with τ = 1 indicating perfect agreement), and τ &lt; 0 indicates that most pairs are discordant (with τ = −1 indicating perfect disagreement). Overall, 15 tasks out of 25 have instruction template paraphrases with negative Kendall's τ, indicating mostly disagreeing LLM rankings.</p>
<p>Absolute model performance varies widely on single-instruction templates. Aside from vastly different relative model rankings, instruction template paraphrases often result in varying absolute model performances. To quantify this variance, we calculated <em>divergence</em>, defined as the number of standard deviations by which the performance, as assessed using the original instruction templates, deviates from the model's average performance over all paraphrases.</p>
<p>The results in Figure 3 reveal noticeable divergence for the LMENTRY benchmark, defined as surpassing one standard deviation (Kazmier et al., 2003). For instance, the performance of the Alpaca-13B with the original instruction templates outperformed its average performance by more than one standard deviation in 7 out of 10 LMENTRY tasks. For lack of space, the figure does not depict the BBH benchmark, but similar patterns of divergence were observed there as well.</p>
<p>In line with Lou et al. (2023), we find that major differences in performance can occur even for very similar paraphrase pairs. For example, the Flan-T5-large model demonstrated an average performance degradation of 28% when changing the word 'excludes' to 'lacks', while the Flan-T5-XL model showed an average performance improvement of 46% on that same edit. See a comprehensive edit distance comparison in Figure 4 and Table 5.</p>
<h3>4.3 LLMs are also Sensitive to Manual Paraphrases</h3>
<p>Inconsistencies observed in our analyses could stem from paraphrases that leaked to the training of the models. To address this, we extended our analysis with instruction paraphrases which were recently written by Sun et al. (2023) for the BBL tasks (7-12 instruction templates per task). Importantly, these human-crafted paraphrases were written <em>after</em> model training.</p>
<table>
<thead>
<tr>
<th>L</th>
<th>L</th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>T1</td>
<td>T2</td>
<td>T3</td>
<td>T4</td>
<td>T5</td>
<td>T6</td>
<td>T7</td>
<td>T8</td>
<td>T9</td>
<td>T10</td>
<td></td>
</tr>
<tr>
<td>t03b</td>
<td>1.97</td>
<td>0.24</td>
<td>0.19</td>
<td>0.87</td>
<td>-0.09</td>
<td>0.35</td>
<td>0.42</td>
<td>-0.05</td>
<td>-0.88</td>
<td>-0.48</td>
<td></td>
</tr>
<tr>
<td>t0pp</td>
<td>0.20</td>
<td>-0.03</td>
<td>-0.27</td>
<td>-0.12</td>
<td>0.16</td>
<td>0.28</td>
<td>0.46</td>
<td>-0.23</td>
<td>-0.71</td>
<td>-0.71</td>
<td></td>
</tr>
<tr>
<td>fal7b</td>
<td>-0.15</td>
<td>0.04</td>
<td>-0.60</td>
<td>1.08</td>
<td>-0.86</td>
<td>1.17</td>
<td>0.97</td>
<td>0.02</td>
<td>2.40</td>
<td>2.00</td>
<td></td>
</tr>
<tr>
<td>mpt7b</td>
<td>1.98</td>
<td>-2.08</td>
<td>1.30</td>
<td>-0.32</td>
<td>-0.02</td>
<td>-0.04</td>
<td>-0.82</td>
<td>-1.47</td>
<td>-0.84</td>
<td>-0.28</td>
<td></td>
</tr>
<tr>
<td>alp7b</td>
<td>1.48</td>
<td>1.74</td>
<td>-0.53</td>
<td>2.14</td>
<td>-0.54</td>
<td>1.79</td>
<td>2.42</td>
<td>1.04</td>
<td>-0.35</td>
<td>-0.44</td>
<td></td>
</tr>
<tr>
<td>alp13b</td>
<td>1.80</td>
<td>2.41</td>
<td>0.48</td>
<td>1.91</td>
<td>-0.62</td>
<td>2.15</td>
<td>1.55</td>
<td>1.04</td>
<td>-0.11</td>
<td>1.96</td>
<td></td>
</tr>
<tr>
<td>ft5-small</td>
<td>-0.29</td>
<td>0.26</td>
<td>-0.46</td>
<td>-0.79</td>
<td>-0.01</td>
<td>0.18</td>
<td>0.18</td>
<td>-0.22</td>
<td>-0.80</td>
<td>-0.31</td>
<td></td>
</tr>
<tr>
<td>ft5-base</td>
<td>-0.53</td>
<td>0.08</td>
<td>-0.82</td>
<td>0.14</td>
<td>-0.57</td>
<td>0.64</td>
<td>0.51</td>
<td>-0.03</td>
<td>-0.89</td>
<td>-0.34</td>
<td></td>
</tr>
<tr>
<td>ft5-large</td>
<td>0.58</td>
<td>0.30</td>
<td>-0.64</td>
<td>0.21</td>
<td>0.39</td>
<td>0.16</td>
<td>0.33</td>
<td>-0.04</td>
<td>-1.22</td>
<td>0.73</td>
<td></td>
</tr>
<tr>
<td>ft5-xl</td>
<td>1.06</td>
<td>0.71</td>
<td>-0.25</td>
<td>2.43</td>
<td>-0.39</td>
<td>0.57</td>
<td>0.47</td>
<td>-0.13</td>
<td>-1.73</td>
<td>-0.12</td>
<td></td>
</tr>
<tr>
<td>ft5-xxl</td>
<td>0.46</td>
<td>0.56</td>
<td>0.03</td>
<td>0.68</td>
<td>0.05</td>
<td>0.67</td>
<td>0.79</td>
<td>-0.08</td>
<td>-3.77</td>
<td>0.09</td>
<td></td>
</tr>
<tr>
<td>air13b</td>
<td>0.41</td>
<td>1.34</td>
<td>0.74</td>
<td>0.28</td>
<td>0.03</td>
<td>1.10</td>
<td>1.20</td>
<td>-0.21</td>
<td>0.29</td>
<td>-0.51</td>
<td></td>
</tr>
<tr>
<td>nou13b</td>
<td>0.59</td>
<td>0.23</td>
<td>0.26</td>
<td>-0.09</td>
<td>-0.68</td>
<td>1.24</td>
<td>1.23</td>
<td>-0.73</td>
<td>-0.52</td>
<td>0.92</td>
<td></td>
</tr>
<tr>
<td>ult13b</td>
<td>0.41</td>
<td>-0.09</td>
<td>-0.35</td>
<td>0.90</td>
<td>0.11</td>
<td>1.50</td>
<td>1.08</td>
<td>-0.48</td>
<td>-0.18</td>
<td>0.38</td>
<td></td>
</tr>
<tr>
<td>vic13b</td>
<td>1.60</td>
<td>2.19</td>
<td>0.52</td>
<td>0.82</td>
<td>-0.52</td>
<td>2.53</td>
<td>1.97</td>
<td>-0.14</td>
<td>0.02</td>
<td>0.88</td>
<td></td>
</tr>
<tr>
<td>min15b</td>
<td>-0.74</td>
<td>-1.55</td>
<td>-0.14</td>
<td>0.22</td>
<td>-0.42</td>
<td>1.78</td>
<td>1.59</td>
<td>-0.57</td>
<td>-0.51</td>
<td>0.45</td>
<td></td>
</tr>
</tbody>
</table>
<p>Table 3: The number of standard deviations by which performance of each model on the original instructions deviates from averaged performance. Dark cells indicate substantial divergence values (&gt;1 std).</p>
<p>We use these annotations to examine model performance. Our analysis revealed similar inconsistencies as observed with automated paraphrases, demonstrating model sensitivity to paraphrasing even when the potential for instruction leakage is minimized. See Table 4 for the Kendall’s W values for all BBL tasks, and Figure 2 for a pair of instruction templates exhibiting the minimal Kendall’s $\tau$ correlation across all BBL tasks.</p>
<h2>5 Different Use Cases Merit Different Metrics</h2>
<p>We have shown that LLM performance is greatly affected by paraphrasing of instruction templates. This calls into question current evaluation practices, which typically rely on LLM performance on a single instruction template. In this section we explore ways to evaluate LLMs using a diverse set of instruction templates.</p>
<p>Most importantly, we argue that the answer should depend on the purpose of the evaluation, and that different extrinsic needs should lead to different evaluation metrics, rather than striving for a coarse catch-all metric. We introduce a set of metrics, each tailored to specific scenarios and realistic user needs.</p>
<p>Notations. In the following, $M$ is a pretrained LLM, $T=\left{\left(x_{i},y_{i}\right)\right}$ denotes an evaluation dataset for $M$, $I_{T}$ is a set of natural language task instruction paraphrases for $T$ (e.g., obtained via automatic paraphrasing), and $\varepsilon(M,T,i)\in[0,1]$ denotes the aggregated performance of $M$ on samples from $T$, using a single instruction template $i\in I_{T}$ according to a standard metric, e.g., accuracy or $F_{1}$.</p>
<h3>5.1 Maximum Performance Metric – For Particular Downstream Applications</h3>
<p>We define the maximum performance (MaxP) of a model $M$ on task $T$ to be the maximum individual instruction template performance this model achieves across all instruction templates:</p>
<p>$\operatorname{MaxP}\left(M,T,I_{T}\right)=\max_{i\in I_{T}} \varepsilon(M,T,i)$</p>
<p>Use case: This metric is useful for developers aiming to integrate an LLM into a specific downstream task and domain (e.g., sentiment analysis in the news domain). In such cases, a user input is often embedded within a fixed instruction template. As such, it makes sense to find the best-performing instruction template for a given model <em>Wei et al. (2021)</em>. To mitigate overfitting, we advise developers to use a new sample set for the task. This ensures the chosen prompt is validated by its ability to maximize performance on these held-out samples irrespective of prior exposure during training.</p>
<h3>5.2 Average Performance Metric – For LLM Developers</h3>
<p>We define the average performance (AvgP) of a model $M$ on task $T$ as the mean of the individual instruction template performances over all instruction templates for the task:</p>
<p>$\operatorname{AvgP}\left(M,T,I_{T}\right)=\frac{1}{\left|I_{T}\right|} \cdot \sum_{i \in I_{T}} \varepsilon(M,T,i)$</p>
<p>Use case: Average prompt performance is useful for assessing model robustness to paraphrases. We believe this should be standard practice for LLM developers when presenting the performance of a new LLM on a range of tasks and prompt paraphrases <em>Le Scao et al. (2022)</em>, as it mitigates outliers in performance.</p>
<h3>5.3 Combined Performance Score</h3>
<p>In the same way the F1 score combines precision and recall into a single metric, we propose a Combined Performance Score (CPS) that unites</p>
<table>
<thead>
<tr>
<th>Change</th>
<th>t03b</th>
<th>t0pp</th>
<th>fa17b</th>
<th>mpt7b</th>
<th>a17b</th>
<th>a113b</th>
<th>ft5s</th>
<th>ft5b</th>
<th>ft5l</th>
<th>ft5x1</th>
<th>ft5xx1</th>
<th>ai13b</th>
<th>no13b</th>
<th>u113b</th>
<th>vi13b</th>
<th>mi15b</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>-0.01</td>
<td>-0.07</td>
<td>0.13</td>
<td>0.02</td>
<td>0.00</td>
<td>0.16</td>
<td>-0.03</td>
<td>0.00</td>
<td>-0.02</td>
<td>-0.13</td>
<td>0.01</td>
<td>-0.01</td>
<td>-0.02</td>
<td>0.06</td>
<td>0.01</td>
<td>0.01</td>
</tr>
<tr>
<td></td>
<td>0.00</td>
<td>0.05</td>
<td>0.01</td>
<td>0.01</td>
<td>0.06</td>
<td>-0.19</td>
<td>0.01</td>
<td>-0.02</td>
<td>0.02</td>
<td>0.00</td>
<td>-0.01</td>
<td>0.00</td>
<td>0.07</td>
<td>0.00</td>
<td>-0.01</td>
<td>0.00</td>
</tr>
<tr>
<td></td>
<td>-0.03</td>
<td>0.01</td>
<td>0.00</td>
<td>0.01</td>
<td>0.00</td>
<td>0.04</td>
<td>-0.11</td>
<td>-0.05</td>
<td>0.06</td>
<td>-0.04</td>
<td>0.06</td>
<td>0.03</td>
<td>0.00</td>
<td>0.04</td>
<td>0.01</td>
<td>0.00</td>
</tr>
<tr>
<td></td>
<td>-0.03</td>
<td>0.04</td>
<td>-0.01</td>
<td>0.04</td>
<td>-0.01</td>
<td>0.14</td>
<td>-0.06</td>
<td>0.00</td>
<td>0.10</td>
<td>-0.01</td>
<td>0.04</td>
<td>0.02</td>
<td>0.03</td>
<td>0.02</td>
<td>0.05</td>
<td>0.00</td>
</tr>
<tr>
<td></td>
<td>-0.09</td>
<td>-0.30</td>
<td>0.00</td>
<td>-0.56</td>
<td>-0.01</td>
<td>0.00</td>
<td>0.00</td>
<td>0.04</td>
<td>0.00</td>
<td>-0.01</td>
<td>0.02</td>
<td>-0.34</td>
<td>-0.20</td>
<td>-0.29</td>
<td>-0.01</td>
<td>-0.61</td>
</tr>
<tr>
<td></td>
<td>0.09</td>
<td>-0.02</td>
<td>0.18</td>
<td>-0.08</td>
<td>0.01</td>
<td>0.02</td>
<td>0.12</td>
<td>-0.03</td>
<td>0.10</td>
<td>-0.05</td>
<td>0.06</td>
<td>0.09</td>
<td>0.09</td>
<td>-0.02</td>
<td>-0.02</td>
<td>-0.03</td>
</tr>
<tr>
<td></td>
<td>0.04</td>
<td>0.00</td>
<td>0.09</td>
<td>0.02</td>
<td>0.02</td>
<td>0.00</td>
<td>0.04</td>
<td>0.09</td>
<td>0.08</td>
<td>0.01</td>
<td>-0.02</td>
<td>0.14</td>
<td>-0.07</td>
<td>-0.07</td>
<td>-0.01</td>
<td>-0.05</td>
</tr>
<tr>
<td></td>
<td>0.00</td>
<td>-0.06</td>
<td>0.42</td>
<td>0.01</td>
<td>0.01</td>
<td>0.13</td>
<td>-0.02</td>
<td>-0.01</td>
<td>-0.25</td>
<td>0.01</td>
<td>0.02</td>
<td>0.02</td>
<td>0.10</td>
<td>-0.03</td>
<td>0.07</td>
<td>0.01</td>
</tr>
<tr>
<td></td>
<td>0.00</td>
<td>-0.02</td>
<td>0.14</td>
<td>-0.01</td>
<td>0.04</td>
<td>0.02</td>
<td>-0.03</td>
<td>0.02</td>
<td>-0.02</td>
<td>0.05</td>
<td>0.00</td>
<td>0.01</td>
<td>-0.08</td>
<td>0.13</td>
<td>-0.10</td>
<td>0.02</td>
</tr>
<tr>
<td></td>
<td>0.01</td>
<td>0.06</td>
<td>-0.01</td>
<td>-0.03</td>
<td>0.00</td>
<td>0.13</td>
<td>-0.02</td>
<td>-0.02</td>
<td>-0.03</td>
<td>0.07</td>
<td>-0.01</td>
<td>-0.02</td>
<td>0.02</td>
<td>-0.02</td>
<td>0.06</td>
<td>0.01</td>
</tr>
<tr>
<td></td>
<td>0.00</td>
<td>0.02</td>
<td>0.27</td>
<td>-0.02</td>
<td>0.02</td>
<td>0.05</td>
<td>-0.03</td>
<td>0.00</td>
<td>0.01</td>
<td>0.18</td>
<td>0.00</td>
<td>-0.01</td>
<td>0.05</td>
<td>0.18</td>
<td>-0.01</td>
<td>0.01</td>
</tr>
<tr>
<td></td>
<td>0.01</td>
<td>0.01</td>
<td>0.07</td>
<td>0.00</td>
<td>-0.01</td>
<td>-0.02</td>
<td>0.00</td>
<td>0.01</td>
<td>0.00</td>
<td>0.05</td>
<td>-0.04</td>
<td>-0.02</td>
<td>0.06</td>
<td>0.14</td>
<td>-0.12</td>
<td>0.02</td>
</tr>
<tr>
<td></td>
<td>0.03</td>
<td>0.00</td>
<td>0.22</td>
<td>0.14</td>
<td>0.02</td>
<td>0.03</td>
<td>0.01</td>
<td>-0.04</td>
<td>-0.01</td>
<td>-0.04</td>
<td>-0.03</td>
<td>0.11</td>
<td>0.06</td>
<td>0.15</td>
<td>0.12</td>
<td>0.17</td>
</tr>
<tr>
<td></td>
<td>0.01</td>
<td>0.02</td>
<td>0.00</td>
<td>0.01</td>
<td>0.00</td>
<td>-0.02</td>
<td>0.23</td>
<td>0.13</td>
<td>0.10</td>
<td>0.03</td>
<td>-0.03</td>
<td>-0.02</td>
<td>0.03</td>
<td>0.04</td>
<td>-0.02</td>
<td>0.02</td>
</tr>
<tr>
<td></td>
<td>0.00</td>
<td>0.01</td>
<td>0.02</td>
<td>0.15</td>
<td>0.01</td>
<td>0.02</td>
<td>0.03</td>
<td>0.00</td>
<td>0.03</td>
<td>-0.03</td>
<td>0.04</td>
<td>-0.02</td>
<td>0.00</td>
<td>0.04</td>
<td>0.02</td>
<td>0.00</td>
</tr>
<tr>
<td></td>
<td>0.01</td>
<td>0.01</td>
<td>-0.01</td>
<td>0.12</td>
<td>-0.01</td>
<td>-0.02</td>
<td>-0.01</td>
<td>-0.02</td>
<td>-0.04</td>
<td>0.00</td>
<td>-0.02</td>
<td>0.02</td>
<td>0.08</td>
<td>-0.02</td>
<td>0.00</td>
<td>-0.02</td>
</tr>
<tr>
<td></td>
<td>0.12</td>
<td>-0.06</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>-0.11</td>
<td>0.01</td>
<td>0.02</td>
<td>0.02</td>
<td>0.14</td>
<td>0.02</td>
<td>0.02</td>
<td>-0.01</td>
<td>-0.02</td>
<td>0.00</td>
<td>-0.08</td>
</tr>
<tr>
<td></td>
<td>0.00</td>
<td>0.10</td>
<td>-0.08</td>
<td>0.00</td>
<td>0.00</td>
<td>0.04</td>
<td>0.13</td>
<td>-0.01</td>
<td>-0.01</td>
<td>0.16</td>
<td>0.01</td>
<td>0.03</td>
<td>0.13</td>
<td>0.21</td>
<td>-0.02</td>
<td>-0.01</td>
</tr>
<tr>
<td></td>
<td>0.00</td>
<td>0.07</td>
<td>0.15</td>
<td>0.00</td>
<td>0.00</td>
<td>0.09</td>
<td>0.12</td>
<td>-0.04</td>
<td>-0.28</td>
<td>0.46</td>
<td>-0.01</td>
<td>0.02</td>
<td>-0.07</td>
<td>0.06</td>
<td>-0.07</td>
<td>-0.01</td>
</tr>
<tr>
<td></td>
<td>0.00</td>
<td>0.05</td>
<td>-0.18</td>
<td>0.00</td>
<td>0.00</td>
<td>-0.11</td>
<td>0.03</td>
<td>0.00</td>
<td>0.04</td>
<td>-0.22</td>
<td>0.02</td>
<td>-0.02</td>
<td>0.27</td>
<td>0.32</td>
<td>0.07</td>
<td>-0.02</td>
</tr>
<tr>
<td></td>
<td>0.02</td>
<td>-0.02</td>
<td>-0.32</td>
<td>0.01</td>
<td>0.00</td>
<td>0.00</td>
<td>0.03</td>
<td>-0.03</td>
<td>-0.01</td>
<td>0.11</td>
<td>0.06</td>
<td>-0.05</td>
<td>0.10</td>
<td>-0.01</td>
<td>0.01</td>
<td>-0.01</td>
</tr>
<tr>
<td></td>
<td>-0.09</td>
<td>-0.11</td>
<td>0.45</td>
<td>-0.01</td>
<td>0.00</td>
<td>-0.04</td>
<td>-0.02</td>
<td>0.00</td>
<td>0.13</td>
<td>-0.11</td>
<td>-0.05</td>
<td>0.02</td>
<td>0.05</td>
<td>0.25</td>
<td>-0.01</td>
<td>0.00</td>
</tr>
</tbody>
</table>
<p>Figure 4: Average performance differences between various models for the most common minimal edits between two instruction templates (e.g., substituting ‘excludes’ with ‘lacks’) in the LMENTRY benchmark.
the maximum and average performance metrics to capture both peak capability and robustness of the model across prompts. To define CPS, we first introduce a model saturation score:</p>
<p>$$
\operatorname{Sat}\left(M, T, I_{T}\right)=1-\left(\operatorname{MaxP}-A v g P\right)
$$</p>
<p>This score measures how closely the model's best performance aligns with its average performance. A high saturation score indicates that the model's performance does not drop significantly for nonoptimal instructions. Then, the CPS is calculated as the product of the model's best performance $(M a x P)$ and its saturation $(\operatorname{Sat})$ :</p>
<p>$$
\operatorname{CPS}\left(M, T, I_{T}\right)=\operatorname{Sat} \cdot M a x P
$$</p>
<p>Use case: This metric is valuable for selecting a model for a suite of applications or a platform offering diverse tasks. For instance, when integrating an LLM into an application with uservisible prompts, such as a multi-functional chatbot, it is crucial for the model to be both effective (high $M a x P$ ) and robust (high Sat). CPS facilitates identifying models that strike a balance between top-tier performance and robust reliability across varying instruction templates.</p>
<h2>6 Multi-Prompt Evaluation</h2>
<p>In Figure 6 we evaluate all our 16 models according to the metrics we proposed in the previous sec-
tion, on sample tasks from each of the three benchmarks (full results for all tasks are available in our repository). We report several interesting observations. First, we find that all aggregate metrics diverge from the performance on the original instruction templates. For the vast majority of the tasks in our study, the top three models determined by the original instruction templates were different from those which ranked first according to the average and maximum metrics.</p>
<p>More broadly, model ranking depended on the metric used. For instance, see Figure 6 (top): In LMENTRY's rhyming word task, Falcon-Instruct7b and Vicuna-13b rank first according to MaxP ( 0.74 , gray and yellow bars), but their average performances $A v g P$ are only 0.17 and 0.15 , respectively. Similarly, across all tasks in the LMENTRY benchmark, LLaMA-based models were competitive with T5-based models in terms of $M a x P$. However, in terms of $A v g P$, they tended to lag behind, due to extremely poor performance on a large number of paraphrases (see Figure 5 for \%paraphrases that achieved at least $5 \%$ accuracy).</p>
<p>Finally, we found that noise stemming from automatic paraphrase generation has virtually no impact on metric-based model rankings. We compute Kendall's $\tau$ to compare model rankings before and after the manual filtering of paraphrases. The results (Table 6) show near-perfect to perfect agreement in rankings across all tasks, except for</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Change</th>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">P1</th>
<th style="text-align: center;">Acc.</th>
<th style="text-align: center;">P2</th>
<th style="text-align: center;">Acc.</th>
<th style="text-align: center;">Diff.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">' $\because$ ' -&gt; ':'</td>
<td style="text-align: center;">nous-hermes</td>
<td style="text-align: center;">Create a word that does not in-</td>
<td style="text-align: center;">.04</td>
<td style="text-align: center;">Create a word that does not in-</td>
<td style="text-align: center;">.65</td>
<td style="text-align: center;">+.61</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">alpaca-13b</td>
<td style="text-align: center;">Create a sentence that concludes</td>
<td style="text-align: center;">.61</td>
<td style="text-align: center;">Create a sentence that concludes</td>
<td style="text-align: center;">.19</td>
<td style="text-align: center;">-.42</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">with the term "[word]".</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">with the term "[word]":</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">$+{ }^{\prime}{ }^{\prime}$</td>
<td style="text-align: center;">alpaca-13b</td>
<td style="text-align: center;">Write a word that lacks the letter</td>
<td style="text-align: center;">.04</td>
<td style="text-align: center;">Write a word that lacks the letter</td>
<td style="text-align: center;">.42</td>
<td style="text-align: center;">+.38</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">"letter"</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">"letter".</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">flan-t5-xl</td>
<td style="text-align: center;">Write a word that omits the letter</td>
<td style="text-align: center;">.77</td>
<td style="text-align: center;">Write a word that omits the letter</td>
<td style="text-align: center;">.54</td>
<td style="text-align: center;">-.23</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">"letter"</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">"letter".</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">$+{ }^{\text {' }}$ using'</td>
<td style="text-align: center;">flan-t5-large</td>
<td style="text-align: center;">Your task is to write a word without the letter "[letter]"</td>
<td style="text-align: center;">.46</td>
<td style="text-align: center;">Your task is to write a word without using the letter "[letter]"</td>
<td style="text-align: center;">.12</td>
<td style="text-align: center;">-.35</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">falcon-7b</td>
<td style="text-align: center;">Write a word without the letter</td>
<td style="text-align: center;">.12</td>
<td style="text-align: center;">Write a word without using the</td>
<td style="text-align: center;">.35</td>
<td style="text-align: center;">+.23</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">{letter}.\mOutput word:</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">letter [letter}.\mOutput word:</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">omits $\rightarrow$ lacks</td>
<td style="text-align: center;">ultralm-13b</td>
<td style="text-align: center;">Write a word that omits the letter</td>
<td style="text-align: center;">.62</td>
<td style="text-align: center;">Write a word that lacks the letter</td>
<td style="text-align: center;">.19</td>
<td style="text-align: center;">-.42</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">"[letter]".</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">"[letter]".</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">flan-t5-xl</td>
<td style="text-align: center;">Write a word that omits the letter</td>
<td style="text-align: center;">.54</td>
<td style="text-align: center;">Write a word that lacks the letter</td>
<td style="text-align: center;">.81</td>
<td style="text-align: center;">+.27</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">"[letter]".</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">"[letter]".</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">contain $\rightarrow$ have</td>
<td style="text-align: center;">falcon-7b</td>
<td style="text-align: center;">Write a word that does not con-</td>
<td style="text-align: center;">.81</td>
<td style="text-align: center;">Write a word that does not have</td>
<td style="text-align: center;">.19</td>
<td style="text-align: center;">-.62</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">tain the letter "[letter]"</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">the letter "[letter]"</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">flan-t5-xxl</td>
<td style="text-align: center;">Please write a word that does not</td>
<td style="text-align: center;">.62</td>
<td style="text-align: center;">Please write a word that does not</td>
<td style="text-align: center;">have the letter "[letter]".</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">contain the letter "[letter]".</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">have the letter "[letter]".</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">include $\rightarrow$ have</td>
<td style="text-align: center;">falcon-7b</td>
<td style="text-align: center;">Write a word that does not in-</td>
<td style="text-align: center;">.81</td>
<td style="text-align: center;">Write a word that does not have</td>
<td style="text-align: center;">.19</td>
<td style="text-align: center;">-.62</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">clude the letter "[letter]".</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">the letter "[letter]".</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">flan-t5-xl</td>
<td style="text-align: center;">Write a word that does not in-</td>
<td style="text-align: center;">.42</td>
<td style="text-align: center;">Write a word that does not have</td>
<td style="text-align: center;">.73</td>
<td style="text-align: center;">+.31</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">clude the letter "[letter]".</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">the letter "[letter]".</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">ultralm-13b</td>
<td style="text-align: center;">Please write a word that does not</td>
<td style="text-align: center;">.46</td>
<td style="text-align: center;">Please write a word that does not</td>
<td style="text-align: center;">.12</td>
<td style="text-align: center;">-.35</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">include the letter "[letter]".</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">have the letter "[letter]".</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">excludes $\rightarrow$ lacks</td>
<td style="text-align: center;">flan-t5-large</td>
<td style="text-align: center;">Write a word that excludes the</td>
<td style="text-align: center;">.54</td>
<td style="text-align: center;">Write a word that lacks the letter</td>
<td style="text-align: center;">.12</td>
<td style="text-align: center;">-.42</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">letter "[letter]".</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">"[letter]".</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">flan-t5-xl</td>
<td style="text-align: center;">Write a word that excludes the</td>
<td style="text-align: center;">.19</td>
<td style="text-align: center;">Write a word that lacks the letter</td>
<td style="text-align: center;">.81</td>
<td style="text-align: center;">+.62</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">letter "[letter]".</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">"[letter]".</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Table 5: Representative examples of instruction template pairs from LMENTRY with very minor differences but notable variations in performance (open-source models).
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 5: Percentage of instruction paraphrases with accuracy higher than 5\% in T5 models (blue) vs. LLaMA models (purple) on LMENTRY tasks.
the "ends with word" task in LMENTRY. Upon examination, this seems to be mostly due to an error in LMENTRY's evaluation script. These results suggest that it may be enough to compute our metrics over range of automatically-generated paraphrases, without having to manually verify them.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Benchmark</th>
<th style="text-align: center;">Max perf.</th>
<th style="text-align: center;">Average perf.</th>
<th style="text-align: center;">Combined perf.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">LMENTRY</td>
<td style="text-align: center;">.963</td>
<td style="text-align: center;">.978</td>
<td style="text-align: center;">.948</td>
</tr>
<tr>
<td style="text-align: left;">BBH</td>
<td style="text-align: center;">.991</td>
<td style="text-align: center;">.983</td>
<td style="text-align: center;">.966</td>
</tr>
</tbody>
</table>
<p>Table 6: Averaged Kendall's Tau values comparing rankings before and after filtering incorrect paraphrases for each metric across all tasks (excluding "ends with word" for LMENTRY).</p>
<h2>7 Small-Scale Evaluation of OpenAI Models on Prompt Paraphrasing</h2>
<p>In this section we perform a small-scale evaluation showing that API LLMs are also sensitive to instruction paraphrasing. Our evaluation focuses on four OpenAI models: davinci, text-davinci002, text-davinci-003, and GPT-3.5-Turbo on the</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 6: The performance of various models according to the metrics proposed in Section 5 evaluated on sample tasks from each of the three benchmarks. The name of the metric appears below each group of columns; height of a column represents value in <em>that specific metric</em>. The order of the columns (i.e., models) between groups is fixed, set according to decreasing performance on the original instruction templates to enable straightforward comparisons of ranking changes.</p>
<h3>LMENTRY benchmark</h3>
<p>Due to budget constraints, we show that the performance of these models diverges significantly between the benchmark's original instruction templates and a selection of paraphrases, in terms of both average and maximum metrics.</p>
<h3>Estimating average performance</h3>
<p>To estimate the average performance of OpenAI models on a specific task, we adopted a randomized approach. For each task sample, we randomly selected a paraphrase from our collection, and evaluated the model's response, scoring the entire set of task samples. To approximate average performance, this experiment was repeated 20 times, determined by the data from our 16 open-source models.</p>
<h3>Estimating maximal performance</h3>
<p>To estimate which of the roughly 175 instruction templates per task performs the best for each model, we implemented a simple greedy search. Initially, we evaluated all paraphrases on 10 task instances, then narrowed down to the top 100 instruction templates for another 10 instances. Finally, the top 10 instruction templates were evaluated on the remaining instances, and the template that performed the best was chosen to estimate the maximum performance.</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 7: Comparison of the <em>maximum performance</em> of four OpenAI models using original prompts (in solid colors) vs. all prompt paraphrases (semi-transparent). Each group of columns corresponds to a different task in the LMENTRY benchmark.</p>
<h3>7.1 Results</h3>
<p>Below we summarize the results of our evaluation of OpenAI models. The full details appear in our repository.</p>
<h3>OpenAI models are also sensitive to minor prompt variations</h3>
<p>Minor changes in the phrasing of the instruction could lead to drastic performance changes for the OpenAI models, similar to our findings in Section 4.2 with smaller-scale LLMs. See representative examples in Table 7 showing nearly identical instruction template pairs resulting in notable variations in performance.</p>
<h3>Average performance is lower than that observed in the original benchmark instructions</h3>
<p>In 72.5% of the cases, the performance of the original instructions was higher than the estimated average across all paraphrases. In the davinci model, the original prompts added on average 21 more accuracy points.</p>
<h3>Original prompt performances fall below all paraphrases' estimated maximum performance</h3>
<p>Figure 7 depicts maximum performance of the <em>original instructions</em> for four LMENTRY tasks in solid colors, with overlaid semi-transparent columns indicating the estimated maximum performance on <em>all paraphrases</em>. Notably, for text-davinci-002, we found paraphrases</p>
<table>
<thead>
<tr>
<th>Change</th>
<th>Model</th>
<th>P1</th>
<th>Acc.</th>
<th>P2</th>
<th>Acc.</th>
<th>Diff.</th>
</tr>
</thead>
<tbody>
<tr>
<td>$[\ldots]-&gt;"[\ldots]$ "</td>
<td>td002</td>
<td>Which word has a greater number of letters, [word1] or [word2]?</td>
<td>.50</td>
<td>Which word has a greater number of letters, "[word1]" or "[word2]"?</td>
<td>.23</td>
<td>-0.27</td>
</tr>
<tr>
<td></td>
<td>td002</td>
<td>Which of the words [word1] and [word2] is alphabetically first?</td>
<td>.54</td>
<td>Which of the words "[word1]" and "[word2]" is alphabetically first?</td>
<td>.77</td>
<td>+0.23</td>
</tr>
<tr>
<td></td>
<td>td003</td>
<td>Which word has a greater number of letters, [word1] or [word2]?</td>
<td>.60</td>
<td>Which word has a greater number of letters, "[word1]" or "[word2]"?</td>
<td>.14</td>
<td>-0.46</td>
</tr>
<tr>
<td></td>
<td>td003</td>
<td>Compare the length of [word1] and [word2] and tell me which one is shorter.</td>
<td>.39</td>
<td>Compare the length of "[word1]" and "[word2]" and tell me which one is shorter.</td>
<td>.73</td>
<td>+0.34</td>
</tr>
<tr>
<td></td>
<td>cgpt</td>
<td>Which word has a greater number of letters, [word1] or [word2]?</td>
<td>.55</td>
<td>Which word has a greater number of letters, "[word1]" or "[word2]"?</td>
<td>.24</td>
<td>-0.31</td>
</tr>
<tr>
<td></td>
<td>cgpt</td>
<td>Compare the length of [word1] and [word2]. Which one is longer?</td>
<td>.04</td>
<td>Compare the length of "[word1]" and "[word2]". Which one is longer?</td>
<td>.70</td>
<td>+0.66</td>
</tr>
<tr>
<td>',' -&gt; ':'</td>
<td>td002</td>
<td>Which word is a rhyme for "[query]", "[word1]" or "[word2]"?</td>
<td>.08</td>
<td>Which word is a rhyme for "[query]"; "[word1]" or "[word2]"?</td>
<td>.85</td>
<td>+0.77</td>
</tr>
<tr>
<td></td>
<td>td003</td>
<td>Which word is a rhyme for "[query]", "[word1]" or "[word2]"?</td>
<td>.48</td>
<td>Which word is a rhyme for "[query]"; "[word1]" or "[word2]"?</td>
<td>.90</td>
<td>+0.42</td>
</tr>
<tr>
<td>',' -&gt; '-'</td>
<td>td002</td>
<td>Which word rhymes with "[query]", "[word1]" or "[word2]"?</td>
<td>.06</td>
<td>Which word rhymes with "[query]" - "[word1]" or "[word2]"?</td>
<td>.73</td>
<td>+0.67</td>
</tr>
<tr>
<td></td>
<td>td003</td>
<td>Which word rhymes with "[query]", "[word1]" or "[word2]"?</td>
<td>.17</td>
<td>Which word rhymes with "[query]" - "[word1]" or "[word2]"?</td>
<td>.60</td>
<td>+0.43</td>
</tr>
<tr>
<td>the $\rightarrow$ a</td>
<td>td002</td>
<td>What is the word that rhymes with "[query]" - "[word1]" or "[word2]"?</td>
<td>.03</td>
<td>What is a word that rhymes with "[query]" - "[word1]" or "[word2]"?</td>
<td>.78</td>
<td>+0.75</td>
</tr>
<tr>
<td>which $\rightarrow$ what</td>
<td>td002</td>
<td>Which word rhymes with "[query]" - "[word1]" or "[word2]"?</td>
<td>.73</td>
<td>What word rhymes with "[query]" - "[word1]" or "[word2]"?</td>
<td>.82</td>
<td>+0.09</td>
</tr>
<tr>
<td></td>
<td>td003</td>
<td>Which word rhymes with "[query]" - "[word1]" or "[word2]"?</td>
<td>.60</td>
<td>What word rhymes with "[query]" - "[word1]" or "[word2]"?</td>
<td>.15</td>
<td>-0.45</td>
</tr>
<tr>
<td>word $\rightarrow$ term</td>
<td>td002</td>
<td>Create a word that excludes the letter "[letter]".</td>
<td>.54</td>
<td>Create a term that excludes the letter "[letter]".</td>
<td>.04</td>
<td>-0.50</td>
</tr>
<tr>
<td></td>
<td>td003</td>
<td>Create a word that excludes the letter "[letter]".</td>
<td>.96</td>
<td>Create a term that excludes the letter "[letter]".</td>
<td>.58</td>
<td>-0.38</td>
</tr>
<tr>
<td></td>
<td>cgpt</td>
<td>Create a word that excludes the letter "[letter]".</td>
<td>.81</td>
<td>Create a term that excludes the letter "[letter]".</td>
<td>.42</td>
<td>-0.39</td>
</tr>
</tbody>
</table>
<p>Table 7: Minimal distance pairs from LMENTRY with large performance differences in OpenAI models.
that improved its maximal accuracy performance above $90 \%$ for 8 out of 10 tasks. Across all four models, 26 out of 40 differences were statistically significant according to the McNemar test.</p>
<p>Model rankings diverge between the different metrics and original instruction templates. Similarly to our main evaluation, there were many mismatches between ranking on the original instruction templates and our metrics. Agreement was observed in only 5 out of 10 tasks for the average metric, and in 4 out of 10 tasks for the maximum metric.</p>
<h2>8 Related Work</h2>
<p>Our work is part of an emerging trend highlighting the many challenges standing in the way of meaningful, scalable, and reproducible evaluation
of large language models.
Perlitz et al. (2023) focus on the rising cost of exhaustive evaluation of LLMs on large number of samples. They developed methods for choosing subsets of the test data which are expected to be a good representative of the whole. An interesting avenue for future work can extend Perlitz et al. (2023)'s approach to also include various instruction templates, thus efficiently approximating our suggested evaluation methods.</p>
<p>Sclar et al. (2023) show that LLMs are sensitive to prompt formatting. These are minor prompt design choices, such as the addition or omission of punctuation marks. They create a large pool of instruction paraphrases, ensuring that paraphrases maintain the meaning of the original prompt. We notice a similar phenomenon, albeit more anecdotally, when our automatic paraphrasing tech-</p>
<p>niques incidentally produce minor changes in formatting (Table 7). Voronov et al. (2024) showed that LLMs are sensitive to the format of in-context examples. For example, they varied the manner in which each input-output is separated, and test how such choices interact with the phrasing of the instruction template, the number of demonstrations, or the model size.</p>
<p>The works discussed above represent a distinct thread within the larger field of model robustness, which is typically defined as a measure of models' ability to adapt to distribution shifts between training and inference (Wang et al., 2022), or to cope with adversarial examples (Wang et al., 2021, 2023). In contrast, these works do not change the underlying instance to be classified (e.g., the homophone pairs in our running example), but rather the task instruction. This challenge arises with the introduction of LLMs which take such instructions as part of the input, rather than through dedicated calibration in training or finetuning.</p>
<h2>9 Conclusions</h2>
<p>Our research highlights the sensitivity of large language models (LLMs) to prompt paraphrasing, challenging the adequacy of single-prompt evaluations. We propose alternative evaluation metrics that use a diverse set of instruction templates for each task, designed for more robust and meaningful LLM evaluation. For example, LLM developers may be interested in measuring the robustness of performance across multiple prompts, which we propose to evaluate as the average across a large collection of prompts. In contrast, when developing a downstream model, different models should be compared according to their corresponding top-performing prompt.</p>
<p>Evaluating based on these metrics underscores the necessity for nuanced evaluation methods, revealing notable differences in absolute performance and relative model rankings compared to traditional evaluations. We hope that our work will help spur more consistency and comparability in LLM evaluation which is strongly coupled to real-world LLM uses. We believe this shift is crucial for accurately understanding and leveraging the true capabilities of LLMs.</p>
<h2>Acknowledgements</h2>
<p>We thank the reviewers for their insightful comments. We further thank Asaf Yehudai and Oyvind</p>
<p>Tafjord for engaging discussions, and the members of SLAB and Hyadata Lab at the Hebrew University of Jerusalem for their thoughtful remarks. This work was supported by the European Research Council (ERC) under the European Union's Horizon 2020 research and innovation programme (grant no. 852686, SIAM) and was partially supported by the Israeli Ministry of Science and Technology (grant no. 2336).</p>
<h2>References</h2>
<p>Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. 2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774.</p>
<p>Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cappelli, Ruxandra Cojocaru, Merouane Debbah, Etienne Goffinet, Daniel Heslow, Julien Launay, Quentin Malartic, et al. 2023. Falcon-40b: an open large language model with state-of-the-art performance. Technical report, Technical report, Technology Innovation Institute.</p>
<p>Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. 2023. Palm: Scaling language modeling with pathways. Journal of Machine Learning Research, 24(240):1-113.</p>
<p>Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. 2024. Scaling instruction-finetuned language models. Journal of Machine Learning Research, 25(70):1-53.</p>
<p>OpenAccess AI Collective. 2023. Minotaur. https://huggingface.co/ openaccess-ai-collective/minotaur-15b. Last Accessed: 2024-04-30.</p>
<p>Gregory W Corder and Dale I Foreman. 2011. Nonparametric Statistics for Non-Statisticians. John Wiley \&amp; Sons, Inc.</p>
<p>Ning Ding, Yulin Chen, Bokai Xu, Yujia Qin, Shengding Hu, Zhiyuan Liu, Maosong Sun,</p>
<p>and Bowen Zhou. 2023. Enhancing chat language models by scaling high-quality instructional conversations. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 3029-3051.</p>
<p>Jon Durbin. 2023. Airoboros. https://github.com/ jondurbin/airoboros. Last Accessed: 2024-0430.</p>
<p>Avia Efrat, Or Honovich, and Omer Levy. 2023. Lmentry: A language model benchmark of elementary language tasks. In Findings of the Association for Computational Linguistics: ACL 2023, pages 10476-10501.</p>
<p>Hila Gonen, Srini Iyer, Terra Blevins, Noah A Smith, and Luke Zettlemoyer. 2023. Demystifying prompts in language models via perplexity estimation. In Findings of the Association for Computational Linguistics: EMNLP 2023, pages 10136-10148.</p>
<p>Jiasheng Gu, Hongyu Zhao, Hanzi Xu, Liangyu Nie, Hongyuan Mei, and Wenpeng Yin. 2023. Robustness of learning from task instructions. In Findings of the Association for Computational Linguistics: ACL 2023, pages 1393513948.</p>
<p>Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2020. Measuring massive multitask language understanding. In International Conference on Learning Representations.</p>
<p>Or Honovich, Thomas Scialom, Omer Levy, and Timo Schick. 2023a. Unnatural instructions: Tuning language models with (almost) no human labor. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 14409-14428.</p>
<p>Or Honovich, Uri Shaham, Samuel R Bowman, and Omer Levy. 2023b. Instruction induction: From few examples to natural language task descriptions. In 61st Annual Meeting of the Association for Computational Linguistics, ACL 2023, pages 1935-1952. Association for Computational Linguistics (ACL).</p>
<p>Leonard J Kazmier, Michael K Staton, Daniel L Fulks, et al. 2003. Business statistics: based on
schaums outline of theory and problems of business statistics, by leonard j. kazmier. Technical report, McGraw-Hill.</p>
<p>Maurice G Kendall. 1945. The treatment of ties in ranking problems. Biometrika, 33(3):239-251.</p>
<p>Maurice G Kendall and B Babington Smith. 1939. The problem of $m$ rankings. The annals of mathematical statistics, 10(3):275-287.</p>
<p>Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilić, Daniel Hesslow, Roman Castagné, Alexandra Sasha Luccioni, François Yvon, Matthias Gallé, et al. 2022. Bloom: A 176b-parameter open-access multilingual language model. arXiv e-prints, pages arXiv-2211.</p>
<p>Brian Lester, Rami Al-Rfou, and Noah Constant. 2021. The power of scale for parameterefficient prompt tuning. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics.</p>
<p>Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, et al. 2023. Holistic evaluation of language models. Transactions on Machine Learning Research.</p>
<p>Renze Lou, Kai Zhang, and Wenpeng Yin. 2023. Is prompt all you need? no. a comprehensive and broader view of instruction learning. arXiv preprint arXiv:2303.10475.</p>
<p>Swaroop Mishra, Daniel Khashabi, Chitta Baral, and Hannaneh Hajishirzi. 2022. Cross-task generalization via natural language crowdsourcing instructions. In 60th Annual Meeting of the Association for Computational Linguistics, ACL 2022, pages 3470-3487. Association for Computational Linguistics (ACL).</p>
<p>NousResearch. 2023. Nous-hermes. https://huggingface.co/NousResearch/ Nous-Hermes-13b. Last Accessed: 2024-04-30.</p>
<p>Yotam Perlitz, Elron Bandel, Ariel Gera, Ofir Arviv, Liat Ein-Dor, Eyal Shnarch, Noam Slonim, Michal Shmueli-Scheuer, and</p>
<p>Leshem Choshen. 2023. Efficient benchmarking (of language models). arXiv preprint arXiv:2308.11696.</p>
<p>Abhinav Rao, Sachin Vashistha, Atharva Naik, Somak Aditya, and Monojit Choudhury. 2023. Tricking llms into disobedience: Understanding, analyzing, and preventing jailbreaks. arXiv preprint arXiv:2305.14965.</p>
<p>Victor Sanh, Albert Webson, Colin Raffel, Stephen Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Arun Raja, Manan Dey, et al. 2021. Multitask prompted training enables zero-shot task generalization. In International Conference on Learning Representations.</p>
<p>Melanie Sclar, Yejin Choi, Yulia Tsvetkov, and Alane Suhr. 2023. Quantifying language models' sensitivity to spurious features in prompt design or: How i learned to start worrying about prompt formatting. In The Twelfth International Conference on Learning Representations.</p>
<p>Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam R Brown, Adam Santoro, Aditya Gupta, Adrià Garriga-Alonso, et al. 2023a. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. Transactions on Machine Learning Research.</p>
<p>Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam R Brown, Adam Santoro, Aditya Gupta, Adrià Garriga-Alonso, et al. 2023b. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. Transactions on Machine Learning Research.</p>
<p>Jiuding Sun, Chantal Shaib, and Byron C Wallace. 2023. Evaluating the zero-shot robustness of instruction-tuned language models. In The Twelfth International Conference on Learning Representations.</p>
<p>Mirac Suzgun, Nathan Scales, Nathanael Schärli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc Le, Ed Chi, Denny Zhou, et al. 2023. Challenging big-bench tasks and whether chain-of-thought
can solve them. In Findings of the Association for Computational Linguistics: ACL 2023, pages 13003-13051.</p>
<p>Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B Hashimoto. 2023. Alpaca: A strong, replicable instructionfollowing model. Stanford Center for Research on Foundation Models. https://crfm.stanford. edu/2023/03/13/alpaca.html. Last Accessed: 2024-04-30.</p>
<p>Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, et al. 2023. Gemini: a family of highly capable multimodal models. arXiv preprint arXiv:2312.11805.</p>
<p>MosaicML NLP Team. 2023. Introducing mpt-7b: A new standard for open-source, commercially usable llms. www.mosaicml.com/blog/mpt-7b. Last Accessed: 2024-04-30.</p>
<p>Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971.</p>
<p>Anton Voronov, Lena Wolf, and Max Ryabinin. 2024. Mind your format: Towards consistent evaluation of in-context learning improvements. arXiv preprint arXiv:2401.06766.</p>
<p>Boxin Wang, Chejian Xu, Shuohang Wang, Zhe Gan, Yu Cheng, Jianfeng Gao, Ahmed Hassan Awadallah, and Bo Li. 2021. Adversarial glue: A multi-task benchmark for robustness evaluation of language models. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2).</p>
<p>Jindong Wang, HU Xixu, Wenxin Hou, Hao Chen, Runkai Zheng, Yidong Wang, Linyi Yang, Wei Ye, Haojun Huang, Xiubo Geng, et al. 2023. On the robustness of chatgpt: An adversarial and out-of-distribution perspective. In ICLR 2023 Workshop on Trustworthy and Reliable LargeScale Machine Learning Models.</p>
<p>Xuezhi Wang, Haohan Wang, and Diyi Yang. 2022. Measure and improve robustness in nlp</p>
<p>models: A survey. In 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL 2022, pages 45694586. Association for Computational Linguistics (ACL).</p>
<p>Lucas Weber, Elia Bruni, and Dieuwke Hupkes. 2023. Mind the instructions: a holistic evaluation of consistency and interactions in promptbased learning. In Proceedings of the 27th Conference on Computational Natural Language Learning (CoNLL), pages 294-313.</p>
<p>Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, and Quoc V Le. 2021. Finetuned language models are zero-shot learners. In International Conference on Learning Representations.</p>
<p>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. 2022. Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems, 35:24824-24837.</p>
<p>Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. 2024. Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in Neural Information Processing Systems, 36.</p>
<p>Kaijie Zhu, Jindong Wang, Jiaheng Zhou, Zichen Wang, Hao Chen, Yidong Wang, Linyi Yang, Wei Ye, Neil Zhenqiang Gong, Yue Zhang, et al. 2023. Promptbench: Towards evaluating the robustness of large language models on adversarial prompts. arXiv preprint arXiv:2306.04528.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{2}$ Calculated as the number of models tested per task $\times$ number of paraphrased instructions per task $\times 100$ samples, across all tasks and benchmarks $\approx 240 \times 16 \times 100 \times 10$ (LMENTRY) $+175 \times 11 \times 100 \times 15(\mathrm{BBH})$.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8087 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8087</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8087</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-146.html">extraction-schema-146</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <p><strong>Paper ID:</strong> paper-53f4fb0e9972989194368faf288ff8e3cba5bd60</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/53f4fb0e9972989194368faf288ff8e3cba5bd60" target="_blank">Chatbot Arena: An Open Platform for Evaluating LLMs by Human Preference</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Machine Learning</p>
                <p><strong>Paper TL;DR:</strong> This paper describes the Chatbot Arena platform, analyzes the data collected so far, and explains the tried-and-true statistical methods used for efficient and accurate evaluation and ranking of models, to establish a robust foundation for the credibility of Chatbot Arena.</p>
                <p><strong>Paper Abstract:</strong> Large Language Models (LLMs) have unlocked new capabilities and applications; however, evaluating the alignment with human preferences still poses significant challenges. To address this issue, we introduce Chatbot Arena, an open platform for evaluating LLMs based on human preferences. Our methodology employs a pairwise comparison approach and leverages input from a diverse user base through crowdsourcing. The platform has been operational for several months, amassing over 240K votes. This paper describes the platform, analyzes the data we have collected so far, and explains the tried-and-true statistical methods we are using for efficient and accurate evaluation and ranking of models. We confirm that the crowdsourced questions are sufficiently diverse and discriminating and that the crowdsourced human votes are in good agreement with those of expert raters. These analyses collectively establish a robust foundation for the credibility of Chatbot Arena. Because of its unique value and openness, Chatbot Arena has emerged as one of the most referenced LLM leaderboards, widely cited by leading LLM developers and companies. Our demo is publicly available at \url{https://chat.lmsys.org}.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8087.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8087.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4 vs Humans (pairwise agreement)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Comparison of GPT-4-as-judge against crowd users and domain experts in pairwise preference labeling</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A validation experiment in which GPT-4 (GPT-4-Turbo / GPT-4-0613) is used as an automated judge and its pairwise decisions are compared to Chatbot Arena crowd votes and two expert annotators; the paper reports pairwise agreement rates and analyzes sources of disagreement.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Chatbot Arena: An Open Platform for Evaluating LLMs by Human Preference</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Open-ended dialogue / helpfulness pairwise preference comparisons</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>Random sample of Chatbot Arena pairwise battles (160 battles sampled for expert relabeling)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>GPT-4 (reported as GPT-4-Turbo and GPT-4-0613)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td>GPT-4 variants used as automated judge (version labels GPT-4-Turbo and GPT-4-0613 appear in the paper); no additional training details provided in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>Crowd users (Chatbot Arena visitors) and two domain experts (graduate students at UC Berkeley) who relabeled battles blind and were allowed external fact-checking.</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>pairwise agreement rate (percent agreement)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td>0.756</td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>subjectivity in prompts leading to expert disagreement; crowd users sometimes overlook factual errors; crowd labeling mistakes; lack of clear ground-truth for many open-ended prompts</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Measured pairwise agreement rates fall in the 72%--83% range between crowd and experts; expert-expert agreement ranged ~79%--90%; crowd vs GPT-4 agreement reported as 75.6% (same value reported across the two setups examined). The paper attributes 5%--10% gap between crowd-vs-expert and expert-vs-expert agreement mainly to crowd mistakes/overlooked factual errors; 10%--20% expert-expert disagreement is attributed to inherent subjectivity/lack of ground truth in some prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>Explicitly used to 'factor out user votes' (i.e., to control variables) and to produce automated judgments quickly and reproducibly; enables fast, consistent relabeling compared to collecting fresh human labels.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Randomly selected 160 battles drawn from Arena between (a) GPT-4-Turbo vs Llama-2-13B and (b) GPT-4-Turbo vs GPT-3.5-Turbo-0613. Two experts relabeled the same blind prompts/answers (asked to fact-check with external resources). Each manual label required ~3-5 minutes. GPT-4 was also run as an automated judge; agreement tabulated in Table 3 and win-rate comparisons in Table 4.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Chatbot Arena: An Open Platform for Evaluating LLMs by Human Preference', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8087.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8087.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4 judge topical win-rate</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-as-judge (GPT-4-turbo) evaluation of model win-rates by topic clusters</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An experiment using GPT-4-turbo as an automated judge to compute model win-rates on small sets of topic-clustered prompts (30 prompts per sampled cluster), showing that GPT-4 wins much more often in coding/reasoning clusters than in creative/subjective clusters.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Chatbot Arena: An Open Platform for Evaluating LLMs by Human Preference</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Model comparison by topic cluster (coding, reasoning, creative writing, recommendations, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>30 sample prompts drawn from selected Chatbot Arena topic clusters</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>GPT-4-turbo (reported also as GPT-4-0613 in win-rate table)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td>GPT-4 variants used as automated judge; exact training/data/version details not provided beyond version labels.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>Not used in this specific topical win-rate experiment (LLM-as-judge was used to 'factor out' user votes).</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>Paper does not provide a direct head-to-head numeric comparison to human judgments for this topical experiment; potential limitation noted implicitly that LLM-as-judge may vary by topic and may not perfectly reflect human preferences across all domains.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>GPT-4-as-judge reports very high win-rates for GPT-4-0613 vs Llama-2-70b-chat on coding/reasoning clusters (e.g., 96.7% for Python game programming, 86.7% for C/C++ multithreading) and lower win-rates (~53%--66%) on creative or recommendation clusters; indicates LLM-as-judge can reveal domain-specific model strengths but outcomes depend on topic.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>Allowed the authors to control for noisy user votes and evaluate models consistently across curated topic samples (i.e., faster and repeatable comparisons across topics).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>For Table 2: sampled 30 prompts from each of several topic clusters; GPT-4-turbo used to judge pairwise responses from GPT-4-0613 vs Llama-2-70b-chat; win-rates per cluster reported in Table 2.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Chatbot Arena: An Open Platform for Evaluating LLMs by Human Preference', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Can large language models be an alternative to human evaluations? <em>(Rating: 2)</em></li>
                <li>Judging LLM-as-a-judge with MT-bench and chatbot arena <em>(Rating: 2)</em></li>
                <li>AlpacaEval: An automatic evaluator of instruction-following models <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8087",
    "paper_id": "paper-53f4fb0e9972989194368faf288ff8e3cba5bd60",
    "extraction_schema_id": "extraction-schema-146",
    "extracted_data": [
        {
            "name_short": "GPT-4 vs Humans (pairwise agreement)",
            "name_full": "Comparison of GPT-4-as-judge against crowd users and domain experts in pairwise preference labeling",
            "brief_description": "A validation experiment in which GPT-4 (GPT-4-Turbo / GPT-4-0613) is used as an automated judge and its pairwise decisions are compared to Chatbot Arena crowd votes and two expert annotators; the paper reports pairwise agreement rates and analyzes sources of disagreement.",
            "citation_title": "here",
            "mention_or_use": "use",
            "paper_title": "Chatbot Arena: An Open Platform for Evaluating LLMs by Human Preference",
            "evaluation_task": "Open-ended dialogue / helpfulness pairwise preference comparisons",
            "dataset_name": "Random sample of Chatbot Arena pairwise battles (160 battles sampled for expert relabeling)",
            "judge_model_name": "GPT-4 (reported as GPT-4-Turbo and GPT-4-0613)",
            "judge_model_details": "GPT-4 variants used as automated judge (version labels GPT-4-Turbo and GPT-4-0613 appear in the paper); no additional training details provided in the paper.",
            "human_evaluator_type": "Crowd users (Chatbot Arena visitors) and two domain experts (graduate students at UC Berkeley) who relabeled battles blind and were allowed external fact-checking.",
            "agreement_metric": "pairwise agreement rate (percent agreement)",
            "agreement_score": 0.756,
            "reported_loss_aspects": "subjectivity in prompts leading to expert disagreement; crowd users sometimes overlook factual errors; crowd labeling mistakes; lack of clear ground-truth for many open-ended prompts",
            "qualitative_findings": "Measured pairwise agreement rates fall in the 72%--83% range between crowd and experts; expert-expert agreement ranged ~79%--90%; crowd vs GPT-4 agreement reported as 75.6% (same value reported across the two setups examined). The paper attributes 5%--10% gap between crowd-vs-expert and expert-vs-expert agreement mainly to crowd mistakes/overlooked factual errors; 10%--20% expert-expert disagreement is attributed to inherent subjectivity/lack of ground truth in some prompts.",
            "advantages_of_llm_judge": "Explicitly used to 'factor out user votes' (i.e., to control variables) and to produce automated judgments quickly and reproducibly; enables fast, consistent relabeling compared to collecting fresh human labels.",
            "experimental_setting": "Randomly selected 160 battles drawn from Arena between (a) GPT-4-Turbo vs Llama-2-13B and (b) GPT-4-Turbo vs GPT-3.5-Turbo-0613. Two experts relabeled the same blind prompts/answers (asked to fact-check with external resources). Each manual label required ~3-5 minutes. GPT-4 was also run as an automated judge; agreement tabulated in Table 3 and win-rate comparisons in Table 4.",
            "uuid": "e8087.0",
            "source_info": {
                "paper_title": "Chatbot Arena: An Open Platform for Evaluating LLMs by Human Preference",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "GPT-4 judge topical win-rate",
            "name_full": "LLM-as-judge (GPT-4-turbo) evaluation of model win-rates by topic clusters",
            "brief_description": "An experiment using GPT-4-turbo as an automated judge to compute model win-rates on small sets of topic-clustered prompts (30 prompts per sampled cluster), showing that GPT-4 wins much more often in coding/reasoning clusters than in creative/subjective clusters.",
            "citation_title": "here",
            "mention_or_use": "use",
            "paper_title": "Chatbot Arena: An Open Platform for Evaluating LLMs by Human Preference",
            "evaluation_task": "Model comparison by topic cluster (coding, reasoning, creative writing, recommendations, etc.)",
            "dataset_name": "30 sample prompts drawn from selected Chatbot Arena topic clusters",
            "judge_model_name": "GPT-4-turbo (reported also as GPT-4-0613 in win-rate table)",
            "judge_model_details": "GPT-4 variants used as automated judge; exact training/data/version details not provided beyond version labels.",
            "human_evaluator_type": "Not used in this specific topical win-rate experiment (LLM-as-judge was used to 'factor out' user votes).",
            "agreement_metric": null,
            "agreement_score": null,
            "reported_loss_aspects": "Paper does not provide a direct head-to-head numeric comparison to human judgments for this topical experiment; potential limitation noted implicitly that LLM-as-judge may vary by topic and may not perfectly reflect human preferences across all domains.",
            "qualitative_findings": "GPT-4-as-judge reports very high win-rates for GPT-4-0613 vs Llama-2-70b-chat on coding/reasoning clusters (e.g., 96.7% for Python game programming, 86.7% for C/C++ multithreading) and lower win-rates (~53%--66%) on creative or recommendation clusters; indicates LLM-as-judge can reveal domain-specific model strengths but outcomes depend on topic.",
            "advantages_of_llm_judge": "Allowed the authors to control for noisy user votes and evaluate models consistently across curated topic samples (i.e., faster and repeatable comparisons across topics).",
            "experimental_setting": "For Table 2: sampled 30 prompts from each of several topic clusters; GPT-4-turbo used to judge pairwise responses from GPT-4-0613 vs Llama-2-70b-chat; win-rates per cluster reported in Table 2.",
            "uuid": "e8087.1",
            "source_info": {
                "paper_title": "Chatbot Arena: An Open Platform for Evaluating LLMs by Human Preference",
                "publication_date_yy_mm": "2024-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Can large language models be an alternative to human evaluations?",
            "rating": 2
        },
        {
            "paper_title": "Judging LLM-as-a-judge with MT-bench and chatbot arena",
            "rating": 2
        },
        {
            "paper_title": "AlpacaEval: An automatic evaluator of instruction-following models",
            "rating": 2
        }
    ],
    "cost": 0.016925,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Chatbot Arena: An Open Platform for Evaluating LLMs by Human Preference</h1>
<p>Wei-Lin Chiang ${ }^{<em> 1}$ Lianmin Zheng ${ }^{</em> 1}$ Ying Sheng ${ }^{2}$ Anastasios N. Angelopoulos ${ }^{1}$ Tianle Li ${ }^{1}$ Dacheng Li ${ }^{1}$<br>Banghua Zhu ${ }^{1}$ Hao Zhang ${ }^{3}$ Michael I. Jordan ${ }^{1}$ Joseph E. Gonzalez ${ }^{1}$ Ion Stoica ${ }^{1}$</p>
<h4>Abstract</h4>
<p>Large Language Models (LLMs) have unlocked new capabilities and applications; however, evaluating the alignment with human preferences still poses significant challenges. To address this issue, we introduce Chatbot Arena, an open platform for evaluating LLMs based on human preferences. Our methodology employs a pairwise comparison approach and leverages input from a diverse user base through crowdsourcing. The platform has been operational for several months, amassing over 240 K votes. This paper describes the platform, analyzes the data we have collected so far, and explains the tried-and-true statistical methods we are using for efficient and accurate evaluation and ranking of models. We confirm that the crowdsourced questions are sufficiently diverse and discriminating and that the crowdsourced human votes are in good agreement with those of expert raters. These analyses collectively establish a robust foundation for the credibility of Chatbot Arena. Because of its unique value and openness, Chatbot Arena has emerged as one of the most referenced LLM leaderboards, widely cited by leading LLM developers and companies. Our demo is publicly available at https://chat.lmsys.org.</p>
<h2>1. Introduction</h2>
<p>Recent advancements in large language models (LLMs) have significantly expanded their capabilities beyond traditional natural language processing boundaries, addressing a broad array of general tasks (OpenAI, 2023; Gemini et al., 2023; Touvron et al., 2023). These developments underscore the potential of LLMs but also have raised concerns with respect to performance evaluation. Current benchmarks often fail to capture the nuanced and diverse aspects of these models, particularly in assessing their alignment with human</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1. Classification of LLM benchmarks: We categorize along two dimensions: whether the questions are from a static dataset or a live, fresh source, and whether the evaluation metric relies on ground truth or (approximated) human preferences. MMLU (Hendrycks et al., 2020), HellaSwag (Zellers et al., 2019), GSM-8K (Cobbe et al., 2021), MT-Bench (Zheng et al., 2023b), and AlpacaEval (Li et al., 2023) are common examples of static benchmarks. Chatbot Arena is the platform introduced in this paper.
preferences in real-world, open-ended tasks.
To assess the performance of LLMs, the research community has introduced a variety of benchmarks. These benchmarks can be categorized based on two factors: the source of questions (either static or live) and the evaluation metric (either ground truth or human preference). According to these factors, benchmarks can be classified into four categories, as shown in Figure 1. While a range of benchmarks is beneficial, the most prevalent current method for evaluating LLMs remains a static, ground-truth-based evaluation, partly because such evaluations are inexpensive and reproducible.</p>
<p>However, these static, ground-truth-based benchmarks exhibit several limitations. Firstly, the questions within these benchmarks are not open-ended, hindering the ability to capture the flexible and interactive use found in real-world settings (Zheng et al., 2023b). Secondly, the test sets in these benchmarks are static, meaning they can become contaminated over time, which undermines the reliability of the evaluation results (Yang et al., 2023). Furthermore, for many complex tasks, establishing a definitive ground truth is not only challenging but sometimes unattainable. Consequently, current benchmarks fail to adequately address the needs of state-of-the-art LLMs, particularly in evaluating user preferences. Thus, there is an urgent necessity for an open, live evaluation platform based on human preference that can more accurately mirror real-world usage.</p>
<p>Creating such a benchmark platform entails significant challenges. It requires the collection of live, fresh, and diverse user questions to accurately represent real-world scenarios.</p>
<p>Additionally, developing scalable, incremental, and efficient ranking systems is essential for evaluating a large number of models. Moreover, ensuring the quality of human evaluations is crucial given the noisy nature of human preferences.</p>
<p>To this end, we introduce Chatbot Arena, a benchmarking platform for LLMs that features anonymous, randomized battles in a crowdsourced setting. Chatbot Arena is a free website open to all users. ${ }^{1}$ On this website, a user can ask a question and get answers from two anonymous LLMs. Afterward, the user casts a vote for the model that delivers the preferred response, with the models' identities revealed only after voting. This crowdsourced method effectively gathers a diverse array of fresh user prompts, accurately reflecting real-world LLM applications. Armed with this data, we employ a suite of powerful statistical techniques, ranging from the statistical model of Bradley \&amp; Terry (1952) to the E-values of Vovk \&amp; Wang (2021), to estimate the ranking over models as reliably and sample-efficiently as possible. With these tools in hand, we have designed efficient sampling algorithms specifically to select model pairs in a way that accelerates the convergence of rankings while retaining statistical validity.</p>
<p>We conduct a thorough analysis of the collected data to ensure the credibility of our platform. We demonstrate that the user-generated questions are sufficiently diverse to encompass a wide range of LLM use cases and are sufficiently challenging to differentiate between models. Furthermore, we confirm that the crowd-sourced votes are highly consistent with expert evaluations.</p>
<p>We have been running our system since Apr 2023 and have received over 240 K votes from about 90 K users in over 100 different languages as of Jan 2024. To encourage user engagement, we have made over 50 state-of-the-art models available for free. We also collaborate with leading model developers such as OpenAI, Google, Anthropic, Mistral, Hugging Face, and various universities, incorporating their latest models into our platform. We keep the community engaged by routinely updating the leaderboard, publishing analytical blogs, releasing datasets, and sharing information via tweets. Because of its unique and significant value, our leaderboard has emerged as one of the most referenced in the LLM field and has become a benchmark for the industry. We commit to making our data and code available, ensuring that this platform is open-source and open-accessible.</p>
<p>We make the following contributions:</p>
<ul>
<li>We build the first large-scale crowd-sourced live LLM evaluation platform with over 1 M users visit. ${ }^{2}$</li>
</ul>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>- We conduct an in-depth analysis of the collected data, including prompt diversity, quality, vote quality, and insights on human feedback.
- We will publicly release a human preference dataset with over 100K pairwise votes collected from Chatbot Arena.
- We design an efficient sampling algorithm that actively chooses which model pairs to show, such that our sample efficiency improves, sometimes to a large degree.</p>
<h2>2. Related Work</h2>
<p>LLM Benchmarks. We briefly review the common LLM benchmarks, following the classification presented in Figure 1. The most prevalent benchmarks are static, ground-truth-based ones, typically in the form of multiple-choice questions or question-answering tasks with predefined answers and test cases. These benchmarks encompass a range of topics including language understanding, mathematics, coding, and logical reasoning. Prominent examples in this category are MMLU (Hendrycks et al., 2020), HellaSwag (Zellers et al., 2019), GSM-8K (Cobbe et al., 2021), BigBench (Srivastava et al., 2023), AGIEval (Zhong et al., 2023), and HumanEval (Chen et al., 2021). Benchmarks focusing on safety, such as ToxicChat (Lin et al., 2023), and comprehensive suites like HELM (Liang et al., 2022), also exist. In addition to closed-ended questions, benchmarks can include open-ended questions that are evaluated by human judgment, which can be rated by experts or crowd workers such as Amazon Mechanical Turk (Karpinska et al., 2021; Geng et al., 2023; Wang et al., 2023). The recent trend includes utilizing GPT-4 for approximating human judgment (Chiang \&amp; Lee, 2023), with notable instances being MT-Bench (Zheng et al., 2023b) and AlpacaEval (Li et al., 2023). In addition to static benchmarks, live benchmarks that include fresh questions are also available. These questions can be obtained from annual exams or weekly online contests such as Codeforces (Li et al., 2022; Huang et al., 2023). They can also be sourced from human interaction. Some studies have explored using live human interaction for reinforcement learning from human preference (Bai et al., 2022; Ouyang et al., 2022; Touvron et al., 2023). However, these studies are typically limited to specific organizations. In this paper, we introduce Chatbot Arena, the first open, large-scale, and crowdsourced benchmark platform that utilizes live human interaction.</p>
<p>Risks of Static Benchmarks. Static benchmarks have certain issues, including contamination, saturation, overfitting, and a lack of human alignment (Yang et al., 2023; Oren et al., 2023). DynaBench (Kiela et al., 2021) identifies these challenges and recommends the use of a live benchmark that incorporates a human-in-the-loop approach for classical NLP benchmarks. Our system adopts a similar spirit. However, our focus is on chatting with LLMs, and we implement</p>
<p>this on a significantly larger user scale.
Ranking System. Ranking systems have been a wellstudied topic in statistics. Related topics include probability models (Hunter, 2004; Rao \&amp; Kupper, 1967), rank elicitation (Szörényi et al., 2015; Busa-Fekete et al., 2014a;b), and online experiment design (Chernoff, 1992; Karimi et al., 2021). The Elo rating system has also been used for LLMs (Bai et al., 2022; Boubdir et al., 2023). Contributing to this literature, we introduce techniques for accelerating ranking convergence and detecting abnormalities, specifically applied to large-scale, real-world settings of LLMs.</p>
<p>Human Preference Dataset. Owing to the significance of human preferences, several datasets and analyses exist that incorporate human preferences. These include OpenAssistant (Köpf et al., 2023), HH-RLHF (Bai et al., 2022), LMSYS-Chat-1M (Zheng et al., 2023a), and synthetic approximations of human preferences like UltraFeedback (Cui et al., 2023) and Nectar (Zhu et al., 2023). Our prior data release, LMSYS-Chat-1M (Zheng et al., 2023a), is similarly collected via crowdsourcing. However, LMSYS-Chat-1M comprises solely conversations and lacks human preference data, rendering it unsuitable for direct use in ranking studies. This paper focuses on the analysis of preference data for ranking purposes.</p>
<h2>3. Human Preference Data Collection</h2>
<p>In this section, we discuss our interface design to collect human preferences and present summary statistics.</p>
<h3>3.1. Interface</h3>
<p>Chatbot Arena crowd-sources feedback from users for model evaluation. Our goal is to design an ease-of-use interface to reduce friction for users to contribute data. Since we collect feedback from many users, it is difficult to set a consistent grading rubric across different people. Hence, we adopt a pairwise comparison mechanism where users only need to compare two model responses and vote for the better one, instead of requiring users to provide an absolute score.</p>
<p>In each battle, two anonymous models are sampled. To encourage data diversity, we do not preset any input prompt on the website. Users are free to input any prompt to the two models. We believe this creates incentives for user engagement, particularly given that we offer a free service. It also helps us collect a diverse set of inputs representing real-world usage. After models provide their answers, user compare them side-by-side and vote for the preferred answer. If a user cannot choose in the first turn, the user can continue chatting until identifying a winner. For those who are unsure, we also present two buttons, "tie" or "both are bad." Figure 8 shows a screenshot of our interface. Before
using our service, users are required to accept terms of use, which gives us their consent to release the data publicly.</p>
<h3>3.2. Data Statistics</h3>
<p>We began collecting data in April 2023. As of Jan 2024, we have received around 240 K votes from over 90 K users. Our data involves more than 50 models, including both proprietary models like GPT-4, Claude, and Gemini, as well as open models such as LLaMA and Mistral. These conversations cover more than 100 languages, with $77 \%$ being in English, 5\% in Chinese, and the remaining languages, such as Russian, German, Spanish, French, and Japanese, each representing less than $2 \%$ of the total. Each data point includes multi-turn conversations between the user and two LLMs, and a vote to indicate which model the user prefers. We summarize statistics in Table 1 along with other existing human preference datasets.</p>
<p>Figure 10 in the Appendix shows the vote count per model. On average, 8 K votes are collected for each model. In Figure 2, we select a set of representative models and present their win rate and the number of battles. Note that we employ non-uniform sampling to concentrate votes on model pairs that have similar performance due to higher uncertainty. This helps us reduce the number of votes required to reach stable results. We later develop an adaptive sampling method and demonstrate its effectiveness against random sampling. See Section 5 for further analysis.</p>
<p>To ensure anonymity, we use keywords to filter out conversations containing model identity such as model name (e.g., GPT, Claude) or companies (e.g., OpenAI, Anthropic). To avoid misuse, we adopt OpenAI moderation API to flag conversations that contain unsafe content. The flagged user requests account for $3 \%$ of the total requests. Figure 9 in the Appendix shows the number of valid user votes over time, where we get 1-2K votes per day in recent months and spikes as we introduce new models or leaderboard updates.</p>
<h2>4. From Pairwise Comparisons to Rankings</h2>
<p>Our data consists of pairwise comparisons-but how can we use these comparisons to recover a ranking over all $M$ models? This is a well-studied topic in the literature on learning to rank (Liu et al., 2009), and we present our perspective here. We let $\mathcal{A}=\left{\left(m, m^{\prime}\right): m&lt;m^{\prime}\right.$ and $\left.m, m^{\prime} \in[M]\right}$ denote our comparative data set.</p>
<p>We consider a sequential setting, where at time $t \in \mathbb{N}$, we serve the human a pair of models $A_{t} \in \mathcal{A}$ (which we pick), and in turn we observe the human's response $H_{t} \in[0,1]$. As an example, we might have that $A_{t}=(1,2)$ and $H_{t}=1$, indicating that the human prefers model 2 over model 1 . In the ensuing text, we will primarily focus on the binary casewhere $H_{t} \in{0,1}$-but our approach will generalize to</p>
<p>Table 1. Statistics of human preference datasets, including Anthropic HH (Bai et al., 2022), OpenAssistant Conversations (Köpf et al., 2023), and Chatbot Arena (as of 2024/1/21). The tokens are counted by Llama2’s tokenizer. “Conv” = Conversation. “Lang” = Language.</p>
<table>
<thead>
<tr>
<th>Dataset</th>
<th># Convs</th>
<th># Models</th>
<th># Users</th>
<th># Langs</th>
<th>Avg. # Turns per Sample</th>
<th>Avg. # Tokens per Prompt</th>
<th>Avg. # Tokens per Response</th>
</tr>
</thead>
<tbody>
<tr>
<td>Anthropic HH</td>
<td>338,704</td>
<td>-</td>
<td>143</td>
<td>1</td>
<td>2.3</td>
<td>18.9</td>
<td>78.9</td>
</tr>
<tr>
<td>OpenAssistant</td>
<td>66,497</td>
<td>-</td>
<td>13,500</td>
<td>35</td>
<td>-</td>
<td>36.9</td>
<td>214.2</td>
</tr>
<tr>
<td>Chatbot Arena (20240121)</td>
<td>243,329</td>
<td>50</td>
<td>90,051</td>
<td>149</td>
<td>1.3</td>
<td>94.9</td>
<td>269.0</td>
</tr>
</tbody>
</table>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p><em>Figure 2.</em> Win-rate (left) and battle count (right) between a subset of models in Chatbot Arena.</p>
<p>any form of feedback, including the possibility of allowing the human to express different degrees of preference or to say the models are tied.</p>
<p>One critical goal is to estimate the <em>win matrix</em>: $\theta^{*}(a)=E[H_t | A_t = a]$, for all $a \in \mathcal{A}$; see the left panel of Figure 2 for an illustration of the (empirical) win matrix. In the binary case, the $a$ entry in the win matrix corresponds to the probability the human prefers model $a_2$ to $a_1$ when shown the pair $a$. Finding the win matrix is a relatively straightforward mean-estimation problem; we will provide details in Section 5.</p>
<p>Formally, consider a <em>score</em> $s(\mathbb{P}) \in \mathbb{R}^{M}$, where $\mathbb{P}$ is a joint distribution over $A$ and $H$ (by default, we will target a uniform distribution over $\mathcal{A}$). Each model has a true score $s(\mathbb{P})_m$, and better models will have higher scores. In particular, we have the rank of model $m$:</p>
<p>$$ \text{rank}(\mathbb{P})<em _M_="[M]" _in="\in" m_="m'">m = 1 + \sum</em>)_m} . \qquad (1) $$} \mathbb{1} {s(\mathbb{P})_m' &gt; s(\mathbb{P</p>
<p>The best model has rank 1. If there is another model tied for best, they will both get assigned rank 1.</p>
<p><strong>Picking a score.</strong> A standard score function in this setting is the vector of Bradley-Terry (BT) coefficients (Bradley &amp; Terry, 1952). In the Bradley-Terry model, $H_t \in {0, 1}$, and the probability model $m$ beats model $m'$ is modeled via a logistic relationship:</p>
<p>$$\mathbb{P}(H_t = 1) = \frac{1}{1 + e^{\xi_m' - \xi_m}}, \tag{2}$$</p>
<p>where $\xi$ is an $M$-length vector of so-called BT coefficients. Without loss of generality, we take $\xi_1 = 0$ (since the model is invariant to addition in $\xi$). Our goal is to estimate the population Bradley-Terry coefficients, i.e., those that minimize the binary cross-entropy:</p>
<p>$$ s(\mathbb{P}) = \operatorname*{argmin}<em _A_H_="(A,H)" _mathbb_P="\mathbb{P" _sim="\sim">{\xi} E</em> \right) \right], \qquad (3) $$}} \left[ \ell \left( H, \frac{1}{1 + e^{\xi_{A_2} - \xi_{A_1}}</p>
<p>where $\ell$ is the binary cross-entropy loss, $\ell(h, p) = -(h \log(p) + (1 - h) \log(1 - p))$.</p>
<p>Although the BT model technically assumes a parametric form for the model win rates, the seminal results of Huber et al. (1967); White (1982) show that maximum likelihood estimators are still asymptotically normal even when these assumptions <em>do not</em> hold, so long as the so-called “sandwich” covariance matrix is used; see Section 5 for details, and see Appendix B for a nonparametric extension of the Bradley-Terry model. Finally, we remark that previous evolutions of our online interface have reported different ranking scores, such as the Elo score (Elo, 1967) instead of the BT coefficients. We made this change because the BT coefficients are better for the purpose of statistical estimation.</p>
<h2>5. Efficient Approximate Ranking</h2>
<p>In Section 4 we described how to calculate the win matrix, score, and rank. Now we describe our estimation procedures.</p>
<p>Win matrix estimation. Estimation of the win matrix is relatively straightforward. Define $X_{t}(a)=$ $\frac{1}{P_{t}(a)} H_{t} \mathbb{1}\left{A_{t}=a\right}$, where $P_{t}(a)$ is the probability of sampling pair $a$ at time $t$, and $X_{t}$ as the according vector. Then the estimator is</p>
<p>$$
\hat{\theta}<em t="1">{T}=\frac{1}{T} \sum</em>
$$}^{T} X_{t</p>
<p>Note that $\mathbb{E}\left[X_{t}(a)\right]=\theta^{<em>}(a)$ for all $t$, and thus $\hat{\theta}_{T}$ is an unbiased estimator of $\theta^{</em>}$. We will furthermore estimate the covariance matrix as</p>
<p>$$
\widehat{\Sigma}<em t="1">{T}=\frac{1}{T} \sum</em>}^{T}\left(X_{t}-\hat{\theta<em t="t">{T}\right)\left(X</em>
$$}-\hat{\theta}_{T}\right)^{\top</p>
<p>Under the appropriate regularity conditions, we have that</p>
<p>$$
\sqrt{T} \widehat{\Sigma}^{-1 / 2}\left(\hat{\theta}-\theta^{*}\right) \rightarrow \mathcal{N}\left(0, I_{d}\right)
$$</p>
<p>and we construct confidence intervals accordingly. For an understanding of the appropriate regularity conditions, see Durrett (2019), Theorem 8.2.8, where condition (ii) is trivially satisfied so long as $P_{t}(a)&gt;\epsilon&gt;0$, and condition (i) is implied by the almost-sure convergence of $P_{t}(a)$ to a limiting distribution $P(a)$.
Estimating the BT scores. To estimate the BT coefficients, mirroring (3), we perform (reweighted) maximum likelihood estimation on our data points:</p>
<p>$$
s(\hat{\mathbb{P}})=\underset{\xi}{\operatorname{argmin}} \sum_{t=1}^{T} \frac{1}{P\left(A_{t}\right)} \ell\left(H_{t}, \frac{1}{1+e^{\xi_{A_{t, 2}}-\xi_{A_{t, 1}}}}\right)
$$</p>
<p>where $A_{t} \sim P$. We perform the inverse weighting by $P\left(A_{t}\right)$ because this allows us to target a score with a uniform distribution over $A$.</p>
<p>To compute confidence intervals on the BT coefficients, we employ two strategies: (1) the pivot bootstrap (DiCiccio \&amp; Efron, 1996), and (2) the "sandwich" robust standard errors outlined in Huber et al. (1967) (see also Freedman (2006) for an outline of the necessary technical assumptions). Ultimately, based on the results of a simulation study described in Appendix A, we choose to deploy the sandwich intervals due to their smaller size in large samples.
Approximate rankings. Finally, we report an approximate ranking for each model that accounts for the uncertainty in the estimation of the score. Given an $M$-dimensional confidence set $\mathcal{C}$ satisfying</p>
<p>$$
\mathbb{P}(s(\mathbb{P}) \in \mathcal{C}) \geq 1-\alpha
$$</p>
<p>we extract an approximate ranking $R_{m}=1+$ $\sum_{m^{\prime} \in[M]} \mathbb{1}\left{\inf \mathcal{C}<em m="m">{m^{\prime}}&gt;\sup \mathcal{C}</em>)}\right}$. The uniform validity of $\mathcal{C}$ directly implies that $\mathbb{P}\left(\exists m: R_{m}&gt;\operatorname{rank}(\mathbb{P<em 1-_alpha_="1-\alpha," M-1="M-1">{m}\right) \leq \alpha-$ i.e., with high probability, no model's performance is understated. A guarantee on the other side-that no model's performance is overstated-is possible by interchanging the inf and sup. To get the uniform confidence set, we construct the chi-squared interval implied by the central limit theorem using the sandwich estimate of the variance. In other words, we construct the interval $\left{\xi: T\left|\hat{V}^{-1 / 2}(\hat{\xi}-\xi)\right| \leq\right.$ $\chi</em>$ is the sandwich variance of the logistic regression.
Active sampling rule. Our sampling rule was to choose the model pair $a \in \mathcal{A}$ proportionally to the reduction in confidence interval size by sampling that pair:}^{2}$, where $\hat{\xi}$ is our MLE of the BT coefficients and $\hat{V}_{\xi</p>
<p>$$
P_{t}(a) \propto \sqrt{\frac{\hat{\Sigma}<em t="t">{t, a, a}}{\left|\left{t: A</em>}=a\right}\right|}}-\sqrt{\frac{\hat{\Sigma<em t="t">{t, a, a}}{\left|\left{t: A</em>
$$}=a\right}\right|+1}</p>
<h3>5.1. Detecting Anomalous Users</h3>
<p>On a different note, we take a first step towards identifying anomalous IP addresses in our dataset. In a dataset of $U$ unique IPs, we let $\mathrm{IP}={1, \ldots, U}$ be the set of all IP addresses. Consider a "test" user, outside this database, who gives ratings $H_{1}^{\prime}, \ldots, H_{n}^{\prime}$ when presented actions $A_{1}^{\prime}, \ldots, A_{n}^{\prime}$. The idea of our procedure is to compare the distribution of ratings for the new user to the historical distribution of ratings for a given action. We let $\mathcal{H}<em t="t">{a}=\left{H</em>=a\right}$ and every time a user submits a vote, we calculate the following number:}: A_{t</p>
<p>$$
p_{i}=\frac{1}{\left|\mathcal{H}<em i="i">{A</em>}^{\prime}}\right|+1}\left(1+\sum_{h \in \mathcal{H<em i="i">{A</em>\right}\right)
$$}^{\prime}}} \mathbb{1}\left{h \geq H_{i}^{\prime</p>
<p>Under the null hypothesis that $\mathcal{H}<em i="i">{A</em>$ is a valid p-value (see Appendix C for a proof). Furthermore, the dependence of these p -values asymptotically is negligible.
With this p-value in hand, we can test against this null hypothesis sequentially by using Fisher's combination test (Fisher, 1928) along with a variant of the Bonferroni correction. In particular, for each user, after their $j$ th vote, we compute $M_{j}=-2 \sum_{i=1}^{j} \log \left(p_{i}\right)$. At 5 randomly chosen values of $j$ between 1 and 100 , we identify a user as anomalous if $M_{j} \geq \chi_{2 j, 1-\alpha / 5}^{2}$. (The times are randomly chosen, as to avoid anomalous users strategizing to hack this p-value.) Despite the heuristic application of this procedure, it seems to work well in our small-scale tests reported in Table 5.}^{\prime}}$ is exchangeable with $H_{i}^{\prime}, p_{i</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3. Similarity matrix of top-16 topic clusters. The number followed by the topic label represents the cluster size in percentage. Note that similarity is computed by cluster's centroid embeddings, hence diagonals are always one.</p>
<h1>6. Data Analysis</h1>
<p>To examine whether Arena's crowdsourced data reflects real-world use cases, we conduct topic modeling on the user prompts. We show how effective are these prompts in distinguishing models. Lastly, we validate the vote quality by relabeling data with experts.</p>
<h3>6.1. Topic Modeling on User Prompts</h3>
<p>To study the prompt diversity, we build a topic modeling pipeline with BERTopic ${ }^{3}$ (Grootendorst, 2022). We start with transforming user prompts into representation vectors using OpenAI's text embedding model (text-embedding-3small). To mitigate the curse of dimensionality for data clustering, we employ UMAP (Uniform Manifold Approximation and Projection) (McInnes et al., 2020) to reduce the embedding dimension from 1,536 to 5 . We then use the hierarchical density-based clustering algorithm, HDBSCAN, to identify topic clusters with minimum cluster size 32. Finally, to obtain topic labels, we sample 10 prompts from each topic cluster and feed into GPT-4-Turbo for topic summarization.</p>
<p>The pipeline identifies 600 clusters covering a wide range of topics including poetry writing, coding, math, and medical queries. We present the top-16 topic clusters in Figure 3. We observe that the largest cluster only accounts for $1 \%$ of the entire set and the rest quickly drop to $&lt;0.5 \%$, and the similarity between clusters is small, showing a long-tail and diverse distribution. Due to space limit, we present the similarity matrix and cluster hierarchy of top-64 clusters in Figure 11 and 12 in Appendix.</p>
<h3>6.2. Can Arena Prompts Distinguish Models?</h3>
<p>Next, we study how effective are these topic clusters in distinguishing models strengths. Constructing challenging prompts has become increasingly difficult due to LLMs'</p>
<p>Table 2. GPT-4-0613's win-rate against Llama-2-70b-chat on 30 sample prompts from various topic clusters. We use GPT-4-turbo as judge to evaluate model responses in pairwise comparison.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Topic Cluster</th>
<th style="text-align: center;">Win-rate</th>
<th style="text-align: center;">Size</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Python Game Programming Challenge</td>
<td style="text-align: center;">$96.7 \%$</td>
<td style="text-align: center;">$0.2 \%$</td>
</tr>
<tr>
<td style="text-align: left;">C/C++ Process Multi-Threading</td>
<td style="text-align: center;">$86.7 \%$</td>
<td style="text-align: center;">$0.3 \%$</td>
</tr>
<tr>
<td style="text-align: left;">SQL Query Database Assistance</td>
<td style="text-align: center;">$73.3 \%$</td>
<td style="text-align: center;">$0.2 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Poetry Writing Prompts</td>
<td style="text-align: center;">$66.7 \%$</td>
<td style="text-align: center;">$1.1 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Python Coding Basics</td>
<td style="text-align: center;">$65.0 \%$</td>
<td style="text-align: center;">$0.2 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Linguistic Analysis \&amp; Wordplay</td>
<td style="text-align: center;">$58.3 \%$</td>
<td style="text-align: center;">$0.7 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Travel Itinerary Planning</td>
<td style="text-align: center;">$58.3 \%$</td>
<td style="text-align: center;">$0.4 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Movie Recommendations \&amp; Ratings</td>
<td style="text-align: center;">$53.3 \%$</td>
<td style="text-align: center;">$0.2 \%$</td>
</tr>
</tbody>
</table>
<p>fast growing capabilities. For example, open models such as Llama-2-70b-chat can likely answer inquiries about movie or travel recommendation as good as GPT-4, but not in other domains such as reasoning or coding. To demonstrate, we sample 30 prompts from seven topic clusters and compare the performance of Llama-2-70b-chat and GPT-4. To control variables, we factor out user votes and consider LLM-as-judge (Zheng et al., 2023b) to evaluate model response. Results are shown in Table 2, where we see GPT-4 has significantly higher win-rate (up to $97 \%$ ) in clusters that require coding and reasoning skills. On the other hand, for clusters with less problem-solving tasks, GPT-4 win-rate drops to below $60 \%$. We show examples in Appendix D.1. This result shows models may exhibit varying strengths in different areas, but also highlights some of the topic clusters in Chatbot Arena are effective in differentiate models.</p>
<p>Building Challenging Benchmark. To further demonstrate the prompt quality, we show it is possible to construct a challenging benchmark with crowd-sourced user prompts. To ensure both topic coverage and quality, we first run the topic modeling pipeline and follow a similar procedure in Zheng et al. (2023a) to select challenging questions sampled from each topic cluster. Examples prompts and evaluation procedures can be found in the Appendix D. 2 and Appendix D.3, respectively. We observe the selected prompts are highly effective in differentiating models. In Figure 4, we compare Arena bench against a widely used LLM benchmark, MTBench (Zheng et al., 2023b). We can see that Arena Bench effectively reveals a significant gap in performance between proprietary and the strongest open models.</p>
<h3>6.3. Validating Vote Quality</h3>
<p>To assess the quality of crowdsourced votes, we randomly selected 160 battles between GPT-4-Turbo and Llama-213B, as well as GPT-4-Turbo and GPT-3.5-Turbo-0613. We then asked experts ${ }^{4}$ to label their preference per comparison. The experts were given the prompts and answers blindly, and asked to carefully fact-check model's answer with external resources like search engine. Manually labeling each</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4. Model's performance between Arena Bench and MTBench, showing an increased gap between open and proprietary models. Both uses GPT-4 as judge.</p>
<p>Table 3. Pairwise agreement rate between crowd-user, gpt-4 judge, and experts on pairwise battles. The top part of the table is between GPT-4-Turbo and Llama-2-13b-chat. The bottom is between GPT-4-Turbo and GPT-3.5-Turbo-0613.</p>
<table>
<thead>
<tr>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>Llama-2-13b</td>
<td>Expert 1</td>
<td>Expert 2</td>
<td>GPT-4</td>
</tr>
<tr>
<td>Crowd</td>
<td>72.8\%</td>
<td>77.8\%</td>
<td>75.6\%</td>
</tr>
<tr>
<td>Expert 1</td>
<td>-</td>
<td>89.8\%</td>
<td>81.0\%</td>
</tr>
<tr>
<td>Expert 2</td>
<td>-</td>
<td>-</td>
<td>78.5\%</td>
</tr>
<tr>
<td>GPT-3.5-Turbo</td>
<td>Expert 1</td>
<td>Expert 2</td>
<td>GPT-4</td>
</tr>
<tr>
<td>Crowd</td>
<td>73.8\%</td>
<td>83.1\%</td>
<td>75.6\%</td>
</tr>
<tr>
<td>Expert 1</td>
<td>-</td>
<td>79.4\%</td>
<td>76.3\%</td>
</tr>
<tr>
<td>Expert 2</td>
<td>-</td>
<td>-</td>
<td>79.3\%</td>
</tr>
</tbody>
</table>
<p>data point took on average 3-5 minutes. For reference, we also use GPT-4 as a judge for pairwise comparisons. The agreement rate between crowd-users, experts, and GPT-4judge are presented in Table 3. The corresponsing win-rate are shown in Table 4.</p>
<p>To summarize, we observe high agreement rates ( $72 \%$ to $83 \%$ ) between Arena crowd-user and experts in both setup. Note that agreement rates between two experts are around similar levels ( $79.4 \%$ and $89.8 \%$ ). As for the $10 \%-20 \%$ disagreement between experts, it is mostly due to some user prompts don't have a ground truth answer. Depending on the preference of the evaluator, sometimes both answers can be argued as being better than the other one, such as the examples in Appendix D.4. The gap between crowd-vs-expert agreement rate and expert-vs-expert agreement rate ( $5 \%-10 \%$ ) is mostly attributed to crowd user making mistakes or overlooking factual errors in model's response. Overall, the agreement rates presented in Table 3 validate the decent quality of crowd-sourced votes in Chatbot Arena.</p>
<p>Table 4. GPT-4-Turbo's win-rate across crowd-user, gpt-4 judge, and experts on pairwise battles against Llama-2-13b and GPT-3.5-Turbo-0613.</p>
<table>
<thead>
<tr>
<th>Baseline</th>
<th>Arena User</th>
<th>Expert 1</th>
<th>Expert 2</th>
<th>GPT-4</th>
</tr>
</thead>
<tbody>
<tr>
<td>Llama-2-13b</td>
<td>81.2\%</td>
<td>89.4\%</td>
<td>86.9\%</td>
<td>78.8\%</td>
</tr>
<tr>
<td>GPT-3.5-Turbo</td>
<td>76.3\%</td>
<td>82.5\%</td>
<td>89.4\%</td>
<td>79.4\%</td>
</tr>
</tbody>
</table>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5. Intervals for the BT coefficients with and without multiplicity correction. The multiplicity correction, in this case a chi-square CLT interval, is technically required for the purpose of calculating the ranking, because it ensures all scores are simultaneously contained in their intervals (and the ranking is a function of all the scores). However, it induces extra conservatism, so we report both intervals.</p>
<h2>7. Experiments</h2>
<h3>7.1. Ranking system</h3>
<p>Computing the rank on real data. In this section, we report results from our experiments on approximate ranking. For this experiment, we ran a replay of $T=213,576$ historical votes from our online platform and calculate the BT coefficients using our earlier-described estimation algorithm with confidence intervals; see Figure 5 for these intervals (with and without multiplicity correction; the formal notion of approximate ranking technically requires multiplicity correction, but it makes the intervals looser).</p>
<p>Evaluating the coverage of the intervals. A natural followup question is whether or not the intervals are doing their job correctly: whether they cover the true BT coefficients with probability at least (and almost exactly) $1-\alpha$. Of course,</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6. Intervals for the BT coefficients as a function of the number of samples and the number of models $M$.
this cannot be evaluated on real data, so we run a simulation. A vector of BT coefficients is drawn, with each coordinate sampled i.i.d. from a distribution beta $(1 / \gamma, 1 / \gamma)$; we take $\gamma=2$ in Figure 6 (and we vary $\gamma$ in Appendix A). Given these coefficients, a dataset is synthesized, and the coverage and average width are computed for each of 20 trials. The results can be seen in Figure 6 for the uncorrected intervals The coverage of the intervals behaves as expected, centering around $1-\alpha$, regardless of the number of models. Meanwhile, the more models are included, the larger the intervals become.</p>
<p>Evaluating the active sampling rule. Next, we discuss the evaluation of our active sampling rule as Equation (9) for win matrix estimation. We evaluate this sampling rule by taking the best fit BT coefficients to our 213,576 point sized holdout set, and then sampling from that distribution using our active sampling algorithm. The results are displayed in Figure 7. It is hard to tell by looking at plots, but the improvement is substantial: To estimate $\theta^{*}$ to a precision of 0.2 , random needs 6,800 samples and adaptive needs 4,400 samples; meanwhile to estimate the score to a precision of 0.3 , random needs 17,200 samples and adaptive needs 16,400 samples. Thus, the random baseline requires $54 \%$ and $5 \%$ more data to achieve the same level of precision, respectively. One can see from the plots in Figure 7 that these results are not cherry-picked: the sample-efficiency of our method is better at all values on the horizontal axis.</p>
<h3>7.2. Anomalous Users Detection</h3>
<p>We evaluate the outlier detection method in Section 5.1. We construct the evaluation set by manually identifying 25 anomalous users whose inputs are highly repetitive or meaningless (e.g., asking "hi" for 100 times or inputting garbled texts). We randomly sample 25 normal users with
<img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7. Interval widths on the win matrix (upper figure) and on the BT coefficients (lower figure) as a function of the number of samples, for random sampling and also adaptive sampling. Improvements from adaptive sampling can be seen in both cases, although they are more subtle on the scale of the score.</p>
<p>Table 5. Confusion matrix of different $\alpha$. "Pred." means predicted. Positive means anomalous and negative means normal.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">$\alpha=0.1$</th>
<th style="text-align: right;">Pred. Positive</th>
<th style="text-align: right;">Pred. Negative</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Actual Positive</td>
<td style="text-align: right;">$13 / 14$</td>
<td style="text-align: right;">$12 / 36$</td>
</tr>
<tr>
<td style="text-align: left;">Actual Negative</td>
<td style="text-align: right;">$1 / 14$</td>
<td style="text-align: right;">$24 / 36$</td>
</tr>
<tr>
<td style="text-align: left;">$\alpha=0.3$</td>
<td style="text-align: right;">Pred. Positive</td>
<td style="text-align: right;">Pred. Negative</td>
</tr>
<tr>
<td style="text-align: left;">Actual Positive</td>
<td style="text-align: right;">$21 / 29$</td>
<td style="text-align: right;">$4 / 21$</td>
</tr>
<tr>
<td style="text-align: left;">Actual Negative</td>
<td style="text-align: right;">$8 / 29$</td>
<td style="text-align: right;">$17 / 21$</td>
</tr>
</tbody>
</table>
<p>at least 50 votes, and inspect their input prompts to ensure no abnormal behaviors. As mentioned in Section 5.1, per user we compute five $M_{j}$ and identify the user as anomalous if $M_{j} \geq \chi_{2 j, 1-\alpha / 5}^{2}$. We present results of two different $\alpha$ (i.e., the significance level) in Table 5. We find the detection method effective (e.g., reaching $90 \%$ true positive and $60-70 \%$ true negative rate). We inspect the false negative errors and find those are from users do not always behave abnormally, making them harder to detect.</p>
<h2>8. Discussion</h2>
<p>Limitations. Although our user base is extensive, we anticipate that it will primarily consist of LLM hobbyists and researchers who are eager to experiment with and evaluate the latest LLMs. This inclination may result in a biased distribution of users. Additionally, despite the wide array of topics encompassed by the prompts discussed in previous sections, the data predominantly comes from our online chat interface. This source might not accurately reflect the real-world usage of LLMs in production environments or specialized domains, potentially leading to a skewed prompt distribution. Moreover, our study concentrates on assessing the helpfulness of LLMs but overlooks their safety aspects. We recognize the possibility and necessity of a parallel</p>
<p>mechanism to evaluate the safety of these models.
Future Directions. In our future work, we plan to develop comprehensive topic leaderboards and establish a dedicated section for multimodal and agent-based LLMs in more dynamic, gamified settings, catering to more complex tasks. We also believe our approach to detecting harmful users could be improved and made more formally rigorous by using the theory of nonnegative supermartingales and Evalues (Howard et al., 2020; Waudby-Smith \&amp; Ramdas, 2020; Vovk \&amp; Wang, 2021; Ramdas et al., 2023); this would deal with the dependence, but the variants we tried did not perform well in terms of power.</p>
<h2>9. Conclusion</h2>
<p>In this paper, we present Chatbot Arena, an open platform for evaluating LLMs through crowdsourced, pairwise human preferences. We conduct an in-depth analysis of the crowdsourced user prompts and preference votes to validate the diversity and quality. We develop an efficient model sampling and ranking algorithm. Our dataset including 100 K pairwise preference votes will be released for future research.</p>
<h2>Acknowledgments</h2>
<p>This project is supported by sponsorship from Kaggle, MBZUAI, a16z, Together AI, Anyscale, and HuggingFace. This project is also partly supported by Accenture, AMD, Google, IBM, Intel, Microsoft, Samsung SDS, SAP, Uber, and VMware. The authors would like to thank Siyuan Zhuang for insightful discussion and Tijana Zrnić for helpful feedback on the manuscript.</p>
<h2>References</h2>
<p>Bai, Y., Jones, A., Ndousse, K., Askell, A., Chen, A., DasSarma, N., Drain, D., Fort, S., Ganguli, D., Henighan, T., et al. Training a helpful and harmless assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862, 2022.</p>
<p>Boubdir, M., Kim, E., Ermis, B., Hooker, S., and Fadaee, M. Elo uncovered: Robustness and best practices in language model evaluation, 2023.</p>
<p>Bradley, R. A. and Terry, M. E. Rank analysis of incomplete block designs: I. the method of paired comparisons. Biometrika, 39(3/4):324-345, 1952.</p>
<p>Busa-Fekete, R., Huellermeier, E., and Szörényi, B. Preference-based rank elicitation using statistical models: The case of mallows. In Xing, E. P. and Jebara, T. (eds.), Proceedings of the 31st International Conference on Machine Learning, volume 32 of Proceedings of Machine</p>
<p>Learning Research, pp. 1071-1079, Bejing, China, 2224 Jun 2014a. PMLR. URL https://proceedings. mlr.press/v32/busa-fekete14.html.</p>
<p>Busa-Fekete, R., Huellermeier, E., and Szörényi, B. Preference-based rank elicitation using statistical models: The case of mallows. In Xing, E. P. and Jebara, T. (eds.), Proceedings of the 31st International Conference on Machine Learning, volume 32 of Proceedings of Machine Learning Research, pp. 1071-1079, Bejing, China, 22-24 Jun 2014b. PMLR. URL https://proceedings. mlr.press/v32/busa-fekete14.html.</p>
<p>Chen, M., Tworek, J., Jun, H., Yuan, Q., Pinto, H. P. d. O., Kaplan, J., Edwards, H., Burda, Y., Joseph, N., Brockman, G., et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021.</p>
<p>Chernoff, H. Sequential Design of Experiments, pp. 345-360. Springer New York, New York, NY, 1992. ISBN 978-1-4612-4380-9. doi: 10.1007/ 978-1-4612-4380-9_27. URL https://doi.org/ 10.1007/978-1-4612-4380-9_27.</p>
<p>Chiang, C.-H. and Lee, H.-y. Can large language models be an alternative to human evaluations? In Rogers, A., Boyd-Graber, J., and Okazaki, N. (eds.), Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 15607-15631, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023. acl-long.870. URL https://aclanthology.org/ 2023.acl-long. 870.</p>
<p>Cobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H., Kaiser, L., Plappert, M., Tworek, J., Hilton, J., Nakano, R., et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021.</p>
<p>Cui, G., Yuan, L., Ding, N., Yao, G., Zhu, W., Ni, Y., Xie, G., Liu, Z., and Sun, M. Ultrafeedback: Boosting language models with high-quality feedback, 2023.</p>
<p>DiCiccio, T. J. and Efron, B. Bootstrap confidence intervals. Statistical science, 11(3):189-228, 1996.</p>
<p>Durrett, R. Probability: theory and examples, volume 49. Cambridge university press, 2019.</p>
<p>Elo, A. E. The proposed uscf rating system, its development, theory, and applications. Chess Life, 22(8):242247, 1967.</p>
<p>Fisher, R. A. Statistical methods for research workers. Number 5. Oliver and Boyd, 1928.</p>
<p>Freedman, D. A. On the so-called "huber sandwich estimator"' and "robust standard errors"'. The American Statistician, 60(4):299-302, 2006.</p>
<p>Gemini, T., Anil, R., Borgeaud, S., Wu, Y., Alayrac, J.-B., Yu, J., Soricut, R., Schalkwyk, J., Dai, A. M., Hauth, A., et al. Gemini: a family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023.</p>
<p>Geng, X., Gudibande, A., Liu, H., Wallace, E., Abbeel, P., Levine, S., and Song, D. Koala: A dialogue model for academic research. Blog post, April 2023. URL https://bair.berkeley.edu/ blog/2023/04/03/koala/.</p>
<p>Grootendorst, M. Bertopic: Neural topic modeling with a class-based tf-idf procedure. arXiv preprint arXiv:2203.05794, 2022.</p>
<p>Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D., and Steinhardt, J. Measuring massive multitask language understanding. In International Conference on Learning Representations, 2020.</p>
<p>Howard, S. R., Ramdas, A., McAuliffe, J., and Sekhon, J. Time-uniform chernoff bounds via nonnegative supermartingales. 2020.</p>
<p>Huang, Y., Lin, Z., Liu, X., Gong, Y., Lu, S., Lei, F., Liang, Y., Shen, Y., Lin, C., Duan, N., et al. Competition-level problems are effective llm evaluators. arXiv preprint arXiv:2312.02143, 2023.</p>
<p>Huber, P. J. et al. The behavior of maximum likelihood estimates under nonstandard conditions. In Proceedings of the fifth Berkeley symposium on mathematical statistics and probability, volume 1, pp. 221-233. Berkeley, CA: University of California Press, 1967.</p>
<p>Hunter, D. R. MM algorithms for generalized BradleyTerry models. The Annals of Statistics, 32(1):384 - 406, 2004. doi: 10.1214/aos/1079120141. URL https: //doi.org/10.1214/aos/1079120141.</p>
<p>Karimi, M. R., Gürel, N. M., Karlaš, B., Rausch, J., Zhang, C., and Krause, A. Online active model selection for pre-trained classifiers. In International Conference on Artificial Intelligence and Statistics, pp. 307-315. PMLR, 2021.</p>
<p>Karpinska, M., Akoury, N., and Iyyer, M. The perils of using Mechanical Turk to evaluate open-ended text generation. In Moens, M.-F., Huang, X., Specia, L., and Yih, S. W.-t. (eds.), Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 1265-1285, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main. 97. URL https://aclanthology.org/2021. emnlp-main. 97.</p>
<p>Kiela, D., Bartolo, M., Nie, Y., Kaushik, D., Geiger, A., Wu, Z., Vidgen, B., Prasad, G., Singh, A., Ringshia, P., et al. Dynabench: Rethinking benchmarking in nlp. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 4110-4124, 2021.</p>
<p>Köpf, A., Kilcher, Y., von Rütte, D., Anagnostidis, S., Tam, Z.-R., Stevens, K., Barhoum, A., Duc, N. M., Stanley, O., Nagyfi, R., et al. Openassistant conversationsdemocratizing large language model alignment. arXiv preprint arXiv:2304.07327, 2023.</p>
<p>Langley, P. Crafting papers on machine learning. In Langley, P. (ed.), Proceedings of the 17th International Conference on Machine Learning (ICML 2000), pp. 1207-1216, Stanford, CA, 2000. Morgan Kaufmann.</p>
<p>Li, X., Zhang, T., Dubois, Y., Taori, R., Gulrajani, I., Guestrin, C., Liang, P., and Hashimoto, T. B. Alpacaeval: An automatic evaluator of instruction-following models. https://github.com/tatsu-lab/ alpaca_eval, 2023.</p>
<p>Li, Y., Choi, D., Chung, J., Kushman, N., Schrittwieser, J., Leblond, R., Eccles, T., Keeling, J., Gimeno, F., Dal Lago, A., et al. Competition-level code generation with alphacode. Science, 378(6624):1092-1097, 2022.</p>
<p>Liang, P., Bommasani, R., Lee, T., Tsipras, D., Soylu, D., Yasunaga, M., Zhang, Y., Narayanan, D., Wu, Y., Kumar, A., et al. Holistic evaluation of language models. arXiv preprint arXiv:2211.09110, 2022.</p>
<p>Lin, Z., Wang, Z., Tong, Y., Wang, Y., Guo, Y., Wang, Y., and Shang, J. ToxicChat: Unveiling hidden challenges of toxicity detection in real-world user-AI conversation. In Bouamor, H., Pino, J., and Bali, K. (eds.), Findings of the Association for Computational Linguistics: EMNLP 2023, pp. 4694-4702, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.findings-emnlp. 311. URL https://aclanthology.org/2023. findings-emnlp. 311.</p>
<p>Liu, T.-Y. et al. Learning to rank for information retrieval. Foundations and Trends ${ }^{\circledR}$ in Information Retrieval, 3(3): 225-331, 2009.</p>
<p>McInnes, L., Healy, J., and Melville, J. Umap: Uniform manifold approximation and projection for dimension reduction, 2020.</p>
<p>OpenAI. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023.</p>
<p>Oren, Y., Meister, N., Chatterji, N., Ladhak, F., and Hashimoto, T. B. Proving test set contamination in black box language models. arXiv preprint arXiv:2310.17623, 2023.</p>
<p>Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C. L., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., Schulman, J., Hilton, J., Kelton, F., Miller, L., Simens, M., Askell, A., Welinder, P., Christiano, P., Leike, J., and Lowe, R. Training language models to follow instructions with human feedback, 2022.</p>
<p>Ramdas, A., Grünwald, P., Vovk, V., and Shafer, G. Gametheoretic statistics and safe anytime-valid inference. Statistical Science, 38(4):576-601, 2023.</p>
<p>Rao, P. V. and Kupper, L. L. Ties in paired-comparison experiments: A generalization of the bradley-terry model. Journal of the American Statistical Association, 62(317): 194-204, 1967. doi: 10.1080/01621459.1967.10482901.</p>
<p>Srivastava, A., Rastogi, A., Rao, A., Shoeb, A. A. M., Abid, A., Fisch, A., Brown, A. R., Santoro, A., Gupta, A., Garriga-Alonso, A., et al. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. Transactions on Machine Learning Research, 2023.</p>
<p>Szörényi, B., Busa-Fekete, R., Paul, A., and Hüllermeier, E. Online rank elicitation for plackettluce: A dueling bandits approach. In Cortes, C., Lawrence, N., Lee, D., Sugiyama, M., and Garnett, R. (eds.), Advances in Neural Information Processing Systems, volume 28. Curran Associates, Inc., 2015. URL https://proceedings.neurips. cc/paper_files/paper/2015/file/ 7eacb532570ff6858afd2723755ff790-Paper. pdf.</p>
<p>Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., et al. Llama 2: Open foundation and finetuned chat models. arXiv preprint arXiv:2307.09288, 2023.</p>
<p>Vovk, V. and Wang, R. E-values: Calibration, combination and applications. The Annals of Statistics, 49(3):17361754, 2021.</p>
<p>Wang, Y., Kordi, Y., Mishra, S., Liu, A., Smith, N. A., Khashabi, D., and Hajishirzi, H. Self-instruct: Aligning language models with self-generated instructions. In Rogers, A., Boyd-Graber, J., and Okazaki, N. (eds.), Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 13484-13508, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/
2023.acl-long.754. URL https://aclanthology. org/2023.acl-long. 754.</p>
<p>Waudby-Smith, I. and Ramdas, A. Estimating means of bounded random variables by betting. arXiv preprint arXiv:2010.09686, 2020.</p>
<p>White, H. Maximum likelihood estimation of misspecified models. Econometrica: Journal of the econometric society, pp. 1-25, 1982.</p>
<p>Yang, S., Chiang, W.-L., Zheng, L., Gonzalez, J. E., and Stoica, I. Rethinking benchmark and contamination for language models with rephrased samples. arXiv preprint arXiv:2311.04850, 2023.</p>
<p>Zellers, R., Holtzman, A., Bisk, Y., Farhadi, A., and Choi, Y. Hellaswag: Can a machine really finish your sentence? In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 4791-4800, 2019.</p>
<p>Zheng, L., Chiang, W.-L., Sheng, Y., Li, T., Zhuang, S., Wu, Z., Zhuang, Y., Li, Z., Lin, Z., Xing, E. P., Gonzalez, J. E., Stoica, I., and Zhang, H. Lmsys-chat-1m: A large-scale real-world llm conversation dataset, 2023a.</p>
<p>Zheng, L., Chiang, W.-L., Sheng, Y., Zhuang, S., Wu, Z., Zhuang, Y., Lin, Z., Li, Z., Li, D., Xing, E., Zhang, H., Gonzalez, J. E., and Stoica, I. Judging LLM-as-a-judge with MT-bench and chatbot arena. In Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2023b. URL https : //openreview.net/forum?id=uccHPGDlao.</p>
<p>Zhong, W., Cui, R., Guo, Y., Liang, Y., Lu, S., Wang, Y., Saied, A., Chen, W., and Duan, N. Agieval: A human-centric benchmark for evaluating foundation models. arXiv preprint arXiv:2304.06364, 2023.</p>
<p>Zhu, B., Frick, E., Wu, T., Zhu, H., and Jiao, J. Starling7b: Improving llm helpfulness \&amp; harmlessness with rlaif, November 2023.</p>
<h1>Chatbot Arena: Benchmarking LLMs in the Wild</h1>
<table>
<thead>
<tr>
<th style="text-align: center;">Blog</th>
<th style="text-align: center;">GitHub</th>
<th style="text-align: center;">Paper</th>
<th style="text-align: center;">Dataset</th>
<th style="text-align: center;">Twitter</th>
<th style="text-align: center;">Discord</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">圄 Rules <br> - Ask any question to two anonymous models (e.g., ChatGPT, Claude, Llama) and vote for the better one! <br> - You can continue chatting until you identify a winner. <br> - Vote won't be counted if model identity is revealed during conversation. <br> Arena Elo Leaderboard <br> We collect 300K+ human votes to compute an Elo-based LLM leaderboard. Find out who is the \&amp;LLM Champion! <br> Chat now!</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">(4) Expand to see the descriptions of 36 models</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">4</td>
</tr>
<tr>
<td style="text-align: center;">(1) Model A</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">(1) Model B</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">I found a property i would like to buy. please draft a letter to the owners expressing my interest to purchase and convincing them of completing a private sale with me. list the benefits of private sale and how it will be done with our lawyers.</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">I found a property i would like to buy. please draft a letter to the owners expressing my interest to purchase and convincing them of completing a private sale with me. list the benefits of private sale and how it will be done with our lawyers.</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Subject: Expression of Interest in Your Property for a Private Sale</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Subject: Expression of Interest to Purchase Your Property $\cdot$ Private Sale Proposal</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Dear [Owner's Name],</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Dear [Owner's Name],</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">I hope this message finds you well. My name is [Your Name], and I am writing to express my sincere interest in purchasing your property located at [Property Address]. I have recently had the privilege of viewing your property and am captivated by its unique features and the potential it holds.</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">I hope this letter finds you well. My name is [Your Name], and I recently had the opportunity to view your property at [Property Address]. I am writing to express my genuine interest in purchasing your property and to propose the possibility of completing this sale privately, directly between us, with the support of our respective lawyers.</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">I would like to propose a private sale as a method of transaction. This proposal aims to create a beneficial situation for both parties involved, and I am confident that this process will be both smooth and efficient.</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Firstly, I want to commend you on the care and upkeep of your property. It's clear that a great deal of effort has been invested into making it a desirable and welcoming space. After careful consideration and viewing several properties, I am convinced that your property meets all the criteria I am looking for in a home.</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">The advantages of a private sale are numerous. Firstly, it offers a more personal approach, as it allows for direct negotiation between the buyer and seller, resulting in potentially better terms for both parties. Secondly, a private sale can generally be completed more quickly than traditional sales, as there are fewer parties involved in the decision-making process. Thirdly, the</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">I understand that selling a property is a significant decision, and I believe that a private sale could offer considerable benefits to both of us. Here are a few reasons why I think a private sale would be advantageous:</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">(1) A is better</td>
<td style="text-align: center;">(2) B is better</td>
<td style="text-align: center;">(3) Tie</td>
<td style="text-align: center;">(4) Both are bad</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Enter your prompt and press ENTER</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Figure 8. Screenshot of Chatbot Arena.
<img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 9. The number of votes over time</p>
<p><img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 10. The number of votes per model.</p>
<h1>Similarity Matrix</h1>
<p><img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>Figure 11. Similarity matrix of top-64 topic clusters.</p>
<h1>Hierarchical Clustering</h1>
<p><img alt="img-10.jpeg" src="img-10.jpeg" /></p>
<p>Figure 12. Top-64 clusters visualized in hierarchy. x-axis represents the cosine similarity distance. y-axis shows the topic title per cluster summarized by gpt-4-turbo.</p>
<p><img alt="img-11.jpeg" src="img-11.jpeg" /></p>
<p>Figure 13. Replay experiment showing the intervals, coverage, and average interval sizes of the bootstrap and of the sandwich intervals. The sandwich intervals, though larger in small samples, are more stable, and in large samples, they actually become smaller. We use the multiplicity corrected version of both intervals, so they both have a coverage of 1. (Coverage here is calculated with respect to the BT coefficient solution on the full dataset, so it is not as meaningful as in the simulation plot below.)
<img alt="img-12.jpeg" src="img-12.jpeg" /></p>
<p>Figure 14. Synthetic experiment. Coefficients are drawn from the BT-coefficient distribution $x$ on the left. Coverage of the uncorrected intervals is shown in the middle. Line plots of set width are shown on the right, and they almost perfectly match.</p>
<h1>A. Confidence Interval Simulation Study</h1>
<p>We conduct a simulation study to evaluate the bootstrap confidence intervals versus the sandwich estimator. To a large extent, both intervals are the same-indeed, their intervals are often identical to the naked eye. Nonetheless, in our experiments, there are some differences. First, in Figure 13, we conduct a replay study using the same 213576 data points mentioned in the main text.</p>
<p>We also do a suite of experiments in simulation using the same beta generating process as in the main text, with $\gamma=2$. The result is shown in Figure 14; results are similar across many choices of the parameter $\gamma$ and the model strength, which indicates that both intervals will have good coverage and width in the practical conditions we would expose them to.</p>
<h2>B. The Nonparametric Bradley-Terry Model</h2>
<p>Nonparametric Bradley-Terry. We next consider a nonparametric extension of the Bradley-Terry (BT) model (Bradley \&amp; Terry, 1952) to the case where the ranking is not necessarily transitive. Let $\mathcal{G}(m)$ denote the set of all paths to the model $m$, i.e.,</p>
<p>$$
\mathcal{G}(m)=\left{g \in \mathcal{B}^{M-1}: g_{i, 1} \neq g_{j, 1}, \forall i \neq j, \text { and } g_{M-1,2}=m\right}
$$</p>
<p>where $\mathcal{B}=\mathcal{A} \cup\left{\left(a_{2}, a_{1}\right): a \in \mathcal{A}\right}$. Each element of $\mathcal{G}(m)$ is a chain of model pairings that leads to $m$; for example, if $m=5$ and $M=6$, one element of $\mathcal{G}(m)$ is $((1,2),(2,4),(4,3),(3,6),(6,5))$. Our score function is given by the average</p>
<p>path-sum of the log odds of the second model winning, over the entirety of $\mathcal{G}(m)$ :</p>
<p>$$
s(\theta)<em _in="\in" _mathcal_G="\mathcal{G" g="g">{m}=\frac{1}{|\mathcal{G}(m)|} \sum</em>\right)
$$}(m)}\left(\log \frac{\theta^{\prime}\left(\left(1, g_{1,1}\right)\right)}{1-\theta^{\prime}\left(\left(1, g_{1,1}\right)\right)}+\sum_{a \in g} \log \frac{\theta^{\prime}(a)}{1-\theta^{\prime}(a)</p>
<p>where $\theta^{\prime}(a)=\theta(a) \mathbb{1}{a \in \mathcal{A}}+\left(1-\theta\left(\left(a_{2}, a_{1}\right)\right)\right) \mathbb{1}{a \notin \mathcal{A}}$, with the convention that $\theta((m, m))=1 / 2$ for all $m$. Note that for any $g \in \mathcal{G}(m)$ where $a \in g$ and $m \notin a$, we also have some $g^{\prime} \in \mathcal{G}(m)$ such that $\left(a_{2}, a_{1}\right) \in g$. Meanwhile, if $a \in g$ and $m \in a$, then $a=\left(m^{\prime}, m\right)$ for some $m^{\prime}$. Thus, we can compute</p>
<p>$$
\begin{aligned}
s(\theta)<em _in="\in" _mathcal_A="\mathcal{A" _substack_a="\substack{a">{m} &amp; =\sum</em> \
m \notin a}} \frac{1}{2}\left(\log \frac{\theta^{\prime}(a)}{1-\theta^{\prime}(a)}+\log \frac{\theta^{\prime}\left(\left(a_{2}, a_{1}\right)\right)}{1-\theta^{\prime}\left(\left(a_{2}, a_{1}\right)\right)}\right)+\sum_{m^{\prime} \in[M] \backslash{m}}\left(\log \frac{\theta^{\prime}\left(\left(m^{\prime}, m\right)\right)}{1-\theta^{\prime}\left(\left(m^{\prime}, m\right)\right)}+\frac{\theta^{\prime}\left(\left(1, m^{\prime}\right)\right)}{1-\theta^{\prime}\left(\left(1, m^{\prime}\right)\right)}\right) \
&amp; =\sum_{\substack{a \in \mathcal{A} \
m \notin a}} \frac{1}{2}\left(\log \frac{\theta(a)}{1-\theta(a)}+\log \frac{1-\theta(a)}{\theta(a)}\right)+\sum_{m^{\prime} \in[M] \backslash{m}}\left(\log \frac{\theta^{\prime}\left(\left(m^{\prime}, m\right)\right)}{1-\theta^{\prime}\left(\left(m^{\prime}, m\right)\right)}+\frac{\theta^{\prime}\left(\left(1, m^{\prime}\right)\right)}{1-\theta^{\prime}\left(\left(1, m^{\prime}\right)\right)}\right) \
&amp; =\sum_{m^{\prime} \in[M] \backslash{m}}\left(\log \frac{\theta^{\prime}\left(\left(m^{\prime}, m\right)\right)}{1-\theta^{\prime}\left(\left(m^{\prime}, m\right)\right)}+\frac{\theta^{\prime}\left(\left(1, m^{\prime}\right)\right)}{1-\theta^{\prime}\left(\left(1, m^{\prime}\right)\right)}\right) \
&amp; =\sum_{m^{\prime} \in[M] \backslash{m}}\left(\left(1-2 \mathbb{1}\left{m^{\prime}&gt;m\right}\right) \log \frac{\theta\left(\left(m^{\prime}, m\right)\right)}{1-\theta\left(\left(m^{\prime}, m\right)\right)}+\frac{\theta\left(\left(1, m^{\prime}\right)\right)}{1-\theta\left(\left(1, m^{\prime}\right)\right)}\right)
\end{aligned}
$$</p>
<p>This score is always well-defined, and is a simple, smooth function of $\theta$. Its derivative is, for all $a \in \mathcal{A}$,</p>
<p>$$
\frac{\partial}{\partial \theta(a)} s(\theta)<em 2="2">{m}=\mathbb{1}\left{a</em>
$$}=m\right}\left(1-2 \mathbb{1}\left{a_{1}&gt;m\right}\right) \frac{1}{\theta(a)(1-\theta(a))}+\mathbb{1}\left{a_{1}=1, a_{2} \neq m\right} \frac{1}{\theta(a)(1-\theta(a))</p>
<p>How is the BT score related to the original Bradley-Terry model? In the original Bradley-Terry model, $H_{t} \in{0,1}$, and the probability of model $m$ beating model $m^{\prime}$ is assumed to be given by</p>
<p>$$
\theta\left(\left(m^{\prime}, m\right)\right)=\frac{e^{\xi_{m}}}{e^{\xi_{m}}+e^{\xi_{m^{\prime}}}}
$$</p>
<p>for some unknown parameters $\xi_{1}, \ldots, \xi_{M}$-the Bradley-Terry coefficients. The basic goal of the Bradley-Terry model is to estimate these parameters from the observed outcomes. In our setting, however, we use the outcomes to get a CLT on $\theta$, and then can immediately recover the coefficients. Taking without loss of generality $\xi_{1}=0$, we have that</p>
<p>$$
\begin{aligned}
\log \frac{\theta\left(\left(1, m^{\prime}\right)\right)}{1-\theta\left(\left(1, m^{\prime}\right)\right)}+\log \frac{\theta\left(\left(m^{\prime}, m\right)\right)}{1-\theta\left(\left(m^{\prime}, m\right)\right)} &amp; =\log \frac{\theta\left(\left(1, m^{\prime}\right)\right)}{\theta\left(\left(m^{\prime}, 1\right)\right)}+\log \frac{\theta\left(\left(m^{\prime}, m\right)\right)}{\theta\left(\left(m, m^{\prime}\right)\right)} \
&amp; =\log \frac{e^{\xi_{m^{\prime}}}\left(e^{\xi_{m^{\prime}}}+1\right)}{e^{\xi_{m^{\prime}}}+1}+\log \frac{e^{\xi_{m}}\left(e^{\xi_{m^{\prime}}}+e^{\xi_{m}}\right)}{e^{\xi_{m^{\prime}}}\left(e^{\xi_{m^{\prime}}}+e^{\xi_{m}}\right)} \
&amp; =\xi_{m^{\prime}}+\xi_{m}-\xi_{m^{\prime}}=\xi_{m}
\end{aligned}
$$</p>
<p>Thus, all the sums over paths in (12) are equal to $\xi_{m}-\xi_{g_{1,1}}$.</p>
<p>$$
\begin{aligned}
&amp; \log \frac{\theta^{\prime}\left(\left(1, g_{1,1}\right)\right)}{1-\theta^{\prime}\left(\left(1, g_{1,1}\right)\right)}+\sum_{a \in g} \log \frac{\theta^{\prime}(a)}{1-\theta^{\prime}(a)} \
= &amp; \xi_{g_{1,1}}+\xi_{g_{1,2}}-\xi_{g_{1,1}}+\xi_{g_{2,2}}-\xi_{g_{2,1}}+\cdots+\xi_{g_{M-1,2}}-\xi_{g_{M-1,1}} \
= &amp; \xi_{g_{M-1,2}}=\xi_{m}
\end{aligned}
$$</p>
<p>Thus, if the parametric BT model is well-specified, the nonparametric version will exactly recover the Bradley-Terry coefficients. However, our nonparametric analogue of the BT model has major advantages over the original: it will retain statistical validity even if $H_{t}$ is not binary, if the win rate is non-transitive, and if the logistic model assumed by the BT model is misspecified. In practice, the nonparametric BT coefficient can be easily computed by (16).</p>
<h1>C. Valid P-Value</h1>
<p>Consider the p -value</p>
<p>$$
p_{i}=\frac{1}{\left|\mathcal{H}<em i="i">{A</em>}^{\prime}}\right|+1}\left(1+\sum_{h \in \mathcal{H<em i="i">{A</em>\right}\right)
$$}^{\prime}}} \mathbb{1}\left{h \geq H_{i}^{\prime</p>
<p>We will prove that this p-value is valid, i.e., that $\mathbb{P}\left(p_{i} \leq t\right) \leq t$, under the null hypothesis that the vector $\mathcal{H}^{\prime}=\left(H_{t}: A_{t}=\right.$ $\left.A_{i}^{\prime}\right) |\left(H_{i}^{\prime}\right)$ is exchangeable, where $|$ denotes the concatenation operator. First, notice that we can equivalently write</p>
<p>$$
p_{i}=\frac{1}{\left|\mathcal{H}^{\prime}\right|} \sum_{h \in \mathcal{H}^{\prime}} \mathbb{1}\left{h \geq H_{i}^{\prime}\right}
$$</p>
<p>We also have that $\mathbb{P}\left(p_{i} \leq t\right) \leq \mathbb{P}\left(p_{i} \leq \frac{k}{\left|\mathcal{H}^{\prime}\right|}\right)$, where $k=\lfloor t\left|\mathcal{H}^{\prime}\right|\rfloor /\left|\mathcal{H}^{\prime}\right|$. Then, since the data points are exchangeable, we have that $H_{i}^{\prime}$ is uniformly distributed among the ranks of $\mathcal{H}^{\prime}$, so $\mathbb{P}\left(p_{i} \leq \frac{k}{\left|\mathcal{H}^{\prime}\right|}\right) \leq \frac{k}{\left|\mathcal{H}^{\prime}\right|} \leq t$, completing the proof.</p>
<h1>D. Sample Prompts</h1>
<h2>D.1. Prompt Examples From Different Clusters</h2>
<p>Example 1: An example user prompt from the cluster "Python Game Programming Challenge". This prompt distinctively separates GPT-4-0613 performance from Llama-2-70b-chat. The user request "higher quality code" from each assistant. While GPT-4-Turbo provides a correct implementation along with helpful explanation, Llama-2-70b-chat fail to provide any program and provided a less helpful response overall.</p>
<div class="codehilite"><pre><span></span><code><span class="o">&lt;|</span><span class="n">User</span> <span class="n">Prompt</span><span class="o">|&gt;</span>
<span class="n">Develop</span> <span class="n">a</span> <span class="n">snowman</span> <span class="n">game</span> <span class="n">similar</span> <span class="n">to</span> <span class="n">the</span> <span class="n">Hangman</span> <span class="n">game</span> <span class="n">but</span> <span class="n">draws</span> <span class="n">a</span> <span class="n">snowman</span>
<span class="n">figure</span> <span class="n">standing</span> <span class="n">on</span> <span class="n">the</span> <span class="n">snow</span> <span class="n">instead</span> <span class="n">of</span> <span class="n">a</span> <span class="n">hangman</span><span class="o">.</span> <span class="n">It</span> <span class="n">also</span> <span class="n">downloads</span> <span class="p">(</span><span class="ow">or</span>
<span class="n">uses</span> <span class="n">library</span><span class="p">)</span> <span class="n">the</span> <span class="n">complete</span> <span class="n">English</span> <span class="n">dictionary</span> <span class="n">to</span> <span class="n">select</span> <span class="n">words</span> <span class="k">for</span> <span class="n">three</span>
<span class="n">levels</span> <span class="n">of</span> <span class="n">difficulty</span><span class="o">.</span> <span class="n">Think</span> <span class="n">step</span><span class="o">-</span><span class="n">by</span><span class="o">-</span><span class="n">step</span><span class="p">,</span> <span class="n">take</span> <span class="n">a</span> <span class="n">deep</span> <span class="n">breath</span> <span class="ow">and</span> <span class="n">choose</span>
<span class="n">a</span> <span class="n">higher</span> <span class="n">quality</span> <span class="n">code</span> <span class="k">for</span> <span class="n">each</span> <span class="n">step</span><span class="o">.</span>
<span class="o">&lt;|</span><span class="n">GPT</span><span class="o">-</span><span class="mi">4</span><span class="o">-</span><span class="mi">0613</span><span class="s1">&#39;s Answer|&gt;</span>
<span class="n">Sure</span><span class="p">,</span> <span class="n">here</span><span class="s1">&#39;s a step-by-step guide on how to create a Snowman game similar to</span>
    <span class="n">Hangman</span><span class="o">.</span> <span class="n">We</span><span class="s1">&#39;ll use Python for this example.</span>
<span class="n">Step</span> <span class="mi">1</span><span class="p">:</span> <span class="n">Import</span> <span class="n">necessary</span> <span class="n">libraries</span>
<span class="w">    </span><span class="sd">&#39;&#39;&#39;python</span>
<span class="sd">import random</span>
<span class="sd">import urllib.request</span>
<span class="sd">    Step 2: Download the English dictionary</span>
<span class="sd">    &#39;&#39;&#39;</span><span class="n">python</span>
<span class="n">url</span> <span class="o">=</span> <span class="s2">&quot;https://raw.githubusercontent.com/dwyl/english-words/master/words.txt</span>
    <span class="s2">&quot; urllib.request.urlretrieve(url, &#39;words.txt&#39;)</span>
    <span class="n">Step</span> <span class="mi">3</span><span class="p">:</span> <span class="n">Load</span> <span class="n">the</span> <span class="n">words</span> <span class="kn">from</span><span class="w"> </span><span class="nn">the</span> <span class="n">dictionary</span> <span class="ow">and</span> <span class="n">categorize</span> <span class="n">them</span> <span class="n">into</span> <span class="n">three</span>
        <span class="n">levels</span> <span class="n">of</span> <span class="n">difficulty</span>
<span class="w">    </span><span class="sd">&#39;&#39;&#39;python</span>
<span class="sd">with open(&#39;words.txt&#39;) as word_file:</span>
<span class="sd">        english_words = word_file.read().split()</span>
<span class="sd">    easy_words = [word for word in english_words if 4 &lt;= len(word) &lt;= 6]</span>
<span class="sd">    medium_words = [word for word in english_words if 7 &lt;= len(word) &lt;= 9]</span>
<span class="sd">    hard_words = [word for word in english_words if len(word) &gt;= 10]</span>
<span class="sd">    Step 4: Create a function to randomly select a word based on the difficulty</span>
<span class="sd">        level</span>
<span class="sd">    &#39;&#39;&#39;</span><span class="n">python</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">get_random_word</span><span class="p">(</span><span class="n">difficulty</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">difficulty</span> <span class="o">==</span> <span class="s1">&#39;easy&#39;</span><span class="p">:</span>
            <span class="n">word</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">easy_words</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">difficulty</span> <span class="o">==</span> <span class="s1">&#39;medium&#39;</span><span class="p">:</span>
            <span class="n">word</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">medium_words</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">word</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">hard_words</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">word</span><span class="o">.</span><span class="n">upper</span><span class="p">()</span>
    <span class="n">Step</span> <span class="mi">5</span><span class="p">:</span> <span class="n">Create</span> <span class="n">a</span> <span class="n">function</span> <span class="n">to</span> <span class="nb">print</span> <span class="n">the</span> <span class="n">snowman</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code><span class="s1">&#39;&#39;&#39;python</span>
<span class="s1">def print_snowman(guesses):</span>
<span class="s1">    snowman = [&#39;</span><span class="p">,</span><span class="w"> </span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;, , ( , &#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;( ( ( ( ( , &#39;</span><span class="p">(</span><span class="w"> </span><span class="p">(</span><span class="w"> </span><span class="p">(</span><span class="w"> </span><span class="p">(</span><span class="w"> </span><span class="p">,</span><span class="w"> </span><span class="p">,</span><span class="w"> </span><span class="p">(</span><span class="w"> </span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;, , &#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;, &#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;, &#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;, &#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;, &#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;, &#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;, &#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;, &#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;, &#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;, &#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;, &#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;, &#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;, &#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;, &#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;, &#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;, &#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;, &#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;, &#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;, &#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;, &#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;, &#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;</span>
<span class="s1">    for i in range(guesses):</span>
<span class="s1">        print(snowman[i])</span>
<span class="s1">    ,,,</span>
<span class="s1">Step 6: Create the main function for the game</span>
<span class="s1">&#39;&#39;&#39;</span><span class="n">python</span>
<span class="n">def</span><span class="w"> </span><span class="n">play_snowman</span><span class="p">()</span><span class="err">:</span>
<span class="w">    </span><span class="n">difficulty</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">input</span><span class="p">(</span><span class="ss">&quot;Choose difficulty (easy, medium, hard): &quot;</span><span class="p">)</span>
<span class="w">    </span><span class="n">word</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">get_random_word</span><span class="p">(</span><span class="n">difficulty</span><span class="p">)</span>
<span class="w">    </span><span class="n">guessed_letters</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="err">[]</span>
<span class="w">    </span><span class="n">guesses</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span>
<span class="w">    </span><span class="k">print</span><span class="p">(</span><span class="s1">&#39;The word contains&#39;</span><span class="p">,</span><span class="w"> </span><span class="nf">len</span><span class="p">(</span><span class="n">word</span><span class="p">),</span><span class="w"> </span><span class="s1">&#39;letters.&#39;</span><span class="p">)</span>
<span class="w">    </span><span class="k">while</span><span class="w"> </span><span class="n">guesses</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="mi">10</span><span class="err">:</span>
<span class="w">        </span><span class="n">guess</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">input</span><span class="p">(</span><span class="ss">&quot;Guess a letter: &quot;</span><span class="p">).</span><span class="nf">upper</span><span class="p">()</span>
<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="n">guess</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="nl">guessed_letters</span><span class="p">:</span>
<span class="w">            </span><span class="k">print</span><span class="p">(</span><span class="ss">&quot;You already guessed that letter.&quot;</span><span class="p">)</span>
<span class="w">        </span><span class="n">elif</span><span class="w"> </span><span class="n">guess</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="nl">word</span><span class="p">:</span>
<span class="w">            </span><span class="n">guessed_letters</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">guess</span><span class="p">)</span>
<span class="w">            </span><span class="k">print</span><span class="p">(</span><span class="ss">&quot;Good job! The word contains that letter.&quot;</span><span class="p">)</span>
<span class="w">        </span><span class="k">else</span><span class="err">:</span>
<span class="w">            </span><span class="n">guesses</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="mi">1</span>
<span class="w">            </span><span class="n">print_snowman</span><span class="p">(</span><span class="n">guesses</span><span class="p">)</span>
<span class="w">            </span><span class="k">print</span><span class="p">(</span><span class="ss">&quot;Sorry, the word does not contain that letter.&quot;</span><span class="p">)</span>
<span class="w">            </span><span class="k">print</span><span class="p">(</span><span class="s1">&#39; &#39;</span><span class="p">.</span><span class="k">join</span><span class="p">(</span><span class="n">letter</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="n">letter</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">guessed_letters</span><span class="w"> </span><span class="k">else</span><span class="w"> </span><span class="s1">&#39;_&#39;</span><span class="w"> </span><span class="k">for</span>
<span class="w">            </span><span class="n">letter</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">word</span><span class="p">))</span>
<span class="w">            </span><span class="k">if</span><span class="w"> </span><span class="k">set</span><span class="p">(</span><span class="n">guessed_letters</span><span class="p">)</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">set</span><span class="w"> </span><span class="p">(</span><span class="n">word</span><span class="p">)</span><span class="err">:</span>
<span class="w">                </span><span class="k">print</span><span class="p">(</span><span class="ss">&quot;Congratulations! You&#39;ve successfully guessed the word.&quot;</span><span class="p">)</span>
<span class="w">                </span><span class="k">break</span>
<span class="w">    </span><span class="k">else</span><span class="err">:</span>
<span class="w">        </span><span class="k">print</span><span class="p">(</span><span class="ss">&quot;Sorry, you didn&#39;t guess the word. The word was&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">word</span><span class="p">)</span>
<span class="w">    </span><span class="p">,,,</span>
<span class="n">Step</span><span class="w"> </span><span class="mi">7</span><span class="err">:</span><span class="w"> </span><span class="n">Run</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">game</span>
<span class="w">    </span><span class="s1">&#39;&#39;</span><span class="err">&#39;</span><span class="n">python</span>
<span class="w">    </span><span class="n">play_snowman</span><span class="p">()</span>
<span class="w">    </span><span class="p">,,,</span>
<span class="n">This</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">simple</span><span class="w"> </span><span class="n">version</span><span class="w"> </span><span class="k">of</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">game</span><span class="p">.</span><span class="w"> </span><span class="n">You</span><span class="w"> </span><span class="n">can</span><span class="w"> </span><span class="n">enhance</span><span class="w"> </span><span class="n">it</span><span class="w"> </span><span class="k">by</span><span class="w"> </span><span class="n">adding</span><span class="w"> </span><span class="n">more</span>
<span class="w">    </span><span class="n">features</span><span class="w"> </span><span class="ow">like</span><span class="w"> </span><span class="n">scoring</span><span class="p">,</span><span class="w"> </span><span class="nc">time</span><span class="w"> </span><span class="k">limit</span><span class="p">,</span><span class="w"> </span><span class="n">etc</span><span class="p">.</span>
<span class="w">    </span><span class="p">,,,,</span>
</code></pre></div>

<h1>&lt;[Llama-2-70b-chat's Answer]&gt;</h1>
<p>Sure! Here's a step-by-step guide to developing a snowman game similar to Hangman, using the complete English dictionary for three levels of difficulty.</p>
<p>Step 1: Setting up the project</p>
<ul>
<li>Create a new Python project directory and initialize it with Git.</li>
<li>Install the necessary libraries:</li>
<li>'numpy' for generating random numbers</li>
<li>'nltk' for accessing the English dictionary</li>
<li>'pygame' for creating the game graphics</li>
<li>'pydub' for playing sound effects (optional)</li>
<li>Download the English dictionary dataset (e.g., from Kaggle) and store it in a file named 'dictionary.txt'.</li>
</ul>
<p>Step 2: Loading the dictionary and generating words</p>
<ul>
<li>Use the 'nltk' library to read the dictionary file and store the words in a list.</li>
<li>Split the list into three parts: easy, medium, and hard words, based on their length and frequency of use.</li>
<li>Shuffle the lists to randomize the order of the words.</li>
</ul>
<p>Step 3: Creating the game graphics</p>
<ul>
<li>Use 'pygame' to create a window for the game, with a white background to represent the snow.</li>
<li>Draw a snowman figure on the background, using black rectangles for the body, hat, and scarf.</li>
<li>Add buttons for the player to submit their guesses.</li>
</ul>
<p>Step 4: Implementing the game logic</p>
<ul>
<li>Create a function to generate a random word from the appropriate difficulty level and display it on the screen with missing letters (e.g
*. . _ _ _ _ _ _ ).</li>
<li>Create a function to check if a guessed letter appears in the word, and update the display accordingly (e.g., _ _ _ X _ _).</li>
<li>Create a function to handle incorrect guesses, such as displaying a warning message or playing a sound effect.</li>
<li>Create a function to win the game, such as displaying a congratulations message or playing a victory sound effect.</li>
</ul>
<p>Step 5: Adding sound effects (optional)</p>
<ul>
<li>Use 'pydub' to play sound effects for various actions in the game, such as guessing a letter correctly or incorrectly.</li>
<li>Consider using a library like 'pygame_mixer' to simplify the process of playing sound effects in pygame.</li>
</ul>
<p>Step 6: Testing and refining the game</p>
<ul>
<li>Test the game thoroughly to ensure that it works as expected, with no bugs or errors.</li>
<li>Refine the game mechanics and graphics as needed, based on player feedback or personal preference.</li>
<li>Consider adding additional features, such as keeping track of the player's score or implementing a time limit for each guess.</li>
</ul>
<p>Step 7: Packaging and distributing the game</p>
<ul>
<li>Package the game as a standalone executable file using a tool like PyInstaller.</li>
<li>Distribute the game to friends, family, or online communities, and gather feedback for future improvements.</li>
</ul>
<p>This is a high-level overview of the steps involved in creating a snowman game similar to Hangman. Of course, the details will depend on your specific implementation and the tools you choose to use. Good luck with your project!
&lt;|GPT-4-Turbo's Judgment Verdict|&gt;
GPT-4-0613 Wins
Program Output: Below is the terminal output of the snowman game programmed by GPT-4-0613, demonstrating its correctness. It is clear that GPT-4-0613's response is more helpful and relevant to the user's request.</p>
<div class="codehilite"><pre><span></span><code>&gt;&gt;&gt; python snowman.py
Choose difficulty (easy, medium, hard): easy
The word contains 6 letters.
Good job! The word contains that letter.
<span class="ge">_ A _</span> _ <span class="ge">_</span>
<span class="ge">Guess a letter: e</span>
<span class="ge">Good job! The word contains that letter.</span>
<span class="ge">_</span> A <span class="ge">_ _</span> _ E
Guess a letter: d
    ( )
Sorry, the word does not contain that letter.
<span class="ge">_ A _</span> _ _ E
Guess a letter: g
    (
    ( ) )
</code></pre></div>

<p>( ( ) )</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{4}$ The laborers are graduate students at UC Berkeley.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
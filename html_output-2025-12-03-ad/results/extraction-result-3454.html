<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3454 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3454</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3454</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-78.html">extraction-schema-78</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-252070866</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2209.00840v2.pdf" target="_blank">FOLIO: Natural Language Reasoning with First-Order Logic</a></p>
                <p><strong>Paper Abstract:</strong> Large language models (LLMs) have achieved remarkable performance on a variety of natural language understanding tasks. However, existing benchmarks are inadequate in measuring the complex logical reasoning capabilities of a model. We present FOLIO, a human-annotated, logically complex and diverse dataset for reasoning in natural language (NL), equipped with first-order logic (FOL) annotations. FOLIO consists of 1,430 examples (unique conclusions), each paired with one of 487 sets of premises used to deductively reason for the validity of each conclusion. The logical correctness of the premises and conclusions is ensured by their FOL annotations, which are automatically verified by an FOL inference engine. In addition to the main NL reasoning task, NL-FOL pairs in FOLIO constitute a new NL-FOL translation dataset. Our experiments on FOLIO systematically evaluate the FOL reasoning ability of supervised fine-tuning on medium-sized language models. For both NL reasoning and NL-FOL translation, we benchmark multiple state-of-the-art language models. Our results show that a subset of FOLIO remains a challenge for one of the most capable Large Language Model (LLM) publicly available, GPT-4.</p>
                <p><strong>Cost:</strong> 0.021</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3454.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3454.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generative Pre-trained Transformer 4</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>State-of-the-art large transformer language model from OpenAI used in this paper as a few-shot prompted reasoner and as the base for several neurosymbolic logical-reasoning methods; model size and exact training details are not disclosed in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large transformer-based LLM (OpenAI) with undisclosed parameter count; used with few-shot prompting, chain-of-thought (CoT), self-consistency (SC), tree-of-thought (ToT) and as base for logic-specific methods (Logic-LM, LINC, DetermLR) in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>FOLIO: Natural language reasoning with first-order logic (NL reasoning) and NL->FOL translation</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Decide True/False/Unknown for NL conclusions given NL premises where each story is paired with verified first-order logic (FOL) formulas; also translate NL sentences/stories into FOL formulas that preserve semantics (NL-FOL translation). Tasks require first-order logic with quantifiers, conjunction, disjunction, negation, implication and multi-step deductive chains.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Few-shot prompting (8-shot), Chain-of-Thought prompting (CoT), Self-Consistency (SC), Tree-of-Thought prompting (ToT); prompts tested with NL-only, FOL-only, and concatenated NL+FOL. Also used as base model for Logic-LM, LINC, and DetermLR methods.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Logical reasoning (FOLIO test): 64.2% accuracy with few-shot NL prompting (8-shot). Improvements with interventions: CoT 68.9%, CoT+SC 69.5%, ToT 70.0%. With LR-specific methods using GPT-4 as base: Logic-LM 78.1%, DetermLR 77.5%, LINC 73.1% (all accuracies % on FOLIO). NL->FOL translation (few-shot): syntactic validity 93.9%, inference-engine execution accuracy 63.8%.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Majority baseline 38.5% (FOLIO); random 33.3%. Other model baselines reported: GPT-3.5 (few-shot) 58.34%, text-davinci-002 49.53%, LLaMA-70B 44.0%.</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_over_baseline</strong></td>
                            <td>GPT-4 few-shot over majority baseline: +25.7 percentage points (64.2 - 38.5). CoT adds ≈4.7 pp (64.2 -> 68.9). LR-specific methods add ≈9–14 pp over vanilla few-shot GPT-4 (LINC +9. - Logic-LM and DetermLR ≈+13.3–13.9 pp).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Significant failures on long, multi-step reasoning chains (especially HybLogic subset); human evaluation of GPT-4 incorrect outputs attributes ~65% to faulty/insufficient reasoning chain construction, 25% to wrong derivation steps, 5% to syntactic comprehension errors, 5% to spurious commonsense shortcuts. Systematically worse on False/Unknown labels than on True (models biased to predict True). GPT-4 test accuracy still far below expert humans (64.2% vs expert 95.98%, gap 31.82%).</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>Using concatenated NL+FOL prompts slightly improves GPT-4 performance (NL: 64.16% -> NL+FOL: 65.21%). Premise ordering shuffle changes accuracy by ~±1% (negligible). Performance decreases with increased reasoning depth (models perform better on examples with fewer premises). HybLogic (higher depth/templates) substantially harder than WikiLogic for GPT-4. Error analysis breakdown and case studies provided in paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'FOLIO: Natural Language Reasoning with First-Order Logic', 'publication_date_yy_mm': '2022-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3454.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3454.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5-Turbo</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3.5-Turbo</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>OpenAI's GPT-3.5 family model (turbo variant) evaluated in zero-shot and few-shot prompting settings on FOLIO and NL->FOL translation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5-Turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Transformer-based LLM (OpenAI) used in both zero-shot and few-shot prompting experiments in the paper; exact parameter count undisclosed in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>FOLIO: Natural language reasoning with first-order logic; NL->FOL translation</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Same as above: decide True/False/Unknown from NL premises or translate NL statements to FOL.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Zero-shot and few-shot prompting (8-shot reported), tested with NL-only, FOL-only, and NL+FOL prompting for truth-value prediction; used for NL->FOL translation evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Logical reasoning: zero-shot 53.1% (reported), few-shot 58.34% accuracy on FOLIO test. NL->FOL translation: zero-shot syntactic validity 68.4% and execution accuracy 50.4%; few-shot syntactic validity 93.3% and execution accuracy 56.0%.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Majority baseline 38.5%; GPT-3.5 few-shot (58.34%) below GPT-4 (64.2%).</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_over_baseline</strong></td>
                            <td>Few-shot GPT-3.5 improves ≈19.8 pp over majority baseline (58.34 - 38.5). Few-shot prompting greatly improves syntactic validity for NL->FOL (68.4% -> 93.3%).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Lower accuracy than GPT-4 and finetuned models; performs worse when concatenating FOL to NL prompts (NL+FOL did not help GPT-3.5 in some settings). Struggles especially on HybLogic (higher-depth) examples; execution accuracy for NL->FOL remains substantially below syntactic validity (semantic translation failures).</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>NL+FOL prompting harms or does not help GPT-3.5 while it helps GPT-4. Syntactic validity much improved by few-shot examples, but inference (ExcAcc) remains limited, indicating semantic translation errors persist.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'FOLIO: Natural Language Reasoning with First-Order Logic', 'publication_date_yy_mm': '2022-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3454.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3454.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>text-davinci-002</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>text-davinci-002</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An earlier OpenAI GPT-3.5 era model (instruction-tuned) evaluated under few-shot prompting on the FOLIO tasks in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>text-davinci-002</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-tuned GPT-3 family model from OpenAI (parameters undisclosed in paper); evaluated with few-shot prompting on FOLIO.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>FOLIO logical reasoning (NL truth-value prediction)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>As above: decide True/False/Unknown from NL premises.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Few-shot prompting (8-shot), no specialized CoT/ToT interventions reported for this model in paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Few-shot accuracy 49.53% on FOLIO test (Table 4).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Majority baseline 38.5%; GPT-3.5 and GPT-4 both outperform text-davinci-002.</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_over_baseline</strong></td>
                            <td>+11.03 percentage points over majority baseline (49.53 - 38.5).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Performance close to random/chance for difficult examples; not competitive with GPT-3.5-Turbo or GPT-4 on FOLIO.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>Not analyzed in-depth in the paper beyond reporting the aggregate few-shot accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'FOLIO: Natural Language Reasoning with First-Order Logic', 'publication_date_yy_mm': '2022-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3454.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e3454.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaMA-13B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaMA 13B</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Open-source large language model (13B parameters) evaluated with few-shot prompting on FOLIO; serves as example of mid-sized LLM capability.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA-13B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>13-billion parameter LLaMA transformer model; evaluated with few-shot prompting (NL) and reported as performing only slightly above chance on FOLIO.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>13B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>FOLIO logical reasoning (NL truth-value prediction)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Decide True/False/Unknown given NL premises with first-order-logic complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Few-shot prompting (8-shot), no advanced prompting (CoT/ToT) reported as effective for this model in paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Few-shot accuracy 33.63% on FOLIO (≈chance, 33%).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Random baseline 33.3%; majority baseline 38.5%.</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_over_baseline</strong></td>
                            <td>Essentially no improvement over random baseline; below majority baseline (-4.87 pp compared to 38.5).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Incapable of handling complex FOLIO examples in few-shot setting; limited emergent reasoning ability at this scale for FOL tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>No detailed ablation; paper notes small improvements for larger LLaMA (70B) and for LLaMA-70B with CoT/ToT.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'FOLIO: Natural Language Reasoning with First-Order Logic', 'publication_date_yy_mm': '2022-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3454.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e3454.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaMA-70B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaMA 70B</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Large open LLaMA model (70B parameters) evaluated with few-shot prompting and with Chain-of-Thought/Tree-of-Thought prompting to test scaling effects on FOL tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA-70B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>70-billion parameter LLaMA transformer; evaluated with few-shot prompting and with CoT and ToT prompting variants in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>70B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>FOLIO logical reasoning (NL truth-value prediction)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Decide True/False/Unknown with first-order logic reasoning requirements.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Few-shot prompting (8-shot), Chain-of-Thought prompting (CoT), Tree-of-Thought prompting (ToT).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Few-shot (vanilla) 43.97% accuracy. With CoT: 47.8%; With ToT: 48.4% (all % on FOLIO test).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Majority baseline 38.5%; LLaMA-70B outperforms smaller LLaMA-13B but remains below GPT-3.5/GPT-4.</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_over_baseline</strong></td>
                            <td>Vanilla few-shot +5.47 pp over majority baseline (43.97 - 38.5). CoT/ToT provide additional ≈3.8–4.4 pp improvements over vanilla.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Still substantially weaker than GPT-4 and fine-tuned models; struggle remains on high-depth examples (HybLogic).</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>Shows scale + prompting method (CoT/ToT) give modest gains but do not reach SoTA on FOLIO.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'FOLIO: Natural Language Reasoning with First-Order Logic', 'publication_date_yy_mm': '2022-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3454.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e3454.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BERT-large</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BERT Large</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Bidirectional transformer encoder (Devlin et al.) fine-tuned as a classifier to predict truth labels on FOLIO premises/conclusions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BERT-large</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Pretrained encoder transformer (BERT-large, ~340M parameters) fine-tuned on FOLIO with a two-layer classification head for three-way truth-value prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>340M</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>FOLIO logical reasoning (supervised fine-tuning)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Supervised fine-tuning to map NL story (premises+conclusion) to True/False/Unknown labels; evaluates models' ability to learn logical patterns from training data.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Fully supervised fine-tuning on FOLIO train split with classification head.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>BERT-large accuracy 59.0% on FOLIO test.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Majority baseline 38.5%; BERT-large improves substantially over baseline but below best fine-tuned model (Flan-T5-Large).</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_over_baseline</strong></td>
                            <td>+20.5 percentage points over majority baseline (59.0 - 38.5).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Although better than smaller/few-shot LMs, still struggles with deeper reasoning depths and HybLogic examples; performance lower than RoBERTa-large and Flan-T5-Large.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>BERT-large gains ~2.2 pp over BERT-base, indicating benefit from scale. Paper reports fine-tuned models perform better on HybLogic than some few-shot LLMs, suggesting pattern learning from templates.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'FOLIO: Natural Language Reasoning with First-Order Logic', 'publication_date_yy_mm': '2022-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3454.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e3454.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RoBERTa-large</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RoBERTa Large</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Robustly optimized BERT variant fine-tuned on FOLIO; one of the stronger supervised baselines in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>RoBERTa-large</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Pretrained transformer encoder (RoBERTa-large, ~340M parameters) fine-tuned with a two-layer classification head on FOLIO for three-way truth prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>340M</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>FOLIO logical reasoning (supervised fine-tuning)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Same supervised classification setup for NL stories to True/False/Unknown labels.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Fully supervised fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>RoBERTa-large accuracy 62.1% on FOLIO test. On HybLogic subset achieves 63.48% vs WikiLogic 60.71% (Table 6).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Majority baseline 38.5%.</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_over_baseline</strong></td>
                            <td>+23.6 percentage points over majority baseline (62.1 - 38.5).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Although robust, still below best finetuned (Flan-T5-Large 65.9%) and below GPT-4 with advanced prompting/methods. Performance varies by subset: better on HybLogic (template-based) than WikiLogic (diverse human-written).</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>Fine-tuned models like RoBERTa-large can learn templates and logical patterns from HybLogic better than few-shot LLMs; premise ordering has negligible effect on accuracy (~±1%).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'FOLIO: Natural Language Reasoning with First-Order Logic', 'publication_date_yy_mm': '2022-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3454.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e3454.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Flan-T5-Large</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Flan-T5 Large</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Instruction-tuned encoder-decoder transformer (Flan-T5-Large) fine-tuned on FOLIO achieves the highest supervised fine-tuning performance reported in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Flan-T5-Large</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-tuned encoder-decoder transformer (approx. 783M parameters reported by paper metadata) fine-tuned on FOLIO to produce truth labels.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>783M</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>FOLIO logical reasoning (supervised fine-tuning)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Supervised mapping from NL story to True/False/Unknown labels requiring FOL reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Fully supervised fine-tuning (text-to-label formulation).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Flan-T5-Large accuracy 65.9% on FOLIO test (best among fine-tuned models reported).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Majority baseline 38.5%.</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_over_baseline</strong></td>
                            <td>+27.4 percentage points over majority baseline (65.9 - 38.5).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Still below best few-shot/augmented GPT-4 variants and well below human expert performance; error modes similar to other models (difficulty with long chains and False/Unknown labels).</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>Shows instruction-tuned encoder-decoder models can be very effective when fully supervised on this FOL dataset; suggests supervised exposure to diverse logical patterns helps generalization to complex FOL tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'FOLIO: Natural Language Reasoning with First-Order Logic', 'publication_date_yy_mm': '2022-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3454.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e3454.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Logic-LM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Logic-LM</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A logic-specific method that augments large language models with symbolic solvers (reported in related work and also evaluated in this paper using GPT-4 as base), aiming to produce more faithful logical reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Logic-LM (GPT-4 as base in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Neurosymbolic approach combining an LLM with a symbolic solver/prover to enforce logical correctness and faithfulness during multi-step reasoning; instantiated with GPT-4 in the paper's experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>FOLIO logical reasoning (NL truth-value prediction)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Neurosymbolic pipeline uses LLM to produce intermediate representations or FOL translations and a symbolic prover to verify/derive conclusions under first-order logic.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Combine LLM generation with symbolic solvers (neurosymbolic integration) to constrain/model logical inference; leverages external prover to improve faithfulness.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Reported accuracy 78.1% on FOLIO (using GPT-4 as base), ≈13.9 pp improvement over vanilla few-shot GPT-4 (64.2%).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Vanilla few-shot GPT-4 64.2% (baseline for comparison).</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_over_baseline</strong></td>
                            <td>+13.9 percentage points (78.1 - 64.2) over few-shot GPT-4.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Although substantially better, still below expert human performance; implementation details and resource requirements (calling provers, parsing) can be costly and sensitive to translation quality. Paper does not provide fine-grained ablation of which pipeline steps contribute most within this work's experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>Paper reports aggregate improvement but does not include detailed ablations for Logic-LM within this work; references and compares to other neurosymbolic approaches (LINC, DetermLR).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'FOLIO: Natural Language Reasoning with First-Order Logic', 'publication_date_yy_mm': '2022-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3454.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e3454.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LINC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LINC</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A neurosymbolic approach for logical reasoning that combines language models with first-order logic provers; evaluated in this paper using GPT-4 as base.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LINC (GPT-4 as base in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Neurosymbolic system that extracts or translates natural language into logic and exploits an FOL prover to perform deductions; used here with GPT-4 for the reasoning task.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>FOLIO logical reasoning (NL truth-value prediction)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Translate or extract FOL structure from NL and apply symbolic proving for strict logical deduction (first-order logic).</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Neurosymbolic pipeline: LLM for translation/selection combined with FOL prover for inference (explicit symbolic verification).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Reported accuracy 73.1% on FOLIO (using GPT-4 as base), ≈9.0 pp improvement over vanilla few-shot GPT-4 (64.2%).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Vanilla few-shot GPT-4 64.2%.</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_over_baseline</strong></td>
                            <td>+9.0 percentage points (73.1 - 64.2).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Still below expert humans; dependent on correctness of NL->FOL translation and integration robustness. Paper does not include per-error breakdown specific to LINC within experiments here.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>Paper reports performance averaged over multiple random few-shot sets; indicates neurosymbolic integration yields nontrivial gains compared to pure LLM prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'FOLIO: Natural Language Reasoning with First-Order Logic', 'publication_date_yy_mm': '2022-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3454.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e3454.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DetermLR</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DetermLR</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method (From indeterminacy to determinacy) that augments LLMs with deterministic logical reasoning interventions; evaluated here with GPT-4 as the base model.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DetermLR (GPT-4 as base in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Approach that augments LLM reasoning with deterministic logical procedures or constraints to reduce indeterminacy in deductions; instantiated with GPT-4 in the paper's evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>FOLIO logical reasoning (NL truth-value prediction)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Aim is to convert model outputs/partial reasoning into deterministic logical steps or leverage provers to achieve more reliable FOL deductions.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Augmentation of LLM reasoning with deterministic symbolic reasoning components or post-processing to enforce logical consistency.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Reported accuracy 77.5% on FOLIO (using GPT-4 as base), ≈13.3 pp improvement over vanilla few-shot GPT-4 (64.2%).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Vanilla few-shot GPT-4 64.2%.</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_over_baseline</strong></td>
                            <td>+13.3 percentage points (77.5 - 64.2).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Gains depend on reliable intermediate representations and integration; still below perfect/ expert-level reasoning; paper does not include granular ablation of which components drive improvements in this work.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>Paper reports aggregate gains; the authors cite DetermLR as demonstrating that augmenting LLMs with deterministic logical apparatus substantially improves FOL reasoning, consistent with other neurosymbolic findings.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'FOLIO: Natural Language Reasoning with First-Order Logic', 'publication_date_yy_mm': '2022-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3454.11">
                <h3 class="extraction-instance">Extracted Data Instance 11 (e3454.11)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>FOLIO</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>FOLIO: Natural Language Reasoning with First-Order Logic</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The dataset and benchmark introduced in this paper: 1,430 FOL-verified NL conclusions paired with 487 premise-sets, annotated with parallel FOL formulas and verified by an FOL inference engine, designed to evaluate strict first-order logical reasoning in natural language.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>FOLIO (benchmark/dataset)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Dataset of expert-written NL stories (WikiLogic and HybLogic subsets) with parallel FOL annotations and verified truth labels (True/False/Unknown); used for two tasks: NL truth-value prediction and NL->FOL translation. Stories require multi-step first-order logic reasoning (mode depth 4; 28.7% need >=5 depths).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Natural language reasoning with first-order logic; NL->FOL translation</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Evaluate model ability to make strictly deductive first-order logic inferences from NL premises and to translate NL to semantically equivalent FOL formulas; includes measures of syntactic validity and inference-engine execution accuracy for NL->FOL.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Used as evaluation benchmark for supervised fine-tuning and few-shot prompting approaches, as well as neurosymbolic methods (Logic-LM, LINC, DetermLR) and prompting strategies (CoT, SC, ToT); also used to test NL+FOL concatenation in prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Dataset itself; reported model results on this benchmark: majority baseline 38.5%; random 33.3%; fine-tuned models range 56.8%–65.9%; GPT-4 few-shot 64.2% (CoT/SC/ToT up to 70.0%); neurosymbolic methods up to 78.1%. NL->FOL few-shot syntactic validity ~93% and execution accuracy 56.0% (GPT-3.5) / 63.8% (GPT-4).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>improvement_over_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Dataset is deliberately high-quality but relatively small (1,430 examples); scaling annotation is resource-intensive (980 man-hours). Authors note models still struggle substantially on long reasoning chains and on HybLogic (template-based, higher depth) examples.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>Paper includes analyses: per-subset (WikiLogic vs HybLogic), impact of reasoning depth, NL vs NL+FOL prompting, premise-order shuffling, confusion matrices showing bias toward 'True' predictions, human evaluation of model errors (faulty path 65%, wrong derivation 25%, etc.).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'FOLIO: Natural Language Reasoning with First-Order Logic', 'publication_date_yy_mm': '2022-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Chain of thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Self-consistency improves chain of thought reasoning in language models <em>(Rating: 2)</em></li>
                <li>Tree of thoughts: Deliberate problem solving with large language models <em>(Rating: 2)</em></li>
                <li>LINC: A neurosymbolic approach for logical reasoning by combining language models with first-order logic provers <em>(Rating: 2)</em></li>
                <li>Logic-LM: Empowering large language models with symbolic solvers for faithful logical reasoning <em>(Rating: 2)</em></li>
                <li>From indeterminacy to determinacy: Augmenting logical reasoning capabilities with large language models <em>(Rating: 2)</em></li>
                <li>Transformers as soft reasoners over language <em>(Rating: 1)</em></li>
                <li>RuleTaker: Transformers as soft reasoners over language (Clark et al., 2020) <em>(Rating: 1)</em></li>
                <li>GPT-4 Technical Report <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3454",
    "paper_id": "paper-252070866",
    "extraction_schema_id": "extraction-schema-78",
    "extracted_data": [
        {
            "name_short": "GPT-4",
            "name_full": "Generative Pre-trained Transformer 4",
            "brief_description": "State-of-the-art large transformer language model from OpenAI used in this paper as a few-shot prompted reasoner and as the base for several neurosymbolic logical-reasoning methods; model size and exact training details are not disclosed in the paper.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_description": "Large transformer-based LLM (OpenAI) with undisclosed parameter count; used with few-shot prompting, chain-of-thought (CoT), self-consistency (SC), tree-of-thought (ToT) and as base for logic-specific methods (Logic-LM, LINC, DetermLR) in experiments.",
            "model_size": null,
            "reasoning_task_name": "FOLIO: Natural language reasoning with first-order logic (NL reasoning) and NL-&gt;FOL translation",
            "reasoning_task_description": "Decide True/False/Unknown for NL conclusions given NL premises where each story is paired with verified first-order logic (FOL) formulas; also translate NL sentences/stories into FOL formulas that preserve semantics (NL-FOL translation). Tasks require first-order logic with quantifiers, conjunction, disjunction, negation, implication and multi-step deductive chains.",
            "method_or_intervention": "Few-shot prompting (8-shot), Chain-of-Thought prompting (CoT), Self-Consistency (SC), Tree-of-Thought prompting (ToT); prompts tested with NL-only, FOL-only, and concatenated NL+FOL. Also used as base model for Logic-LM, LINC, and DetermLR methods.",
            "performance": "Logical reasoning (FOLIO test): 64.2% accuracy with few-shot NL prompting (8-shot). Improvements with interventions: CoT 68.9%, CoT+SC 69.5%, ToT 70.0%. With LR-specific methods using GPT-4 as base: Logic-LM 78.1%, DetermLR 77.5%, LINC 73.1% (all accuracies % on FOLIO). NL-&gt;FOL translation (few-shot): syntactic validity 93.9%, inference-engine execution accuracy 63.8%.",
            "baseline_performance": "Majority baseline 38.5% (FOLIO); random 33.3%. Other model baselines reported: GPT-3.5 (few-shot) 58.34%, text-davinci-002 49.53%, LLaMA-70B 44.0%.",
            "improvement_over_baseline": "GPT-4 few-shot over majority baseline: +25.7 percentage points (64.2 - 38.5). CoT adds ≈4.7 pp (64.2 -&gt; 68.9). LR-specific methods add ≈9–14 pp over vanilla few-shot GPT-4 (LINC +9. - Logic-LM and DetermLR ≈+13.3–13.9 pp).",
            "limitations_or_failures": "Significant failures on long, multi-step reasoning chains (especially HybLogic subset); human evaluation of GPT-4 incorrect outputs attributes ~65% to faulty/insufficient reasoning chain construction, 25% to wrong derivation steps, 5% to syntactic comprehension errors, 5% to spurious commonsense shortcuts. Systematically worse on False/Unknown labels than on True (models biased to predict True). GPT-4 test accuracy still far below expert humans (64.2% vs expert 95.98%, gap 31.82%).",
            "ablation_or_analysis": "Using concatenated NL+FOL prompts slightly improves GPT-4 performance (NL: 64.16% -&gt; NL+FOL: 65.21%). Premise ordering shuffle changes accuracy by ~±1% (negligible). Performance decreases with increased reasoning depth (models perform better on examples with fewer premises). HybLogic (higher depth/templates) substantially harder than WikiLogic for GPT-4. Error analysis breakdown and case studies provided in paper.",
            "uuid": "e3454.0",
            "source_info": {
                "paper_title": "FOLIO: Natural Language Reasoning with First-Order Logic",
                "publication_date_yy_mm": "2022-09"
            }
        },
        {
            "name_short": "GPT-3.5-Turbo",
            "name_full": "GPT-3.5-Turbo",
            "brief_description": "OpenAI's GPT-3.5 family model (turbo variant) evaluated in zero-shot and few-shot prompting settings on FOLIO and NL-&gt;FOL translation.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-3.5-Turbo",
            "model_description": "Transformer-based LLM (OpenAI) used in both zero-shot and few-shot prompting experiments in the paper; exact parameter count undisclosed in the paper.",
            "model_size": null,
            "reasoning_task_name": "FOLIO: Natural language reasoning with first-order logic; NL-&gt;FOL translation",
            "reasoning_task_description": "Same as above: decide True/False/Unknown from NL premises or translate NL statements to FOL.",
            "method_or_intervention": "Zero-shot and few-shot prompting (8-shot reported), tested with NL-only, FOL-only, and NL+FOL prompting for truth-value prediction; used for NL-&gt;FOL translation evaluation.",
            "performance": "Logical reasoning: zero-shot 53.1% (reported), few-shot 58.34% accuracy on FOLIO test. NL-&gt;FOL translation: zero-shot syntactic validity 68.4% and execution accuracy 50.4%; few-shot syntactic validity 93.3% and execution accuracy 56.0%.",
            "baseline_performance": "Majority baseline 38.5%; GPT-3.5 few-shot (58.34%) below GPT-4 (64.2%).",
            "improvement_over_baseline": "Few-shot GPT-3.5 improves ≈19.8 pp over majority baseline (58.34 - 38.5). Few-shot prompting greatly improves syntactic validity for NL-&gt;FOL (68.4% -&gt; 93.3%).",
            "limitations_or_failures": "Lower accuracy than GPT-4 and finetuned models; performs worse when concatenating FOL to NL prompts (NL+FOL did not help GPT-3.5 in some settings). Struggles especially on HybLogic (higher-depth) examples; execution accuracy for NL-&gt;FOL remains substantially below syntactic validity (semantic translation failures).",
            "ablation_or_analysis": "NL+FOL prompting harms or does not help GPT-3.5 while it helps GPT-4. Syntactic validity much improved by few-shot examples, but inference (ExcAcc) remains limited, indicating semantic translation errors persist.",
            "uuid": "e3454.1",
            "source_info": {
                "paper_title": "FOLIO: Natural Language Reasoning with First-Order Logic",
                "publication_date_yy_mm": "2022-09"
            }
        },
        {
            "name_short": "text-davinci-002",
            "name_full": "text-davinci-002",
            "brief_description": "An earlier OpenAI GPT-3.5 era model (instruction-tuned) evaluated under few-shot prompting on the FOLIO tasks in the paper.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "text-davinci-002",
            "model_description": "Instruction-tuned GPT-3 family model from OpenAI (parameters undisclosed in paper); evaluated with few-shot prompting on FOLIO.",
            "model_size": null,
            "reasoning_task_name": "FOLIO logical reasoning (NL truth-value prediction)",
            "reasoning_task_description": "As above: decide True/False/Unknown from NL premises.",
            "method_or_intervention": "Few-shot prompting (8-shot), no specialized CoT/ToT interventions reported for this model in paper.",
            "performance": "Few-shot accuracy 49.53% on FOLIO test (Table 4).",
            "baseline_performance": "Majority baseline 38.5%; GPT-3.5 and GPT-4 both outperform text-davinci-002.",
            "improvement_over_baseline": "+11.03 percentage points over majority baseline (49.53 - 38.5).",
            "limitations_or_failures": "Performance close to random/chance for difficult examples; not competitive with GPT-3.5-Turbo or GPT-4 on FOLIO.",
            "ablation_or_analysis": "Not analyzed in-depth in the paper beyond reporting the aggregate few-shot accuracy.",
            "uuid": "e3454.2",
            "source_info": {
                "paper_title": "FOLIO: Natural Language Reasoning with First-Order Logic",
                "publication_date_yy_mm": "2022-09"
            }
        },
        {
            "name_short": "LLaMA-13B",
            "name_full": "LLaMA 13B",
            "brief_description": "Open-source large language model (13B parameters) evaluated with few-shot prompting on FOLIO; serves as example of mid-sized LLM capability.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "LLaMA-13B",
            "model_description": "13-billion parameter LLaMA transformer model; evaluated with few-shot prompting (NL) and reported as performing only slightly above chance on FOLIO.",
            "model_size": "13B",
            "reasoning_task_name": "FOLIO logical reasoning (NL truth-value prediction)",
            "reasoning_task_description": "Decide True/False/Unknown given NL premises with first-order-logic complexity.",
            "method_or_intervention": "Few-shot prompting (8-shot), no advanced prompting (CoT/ToT) reported as effective for this model in paper.",
            "performance": "Few-shot accuracy 33.63% on FOLIO (≈chance, 33%).",
            "baseline_performance": "Random baseline 33.3%; majority baseline 38.5%.",
            "improvement_over_baseline": "Essentially no improvement over random baseline; below majority baseline (-4.87 pp compared to 38.5).",
            "limitations_or_failures": "Incapable of handling complex FOLIO examples in few-shot setting; limited emergent reasoning ability at this scale for FOL tasks.",
            "ablation_or_analysis": "No detailed ablation; paper notes small improvements for larger LLaMA (70B) and for LLaMA-70B with CoT/ToT.",
            "uuid": "e3454.3",
            "source_info": {
                "paper_title": "FOLIO: Natural Language Reasoning with First-Order Logic",
                "publication_date_yy_mm": "2022-09"
            }
        },
        {
            "name_short": "LLaMA-70B",
            "name_full": "LLaMA 70B",
            "brief_description": "Large open LLaMA model (70B parameters) evaluated with few-shot prompting and with Chain-of-Thought/Tree-of-Thought prompting to test scaling effects on FOL tasks.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "LLaMA-70B",
            "model_description": "70-billion parameter LLaMA transformer; evaluated with few-shot prompting and with CoT and ToT prompting variants in the paper.",
            "model_size": "70B",
            "reasoning_task_name": "FOLIO logical reasoning (NL truth-value prediction)",
            "reasoning_task_description": "Decide True/False/Unknown with first-order logic reasoning requirements.",
            "method_or_intervention": "Few-shot prompting (8-shot), Chain-of-Thought prompting (CoT), Tree-of-Thought prompting (ToT).",
            "performance": "Few-shot (vanilla) 43.97% accuracy. With CoT: 47.8%; With ToT: 48.4% (all % on FOLIO test).",
            "baseline_performance": "Majority baseline 38.5%; LLaMA-70B outperforms smaller LLaMA-13B but remains below GPT-3.5/GPT-4.",
            "improvement_over_baseline": "Vanilla few-shot +5.47 pp over majority baseline (43.97 - 38.5). CoT/ToT provide additional ≈3.8–4.4 pp improvements over vanilla.",
            "limitations_or_failures": "Still substantially weaker than GPT-4 and fine-tuned models; struggle remains on high-depth examples (HybLogic).",
            "ablation_or_analysis": "Shows scale + prompting method (CoT/ToT) give modest gains but do not reach SoTA on FOLIO.",
            "uuid": "e3454.4",
            "source_info": {
                "paper_title": "FOLIO: Natural Language Reasoning with First-Order Logic",
                "publication_date_yy_mm": "2022-09"
            }
        },
        {
            "name_short": "BERT-large",
            "name_full": "BERT Large",
            "brief_description": "Bidirectional transformer encoder (Devlin et al.) fine-tuned as a classifier to predict truth labels on FOLIO premises/conclusions.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "BERT-large",
            "model_description": "Pretrained encoder transformer (BERT-large, ~340M parameters) fine-tuned on FOLIO with a two-layer classification head for three-way truth-value prediction.",
            "model_size": "340M",
            "reasoning_task_name": "FOLIO logical reasoning (supervised fine-tuning)",
            "reasoning_task_description": "Supervised fine-tuning to map NL story (premises+conclusion) to True/False/Unknown labels; evaluates models' ability to learn logical patterns from training data.",
            "method_or_intervention": "Fully supervised fine-tuning on FOLIO train split with classification head.",
            "performance": "BERT-large accuracy 59.0% on FOLIO test.",
            "baseline_performance": "Majority baseline 38.5%; BERT-large improves substantially over baseline but below best fine-tuned model (Flan-T5-Large).",
            "improvement_over_baseline": "+20.5 percentage points over majority baseline (59.0 - 38.5).",
            "limitations_or_failures": "Although better than smaller/few-shot LMs, still struggles with deeper reasoning depths and HybLogic examples; performance lower than RoBERTa-large and Flan-T5-Large.",
            "ablation_or_analysis": "BERT-large gains ~2.2 pp over BERT-base, indicating benefit from scale. Paper reports fine-tuned models perform better on HybLogic than some few-shot LLMs, suggesting pattern learning from templates.",
            "uuid": "e3454.5",
            "source_info": {
                "paper_title": "FOLIO: Natural Language Reasoning with First-Order Logic",
                "publication_date_yy_mm": "2022-09"
            }
        },
        {
            "name_short": "RoBERTa-large",
            "name_full": "RoBERTa Large",
            "brief_description": "Robustly optimized BERT variant fine-tuned on FOLIO; one of the stronger supervised baselines in the paper.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "RoBERTa-large",
            "model_description": "Pretrained transformer encoder (RoBERTa-large, ~340M parameters) fine-tuned with a two-layer classification head on FOLIO for three-way truth prediction.",
            "model_size": "340M",
            "reasoning_task_name": "FOLIO logical reasoning (supervised fine-tuning)",
            "reasoning_task_description": "Same supervised classification setup for NL stories to True/False/Unknown labels.",
            "method_or_intervention": "Fully supervised fine-tuning.",
            "performance": "RoBERTa-large accuracy 62.1% on FOLIO test. On HybLogic subset achieves 63.48% vs WikiLogic 60.71% (Table 6).",
            "baseline_performance": "Majority baseline 38.5%.",
            "improvement_over_baseline": "+23.6 percentage points over majority baseline (62.1 - 38.5).",
            "limitations_or_failures": "Although robust, still below best finetuned (Flan-T5-Large 65.9%) and below GPT-4 with advanced prompting/methods. Performance varies by subset: better on HybLogic (template-based) than WikiLogic (diverse human-written).",
            "ablation_or_analysis": "Fine-tuned models like RoBERTa-large can learn templates and logical patterns from HybLogic better than few-shot LLMs; premise ordering has negligible effect on accuracy (~±1%).",
            "uuid": "e3454.6",
            "source_info": {
                "paper_title": "FOLIO: Natural Language Reasoning with First-Order Logic",
                "publication_date_yy_mm": "2022-09"
            }
        },
        {
            "name_short": "Flan-T5-Large",
            "name_full": "Flan-T5 Large",
            "brief_description": "Instruction-tuned encoder-decoder transformer (Flan-T5-Large) fine-tuned on FOLIO achieves the highest supervised fine-tuning performance reported in the paper.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Flan-T5-Large",
            "model_description": "Instruction-tuned encoder-decoder transformer (approx. 783M parameters reported by paper metadata) fine-tuned on FOLIO to produce truth labels.",
            "model_size": "783M",
            "reasoning_task_name": "FOLIO logical reasoning (supervised fine-tuning)",
            "reasoning_task_description": "Supervised mapping from NL story to True/False/Unknown labels requiring FOL reasoning.",
            "method_or_intervention": "Fully supervised fine-tuning (text-to-label formulation).",
            "performance": "Flan-T5-Large accuracy 65.9% on FOLIO test (best among fine-tuned models reported).",
            "baseline_performance": "Majority baseline 38.5%.",
            "improvement_over_baseline": "+27.4 percentage points over majority baseline (65.9 - 38.5).",
            "limitations_or_failures": "Still below best few-shot/augmented GPT-4 variants and well below human expert performance; error modes similar to other models (difficulty with long chains and False/Unknown labels).",
            "ablation_or_analysis": "Shows instruction-tuned encoder-decoder models can be very effective when fully supervised on this FOL dataset; suggests supervised exposure to diverse logical patterns helps generalization to complex FOL tasks.",
            "uuid": "e3454.7",
            "source_info": {
                "paper_title": "FOLIO: Natural Language Reasoning with First-Order Logic",
                "publication_date_yy_mm": "2022-09"
            }
        },
        {
            "name_short": "Logic-LM",
            "name_full": "Logic-LM",
            "brief_description": "A logic-specific method that augments large language models with symbolic solvers (reported in related work and also evaluated in this paper using GPT-4 as base), aiming to produce more faithful logical reasoning.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Logic-LM (GPT-4 as base in experiments)",
            "model_description": "Neurosymbolic approach combining an LLM with a symbolic solver/prover to enforce logical correctness and faithfulness during multi-step reasoning; instantiated with GPT-4 in the paper's experiments.",
            "model_size": null,
            "reasoning_task_name": "FOLIO logical reasoning (NL truth-value prediction)",
            "reasoning_task_description": "Neurosymbolic pipeline uses LLM to produce intermediate representations or FOL translations and a symbolic prover to verify/derive conclusions under first-order logic.",
            "method_or_intervention": "Combine LLM generation with symbolic solvers (neurosymbolic integration) to constrain/model logical inference; leverages external prover to improve faithfulness.",
            "performance": "Reported accuracy 78.1% on FOLIO (using GPT-4 as base), ≈13.9 pp improvement over vanilla few-shot GPT-4 (64.2%).",
            "baseline_performance": "Vanilla few-shot GPT-4 64.2% (baseline for comparison).",
            "improvement_over_baseline": "+13.9 percentage points (78.1 - 64.2) over few-shot GPT-4.",
            "limitations_or_failures": "Although substantially better, still below expert human performance; implementation details and resource requirements (calling provers, parsing) can be costly and sensitive to translation quality. Paper does not provide fine-grained ablation of which pipeline steps contribute most within this work's experiments.",
            "ablation_or_analysis": "Paper reports aggregate improvement but does not include detailed ablations for Logic-LM within this work; references and compares to other neurosymbolic approaches (LINC, DetermLR).",
            "uuid": "e3454.8",
            "source_info": {
                "paper_title": "FOLIO: Natural Language Reasoning with First-Order Logic",
                "publication_date_yy_mm": "2022-09"
            }
        },
        {
            "name_short": "LINC",
            "name_full": "LINC",
            "brief_description": "A neurosymbolic approach for logical reasoning that combines language models with first-order logic provers; evaluated in this paper using GPT-4 as base.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "LINC (GPT-4 as base in experiments)",
            "model_description": "Neurosymbolic system that extracts or translates natural language into logic and exploits an FOL prover to perform deductions; used here with GPT-4 for the reasoning task.",
            "model_size": null,
            "reasoning_task_name": "FOLIO logical reasoning (NL truth-value prediction)",
            "reasoning_task_description": "Translate or extract FOL structure from NL and apply symbolic proving for strict logical deduction (first-order logic).",
            "method_or_intervention": "Neurosymbolic pipeline: LLM for translation/selection combined with FOL prover for inference (explicit symbolic verification).",
            "performance": "Reported accuracy 73.1% on FOLIO (using GPT-4 as base), ≈9.0 pp improvement over vanilla few-shot GPT-4 (64.2%).",
            "baseline_performance": "Vanilla few-shot GPT-4 64.2%.",
            "improvement_over_baseline": "+9.0 percentage points (73.1 - 64.2).",
            "limitations_or_failures": "Still below expert humans; dependent on correctness of NL-&gt;FOL translation and integration robustness. Paper does not include per-error breakdown specific to LINC within experiments here.",
            "ablation_or_analysis": "Paper reports performance averaged over multiple random few-shot sets; indicates neurosymbolic integration yields nontrivial gains compared to pure LLM prompting.",
            "uuid": "e3454.9",
            "source_info": {
                "paper_title": "FOLIO: Natural Language Reasoning with First-Order Logic",
                "publication_date_yy_mm": "2022-09"
            }
        },
        {
            "name_short": "DetermLR",
            "name_full": "DetermLR",
            "brief_description": "A method (From indeterminacy to determinacy) that augments LLMs with deterministic logical reasoning interventions; evaluated here with GPT-4 as the base model.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "DetermLR (GPT-4 as base in experiments)",
            "model_description": "Approach that augments LLM reasoning with deterministic logical procedures or constraints to reduce indeterminacy in deductions; instantiated with GPT-4 in the paper's evaluations.",
            "model_size": null,
            "reasoning_task_name": "FOLIO logical reasoning (NL truth-value prediction)",
            "reasoning_task_description": "Aim is to convert model outputs/partial reasoning into deterministic logical steps or leverage provers to achieve more reliable FOL deductions.",
            "method_or_intervention": "Augmentation of LLM reasoning with deterministic symbolic reasoning components or post-processing to enforce logical consistency.",
            "performance": "Reported accuracy 77.5% on FOLIO (using GPT-4 as base), ≈13.3 pp improvement over vanilla few-shot GPT-4 (64.2%).",
            "baseline_performance": "Vanilla few-shot GPT-4 64.2%.",
            "improvement_over_baseline": "+13.3 percentage points (77.5 - 64.2).",
            "limitations_or_failures": "Gains depend on reliable intermediate representations and integration; still below perfect/ expert-level reasoning; paper does not include granular ablation of which components drive improvements in this work.",
            "ablation_or_analysis": "Paper reports aggregate gains; the authors cite DetermLR as demonstrating that augmenting LLMs with deterministic logical apparatus substantially improves FOL reasoning, consistent with other neurosymbolic findings.",
            "uuid": "e3454.10",
            "source_info": {
                "paper_title": "FOLIO: Natural Language Reasoning with First-Order Logic",
                "publication_date_yy_mm": "2022-09"
            }
        },
        {
            "name_short": "FOLIO",
            "name_full": "FOLIO: Natural Language Reasoning with First-Order Logic",
            "brief_description": "The dataset and benchmark introduced in this paper: 1,430 FOL-verified NL conclusions paired with 487 premise-sets, annotated with parallel FOL formulas and verified by an FOL inference engine, designed to evaluate strict first-order logical reasoning in natural language.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "FOLIO (benchmark/dataset)",
            "model_description": "Dataset of expert-written NL stories (WikiLogic and HybLogic subsets) with parallel FOL annotations and verified truth labels (True/False/Unknown); used for two tasks: NL truth-value prediction and NL-&gt;FOL translation. Stories require multi-step first-order logic reasoning (mode depth 4; 28.7% need &gt;=5 depths).",
            "model_size": null,
            "reasoning_task_name": "Natural language reasoning with first-order logic; NL-&gt;FOL translation",
            "reasoning_task_description": "Evaluate model ability to make strictly deductive first-order logic inferences from NL premises and to translate NL to semantically equivalent FOL formulas; includes measures of syntactic validity and inference-engine execution accuracy for NL-&gt;FOL.",
            "method_or_intervention": "Used as evaluation benchmark for supervised fine-tuning and few-shot prompting approaches, as well as neurosymbolic methods (Logic-LM, LINC, DetermLR) and prompting strategies (CoT, SC, ToT); also used to test NL+FOL concatenation in prompts.",
            "performance": "Dataset itself; reported model results on this benchmark: majority baseline 38.5%; random 33.3%; fine-tuned models range 56.8%–65.9%; GPT-4 few-shot 64.2% (CoT/SC/ToT up to 70.0%); neurosymbolic methods up to 78.1%. NL-&gt;FOL few-shot syntactic validity ~93% and execution accuracy 56.0% (GPT-3.5) / 63.8% (GPT-4).",
            "baseline_performance": null,
            "improvement_over_baseline": null,
            "limitations_or_failures": "Dataset is deliberately high-quality but relatively small (1,430 examples); scaling annotation is resource-intensive (980 man-hours). Authors note models still struggle substantially on long reasoning chains and on HybLogic (template-based, higher depth) examples.",
            "ablation_or_analysis": "Paper includes analyses: per-subset (WikiLogic vs HybLogic), impact of reasoning depth, NL vs NL+FOL prompting, premise-order shuffling, confusion matrices showing bias toward 'True' predictions, human evaluation of model errors (faulty path 65%, wrong derivation 25%, etc.).",
            "uuid": "e3454.11",
            "source_info": {
                "paper_title": "FOLIO: Natural Language Reasoning with First-Order Logic",
                "publication_date_yy_mm": "2022-09"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Chain of thought prompting elicits reasoning in large language models",
            "rating": 2,
            "sanitized_title": "chain_of_thought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Self-consistency improves chain of thought reasoning in language models",
            "rating": 2,
            "sanitized_title": "selfconsistency_improves_chain_of_thought_reasoning_in_language_models"
        },
        {
            "paper_title": "Tree of thoughts: Deliberate problem solving with large language models",
            "rating": 2,
            "sanitized_title": "tree_of_thoughts_deliberate_problem_solving_with_large_language_models"
        },
        {
            "paper_title": "LINC: A neurosymbolic approach for logical reasoning by combining language models with first-order logic provers",
            "rating": 2,
            "sanitized_title": "linc_a_neurosymbolic_approach_for_logical_reasoning_by_combining_language_models_with_firstorder_logic_provers"
        },
        {
            "paper_title": "Logic-LM: Empowering large language models with symbolic solvers for faithful logical reasoning",
            "rating": 2,
            "sanitized_title": "logiclm_empowering_large_language_models_with_symbolic_solvers_for_faithful_logical_reasoning"
        },
        {
            "paper_title": "From indeterminacy to determinacy: Augmenting logical reasoning capabilities with large language models",
            "rating": 2,
            "sanitized_title": "from_indeterminacy_to_determinacy_augmenting_logical_reasoning_capabilities_with_large_language_models"
        },
        {
            "paper_title": "Transformers as soft reasoners over language",
            "rating": 1,
            "sanitized_title": "transformers_as_soft_reasoners_over_language"
        },
        {
            "paper_title": "RuleTaker: Transformers as soft reasoners over language (Clark et al., 2020)",
            "rating": 1,
            "sanitized_title": "ruletaker_transformers_as_soft_reasoners_over_language_clark_et_al_2020"
        },
        {
            "paper_title": "GPT-4 Technical Report",
            "rating": 1,
            "sanitized_title": "gpt4_technical_report"
        }
    ],
    "cost": 0.02119975,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>FOLIO: Natural Language Reasoning with First-Order Logic
11 Oct 2024</p>
<p>Simeng Han 
Yale University</p>
<p>Hailey Schoelkopf 
Yale University</p>
<p>Yilun Zhao 
Yale University</p>
<p>Zhenting Qi 
Harvard University</p>
<p>Martin Riddell 
Yale University</p>
<p>Wenfei Zhou 
NVIDIA</p>
<p>James Coady 
Yale University</p>
<p>David Peng 
Yale University</p>
<p>Yujie Qiao 
Yale University</p>
<p>Luke Benson 
Yale University</p>
<p>Lucy Sun 
Yale University</p>
<p>Alex Wardle-Solano 
Yale University</p>
<p>Hannah Szabo 
Yale University</p>
<p>Ekaterina Zubova 
Yale University</p>
<p>Matthew Burtell 
Yale University</p>
<p>Jonathan Fan 
Iowa City West High School</p>
<p>Yixin Liu 
Yale University</p>
<p>Brian Wong 
Yale University</p>
<p>Malcolm Sailor 
Yale University</p>
<p>Ansong Ni 
Yale University</p>
<p>Linyong Nan 
Yale University</p>
<p>Jungo Kasai 
University of Washington</p>
<p>Tao Yu 
University of Hong Kong</p>
<p>Rui Zhang 
Penn State University
8 Meta AI</p>
<p>Alexander R Fabbri 
Salesforce Research</p>
<p>Wojciech Kryści Ński 
Salesforce Research</p>
<p>Semih Yavuz 
Salesforce Research</p>
<p>Ye Liu 
Salesforce Research</p>
<p>Victoria Xi 
Lin 
Shafiq Joty 
Salesforce Research</p>
<p>Yingbo Zhou 
Salesforce Research</p>
<p>Caiming Xiong 
Salesforce Research</p>
<p>Rex Ying 
Yale University</p>
<p>Arman Cohan 
Yale University</p>
<p>Dragomir Radev 
Yale University</p>
<p>Salesforce Research</p>
<p>FOLIO: Natural Language Reasoning with First-Order Logic
11 Oct 202491EE60091D091474A0C7AA791AED16AEarXiv:2209.00840v3[cs.CL]
Large language models (LLMs) have achieved remarkable performance on a variety of natural language understanding tasks.However, existing benchmarks are inadequate in measuring the complex logical reasoning capabilities of a model.We present FOLIO, a human-annotated, logically complex and diverse dataset for reasoning in natural language (NL), equipped with first-order logic (FOL) annotations.FOLIO consists of 1,430 examples (unique conclusions), each paired with one of 487 sets of premises used to deductively reason for the validity of each conclusion.The logical correctness of the premises and conclusions is ensured by their FOL annotations, which are automatically verified by an FOL inference engine.In addition to the main NL reasoning task, NL-FOL pairs in FOLIO constitute a new NL-FOL translation dataset.Our experiments on FOLIO systematically evaluate the FOL reasoning ability of supervised fine-tuning on medium-sized language models.For both NL reasoning and NL-FOL translation, we benchmark multiple state-of-the-art language models.Our results show that a subset of FOLIO presents a challenge for one of the most capable Large Language Model (LLM) publicly available, GPT-4.</p>
<p>Introduction</p>
<p>Large language models (LLMs) have achieved remarkable performance on a variety of natural language tasks (OpenAI et al., 2023;Touvron et al., 2023;Srivastava et al., 2023;Wang et al., 2019a).Logical reasoning is a central component for intelligent systems and should be sufficiently and independently evaluated (Russell and Norvig, 2010).</p>
<p>However, existing natural language tasks are inadequate in measuring the complex logical reasoning capability of a model (Srivastava et al., 2023;Saparov and He, 2023;Tian et al., 2021).</p>
<p>Several datasets related to logical reasoning have recently been proposed.However, existing benchmarks often exhibit limited complexity in reasoning or lack language naturalness.Some of these common benchmarks do not specifically evaluate logical reasoning independently of other forms of reasoning (Yu et al., 2020;Liu et al., 2021).Those specifically designed for measuring logical reasoning are insufficient in terms of logical reasoning complexity and natural language variety.As shown in Table 1, examples in RuleTaker (Clark et al., 2020) and LogicNLI (Tian et al., 2021) need at most five depths of reasoning.The entire corpus of RuleTaker or LogicNLI has fewer than 50 distinct abstract syntax trees.RuleTaker has only 101 words in its vocabulary and LogicNLI has 1077 words in the vocabulary.Moreover, none of them are written by humans with information drawn from real-world knowledge, making them less applicable to real-world reasoning scenarios.The logical deduction portion in BigBench (Srivastava et al., 2023) requires commonsense reasoning besides logical deduction.ProntoQA (Saparov and He, 2023) only contains logical reasoning questions that are answerable with repeated applications of the Modus Ponens inference rule.</p>
<p>We present a natural language reasoning dataset, FOLIO, with first-order logic reasoning problems which require the models to decide the correctness of conclusions given a world defined by the premises.In FOLIO, we aim to ensure high lan- A FOLIO example based on the Wild Turkey Wikipedia page: https://en.wikipedia.org/wiki/Wild_turkeyNL premises NL Conclusions -&gt; Labels 1.There are six types of wild turkeys: Eastern wild turkey, Osceola wild turkey, Gould's wild turkey, A. Tom is an Ocellated wild turkey.-&gt; True Merriam's wild turkey, Rio Grande wild turkey, and the Ocellated wild turkey.</p>
<p>B. Tom is an Eastern wild turkey.-&gt; False 2. Tom is not an Eastern wild turkey.</p>
<p>C. Joey is a wild turkey.-&gt; Unknown 3. Tom is not an Osceola wild turkey.4. Tom is also not a Gould's wild turkey.5. Tom is neither a Merriam's wild turkey, nor a Rio Grande wild turkey.6. Tom is a wild turkey.</p>
<p>FOL Premises</p>
<p>FOL conclusions -&gt; Labels
1. ∀x(WildTurkey(x) → (EasternWildTurkey(x) ∨ OsceolaWildTurkey(x) ∨ GouldsWildTurkey(x) A. OcellatedWildTurkey(tom) -&gt; True ∨ MerriamsWildTurkey(x) ∨ RiograndeWildTurkey(x) ∨ OcellatedWildTurkey(x))) B. EasternWildTurkey(tom) -&gt; False 2. ¬EasternWildTurkey(tom)
C. WildTurkey(joey) -&gt; Unknown 3. ¬OsceolaWildTurkey(tom)) 4. ¬GouldsWildTurkey(tom) 5. ¬MerriamsWildTurkey(tom) ∧ ¬RiograndeWildTurkey(tom) 6. WildTurkey(tom) Table 2: An example story in FOLIO based on the knowledge from the Wikipedia page on wild turkeys.The story consists of five premises and three conclusions with their corresponding FOL formulas and labels for the conclusions.All five premises are needed to infer the conclusions.The model needs to reason under logic patterns with universal quantification (∀), negation (¬), conjunction (∧), and disjunction (∨).guage naturalness and complexity, an abundant vocabulary, and factuality while also maintaining high reasoning complexity.FOLIO is a high-quality and manually curated dataset, written by CS undergraduate and graduate students and researchers in academia and industry.To ensure the conclusions of our examples follow the premises logically, we annotated all reasoning examples with first-order logic (FOL) formulas.An example of FOLIO is shown in Table 2. Based on our annotations, we propose a new NL-FOL translation task where an NL reasoning example is translated into its FOL counterpart.Finally, we benchmark the performance of strong LMs in both fully supervised and few-shot settings to understand their capabilities in logical reasoning (i.e., deriving the truth value of a logical conclusion from NL premises).</p>
<p>Under the few-shot setting, the most capable publicly available LLM so far achieves only 53.1% on the stories written in a hybrid manner, which is slightly better than random.</p>
<p>To sum up, the contributions of this paper are threefold.1) We release a natural language reasoning dataset written by expert annotators, FOLIO, with first-order logical reasoning problems.2) We use formal logic, i.e., FOL to ensure the logical validity of the examples written in NL and propose a new NL-FOL translation task.3) We benchmark the performance of LMs by fine-tuning models and prompting LLMs with few-shot examples, on the FOLIO reasoning task.We hope that FOLIO, as a challenging logical reasoning dataset, will be used to facilitate measuring progress in the logical reasoning capabilities of language models.</p>
<p>Related Work</p>
<p>Datasets for reasoning from text</p>
<p>Developing models that can reason in texts has been a core goal in NLP since the field's early days (Cooper et al., 1996).Since then, there has been massive progress in reasoning over text.Various benchmarks that focus on different aspects of reasoning over textual inputs are proposed, including natural language inference (NLI) (Bowman et al., 2015;Wang et al., 2019b), reasoning for commonsense knowledge (Talmor et al., 2019;He et al., 2021) and multi-hop reasoning (Yang et al., 2018;Chen et al., 2020).Among these reasoning abilities, logical reasoning has recently attracted an increasing amount of study.ReClor (Yu et al., 2020) and LogiQA (Liu et al., 2021) both collected multiplechoice questions from standardized graduate admission examinations, answering which requires various types of logical reasoning.However, these datasets cover mixed forms of reasoning and are not intended to test logical reasoning in isolation.</p>
<p>Meanwhile, testing logical reasoning in isolation without involving other forms of reasoning has also attracted researchers in recent years.CLUTRR (Sinha et al., 2019) covers inductive reasoning, which is beyond the scope of first-order logic.Synthetic corpuses of deductive reasoning are proposed to evaluate the deductive reasoning ability of pretrained LMs (Clark et al., 2021;Saeed et al., 2021;Tian et al., 2021).However, these datasets do not contain highly natural sentences and often cover limited forms of logic while FOL is much more expressive.Kazemi et al. (2023) created a dataset for reasoning with contradictory information.Kawabata and Sugawara (2023) crowdsourced rationales for over 3000 examples based on ReClor (Yu et al., 2020).ProntoQA (Saparov and He, 2023) is comprised solely of logical reasoning queries that can be resolved through applying the Modus Ponens inference rule while FOLIO questions require applications of multiple types of inference rules.As shown in Table 1, FOLIO is the first large-scale first-order logic (FOL) reasoning dataset with formal logic annotations in FOL.FO-LIO is logically diverse and complex with complex natural language sentences and a rich vocabulary.</p>
<p>Reasoning using large language models</p>
<p>Reasoning has been demonstrated as one of the emergent abilities of LLMs of sufficient scale recently (Talmor et al., 2020;Wei et al., 2022a;Chowdhery et al., 2022).One such emergent behavior, Chain-of-Thought prompting (Wei et al., 2022b), consists of a series of intermediate reasoning steps output by an LLM.This improves the performance on arithmetic, commonsense, and symbolic reasoning benchmarks significantly.There has been a line of research continuing on from Chain-of-Thought (Kojima et al., 2022;Li et al., 2022;Yao et al., 2023) to elicit reasoning behavior from LLMs.Building on Chain-of-Thought prompting, many techniques used on top of LLMs to improve downstream performance have been formalized into control flows and programs.These are called language model cascades (Dohan et al., 2022), subsuming techniques such as Chain-of-Thought prompting, STaR (Zelikman et al., 2022), and Selection-Inference (Creswell et al., 2022) for reasoning.Dasgupta et al. (2022) studied the reasoning ability of LLMs but only used a small set of 48 syllogisms with only two premises each.Saparov and He (2023) created a synthetic dataset that and showed that LLMs are capable of making correct individual deduction steps.</p>
<p>With FOLIO, we aim to set a high standard, ensuring that achieving high performance through superficial strategies and shallow heuristics is prevented, allowing a robust evaluation of the firstorder logic reasoning capabilities of LLMs.We show that many LLMs fall short on complex firstorder logic reasoning, and that significant room for improvement in this area remains.</p>
<p>FOLIO Corpus Construction</p>
<p>We collected FOLIO through a carefully designed manual annotation process to achieve high-quality examples that necessitate complex logical reasoning.Writing natural language reasoning stories with FOL requires sufficient knowledge in both semantic parsing and first-order logic, as well as strong analytical skills.Given the complexities of such annotations, we selected annotators based on a few important criteria to ensures that our dataset is annotated with the highest level of precision and expertise, reflecting the complexity and nuance required for first-order logical reasoning.1).Our annotators are either college or graduate students who are native English speakers or possess nearnative proficiency in English.4 2).They possess formal education in first-order logic, having either completed relevant coursework or undertaken self-directed studies in first-order logic or seman-tic parsing.At the NL quality check stage, only annotators who are experts in natural language processing or computational linguistics are involved.For the FOL quality check, only annotators who are experts in first-order logic are involved.We also give the annotators several training sessions on how to write a story, by providing them with detailed annotation guidelines.All stories and FOL annotations in FOLIO are written and reviewed by expert annotators, including CS undergraduate and graduate students, and senior researchers, who met the aforementioned criteria.</p>
<p>We develop our dataset in six stages: WikiLogic collection, HybLogic collection, NL quality control, FOL quality control, NL-FOL alignment and FOL verification, spending 980 man-hours in total.</p>
<p>Example collection</p>
<p>We collected our dataset using two different methods in order to obtain examples that are both logically diverse and complex and have abundant abstract syntax tree (AST) variations.The annotators are free to write stories based on any topic they want while writing the stories.</p>
<p>WikiLogic: annotation from scratch using Wikipedia articles as seeds.At this annotation stage, the annotators are asked to select random Wikipedia pages by repeatedly using the Wikipedia Special Random link. 1 The Wikipedia articles are used to develop ideas for topics to write new stories.We ask the annotators to create new stories from scratch without using templates based on realworld knowledge, which should be plausible in general.Each of the stories is composed of several premises and conclusions with truth values of True, False, or Unknown (see Table 2 for an example).We also ask the annotators to write parallel FOL sentences for both the premises and conclusions.This results in a wide range of topics, abundant AST variations, and a wide vocabulary for FOLIO.Table 1 shows a comparison of FOLIO with other reasoning datasets that purely evaluate first-order logic or deductive reasoning.</p>
<p>HybLogic: hybrid annotation The task of generating logically sound stories from scratch for a set of facts is very time-consuming for human writers, where the main challenge is to create complex and varied logical patterns to arrive at a conclusion.To address the problems of solely using manual 1 https://en.wikipedia.org/wiki/Special:Randomannotation, we also consider a hybrid approach to facilitate the process.Our hybrid method is based on a common form of logical stories: syllogisms.A syllogism consists of two premises and a single conclusion, and the conclusion states some facts about the entities and categories in the premises.</p>
<p>In this approach, we first generate logically valid stories, which are templates containing abstract categories and entities, by combining multiple syllogisms into a single story template: the conclusion of one syllogism is used as a premise for the next syllogism.There are 256 logically distinct types of syllogisms and 24 of them are valid (Lehman, 1973).We use various combinations of 24 valid syllogisms.We also add in conjunction, disjunction, and implication.We show an example of the resulting templates in Appendix B. We then ask human annotators to assign nouns, phrases, or clauses to the abstract entities or categories that reflect real-life scenarios to each template and write logically-valid stories in natural language.The usage of the template is to ensure that we have a set of varied and complex logical stories with multiple conclusions.There are many ways of expressing the same logic template in natural language, and so the generated templates augment, rather than limit, the creativity of humans.</p>
<p>Quality control for NL sentences</p>
<p>To ensure the highest quality of the dataset, we dedicated considerable attention to the following key aspects of the natural language sentences during the quality control process.</p>
<p>Factuality and bias Our dataset prioritizes realism and factual accuracy, steering clear of biases and stereotypes linked to identity markers like race, ethnicity, gender, sexuality, nationality, class, and religion.Toward these objectives, we manually screened all stories and found that 39.2% of the stories suffer from at least one of these issues.We implemented a detailed protocol to rewrite these stories.The protocol is in Appendix C.</p>
<p>Language quality Apart from grammar, we make sure the sentences in our dataset are highly natural.All the sentences are first checked with a grammar checking tool, Grammarly.Our annotators who have graduated from or are senior students studying English Literature conducted a thorough round of review for grammatical correctness and language naturalness.We also eliminate natural language ambiguity when it is possible.We include rules on eliminating ambiguity in Appendix D. Employing these rules effectively reduces the ambiguity of natural language in this reasoning dataset, but incurs the tradeoff of limiting variations in some usage of language.However, we note that there is still sufficient variation in terms of sentence structures and logical structures as shown in Table 1.</p>
<p>Quality control for FOL formulas</p>
<p>We adopt the FOL definitions and syntax most widely used in the AI community (Russell and Norvig, 2010).We include more details on the definition of FOL we consider and the FOL modelling convention in Appendix E In preliminary investigations, we found that the human-written FOL formulas suffer from FOL consistency issues, which necessitates an additional round of quality control for FOL formulas.</p>
<p>FOL consistency One NL sentence can be translated into FOL through multiple non-equivalent ways.For example, sometimes additional information inferred from a sentence can be represented in FOL, leading to multiple representations.We therefore design an annotation protocol for FOL translation in order to ensure that our FOL translations are as consistent as possible across all examples in our dataset.We highlight a few important strategies used in the annotation protocol in Appendix F.</p>
<p>NL-FOL alignment review</p>
<p>Apart from checking whether NL and FOL express equivalent meanings, we also add necessary commonsense knowledge in both the NL and FOL premises.Sometimes humans do not write certain commonsense knowledge in the premises that is required in the FOL reasoning process, which is based solely on the premises given.We add such knowledge as additional premises at this stage.In particular, intrinsic properties of some predicates are required in the FOL reasoning process.For example, "LocatedIn(x,y)" should be transitive and "BeFamily(x,y)" should be symmetric.</p>
<p>FOL verification</p>
<p>Recognizing that the FOL formula annotations can be error-prone, we verify the syntactic validity and label consistency of FOL formula annotations with an FOL inference engine.We include the details of the FOL inference engine in Appendix G.</p>
<p>Dataset statistics</p>
<p>We show basic statistics of FOLIO and demonstrate the abundant vocabulary and logical complexity of FOLIO: Tables 1, 3 and Figure 1.Natural language complexity We use the Dale-Chall Readability Formula (Dale andChall, 1948, 1995) to show the text complexity of FOLIO following (Singh et al., 2023;Arps et al., 2022;Wei et al., 2021).We show the distribution of readability in Appendix H.</p>
<p>Basic statistics</p>
<p>Logical complexity and diversity statistics</p>
<p>As shown in Figure 1, the mode of reasoning depths is four in FOLIO.28.7% of the examples need five or more depths of reasoning to infer the conclusions, while the previous datasets needed at most five reasoning depths as shown in Table 1.This illustrates the logical complexity of FOLIO.Table 1 shows that FOLIO also has a much larger number of distinct ASTs than the previous datasets, indicating that FOLIO is much more logically diverse.larger than the previous synthetically constructed datasets for logical reasoning.</p>
<p>Vocabulary and topics</p>
<p>Task Definition</p>
<p>We define two new tasks based on FOLIO, natural language reasoning with first-order logic and NL-FOL translation.</p>
<p>Natural language reasoning with first-order logic</p>
<p>Each natural language (NL) story S in FOLIO consists of n premises: P = {p 1 , p 2 , ..., p n } and m conclusions: H = {h 1 , h 2 , ..., h m }.All NL stories are annotated with parallel FOL stories SF , which are sets of FOL formulas consisting of n premises P F = {pf 1 , pf 2 , ..., pf n } and m conclusions HF = {hf 1 , hf 2 , ..., hf m }. pf i and hf i are logically and semantically similar to p i and h i , respectively.Given P and H, the goal is to determine the truth values of the conclusions: "True", "False" or "Unknown", based on FOL reasoning.</p>
<p>NL-FOL translation</p>
<p>We propose a new natural language to first-order logic translation (NL-FOL translation) task alongside our reasoning dataset.The goal of this task is to translate an NL story S to an FOL story F S.</p>
<p>In particular, each of the NL sentence p i or h i and the parallel FOL formula pf i or hf i should be logically and semantically equivalent.Moreover, the truth values for the conclusions should be the same based on the NL story S and the parallel FOL story F S. In our dataset, the premises and conclusions are set up in such a way to ensure that the inference engine always returns an answer given enough resources such as time and memory.Unlike previous work (Singh et al., 2020) which translates problems with a single premise and a single hypothesis, our task is on translating examples of various lengths with a focus on stories with multiple premises.Thus, it also requires the models to consider discourse-level consistencies as opposed to translation at the sentence level.</p>
<p>NL-FOL evaluation metrics Two metrics are adopted to evaluate NL-FOL translation to capture different aspects of the generation results: 1).Syntactic validity (SynV).The Syntactic Validity score measures whether the FOL formulas are syntactically valid.The score will be 1 if all FOL formulas of an example can pass the syntactic check and 0 otherwise 2).Inference Engine execution accuracy (ExcAcc).The group of translated FOL for premises and conclusions in one story is fed into our inference engine to output the truth value for each conclusion.We define the accuracy of the output labels as the execution accuracy.We leave for future work the design of a more reliable metric of NL-FOL translation.</p>
<p>Experiments</p>
<p>In this section, we describe our experiments and main results.</p>
<p>Experimental setup</p>
<p>Tasks We conduct experiments on the two tasks in §4: NL reasoning with first-order logic (logical reasoning) and NL-FOL translation (NL-FOL).</p>
<p>Dataset split We split FOLIO by 70%/15%/15% split for the train/validation/test sets with 1,001/203/226 examples respectively.We split by story so that models are evaluated on unseen stories.</p>
<p>Evaluation metrics</p>
<p>We use accuracy for evaluating logical reasoning performance.For NL-FOL translation, we use the metrics in Section 4.2.</p>
<p>Models</p>
<p>We test the logical reasoning capabilities of LMs using fully supervised fine-tuning and few-shot prompting.We also test NL-FOL translation with few-shot prompting.</p>
<p>Fully supervised fine-tuning As fine-tuning baselines, we experiment with BERT (Devlin et al., 2019), and RoBERTa (Liu et al., 2020).We finetune the base and large versions of both BERT and RoBERTa, with an additional two-layer classification layer to predict the truth values.For the second task, i.e., NL-FOL translation, we only report fewshot prompting methods.Prompting strategies We experiment with incorporating recent prompting strategies into GPT-4 as they have shown improvements in the general reasoning performance of LLMs.The prompting strategies include chain-of-thought (CoT) prompting (Wei et al., 2022b), chain-of-thought prompting with self-consistency (Wang et al., 2023) and treeof-thought prompting (Yao et al., 2023).</p>
<p>Few-shot prompting</p>
<p>Logical reasoning methods</p>
<p>We also test recent methods specifically designed for logical reasoning: Logic-LM (2023), LINC (Olausson et al., 2023) and DetermLR (Sun et al., 2023), using GPT-4 as the base model.For the second task (NL-FOL translation), we use the same examples as in the Few-Shot NL experiments except that all the conclusions are included in each example.</p>
<p>We run experiments on five randomly sampled sets of examples from the training set and report the average accuracy.</p>
<p>Main results</p>
<p>Logical reasoning</p>
<p>The majority baseline of our dataset is 38.5% since in our test set, there are 87, 78 and 61 examples with labels of true, false and unknown respectively.As shown in Table 4, BERTbase and RoBERTa-base have similar performance on FOLIO with 56.83% accuracy.BERT-large has a 2.2% improvement over BERT-base.RoBERTalarge improves 3.1% over BERT-large.Flan-T5-Large achieves the highest performance in the finetuning setting and the accuracy is 65.7%.The model sizes of text-davinci-002, GPT-3.5-Turbo and GPT-4 are hidden from public3 .CoT stands for chain-of-thought prompting (Wei et al., 2022b).SC stands for self-consistency (Wang et al., 2023).ToT stands for tree-of-thought prompting (Yao et al., 2023).</p>
<p>We show that zero-shot prompting GPT-3.5 achieves better results than few-shot prompting text-davinci-002.Under few-shot NL prompting setting, LLama-13B achieves 33.63%, which is only slightly better than chance (33%).LLama-70B achieves 43.97%, around 10% better than LLaMA-13B and obtains improvements of around 4% with Chain-of-thought prompting and Tree of Thought prompting.Text-davinci-002 achieves 49.53% and GPT-3.5 achieves 58.34%.GPT-4 achieves the best results among GPT series models.</p>
<p>Incorporating recent prompting strategies increases the performance of vanilla few-shot prompting.Chain-of-thought prompting achieves more than a 4% increase over GPT-4.Self-consistency (SC) improves chain-of-thought prompting by 0.6% percent.Tree-of-thought prompting achieves slightly better result than self-consistency with chain-of-thought prompting.For the results of recent methods developed for logical reasoning, LINC (Olausson et al., 2023) achieves around a 9% increase over few-shot prompting GPT-4.Both Logic-LM (GPT-4)( 2023) and DetermLR ( 2023) achieves more than a 13% increase over few-shot prompting GPT-4, showing the superiority of the methods on logical reasoning.</p>
<p>NL-FOL translation</p>
<p>Table 5 shows the results of NL-FOL translation.The syntactic validity scores are around 93% with both GPT-3.5-Turbo and GPT-4.This indicates that language models with sufficient scales are good at picking up the patterns for FOL formulas and generating syntactically valid FOL formulas.However, GPT-3.5-Turbo and GPT-4 are not yet good at translating an NL story to a logically or semantically similar FOL counterpart, as indicated by the low inference engine execution accuracy score.</p>
<p>Error Analysis</p>
<p>Below we provide analysis of our results and key findings, providing additional insights into our dataset FOLIO and the current capabilities of LLMs in logical reasoning.example chains.We note that the presence and prevalence of these difficult examples are unique to FOLIO.FOLIO's unique complexity reveals that current LMs are limited in their ability to extrapolate to longer and more complex reasoning chains, and suggests an avenue for further study.Models have higher accuracy on WikiLogic than on HybLogic As shown in Table 6, in logical reasoning, GPT-3.5 and GPT-4 achieve substantially lower results on HybLogic than on WikiLogic and the result is slightly higher than chance.We hypothesize that this is because HybLogic has high logical complexity that the SoTA LLMs like GPT-4 cannot solve yet while WikiLogic examples require shorter reasoning chains which the model is already capable of solving.Moreover, since the examples in WikiLogic are created from scratch by humans, it is possible that LLMs have seen similar texts with similar logical patterns in the training data.However, fine-tuning RoBERTa-large achieves higher performance on HybLogic than on WikiLogic.This is likely because HybLogic is created from templates and some of the logical patterns can be learned during fine-tuning.</p>
<p>Models have higher accuracy on examples with fewer reasoning depths than on those with higher number of reasoing depths</p>
<p>In NL-FOL translation, performs 10 points better on HybLogic than WikiLogic.This could be because WikiLogic has more distinct and diverse sentence-level logical and language patterns and FOL annotations.WikiLogic has 53 ASTs while HybLogic has 33.Despite being more logically complex on a story level, FOL translations for Hy-bLogic stories have simpler logical structures on a statement level.We include case study for one WikiLogic example and one HybLogic example in Appendix I and further analysis on model performance in Appendix J.</p>
<p>Faulty path 65%</p>
<p>Wrong derivation 25%</p>
<p>Wrong syntactic comprehension 5%</p>
<p>Spurious shortcut 5%</p>
<p>GPT-4 Output</p>
<p>We know that all children are human (premise 1) and if someone is underage, they are a child (premise 2).People are either underage or of age (premise 3).If someone is of age, they can vote (premise 4) and get married (premise 5).If Jack is a child and a human, then Jack is neither able to vote nor get married (premise 6).We don't have any information about Jack's age, so we cannot determine if he is a child or of age.Therefore, we cannot determine if Jack is able to vote and get married.</p>
<p>Table 8: Case study for the scenario where a model is unable to form the correct reasoning chain.</p>
<p>Human evaluation on model outputs We conduct human evaluation on the GPT-4 model outputs with wrong truth value predictions.As shown in Table 7, approximately 65% of the time, the model struggles to construct accurate reasoning chains for complex problems with intricate steps, leading to faulty reasoning paths and indicating a limited ability to solve problems with long reasoning chains.In 25% of cases, erroneous derivations occur within certain reasoning steps, highlighting potential inaccuracies and flaws in logical deductions.5% of conclusions in FOLIO have a complex syntactic structure, posing comprehension challenges for GPT-4.5% of outputs show that GPT-4 leverage commonsense reasoning to employ spurious shortcuts that lead to the wrong truth value for the conclusion.We provide a case study for the "Faulty path" scenario in Table 8.In this instance, the model can perform simple derivations from the premises, like "If someone is of age, they can vote and get married."However, because of the problem's complexity, the model struggles to identify the essential intermediate steps and cannot ascertain the truth value of conclusions, such as "Jack is not a child."</p>
<p>Human performance</p>
<p>We collected truth value annotations of logical reasoning for FOLIO test set from expert and nonexpert annotators.Our expert annotators are computer science college students familiar with FOL.Non-expert annotators are community college or high school students who have not taken the SAT.Both expert and non-expert annotators are native English speakers.Expert annotations achieve an accuracy of 95.98% while non-expert annotations achieves 61.82%, with a gap of 34.16%.This shows that sufficient domain knowledge of FOL is necessary for good performance on FOLIO.The expert and GPT-4 gap is 31.82%,suggesting significant room for model improvement.</p>
<p>Conclusion</p>
<p>We introduced FOLIO, an expert-written dataset for logical reasoning equipped with FOL formulas.The examples in FOLIO are created based on real-world knowledge with natural language.It exhibits a large number of distinct logic patterns and a large vocabulary.Experiments show that FOLIO presents a challenge for one of the most capable Large Language Model publicly available.</p>
<p>Limitations</p>
<p>We focus on collecting a very high-quality dataset in evaluating logical reasoning rather than merely a large dataset.Optimizing for quality required us to adopt a rigorous annotation process with domain experts selected based on a few important criteria as mentioned in Appendix A: Annotator Selection.Significantly scaling up this process would have required resources beyond our current means and we are unable further expand our dataset for investigating how the size of training data affects the performance of fine-tuning experiments.We encourage the community to apply our annotation protocol to expand this realistic and complex FOL reasoning story set.</p>
<p>A Annotator Selection</p>
<p>Given the complexities of our annotations, we selected annotators based on a few important criteria 1).Our annotators are either college or graduate students who are native English speakers or possess near-native proficiency in English. 42).They possess formal education in first-order logic, having either completed relevant coursework or undertaken self-directed studies in first-order logic or semantic parsing.At the NL quality check stage, only annotators who are experts in natural language processing or computational linguistics are involved.For the FOL quality check, only annotators who are experts in first-order logic are involved.We also give the annotators several training sessions on how to write a story, by providing them with detailed annotation guidelines.All stories and FOL annotations in FOLIO are written and reviewed by expert annotators, including CS undergraduate and graduate students, and senior researchers, who met the aforementioned criteria.</p>
<p>B HybLogic Template Example</p>
<p>An example the resulting template is as follows:</p>
<p>Conclusions:</p>
<p>[Unknown] a is an S.</p>
<p>[True] If a is either a C or a D, then a is not either an A or a B.</p>
<p>C Factuality and Bias Elimination Protocol</p>
<p>We rewrote those that are not reflective of wellestablished scientific, historical, or legal facts.We took out stories that had strongly opinionated language and contained gender, racial, and classist biases.We accept certain classes of "psychologically fundamental generalizations" (Leslie, 2008), however, such as "Covid is transmitted through the air" or "Tigers eat other animals," that may not be factually invariant but add logical and semantic nuances to the stories.For stories that pertain to generalization, such as "All As are Bs," we have added specifiers like "all Dan knows" to give a degree of reasonable factuality.For example, "All science fiction that Dan knows comes from an imaginative process" has a more reasonable degree of factuality than "All science fiction comes from an imaginative process."</p>
<p>D Language Quality Control</p>
<p>• We always use "either-or" to express exclusive disjunction.We use either "A or B" or "A or B, or both" to express inclusive disjunction.In English "or" itself can be interpreted as either inclusive disjunction or exclusive disjunction.Adding "or both" cancels the exclusive disjunction distinctly.However, it is less common in the wild than just using "or".we could add "or both" if it is important to emphasize the inclusive part semantically or contextually or for factuality; and do not add "or both" if it is not.We rely on the language model to figure out if it should be inclusive or exclusive, therefore not sacrificing naturalness.</p>
<p>• It is more natural to say "Some A is B" rather than "there exists an A such that A is B." "All A are B" can be more natural than "If A then B".</p>
<p>• Writing NL sentences that express negation over exclusive-or ("either both or neither") can be cumbersome but we found one natural ways of expressing these situations: "Each morning, John either works out and stretches, or he does neither".</p>
<p>Other common issues in NL quality include singular/plural issues, especially in statements that deal with both categories and individual members of those categories; as well as ambiguities resulting from improper introduction of, or failure to introduce, proper nouns.</p>
<p>E First-Order Logic E.1 First-Order Logic VS Natural Language FOL enables deriving facts from other facts (Russell and Norvig, 2010).In the context of logical reasoning in modern NLP, FOL, as a logical form, is a more explicit logical representation than its NL counterpart and can be used as input to an FOL prover in order to obtain the exact truth values for the conclusions.FOL has no ambiguity while ambiguity can occur at various levels of NLP.FOL can thus be a good interface between how LMs are trained and how logical conclusions are reasoned.</p>
<p>E.2 FOL definition</p>
<p>We include the following operators: negation ¬, conjunction ∧, disjunction ∨, implication →, universal quantifier ∀, existential quantifier ∃, equal =.Following (Russell and Norvig, 2010), we consider temporal logic and modal logic as special-purpose logics.Consequently, they are beyond the scope of the definition of first-order logic used in our dataset.</p>
<p>E.3 FOL modeling conventions</p>
<p>We use n-place predicates when applicable for the expressivity of the FOL formulas.However, we do not use the Davidsonian (Davidson, 2001) or neo-Davidsonian semantics (Parsons, 1990) because translating the majority of the FOL formulas in our dataset only requires one-place and twoplace predicates.Therefore the Davidsonian or neo-Davidsonian semantics are not necessary for the expressivity of the FOL formulas.</p>
<p>For example, "Enjoy dressing up in oldfashioned clothing" is rendered as "Enjoy(x, dressingUp, oldFashionedClothing)".</p>
<p>F FOL Annotation Protocol</p>
<p>We therefore design an annotation protocol for first-order logic translation in order to ensure that our FOL translations are as consistent as possible across all examples in our dataset.We highlight a few important strategies used in the annotation protocol.a).First-order logic formulas need to preserve as much as possible the semantics of natural language sentences.b).First-order logic formulas should stay as faithful to the structure of the original NL sentence as possible.c).Semantic decomposition is not needed unless necessary for maintaining the NL expressivity.This means that "John is a bachelor" can be translated into FOL simply as "Bachelor(John)".d).In terms of abstraction, we neglect tense and remove all the plural forms of verbs.</p>
<p>G FOL Inference Engine</p>
<p>Although there are many provers widely used in the community (McCune, 2005(McCune, -2010;;Sutcliffe, 2017;Nipkow et al., 2002) , we adopt the inference engine provided in the Stanford CS221 course page5 , which is a compact module designed specifically for procesing first-order logic statements.The inference engine does not support input in the FOL syntax adopted by standard education material (Russell and Norvig, 2010), which is used in our dataset.We therefore developed a FOL parser in order to convert the FOL formulas written by humans to the input format of the inference engine.The converter is a semantic parser tool written in Python.Although LLMs such as GPT-4 can be utilized to conduct the conversion, it is hard to ensure the GPT-4 outputs are always correct.</p>
<p>Proving a story requires three steps.First, the FOL statements of the premises and conclusions of a story annotated by humans are converted to Python code.Then, the code snippets are used as input to the theorem prover.Finally, the theorem prover outputs whether the conclusions are True / False / Unknown, based on the premises.predictions are wrong for all conclusions.</p>
<p>Table 10 shows a story from HybLogic with a more complex FOL reasoning process.Inferred from premises 4 and 5, James does not perform better than others.With premises 3, 2 and 1, we know that James is not good at time management.Therefore, conclusion B is False.It cannot be determined if James exercises every week, thus the first conclusion is Unknown.The truth value of p → q is the same as ¬p ∨ q.It is not true that James does not perform better than others.It is also false that James exercises every week and is good at time management.Thus conclusion C is False.For this example, GPT-4 predicted the correct truth value only for conclusion A and RoBERTa made correct predictions for conclusions A and B.</p>
<p>J Model Performance Analysis</p>
<p>Models have more tendency to predict "True" compared with "False" or "Unknown" labels Confusion matrices in Figure 4 for the fine-tuning and 8-shot NL prompt results both show that LLMs are significantly better at making the correct predictions for conclusions with labels of True than the conclusions with labels of False or Unknown.The accuracy on examples with False or Unknown conclusions is 61.9% with fine-tuning and 54.0% with few-shot prompting.They also tend to make more predictions of True than the other labels.</p>
<p>Model performance is not affected by the premise ordering To test if the premise ordering in FOLIO has spurious correlations with the conclusion label which a model can exploit, we shuffle the input premises to evaluate models.We find that accuracy increases or decreases by roughly 1% in most settings compared to our unshuffled premises.This indicates that the ordering of premises in FO-LIO examples does not yield significant information about the label, and thus models will not be able to use the premise ordering as a strong heuris- tic or statistical feature for its predictions.</p>
<p>Using both NL sentences and FOL formulas in the prompt performs better FOL formulas have a clearer and more straightforward logical structure than NL sentences.Therefore, we test GPT-3.5 and GPT-4 with another two settings for truth value prediction using few-shot prompting: 1) using only FOL formulas in the prompt; 2) using both NL sentences and FOL formulas by concatenating each NL sentence and its annotated FOL statement.As shown in Table 11, the performance slightly increases in the NL+FOL setting for GPT-4 while GPT-3.5 performs worse in both the NL+FOL and the FOL-only settings.In other words, FOL always serves as additional useful information for GPT-4, but not for GPT-3.5 regardless of whether FOL is concatenated with NL.This observation resonates with the finding that GPT-4 performs much better than GPT-3.5 on code-related tasks (Ni et al., 2023).</p>
<p>Figure 1 :
1
Figure 1: Distribution of reasoning depths</p>
<p>Figure 1 demonstrates the distribution of the number of examples in the WikiLogic and HybLogic sets versus the number of premises needed to arrive at a conclusion, showing that most of the conclusions from WikiLogic require one to five premises while those from HybLogic require five to eight premises.</p>
<p>Figure</p>
<p>Figure Accuracies of different models categorized into examples with different reasoning depths.</p>
<p>P. All S are M. Either S or A. All A are B. All D are B. No C are B. a is either a C or a P.</p>
<p>Figure 4 :
4
Figure 4: Confusion matrices for the results of finetuning RoBERTa-Large and few-shot prompting GPT-4.</p>
<p>Table 1 :
1
Comparison of FOLIO with other datasets related to logical reasoning.#Distinct AST stands for the number of distinct abstract syntax trees, representing the number of distinct sentence-level logic structures in the corpus.FOLIO is the first expert-written dataset for FOL reasoning equipped with parallel FOL formulas.The examples are mostly aligned with real-world knowledge and use highly natural wordings.It also has a greater variety than the previous datasets in terms of reasoning depths with a larger number of distinct logic patterns and a large vocabulary.
DatasetSizeReasoningText SourceReal-World Resources# Reasoning DepthVocab# Distinct ASTCLUTTER (2019)6k InductiveSynthetic××-×RECLOR (2020)6k Mixed forms GMAT, LSAT exams✓×-×LogiQA (2021)8.6k Mixed forms NCSE exams✓×-×RuleTaker (2020)500k DeductiveSynthetic×0 ∼ 510148ProofWriter (2021) 500k DeductiveSynthetic×0 ∼ 510148LogicNLI (2021)20k FOLSynthetic×1 ∼ 5107730BigBench (2022)1300 Mixed forms Human-WrittenPartially×--ProntoQA (2023)200 DeductiveSynthetic✓1, 3, 5--FOLIO (ours)1,435 FOLExpert-written✓0 ∼ 7435176</p>
<p>Table 3 shows that examples based on Wikipedia make up the largest portion of FOLIO, with 304 stories, 1,353 NL and FOL premise pairs, and 753 NL and FOL conclusion pairs.Hybrid annotations consist of 183 stories with 1,054 NL and FOL premise pairs, and 682 NL and FOL conclusion pairs in total.</p>
<p>Table 3 :
3
Table 3 shows that our dataset has a vocabulary of 4,351 words, and the examples based on Wikipedia account for 74% of the total vocabulary even though the WikiLogic stories take up only 63% of the total number of stories.The vocabulary of FOLIO is also significantly Statistics based on different data collection methods of FOLIO.#Words is the average number of words per NL sentence.
Source#Stories #Premises #ConclusionsNLLogicVocab #Words Complexity #Depth ASTWikiLogic304135375332508.500 -14 grade1 -551HybLogic1831054682190211.520 -14 grade5 -825Total4872407143543519.860 -14 grade765-8</p>
<p>Table 4 :
4
Logical reasoning results of fully supervised fine-tuning and few-shot prompting on FOLIO test set.
ModelSizeAcc (%)majority baseline-38.5%random probability-33.3 %Fully supervised fine-tuneBERT-base110M56.8BERT-large340M59.0RoBERTa-base110M56.8RoBERTa-large340M62.1Flan-T5-Large783M65.90-shot NL PromptGPT-3.5-Turbo-53.1GPT-4-61.38-shot NL PromptLLama-13B13B33.6LLama-70B70B44.0LLama-70B -CoT70B47.8LLama-70B -ToT70B48.4text-davinci-002-49.5GPT-3.5-Turbo-58.3GPT-4-64.2GPT-4 -CoT (2022b)-68.9GPT-4 -CoT with SC (2023) -69.5GPT-4 ToT (2023)-70.0LR-specific MethodsLogic-LM (2023)-78.1LINC (2023)-73.1DetermLR (2023)-77.5</p>
<p>Table 5 :
5
NL-FOL translation results on FOLIO.SynV measures syntactic validity and ExcAcc measures the inference engine execution accuracy.
ModelZero-ShotFew-ShotSynv ExcAcc Sync ExcAccGPT-3.5-Turbo 68.450.4 93.3 56.0GPT-486.151.7 93.9 63.8</p>
<p>Table 6 :
6
Performance differences on the WikiLogic and HybLogic subset of FOLIO.WikiLogic has more diverse logical structures while HybLogic stories have higher reasoning depths.
MethodModelWikiHybFine-tuningRoBERTa-large 60.71 63.48NL PromptingGPT-3.5-Turbo68.88 47.70GPT-475.43 53.10NL-FOL ExcAcc GPT-3.5-Turbo45.17 61.82GPT-459.12 67.93</p>
<p>Table 7 :
7
Human evaluation on GPT-4 model outputs with incorrect truth value predictions Example Premises 1.All children are human.2. If someone is underage, then they are a child.3.People are either underage or of age. 4. If someone is of age, then they can vote.5.If someone is of age, they can legally get married.6.If Jack is a child and a human, then Jack is neither able to vote nor able to get married.Conclusion -&gt; Label: Jack is able to vote and get married.-&gt; True.</p>
<p>Table 11 :
11
Comparison of the results across different input formats with few-shot prompting.NL, NL-FOL, FOL, NL + FOL stands for NL prompting, execution accuracy of NL-FOL translation, using only FOL in the prompt and using concatenated NL and FOL in the prompt respectively.
ModelNLNL-FOL FOL NL+FOLGPT-3.5 58.3455.9657.9257.75GPT-464.1663.8264.0165.21
In experimenting with different prompts, we found 8 shot examples to perform slightly better. It is also the maximum number of examples that fits in the text-davinci-002 context.
Hereafter, "GPT-3.5" refers to GPT-3.5-Turbo.
By "near-native" we mean with English speaking and understanding ability that closely mirrors that of a native English speakers.
https://stanford-cs221.github.io/spring2022/ assignments/logic/index.html
H Distribution of ReadabilityWe show the distribution of readability in Figure3.I Case studyTable9shows a story from WikiLogic along with the GPT-4 and RoBERTa-Large predictions.Conclusion A is True given premises 5 and 3. From the premises, it cannot be determined if Cerura vinula has thin antennae or if it is a pest.Thus conclusions B and C are Unknown.GPT-4 predictions are correct for conclusions A and C while RoBERTa
HHUplexity at text complexity DE challenge 2022. David Arps, Jan Kels, Florian Krämer, Yunus Renz, Regina Stodden, Wiebke Petersen, Proceedings of the GermEval 2022 Workshop on Text Complexity Assessment of German Text. the GermEval 2022 Workshop on Text Complexity Assessment of German TextPotsdam, GermanyAssociation for Computational Linguistics2022</p>
<p>Sid Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He, Connor Leahy, Kyle Mcdonell, Jason Phang, Michael Pieler, Shivanshu Usvsn Sai Prashanth, Laria Purohit, Jonathan Reynolds, Ben Tow, Samuel Wang, Weinbach, 10.48550/ARXIV.2204.06745Gpt-neox-20b: An open-source autoregressive language model. 2022arXiv preprint</p>
<p>A large annotated corpus for learning natural language inference. R Samuel, Gabor Bowman, Christopher Angeli, Christopher D Potts, Manning, 10.18653/v1/D15-1075Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. the 2015 Conference on Empirical Methods in Natural Language ProcessingLisbon, PortugalAssociation for Computational Linguistics2015</p>
<p>Alec Radford, Ilya Sutskever, and Dario Amodei. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Advances in Neural Information Processing Systems. Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlishCurran Associates, Inc202033Language models are few-shot learners</p>
<p>Hy-bridQA: A dataset of multi-hop question answering over tabular and textual data. Wenhu Chen, Hanwen Zha, Zhiyu Chen, Wenhan Xiong, Hong Wang, William Yang, Wang , 10.18653/v1/2020.findings-emnlp.91Findings of the Association for Computational Linguistics: EMNLP 2020. Online. Association for Computational Linguistics2020</p>
<p>Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, arXiv:2204.02311Palm: Scaling language modeling with pathways. 2022arXiv preprint</p>
<p>Peter Clark, Oyvind Tafjord, Kyle Richardson, CoRR, abs/2002.05867Transformers as soft reasoners over language. 2020</p>
<p>Transformers as soft reasoners over language. Peter Clark, Oyvind Tafjord, Kyle Richardson, Proceedings of the Twenty-Ninth International Conference on International Joint Conferences on Artificial Intelligence. the Twenty-Ninth International Conference on International Joint Conferences on Artificial Intelligence2021</p>
<p>Using the framework. Robin Cooper, Dick Crouch, Jan Van Eijck, Chris Fox, Johan Van Genabith, Jan Jaspars, Hans Kamp, David Milward, Manfred Pinkal, Massimo Poesio, LRE 62-051 D-16The FraCaS Consortium. 1996Technical Report</p>
<p>Selection-inference: Exploiting large language models for interpretable logical reasoning. Antonia Creswell, Murray Shanahan, Irina Higgins, arXiv:2205.097122022arXiv preprint</p>
<p>A formula for predicting readability. Edgar Dale, Jeanne S Chall, Educational Research Bulletin. 2711948</p>
<p>Readability Revisited: The New Dale-Chall Readability Formula. Edgar Dale, Jeanne S Chall, 1995Brookline Books</p>
<p>Ishita Dasgupta, Stephanie Cy Andrew K Lampinen, Antonia Chan, Dharshan Creswell, James L Kumaran, Felix Mcclelland, Hill, arXiv:2207.07051Language models show human-like content effects on reasoning. 2022arXiv preprint</p>
<p>105The Logical Form of Action Sentences. Donald Davidson, 10.1093/0199246270.003.0006Essays on Actions and Events. Oxford University Press2001</p>
<p>BERT: Pre-training of deep bidirectional transformers for language understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, 10.18653/v1/N19-1423Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Long and Short Papers. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMinneapolis, MinnesotaAssociation for Computational Linguistics20191</p>
<p>David Dohan, Winnie Xu, Aitor Lewkowycz, Jacob Austin, David Bieber, Raphael Gontijo Lopes, Yuhuai Wu, Henryk Michalewski, 10.48550/ARXIV.2207.10342Language model cascades. Rif A. Saurous; Kevin Murphy, and Charles Sutton2022arXiv preprint</p>
<p>WinoLogic: A zero-shot logic-based diagnostic dataset for Winograd Schema Challenge. Weinan He, Canming Huang, Yongmei Liu, Xiaodan Zhu, 10.18653/v1/2021.emnlp-main.307Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. the 2021 Conference on Empirical Methods in Natural Language ProcessingDominican Republic. Association for Computational Linguistics2021Online and Punta Cana</p>
<p>Evaluating the rationale understanding of critical reasoning in logical reading comprehension. Akira Kawabata, Saku Sugawara, arXiv:2311.183532023Preprint</p>
<p>Boardgameqa: A dataset for natural language reasoning with contradictory information. Mehran Kazemi, Quan Yuan, Deepti Bhatia, Najoung Kim, arXiv:2306.079342023PreprintXin Xu, Vaiva Imbrasaite, and Deepak Ramachandran</p>
<p>Large language models are zero-shot reasoners. Takeshi Kojima, Shane Shixiang, Machel Gu, Yutaka Reid, Yusuke Matsuo, Iwasawa, arXiv:2205.119162022arXiv preprint</p>
<p>Two sets of perfect syllogisms. Anne Lehman, 10.1305/ndjfl/1093891016Notre Dame Journal of Formal Logic. 1431973</p>
<p>Generics: Cognition and Acquisition. Sarah-Jane Leslie, 10.1215/00318108-2007-023The Philosophical Review. 11712008</p>
<p>Yifei Li, Zeqi Lin, Shizhuo Zhang, Qiang Fu, Bei Chen, Jian-Guang Lou, Weizhu Chen, arXiv:2206.02336On the advance of making language models better reasoners. 2022arXiv preprint</p>
<p>Logiqa: a challenge dataset for machine reading comprehension with logical reasoning. Jian Liu, Leyang Cui, Hanmeng Liu, Dandan Huang, Yile Wang, Yue Zhang, Proceedings of the Twenty-Ninth International Conference on International Joint Conferences on Artificial Intelligence. the Twenty-Ninth International Conference on International Joint Conferences on Artificial Intelligence2021</p>
<p>Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov, arXiv:1907.11692Ro{bert}a: A robustly optimized {bert} pretraining approach. 2020arXiv preprint</p>
<p>. W Mccune, 2005-2010. Prover9 and mace4</p>
<p>Dragomir Radev, and Arman Cohan. 2023. L2ceval: Evaluating language-to-code generation capabilities of large language models. Ansong Ni, Pengcheng Yin, Yilun Zhao, Martin Riddell, Troy Feng, Rui Shen, Stephen Yin, Ye Liu, Semih Yavuz, Caiming Xiong, Shafiq Joty, Yingbo Zhou, arXiv:2309.17446Preprint</p>
<p>Isabelle/Hol a Proof Assistant for Higher-Order Logic. Tobias Nipkow, Lawrence C Paulson, Markus Wenzel, 2002Springer</p>
<p>LINC: A neurosymbolic approach for logical reasoning by combining language models with first-order logic provers. Theo Olausson, Alex Gu, Ben Lipkin, Cedegao Zhang, Armando Solar-Lezama, Joshua Tenenbaum, Roger Levy, 10.18653/v1/2023.emnlp-main.313Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language ProcessingSingaporeAssociation for Computational Linguistics2023</p>
<p>Josh Openai, Others Achiam, arXiv:2303.08774Gpt-4 technical report. 2023Preprint</p>
<p>Logic-LM: Empowering large language models with symbolic solvers for faithful logical reasoning. Liangming Pan, Alon Albalak, Xinyi Wang, William Wang, 10.18653/v1/2023.findings-emnlp.248Findings of the Association for Computational Linguistics: EMNLP 2023. SingaporeAssociation for Computational Linguistics2023</p>
<p>Events in the Semantics of English. Terence Parsons, 1990MIT PressCambridge, MA, USA</p>
<p>Stuart Russell, Peter Norvig, Artificial Intelligence: A Modern Approach. Prentice Hall20103 edition</p>
<p>RuleBERT: Teaching soft rules to pre-trained language models. Mohammed Saeed, Naser Ahmadi, Preslav Nakov, Paolo Papotti, 10.18653/v1/2021.emnlp-main.110Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. the 2021 Conference on Empirical Methods in Natural Language ProcessingDominican Republic. Association for Computational Linguistics2021Online and Punta Cana</p>
<p>Language models can (kind of) reason: A systematic formal analysis of chain-of-thought. Abulhair Saparov, He He, International Conference on Learning Representations. 2023</p>
<p>Exploring neural models for parsing natural language into first-order logic. Hrituraj Singh, Milan Aggrawal, Balaji Krishnamurthy, arXiv:2002.065442020arXiv preprint</p>
<p>Misery loves complexity: Exploring linguistic complexity in the context of emotion detection. Pranaydeep Singh, Luna De Bruyne, Orphée De Clercq, Els Lefever, 10.18653/v1/2023.findings-emnlp.857Findings of the Association for Computational Linguistics: EMNLP 2023. SingaporeAssociation for Computational Linguistics2023</p>
<p>CLUTRR: A diagnostic benchmark for inductive reasoning from text. Koustuv Sinha, Shagun Sodhani, Jin Dong, Joelle Pineau, William L Hamilton, 10.18653/v1/D19-1458Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)Hong Kong, ChinaAssociation for Computational Linguistics2019</p>
<p>Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. Aarohi Srivastava, +447 AuthorsAbhinav Rastogi, +447 AuthorsarXiv:2206.046152023Preprint</p>
<p>Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal, Md Shoeb, Abubakar Abid, Adam Fisch, Adam Adam R Brown, Aditya Santoro, Adrià Gupta, Garriga-Alonso, arXiv:2206.04615Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. 2022arXiv preprint</p>
<p>From indeterminacy to determinacy: Augmenting logical reasoning capabilities with large language models. Hongda Sun, Weikai Xu, Wei Liu, Jian Luan, Bin Wang, Shuo Shang, Ji-Rong Wen, Rui Yan, arXiv:2310.186592023Preprint</p>
<p>G Sutcliffe, The TPTP Problem Library and Associated Infrastructure. From CNF to TH0, TPTP. 201759v6.4.0.</p>
<p>ProofWriter: Generating implications, proofs, and abductive statements over natural language. Oyvind Tafjord, Bhavana Dalvi, Peter Clark, 10.18653/v1/2021.findings-acl.317Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021. Online. Association for Computational Linguistics2021</p>
<p>CommonsenseQA: A question answering challenge targeting commonsense knowledge. Alon Talmor, Jonathan Herzig, Nicholas Lourie, Jonathan Berant, 10.18653/v1/N19-1421Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Long and Short Papers. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMinneapolis, MinnesotaAssociation for Computational Linguistics20191</p>
<p>Leap-of-thought: Teaching pre-trained models to systematically reason over implicit knowledge. Alon Talmor, Oyvind Tafjord, Peter Clark, Yoav Goldberg, Jonathan Berant, Advances in Neural Information Processing Systems. 202033</p>
<p>Diagnosing the firstorder logical reasoning ability through LogicNLI. Jidong Tian, Yitian Li, Wenqing Chen, Liqiang Xiao, Hao He, Yaohui Jin, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. the 2021 Conference on Empirical Methods in Natural Language ProcessingDominican Republic. Association for Computational Linguistics2021Online and Punta Cana</p>
<p>Edouard Grave, and Guillaume Lample. 2023. Llama: Open and efficient foundation language models. Thibaut Hugo Touvron, Gautier Lavril, Xavier Izacard, Marie-Anne Martinet, Timothée Lachaux, Baptiste Lacroix, Naman Rozière, Eric Goyal, Faisal Hambro, Aurelien Azhar, Armand Rodriguez, Joulin, arXiv:2302.13971Preprint</p>
<p>Superglue: A stickier benchmark for general-purpose language understanding systems. Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, Samuel Bowman, Advances in Neural Information Processing Systems. Curran Associates, Inc2019a32</p>
<p>Superglue: A stickier benchmark for general-purpose language understanding systems. Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, Samuel Bowman, Advances in neural information processing systems. 2019b32</p>
<p>Self-consistency improves chain of thought reasoning in language models. Xuezhi Wang, Jason Wei, Dale Schuurmans, Ed H Quoc V Le, Sharan Chi, Aakanksha Narang, Denny Chowdhery, Zhou, The Eleventh International Conference on Learning Representations. 2023</p>
<p>Linguistic complexity loss in text-based therapy. Jason Wei, Kelly Finn, Emma Templeton, Thalia Wheatley, Soroush Vosoughi, 10.18653/v1/2021.naacl-main.352Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesOnline. Association for Computational Linguistics2021</p>
<p>Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, arXiv:2206.07682Emergent abilities of large language models. 2022aarXiv preprint</p>
<p>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, Denny Zhou, arXiv:2201.11903Chain of thought prompting elicits reasoning in large language models. 2022barXiv preprint</p>
<p>HotpotQA: A dataset for diverse, explainable multi-hop question answering. Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, Christopher D Manning, 10.18653/v1/D18-1259Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. the 2018 Conference on Empirical Methods in Natural Language ProcessingBrussels, BelgiumAssociation for Computational Linguistics2018</p>
<p>Tree of thoughts: Deliberate problem solving with large language models. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L Griffiths, Yuan Cao, Karthik R Narasimhan, Thirty-seventh Conference on Neural Information Processing Systems. 2023</p>
<p>Reclor: A reading comprehension dataset requiring logical reasoning. Weihao Yu, Zihang Jiang, Yanfei Dong, Jiashi Feng, International Conference on Learning Representations. 2020</p>
<p>Eric Zelikman, Yuhuai Wu, Jesse Mu, Noah D Goodman, 10.48550/ARXIV.2203.14465Star: Bootstrapping reasoning with reasoning. 2022arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
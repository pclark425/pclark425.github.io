<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6503 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6503</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6503</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-129.html">extraction-schema-129</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <p><strong>Paper ID:</strong> paper-273233520</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2410.08068v1.pdf" target="_blank">Teaching-Inspired Integrated Prompting Framework: A Novel Approach for Enhancing Reasoning in Large Language Models</a></p>
                <p><strong>Paper Abstract:</strong> Large Language Models (LLMs) exhibit impressive performance across various domains but still struggle with arithmetic reasoning tasks. Recent work shows the effectiveness of prompt design methods in enhancing reasoning capabilities. However, these approaches overlook crucial requirements for prior knowledge of specific concepts, theorems, and tricks to tackle most arithmetic reasoning problems successfully. To address this issue, we propose a novel and effective Teaching-Inspired Integrated Framework, which emulates the instructional process of a teacher guiding students. This method equips LLMs with essential concepts, relevant theorems, and similar problems with analogous solution approaches, facilitating the enhancement of reasoning abilities. Additionally, we introduce two new Chinese datasets, MathMC and MathToF, both with detailed explanations and answers. Experiments are conducted on nine benchmarks which demonstrates that our approach improves the reasoning accuracy of LLMs. With GPT-4 and our framework, we achieve new state-of-the-art performance on four math benchmarks (AddSub, SVAMP, Math23K and AQuA) with accuracies of 98.2% (+3.3%), 93.9% (+0.2%), 94.3% (+7.2%) and 81.1% (+1.2%). Our data and code are available at https://github.com/SallyTan13/Teaching-Inspired-Prompting.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6503.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6503.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TII-Framework</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Teaching-Inspired Integrated Prompting Framework</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting framework that imitates teacher guidance by retrieving background knowledge and similar problems, combining few-shot Chain-of-Thought and Program-of-Thought exemplars, generating Python programs for computation, sampling multiple reasoning paths (self-consistency), and performing double-checking and answer-selection (including English–Chinese ensembling).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4; GPT-3.5-Turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Teaching-Inspired Integrated Prompting (retrieval + CoT + PoT + self-consistency + program verification + answer selection)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>retrieval-based + ensemble + tool-augmented + sequential</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>mixed</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>AddSub, SVAMP, Math23K, AQuA, GSM8K, SingleEQ, MultiArith, MathMC, MathToF</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Grade-school and middle-school mathematical/arithmetic reasoning (word problems, multiple-choice, true/false).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>98.2</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>Chain-of-Thought (CoT) baseline</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td>3.3</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>The framework augments standard prompting by adding retrieved background/theorems and similar worked examples, program-based verification, and multi-path sampling; authors report consistent gains across nine math benchmarks and SOTA on four benchmarks with GPT-4. The approach explicitly mixes diverse reasoning paths (self-consistency sampling) with targeted, similar-example guidance (which can be homogeneous if the same exemplar is repeated).</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Teaching-Inspired Integrated Prompting Framework: A Novel Approach for Enhancing Reasoning in Large Language Models', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6503.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6503.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TII-GPT4-AddSub</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Teaching-Inspired Integrated Prompting on GPT-4 evaluated on AddSub</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Application of the integrated teaching-inspired prompting on GPT-4 for the AddSub arithmetic benchmark, achieving state-of-the-art accuracy reported in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Teaching-Inspired Integrated Prompting (inc. retrieval, CoT, PoT, self-consistency, Python double-check)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>retrieval-based + ensemble + tool-augmented + sequential</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>mixed</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>AddSub</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Simple arithmetic word problems (addition/subtraction problems).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>98.2</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>previous baseline / Chain-of-Thought</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td>3.3</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>Authors report TII framework with GPT-4 achieves new SOTA on AddSub (98.2%, +3.3). Improvement attributed to background-theorem retrieval and similar-problem exemplars guiding correct formulation.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Teaching-Inspired Integrated Prompting Framework: A Novel Approach for Enhancing Reasoning in Large Language Models', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6503.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6503.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TII-GPT4-Math23K</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Teaching-Inspired Integrated Prompting on GPT-4 evaluated on Math23K</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The integrated prompting method applied to GPT-4 on the Chinese Math23K benchmark, yielding a large improvement reported in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Teaching-Inspired Integrated Prompting (retrieval + CoT + PoT + verification)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>retrieval-based + ensemble + tool-augmented</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>mixed</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Math23K</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Chinese grade-school arithmetic and algebra word problems.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>94.3</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>previous baseline / Chain-of-Thought</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td>7.2</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>Paper reports a large gain on Math23K (94.3%, +7.2% relative to prior result). Authors emphasize that retrieval of relevant theorems/background and similar worked examples is especially helpful for datasets where domain-specific knowledge or tactics are needed.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Teaching-Inspired Integrated Prompting Framework: A Novel Approach for Enhancing Reasoning in Large Language Models', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6503.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6503.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TII-GPT3.5-GSM8K</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Teaching-Inspired Integrated Prompting on GPT-3.5-Turbo evaluated on GSM8K</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Applying the integrated prompting framework on GPT-3.5-Turbo for the GSM8K benchmark produced significant improvement over Chain-of-Thought baselines according to the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5-Turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Teaching-Inspired Integrated Prompting (retrieval + CoT + PoT + self-consistency + python-check)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>retrieval-based + ensemble + tool-augmented</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>mixed</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>GSM8K</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Grade-school math word problems requiring multi-step arithmetic reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>Chain-of-Thought (CoT)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td>8.8</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>On GPT-3.5-Turbo authors report an 8.8% improvement on GSM8K. Ablation shows program-based verification and double-checking are important for more complex datasets to mitigate computational errors (removal caused a 9.1% drop on GSM8K).</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Teaching-Inspired Integrated Prompting Framework: A Novel Approach for Enhancing Reasoning in Large Language Models', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6503.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e6503.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-Consistency</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Consistency (sampling multiple reasoning paths and voting)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An ensembling decoding strategy that samples multiple chain-of-thought reasoning paths from an LLM and selects the final answer by majority/consensus across sampled paths to improve robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4; GPT-3.5-Turbo (as used within framework)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Self-Consistency</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>ensemble</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>diverse</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>multiple math benchmarks (used inside framework)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Multi-step arithmetic reasoning where multiple sampled chains may produce different answers.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy (via majority voting)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>greedy CoT decoding</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>The paper uses self-consistency to generate N different reasoning/programming paths; authors treat these as diverse reasoning styles and then apply consistency checks and majority voting. They report self-consistency is a core component of their answer-generation stage.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Teaching-Inspired Integrated Prompting Framework: A Novel Approach for Enhancing Reasoning in Large Language Models', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6503.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e6503.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PoT+Python-Verification</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Program-of-Thought / Python program generation with double-check verification</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A tool-augmented approach where the LLM generates Python programs to compute arithmetic results; outputs from programs are compared with step-by-step natural-language solutions to double-check and guide final selection.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4; GPT-3.5-Turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Program-of-Thought / Python verification</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>tool-augmented + ensemble</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>mixed</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>GSM8K, Math23K and other math benchmarks</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Numeric computation heavy multi-step math problems where exact calculation matters.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>no-program baseline (pure CoT without code verification)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>Ablation results show removing Python program generation and double-checking had small/no effect on simple benchmarks but caused notable accuracy drops on complex datasets (reported −9.1% on GSM8K and −4.1% on Math23K), indicating program-based verification reduces computation errors and improves reliability.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Teaching-Inspired Integrated Prompting Framework: A Novel Approach for Enhancing Reasoning in Large Language Models', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6503.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e6503.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Similar-Problem Retrieval & Repetition</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Retrieval of similar problems and optional repetition of identical exemplars inside prompts</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A retrieval-based component that finds similar solved problems (excluding trivial numeric variants) and includes their analyses in the prompt; experiment shows that both the similarity of retrieved problems and repeating the same top-similar problem K times affect performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5-Turbo (experiments reported primarily on this model)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>retrieval-augmented exemplars (BM25 + LCS ranking), repeated exemplars</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>retrieval-based + sequential</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>either diverse or homogeneous depending on selections (paper describes both effects)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>nine math benchmarks (AddSub, SingleEQ, SVAMP, MultiArith, GSM8K, AQuA, Math23K, MathMC, MathToF)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Arithmetic reasoning problems where worked examples can transfer solution strategy.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>no-similar-problem baseline</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>Authors report removing similar problems and background knowledge reduces accuracy across datasets. They find that adding more similar examples helps only within a similarity threshold; adding dissimilar examples can hurt. Repeating the same top-similar example multiple times (homogeneous repetition) improved accuracy—an empirical observation about how pump priming prompt style similarity vs diversity influences performance.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Teaching-Inspired Integrated Prompting Framework: A Novel Approach for Enhancing Reasoning in Large Language Models', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6503.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e6503.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Chain-of-Thought (CoT) baseline</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Thought prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Few-shot prompting with step-by-step intermediate reasoning (CoT) used as a baseline for multi-step reasoning; the paper compares its integrated framework against this approach.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4; GPT-3.5-Turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Chain-of-Thought (CoT)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>sequential</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>single style (unless combined with other strategies)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>multiple math benchmarks (AddSub, SVAMP, GSM8K, Math23K, AQuA, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Multi-step arithmetic reasoning using few-shot exemplars that include intermediate steps.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>Teaching-Inspired Integrated Prompting</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>The paper directly compares the proposed integrated framework to CoT; reported improvements (e.g., GPT-3.5-Turbo: GSM8K +8.8%, Math23K +24.8%, SingleEQ +8.0%, AQuA +10.2%) indicate that adding retrieval of background/theorems, similar worked examples, program verification, and ensemble selection gives large gains over CoT alone.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Teaching-Inspired Integrated Prompting Framework: A Novel Approach for Enhancing Reasoning in Large Language Models', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6503.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e6503.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>English-Chinese Ensembling</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>English–Chinese Ensembling (translate-then-solve for Chinese problems)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A strategy that, for Chinese problems, asks the LLM in additional solution requests to translate the problem, background, and similar problems into English and re-solve, then ensembles results across languages.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4; GPT-3.5-Turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>language-ensembling (translate-and-solve)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>ensemble / ensembling</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>mixed</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>MathMC, MathToF, Math23K (Chinese datasets)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Chinese arithmetic problems where translation may aid understanding.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>no-translation baseline</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>Ablation shows removing English–Chinese ensembling reduces accuracy on MathMC by 3.0% and MathToF by 1.0%, suggesting that translating to English can help LLMs better interpret some Chinese expressions and produce more accurate solutions.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Teaching-Inspired Integrated Prompting Framework: A Novel Approach for Enhancing Reasoning in Large Language Models', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Chain-of-thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Self-consistency improves chain of thought reasoning in language models <em>(Rating: 2)</em></li>
                <li>Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks <em>(Rating: 2)</em></li>
                <li>MathPrompter: Mathematical reasoning using large language models <em>(Rating: 1)</em></li>
                <li>Least-to-most prompting enables complex reasoning in large language models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6503",
    "paper_id": "paper-273233520",
    "extraction_schema_id": "extraction-schema-129",
    "extracted_data": [
        {
            "name_short": "TII-Framework",
            "name_full": "Teaching-Inspired Integrated Prompting Framework",
            "brief_description": "A prompting framework that imitates teacher guidance by retrieving background knowledge and similar problems, combining few-shot Chain-of-Thought and Program-of-Thought exemplars, generating Python programs for computation, sampling multiple reasoning paths (self-consistency), and performing double-checking and answer-selection (including English–Chinese ensembling).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4; GPT-3.5-Turbo",
            "model_size": null,
            "reasoning_method_name": "Teaching-Inspired Integrated Prompting (retrieval + CoT + PoT + self-consistency + program verification + answer selection)",
            "reasoning_method_type": "retrieval-based + ensemble + tool-augmented + sequential",
            "reasoning_style_diversity": "mixed",
            "benchmark_name": "AddSub, SVAMP, Math23K, AQuA, GSM8K, SingleEQ, MultiArith, MathMC, MathToF",
            "task_description": "Grade-school and middle-school mathematical/arithmetic reasoning (word problems, multiple-choice, true/false).",
            "performance_metric": "accuracy",
            "performance_value": 98.2,
            "comparison_target_method": "Chain-of-Thought (CoT) baseline",
            "performance_difference": 3.3,
            "statistical_significance": null,
            "analysis_notes": "The framework augments standard prompting by adding retrieved background/theorems and similar worked examples, program-based verification, and multi-path sampling; authors report consistent gains across nine math benchmarks and SOTA on four benchmarks with GPT-4. The approach explicitly mixes diverse reasoning paths (self-consistency sampling) with targeted, similar-example guidance (which can be homogeneous if the same exemplar is repeated).",
            "ablation_study_present": true,
            "uuid": "e6503.0",
            "source_info": {
                "paper_title": "Teaching-Inspired Integrated Prompting Framework: A Novel Approach for Enhancing Reasoning in Large Language Models",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "TII-GPT4-AddSub",
            "name_full": "Teaching-Inspired Integrated Prompting on GPT-4 evaluated on AddSub",
            "brief_description": "Application of the integrated teaching-inspired prompting on GPT-4 for the AddSub arithmetic benchmark, achieving state-of-the-art accuracy reported in the paper.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_size": null,
            "reasoning_method_name": "Teaching-Inspired Integrated Prompting (inc. retrieval, CoT, PoT, self-consistency, Python double-check)",
            "reasoning_method_type": "retrieval-based + ensemble + tool-augmented + sequential",
            "reasoning_style_diversity": "mixed",
            "benchmark_name": "AddSub",
            "task_description": "Simple arithmetic word problems (addition/subtraction problems).",
            "performance_metric": "accuracy",
            "performance_value": 98.2,
            "comparison_target_method": "previous baseline / Chain-of-Thought",
            "performance_difference": 3.3,
            "statistical_significance": null,
            "analysis_notes": "Authors report TII framework with GPT-4 achieves new SOTA on AddSub (98.2%, +3.3). Improvement attributed to background-theorem retrieval and similar-problem exemplars guiding correct formulation.",
            "ablation_study_present": true,
            "uuid": "e6503.1",
            "source_info": {
                "paper_title": "Teaching-Inspired Integrated Prompting Framework: A Novel Approach for Enhancing Reasoning in Large Language Models",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "TII-GPT4-Math23K",
            "name_full": "Teaching-Inspired Integrated Prompting on GPT-4 evaluated on Math23K",
            "brief_description": "The integrated prompting method applied to GPT-4 on the Chinese Math23K benchmark, yielding a large improvement reported in the paper.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_size": null,
            "reasoning_method_name": "Teaching-Inspired Integrated Prompting (retrieval + CoT + PoT + verification)",
            "reasoning_method_type": "retrieval-based + ensemble + tool-augmented",
            "reasoning_style_diversity": "mixed",
            "benchmark_name": "Math23K",
            "task_description": "Chinese grade-school arithmetic and algebra word problems.",
            "performance_metric": "accuracy",
            "performance_value": 94.3,
            "comparison_target_method": "previous baseline / Chain-of-Thought",
            "performance_difference": 7.2,
            "statistical_significance": null,
            "analysis_notes": "Paper reports a large gain on Math23K (94.3%, +7.2% relative to prior result). Authors emphasize that retrieval of relevant theorems/background and similar worked examples is especially helpful for datasets where domain-specific knowledge or tactics are needed.",
            "ablation_study_present": true,
            "uuid": "e6503.2",
            "source_info": {
                "paper_title": "Teaching-Inspired Integrated Prompting Framework: A Novel Approach for Enhancing Reasoning in Large Language Models",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "TII-GPT3.5-GSM8K",
            "name_full": "Teaching-Inspired Integrated Prompting on GPT-3.5-Turbo evaluated on GSM8K",
            "brief_description": "Applying the integrated prompting framework on GPT-3.5-Turbo for the GSM8K benchmark produced significant improvement over Chain-of-Thought baselines according to the paper.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3.5-Turbo",
            "model_size": null,
            "reasoning_method_name": "Teaching-Inspired Integrated Prompting (retrieval + CoT + PoT + self-consistency + python-check)",
            "reasoning_method_type": "retrieval-based + ensemble + tool-augmented",
            "reasoning_style_diversity": "mixed",
            "benchmark_name": "GSM8K",
            "task_description": "Grade-school math word problems requiring multi-step arithmetic reasoning.",
            "performance_metric": "accuracy",
            "performance_value": null,
            "comparison_target_method": "Chain-of-Thought (CoT)",
            "performance_difference": 8.8,
            "statistical_significance": null,
            "analysis_notes": "On GPT-3.5-Turbo authors report an 8.8% improvement on GSM8K. Ablation shows program-based verification and double-checking are important for more complex datasets to mitigate computational errors (removal caused a 9.1% drop on GSM8K).",
            "ablation_study_present": true,
            "uuid": "e6503.3",
            "source_info": {
                "paper_title": "Teaching-Inspired Integrated Prompting Framework: A Novel Approach for Enhancing Reasoning in Large Language Models",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Self-Consistency",
            "name_full": "Self-Consistency (sampling multiple reasoning paths and voting)",
            "brief_description": "An ensembling decoding strategy that samples multiple chain-of-thought reasoning paths from an LLM and selects the final answer by majority/consensus across sampled paths to improve robustness.",
            "citation_title": "mention",
            "mention_or_use": "use",
            "model_name": "GPT-4; GPT-3.5-Turbo (as used within framework)",
            "model_size": null,
            "reasoning_method_name": "Self-Consistency",
            "reasoning_method_type": "ensemble",
            "reasoning_style_diversity": "diverse",
            "benchmark_name": "multiple math benchmarks (used inside framework)",
            "task_description": "Multi-step arithmetic reasoning where multiple sampled chains may produce different answers.",
            "performance_metric": "accuracy (via majority voting)",
            "performance_value": null,
            "comparison_target_method": "greedy CoT decoding",
            "performance_difference": null,
            "statistical_significance": null,
            "analysis_notes": "The paper uses self-consistency to generate N different reasoning/programming paths; authors treat these as diverse reasoning styles and then apply consistency checks and majority voting. They report self-consistency is a core component of their answer-generation stage.",
            "ablation_study_present": true,
            "uuid": "e6503.4",
            "source_info": {
                "paper_title": "Teaching-Inspired Integrated Prompting Framework: A Novel Approach for Enhancing Reasoning in Large Language Models",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "PoT+Python-Verification",
            "name_full": "Program-of-Thought / Python program generation with double-check verification",
            "brief_description": "A tool-augmented approach where the LLM generates Python programs to compute arithmetic results; outputs from programs are compared with step-by-step natural-language solutions to double-check and guide final selection.",
            "citation_title": "mention",
            "mention_or_use": "use",
            "model_name": "GPT-4; GPT-3.5-Turbo",
            "model_size": null,
            "reasoning_method_name": "Program-of-Thought / Python verification",
            "reasoning_method_type": "tool-augmented + ensemble",
            "reasoning_style_diversity": "mixed",
            "benchmark_name": "GSM8K, Math23K and other math benchmarks",
            "task_description": "Numeric computation heavy multi-step math problems where exact calculation matters.",
            "performance_metric": "accuracy",
            "performance_value": null,
            "comparison_target_method": "no-program baseline (pure CoT without code verification)",
            "performance_difference": null,
            "statistical_significance": null,
            "analysis_notes": "Ablation results show removing Python program generation and double-checking had small/no effect on simple benchmarks but caused notable accuracy drops on complex datasets (reported −9.1% on GSM8K and −4.1% on Math23K), indicating program-based verification reduces computation errors and improves reliability.",
            "ablation_study_present": true,
            "uuid": "e6503.5",
            "source_info": {
                "paper_title": "Teaching-Inspired Integrated Prompting Framework: A Novel Approach for Enhancing Reasoning in Large Language Models",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Similar-Problem Retrieval & Repetition",
            "name_full": "Retrieval of similar problems and optional repetition of identical exemplars inside prompts",
            "brief_description": "A retrieval-based component that finds similar solved problems (excluding trivial numeric variants) and includes their analyses in the prompt; experiment shows that both the similarity of retrieved problems and repeating the same top-similar problem K times affect performance.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3.5-Turbo (experiments reported primarily on this model)",
            "model_size": null,
            "reasoning_method_name": "retrieval-augmented exemplars (BM25 + LCS ranking), repeated exemplars",
            "reasoning_method_type": "retrieval-based + sequential",
            "reasoning_style_diversity": "either diverse or homogeneous depending on selections (paper describes both effects)",
            "benchmark_name": "nine math benchmarks (AddSub, SingleEQ, SVAMP, MultiArith, GSM8K, AQuA, Math23K, MathMC, MathToF)",
            "task_description": "Arithmetic reasoning problems where worked examples can transfer solution strategy.",
            "performance_metric": "accuracy",
            "performance_value": null,
            "comparison_target_method": "no-similar-problem baseline",
            "performance_difference": null,
            "statistical_significance": null,
            "analysis_notes": "Authors report removing similar problems and background knowledge reduces accuracy across datasets. They find that adding more similar examples helps only within a similarity threshold; adding dissimilar examples can hurt. Repeating the same top-similar example multiple times (homogeneous repetition) improved accuracy—an empirical observation about how pump priming prompt style similarity vs diversity influences performance.",
            "ablation_study_present": true,
            "uuid": "e6503.6",
            "source_info": {
                "paper_title": "Teaching-Inspired Integrated Prompting Framework: A Novel Approach for Enhancing Reasoning in Large Language Models",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Chain-of-Thought (CoT) baseline",
            "name_full": "Chain-of-Thought prompting",
            "brief_description": "Few-shot prompting with step-by-step intermediate reasoning (CoT) used as a baseline for multi-step reasoning; the paper compares its integrated framework against this approach.",
            "citation_title": "mention",
            "mention_or_use": "use",
            "model_name": "GPT-4; GPT-3.5-Turbo",
            "model_size": null,
            "reasoning_method_name": "Chain-of-Thought (CoT)",
            "reasoning_method_type": "sequential",
            "reasoning_style_diversity": "single style (unless combined with other strategies)",
            "benchmark_name": "multiple math benchmarks (AddSub, SVAMP, GSM8K, Math23K, AQuA, etc.)",
            "task_description": "Multi-step arithmetic reasoning using few-shot exemplars that include intermediate steps.",
            "performance_metric": "accuracy",
            "performance_value": null,
            "comparison_target_method": "Teaching-Inspired Integrated Prompting",
            "performance_difference": null,
            "statistical_significance": null,
            "analysis_notes": "The paper directly compares the proposed integrated framework to CoT; reported improvements (e.g., GPT-3.5-Turbo: GSM8K +8.8%, Math23K +24.8%, SingleEQ +8.0%, AQuA +10.2%) indicate that adding retrieval of background/theorems, similar worked examples, program verification, and ensemble selection gives large gains over CoT alone.",
            "ablation_study_present": true,
            "uuid": "e6503.7",
            "source_info": {
                "paper_title": "Teaching-Inspired Integrated Prompting Framework: A Novel Approach for Enhancing Reasoning in Large Language Models",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "English-Chinese Ensembling",
            "name_full": "English–Chinese Ensembling (translate-then-solve for Chinese problems)",
            "brief_description": "A strategy that, for Chinese problems, asks the LLM in additional solution requests to translate the problem, background, and similar problems into English and re-solve, then ensembles results across languages.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4; GPT-3.5-Turbo",
            "model_size": null,
            "reasoning_method_name": "language-ensembling (translate-and-solve)",
            "reasoning_method_type": "ensemble / ensembling",
            "reasoning_style_diversity": "mixed",
            "benchmark_name": "MathMC, MathToF, Math23K (Chinese datasets)",
            "task_description": "Chinese arithmetic problems where translation may aid understanding.",
            "performance_metric": "accuracy",
            "performance_value": null,
            "comparison_target_method": "no-translation baseline",
            "performance_difference": null,
            "statistical_significance": null,
            "analysis_notes": "Ablation shows removing English–Chinese ensembling reduces accuracy on MathMC by 3.0% and MathToF by 1.0%, suggesting that translating to English can help LLMs better interpret some Chinese expressions and produce more accurate solutions.",
            "ablation_study_present": true,
            "uuid": "e6503.8",
            "source_info": {
                "paper_title": "Teaching-Inspired Integrated Prompting Framework: A Novel Approach for Enhancing Reasoning in Large Language Models",
                "publication_date_yy_mm": "2024-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Chain-of-thought prompting elicits reasoning in large language models",
            "rating": 2,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Self-consistency improves chain of thought reasoning in language models",
            "rating": 2,
            "sanitized_title": "selfconsistency_improves_chain_of_thought_reasoning_in_language_models"
        },
        {
            "paper_title": "Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks",
            "rating": 2,
            "sanitized_title": "program_of_thoughts_prompting_disentangling_computation_from_reasoning_for_numerical_reasoning_tasks"
        },
        {
            "paper_title": "MathPrompter: Mathematical reasoning using large language models",
            "rating": 1,
            "sanitized_title": "mathprompter_mathematical_reasoning_using_large_language_models"
        },
        {
            "paper_title": "Least-to-most prompting enables complex reasoning in large language models",
            "rating": 1,
            "sanitized_title": "leasttomost_prompting_enables_complex_reasoning_in_large_language_models"
        }
    ],
    "cost": 0.0140985,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Teaching-Inspired Integrated Prompting Framework: A Novel Approach for Enhancing Reasoning in Large Language Models
10 Oct 2024</p>
<p>Wenting Tan 
Dongxiao Chen chendx@rd.netease.com 
Zihao Wang 
Taijie Chen </p>
<p>The University of Hong Kong</p>
<p>The University of Hong Kong</p>
<p>Teaching-Inspired Integrated Prompting Framework: A Novel Approach for Enhancing Reasoning in Large Language Models
10 Oct 2024A5530C0691B014903B2D2263B7F48DE5arXiv:2410.08068v1[cs.CL]
Large Language Models (LLMs) exhibit impressive performance across various domains but still struggle with arithmetic reasoning tasks.Recent work shows the effectiveness of prompt design methods in enhancing reasoning capabilities.However, these approaches overlook crucial requirements for prior knowledge of specific concepts, theorems, and tricks to tackle most arithmetic reasoning problems successfully.To address this issue, we propose a novel and effective Teaching-Inspired Integrated Framework, which emulates the instructional process of a teacher guiding students.This method equips LLMs with essential concepts, relevant theorems, and similar problems with analogous solution approaches, facilitating the enhancement of reasoning abilities.Additionally, we introduce two new Chinese datasets, MathMC and MathToF, both with detailed explanations and answers.Experiments are conducted on nine benchmarks which demonstrates that our approach improves the reasoning accuracy of LLMs.With GPT-4 and our framework, we achieve new state-of-the-art performance on four math benchmarks (AddSub, SVAMP, Math23K and AQuA) with accuracies of 98.2% (+3.3%), 93.9% (+0.2%), 94.3% (+7.2%) and 81.1% (+1.2%).Our data and code are available at https://github.com/SallyTan13/Teaching-Inspired-Prompting.</p>
<p>Introduction</p>
<p>Large Language Models (LLMs) have made significant strides in the field of Natural Language Processing (NLP), demonstrating outstanding performance across various tasks (Devlin et al., 2018;Brown et al., 2020;Chowdhery et al., 2022).Nonetheless, handling reasoning tasks effectively remains a challenge for LLMs.Evidence suggests that simply scaling up the model size does not pro-* Work done during internship at Youdao, NetEase.</p>
<p>vide an adequate solution to this issue (Rae et al., 2021;Srivastava et al., 2022).</p>
<p>To address this issue, a series of new prompting methods are proposed to enhance reasoning in LLMs.The effectiveness of these methods is substantiated by extensive experiments.Chain-of-Thought (CoT) prompting (Wei et al., 2022), which mimics the human approach to solving multi-step problems by providing LLMs with few-shot exemplars including intermediate reasoning steps.Based on CoT, subsequent studies have further refined this method and improved the performance, such as Zero-shot-CoT (Kojima et al., 2022), Complexitybased CoT (Fu et al., 2022) and Least-to-Most Prompting (Zhou et al., 2022).Self-consistency (SC) is also a breakthrough method that replaces the greedy decoding strategy used in CoT but samples various reasoning paths and selects the answer with the highest consistency (Wang et al., 2022).From another perspective, MathPrompter (Imani et al., 2023) and Program of Thoughts (PoT) prompting (Chen et al., 2022) empower LLMs to generate programming language statements, enabling them to provide more accurate solutions for complex mathematical calculations.</p>
<p>While prompting methods mentioned above greatly improve the reasoning performance of LLMs, they miss the crucial need for a strong grasp of concepts, theorems, and strategies.Firstly, the knowledge repository of LLMs may be incomplete, lacking enough conceptual and theoretical foundation to tackle certain arithmetic reasoning problems.Secondly, lack of familiarity with specific problemsolving strategies may lead LLMs to incorrect preconditions, resulting in inaccurate final answers despite correct intermediate calculations.These challenges also arise in the process of human practice and problem-solving.Like teachers who provide foundational concepts and examples for students before practice, LLMs require educational-sourced information to ensure accurate reasoning and so-lutions.Therefore, drawing inspiration from traditional teaching methods, we propose a Teaching-Inspired Integrated Prompting Framework.This framework imitates the guidance provided by teachers to students by drawing from curated educational databases to offer clear, problem-specific concepts or theorems as background knowledge.It also presents similar problems with easy-to-learn solution ideas as examples.Additionally, it incorporates answer double-checking and selection mechanisms to enhance the overall reasoning ability of LLMs.</p>
<p>Existing arithmetic benchmarks contain a limited number of Multiple-Choice and True-or-False questions.Hence, we create two Chinese mathematical datasets called MathMC and MathToF comprising 1,000 Multiple-Choice math problems and 1,000 True-or-False math problems respectively with answers and detailed rationales.</p>
<p>Our approach is evaluated on nine benchmarks, including six English datasets, one Chinese dataset, and two datasets we created.These experiments are conducted on GPT-3.5-Turbo(Ouyang et al., 2022) and GPT-4 (OpenAI, 2023) respectively.Experimental results demonstrate that the reasoning performances of both language models on nine benchmarks are improved.</p>
<p>Our main contributions are as follows:</p>
<p>• A novel teaching-inspired integrated prompting framework is proposed to improve the reasoning capabilities of LLMs.</p>
<p>• Two Chinese arithmetic datasets (MathMC and MathToF) with answers and detailed rationales are constructed for further facilitating the study of arithmetic reasoning tasks.1</p>
<p>• Comprehensive experiments show the effectiveness of our proposed integrated framework, and it achieves new state-of-the-art performance on four benchmarks.</p>
<p>2 Related Work</p>
<p>In-context Learning</p>
<p>In-context learning (ICL) has emerged as a successful and widely adopted approach for NLP tasks.It enables language models to learn and make predictions based on a few examples (Dong et al., 2022).Unlike supervised learning, ICL does not rely on vast amounts of data and resources for training and fine-tuning language models which makes LLMs easier to apply to various tasks as a service (Sun et al., 2022).By designing the template or format of demonstration and select more relevant exemplars (Wei et al., 2022;Fu et al., 2022;Chen et al., 2022;Xiong et al., 2023), the effectiveness of utilizing LLMs to address complex reasoning tasks, such as arithmetic reasoning and commonsense reasoning, has significantly improved.</p>
<p>Reasoning with Prompting</p>
<p>Chain-of-Thought Based Prompting.As for CoT prompting (Wei et al., 2022) (Lee et al., 2024;Asai et al., 2023).</p>
<p>MathMC and MathToF</p>
<p>We create two datasets, MathMC and MathToF, featuring 1,000 Chinese mathematical Multiple-Choice and 1,000 Chinese True-or-False questions, accompanied by detailed explanations addressing the lack of diverse question types in existing Chinese arithmetic datasets.</p>
<p>In constructing these datasets, we initiated by gathering a set of 4,000 elementary school-level seed Multiple-Choice questions and 4,000 seed True-or-False questions spanning grades 1 to 6, focusing on math problems typically encountered in grades 4 to 6.These questions are then carefully filtered and proofread to ensure a broad coverage of knowledge points in each question.Through this rigorous filtering and selection process, we create a final dataset of 1,000 Multiple-Choice questions and 1,000 True-or-False questions, each meticulously labeled with answers and explanations.Two datasets feature a wide range of question types, including arithmetic, algebra, geometry, statistics, reasoning, and others.Specific question types statistics are shown in Table 1.</p>
<p>Appendix B presents details of our created two datasets and two sample Multiple-Choice and Trueor-False questions from our datasets.</p>
<p>Teaching-Inspired Integrated Prompting Framework</p>
<p>We construct a prompting framework, as illustrated in Figure 1 which consists of three stages.</p>
<p>Teaching-Inspired Prompts Generation</p>
<p>Prompts are generated by drawing inspiration from traditional pedagogical methods, emphasizing the use of educational sources.Students begin with foundational theories and concepts from textbooks and workbooks to deeply understand problem principles, then apply these through extensive exercises and examples.To enhance the capability of LLMs to solve mathematical reasoning problems, we adapt the aforementioned teaching strategy to reasoning tasks, feeding the LLMs with similar problems and the essential background knowledge (e.g.theorems, concepts, and term definitions) required to solve the specific problem.</p>
<p>Figure 3(a) illustrates the process of obtaining similar problems.We tokenize the test problem, preserving special math expressions, and use BM25 (Robertson et al., 2009) to retrieve a set of candidate problems, P. Identical problems and those differing only in numerical values are excluded.Candidates are ranked by their Longest Common Subsequence (LCS) length with the test problem.</p>
<p>Figure 3(b) describes background knowledge acquisition.We tokenize the test problem and analyses, constructing a token set T by removing stopwords and operands.An LLM aids in extracting key knowledge points and uncertain theorems.These tokens serve as queries to retrieve relevant theorems and conceptual knowledge from a knowledge database, yielding background knowledge candidates, K. Candidates are ranked by LCS length, with the top three selected.Similar to obtaining similar problems, the LCS length between each candidate k i and the combined text is computed.Top-3 candidates are selected based on LCS length.</p>
<p>Therefore, prompts are mainly composed of three elements: few-shot CoT + PoT exemplars (2 cases) (one case = question + CoT exemplars + Python program exemplars + answer), similar questions and their analyses, and background knowledge.These prompts assist LLMs in generating the intermediate steps of the final answer and devising the Python program needed to solve the problem.</p>
<p>Answers Generation</p>
<p>We employ the self-consistency method, allowing LLMs to iterate N times, generating N different ....</p>
<p>Step-by-Step</p>
<p>Analysis A 1</p>
<p>'</p>
<p>Step-by-Step</p>
<p>Analysis A 2</p>
<p>'</p>
<p>Step-by-Step</p>
<p>Analysis A 2n-1 ' ....</p>
<p>Top-1 Python Program Answer</p>
<p>Top-1 Step-by-Step Analysis Answer</p>
<p>Ranking by Frequency</p>
<p>Final Answer</p>
<p>Select the most common answer</p>
<p>Answers Selection</p>
<p>Double-Check Verification.We initially compare the results generated by each pathway in the N possible solution paths, i.e., verifying if the outputs of the Python programs align with the corresponding step-by-step answers.This comparison process double-checks the computation results, thereby enhancing the trustworthiness of the final answer.If all paths yield consistent answers, the most frequent answer from the consistent answers is chosen as the output via majority voting.Otherwise, the LLM is tasked to provide N-1 with more answers to the problem.After that, the process transitions to the Further Selection stage.The inclusion of Python programs and the implementation of a double-check-verification strategy reduce the probability of simple calculation errors and enhance the reliability of the language model.English-Chinese Ensembling.In the additional N-1 solution requests, if the problem is in Chinese, we instruct the language model to translate the test problem, the background knowledge, and similar problems into English before generating solutions.This approach is adopted since LLM might not fully understand certain Chinese expressions, and translation can aid in generating accurate results.Further Selection.We evaluate the frequency of the most common answer in both the Python pro-gram outputs (code-ans) and the results derived from step-by-step solutions (step-by-step-ans).If the most frequent answer from the Python program (top-code-ans) has a frequency equal to or higher than that of the most frequent answer from the Stepby-Step solution (top-step-by-step-ans), then the top-code-ans is selected as the output.Conversely, top-step-by-step-ans is chosen as the final output.</p>
<p>Experiments</p>
<p>Experimental Settings</p>
<p>Datasets.Our method is evaluated on six English mathematical reasoning benchmarks: AddSub (Hosseini et al., 2014), SingleEQ (Koncel-Kedziorski et al., 2015), SVAMP (Patel et al., 2021), MultiArith (Roy and Roth, 2015), GSM8K (Cobbe et al., 2021), AQuA (Ling et al., 2017), one Chinese math reasoning benchmark, Math23K (Wang et al., 2017), and two datasets (MathMC and MathToF) we construct.Models.For our experiments, we use two LLMs from the GPT series: GPT-3.5-Turbo(Ouyang et al., 2022) and GPT-4 (OpenAI, 2023) Table 2: Evaluation results of teaching-inspired integrated prompting framework applied on different models, GPT-3.5-Turbo and GPT-4.Our approach improves performance on different models and benchmarks.Our approach achieves state-of-the-art performance on AddSub, SVAMP, Math23K and AQuA benchmarks on GPT-4.Previous state-of-the-art performance are from (Gao et al., 2023) for SingleEQ, (Wang et al., 2022) for MultiArith, (Zhao et al., 2023) for AddSub and SVAMP, (Zhou et al., 2023) for GSM8K, (Zheng et al., 2023) for AQuA, (Zhang et al., 2022) for Math23K dataset.</p>
<p>to 0.0, while for the Self-Consistency strategy, it is adjusted to 0.5.</p>
<p>Main Results</p>
<p>We compare the evaluation results of our integrated prompting framework with the Chain-of-Thought method on GPT-3.5-Turbo and GPT-4 models.</p>
<p>As shown in Table 2, our framework improves the mathematical reasoning performance significantly over two models on seven math datasets, especially improving 8.8% on GSM8K, 24.8% on Math23K, 8.0% on SingleEQ and 10.2% on AQuA when used on GPT-3.5-Turbo.Surprisingly, with GPT-4 and our integrated prompting framework, we achieve the new state-of-the-art performance on four math benchmarks (AddSub, SVAMP, Math23K and AQuA) with accuracies of 98.2% (+3.3%), 93.9% (+0.2%), 94.3% (+7.2%) and 81.1% (+1.2%).</p>
<p>Additionally, we present the results of GPT-3.5-Turbo and GPT-4 on two datasets we created, MathMC and MathToF.As seen in Table 2, leveraging our prompt method on GPT-3.5-Turboyields a significant enhancement in performance, boosting reasoning accuracy on MathMC by 18.8% and MathToF by 10.5%, respectively.On deploying the same prompting framework on GPT-4, we observed a marked improvement as well, with increases of 4.1% and 6.7% in respective metrics.These results demonstrate the efficacy of our approach in facilitating reasoning tasks.</p>
<p>Ablation Study</p>
<p>Similar problems and background</p>
<p>knowledge.</p>
<p>Removing similar problems and background knowledge from the prompts leads to a general decline in accuracy across nine datasets shown in Table 3.This indicates that similar problems and background play a guiding role in enhancing LLM reasoning.</p>
<p>Python programs and double-check verification strategy.</p>
<p>The results in Table 3 demonstrate that removing the Python program generation and double-check strategies had minimal impact or even a slight improvement in accuracy for simpler math problem sets (AddSub, SingleEQ and MultiArith).However, for more complex problem sets (GSM8K, Math23K), there is a noticeable decrease in accuracy by 9.1% and 4.1% respectively.This indicates that incorporating Python program-generated answers and the double-check strategy helps compensate for LLMs susceptibility to computational errors in complex calculations.</p>
<p>Answer Selection strategy.</p>
<p>Analyzing the experimental results, it can be found that the accuracy decrease among nine datasets, especially on more complex datasets including GSM8K, AQuA, and Math23K, where the accuracy dropped by 8.  calculation methods (Python program or natural language) would produce the same results for a given problem.</p>
<p>English-Chinese-Ensembling strategy.</p>
<p>We evaluate the impact of the English-Chinese Ensembling strategy on three Chinese datasets.When we removed this component, the accuracy on MathMC and MathToF dropped by 3.0% and 1.0%.This finding suggests that translating Chinese problems into English can make it easier for the language model to understand, thereby generating more accurate solutions.</p>
<p>5.3.5</p>
<p>The number of similar problems.</p>
<p>We explore the affect of the number of similar problems by adding different number of varying or the same similar problems 2 to prompts. Figure 2(a) shows that the effectiveness of adding similar questions is not solely determined by quantity.When the added questions differ significantly from the target question, it can negatively affect accuracy.However, within a certain similarity threshold, increasing the quantity of similar questions improves LLMs reasoning accuracy.Figure 2(b) demonstrates that including the multiple same top-similar questions in prompts leads to an notable improvement in LLMs reasoning accuracy.This approach indirectly addresses the challenge of acquiring external information, benefiting LLMs in capturing and utilizing relevant external information more effectively.</p>
<p>2 Adding K number of the same similar problems refers to repeating the exact exact same similar problem K times within the prompt.</p>
<p>Conclusion</p>
<p>This paper presents an innovative teaching-inspired integrated prompting framework, to conquer the limitations of LLMs in arithmetic reasoning tasks.The framework emulates the teaching process to introduce essential concepts, theorems, and similar problems to LLMs, and designing double-checking and answer selection mechanisms thereby significantly enhancing their capability to perform arithmetic reasoning tasks.Empirical results reveal that employing our framework leads to substantial im-provements in arithmetic reasoning accuracy.Our study also underscores the need for more diverse and comprehensive benchmarks for evaluating the performance on arithmetic reasoning, which we address by introducing the MathMC and MathToF datasets.In future work, researchers can further refine and explore its applicability to other domains.</p>
<p>Figure 3 illustrates the process of similar problems and background knowledge retrieval.</p>
<p>For background knowledge database, we curated a rich repository from mathematical textbooks, exercise workbooks, and web sources.From these resources, we extract a wealth of knowledge points and background information, encompassing theories, theorems, and problem-solving methodologies.Subsequently, we compile and store these findings to establish a comprehensive knowledge base.</p>
<p>In parallel, the similar problems database comprises problems sourced from mathematical textbooks and workbooks, each accompanied by detailed analyses derived from these materials.</p>
<p>B Dataset Details and Sample Questions</p>
<p>In this section, we discuss the details and specific segments of the two datasets we have established.Figure 5 showcases two sample questions from the dataset, one being a multiple-choice question, and the other a true-or-false question.</p>
<p>Each multiple-choice question is divided into two segments.The first segment, the question stem, is composed of the question itself and the possible choices.The second segment includes the correct answer and a comprehensive explanation of the solution.Similarly, the structure of the true-or-false questions includes a question stem, the correct answer (either True or False), and a detailed rationale.Example Question from MathMC "quest_stem": { "options": [ { "bullet": "A", "text": "7" }, { "bullet": "B", "text": "13" }, { "bullet": "C", "text": "27" } ], "text": "在⼀组不同的数字中，最⼤数是15，这组数的平均数不可能是( ) " }, "quest_ref": { "texts": [ "C"], "analyses":
["根据平均数的求法，求出这组数的和再除以这组数的个数，如果这组数 ⼤⼩不同，平均数要⽐最⼤的数⼩，⽐最⼩的数⼤; 如果这组数⼤⼩相同，最⼤数、最⼩ 数、平均数相等，因为C选项⼤于最⼤数15，所以是不可能的，选择C选项"] } Example Question from MathToF "quest_stem": { "text": "⼀个数除以15，商是30，余数是8，这个数是150" }, "quest_ref": { "texts": ["False"], "analyses": [ "求被除数，根据:被除数=商×除数+余数，解答即可。在本题中除数是15，商 是30，余数是8，被除数=30×15+8=458，所以该说法错误。" ] }
Example Question from MathMC "quest_stem": { "options": [ { "bullet": "A", "text": "7" }, { "bullet": "B", "text": "13" }, { "bullet": "C", "text": "27" } ], "text": "In a set of numbers, the largest number is 15, and the average of the set cannot be () " }, "quest_ref": { "texts": [ "C"], "analyses": ["According to the method of calculating the average, we calculate the sum of this group of numbers and then divide it by the count of numbers in this group.If the numbers in this group are different, the average will be smaller than the largest number and larger than the smallest number.If the numbers in this group are the same, the largest number, the smallest number, and the average will be equal.Since option C is greater than the maximum number 15, it is impossible.Therefore, option C is not possible, so you should choose option C."] } Example Question from MathToF "quest_stem": { "text": "A number divided by 15, the quotient is 30, the remainder is 8, and this number is 150" }, "quest_ref": { "texts": ["False"], "analyses": [ "According to: dividend = quotient × divisor + remainder.In this case, the divisor is 15, the quotient is 30, and the remainder is 8, so the dividend is equal to 30×15+8=458."] }</p>
<p>C.1 System Prompt</p>
<p>You are an super smart elementary school math teacher.You need read the math problem carefully and solve it in a step by step way to be sure you have the right answer.You often make mistakes in calculations, so please be careful when calculating.</p>
<p>Please do not be influenced by the typos in the question and reason based on the semantics of the question.</p>
<p>Please make sure your replies as simple and easy to understand as possible.</p>
<p>C.2 Teaching-Inspired Prompt</p>
<p>This section shows the Teaching-Inspired Prompting format and one example of our proposed prompt.If there is a reference question and the reference question is very similar to the question you need to answer, you should think based on the analysis process of the reference question, but you cannot be affected by its question stem.You still need to return the complete analysis process of the question you need to answer.Reference question: sim_stem Reference analysis: sim_analysis Reference answer: sim_ans You may use the following background knowledge when analyzing the problem: Background: background Question: question to be solved</p>
<p>C.2.1 Example</p>
<p>If there is a reference question and the reference question is very similar to the question you need to answer, you should think based on the analysis process of the reference question, but you cannot be affected by its question stem.You still need to return the complete analysis process of the question you need to answer.Reference question: In Class 6, there are a total of 52 students.Among them, 30 students like to eat rice, and 29 students prefer noodles.The number of students who like both rice and noodles is ( ).Reference analysis: Based on the information "There are a total of 30 students who like to eat rice and 29 students who prefer noodles," we can calculate the total number of students who like either rice or noodles: 30 + 29 = 59.However, this count includes the students who like both rice and noodles twice.Therefore, applying the principle of inclusion-exclusion, we can determine that the number of students who like both rice and noodles is 59 -52 = 7.Thus, the answer is 7. Question: Xiaoming is 5 years old this year, and his father is 25 years old this year.How old will Xiaoming be when his father is 30 years old?Analysis: thought: When the father is 30 years old, 5 years have passed since he was 25.At this time, Little Ming should be 10 years old (5 + 5).steps: 1.We need to figure out how many years it will take for the father to reach 30 years old from now (25 years old).This can be obtained by subtracting 25 from 30, that is, 30-25=5.Therefore, the father still needs 5 years to reach 30 years old.2. We know that Little Ming is now 5 years old, so his age will increase in the next 5 years.Since his age increases by 1 year every year, in 5 years his age will increase by 5 years.3. If we add Little Ming's current age of 5 to the increase of 5 years in the next 5 years, we can get Little Ming's age when his father is 30 years old.That is, 5+5=10.answer: 10 Question: Xiaoming read 30 pages on the second day, and read one more page than the second day on the first day.How many pages did he read on the first day?Analysis: thought: Since Xiaoming read 30 pages on the second day and read one more page than the second day on the first day, Xiaoming read 31 pages on the first day.steps: question, with as much detail as possible.answer: It's necessary.The specific option to the question, such as A/B/C/D Important: Your return format must be consistent with the Examples Important: The content you return must include the keyword: thought, steps and answer and the content of every keyword cannot be empty.Besides, each keyword should be in English.</p>
<p>C.3.3 True-or-False Problems</p>
<p>Examples: Question: True or False: The number that is 100 more than the largest three-digit number is 1999.'thought: Firstly, we need to know what the largest three-digit number is, and then calculate the largest three-digit number plus 100 to determine whether the result is equal to 1999.If the result is not equal to 1999, then the statement is false.If it is equal to 1999, then the statement is true.steps: 1.The largest three-digit number is 999.2. Adding 100 to 999 results in 1099.3. The result of the calculation is 1099, which is not equal to 1999.Therefore, the answer to this question is false.answer: False Question: True or False: The '9' in 0.019 is in the hundredths place.thought: The first decimal place to the right of the decimal point is the tenths place, the second decimal place is the hundredths place, and the third decimal place is the thousandths place.steps: 1.To determine the hundredths place, we need to look at the second decimal place to the right of the decimal point.2. Looking at the third decimal place to the right of the decimal point in 0.019, we find that it is 9. 3. We can conclude that the "9" in 0.019 is in the thousandths place.4. Therefore, the statement in the question is false.answer: False Question: True or False: The remainder is never greater than the quotient.thought: This statement can be judged by the relationship between the remainder, divisor, and quotient, or by giving examples to see if the statement is true or false.steps: 1.Generally, the remainder cannot be greater than the divisor, but there is no absolute relationship between the remainder and the quotient.2. For example, 104 divided by 33 equals 3 with a remainder of 5, where the remainder 5 is greater than the quotient 3. 3. Another example is 3 divided by 4, which equals 0 with a remainder of 3, where the remainder 3 is greater than the quotient 0. 4. Therefore, based on the counterexamples and concept relationships, we can conclude that this statement is false.5. Therefore, the final answer is false.answer: False b) Replay Format When you are certain that the answer is correct, you need to return the following content: thought: It's necessary.Return your thinking process for solving this problem.steps: It's necessary.The steps for solving the question, with as much detail as possible.answer: It's necessary.If you believe that the statement in the question is correct, return True.If you believe that the statement in the question is false, return False.Important: Your return format must be consistent with the Examples Important: The content you return must include the keyword: thought, steps and answer.and the content of every keyword cannot be empty.Besides, each keyword should be in English.</p>
<p>Figure 1 :
1
Figure 1: Architecture of our Teaching-Inspired Integrated Prompting Framework</p>
<p>Ablation study for different components of our proposed integrated prompting framework.The study utilized seven public datasets and two datasets we created, with all experiments on the GPT-3.5-Turbomodel.</p>
<p>Figure 2 :
2
Figure 2: Experiment results of adding different number of varying or the same similar problems.</p>
<p>Figure 3 :
3
Figure 3: The procedure of similar problems retrieval (a) and the background knowledge generation (b).</p>
<p>Figure 4 :
4
Figure 4: Sample questions of MathMC and MathToF datasets.The left side displays the original questions in Chinese, while the right side shows the same questions translated into English</p>
<p>Reference answer: 7You may use the following background knowledge when analyzing the problem: Background: principle of inclusion-exclusion: |A∪ B| = |A| + |B| − |A ∩ B| Question: In order to prepare for the fruit party, Class 3 made statistics on the two kinds of fruits that everyone liked.38 students like to eat bananas, 32 students like to eat fragrant pears, and 10 students like both.How many students in Class 3?</p>
<p>STAGE 1: Prompts Generation
STAGE 2: Answers GenerationAnswers from differentFew-Shot CoTreasoning pathsExemplarsPython ProgramStep-by-StepFew-Shot ProgramAnswer A 1Analysis A 1Exemplarsiterate N timesTest Question+Similar Problems Similar ProblemsPromptsLLMPython Program Answer A 2 . . .Step-by-Step ' Analysis A 2 . . .AnalysesPython ProgramStep-by-StepInput QuestionBackground KnowledgeNoAnswer A n Double Check: verify whether the answer from Python program and Step-by-Analysis A n'Step analysis in eachiterate N-1 timesreasoning path are the sameLLMPrompts in EnglishtranslateLLMYesTest Question is in Chinese?NoA k = A k ' ?English-Chinese-EnsemblingYesPython ProgramPython ProgramPython ProgramAnswer A 1Answer A 2Answer A 2n-1Answer A 1Answer A 2....Answer A nMajority VotingFinal AnswerSTAGE 3: Answer Selection
'. . .</p>
<p>We will make the dataset publicly when the paper is published.
AppendixA Similar Problems and Background Knowledge Retrieval1. Xiaoming read one more page on the first day than on the second day.2. Xiaoming read 30 pages on the second day.3. Therefore, the number of pages Xiaoming read on the first day is one more than that of the second day.4. Thus, Xiaoming read 30 pages + 1 page on the first day, which is equal to 31 pages.
Self-rag: Learning to retrieve, generate, and critique through self-reflection. Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, Hannaneh Hajishirzi, ArXiv, abs/2310.115112023</p>
<p>Language models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Advances in neural information processing systems. 202033</p>
<p>Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks. Wenhu Chen, Xueguang Ma, Xinyi Wang, William W Cohen, arXiv:2211.125882022arXiv preprint</p>
<p>Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, arXiv:2204.02311Palm: Scaling language modeling with pathways. 2022arXiv preprint</p>
<p>Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, arXiv:2110.14168Training verifiers to solve math word problems. 2021arXiv preprint</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, arXiv:1810.04805Pre-training of deep bidirectional transformers for language understanding. Bert2018arXiv preprint</p>
<p>Active prompting with chain-ofthought for large language models. Shizhe Diao, Pengcheng Wang, Yong Lin, Tong Zhang, arXiv:2302.122462023arXiv preprint</p>
<p>Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, Zhifang Sui, arXiv:2301.00234A survey for in-context learning. 2022arXiv preprint</p>
<p>Yao Fu, Hao Peng, Ashish Sabharwal, Peter Clark, Tushar Khot, arXiv:2210.00720Complexity-based prompting for multi-step reasoning. 2022arXiv preprint</p>
<p>Pal: Program-aided language models. Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, Graham Neubig, International Conference on Machine Learning. PMLR2023</p>
<p>Hangfeng He, Hongming Zhang, Dan Roth, arXiv:2301.00303Rethinking with retrieval: Faithful large language model inference. 2022arXiv preprint</p>
<p>Learning to solve arithmetic word problems with verb categorization. Mohammad Javad Hosseini, Hannaneh Hajishirzi, Oren Etzioni, Nate Kushman, Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)2014</p>
<p>Shima Imani, Liang Du, Harsh Shrivastava, arXiv:2303.05398Mathprompter: Mathematical reasoning using large language models. 2023arXiv preprint</p>
<p>Large language models are zero-shot reasoners. Takeshi Kojima, Shane Shixiang, Machel Gu, Yutaka Reid, Yusuke Matsuo, Iwasawa, Advances in neural information processing systems. 202235</p>
<p>Parsing algebraic word problems into equations. Rik Koncel-Kedziorski, Hannaneh Hajishirzi, Ashish Sabharwal, Oren Etzioni, Siena Dumas, Ang , Transactions of the Association for Computational Linguistics. 32015</p>
<p>Planrag: A plan-then-retrieval augmented generation for generative large language models as decision makers. Myeonghwa Lee, Seonho An, Min-Soo Kim, ArXiv, abs/2406.124302024</p>
<p>Program induction by rationale generation: Learning to solve and explain algebraic word problems. Wang Ling, Dani Yogatama, Chris Dyer, Phil Blunsom, arXiv:1705.041462017arXiv preprint</p>
<p>Dynamic prompt learning via policy gradient for semi-structured mathematical reasoning. Pan Lu, Liang Qiu, Kai-Wei Chang, Ying Nian Wu, Song-Chun Zhu, Tanmay Rajpurohit, Peter Clark, Ashwin Kalyan, arXiv:2209.146102022arXiv preprint</p>
<p>arXiv:2303.08774Gpt-4 technical report. 2023. 2023OpenAIarXiv preprint</p>
<p>Training language models to follow instructions with human feedback. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, Advances in Neural Information Processing Systems. 202235</p>
<p>Arkil Patel, Satwik Bhattamishra, Navin Goyal, arXiv:2103.07191Are nlp models really able to solve simple math word problems?. 2021arXiv preprint</p>
<p>Sebastian Jack W Rae, Trevor Borgeaud, Katie Cai, Jordan Millican, Francis Hoffmann, John Song, Sarah Aslanides, Roman Henderson, Susannah Ring, Young, arXiv:2112.11446Scaling language models: Methods, analysis &amp; insights from training gopher. 2021arXiv preprint</p>
<p>The probabilistic relevance framework: Bm25 and beyond. Stephen Robertson, Hugo Zaragoza, Foundations and Trends® in Information Retrieval. 342009</p>
<p>Solving general arithmetic word problems. Subhro Roy, Dan Roth, Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. the 2015 Conference on Empirical Methods in Natural Language Processing2015</p>
<p>Learning to retrieve prompts for in-context learning. Ohad Rubin, Jonathan Herzig, Jonathan Berant, Proceedings of the 2022 Conference of the North American Chapter. the 2022 Conference of the North American ChapterHuman Language Technologies2022</p>
<p>Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal, Md Shoeb, Abubakar Abid, Adam Fisch, Adam Adam R Brown, Aditya Santoro, Adrià Gupta, Garriga-Alonso, arXiv:2206.04615Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. 2022arXiv preprint</p>
<p>Xuanjing Huang, and Xipeng Qiu. 2022. Black-box tuning for language-model-as-a-service. Tianxiang Sun, Yunfan Shao, Hong Qian, International Conference on Machine Learning. PMLR</p>
<p>Self-consistency improves chain of thought reasoning in language models. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, Denny Zhou, arXiv:2203.111712022arXiv preprint</p>
<p>Deep neural solver for math word problems. Yan Wang, Xiaojiang Liu, Shuming Shi, Proceedings of the 2017 conference on empirical methods in natural language processing. the 2017 conference on empirical methods in natural language processing2017</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Denny Quoc V Le, Zhou, Advances in Neural Information Processing Systems. 202235</p>
<p>Dq-lore: Dual queries with low rank approximation re-ranking for in-context learning. Jing Xiong, Zixuan Li, Chuanyang Zheng, Zhijiang Guo, Yichun Yin, Enze Xie, Zhicheng Yang, Qingxing Cao, Haiming Wang, Xiongwei Han, arXiv:2310.029542023arXiv preprint</p>
<p>Wenqi Zhang, Yongliang Shen, Yanna Ma, Xiaoxia Cheng, Zeqi Tan, Qingpeng Nong, Weiming Lu, arXiv:2210.11694Multi-view reasoning: Consistent contrastive learning for math word problem. 2022arXiv preprint</p>
<p>Automatic model selection with large language models for reasoning. Xu Zhao, Yuxi Xie, Kenji Kawaguchi, Junxian He, Qizhe Xie, arXiv:2305.143332023arXiv preprint</p>
<p>Chuanyang Zheng, Zhengying Liu, Enze Xie, Zhenguo Li, Yu Li, arXiv:2304.09797Progressive-hint prompting improves reasoning in large language models. 2023arXiv preprint</p>
<p>Solving challenging math word problems using gpt-4 code interpreter with code-based self-verification. Aojun Zhou, Ke Wang, Zimu Lu, Weikang Shi, Sichun Luo, Zipeng Qin, Shaoqing Lu, Anya Jia, Linqi Song, Mingjie Zhan, arXiv:2308.079212023arXiv preprint</p>
<p>Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc Le, arXiv:2205.10625Least-to-most prompting enables complex reasoning in large language models. 2022arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
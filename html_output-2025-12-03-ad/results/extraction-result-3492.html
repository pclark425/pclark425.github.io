<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3492 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3492</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3492</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-78.html">extraction-schema-78</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-636a79420d838eabe4af7fb25d6437de45ab64e8</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/636a79420d838eabe4af7fb25d6437de45ab64e8" target="_blank">RACE: Large-scale ReAding Comprehension Dataset From Examinations</a></p>
                <p><strong>Paper Venue:</strong> Conference on Empirical Methods in Natural Language Processing</p>
                <p><strong>Paper TL;DR:</strong> The proportion of questions that requires reasoning is much larger in RACE than that in other benchmark datasets for reading comprehension, and there is a significant gap between the performance of the state-of-the-art models and the ceiling human performance.</p>
                <p><strong>Paper Abstract:</strong> We present RACE, a new dataset for benchmark evaluation of methods in the reading comprehension task. Collected from the English exams for middle and high school Chinese students in the age range between 12 to 18, RACE consists of near 28,000 passages and near 100,000 questions generated by human experts (English instructors), and covers a variety of topics which are carefully designed for evaluating the students’ ability in understanding and reasoning. In particular, the proportion of questions that requires reasoning is much larger in RACE than that in other benchmark datasets for reading comprehension, and there is a significant gap between the performance of the state-of-the-art models (43%) and the ceiling human performance (95%). We hope this new dataset can serve as a valuable resource for research and evaluation in machine comprehension. The dataset is freely available at http://www.cs.cmu.edu/~glai1/data/race/ and the code is available at https://github.com/qizhex/RACE_AR_baselines.</p>
                <p><strong>Cost:</strong> 0.012</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3492.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3492.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Stanford AR</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Stanford Attentive Reader</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A recurrent-attention reader that encodes passage and question with bidirectional GRUs, uses a bilinear attention to summarize relevant passage portions, encodes answer options and scores them via a bilinear matching to the attended passage representation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>A thorough examination of the cnn/daily mail reading comprehension task.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Stanford Attentive Reader</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Bi-GRU based reader: passage and question encoded with bidirectional GRUs; a bilinear attention (softmax over h_i^p^T W h^q) produces a passage summary s^p; each option is encoded by a Bi-GRU to h^{o_i}; option scores computed by bilinear matching h^{o_i} W_2 s^p and passed through softmax. In this paper initialization uses 100-d GloVe embeddings, hidden dim 128, single-layer GRUs, trained with SGD, dropout and gradient clipping.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>RACE (Reading Comprehension with reasoning: single-sentence, multi-sentence, passage summarization, attitude analysis, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>RACE is a large-scale multiple-choice reading comprehension benchmark collected from English exams for Chinese middle/high school students (27,933 passages, 97,687 questions). Questions require higher proportions of reasoning (single- and multi-sentence inference, passage summarization, attitude analysis, and some world-knowledge/arithmetic).</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Bilinear attention to summarize passage conditioned on question; bidirectional GRU encoders for passage/question/options; use of pretrained GloVe embeddings, dropout, and SGD training hyperparameter tuning. No further neuro-symbolic or chain-of-thought style interventions were applied in this work.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Accuracy on RACE: overall 43.3%; RACE-M 44.2%; RACE-H 43.0%. On other datasets (reported from literature) Stanford AR achieves much higher results (e.g., CNN/Daily Mail ~73.6%).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Random baseline on RACE overall: 24.9%; Sliding Window baseline: 32.2% (RACE overall), Sliding Window RACE-M 37.3%, RACE-H 30.4%. Human Turkers: RACE overall ~73.3%; Ceiling human performance: 94.5% (RACE overall).</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_over_baseline</strong></td>
                            <td>Stanford AR improves over random baseline by ~18.4 percentage points (43.3% vs 24.9) and over Sliding Window by ~11.1 percentage points (43.3% vs 32.2) on RACE overall. However, it remains substantially below human performance (gap ~51.2 percentage points vs ceiling).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Significant gap to human ceiling (models achieve <45% vs ~94.5% ceiling). Models struggle especially on reasoning-heavy questions (single- and multi-sentence reasoning, passage summarization, attitude analysis). Stanford AR did not show notably stronger performance on simple word-matching than on reasoning categories, possibly due to similar embeddings for candidate answers (e.g., color words) making option discrimination difficult. Overall poor generalization from datasets like CNN/DM (where these models are near ceiling) to RACE, indicating limitations in handling richer reasoning demands.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>Paper provides per-question-type analysis (Figure 1) showing lower model performance on reasoning categories; authors note that a large fraction (59.2%) of RACE questions require single- or multi-sentence reasoning. Implementation and hyperparameter details (embedding size, hidden dims, dropout, learning rates) and grid search on validation set are reported; no formal ablation study isolating individual components of Stanford AR within RACE experiments is reported.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'RACE: Large-scale ReAding Comprehension Dataset From Examinations', 'publication_date_yy_mm': '2017-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3492.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3492.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Gated-Attention Reader</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multi-hop attentive reader that refines document token representations via gated multiplicative interactions with the query and iteratively attends to the document to build a query-specific representation for answer prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Gated-attention readers for text comprehension.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Gated-Attention Reader (GA)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>GA builds query-specific token representations by multiplicative (gated) interactions between query and document embeddings, using a multi-hop architecture to iteratively refine representations and attention. In the original GA, an Attention-Sum layer and optional character-level embeddings are used; in this paper the authors' implementation omits the Attention-Sum final layer and does not use character-level embeddings. Training uses 100-d GloVe embeddings, Bi-GRU encoders with hidden dim 128, single-layer, SGD, dropout, and gradient clipping.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>RACE (Reading Comprehension with reasoning: single-sentence, multi-sentence, passage summarization, attitude analysis, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Same RACE benchmark described above: multiple-choice reading comprehension with a substantial fraction of questions requiring reasoning across sentences and summarization/attitude inference.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Gated (multiplicative) attention, multi-hop document-question interactions to iteratively refine token representations; standard ML interventions used: pretrained GloVe embeddings, dropout, SGD, grid-searched hyperparameters. The authors' implementation intentionally omits the Attention-Sum final layer and character-level embeddings compared to the original GA design.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Accuracy on RACE: overall 44.1%; RACE-M 43.7%; RACE-H 44.2%. GA achieves modestly better performance than Stanford AR on overall RACE (44.1% vs 43.3%) in these experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Random baseline on RACE overall: 24.9%; Sliding Window baseline: 32.2% (RACE overall). Stanford AR: 43.3% (overall). Human Turkers: RACE overall ~73.3%; Ceiling human performance: 94.5% (RACE overall).</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_over_baseline</strong></td>
                            <td>GA improves over random baseline by ~19.2 percentage points (44.1% vs 24.9) and over Sliding Window by ~11.9 percentage points (44.1% vs 32.2). GA's improvement over Stanford AR on RACE overall is small (~0.8 percentage points).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Despite multi-hop gated attention, GA's absolute performance remains low relative to human ceiling; it particularly struggles on multi-sentence reasoning and high-level tasks like passage summarization and attitude analysis. The paper notes that the GA implementation used here lacks the Attention-Sum final layer and character-level embeddings, which could reduce its reported effectiveness compared to the original GA variants on other datasets. Overall, models that do well on cloze or span-extraction datasets (CNN/DM, CBT) do not transfer to RACE's richer reasoning demands.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>Authors report that their GA implementation differs from the original (no Attention-Sum layer, no char embeddings) but do not present a controlled ablation isolating these components in RACE. They include per-reasoning-type evaluation (Figure 1) showing GA struggles on reasoning categories; hyperparameter grid search details reported. No further internal ablation study is presented in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'RACE: Large-scale ReAding Comprehension Dataset From Examinations', 'publication_date_yy_mm': '2017-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3492.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3492.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Sliding Window</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sliding Window Algorithm (TF-IDF style matching baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A rule-based baseline that concatenates question and candidate option and computes TF-IDF style matching score against windows of the passage; selects the option with highest match score.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Mctest: A challenge dataset for the open-domain machine comprehension of text.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Sliding Window (TF-IDF matching baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Non-neural, rule-based baseline: for each candidate option, concatenate with question and compute TF-IDF-style similarity score with sliding windows (spans) over the passage; choose the option with maximum score. Window size chosen by tuning on training/dev sets.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>RACE (Reading Comprehension requiring deeper reasoning than span-matching tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>RACE dataset where candidate answers are human-written and not necessarily exact spans in the passage; many questions require reasoning beyond surface lexical overlap.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>No learning; uses TF-IDF style matching and windowed search; serves as a heuristic baseline to quantify how many questions can be solved by surface matching.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Accuracy on RACE: overall 32.2%; RACE-M 37.3%; RACE-H 30.4%. Improvement over random on RACE overall is ~7.3 percentage points (32.2% vs 24.9%).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Random baseline: 24.9% (RACE overall). Neural models: Stanford AR 43.3% and GA 44.1% on RACE overall.</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_over_baseline</strong></td>
                            <td>Sliding Window improves over random by ~7.3 percentage points on RACE overall; however, its relative improvement versus random is smaller on RACE than on some cloze datasets (e.g., much larger improvements were observed on CBT subsets and WDW in referenced work), indicating RACE contains fewer surface-match solvable questions and more reasoning-required questions.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Performs substantially worse than neural readers and far worse than human performance; fails on many questions requiring inference across sentences, summarization, attitude analysis, or when candidate answers are not lexically similar to passage spans.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>The paper uses Sliding Window primarily as a baseline and analyzes its relative improvement over random across datasets (shows much larger gains on cloze datasets than on RACE), supporting the claim that RACE requires deeper reasoning; no internal ablation reported.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'RACE: Large-scale ReAding Comprehension Dataset From Examinations', 'publication_date_yy_mm': '2017-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>A thorough examination of the cnn/daily mail reading comprehension task. <em>(Rating: 2)</em></li>
                <li>Gated-attention readers for text comprehension. <em>(Rating: 2)</em></li>
                <li>Mctest: A challenge dataset for the open-domain machine comprehension of text. <em>(Rating: 2)</em></li>
                <li>Teaching machines to read and comprehend. <em>(Rating: 1)</em></li>
                <li>The goldilocks principle: Reading children's books with explicit memory representations. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3492",
    "paper_id": "paper-636a79420d838eabe4af7fb25d6437de45ab64e8",
    "extraction_schema_id": "extraction-schema-78",
    "extracted_data": [
        {
            "name_short": "Stanford AR",
            "name_full": "Stanford Attentive Reader",
            "brief_description": "A recurrent-attention reader that encodes passage and question with bidirectional GRUs, uses a bilinear attention to summarize relevant passage portions, encodes answer options and scores them via a bilinear matching to the attended passage representation.",
            "citation_title": "A thorough examination of the cnn/daily mail reading comprehension task.",
            "mention_or_use": "use",
            "model_name": "Stanford Attentive Reader",
            "model_description": "Bi-GRU based reader: passage and question encoded with bidirectional GRUs; a bilinear attention (softmax over h_i^p^T W h^q) produces a passage summary s^p; each option is encoded by a Bi-GRU to h^{o_i}; option scores computed by bilinear matching h^{o_i} W_2 s^p and passed through softmax. In this paper initialization uses 100-d GloVe embeddings, hidden dim 128, single-layer GRUs, trained with SGD, dropout and gradient clipping.",
            "model_size": null,
            "reasoning_task_name": "RACE (Reading Comprehension with reasoning: single-sentence, multi-sentence, passage summarization, attitude analysis, etc.)",
            "reasoning_task_description": "RACE is a large-scale multiple-choice reading comprehension benchmark collected from English exams for Chinese middle/high school students (27,933 passages, 97,687 questions). Questions require higher proportions of reasoning (single- and multi-sentence inference, passage summarization, attitude analysis, and some world-knowledge/arithmetic).",
            "method_or_intervention": "Bilinear attention to summarize passage conditioned on question; bidirectional GRU encoders for passage/question/options; use of pretrained GloVe embeddings, dropout, and SGD training hyperparameter tuning. No further neuro-symbolic or chain-of-thought style interventions were applied in this work.",
            "performance": "Accuracy on RACE: overall 43.3%; RACE-M 44.2%; RACE-H 43.0%. On other datasets (reported from literature) Stanford AR achieves much higher results (e.g., CNN/Daily Mail ~73.6%).",
            "baseline_performance": "Random baseline on RACE overall: 24.9%; Sliding Window baseline: 32.2% (RACE overall), Sliding Window RACE-M 37.3%, RACE-H 30.4%. Human Turkers: RACE overall ~73.3%; Ceiling human performance: 94.5% (RACE overall).",
            "improvement_over_baseline": "Stanford AR improves over random baseline by ~18.4 percentage points (43.3% vs 24.9) and over Sliding Window by ~11.1 percentage points (43.3% vs 32.2) on RACE overall. However, it remains substantially below human performance (gap ~51.2 percentage points vs ceiling).",
            "limitations_or_failures": "Significant gap to human ceiling (models achieve &lt;45% vs ~94.5% ceiling). Models struggle especially on reasoning-heavy questions (single- and multi-sentence reasoning, passage summarization, attitude analysis). Stanford AR did not show notably stronger performance on simple word-matching than on reasoning categories, possibly due to similar embeddings for candidate answers (e.g., color words) making option discrimination difficult. Overall poor generalization from datasets like CNN/DM (where these models are near ceiling) to RACE, indicating limitations in handling richer reasoning demands.",
            "ablation_or_analysis": "Paper provides per-question-type analysis (Figure 1) showing lower model performance on reasoning categories; authors note that a large fraction (59.2%) of RACE questions require single- or multi-sentence reasoning. Implementation and hyperparameter details (embedding size, hidden dims, dropout, learning rates) and grid search on validation set are reported; no formal ablation study isolating individual components of Stanford AR within RACE experiments is reported.",
            "uuid": "e3492.0",
            "source_info": {
                "paper_title": "RACE: Large-scale ReAding Comprehension Dataset From Examinations",
                "publication_date_yy_mm": "2017-04"
            }
        },
        {
            "name_short": "GA",
            "name_full": "Gated-Attention Reader",
            "brief_description": "A multi-hop attentive reader that refines document token representations via gated multiplicative interactions with the query and iteratively attends to the document to build a query-specific representation for answer prediction.",
            "citation_title": "Gated-attention readers for text comprehension.",
            "mention_or_use": "use",
            "model_name": "Gated-Attention Reader (GA)",
            "model_description": "GA builds query-specific token representations by multiplicative (gated) interactions between query and document embeddings, using a multi-hop architecture to iteratively refine representations and attention. In the original GA, an Attention-Sum layer and optional character-level embeddings are used; in this paper the authors' implementation omits the Attention-Sum final layer and does not use character-level embeddings. Training uses 100-d GloVe embeddings, Bi-GRU encoders with hidden dim 128, single-layer, SGD, dropout, and gradient clipping.",
            "model_size": null,
            "reasoning_task_name": "RACE (Reading Comprehension with reasoning: single-sentence, multi-sentence, passage summarization, attitude analysis, etc.)",
            "reasoning_task_description": "Same RACE benchmark described above: multiple-choice reading comprehension with a substantial fraction of questions requiring reasoning across sentences and summarization/attitude inference.",
            "method_or_intervention": "Gated (multiplicative) attention, multi-hop document-question interactions to iteratively refine token representations; standard ML interventions used: pretrained GloVe embeddings, dropout, SGD, grid-searched hyperparameters. The authors' implementation intentionally omits the Attention-Sum final layer and character-level embeddings compared to the original GA design.",
            "performance": "Accuracy on RACE: overall 44.1%; RACE-M 43.7%; RACE-H 44.2%. GA achieves modestly better performance than Stanford AR on overall RACE (44.1% vs 43.3%) in these experiments.",
            "baseline_performance": "Random baseline on RACE overall: 24.9%; Sliding Window baseline: 32.2% (RACE overall). Stanford AR: 43.3% (overall). Human Turkers: RACE overall ~73.3%; Ceiling human performance: 94.5% (RACE overall).",
            "improvement_over_baseline": "GA improves over random baseline by ~19.2 percentage points (44.1% vs 24.9) and over Sliding Window by ~11.9 percentage points (44.1% vs 32.2). GA's improvement over Stanford AR on RACE overall is small (~0.8 percentage points).",
            "limitations_or_failures": "Despite multi-hop gated attention, GA's absolute performance remains low relative to human ceiling; it particularly struggles on multi-sentence reasoning and high-level tasks like passage summarization and attitude analysis. The paper notes that the GA implementation used here lacks the Attention-Sum final layer and character-level embeddings, which could reduce its reported effectiveness compared to the original GA variants on other datasets. Overall, models that do well on cloze or span-extraction datasets (CNN/DM, CBT) do not transfer to RACE's richer reasoning demands.",
            "ablation_or_analysis": "Authors report that their GA implementation differs from the original (no Attention-Sum layer, no char embeddings) but do not present a controlled ablation isolating these components in RACE. They include per-reasoning-type evaluation (Figure 1) showing GA struggles on reasoning categories; hyperparameter grid search details reported. No further internal ablation study is presented in this paper.",
            "uuid": "e3492.1",
            "source_info": {
                "paper_title": "RACE: Large-scale ReAding Comprehension Dataset From Examinations",
                "publication_date_yy_mm": "2017-04"
            }
        },
        {
            "name_short": "Sliding Window",
            "name_full": "Sliding Window Algorithm (TF-IDF style matching baseline)",
            "brief_description": "A rule-based baseline that concatenates question and candidate option and computes TF-IDF style matching score against windows of the passage; selects the option with highest match score.",
            "citation_title": "Mctest: A challenge dataset for the open-domain machine comprehension of text.",
            "mention_or_use": "use",
            "model_name": "Sliding Window (TF-IDF matching baseline)",
            "model_description": "Non-neural, rule-based baseline: for each candidate option, concatenate with question and compute TF-IDF-style similarity score with sliding windows (spans) over the passage; choose the option with maximum score. Window size chosen by tuning on training/dev sets.",
            "model_size": null,
            "reasoning_task_name": "RACE (Reading Comprehension requiring deeper reasoning than span-matching tasks)",
            "reasoning_task_description": "RACE dataset where candidate answers are human-written and not necessarily exact spans in the passage; many questions require reasoning beyond surface lexical overlap.",
            "method_or_intervention": "No learning; uses TF-IDF style matching and windowed search; serves as a heuristic baseline to quantify how many questions can be solved by surface matching.",
            "performance": "Accuracy on RACE: overall 32.2%; RACE-M 37.3%; RACE-H 30.4%. Improvement over random on RACE overall is ~7.3 percentage points (32.2% vs 24.9%).",
            "baseline_performance": "Random baseline: 24.9% (RACE overall). Neural models: Stanford AR 43.3% and GA 44.1% on RACE overall.",
            "improvement_over_baseline": "Sliding Window improves over random by ~7.3 percentage points on RACE overall; however, its relative improvement versus random is smaller on RACE than on some cloze datasets (e.g., much larger improvements were observed on CBT subsets and WDW in referenced work), indicating RACE contains fewer surface-match solvable questions and more reasoning-required questions.",
            "limitations_or_failures": "Performs substantially worse than neural readers and far worse than human performance; fails on many questions requiring inference across sentences, summarization, attitude analysis, or when candidate answers are not lexically similar to passage spans.",
            "ablation_or_analysis": "The paper uses Sliding Window primarily as a baseline and analyzes its relative improvement over random across datasets (shows much larger gains on cloze datasets than on RACE), supporting the claim that RACE requires deeper reasoning; no internal ablation reported.",
            "uuid": "e3492.2",
            "source_info": {
                "paper_title": "RACE: Large-scale ReAding Comprehension Dataset From Examinations",
                "publication_date_yy_mm": "2017-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "A thorough examination of the cnn/daily mail reading comprehension task.",
            "rating": 2
        },
        {
            "paper_title": "Gated-attention readers for text comprehension.",
            "rating": 2
        },
        {
            "paper_title": "Mctest: A challenge dataset for the open-domain machine comprehension of text.",
            "rating": 2
        },
        {
            "paper_title": "Teaching machines to read and comprehend.",
            "rating": 1
        },
        {
            "paper_title": "The goldilocks principle: Reading children's books with explicit memory representations.",
            "rating": 1
        }
    ],
    "cost": 0.011819249999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>RACE: Large-scale ReAding Comprehension Dataset From Examinations</h1>
<p>Guokun Lai<em> and Qizhe Xie</em> and Hanxiao Liu and Yiming Yang and Eduard Hovy<br>{guokun, qzxie, hanxiaol, yiming, hovy} @cs.cmu.edu<br>Language Technologies Institute<br>Carnegie Mellon University<br>Pittsburgh, PA 15213</p>
<h4>Abstract</h4>
<p>We present RACE, a new dataset for benchmark evaluation of methods in the reading comprehension task. Collected from the English exams for middle and high school Chinese students in the age range between 12 to 18 , RACE consists of near 28,000 passages and near 100,000 questions generated by human experts (English instructors), and covers a variety of topics which are carefully designed for evaluating the students' ability in understanding and reasoning. In particular, the proportion of questions that requires reasoning is much larger in RACE than that in other benchmark datasets for reading comprehension, and there is a significant gap between the performance of the state-of-the-art models ( $43 \%$ ) and the ceiling human performance ( $95 \%$ ). We hope this new dataset can serve as a valuable resource for research and evaluation in machine comprehension. The dataset is freely available at http://www.cs.cmu.edu/ glail/data/race/ and the code is available at https://github.com/ qizhex/RACE_AR_baselines</p>
<h2>1 Introduction</h2>
<p>Constructing an intelligence agent capable of understanding text as people is the major challenge of NLP research. With recent advances in deep learning techniques, it seems possible to achieve human-level performance in certain language understanding tasks, and a surge of effort has been devoted to the machine comprehension task where people aim to construct a system with the ability to</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>answer questions related to a document that it has to comprehend (Chen et al., 2016; Kadlec et al., 2016; Dhingra et al., 2016; Yang et al., 2017).</p>
<p>Towards this goal, several large-scale datasets (Rajpurkar et al., 2016; Onishi et al., 2016; Hill et al., 2015; Trischler et al., 2016; Hermann et al., 2015) have been proposed, which allow researchers to train deep learning systems and obtain results comparable to the human performance. While having a suitable dataset is crucial for evaluating the system's true ability in reading comprehension, the existing datasets suffer several critical limitations. Firstly, in all datasets, the candidate options are directly extracted from the context (as a single entity or a text span), which leads to the fact that lots of questions can be solved trivially via word-based search and context-matching without deeper reasoning; this constrains the types of questions as well. Secondly, answers and questions of most datasets are either crowd-sourced or automatically-generated, bringing a significant amount of noises in the datasets and limits the ceiling performance by domain experts, such as $82 \%$ for Childrens Book Test and $84 \%$ for Who-didWhat. Yet another issue in existing datasets is that the topic coverages are often biased due to the specific ways that the data were initially collected, making it hard to evaluate the ability of systems in text comprehension over a broader range of topics.</p>
<p>To address the aforementioned limitations, we constructed a new dataset by collecting a large set of questions, answers and associated passages in the English exams for middle-school and high-school Chinese students within the 12-18 age range. Those exams were designed by domain experts (instructors) for evaluating the reading comprehension ability of students, with ensured quality and broad topic coverage. Furthermore, the answers by machines or by humans can be objectively graded for evaluation</p>
<p>and comparison using the same evaluation metrics. Although efforts have been made with a similar motivation, including the MCTest dataset created by (Richardson et al., 2013) (containing 500 passages and 2000 questions) and several others (Peñas et al., 2014; Rodrigo et al., 2015; Khashabi et al., 2016; Shibuki et al., 2014), the usefulness of those datasets is significantly restricted due to their small sizes, especially not suitable for training powerful deep neural networks whose success relies on the availability of relatively large training sets.</p>
<p>Our new dataset, namely RACE, consists of 27,933 passages and 97,687 questions. After reading each passage, each student is asked to answer several questions where each question is provided with four candidate answers - only one of them is correct . Unlike existing datasets, both the questions and candidate answers in RACE are not restricted to be the text spans in the original passage; instead, they can be described in any words. A sample from our dataset is presented in Table 1.</p>
<p>Our latter analysis shows that correctly answering a large portion of questions in RACE requires the ability of reasoning, the most important feature as a machine comprehension dataset (Chen et al., 2016). RACE also offers two important subdivisions of the reasoning types in its questions, namely passage summarization and attitude analysis, which have not been introduced by the any of the existing large-scale datasets to our knowledge.</p>
<p>In addition, compared to other existing datasets where passages are either domain-specific or of a single fixed style (namely news stories for CNN/Daily Mail, NEWSQA and Who-did-What, fiction stories for Children's Book Test and Book Test, and Wikipedia articles for SQUAD), passages in RACE almost cover all types of human articles, such as news, stories, ads, biography, philosophy, etc., in a variety of styles. This comprehensiveness of topic/style coverage makes RACE a desirable resource for evaluating the reading comprehension ability of machine learning systems in general.</p>
<p>The advantages of our proposed dataset over existing large datasets in machine reading comprehension can be summarized as follows:</p>
<ul>
<li>All questions and candidate options are generated by human experts, which are intentionally designed to test human agent's ability in reading comprehension. This makes RACE a relatively accurate indicator for reflecting the
text comprehension ability of machine learning systems under human judge.</li>
<li>The questions are substantially more difficult than those in existing datasets, in terms of the large portion of questions involving reasoning. At the meantime, it is also sufficiently large to support the training of deep learning models.</li>
<li>Unlike existing large-scale datasets, candidate options in RACE are human generated sentences which may not appear in the original passage. This makes the task more challenging and allows a rich type of questions such as passage summarization and attitude analysis.</li>
<li>Broad coverage in various domains and writing styles: a desirable property for evaluating generic (in contrast to domain/style-specific) comprehension ability of learning models.</li>
</ul>
<h2>2 Related Work</h2>
<p>In this section, we briefly outline existing datasets for the machine reading comprehension task, including their strengths and weaknesses.</p>
<h3>2.1 MCTest</h3>
<p>MCTest (Richardson et al., 2013) is a popular dataset for question answering in the same format as RACE, where each question is associated with four candidate answers with a single correct answer. Although questions in MCTest are of high-quality ensured by careful examinations through crowdsourcing, it contains only 500 stores and 2000 questions, which substantially restricts its usage in training advanced machine comprehension models. Moreover, while MCTest is designed for 7 years old children, RACE is constructed for middle and high school students at 12-18 years old hence is more complicated and requires stronger reasoning skills. In other words, RACE can be viewed as a larger and more difficult version of the MCTest dataset.</p>
<h3>2.2 Cloze-style datasets</h3>
<p>The past few years have witnessed several largescale cloze-style datasets (Hermann et al., 2015; Hill et al., 2015; Bajgar et al., 2016; Onishi et al., 2016), whose questions are formulated by obliterating a word or an entity in a sentence.</p>
<h1>Passage:</h1>
<p>In a small village in England about 150 years ago, a mail coach was standing on the street. It didn't come to that village often. People had to pay a lot to get a letter. The person who sent the letter didn't have to pay the postage, while the receiver had to. "Here's a letter for Miss Alice Brown," said the mailman.
"I'm Alice Brown," a girl of about 18 said in a low voice.
Alice looked at the envelope for a minute, and then handed it back to the mailman.
"I'm sorry I can't take it, I don't have enough money to pay it", she said.
A gentleman standing around were very sorry for her. Then he came up and paid the postage for her.
When the gentleman gave the letter to her, she said with a smile, " Thank you very much, This letter is from Tom. I'm going to marry him. He went to London to look for work. I've waited a long time for this letter, but now I don't need it, there is nothing in it."
"Really? How do you know that?" the gentleman said in surprise.
"He told me that he would put some signs on the envelope. Look, sir, this cross in the corner means that he is well and this circle means he has found work. That's good news."
The gentleman was Sir Rowland Hill. He didn't forgot Alice and her letter.
"The postage to be paid by the receiver has to be changed," he said to himself and had a good plan.
"The postage has to be much lower, what about a penny? And the person who sends the letter pays the postage. He has to buy a stamp and put it on the envelope." he said. The government accepted his plan. Then the first stamp was put out in 1840. It was called the "Penny Black". It had a picture of the Queen on it.</p>
<h2>Questions:</h2>
<p>1): The first postage stamp was made ..
A. in England B. in America C. by Alice D. in 1910
2): The girl handed the letter back to the mailman because ..
A. she didn't know whose letter it was
B. she had no money to pay the postage
C. she received the letter but she didn't want to open it
D. she had already known what was written in the letter
3): We can know from Alice's words that . .
A. Tom had told her what the signs meant before leaving
B. Alice was clever and could guess the meaning of the signs
C. Alice had put the signs on the envelope herself
D. Tom had put the signs as Alice had told him to
4): The idea of using stamps was thought of by . . A. the government
B. Sir Rowland Hill
C. Alice Brown
D. Tom
5): From the passage we know the high postage made . . A. people never send each other letters
B. lovers almost lose every touch with each other
C. people try their best to avoid paying it
D. receivers refuse to pay the coming letters</p>
<p>Answer: ADABC</p>
<p>Table 1: Sample reading comprehension problems from our dataset.</p>
<p>CNN/Daily Mail (Hermann et al., 2015) are the largest machine comprehension datasets with 1.4 M questions. However, both require limited reasoning ability (Chen et al., 2016). In fact, the best machine performance obtained by researchers (Chen et al., 2016; Dhingra et al., 2016) is close to human's performance on CNN/Daily Mail.</p>
<p>Childrens Book Test (CBT) (Hill et al., 2015) and Book Test (BT) (Bajgar et al., 2016) are constructed in a similar manner. Each passage in CBT consist of 20 contiguous sentences extracted from children's books and the next ( 21 st) sentence is used to make the question. The main difference between the two datasets is the size of BT being 60 times larger. Machine comprehension models have also matched human performance on CBT (Bajgar et al., 2016).</p>
<p>Who Did What (WDW) (Onishi et al., 2016) is yet another cloze-style dataset constructed from the LDC English Gigaword newswire corpus. The authors generate passages and questions by picking two news articles describing the same event,
using one as the passage and the other as the question.</p>
<p>High noise is inevitable in cloze-style datasets due to their automatic generation process, which is reflected in the human performance on these datasets: $82 \%$ for CBT and $84 \%$ for WDW.</p>
<h3>2.3 Datasets with Span-based Answers</h3>
<p>In datasets such as SQUAD (Rajpurkar et al., 2016), NEWSQA (Trischler et al., 2016) and MS MARCO (Nguyen et al., 2016), the answer to each question is in the form of a text span in the article. Articles of SQUAD, NEWSQA and MS MARCO come from Wikipedia, CNN news and the Bing search engine respectively. The answer to a certain question may not be unique and could be multiple spans. Instead of evaluating the accuracy, researchers need to use F1 score, BLEU (Papineni et al., 2002) or ROUGE (Lin and Hovy, 2003) as metrics, which measure the overlap between the prediction and ground truth answers since the questions come without candidate spans.</p>
<p>Datasets with span-based answers are challenging as the space of possible spans is usually large. However, restricting answers to be text spans in the context passage may be unrealistic and more importantly, may not be intuitive even for humans, indicated by the suffered human performance of $80.3 \%$ on SQUAD (or $65 \%$ claimed by Trischler et al. (2016)) and $46.5 \%$ on NEWSQA. In other words, the format of span-based answers may not necessarily be a good examination of reading comprehension of machines whose aim is to approach the comprehension ability of humans.</p>
<h3>2.4 Datasets from Examinations</h3>
<p>There have been several datasets extracted from examinations, aiming at evaluating systems under the same conditions as how humans are evaluated in schools. E.g., the AI2 Elementary School Science Questions dataset (Khashabi et al., 2016) contains 1080 questions for students in elementary schools; NTCIR QA Lab (Shibuki et al., 2014) evaluates systems by the task of solving real-world university entrance exam questions; The Entrance Exams task at CLEF QA Track (Peñas et al., 2014; Rodrigo et al., 2015) evaluates the system's reading comprehension ability. However, data provided in these existing tasks are far from sufficient for the training of advanced data-driven machine reading models, partially due to the expensive data generation process by human experts.</p>
<p>To the best of our knowledge, RACE is the first large-scale dataset of this type, where questions are created based on exams designed to evaluate human performance in reading comprehension.</p>
<h2>3 Data Analysis</h2>
<p>In this section, we study the nature of questions covered in RACE at a detailed level. Specifically, we present the dataset statistics in Section 3.1, and then analyze different reasoning/question types in RACE in the remaining subsections.</p>
<h3>3.1 Dataset Statistics</h3>
<p>As mentioned in section 1, RACE is collected from English examinations designed for 12-15 year-old middle school students, and 15-18 yearold high school students in China. To distinguish the two subgroups with drastic difficulty gap, RACE-M denotes the middle school examinations and RACE-H denotes high school examinations. We split 5\% data as the development set
and $5 \%$ as the test set for RACE-M and RACE-H respectively. The number of samples in each set is shown in Table 2. The statistics for RACE-M and RACE-H is summarized in Table 3. We can find that the length of the passages and the vocabulary size in the RACE-H are much larger than that of the RACE-M, an evidence of the higher difficulty of high school examinations.</p>
<p>However, notice that since the articles and questions are selected and designed to test Chinese students learning English as a foreign language, the vocabulary size and the complexity of the language constructs are simpler than news articles and Wikipedia articles in other QA datasets.</p>
<h3>3.2 Reasoning Types of the Questions</h3>
<p>To get a comprehensive picture about the reasoning difficulty requirement of RACE, we conduct human annotations of questions types. Following Chen et al. (2016); Trischler et al. (2016), we stratify the questions into five classes as follows with ascending order of difficulty:</p>
<ul>
<li>Word matching: The question exactly matches a span in the article. The answer is self-evident.</li>
<li>Paraphrasing: The question is entailed or paraphrased by exactly one sentence in the passage. The answer can be extracted within the sentence.</li>
<li>Single-sentence reasoning: The answer could be inferred from a single sentence of the article by recognizing incomplete information or conceptual overlap.</li>
<li>Multi-sentence reasoning: The answer must be inferred from synthesizing information distributed across multiple sentences.</li>
<li>Insufficient/Ambiguous: The question has no answer or the answer is not unique based on the given passage.</li>
</ul>
<p>We refer readers to (Chen et al., 2016; Trischler et al., 2016) for examples of each category.</p>
<p>To obtain the proportion of different question types, we sample 100 passages from RACE ( 50 from RACE-M and 50 from RACE-H), all of which have 5 questions hence there are 500 questions in total. We put the passages on Amazon Mechanical Turk ${ }^{1}$, and a Hit is generated by a passage</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>| Dataset | RACE-M |  |  | RACE-H |  |  | RACE |  |  |  |
| Subset | Train | Dev | Test | Train | Dev | Test | Train | Dev | Test | All |
| :-- | --: | --: | --: | --: | --: | --: | --: | --: | --: | --: |
| # passages | 6,409 | 368 | 362 | 18,728 | 1,021 | 1,045 | 25,137 | 1,389 | 1,407 | 27,933 |
| # questions | 25,421 | 1,436 | 1,436 | 62,445 | 3,451 | 3,498 | 87,866 | 4,887 | 4,934 | 97,687 |</p>
<p>Table 2: The separation of the training, development and test sets of RACE-M,RACE-H and RACE</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Dataset</th>
<th style="text-align: right;">RACE-M</th>
<th style="text-align: right;">RACE-H</th>
<th style="text-align: right;">RACE</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Passage Len</td>
<td style="text-align: right;">231.1</td>
<td style="text-align: right;">353.1</td>
<td style="text-align: right;">321.9</td>
</tr>
<tr>
<td style="text-align: left;">Question Len</td>
<td style="text-align: right;">9.0</td>
<td style="text-align: right;">10.4</td>
<td style="text-align: right;">10.0</td>
</tr>
<tr>
<td style="text-align: left;">Option Len</td>
<td style="text-align: right;">3.9</td>
<td style="text-align: right;">5.8</td>
<td style="text-align: right;">5.3</td>
</tr>
<tr>
<td style="text-align: left;">Vocab size</td>
<td style="text-align: right;">32,811</td>
<td style="text-align: right;">125,120</td>
<td style="text-align: right;">136,629</td>
</tr>
</tbody>
</table>
<p>Table 3: Statistics of RACE where Len denotes length and Vocab denotes Vocabulary.
with 5 questions. Each question is labeled by two crowdworkers. We require the turkers to both answer the questions and label the reasoning type. We pay $\$ 0.70$ and $\$ 1.00$ per passage in RACE-M and RACE-H respectively, and restrict the access to master turkers only. Finally, we get 1000 labels for the 500 questions.</p>
<p>The statistics about the reasoning type is summarized in Table 4. The higher difficulty level of RACE is justified by its higher ratio of reasoning questions in comparison to CNN, SQUAD and NEWSQA. Specifically, $59.2 \%$ questions of RACE are either in the category of single-sentence reasoning or in the category of multi-sentence reasoning, while the ratio is $21 \%, 20.5 \%$ and $33.9 \%$ for CNN, SQUAD and NEWSQA respectively. Also notice that the ratio of word matching questions on RACE is only $15.8 \%$, the lowest among several categories. In addition, questions in RACE-H are more complex than questions in RACE-M since RACE-M has more word matching questions and fewer reasoning questions.</p>
<h3>3.3 Subdividing Reasoning Types</h3>
<p>To better understand our dataset and facilitate future research, we list the subdivisions of questions under the reasoning category. We find the most frequent reasoning subdivisions include: detail reasoning, whole-picture understanding, passage summarization, attitude analysis and world knowledge. One question may fall into multiple divisions. Definition of these subdivisions and their associated examples are as follows:</p>
<ol>
<li>Detail reasoning: to answer the question, the agent should be clear about the details of the pas-
sage. The answer appears in the passage but it cannot be found by simply matching the question with the passage. For example, Question 1 in the sample passage falls into this category.</li>
<li>Whole-picture reasoning: the agent needs to understand the whole picture of the story to obtain the correct answer. For example, to answer the Question 2 in the sample passage, the agent is required to comprehend the entire story.</li>
<li>Passage summarization: The question requires the agent to select the best summarization of the passage among four candidate summarizations. A typical question of this type is "The main idea of this passage is __". An example question can be found in Appendix A.1.</li>
<li>
<p>Attitude analysis: The question asks about the opinions/attitudes of the author or a character in the story towards somebody or something, e.g.,
<sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<ul>
<li>Question: What was the author's attitude towards the industry awards for quieter?</li>
<li>
<p>Options: A.suspicious B.positive C.enthusiastic D.indifferent</p>
</li>
<li>
<p>World knowledge: Certain external knowledge is needed. Most frequent questions under this category involve simple arithmetic.</p>
</li>
<li>Evidence: "The park is open from 8 am to 5 pm ."</li>
<li>Question: The park is open for $\qquad$ hours a day.</li>
<li>Options: A.eight B.nine C.ten D.eleven</li>
</ul>
<p>To the best of our knowledge, questions like passage summarization and attitude analysis have not been introduced by any of the existing largescale machine comprehension datasets. Both are crucial components in evaluating humans' reading comprehension abilities.</p>
</li>
</ol>
<table>
<thead>
<tr>
<th style="text-align: left;">Dataset</th>
<th style="text-align: center;">RACE-M</th>
<th style="text-align: center;">RACE-H</th>
<th style="text-align: center;">RACE</th>
<th style="text-align: center;">CNN</th>
<th style="text-align: center;">SQUAD</th>
<th style="text-align: center;">NEWSQA</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Word Matching</td>
<td style="text-align: center;">$29.4 \%$</td>
<td style="text-align: center;">$11.3 \%$</td>
<td style="text-align: center;">$15.8 \%$</td>
<td style="text-align: center;">$13.0 \%^{\dagger}$</td>
<td style="text-align: center;">$39.8 \%^{*}$</td>
<td style="text-align: center;">$32.7 \%^{*}$</td>
</tr>
<tr>
<td style="text-align: left;">Paraphrasing</td>
<td style="text-align: center;">$14.8 \%$</td>
<td style="text-align: center;">$20.6 \%$</td>
<td style="text-align: center;">$19.2 \%$</td>
<td style="text-align: center;">$41.0 \%^{\dagger}$</td>
<td style="text-align: center;">$34.3 \%^{*}$</td>
<td style="text-align: center;">$27.0 \%^{*}$</td>
</tr>
<tr>
<td style="text-align: left;">Single-Sentence Reasoning</td>
<td style="text-align: center;">$31.3 \%$</td>
<td style="text-align: center;">$34.1 \%$</td>
<td style="text-align: center;">$33.4 \%$</td>
<td style="text-align: center;">$19.0 \%^{\dagger}$</td>
<td style="text-align: center;">$8.6 \%^{*}$</td>
<td style="text-align: center;">$13.2 \%^{*}$</td>
</tr>
<tr>
<td style="text-align: left;">Multi-Sentence Reasoning</td>
<td style="text-align: center;">$22.6 \%$</td>
<td style="text-align: center;">$26.9 \%$</td>
<td style="text-align: center;">$25.8 \%$</td>
<td style="text-align: center;">$2.0 \%^{\dagger}$</td>
<td style="text-align: center;">$11.9 \%^{*}$</td>
<td style="text-align: center;">$20.7 \%^{*}$</td>
</tr>
<tr>
<td style="text-align: left;">Ambiguous/Insufficient</td>
<td style="text-align: center;">$1.8 \%$</td>
<td style="text-align: center;">$7.1 \%$</td>
<td style="text-align: center;">$5.8 \%$</td>
<td style="text-align: center;">$25.0 \%^{\dagger}$</td>
<td style="text-align: center;">$5.4 \%^{*}$</td>
<td style="text-align: center;">$6.4 \%^{*}$</td>
</tr>
</tbody>
</table>
<p>Table 4: Statistic information about Reasoning type in different datasets. * denotes the numbers coming from (Trischler et al., 2016) based on 1000 samples per dataset, and numbers with $\dagger$ come from (Chen et al., 2016).</p>
<h2>4 Collection Methodology</h2>
<p>We collected the raw data from three large free public websites ${ }^{234}$ in China ${ }^{5}$, where the reading comprehension problems are extracted from English examinations designed by teachers in China. The data before cleaning contains 137,918 passages and 519,878 questions in total, where there are 38,159 passages with 156,782 questions in the middle school group, and 99,759 passages with 363,096 questions in the high school group.</p>
<p>The following filtering steps are conducted to clean the raw data. Firstly, we remove all problems and questions that do not have the same format as our problem setting, e.g., a question would be removed if the number of its options is not four. Secondly, we filter all articles and questions that are not self-contained based on the text information, i.e. we remove the articles and questions containing images or tables. We also remove all questions containing keywords "underlined" or "paragraph", since it is difficult to reproduce the effect of underlines and the paragraph segment information. Thirdly, we remove all duplicated articles.</p>
<p>On one of the websites (xkw.com), the answers are stored as images. We used two standard OCR programs tesseract ${ }^{6}$ and ABBYY FineReader ${ }^{7}$ to process the images. We remove all the answers that two software disagree. The OCR task is easy since we only need to recognize printed alphabet A, B, C, D with a standard font. Finally, we get the cleaned dataset RACE, with 27,933 passages and 97,687 questions.</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h2>5 Experiments</h2>
<p>In this section, we compare the performance of several state-of-the-art reading comprehension models with human performance. We use accuracy as the metric to evaluate different models.</p>
<h3>5.1 Methods for Comparison</h3>
<p>Sliding Window Algorithm Firstly, we build the rule-based baseline introduced by Richardson et al. (2013). It chooses the answer having the highest matching score. Specifically, it first concatenates the question and the answer and then calculates the TF-IDF style matching score between the concatenated sentence with every window (a span of text) of the article. The window size is decided by the model performance in the training and dev sets.</p>
<p>Stanford Attentive Reader Stanford Attentive Reader (Stanford AR) (Chen et al., 2016) is a strong model that achieves state-of-the-art results on CNN/Daily Mail. Moreover, the authors claim that their model has nearly reached the ceiling performance on these two datasets.</p>
<p>Suppose that the triple of passage, question and options is denoted by $\left(p, q, o_{1}, \ldots, 4\right)$. We first employ bidirectional GRUs to encode $p$ and $q$ respectively into $h_{1}^{p}, h_{2}^{p}, \ldots, h_{n}^{p}$ and $h^{q}$. Then we summarize the most relevant part of the passage into $s^{p}$ with an attention model. Following Chen et al. (2016), we adopt a bilinear attention form. Specifically,</p>
<p>$$
\begin{aligned}
&amp; \alpha_{i}=\operatorname{Softmax}<em i="i">{i}\left(\left(h</em>\right) \
&amp; s^{p}=\sum_{i} \alpha_{i} h_{i}^{p}
\end{aligned}
$$}^{p}\right)^{T} W_{1} h^{q</p>
<p>Similarly, we use bidirectional GRUs to encode option $o_{i}$ into a vector $h^{o_{i}}$. Finally, we compute the matching score between the $i$-th option $(i=1, \cdots, 4)$ and the summarized passage using</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">RACE-M</th>
<th style="text-align: center;">RACE-H</th>
<th style="text-align: center;">RACE</th>
<th style="text-align: center;">MCTest</th>
<th style="text-align: center;">CNN</th>
<th style="text-align: center;">DM</th>
<th style="text-align: center;">CBT-N</th>
<th style="text-align: center;">CBT-C</th>
<th style="text-align: center;">WDW</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Random</td>
<td style="text-align: center;">24.6</td>
<td style="text-align: center;">25.0</td>
<td style="text-align: center;">24.9</td>
<td style="text-align: center;">24.8</td>
<td style="text-align: center;">0.06</td>
<td style="text-align: center;">0.06</td>
<td style="text-align: center;">10.6</td>
<td style="text-align: center;">10.2</td>
<td style="text-align: center;">$32.0^{\dagger}$</td>
</tr>
<tr>
<td style="text-align: left;">Sliding Window</td>
<td style="text-align: center;">37.3</td>
<td style="text-align: center;">30.4</td>
<td style="text-align: center;">32.2</td>
<td style="text-align: center;">$51.5^{\dagger}$</td>
<td style="text-align: center;">24.8</td>
<td style="text-align: center;">30.8</td>
<td style="text-align: center;">$16.8^{\dagger}$</td>
<td style="text-align: center;">$19.6^{\dagger}$</td>
<td style="text-align: center;">$48.0^{\dagger}$</td>
</tr>
<tr>
<td style="text-align: left;">Stanford AR</td>
<td style="text-align: center;">44.2</td>
<td style="text-align: center;">43.0</td>
<td style="text-align: center;">43.3</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$73.6^{\dagger}$</td>
<td style="text-align: center;">$76.6^{\dagger}$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$64.0^{\dagger}$</td>
</tr>
<tr>
<td style="text-align: left;">GA</td>
<td style="text-align: center;">43.7</td>
<td style="text-align: center;">44.2</td>
<td style="text-align: center;">44.1</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$77.9^{\dagger}$</td>
<td style="text-align: center;">$80.9^{\dagger}$</td>
<td style="text-align: center;">$70.1^{\dagger}$</td>
<td style="text-align: center;">$67.3^{\dagger}$</td>
<td style="text-align: center;">$71.2^{\dagger}$</td>
</tr>
<tr>
<td style="text-align: left;">Turkers</td>
<td style="text-align: center;">85.1</td>
<td style="text-align: center;">69.4</td>
<td style="text-align: center;">73.3</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">Ceiling Performance</td>
<td style="text-align: center;">95.4</td>
<td style="text-align: center;">94.2</td>
<td style="text-align: center;">94.5</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$81.6^{\dagger}$</td>
<td style="text-align: center;">$81.6^{\dagger}$</td>
<td style="text-align: center;">$84^{\dagger}$</td>
</tr>
</tbody>
</table>
<p>Table 5: Accuracy of models and human on the each dataset, where $\dagger$ denotes the results coming from previous publications. DM denotes Daily Mail and WDW denotes Who-Did-What .
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Test accuracy of different baselines on each reasoning type category introduced in Section 3.2, where Word-Match, Single-Reason, Multi-Reason and Ambiguous are the abbreviations for Word matching, Single-sentence Reasoning, Multi-sentence Reasoning and Insufficient/Ambiguous respectively.
a bilinear attention. We pass the scores through softmax to get a probability distribution. Specifically, the probability of option $i$ being the right answer is calculated as</p>
<p>$$
p_{i}=\operatorname{Softmax}<em i="i">{i}\left(h^{o</em>\right)
$$}} W_{2} s^{d</p>
<p>Gated-Attention Reader Gated AR (Dhingra et al., 2016) is the state-of-the-art model on multiple datasets. To build query-specific representations of tokens in the document, it employs an attention mechanism to model multiplicative interactions between the query embedding and the document representation. With a multi-hop architecture, GA also enables a model to scan the document and the question iteratively for multiple passes. In other words, the multi-hop structure makes it possible for the reader to refine token representations iteratively and the attention mechanism find the most relevant part of the document. We refer readers to (Dhingra et al., 2016) for more details.</p>
<p>After obtaining a query specific document representation $s^{d}$, we use the same method as bilinear operation listed in Equation 2 to get the output.</p>
<p>Note that our implementation slightly differs from the original GA reader. Specifically, the Attention Sum layer is not applied at the final layer and no character-level embeddings are used.</p>
<p>Implementation Details We follow Chen et al. (2016) in our experiment settings. The vocabulary size is set to 50 k . We choose word embedding size $d=100$ and use the 100 -dimensional Glove word embedding (Pennington et al., 2014) as embedding initialization. GRU weights are initialized from Gaussian distribution $\mathcal{N}(0,0.1)$. Other parameters are initialized from a uniform distribution on $(-0.01,0.01)$. The hidden dimensionality is set to 128 and the number of layers is set to one for both Stanford AR and GA. We use vanilla stochastic gradient descent (SGD) to train our models. We apply dropout on word embeddings and the gradient is clipped when the norm</p>
<p>of the gradient is larger than 10 . We use a grid search on validation set to choose the learning rate within ${0.05,0.1,0.3,0.5}$ and dropout rate within ${0.2,0.5,0.7}$. The highest accuracy on validation set is obtained by setting learning rate to 0.1 for Stanford AR and 0.3 for GA and dropout rate to 0.5 . The data of RACE-M and RACE-H is used together to train our model and testing is performed separately.</p>
<h3>5.2 Human Evaluation</h3>
<p>As described in section 3.2, a randomly sampled subset of test set has been labeled by Amazon Turkers, which contains 500 questions with half from RACE-H and with the other half from RACE-M. The turkers' performance is $85 \%$ for RACE-M and 70\% for RACE-H. However, it is hard to guarantee that every turker performs the survey carefully, given the difficult and long passages of high school problems. Therefore, to obtain the ceiling human performance on RACE, we manually labeled the proportion of valid questions. A question is valid if it is unambiguous and has a correct answer. We found that $94.5 \%$ of the data is valid, which sets the ceiling human performance. Similarly, the ceiling performance on RACE-M and RACE-H is $95.4 \%$ and $94.2 \%$ respectively.</p>
<h3>5.3 Main Results</h3>
<p>We compare models' and human ceiling performance on datasets which have the same evaluation metric with RACE. The compared datasets include RACE, MCTest, CNN/Daily Mail (CNN and DM), CBT and WDW. On CBT, we report performance on two subsets where the missing token is either a common noun (CBT-C) or name entity (CBT-N) since the language models have already reached human-level performance on other types (Hill et al., 2015). The comparison is shown in Table 5.</p>
<p>Performance of Sliding Window We first compare MCTest with RACE using Sliding Window, where it is unable to train Stanford AR and Gated AR on MCTest's limited training data. Sliding Window achieves an accuracy of $51.5 \%$ on MCTest while only $37.3 \%$ on RACE, meaning that to answer the questions of RACE requires more reasoning than MCTest.</p>
<p>The performance of sliding window on RACE is not directly comparable with CBT and WDW
since CBT has ten candidate answers for each question and WDW has an average of three. Instead, we evaluate the performance improvement of sliding window on the random baseline. Larger improvement indicates more questions solvable by simple matching. On RACE, Sliding Window is $28.6 \%$ better than the random baseline, while the improvement is $58.5 \%, 92.2 \%$ and $50 \%$ for CBTN, CBT-C and WDW.</p>
<p>The accuracy on RACE-M (37.3\%) and RACEH (30.4\%) indicates that the middle school questions are simpler based on the matching algorithm.</p>
<p>Performance of Neural Models We further compare the difficulty of different datasets by state-of-the-art neural models' performance. A lower performance means that more problems are unsolvable by machines. The Stanford AR and Gated AR achieve an accuracy of only $43.3 \%$ and $44.1 \%$ on RACE while their accuracy is much higher on CNN/Daily Mail, Childrens Book Test and Who-Did-What. It justifies the fact that, among current large-scale machine comprehension datasets, RACE is the most challenging one.</p>
<p>Human Ceiling Performance The human performance is $94.5 \%$ which shows our data is quite clean compared to other large-scale machine comprehension datasets. Since we cannot enforce every turker do the test cautiously, the result shows a gap between turkers' performance and human performance. Reasonably, problems in the high school group with longer passages and more complex questions lead to more significant divergence. Nevertheless, the start-of-the-art models still have a large room to be improved to reach turkers' performance. The performance gap is $41 \%$ for the middle school problems and $25 \%$ for the high school problems. What's more, The performance of Stanford AR and GA is only less than a half of the ceiling human performance, which indicates that to match the humans' reading comprehension ability, we still have a long way to go.</p>
<h3>5.4 Reason Types Analysis</h3>
<p>We evaluate human and models on different types of questions, shown in Figure 1. Turkers do the best on word matching problems while doing the worst on reasoning problems. Sliding window performs better on word matching than problems needing reasoning or paraphrasing. Surprisingly, Stanford AR does not have a stronger performance</p>
<p>on the word matching category than reasoning categories. A possible reason is that the proportion of data in reasoning categories is larger than that of data. Also, the candidate answers of simple matching questions may share similar word embeddings. For example, if the question is about color, it is difficult to distinguish candidate answers, "green", "red", "blue" and "yellow", in the embedding vector space. The similar performance on different categories also explains the reason that the performance of the neural models is close in the middle and high school groups in Table 5.</p>
<h2>6 Conclusion</h2>
<p>We introduce a large, high-quality dataset for reading comprehension that is carefully designed to examine human ability on this task. Some desirable properties of RACE include the broad coverage of domains/styles and the richness in the question format. Most importantly, it requires substantially more reasoning to do well on RACE than on other datasets, as there is a significant gap between the performance of state-of-the-art machine comprehension models and that of the human. We hope this dataset will stimulate the development of more advanced machine comprehension models.</p>
<h2>Acknowledgement</h2>
<p>We would like to thank Graham Neubig for suggestions on the draft and Diyi Yang's help on obtaining the crowdsourced labels.</p>
<p>This research was supported in part by DARPA grant FA8750-12-2-0342 funded under the DEFT program.</p>
<h2>References</h2>
<p>Ondrej Bajgar, Rudolf Kadlec, and Jan Kleindienst. 2016. Embracing data abundance: Booktest dataset for reading comprehension. arXiv preprint arXiv:1610.00956 .</p>
<p>Danqi Chen, Jason Bolton, and Christopher D Manning. 2016. A thorough examination of the cnn/daily mail reading comprehension task. arXiv preprint arXiv:1606.02858 .</p>
<p>Bhuwan Dhingra, Hanxiao Liu, William W Cohen, and Ruslan Salakhutdinov. 2016. Gated-attention readers for text comprehension. arXiv preprint arXiv:1606.01549 .</p>
<p>Karl Moritz Hermann, Tomas Kocisky, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Suleyman, and Phil Blunsom. 2015. Teaching ma-
chines to read and comprehend. In Advances in Neural Information Processing Systems. pages 16931701 .</p>
<p>Felix Hill, Antoine Bordes, Sumit Chopra, and Jason Weston. 2015. The goldilocks principle: Reading children's books with explicit memory representations. arXiv preprint arXiv:1511.02301 .</p>
<p>Rudolf Kadlec, Martin Schmid, Ondrej Bajgar, and Jan Kleindienst. 2016. Text understanding with the attention sum reader network. arXiv preprint arXiv:1603.01547 .</p>
<p>Daniel Khashabi, Tushar Khot, Ashish Sabharwal, Peter Clark, Oren Etzioni, and Dan Roth. 2016. Question answering via integer programming over semi-structured knowledge. arXiv preprint arXiv:1604.06076 .</p>
<p>Chin-Yew Lin and Eduard Hovy. 2003. Automatic evaluation of summaries using n-gram cooccurrence statistics. In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology-Volume 1. Association for Computational Linguistics, pages 71-78.</p>
<p>Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan Majumder, and Li Deng. 2016. Ms marco: A human generated machine reading comprehension dataset. arXiv preprint arXiv:1611.09268 .</p>
<p>Takeshi Onishi, Hai Wang, Mohit Bansal, Kevin Gimpel, and David McAllester. 2016. Who did what: A large-scale person-centered cloze dataset. arXiv preprint arXiv:1608.05457 .</p>
<p>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting on association for computational linguistics. Association for Computational Linguistics, pages 311-318.</p>
<p>Anselmo Peñas, Yusuke Miyao, Álvaro Rodrigo, Eduard H Hovy, and Noriko Kando. 2014. Overview of clef qa entrance exams task 2014. In CLEF (Working Notes). pages 1194-1200.</p>
<p>Jeffrey Pennington, Richard Socher, and Christopher D Manning. 2014. Glove: Global vectors for word representation. In EMNLP. volume 14, pages 15321543 .</p>
<p>Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. Squad: 100,000+ questions for machine comprehension of text. arXiv preprint arXiv:1606.05250 .</p>
<p>Matthew Richardson, Christopher JC Burges, and Erin Renshaw. 2013. Mctest: A challenge dataset for the open-domain machine comprehension of text. In EMNLP. volume 3, page 4.</p>
<p>Álvaro Rodrigo, Anselmo Peñas, Yusuke Miyao, Eduard H Hovy, and Noriko Kando. 2015. Overview of clef qa entrance exams task 2015. In CLEF (Working Notes).</p>
<p>Hideyuki Shibuki, Kotaro Sakamoto, Yoshinobu Kano, Teruko Mitamura, Madoka Ishioroshi, Kelly Y Itakura, Di Wang, Tatsunori Mori, and Noriko Kando. 2014. Overview of the ntcir-11 qa-lab task. In NTCIR.</p>
<p>Adam Trischler, Tong Wang, Xingdi Yuan, Justin Harris, Alessandro Sordoni, Philip Bachman, and Kaheer Suleman. 2016. Newsqa: A machine comprehension dataset. arXiv preprint arXiv:1611.09830 .</p>
<p>Zhilin Yang, Junjie Hu, Ruslan Salakhutdinov, and William W Cohen. 2017. Semi-supervised qa with generative domain-adaptive nets. arXiv preprint arXiv:1702.02206 .</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{2}$ http://www.21cnjy.com/
${ }^{3}$ http://5utk.kx5u.com/
${ }^{4}$ http://zujuan.xkw.com/
${ }^{5}$ We checked that our dataset does not include example questions of exams with copyright, such as SSAT, SAT, TOEFL and GRE.
${ }^{6}$ https://github.com/tesseract-ocr
${ }^{7}$ https://www.abbyy.com/FineReader&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
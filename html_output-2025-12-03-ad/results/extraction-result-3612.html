<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3612 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3612</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3612</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-89.html">extraction-schema-89</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) or related AI systems being used to distill, extract, or induce qualitative laws, rules, or scientific principles from large collections of scholarly or scientific papers.</div>
                <p><strong>Paper ID:</strong> paper-265157731</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2311.08189v3.pdf" target="_blank">All Data on the Table: Novel Dataset and Benchmark for Cross-Modality Scientific Information Extraction</a></p>
                <p><strong>Paper Abstract:</strong> Extracting key information from scientific papers has the potential to help researchers work more efficiently and accelerate the pace of scientific progress. Over the last few years, research on Scientific Information Extraction (SciIE) witnessed the release of several new systems and benchmarks. However, existing paper-focused datasets mostly focus only on specific parts of a manuscript (e.g., abstracts) and are single-modality (i.e., text- or table-only), due to complex processing and expensive annotations. Moreover, core information can be present in either text or tables or across both. To close this gap in data availability and enable cross-modality IE, while alleviating labeling costs, we propose a semi-supervised pipeline for annotating entities in text, as well as entities and relations in tables, in an iterative procedure. Based on this pipeline, we release novel resources for the scientific community, including a high-quality benchmark, a large-scale corpus, and a semi-supervised annotation pipeline. We further report the performance of state-of-the-art IE models on the proposed benchmark dataset, as a baseline. Lastly, we explore the potential capability of large language models such as ChatGPT for the current task. Our new dataset, results, and analysis validate the effectiveness and efficiency of our semi-supervised pipeline, and we discuss its remaining limitations.</p>
                <p><strong>Cost:</strong> 0.012</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3612.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3612.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) or related AI systems being used to distill, extract, or induce qualitative laws, rules, or scientific principles from large collections of scholarly or scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChatGPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChatGPT (OpenAI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large pretrained conversational language model used in this paper in few-shot in-context learning (ICL) prompts to perform scientific information extraction (NER and table RE) from research papers; evaluated against a semi-supervised extraction pipeline and fine-tuned IE models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGPT</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Described in the paper as an LLM pre-trained on massive corpora and accessed via the official API; used in few-shot in-context learning (1-shot and 2-shot) prompting to produce entity and relation extraction outputs over scientific text and tables.</td>
                        </tr>
                        <tr>
                            <td><strong>input_domain</strong></td>
                            <td>Scholarly scientific papers (domains evaluated: Computer Science, Statistics, Electrical Engineering & Systems Science; general scientific literature via SCICM benchmark).</td>
                        </tr>
                        <tr>
                            <td><strong>corpus_size</strong></td>
                            <td>Evaluated on the SCICM benchmark which includes a large auto-annotated corpus of 12,817 papers and a manually reviewed test set (IID & OOD papers drawn from CS, STAT, EESS); per-paper averages reported (e.g., ~279 sentences and ~3.9 tables per test paper).</td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td>Not applicable — the paper uses ChatGPT to extract named entities and table relations (NER, table RE), not to distill higher-level qualitative scientific laws, heuristics, or principles.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Few-shot in-context learning (ICL) prompting (1-shot and 2-shot) with carefully designed prompt templates and demonstration examples to elicit entity and relation extraction from paper text and tables.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Compared ChatGPT outputs against expert-reviewed annotations on the SCICM benchmark using standard IE metrics (precision, recall, F1); manual error analysis categorizing missing entities/relations, incorrect types, undefined types, and format/errors.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>ChatGPT was capable of producing entity and relation extraction outputs under few-shot prompts but performed substantially worse than fine-tuned, task-specific IE models on text NER and table NER; it achieved lower F1 scores overall on NER tasks, though it achieved a competitive F1 on table RE (reported 47.7 F1) while being much slower (0.2 tables/s) compared to the pipeline. The paper reports that 2-shot ICL generally improved over 1-shot but still did not match fine-tuned models.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Observed issues include missing entities/relations, unannotated spurious outputs, incorrect entity types, outputs with undefined types, sensitivity to prompt design, hallucination and overconfidence, frequency bias (favoring frequent entity types), slower inference speed compared to the automated pipeline, and poorer overall extraction accuracy relative to fine-tuned extractors.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'All Data on the Table: Novel Dataset and Benchmark for Cross-Modality Scientific Information Extraction', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3612.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3612.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) or related AI systems being used to distill, extract, or induce qualitative laws, rules, or scientific principles from large collections of scholarly or scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Galactica</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Galactica</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A scientific large language model cited in the paper as an example of domain-focused LLMs intended to support scientific scenarios such as citation suggestion, answering scientific questions, and writing scientific code.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Galactica</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Mentioned as a scientific LLM (cited as Taylor et al., 2022) that is trained for science-related tasks; paper does not provide architecture or size details beyond this citation mention.</td>
                        </tr>
                        <tr>
                            <td><strong>input_domain</strong></td>
                            <td>Scientific literature / scholarly content (general scientific domain as per cited description).</td>
                        </tr>
                        <tr>
                            <td><strong>corpus_size</strong></td>
                            <td>Not specified in this paper (only referenced as an example of a scientific LLM).</td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td>Not applicable in this paper — Galactica is cited as enabling practical science scenarios (citation suggestion, Q&A, code generation), not described as being used to distill qualitative scientific laws within this work.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Not applicable in this paper — Galactica is only referenced in related-work context; no method described here for distilling laws or principles.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Not evaluated in this paper (only mentioned in related work as an example of scientific LLMs).</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>No results reported in this paper regarding Galactica performing law/rule distillation; it is cited to motivate the broader context that scientific LLMs exist and can support practical scientific tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>The paper notes general concerns about LLMs (including hallucination, frequency bias, overconfidence) and suggests that SciIE plus retrieval/QA systems can help alleviate such problems, but does not report Galactica-specific limitations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'All Data on the Table: Novel Dataset and Benchmark for Cross-Modality Scientific Information Extraction', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3612.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3612.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) or related AI systems being used to distill, extract, or induce qualitative laws, rules, or scientific principles from large collections of scholarly or scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Mozi</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mozi</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A recently developed scientific large-scale language model referenced in the paper as an example of domain-specialized LLMs enabling practical scientific use cases.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Mozi</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Cited in the paper (Lan et al., 2023) as a scientific large-scale language model; the present paper only references Mozi as part of the motivation for scientific LLMs and does not provide architecture, size, or implementation details.</td>
                        </tr>
                        <tr>
                            <td><strong>input_domain</strong></td>
                            <td>Scientific literature / scholarly content (as per citation context in the paper).</td>
                        </tr>
                        <tr>
                            <td><strong>corpus_size</strong></td>
                            <td>Not specified in this paper (Mozi is only mentioned in related work).</td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td>Not applicable — Mozi is mentioned as enabling scientific scenarios (e.g., suggesting citations, answering questions, writing code) but not described as being used to extract or induce qualitative laws in this work.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Not applicable in this paper — Mozi is referenced only in related-work context; no distillation procedure is described here.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Not evaluated in this paper (only mentioned in related work).</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>No results reported in this paper about Mozi performing law/rule distillation; Mozi is cited to exemplify scientific LLM development and potential applications.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>General LLM concerns discussed in the paper (e.g., hallucination, need for verification) apply broadly; no Mozi-specific evaluation or failure modes are reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'All Data on the Table: Novel Dataset and Benchmark for Cross-Modality Scientific Information Extraction', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Is information extraction solved by ChatGPT? An analysis of performance, evaluation criteria, robustness and errors <em>(Rating: 2)</em></li>
                <li>Evaluating ChatGPT's information extraction capabilities: An assessment of performance, explainability, calibration, and faithfulness <em>(Rating: 2)</em></li>
                <li>Galactica: A large language model for science <em>(Rating: 2)</em></li>
                <li>Mozi: A scientific large-scale language model <em>(Rating: 2)</em></li>
                <li>Fine-tuned LLMs know more, hallucinate less with few-shot sequence-to-sequence semantic parsing over Wikidata <em>(Rating: 1)</em></li>
                <li>TIARA: Multi-grained retrieval for robust question answering over large knowledge base <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3612",
    "paper_id": "paper-265157731",
    "extraction_schema_id": "extraction-schema-89",
    "extracted_data": [
        {
            "name_short": "ChatGPT",
            "name_full": "ChatGPT (OpenAI)",
            "brief_description": "A large pretrained conversational language model used in this paper in few-shot in-context learning (ICL) prompts to perform scientific information extraction (NER and table RE) from research papers; evaluated against a semi-supervised extraction pipeline and fine-tuned IE models.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "ChatGPT",
            "model_description": "Described in the paper as an LLM pre-trained on massive corpora and accessed via the official API; used in few-shot in-context learning (1-shot and 2-shot) prompting to produce entity and relation extraction outputs over scientific text and tables.",
            "input_domain": "Scholarly scientific papers (domains evaluated: Computer Science, Statistics, Electrical Engineering & Systems Science; general scientific literature via SCICM benchmark).",
            "corpus_size": "Evaluated on the SCICM benchmark which includes a large auto-annotated corpus of 12,817 papers and a manually reviewed test set (IID & OOD papers drawn from CS, STAT, EESS); per-paper averages reported (e.g., ~279 sentences and ~3.9 tables per test paper).",
            "law_type": "Not applicable — the paper uses ChatGPT to extract named entities and table relations (NER, table RE), not to distill higher-level qualitative scientific laws, heuristics, or principles.",
            "distillation_method": "Few-shot in-context learning (ICL) prompting (1-shot and 2-shot) with carefully designed prompt templates and demonstration examples to elicit entity and relation extraction from paper text and tables.",
            "evaluation_method": "Compared ChatGPT outputs against expert-reviewed annotations on the SCICM benchmark using standard IE metrics (precision, recall, F1); manual error analysis categorizing missing entities/relations, incorrect types, undefined types, and format/errors.",
            "results_summary": "ChatGPT was capable of producing entity and relation extraction outputs under few-shot prompts but performed substantially worse than fine-tuned, task-specific IE models on text NER and table NER; it achieved lower F1 scores overall on NER tasks, though it achieved a competitive F1 on table RE (reported 47.7 F1) while being much slower (0.2 tables/s) compared to the pipeline. The paper reports that 2-shot ICL generally improved over 1-shot but still did not match fine-tuned models.",
            "limitations_or_challenges": "Observed issues include missing entities/relations, unannotated spurious outputs, incorrect entity types, outputs with undefined types, sensitivity to prompt design, hallucination and overconfidence, frequency bias (favoring frequent entity types), slower inference speed compared to the automated pipeline, and poorer overall extraction accuracy relative to fine-tuned extractors.",
            "uuid": "e3612.0",
            "source_info": {
                "paper_title": "All Data on the Table: Novel Dataset and Benchmark for Cross-Modality Scientific Information Extraction",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "Galactica",
            "name_full": "Galactica",
            "brief_description": "A scientific large language model cited in the paper as an example of domain-focused LLMs intended to support scientific scenarios such as citation suggestion, answering scientific questions, and writing scientific code.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "Galactica",
            "model_description": "Mentioned as a scientific LLM (cited as Taylor et al., 2022) that is trained for science-related tasks; paper does not provide architecture or size details beyond this citation mention.",
            "input_domain": "Scientific literature / scholarly content (general scientific domain as per cited description).",
            "corpus_size": "Not specified in this paper (only referenced as an example of a scientific LLM).",
            "law_type": "Not applicable in this paper — Galactica is cited as enabling practical science scenarios (citation suggestion, Q&A, code generation), not described as being used to distill qualitative scientific laws within this work.",
            "distillation_method": "Not applicable in this paper — Galactica is only referenced in related-work context; no method described here for distilling laws or principles.",
            "evaluation_method": "Not evaluated in this paper (only mentioned in related work as an example of scientific LLMs).",
            "results_summary": "No results reported in this paper regarding Galactica performing law/rule distillation; it is cited to motivate the broader context that scientific LLMs exist and can support practical scientific tasks.",
            "limitations_or_challenges": "The paper notes general concerns about LLMs (including hallucination, frequency bias, overconfidence) and suggests that SciIE plus retrieval/QA systems can help alleviate such problems, but does not report Galactica-specific limitations.",
            "uuid": "e3612.1",
            "source_info": {
                "paper_title": "All Data on the Table: Novel Dataset and Benchmark for Cross-Modality Scientific Information Extraction",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "Mozi",
            "name_full": "Mozi",
            "brief_description": "A recently developed scientific large-scale language model referenced in the paper as an example of domain-specialized LLMs enabling practical scientific use cases.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "Mozi",
            "model_description": "Cited in the paper (Lan et al., 2023) as a scientific large-scale language model; the present paper only references Mozi as part of the motivation for scientific LLMs and does not provide architecture, size, or implementation details.",
            "input_domain": "Scientific literature / scholarly content (as per citation context in the paper).",
            "corpus_size": "Not specified in this paper (Mozi is only mentioned in related work).",
            "law_type": "Not applicable — Mozi is mentioned as enabling scientific scenarios (e.g., suggesting citations, answering questions, writing code) but not described as being used to extract or induce qualitative laws in this work.",
            "distillation_method": "Not applicable in this paper — Mozi is referenced only in related-work context; no distillation procedure is described here.",
            "evaluation_method": "Not evaluated in this paper (only mentioned in related work).",
            "results_summary": "No results reported in this paper about Mozi performing law/rule distillation; Mozi is cited to exemplify scientific LLM development and potential applications.",
            "limitations_or_challenges": "General LLM concerns discussed in the paper (e.g., hallucination, need for verification) apply broadly; no Mozi-specific evaluation or failure modes are reported here.",
            "uuid": "e3612.2",
            "source_info": {
                "paper_title": "All Data on the Table: Novel Dataset and Benchmark for Cross-Modality Scientific Information Extraction",
                "publication_date_yy_mm": "2023-11"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Is information extraction solved by ChatGPT? An analysis of performance, evaluation criteria, robustness and errors",
            "rating": 2,
            "sanitized_title": "is_information_extraction_solved_by_chatgpt_an_analysis_of_performance_evaluation_criteria_robustness_and_errors"
        },
        {
            "paper_title": "Evaluating ChatGPT's information extraction capabilities: An assessment of performance, explainability, calibration, and faithfulness",
            "rating": 2,
            "sanitized_title": "evaluating_chatgpts_information_extraction_capabilities_an_assessment_of_performance_explainability_calibration_and_faithfulness"
        },
        {
            "paper_title": "Galactica: A large language model for science",
            "rating": 2,
            "sanitized_title": "galactica_a_large_language_model_for_science"
        },
        {
            "paper_title": "Mozi: A scientific large-scale language model",
            "rating": 2,
            "sanitized_title": "mozi_a_scientific_largescale_language_model"
        },
        {
            "paper_title": "Fine-tuned LLMs know more, hallucinate less with few-shot sequence-to-sequence semantic parsing over Wikidata",
            "rating": 1,
            "sanitized_title": "finetuned_llms_know_more_hallucinate_less_with_fewshot_sequencetosequence_semantic_parsing_over_wikidata"
        },
        {
            "paper_title": "TIARA: Multi-grained retrieval for robust question answering over large knowledge base",
            "rating": 1,
            "sanitized_title": "tiara_multigrained_retrieval_for_robust_question_answering_over_large_knowledge_base"
        }
    ],
    "cost": 0.012353,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>All Data on the Table: Novel Dataset and Benchmark for Cross-Modality Scientific Information Extraction</p>
<p>Yuhan Li 
Nankai University</p>
<p>Jian Wu 
Tokyo Institute of Technology</p>
<p>Zhiwei Yu 
Microsoft Research Asia</p>
<p>Börje F Karlsson 
Beijing Academy of Artificial Intelligence (BAAI)</p>
<p>Wei Shen 
Nankai University</p>
<p>Manabu Okumura 
Tokyo Institute of Technology</p>
<p>Chin-Yew Lin 
Microsoft Research Asia</p>
<p>Microsoft Research Asia</p>
<p>All Data on the Table: Novel Dataset and Benchmark for Cross-Modality Scientific Information Extraction
875A2EC8AE7417450E7A9923732F1DCB
Extracting key information from scientific papers has the potential to help researchers work more efficiently and accelerate the pace of scientific progress.Over the last few years, research on Scientific Information Extraction (SciIE) witnessed the release of several new systems and benchmarks.However, existing paperfocused datasets mostly focus only on specific parts of a manuscript (e.g., abstracts) and are single-modality (i.e., text-or table-only), due to complex processing and expensive annotations.Moreover, core information can be present in either text or tables or across both.To close this gap in data availability and enable crossmodality IE, while alleviating labeling costs, we propose a semi-supervised pipeline for annotating entities in text, as well as entities and relations in tables, in an iterative procedure.Based on this pipeline, we release SCICM, a high-quality scientific cross-modality IE benchmark, with a large-scale corpus and a semisupervised annotation pipeline.We further report the performance of state-of-the-art IE models on the proposed benchmark dataset, as a baseline ‡ .Lastly, we explore the potential capability of large language models such as ChatGPT for the current task.Our new dataset, results, and analysis validate the effectiveness and efficiency of our semi-supervised pipeline, and we discuss its remaining limitations.</p>
<p>Introduction</p>
<p>As scientific communities grow and evolve, there has been an explosion in the number of scientific papers being published in recent years 1 , which makes it increasingly difficult for researchers to discover useful insights and new techniques in the vast amount of information.One approach to help researchers keep abreast with the latest scientific advances and quickly identify new challenges and opportunities is to automatically extract and organize crucial scientific information from collections of research publications (Viswanathan et al., 2021).Scientific information extraction (SciIE) aims to extract such information from scientific literature corpora and has seen growing interest recently, with the rapid evolution of systems and benchmarks (Jain et al., 2020;Zhuang et al., 2022).</p>
<p>SciIE serves as an important pre-processing step for many downstream tasks, including scientific knowledge graph construction (Wang et al., 2021), academic question answering (Dasigi et al., 2021), and method recommendation (Luan, 2018).Additionally, the development of scientific large language models (LLMs), such as Galactica (Taylor et al., 2022) and Mozi (Lan et al., 2023), allows the exploration of several practical science scenarios (e.g., suggest citations, ask scientific questions, and write scientific code).However, language models can hallucinate without verification.Moreover, language models are frequency-biased and often overconfident.SciIE along with appropriate QA or retrieval systems -e.g., TIARA (Shu et al., 2022) -can help alleviate such problems and facilitate model performance on downstream tasks, as similarly demonstrated in leveraging Wikidata to improve LLM factuality (Xu et al., 2023).</p>
<p>Initial corpora and benchmarks extracted information from specific parts of a paper text, such as abstracts (Gábor et al., 2018;Luan et al., 2018) or selected paragraphs (Augenstein et al., 2017;D'Souza and Auer, 2020;Hou et al., 2021).However, scientific entities are spread through the whole paper body; thus neglecting any text fragments or tables will likely result in missing key information.This is especially true in tables that condense complex information and data on experimental results.Jain et al. (2020) (Gábor et al., 2018) RE Text (abstract) CS (NLP) 500 SCIERC (Luan et al., 2018) NER, RE Text (abstract) CS 500 NLP-TDMS (Hou et al., 2019) NER, ResE Text (abstract), Tables CS (NLP) 332 SCIREX (Jain et al., 2020) NER, N-ary RE Text (full) CS (ML) 438 NLPCONTRIBUTIONS (D'Souza and Auer, 2020) NER, RE Text (several paragraphs) CS (NLP) 50 TDMSCI (Hou et al., 2021) NER Text (several sentences) CS (NLP) 2,000 ORKG-TDM (Kabongo et al., 2021) NER, ResE Text (several sections), Tables CS  [5,361] TELIN (Yang et al., 2022) NER, ResE Tables CS (ML) 731 GASP-NER (Otto et al.,  and Hou et al. (2019) first annotates entities in tables as well as image captions.Unfortunately, these benchmarks mostly focus on a single modality of paper content and coarser annotations due to the large gap between modalities, plus high processing and annotation costs.Text-only modality covers information presented in an unstructured and narrative way, whereas table-only modality for structured and concise information representation.Nonetheless, it is increasingly clear that information obtained from a single modality will inevitably miss critical information in a given paper, e.g., it is hard to extract experiment results and settings only from text or to extract models and metrics from tables.Combining modalities is thus critical and enables further scenarios like hybrid QA (Wu et al., 2023).However, it is non-trivial to directly extract entities from tables due to the lack of context information and the label imbalance issue.</p>
<p>Leveraging entities consistently appearing in both text and tables can intuitively facilitate this process.To address the above-mentioned gaps in data availability, a new cross-modality and document-level benchmark dataset for SciIE is needed.Annotating such a benchmark remains challenging because 1) it requires domain expertise and considerable annotation effort to comprehensively label a sizeable benchmark, 2) it requires annotators to understand the domain and the whole paper to maintain annotation consistency across different modalities, and 3) annotations need to be fine-grained enough (and consistent across documents) to unlock relevant semantics.To overcome these annotation challenges, we develop a semisupervised pipeline for annotating entities in text and both entities and relations in tables of academic papers, which involves a two-stage iterative procedure.Specifically, a replaceable extractor is first trained on a small amount of high-quality manually annotated papers and then utilized to label a large number of papers automatically.Experts are introduced next to correct any false labels.This process can be repeated iteratively multiple times, with the extractor becoming more accurate as it can use the newly labeled data to improve its performance.</p>
<p>During training, we also adopt label mapping in text extraction via leveraging existing benchmarks (Luan et al., 2018;Jain et al., 2020) to enrich our annotations.We also prove that informing the table extractor with text extraction results leads to more accurate and context-rich table annotations.</p>
<p>Based on this pipeline, we release 1) SCICM, a high-quality expert annotated benchmark that supports multiple Scientific Cross-Modality IE tasks; 2) a large-scale corpus containing automatically annotated papers with different domains; 3) a visualization tool that enables researchers to get a global view of key information in scientific papers; and 4) the pipeline itself, which can be utilized as provided or further extended.We conduct experiments on SCICM utilizing SoTA IE models and popular LLMs such as ChatGPT to showcase its characteristics and as a baseline.To further investigate the efficacy of our pipeline, we assess both the quality and the speed of annotation on SCICM.Additionally, a thorough error analysis is conducted.</p>
<p>We summarize our key contributions as follows: 1) we are the first to explore cross-modality IE to build a bridge between text and tables in scientific articles; 2) we develop a semi-supervised pipeline for annotating scientific terms along with their relations without requesting lots of human supervision at document-level; 3) we release a high-quality benchmark that enables diverse SciIE tasks and a large-scale corpus to facilitate research over the scientific literature; 4) extensive experiments validate the efficiency, effectiveness, and adaptability of our semi-supervised pipeline.</p>
<p>An overview of existing SciIE benchmarks is shown in Table 1.The field of SciIE began with extracting information from only the text modality.Augenstein et al. (2017) proposed SEMEVAL-2017 TASK 10 to support the task of identifying entities and relations in a corpus of 500 paragraphs taken from open-access journals.Gábor et al. (2018) presented SEMEVAL-2018 TASK 7 on relation extraction from 500 abstracts of NLP papers.Luan et al. (2018) released SCIERC containing 500 abstracts with more fine-grained types of entities (i.e., 6 types) and relations (i.e., 7 types).More recently, D'Souza and Auer (2020) and Hou et al. (2021) proposed benchmarks which are composed of several paragraphs from NLP papers, aiming to capture contributed scientific terms and extract <Task, Dataset, Metric> (TDM) triples, respectively.Since scientific terms can appear anywhere in the paper, Jain et al. (2020) proposed SCIREX that tries to comprehensively annotate the full paper text.Otto et al. (2023) manually annotated GASP-NER for identifying named entities associated with the interplay between machine learning model entities and dataset entities.Inspired by them, we consider document-level extraction and define more complete entity types compared with prior works.We also leverage two high-quality benchmarks, i.e., SCIERC and SCIREX, to boost our text extractor for a subset of entities.</p>
<p>Extracting information from the table modality has attracted much research attention in recent years since they contain relevant structural knowledge that aids extraction.To the best of our knowledge, there are currently only three datasets that explore cross-modality extraction in the scientific domain: NLP-TDMS (Hou et al., 2019), ORKG-TDM (Kabongo et al., 2021), and TELIN (Yang et al., 2022).These datasets associate score entities extracted from result tables with their corresponding TDM triples, but they only concentrate on simple result extraction and ignore many valuable types of entities that tables can provide such as Model and Metric.Additionally, there has been a lack of benchmarks supporting relation extraction in the table modality, which may lead to the absence of important relationships, as well as tabletext relationships.According to these limitations, our goal is to develop a benchmark covering information extraction in both text and table modalities and across them, to close the gap in scientific data availability and facilitate more comprehensive and accurate information extraction.</p>
<p>Preliminaries</p>
<p>Data Collection</p>
<p>Extracting large amounts of tables, body text, and other metadata from scientific articles requires expert knowledge and comprehension of the article, which can be very time-consuming for expert annotation.To collect paper data, we propose a method to automatically download and pre-process L A T E X source files of scientific articles.Leveraging the structure of L A T E X source files, we can easily locate each component of papers such as titles, sections, and tables.</p>
<p>Specifically, we first search and crawl the arXiv L A T E X sources from its official website2 using its official library3 .We design a simple parser to extract textual content (including all sections and subsections) and tables from a scientific paper.The titles of sections and sub-sections are retained.Each extracted table contains its caption and all table cells.To ensure that the extracted data is in a readable format for annotations, we utilize two public libraries, e.g., latexml4 and dashtable5 to "clean" the extracted data.This process enables us to efficiently collect and pre-process large amounts of scientific papers for annotation and analysis.</p>
<p>Task Definition</p>
<p>We aim to perform NER on a corpus of scientific papers across both text and table modalities.Unlike previous benchmarks (Augenstein et al., 2017;Luan et al., 2018), we only perform RE on tables.Cross-sentence relations in text can be very challenging to be annotated (Jain et al., 2020) and error prone.Formally, we denote each scientific paper as D, containing a sequence of paragraphs P = {p 1 , p 2 , ..., p |P | } and a sequence of tables T = {t 1 , t 2 , ..., t |T | }.Each paragraph p is composed of a sequence of sentences {s 1 , s 2 , ...s n } and each sentence is composed of a sequence of words {w 1 , w 2 , ..., w i }.Each table t is flattened via inserting separators and concatenated with its caption as a sequence of cells {c 1,1 , c 1,2 , .., c y,z }, where y and z are the numbers of table rows and columns.Each cell is also composed of a sequence of words {w 1 , w 2 , ..., w j }.Formally, the three IE tasks we support are defined as follows.</p>
<p>Text/Table NER The goal of text/table NER is to examine all possible spans of words, denoted as {w l , ..., w r } within a sentence/cell, where l and r represent the left and right indices of the span, and recognize if the span describes an entity and classifies it with its type if any.</p>
<p>Table RE</p>
<p>The goal of this task is to examine all unordered pairs of entities appearing within a given table, denoted as (e 1 , e 2 ), and determine the existence of any relationship between them.</p>
<p>4 Semi-supervised Annotation Pipeline</p>
<p>Overview</p>
<p>The overview of our semi-supervised pipeline is depicted in Figure 1, which involves a two-stage iterative process.Specifically, two extractors are trained on a small amount of manually annotated or reviewed papers and leveraged to make automatic annotations on the unlabeled papers (Stage 1).Expert reviewers are introduced later to perform necessary corrections to generate high-quality annotations, which are utilized to update the extractors for iterative training (Stage 2).The pipeline can be repeated multiple times, with extractors becoming more accurate as they have the ability to use the newly labeled data to improve their extraction performance.</p>
<p>Our proposed pipeline exhibits exceptional performance in the annotation of cross-modality entities and relations in the scientific literature.Specifically, it demonstrates the following advantages: i)</p>
<p>Efficiency.The curation of datasets composed of long scientific documents with both paragraphs and tables can be a costly and labor-intensive task.However, our pipeline only requires a smaller number of labeled papers for training than fullysupervised learning, making it a cost-effective solution in comparison to existing benchmarks.ii) Effectiveness.We employ techniques such as label mapping and label guiding for text and tables respectively, to ensure high-quality annotations.iii) Adaptability.We conduct both in-domain (IID) and out-of-domain (OOD) evaluations to demonstrate that the pipeline can adapt to new domains with ease, which is particularly useful in the scientific domain where new research is continually being published.The performance of our pipeline will be further investigated in Section 5.</p>
<p>Text Information Extraction</p>
<p>Entity types We find that existing benchmarks (Luan et al., 2018;Hou et al., 2019;Jain et al., 2020;Hou et al., 2021;Kabongo et al., 2021) normally consider fine-grained knowledge with multiple entity types.However, all these datasets simply regard the "models" proposed by papers as "methods" and do not treat Model as a separate type of entity.It is not reasonable, as "models" and "methods" are distinct entities with different characteristics, with "models" being more representative of the paper and more likely to appear in experimental tables.To address this issue, we present more appropriate entity types, including Task, Dataset, Metric, Model, and Method (see Appendix A.1).</p>
<p>Label mapping Due to the mappable label sets between SciIE benchmarks, we are able to reuse some high-quality labels collected from existing benchmarks to boost the performance of extraction.Inspired by (Yang et al., 2017;Francis et al., 2019), we perform a label mapping step prior to training, using two fully-supervised annotated datasets (Luan et al., 2018;Jain et al., 2020).To be specific, we collect Task, Metric, and Method entities from SCIERC (Luan et al., 2018) and Task, Dataset, Metric, and Method entities from SCIREX (Jain et al., 2020) to enrich our entity annotations.</p>
<p>Extractor We apply the publicly available SciIE model PL-Marker (Ye et al., 2022) to perform text NER.PL-Marker inserts levitated markers in text and incorporate two packing strategies to achieve state-of-the-art F 1 scores and a notable level of efficiency on SCIERC (Luan et al., 2018).</p>
<p>Table Information Extraction</p>
<p>Entity and relation types In contrast to text NER, in table NER we extract two additional types of entities: Score and Setting.Scores rarely appear in the paper text but frequently appear in tables.We follow previous benchmarks (Hou et al., 2019;Kabongo et al., 2021;Yang et al., 2022) in extracting Score entities from tables to enable a more comprehensive understanding of the experimental results reported in scientific papers.Setting is another important entity type, as it refers to the context or environment in which the study was conducted.We use Setting entities to indicate different experimental settings, for example, BERT large and BERT base , one-hop and multi-hop.</p>
<p>Similar to (Hou et al., 2019;Jain et al., 2020;D'Souza and Auer, 2020;Hou et al., 2021;Kabongo et al., 2021), we do not pre-define specific relation types and instead aim to uncover indirect relationships between pairs of entities.For more details please see Appendix A.2.</p>
<p>Label Guiding Inconsistency in NER predictions between tables and text can result in entities with the same name being assigned different entity types across different modalities.To address this issue, we utilize a label guiding rule that leverages the annotation results of the previous stage, i.e., text NER.Specifically, when an entity appears in both text and tables, we maintain consistency by assigning the same entity type in the table as the entity type identified in the text.This approach ensures that the entity types are consistent across different modalities.While TURL (Deng et al., 2020) can solve column relation extraction, it cannot extract relations in fine-grained granularity, such as at the cell level.To address this limitation, we design a solution for table RE, which is a binary classification task that determines whether two cells in a table have a relation or not.The template statement is "Whether the cell c i,j , located in row i, column j, has a relation with the cell c p,q , located in row p, column q, or not?", where the label is either 0 or 1.We follow the model architecture in Figure 2, where the input table is encoded by a pre-trained table model and a binary classifier is used to output the relation probabilities of any two cells in the same table.</p>
<p>Visualization</p>
<p>We utilize OneLabeler (Zhang et al., 2022) 6 , a tool to aid in the annotation process, enabling reviewers to label data objects of various entities and relations.To incorporate semi-supervised pipeline in the labeling tool, we start with the labeling workflow template and add a labeling module, which we configure to be implemented with the built-in tagger of OneLabeler.As a result, we are able to load the automatically extracted entities and relations into the tool, which allows annotators to just remove incorrect detections and create any missing false negative spans/relations, saving time and effort.More details about annotation rules and notes can be found in Appendix A.3.</p>
<p>In Figure 3, we show a portion of the text of the BERT (Devlin et al., 2019) paper, a milestone in the field of NLP with high citations.We annotate various types of entities, such as Model, Task, and Method in the abstract and other sections of the paper using different colors to distinguish between them, which allows annotators to efficiently and accurately label different kinds of entities.Due to space limitations, we provide two additional visualization examples in Appendix B.</p>
<p>Released Benchmark</p>
<p>This section describes the corpora constructed to train and evaluate our semi-supervised pipeline for cross-modality SciIE, as shown in Table 2 and the final dataset (SciCM).</p>
<p>Over 90% of papers with arXiv IDs have source code available, allowing us to directly download and preprocess a large number of papers.For the first round of training, we use 10 manually annotated papers as seeds in the computer science (CS) domain with human-annotated entities and relations.We then add 30 new papers in the same domain and ask domain experts to review the model's predicted results on these papers.We use these human-reviewed 30 papers as added corpus to fur-ther train a new version of extractors.We then automatically annotate other 30 papers and review them for both in-domain (IID) and out-of-domain (OOD) evaluation, with 10 papers each from CS, statistics (STAT), and electrical engineering and systems science (EESS), respectively.Finally, we utilize the extractors to automatically annotate 12817 new papers in five different domains to facilitate research in scientific literature.This large-scale annotation of papers enables researchers to efficiently search for relevant information and extract insights from a large corpus of scientific literature.5 Experiments</p>
<p>Dataset and Annotation Statistics</p>
<p>We evaluate our semi-supervised pipeline on the test set that includes both IID evaluation and OOD evaluation.Table 4 presents the data statistics of the added corpus and test set, both of which are first automatically annotated and then manually reviewed.It can be seen from the table that scientific tables contain a considerable number of entities and relations, demonstrating the necessity of information extraction from tables.In addition, the long length (i.e., averaging 7000+ words per paper) and cross-modality attributes of scientific papers make full annotation challenging, prompting us to focus on a more efficient annotation pipeline.</p>
<p>Evaluation Metrics</p>
<p>We follow the standard evaluation protocol (Zhong and Chen, 2021) and use precision, recall, and F 1 as evaluation metrics.For text NER and table NER, both entity boundary and type are required to be correctly predicted.For table RE, the boundaries of the subject entity and the object entity should be correctly identified.</p>
<p>Implementation Details</p>
<p>Both our text and table IE models are implemented using Pytorch version 1.13 and the Huggingface's Transformers (Wolf et al., 2020) (Loshchilov and Hutter, 2018) with a learning rate of 2e-5 (5e-5 for tables) and a batch size of 16 (32 for tables).Since scientific papers can be very long, we leverage cross-sentence information (Luan et al., 2019;Ye et al., 2022) to extend each sentence by its neighbor sentences and set the maximum length as 512, which is the input length limit for many transformer-based models.</p>
<p>Overall Performance</p>
<p>We report the performance of our pipeline in Table 3 with two rounds: Round 1 has access to only a small set of seeds for training, and Round 2 extends the training set by incorporating expert-reviewed papers.Our discussion focuses on the effectiveness and adaptability of our pipeline, taking into account the evaluation results.We also provide an analysis of ChatGPT's performance on the benchmark.Table 5: We compare our pipeline and ChatGPT in both accuracy and annotation speed.The accuracy is measured as the F 1 on the test set.The speed is measured on a single A100 GPU with a batch size of 32.</p>
<p>ChatGPT Evaluation.LLMs pre-trained on massive corpora, such as ChatGPT, have demonstrated impressive few-shot learning ability on many NLP tasks.We investigate ChatGPT's capabilities on our benchmark in terms of the few-shot In-context Learning (ICL) setting.To construct few-shot ICL prompts, we design the prompt template carefully and select demonstrations from the training set.For more details see Appendix E. We use the official API 7 to generate all outputs from ChatGPT.To prevent the influence of dialogue history, we generate the response separately for each testing sample.We compare ChatGPT with our pipeline for all sub-tasks in Table 3. Comparing 2-shot ICL with 1-shot ICL, it can be seen that providing more demonstrations generally leads to improvements (i.e., +2.7, +9.0, +2.4 F 1 points, respectively).We also observe that ChatGPT still struggles to achieve comparable performance with traditional fine-tuned IE models, indicating a need for further exploration in improving the performance of LLMs in cross-modality SciIE.</p>
<p>Annotation Speed</p>
<p>Error Analysis</p>
<p>To further explore the limitations of our pipeline, we conduct an error analysis during the autoannotation process and categorize major errors in both text and table modalities as follows.We also analyze ChatGPT's errors in Appendix C.</p>
<p>Text IE Errors i) Over Annotation.It occurs when terms that are too general to offer useful scientific information are labeled as entities.For instance, "neural network" and "natural language processing" are examples of scientific terms that are mistakenly labeled as Model entities.ii) Missing Abbreviation.Abbreviations appear frequently in scientific papers to represent entities with long names.Abbreviations are usually missed during auto-annotation.For example, "coupled multilayer attention" can be recognized successfully, while its abbreviation "CMLA" is missed.iii) Nested Entity Annotation.Some entities that contain nested entities, such as "Bi-LSTM-CRF + CNN-char", should be recognized as a complete entity.However, the extractor tends to extract "Bi-LSTM-CRF" and "CNN-char" separately, leading to incomplete annotation.iv) Inconsistent Annotation.Same entity is specified with different entity types even if it appears in the same paragraph.v) Insensitive to Context.Sometimes an entity will be recognized as different types due to the different contextual environments.</p>
<p>Conclusion and Future Work</p>
<p>In this paper, we present SCICM, a novel dataset for training and evaluating cross-modality SciIE.</p>
<p>Along with it, we propose a semi-supervised pipeline that automatically annotates entities and relations with high effectiveness and efficiency.Our pipeline performs well across SciIE tasks, with good inference speed for iterative runs.Moreover, we release a visualization tool that helps users to annotate scientific items with a global view.In future work, we plan to extend our corpus by incorporating images from scientific papers.We hope our release can facilitate downstream tasks in the scientific domain.</p>
<p>Limitations</p>
<p>In this paper, our focus was on proposing a benchmark that includes paper text and tables and a semisupervised pipeline for automatic annotation.However, we did not consider images in the papers, which also contain a wealth of information.Beyond tables, images often illustrate the work process, pipeline, or framework presented in the scientific paper.In our future work, we plan to introduce and process images from scientific papers into our benchmark to further assist researchers in comprehending papers.This will enable us to provide a more holistic approach to understanding scientific papers and ensure that the benchmark covers all important modalities of information in scientific papers.</p>
<p>A Annotation Guideline</p>
<p>A.1 Entity Category</p>
<p>• Task: The specific task or problem that the paper aims to address.E.g., information extraction, machine reading comprehension, image segmentation, etc.</p>
<p>• Model: A formal representation or abstraction of the proposed system, which can be applied to solve the specific Task.E.g., BERT, ResNet, etc.</p>
<p>• Method: The approach, technique, and tool that is used to construct the Model to solve the Task.E.g., self-attention, data augmentation, Adam, batch normalization, etc.</p>
<p>• Dataset: A collection of data that is used for training, validating, and testing the proposed Model.E.g., GLUE, COCO, CoNLL-2003, etc.</p>
<p>• Metric: A quantitative measure or evaluation criterion that is used to assess the performance or quality of a Model.E.g., accuracy, F 1 score, etc.</p>
<p>• Setting: It often appears in tables and refers to the context or environment in which the study was conducted.Setting is an important aspect of a research paper, as it provides the necessary information to reproduce or replicate the experimental results.E.g., one-hop, multi-hop, dev set, test set, etc.</p>
<p>• Score: It refers to a numerical value that is used to evaluate the performance of a Model on a specific Task, using a particular Dataset and Metric, under a specific Setting.</p>
<p>A.2 Relation Category</p>
<p>Due to the relative scarcity of explicit relationships in body text, we only focus on annotating relations presented in tables.Similar to previous SciIE benchmarks (Hou et al., 2019;Jain et al., 2020;D'Souza and Auer, 2020;Hou et al., 2021;Kabongo et al., 2021), we link different types of entities that are related, without requiring a specific type.This methodology allows for a more flexible representation of the relationships between entities, enabling an easier and deeper understanding of the underlying patterns and connections within the table content.</p>
<p>A.3 Rules and Notes</p>
<p>Considering that annotators may have varying understandings of the annotation details, we have defined a set of rules and notes to standardize the annotation process.The rules defined are the following:</p>
<ol>
<li>
<p>Differences between Model and Method: A Model entity refers to the name of a model that can be applied to a specific task independently, while a Method entity cannot be used directly to solve a problem or task but can help models improve their performance.</p>
</li>
<li>
<p>The proposed framework, which stacks several models should be classified into Model.For example, "YOLOv5" should be annotated as Model.</p>
</li>
<li>
<p>We should annotate a combination of a Model and a Method as a Method rather than as a Model.For example, the "RNN-based encoder" belongs to a Method.</p>
</li>
<li>
<p>Terms such as "networks", "neural", and "model", which do not convey specific meaning and often appear at the beginning or end of entity names, should be excluded.For example, "FasterRCNN network" → "Faster-RCNN", "neural machine translation" → "machine translation", and "question answering model" → "question answering" 5. Avoid annotating broad and unclear noun phrases as entities.Scientific terms should be specific and well-defined concepts to ensure clarity and precision.For example, phrases such as "neural network" and "encoder-decoder architecture" should not be considered as Model entities.</p>
</li>
<li>
<p>Adjectives that are not directly related to the domain of the research, such as "state-of-theart", should be avoided.Instead, more specific adjectives that accurately describe the Model or Method should be kept.For instance, "Bidirectional LSTM".</p>
</li>
<li>
<p>Do not include any determinators (e.g., "the", "a"), or adjective pronouns (e.g., "this", "its", "these", "such") to the entity span.</p>
</li>
<li>
<p>Two entities with the same entity type should not have a relationship.Figure 4 displays a visualization of Table 2 from the TIARA paper, which can be found on arXiv with id "2210.12925".We annotate various types of entities in tables using different colors to make them easily identifiable, which allows reviewers to efficiently and accurately label different kinds of entities and discover relations.Specifically, green represents the Model, dark orange represents the Score, light orange represents the Setting, blue represents Metric, and red represents the Method.The interface for annotating entities and relations is depicted in Figure 5.Each entity is labeled with an associated entity type and its corresponding start and end positions within the document.Each relation is labeled with two entities.Entity annotation is done by directly selecting the boundary of the entity, while relation annotation is performed Text NER</p>
</li>
</ol>
<p>B Visualization Examples</p>
<p>Error types Cases</p>
<p>Missing entities</p>
<p>ChatGPT struggles with extracting entities with long names.For example, "Question-Answer driven Semantic Role Labeling" fails to be extracted as a Task entity, and "dual discourse-level target-guided strategy" fails to be accurately identified as a Method entity.</p>
<p>Undefined types</p>
<p>In text NER, even with a predefined set of entity types, it is common for some miscellaneous types to be introduced, such as ["Gábor et al.", Author],</p>
<p>["Danqi Chen", Person], etc.</p>
<p>Table NER</p>
<p>Error types Cases</p>
<p>Unannotated entities "Our model" in the table is not an entity but has been erroneously predicted as a Model entity.Similarly, "Train data" in the table is not an entity but has been incorrectly predicted as a Dataset entity.</p>
<p>Incorrect types "K-means" should be labeled as a Method entity, but it has been incorrectly predicted as a Model entity.Similarly, both "dev set" and "test set" should be labeled as Setting entities, but they have been incorrectly predicted as Dataset entities.</p>
<p>Table RE</p>
<p>Error types Cases</p>
<p>Missing relations</p>
<p>If a cell contains a Model entity, it should be annotated in relation to other cells in the same row that contain Score entities.However, this relationship is often missing in ChatGPT.by clicking on the second button in the upper-right corner of the entity box.All entities and relations can be deleted by clicking the delete button.</p>
<p>C Error Analysis of ChatGPT</p>
<p>In this section, we follow (Han et al., 2023) to categorize and analyze ChatGPT's errors on our three sub-tasks.Through manual checking, we find that the errors mainly include:</p>
<p>• Missing entities/relations: Missing one or more annotated target entities or relations.</p>
<p>• Unannotated entities/relations: Output the entities or relations that are not annotated in the test set.</p>
<p>• Incorrect types: The entity boundaries are correct, while the corresponding type comes from the set of pre-defined types, but does not match the annotated type.</p>
<p>• Undefined types: Output the types beyond the pre-defined types when the corresponding entity boundaries are correct.• Others: Other errors apart from the above errors, such as incorrect output format, output unexpected information, etc.</p>
<p>D Discussion of Table NER</p>
<p>As shown in Figure 6, we observe a long-tail problem of entity type distribution of tables, where Score entity occupies over 60% of the total entity number.In this case, table NER models are more likely to predict the frequent Score entity, which can influence the model's generalization ability.Additionally, some numerical values in tables are simply parameters, data sizes, and other experimental information, but are prone to be predicted as Score entities.</p>
<p>In text NER and table RE tasks, label imbalance is unlikely to occur since Score entities are relatively rare in text NER, and there is no specific type of relation in table RE.To address this label imbalance issue, we try to re-train a table NER model and test ChatGPT without the Score entities.</p>
<p>D.1 Model Re-train</p>
<p>We remove the Score entities, and the experiment results shown in</p>
<p>D.2 ChatGPT without Scores</p>
<p>We conduct experiments on ChatGPT without the Score entities, and as shown in Table 8, the experimental results indicate that removing the Score entities leads to a significant improvement in recall for ChatGPT on both 1-shot and 2-shot ICL settings.However, it is important to note that removing the Score entities also results in a reduction in precision.Overall, our findings suggest that removing the Score entities could be a promising approach to improving the performance of table NER.</p>
<p>E Exemplar of the Prompt</p>
<p>When evaluating large language models, prompting is a brittle process wherein small modifications to the prompt can cause large variations in the model predictions, and therefore significant effort should be dedicated to designing a painstakingly crafted perfect prompt for the given task (Arora et al., 2022;Diao et al., 2023).In this study, we follow (Li et al., 2023) and (Han et al., 2023), who have conducted extensive evaluations of the power of Chat-GPT in multiple information extraction tasks, to design prompt templates that are well-suited to our tasks.We investigate the performance of few-shot In-context Learning (ICL) on our benchmark.To eliminate the randomness, we manually select two demonstrations for each task, ensuring that all entity types are covered.</p>
<p>We give our designed input examples for three different kinds of scientific information extraction tasks to help readers understand our implementation, as shown in Table 9, 10, and 11, respectively.</p>
<p>Prompts of text NER (2-shot In-context Learning)</p>
<p>Scientific named entity extraction is a task in natural language processing that aims to identify specific entities with semantic meaning from paper text and classify them into predefined types.</p>
<p>Paper text is typically segmented into sequences of tokens and each entity is labeled with a tag indicating its type.Entity types include: Task: The specific task or problem that the paper aims to address.E.g., information extraction, machine reading comprehension, image segmentation, etc. Model: A formal representation or abstraction of the proposed system, which can be applied to solve the specific Task.E.g., BERT, ResNet, etc. Method: The approach, technique, and tool that is used to construct the Model to solve the Task.E.g., self-attention, data augmentation, Adam, batch normalization, etc. Dataset: A collection of data that is used for training, validating, and testing the proposed Model.E.g., GLUE, COCO, CoNLL-2003, etc. Metric: A quantitative measure or evaluation criterion that is used to assess the performance or quality of a Model.E.g., accuracy, F1 score, etc.</p>
<p>Here are two demonstrations:</p>
<p>Given type set: [Task, Model, Method, Dataset, Metric].Sentences: Our task is to classify images in the CIFAR-10 dataset into their respective classes, which include animals, vehicles, and household items.This task has practical applications in areas such as autonomous driving, object recognition, and image search.In this paper, we propose a novel approach for image classification using a deep learning model based on the EfficientNet architecture and transfer learning techniques.Our proposed model, named EfficientNet-Transfer, is a modified version of the EfficientNet-B0 architecture that has been pre-trained on the ImageNet dataset and fine-tuned on our target dataset.We use the CIFAR-10 dataset, which contains 60,000 32x32 pixel color images in 10 classes, as our target dataset.We evaluate our model using classification accuracy, precision, recall, and F1-score.Sentences: We perform sentiment analysis on movie reviews using deep learning techniques.We propose AttRNN, a novel model architecture based on a recurrent neural network (RNN) with attention mechanisms.Our model integrates both word-level and sentence-level attention mechanisms to improve its discriminative power and capture more relevant features from the input text.We evaluate our proposed model on the Movie Review Sentiment Analysis dataset, which consists of 50,000 movie reviews labeled as positive or negative.We use the accuracy metric to measure the performance of our model, which is defined as the percentage of correctly classified movie reviews in the test set.We also report the F1-score, which takes into account both precision and recall of the positive and negative classes.</p>
<p>Figure 1 :
1
Figure 1: Overview of our semi-supervised pipeline.It consists of two stages: 1) training of text and table extractors through a limited number of manually annotated in-domain papers.Extractors are then employed to automatically annotate a larger set of both out-of-domain and in-domain papers; 2) expert reviewers rectify any false labels to obtain golden annotations, which are used to expand training and update extractors via iterative learning.</p>
<p>Figure 3 :
3
Figure 3: Visualization of annotated entities of BERT paper.Different types of entities are assigned distinct colors to facilitate their identification, with dark green representing the Model, light green representing the Task, and red representing the Method.</p>
<p>Figure 4 :
4
Figure 4: Visualization of annotated entities in tables.</p>
<p>Figure 5 :
5
Figure 5: The interface for visualizing NER and RE annotations consists of two rectangles.The left rectangle, labeled "Spans", shows the entity name, entity type, and entity span.The right rectangle, labeled "Relations", shows the related entities within their types.</p>
<p>Question: Please extract the named entity from the given sentences.Based on the given label set, provide the extraction results in the format: [[Entity Name, Entity Type]] without any additional things including your explanations or notes.If there is no entity in the given sentence, please return a null list like [].Entities: [['CIFAR-10', 'Dataset'], ['autonomous driving', 'Task'], ['object recognition', 'Task'], ['image search', 'Task'], ['image classification', 'Task'], ['EfficientNet', 'Method'], ['transfer learning', 'Method'], ['EfficientNet-Transfer', 'Model'], ['EfficientNet-B0', 'Model'], ['ImageNet', 'Dataset'], ['accuracy', 'Metric'], ['precision', 'Metric'], ['recall', 'Metric'], ['F1-score', 'Metric']] Given type set: [Task, Model, Method, Dataset, Metric].</p>
<p>Question: Please extract the named entity from the given sentences.Based on the given label set, provide the extraction results in the format: [[Entity Name, Entity Type]] without any additional things including your explanations or notes.If there is no entity in the given sentence, please return a null list like [].Entities: [['sentiment analysis', 'Task'], ['AttRNN', 'Model'], ['recurrent neural network', 'Method'], ['RNN', 'Method'], ['word-level and sentence-level attention mechanisms', 'Method'], ['attention mechanisms', 'Method'], ['Movie Review', 'Dataset'], ['accuracy', 'Metric'], ['F1-score', 'Metric']] Given type set: [Task, Model, Method, Dataset, Metric].Sentences: [S] Question: Please extract the named entity from the given sentences.Based on the given label set, provide the extraction results in the format: [[Entity Name, Entity Type]] without any additional things including your explanations or notes.If there is no entity in the given sentence, please return a null list like [].Entities:Table 9: The prompt template of text NER leveraging 2-shot In-context Learning.[S] denotes the sentences we want to extract.</p>
<p>first attempts to create a scientific IE benchmark at the document level arXiv:2311.08189v3[cs.CL] 18 Dec 2023
BenchmarksIE TaskModality &amp; CoverageDomainSizeSEMEVAL-2017 TASK 10 (Augenstein et al., 2017) NER, REText (several paragraphs)CS, MS, Phy500SEMEVAL-2018 TASK 7</p>
<p>Table 1 :
1
An overview of existing scientific IE benchmarks.Domain acronyms: CS refers to Computer Science; MS refers to Material Science; Phy refers to Physics.Task acronyms: NER refers to Named Entity Recognition; RE refers to Relation Extraction; ResE refers to Result Extraction."[ ]" indicates automated annotations.
2023)NERText (full)CS (ML)100SCICM (ours)NER, ResE, RE Text (full), TablesCS, Stat, EESS, ... 70 + [12,817]</p>
<p>Table IE
IE
We treat table IE tasks, which include table NER and table RE, as classification tasks.The model structure of our table IE model is depicted in Figure2, which comprises an encoding layer and a classification layer.Prompting the table NER model is straightforward.We prompt the model with a statement that specifies the entity type of a given cell in a table, i.e., "The entity type of the cell c i,j , in row i, column j is[E].",where[E]∈ Γ and Γ is the set of predefined entity types.For each table cell, we generate m prompts and match these prompts with their entity types, where m is the number of entity types.Let E = {s i , t i , E + i , E − i,1 , E − i,m−1 } m i=1be the training data that consists of m instances.Each instance consists of a statement s i , a table t i , a correct (positive) entity type E + i , along with (m-1) wrong (negative) entity types E − i,j .
ClassifierTable Pretrained Model[CLS]Tok 1Tok N[SEP]Tok 1Tok M[SEP]Prompt StatementLinearized Table Cells
Figure 2:Table IE model architecture.A pre-trained model encodes the input table and its output is fed into a classifier to output the probabilities of entity types for each cell, or the relation probabilities of any two cells.</p>
<p>Table 2 :
2
Domain distribution and labeling methods of different parts of SCICM.</p>
<p>Table 3 :
3
library, running Evaluating SoTA SciIE models on the test set with different domains.We evaluate in two rounds: Round 1 that is only trained on a small amount of paper seeds and Round 2 that is boosted leveraging added corpus.We also report the performance of ChatGPT on the few-shot ICL setting, providing a different number of demonstrations.
Test domainsSettingsText NERTable NERTable REPRF1PRF1PRF1Round 1 EvaluationOverall (IID &amp; OOD)Fine-tune67.439.549.849.310.617.54.247.27.7<em> CS (IID)-67.431.943.351.010.617.65.347.29.5</em> STAT (OOD)-71.647.757.255.111.018.42.938.35.5<em> EESS (OOD)-60.737.946.738.110.015.85.066.79.3Round 2 EvaluationOverall (IID &amp; OOD)Fine-tune79.075.877.446.783.059.883.128.442.3+11.6%+36.3%+27.6%-2.6%+72.4%+42.3%+78.9%-18.8%+34.6%</em> CS (IID)-79.465.571.850.383.662.883.821.834.6<em> STAT (OOD)-83.987.485.641.876.253.981.533.847.7</em> EESS (OOD)-71.072.871.939.780.853.185.635.950.6ChatGPT EvaluationOverall (IID &amp; OOD) 1-shot ICL24.045.231.453.116.825.573.532.845.3Overall (IID &amp; OOD) 2-shot ICL30.438.834.157.424.734.576.234.747.7Statistics (avg per paper) Added corpus Test setSentences269.9279.0Words6787.77872.1Tables5.73.9Cells390.3190.3Entities in text133.2152.9Entities in tables151.694.4Relations in tables81.359.5</p>
<p>Table 4 :
4
Statistics of the added corpus and test set.</p>
<p>(Beltagy et al., 2019)cally, we adopt in-domain scibert-scivocab-uncased(Beltagy et al., 2019)encoder with 110M parameters for PL-Marker and tapas-base encoder with 110M parameters for our table IE models.During training, we adopt AdamW</p>
<p>Table 5
5presents the inference speed of our pipelineand ChatGPT. Our automatic labeling approachenables us to process 49.9 sentences per second intext NER and 33.0/4.6 tables per second in table IE.In text NER, our pipeline outperforms ChatGPTby achieving 77.4 F 1 points and a faster processingspeed of 49.9 tables/s. Similarly, in table NER,our pipeline achieves a higher F 1 score (59.8) anda faster processing speed of 33.0 tables/s, whileChatGPT achieves a lower F 1 score of 49.8 and aslower processing speed of 0.2 tables/s. However,
in table RE, ChatGPT achieves a better F 1 score of 47.7 compared to our table RE model, yet it still has a slower processing speed of 0.2 tables/s. 7https://platform.openai.com/docs</p>
<p>Table IE Errors i) Entity Type Error.It occurs when an entity in the table is labeled with the wrong entity type.ii) Inconsistency in Table Structure.The structure of a table always provides important context and cues for extraction.Sometimes, there may be obvious inconsistencies in the entity types within the same column or row.For instance, recognizing Dataset, Model, and Score entities in the same column or row.iii) NER Misleads RE.It occurs when the NER model incorrectly identifies an entity, leading to incorrect relationship extraction of the current entity.iv) Missing Relations: RE model fails to identify a relationship between cells.</p>
<p>Table 6 :
6
Cases of different types of extraction errors that can be output by ChatGPT.</p>
<p>Table NER w
NER
The entity types distribution of tables in the added corpus.Scores 46.7 83.0 59.8 Table NER w/o Scores 51.4 87.9 64.9
Model472Metric303Task114Setting514Dataset440Method153DatasetMethodSetting8%3%10%Task2%Metric6%Model 9%Score 62%Figure 6: SettingsTable NERPRF1</p>
<p>Table 7 :
7
The performance of our table NER model with and without the Score entities.</p>
<p>Table 6
6presents several cases of extraction errorsmade by ChatGPT.</p>
<p>Table 7 demonstrate that our table NER without Score entities outperforms the previous table NER model, with improvements of 4.7, 4.9, and 5.1 points on precision, recall, and F 1 , respectively.
SettingsTable NERPRF11-shot ICL w Scores53.1 16.8 25.51-shot ICL w/o Scores 52.3 41.5 46.32-shot ICL w Scores57.4 24.7 34.52-shot ICL w/o Scores 53.2 49.2 51.1</p>
<p>Table 8 :
8
The performance of ChatGPT in the few-shot ICL setting with and without the Score entities.</p>
<p>Table 11 :
11
The prompt template of table RE leveraging 2-shot In-context Learning.[T] denotes the table we want to extract.Since the relations are very dense in tables, we here list part of the relations.</p>
<p>https://arxiv.org/
https://github.com/lukasschwab/arxiv.py
https://math.nist.gov/~BMiller/LaTeXML/
https://dashtable.readthedocs.io/en/latest
https://microsoft.github.io/OneLabeler-doc
Ethics StatementWe collect paper from the free distribution service arXiv 8 .The random crawled papers collected may not have been peer-reviewed.Currently, extraction is based on the assumption of the correctness of the public papers.
Ask me anything: A simple strategy for prompting language models. Avanika References Simran Arora, Narayan, Laurel J Mayee F Chen, Neel Orr, Kush Guha, Ines Bhatia, Frederic Chami, Christopher Sala, Ré, arXiv:2210.024412022arXiv preprint</p>
<p>Semeval 2017 task 10: Scienceie-extracting keyphrases and relations from scientific publications. Isabelle Augenstein, Mrinal Das, Sebastian Riedel, Lakshmi Vikraman, Andrew Mccallum, Proceedings of the 11th International Workshop on Semantic Evaluation. the 11th International Workshop on Semantic Evaluation2017. SemEval-2017</p>
<p>A dataset of information-seeking questions and answers anchored in research papers. Iz Beltagy, Kyle Lo, Arman Cohan, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)Kyle Lo, Iz Beltagy, Arman Cohan, Noah A Smith, and Matt Gardnerthe Association for Computational Linguistics2019. 20218Human Language Technologies</p>
<p>Turl: Table understanding through representation learning. Xiang Deng, Huan Sun, Alyssa Lees, You Wu, Cong Yu, 2020SIGMOD Rec51</p>
<p>Bert: Pre-training of deep bidirectional transformers for language understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, ArXiv, abs/1810.048052019</p>
<p>Active prompting with chain-ofthought for large language models. Shizhe Diao, Pengcheng Wang, Yong Lin, Tong Zhang, arXiv:2302.122462023arXiv preprint</p>
<p>Nlpcontributions: An annotation scheme for machine reading of scholarly contributions in natural language processing literature. D' Jennifer, Sören Souza, Auer, Proceedings of the 1st Workshop on Extraction and Evaluation of Knowledge Entities from Scientific Documents co-located with the ACM/IEEE Joint Conference on Digital Libraries in 2020. the 1st Workshop on Extraction and Evaluation of Knowledge Entities from Scientific Documents co-located with the ACM/IEEE Joint Conference on Digital Libraries in 2020AachenRWTH2020. 2020</p>
<p>Transfer learning for named entity recognition in financial and biomedical documents. Sumam Francis, Jordy Van Landeghem, Marie-Francine Moens, Information. 1082482019</p>
<p>SemEval-2018 task 7: Semantic relation extraction and classification in scientific papers. Kata Gábor, Davide Buscaldi, Anne-Kathrin Schumann, Behrang Qasemizadeh, Haïfa Zargayouna, Thierry Charnois, Proceedings of The 12th International Workshop on Semantic Evaluation. The 12th International Workshop on Semantic Evaluation2018</p>
<p>Is information extraction solved by chatgpt? an analysis of performance, evaluation criteria, robustness and errors. Ridong Han, Tao Peng, Chaohao Yang, Benyou Wang, Lu Liu, Xiang Wan, arXiv:2305.144502023arXiv preprint</p>
<p>Identification of tasks, datasets, evaluation metrics, and numeric scores for scientific leaderboards construction. Yufang Hou, Charles Jochim, Martin Gleize, Francesca Bonin, Debasis Ganguly, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. the 57th Annual Meeting of the Association for Computational Linguistics2019</p>
<p>Tdmsci: A specialized corpus for scientific literature entity tagging of tasks datasets and metrics. Yufang Hou, Charles Jochim, Martin Gleize, Francesca Bonin, Debasis Ganguly, Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume. the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume2021</p>
<p>Scirex: A challenge dataset for document-level information extraction. Sarthak Jain, Madeleine Van Zuylen, Hannaneh Hajishirzi, Iz Beltagy, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational Linguistics2020</p>
<p>Automated mining of leaderboards for empirical ai research. Salomon Kabongo, D' Jennifer, Sören Souza, Auer, International Conference on Asian Digital Libraries. Springer2021</p>
<p>Tian Lan, Tianyi Che, Zewen Chi, Xuhao Hu, Xianling Mao, Mozi: A scientific large-scale language model. 2023</p>
<p>Evaluating chatgpt's information extraction capabilities: An assessment of performance, explainability, calibration, and faithfulness. Bo Li, Gexiang Fang, Yang Yang, Quansen Wang, Wei Ye, Wen Zhao, Shikun Zhang, arXiv:2304.116332023arXiv preprint</p>
<p>Decoupled weight decay regularization. Ilya Loshchilov, Frank Hutter, International Conference on Learning Representations. 2018</p>
<p>Information extraction from scientific literature for method recommendation. Yi Luan, arXiv:1901.004012018arXiv preprint</p>
<p>Multi-task identification of entities, relations, and coreference for scientific knowledge graph construction. Yi Luan, Luheng He, Mari Ostendorf, Hannaneh Hajishirzi, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. the 2018 Conference on Empirical Methods in Natural Language Processing2018</p>
<p>A general framework for information extraction using dynamic span graphs. Yi Luan, Dave Wadden, Luheng He, Amy Shah, Mari Ostendorf, Hannaneh Hajishirzi, Proceedings of the 2019 Conference of the North American Chapter. Long and Short Papers. the 2019 Conference of the North American ChapterHuman Language Technologies20191</p>
<p>Gsap-ner: A novel task, corpus, and baseline for scholarly entity extraction focused on machine learning models and datasets. Wolfgang Otto, Matthäus Zloch, Lu Gan, Saurav Karmakar, Stefan Dietze, Findings of the Association for Computational Linguistics: EMNLP 2023. 2023</p>
<p>TIARA: Multi-grained retrieval for robust question answering over large knowledge base. Yiheng Shu, Zhiwei Yu, Yuhan Li, Börje F Karlsson, Tingting Ma, Yuzhong Qu, Chin-Yew Lin, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language ProcessingAbu Dhabi, United Arab Emirates2022Association for Computational Linguistics</p>
<p>Citationie: Leveraging the citation graph for scientific information extraction. Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony Hartshorn, Elvis Saravia, Andrew Poulton, Viktor Kerkez, Robert Stojnic, arXiv:2211.09085Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. Long Papers. the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing2022. 20211arXiv preprintGalactica: A large language model for science</p>
<p>Covid-19 literature knowledge graph construction and drug repurposing report generation. Qingyun Wang, Manling Li, Xuan Wang, Nikolaus Parulian, Guangxing Han, Jiawei Ma, Jingxuan Tu, Ying Lin, Ranran Haoran Zhang, Weili Liu, Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Demonstrations. the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Demonstrations2021</p>
<p>Transformers: State-of-the-art natural language processing. Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Clara Patrick Von Platen, Yacine Ma, Julien Jernite, Canwen Plu, Teven Xu, Sylvain Le Scao, Mariama Gugger, Quentin Drame, Alexander Lhoest, Rush, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations. the 2020 Conference on Empirical Methods in Natural Language Processing: System DemonstrationsOnline. Association for Computational Linguistics2020</p>
<p>TACR: A Table-alignment-based Cell-selection and Reasoning Model for Hybrid Question-Answering. Jian Wu, Yicheng Xu, Yan Gao, Jian-Guang Lou, Börje F Karlsson, Manabu Okumura, Findings of the Association for Computational Linguistics: ACL 2023. Toronto, CanadaAssociation for Computational Linguistics2023</p>
<p>Fine-tuned llms know more, hallucinate less with few-shot sequence-to-sequence semantic parsing over wikidata. Silei Xu, Shicheng Liu, Theo Culhane, Elizaveta Pertseva, Meng-Hsi Wu, Sina J Semnani, Monica S Lam, Proceedings of EMNLP2023. EMNLP20232023</p>
<p>Telin: Table entity linker for extracting leaderboards from machine learning publications. Sean Yang, Chris Tensmeyer, Curtis Wigington, Proceedings of the first Workshop on Information Extraction from Scientific Publications. the first Workshop on Information Extraction from Scientific Publications2022</p>
<p>Transfer learning for sequence tagging with hierarchical recurrent networks. Zhilin Yang, Ruslan Salakhutdinov, William W Cohen, 2017In ICLR (Poster</p>
<p>Packed levitated marker for entity and relation extraction. Deming Ye, Yankai Lin, Peng Li, Maosong Sun, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 60th Annual Meeting of the Association for Computational Linguistics20221</p>
<p>Onelabeler: A flexible system for building data labeling tools. Yu Zhang, Yun Wang, Haidong Zhang, Bin Zhu, Si Chen, Dongmei Zhang, Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems. the 2022 CHI Conference on Human Factors in Computing Systems2022</p>
<p>A frustratingly easy approach for entity and relation extraction. Zexuan Zhong, Danqi Chen, Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies2021</p>
<p>Prompts of table NER (2-shot In-context Learning) without Score entities Considering 6 entity types including 'Task. Yuchen Zhuang, Yinghao Li, Jerry Junyang Cheung, Yue Yu, Yingjun Mou, Xiang Chen, Le Song, Chao Zhang, arXiv:2210.14427Here are two demonstrations: Given type set. 2022arXiv preprintResel: N-ary relation extraction from scientific text and tables by learning to retrieve and select</p>
<p>If no entities are presented in any categories keep it None. Entities: {'Task': ['QQP', 'MRPC'], 'Dataset': ['MNLI-(m/mm). ' System, ' Mnli-(m/Mm)', ' Qqp', ' Qnli', ' Sst-2', 'cola, ' , ' Sts-B', ' Mrpc', ' Rte', 'average'], ['' , ' , ] , ['pre-Openai Sota ; ' , ' Qqp', ' Qnli', ' Sst-2', 'cola, ' , ' Sts-B', ' Mrpc ', ' Rte', 'average, 72.1OpenAI GPT. GLUE Test. Pre-OpenAIWNLI set. OpenAI GPT', 'BERT'], 'Method': [], 'Metric': ['F1 scores', 'Spearman correlations', 'accuracy scores'], 'Setting': []} Given type set</p>
<p>' System, ' Dev, ' Dev, ' Test, " Test, ' ] , ['' , ' Em', 'f1' , ' Em', ' F1'], 91.8Top Leaderboard Systems. Dec 10th. 2018. Dec 10th. 2018. Dec 10th. 2018. Dec 10th. 2018. Dec 10th. 2018Top Leaderboard Systems. 82.3', '91.2'], ['#1 Ensemblenlnet. 86.0', '91.7'], ['#2 Ensemble -QANet. 84.5', "90.5'. BiDAF+ELMo (Single)', '-', "85.6', '-', '85.8'], ['R.M. Reader (Ensemble)', '81.2', '87.9', '82.3', '88.5'], ['Ours', 'Ours', 'Ours', 'Ours', 'Ours'], ['bertbase(Single)', '80.8', '88.5', '-', '-'], ['bertlarge(Single. 'bertlarge(Sgl.+TriviaQA)', '84.2', '91.1', '85.1', '91.8'], ['bertlarge(Ens.+TriviaQA)', '86.2', '92.2', '87.4', '93.2']</p>
<p>If no entities are presented in any categories keep it None. Entities: {'Task. #1 Ensemble-nlnet', '#2 Ensemble -QANet', 'BiDAF+ELMo (Single)', 'R.M. Reader (Ensemble). Dec 10th. 2018Top Leaderboard Systems. Sgl.+TriviaQA', 'Ens.+TriviaQA']} Given type set: ['Task', 'Model', 'Method, 'Dataset', 'Metric', 'Setting'</p>
<p>Question: Please extract all relations from the given table and output a JSON object that contains the following: {[cell: cell]}. If no relations are presented keep it None. ' , ' Qqp', ' Qnli', ' Sst-2', 'cola, ' , ' Sts-B', ' Mrpc', ' Rte', 'average'], ['' , ' , ] , ['pre-Openai Sota, 88.5Relations: {['Pre-OpenAI SOTA':'MNLI-(m/mm)', 'Pre-OpenAI SOTA':'QQP','Pre-OpenAI SOTA':'QNLI','Pre-OpenAI SOTA':'SST-2', 'Pre-OpenAI SOTA':'CoLA', 'Pre-OpenAI SOTA':'STS-B', 'Pre-OpenAI SOTA':'MRPC', 'Pre-OpenAI SOTA':'RTE', 'Pre-OpenAI SOTA':'Average. Top Leaderboard Systems. R M Reader, 2 Ensemble -QANetDec 10th. 2018. Dec 10th. 2018. Dec 10th. 2018. Dec 10th. 2018. Dec 10th. 2018#1 Ensemblenlnet. 84.5', "90.5'. 82.3', '88.5'. bertlarge(Single)', '84.1', '90.9', '-', '-'], ['bertlarge(Ensemble)', '85.8', '91.8', '-', '-'], ['bertlarge(Sgl.+TriviaQA)', '84.2', '91.1', '85.1', '91.8'], ['bertlarge(Ens.+TriviaQA)', '86.2', '92.2', '87.4', '93.2']</p>
<p>Question: Please extract all relations from the given table and output a JSON object that contains the following: {[cell: cell]}. If no relations are presented keep it None. Relations: {['#1 Ensemble -nlnet':'Dev', '#1 Ensemble -nlnet':'EM', '#1 Ensemble -nlnet. )', '#1 Ensemble -nlnet': '86.0', '#1 Ensemble -nlnet': 'Test. Dec 10th, 2018Top Leaderboard Systems. #1 Ensemble -nlnet':'F1', '#1 Ensemblenlnet': '91.7' ]} Table: [T] Question: Please extract all relations from the given table and n output a JSON object that contains the following: {[cell: cell]}. If no relations are presented keep it None. Entities</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7674 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7674</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7674</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-143.html">extraction-schema-143</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to distill quantitative laws, equations, or functional relationships from collections of scholarly papers, including details of the models, prompting or fine‑tuning approaches, input corpora, extraction methods, types of laws, representation formats, evaluation datasets, metrics, baseline comparisons, validation procedures, and reported performance or limitations.</div>
                <p><strong>Paper ID:</strong> paper-252065987</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2306.09339v1.pdf" target="_blank">From BERT to GPT-3 Codex: Harnessing the Potential of Very Large Language Models for Data Management</a></p>
                <p><strong>Paper Abstract:</strong> Large language models have recently advanced the state of the art on many natural language processing benchmarks. The newest generation of models can be applied to a variety of tasks with little to no specialized training. This technology creates various opportunities for applications in the context of data management. The tutorial will introduce participants to basic background on language models, discuss different methods to use language models, and give an overview and short demonstration of available libraries and APIs. Models for generating natural language will be considered as well as models, such as GPT-3 Codex, which complete program code or generate code from natural language instructions. Finally, the tutorial will discuss recent research in the database community that exploits language models in the context of traditional database systems or proposes novel system architectures that are based on them. The tutorial is targeted at database researchers. No prior background on language models is required. The goal of the tutorial is to introduce database researchers to the latest generation of language models, and to their use cases in the domain of data management.</p>
                <p><strong>Cost:</strong> 0.005</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Language models as knowledge bases: On entity representations, storage capacity, and paraphrased queries <em>(Rating: 2)</em></li>
                <li>Can Foundation Models Wrangle Your Data?. <em>(Rating: 2)</em></li>
                <li>Language Models Enable Simple Systems for Generating Structured Views of Heterogeneous Data Lakes. <em>(Rating: 2)</em></li>
                <li>Can deep neural networks predict data correlations from column names <em>(Rating: 1)</em></li>
                <li>Language models are few-shot learners <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7674",
    "paper_id": "paper-252065987",
    "extraction_schema_id": "extraction-schema-143",
    "extracted_data": [],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Language models as knowledge bases: On entity representations, storage capacity, and paraphrased queries",
            "rating": 2,
            "sanitized_title": "language_models_as_knowledge_bases_on_entity_representations_storage_capacity_and_paraphrased_queries"
        },
        {
            "paper_title": "Can Foundation Models Wrangle Your Data?.",
            "rating": 2,
            "sanitized_title": "can_foundation_models_wrangle_your_data"
        },
        {
            "paper_title": "Language Models Enable Simple Systems for Generating Structured Views of Heterogeneous Data Lakes.",
            "rating": 2,
            "sanitized_title": "language_models_enable_simple_systems_for_generating_structured_views_of_heterogeneous_data_lakes"
        },
        {
            "paper_title": "Can deep neural networks predict data correlations from column names",
            "rating": 1,
            "sanitized_title": "can_deep_neural_networks_predict_data_correlations_from_column_names"
        },
        {
            "paper_title": "Language models are few-shot learners",
            "rating": 1,
            "sanitized_title": "language_models_are_fewshot_learners"
        }
    ],
    "cost": 0.00511425,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>From BERT to GPT-3 Codex: Harnessing the Potential of Very Large Language Models for Data Management
15 Jun 2023</p>
<p>Immanuel Trummer itrummer@cornell.edu 
Cornell University Ithaca
NY</p>
<p>From BERT to GPT-3 Codex: Harnessing the Potential of Very Large Language Models for Data Management
15 Jun 20233D3BE327D1DA4352ADE8DAC79EBE492E10.14778/3554821.3554896arXiv:2306.09339v1[cs.DB]
Large language models have recently advanced the state of the art on many natural language processing benchmarks.The newest generation of models can be applied to a variety of tasks with little to no specialized training.This technology creates various opportunities for applications in the context of data management.The tutorial will introduce participants to basic background on language models, discuss different methods to use language models, and give an overview and short demonstration of available libraries and APIs.Models for generating natural language will be considered as well as models, such as GPT-3 Codex, which complete program code or generate code from natural language instructions.Finally, the tutorial will discuss recent research in the database community that exploits language models in the context of traditional database systems or proposes novel system architectures that are based on them.The tutorial is targeted at database researchers.No prior background on language models is required.The goal of the tutorial is to introduce database researchers to the latest generation of language models, and to their use cases in the domain of data management.</p>
<p>INTRODUCTION</p>
<p>The area of natural language processing (NLP) has recently been revolutionized by the advent of large "language models", trained on huge quantities of unlabeled text [97].Given sufficiently large amounts of training data and parameters, such models can tackle a broad range of tasks with little to no specialized training [5].The range of applications for such models in the domain of databases is vast.It ranges from novel interfaces [11,12,32,59,69,83,88] to new system architectures [77,84], based on data representations and processing mechanisms that are enabled by the latest generation of language models.The goal of this tutorial is to introduce database researchers to the possibilities offered by these models, to provide pointers to libraries and APIs that make them accessible [60,97], and to review recent research in the database community exploiting them.The tutorial will cover language models that process and generate natural language text [15,18], as well as more recent models that generate program code from natural language descriptions [9].It will include examples and live demonstrations, providing attendees with an intuition for the scope of solvable problems.</p>
<p>The tutorial is aimed at database researchers.No prior background in language models or NLP is expected.The tutorial will start with a short, high-level introduction to the Transformer [89], a novel neural network architecture that has has enabled many of the recent advances in NLP.Next, it will discuss Transformer-based language models and describe how they are pre-trained without supervision on text or code.For model sizes in the hundreds of millions of parameters [15,45,52,63], pre-training is typically followed by another (short) training phase on task-specific samples ("fine-tuning").Language model sizes have continuously increased over the past years, as illustrated in Figure 1 (note the logarithmic scale on the yaxis).The latest generation of language models with sizes in the hundreds of billions of parameters [9,13,17,18,27,50,64,65,73,76,103] can often be used without further specialization ("prompting").The tutorial will discuss and demonstrate both methods.Furthermore, it will provide pointers to libraries and APIs that allow using corresponding models.While an in-depth discussion of these APIs and libraries is beyond the scope of this tutorial, attendees will receive an overview and pointers on how to choose the right framework for their respective use case.</p>
<p>Finally, the tutorial will discuss recent research in the database community that exploits language models.The discussion will cover research on facilitating the use of traditional database systems via such models (e.g., by advanced user interfaces [71,75]).Also, it will include research that exploits language models to revise fundamental design decisions in database systems [26,77,84].The total duration of the tutorial is 1.5 hours, including questions and discussions.</p>
<p>The reminder of this proposal is organized as follows.Section 2 describes the topics covered in the tutorial in more detail.Section 3 describes the organization and timeline of the tutorial.Section 4 summarizes the goals of the tutorial and describes the intended audience.Section 5 contrasts the tutorial content from other, recent tutorials in the database community.Finally, Section 6 contains biographical details on the presenter.</p>
<p>TOPICS COVERED</p>
<p>The tutorial will cover the following topics.</p>
<p>Rise of the Transformer</p>
<p>At the heart of the NLP revolution is a novel neural network architecture, the so-called Transformer [89].The Transformer is nowadays the dominant architecture for various NLP tasks [97].Beyond NLP, it is increasingly being adopted in other domains such as computer vision [1,16,22,24,51,53,56,101,102,106], audio analysis [4,8,20,21,42,55,57,66,90,91], and multi-modal data analysis [6,7,14,19,29,49,62,70,72,92,104].</p>
<p>The tutorial will introduce the main ideas behind the Transformer model.In particular, it will discuss the concept of attention mechanisms [89].The goal of this part is to give the audience an intuition for why Transformer models were able to advance the state of the art in NLP, compared to prior methods such as recurrent neural networks [43].Explanations will be kept at a relatively high level of abstraction.Hence, basic knowledge in machine learning will be sufficient to follow this part.</p>
<p>Pre-Trained Language Models</p>
<p>Compared to prior architectures, the Transformer makes parallelizing the training process easier.In part, this has enabled the creation of very large language models.Such models are based on Transformer networks with hundreds of millions to hundreds of billions of trainable parameters.</p>
<p>Language models are trained on tasks for which large amounts of training data are readily available.For instance, models such as BERT [15] learn to fill in obfuscated words in Web text (masked language modeling).Models such as GPT-3 learn to complete text or code based on a prefix [18].In all those cases, manual labeling of training data is not required.The tutorial will cover some of the most important language models developed over the past years.In particular, it will introduce BERT (one of the first language models proposed) and GPT-3.For the latter model, the tutorial will cover the base version [18] (optimized for completing natural language text) as well as the Codex variant [9] (optimized for generating code from natural language instructions).</p>
<p>Fine-Tuning and Prompting</p>
<p>Language models provide the fundament for approaches that solve various tasks, related to natural language and code.Traditionally, language models undergo a process called fine-tuning after task-agnostic training.Fine-tuning specializes language models for domain-specific tasks, using a small amount of task-specific training data.Compared to training a new network from scratch, fine-tuning reduces the amount of training data and computational overheads very significantly [28].This is possible due to transfer learning [67], as generic knowledge about language can be transferred across different tasks.</p>
<p>Fine-tuning has been the primary method of using language models until quite recently.As language models grew further in size, it became apparent that providing task-specific instructions as input, together with few or even no examples [5], is often sufficient to solve formerly unseen tasks.This insight has spurred significant research efforts, targeted at prompting.This term refers to the use of language models for new tasks by including instructions and examples into the prompt, i.e. the input to be completed by the language model.The tutorial will discuss fine-tuning briefly and focus on prompting.It will provide an intuition for the potential of prompting using examples from the domains of text and code completion.</p>
<p>APIs and Libraries</p>
<p>Language models are nowadays available via various channels.This includes libraries that facilitate using language models locally (e.g., the Huggingface Transformers library [97]).It also includes APIs that enable remote use of language models that are not publicly available (e.g., OpenAI's GPT-3 model [18]).</p>
<p>The tutorial will introduce some of the most popular frameworks for accessing language models.Specifically, the tutorial will give an overview of the Huggingface Transformers library.This library facilitates tasks such as training and inference.Also, the tutorial will include a demonstration based on OpenAI's API.This API enables access to the GPT-3 series of language models, including</p>
<p>Applications in Data Management</p>
<p>Finally, the tutorial will discuss novel applications of language models in the database area.This tutorial section will be split into two parts.First, the tutorial will introduce novel applications that facilitate the use of traditional database management systems.Perhaps the most classical use case for NLP in the context of database systems is text-to-SQL translation [23, 46, 68, 69, 94-96, 98-100, 105].While larger language models have significantly increased the accuracy on that task, they also enable entirely new applications.Here, the tutorial will cover recent research leveraging language models for tasks such as data preparation and integration [2,74,75], fact checking from data [10, 25, 33-40, 81, 82], or database tuning [78][79][80][85][86][87].</p>
<p>Second, the tutorial will discuss novel architectures for data processing systems that are enabled by the advent of large language models.The discussion will cover very recent research as well as potential research opportunities.Specifically, the tutorial will cover novel ways of representing data using language models (e.g., by storing data as natural language facts [77] or by integrating data within the language model [26]).Also, it will discuss the use of language models in the execution engine (e.g., to implement operators [74,77] or to synthesize code for data processing [84]).</p>
<p>TUTORIAL ORGANIZATION</p>
<p>Table 1 gives an overview of the tutorial parts, as well as their estimated duration.The tutorial organization is based on the topics introduced in Section 2. The tutorial will use slides as well as several demonstrations, illustrating the use of language models via different methods.Questions and comments are welcome throughout the tutorial.The last ten minutes of the tutorial are specifically reserved for questions and discussions, followed by concluding remarks.</p>
<p>GOALS AND AUDIENCE</p>
<p>The goal of this tutorial is to introduce the database community to the latest generation of language models.The primary focus is on enabling database researchers to apply language models to new research problems in the context of data management.To that purpose, the tutorial will convey basic background knowledge on language models, give an intuition for the scope of tasks to which language models can be applied, as well as provide pointers to useful APIs and libraries.Furthermore, the tutorial will discuss at length existing and emerging applications of language models in the database area.</p>
<p>In line with the goals of the tutorial, no prior background knowledge on language models is expected from the audience.Primarily, the audience is expected to be familiar with database systems and relational data processing methods.Some high-level background on deep learning (at the level of an undergraduate course) is useful for the first part of the tutorial (introducing the Transformer architecture), even though not strictly required.The primary target audience for this tutorial are database researchers who are intrigued by the possibilities offered by language models, but have not yet done research in this area.</p>
<p>RELATIONSHIP TO PRIOR TUTORIALS</p>
<p>The proposed tutorial connects but is complementary to prior tutorials in the database community.Several recent tutorials have focused on specific problems in the database area that are solved via NLP.Most notably, several recent tutorials [3,41] discussed approaches for text-to-SQL translation in detail.Other recent tutorials covered approaches for automated fact checking [44], information extraction [58], or entity embedding [61].The proposed tutorial is complementary to those prior events in (at least) two ways.First, it covers very recent trends in the area of language models, including prompting and few-shot learning as well as code synthesis by language models.The underlying technologies, e.g. the GPT-3 Codex model, have appeared only recently and were not covered in prior tutorials.Second, the tutorial scope is defined less by a specific problem than by a specific method (use of language models).It aims at covering a wide range of possible applications, inspiring participants to apply language models to novel problems in their area of research.</p>
<p>More broadly, the proposed tutorial relates to prior events, connecting databases and machine learning topics [30,31,47,48,54,93].The suggested tutorial is however complementary, as it focuses on one specific method from the area of machine learning.</p>
<p>Figure 1 :
1
Figure 1: Evolution of parameter counts in language models.</p>
<p>Table 1 :
1
Tutorial organization overview.Codex model that generates code from natural language instructions.The goal of the tutorial is not to cover any of those APIs in depth.Instead, it aims at giving an intuition for the potential use cases of each framework, as well as references for studying them in more detail.
PartDurationWelcome and introduction5 minRise of the Transformer10 minPre-trained language models10 minFine-tuning and prompting10 minAPIs and libraries20 minApplications in data management 25 minFinal discussion and conclusion10 minthe GPT-3
PRESENTERImmanuel Trummer is assistant professor for computer science at Cornell University. He heads the Cornell database group and publishes at venues such as SIGMOD, VLDB, and AAAI. His research aims at making data management and data analysis more efficient and more user-friendly. Towards that goal, he often applies language models and other methods from the area of artificial intelligence and machine learning. Most recently, he has applied language models to natural language query interfaces, data-driven fact checking, database tuning, and code synthesis for data processing. His papers were selected for "Best of VLDB", "Best of SIGMOD", for the ACM SIGMOD Research Highlight Award, and for publication in CACM as CACM Research Highlight. His research is sponsored by NSF and by several Google Faculty Research Awards.
The source code, data, and/or other artifacts have been made available at https://itrummer.github.io/lm4db/.
ViViT: A Video Vision Transformer. Anurag Arnab, Mostafa Dehghani, Georg Heigold, Chen Sun, Mario Lučić, Cordelia Schmid, 10.1109/ICCV48922.2021.00676arXiv:2103.15691Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer Vision2021. 2021</p>
<p>Language Models Enable Simple Systems for Generating Structured Views of Heterogeneous Data Lakes. Simran Arora, Brandon Yang, Sabri Eyuboglu, Avanika Narayan, Andrew Hojel, Immanuel Trummer, Christopher Ré, 10.48550/arXiv.2304.09433arXiv:2304.094332023. 2023</p>
<p>State of the Art and Open Challenges in Natural Language Interfaces to Data. Fatma Åzcan, Abdul Quamar, Jaydeep Sen, Chuan Lei, Vasilis Efthymiou, 10.1145/3318464.3383128Proceedings of the ACM SIGMOD International Conference on Management of Data. the ACM SIGMOD International Conference on Management of Data2020. 2020</p>
<p>MAE-AST: Masked Autoencoding Audio Spectrogram Transformer. Alan Baade, Puyuan Peng, David Harwath, 10.21437/Interspeech.2022-10961arXiv:2203.16691Proceedings of the Annual Conference of the International Speech Communication Association. the Annual Conference of the International Speech Communication Association2022</p>
<p>Language models are few-shot learners. Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, arXiv:2005.14165Advances in Neural Information Processing Systems. Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford2020Ilya Sutskever, and Dario Amodei</p>
<p>On Pursuit of Designing Multi-modal Transformer for Video Grounding. Meng Cao, Long Chen, Mike Zheng Shou, Can Zhang, Yuexian Zou, 10.18653/v1/2021.emnlp-main.773arXiv:2109.06085EMNLP 2021 -2021 Conference on Empirical Methods in Natural Language Processing, Proceedings. 9810-9823. 2021</p>
<p>MM-ViT: Multi-Modal Video Transformer for Compressed Video Action Recognition. Jiawei Chen, Man Chiu, Ho, 10.1109/WACV51458.2022.00086arXiv:2108.09322Proceedings -2022 IEEE/CVF Winter Conference on Applications of Computer Vision. -2022 IEEE/CVF Winter Conference on Applications of Computer Vision2022</p>
<p>Hts-At: a Hierarchical Token-Semantic Audio Transformer for Sound Classification and Detection. Ke Chen, Xingjian Du, Bilei Zhu, Zejun Ma, Taylor Berg-Kirkpatrick, Shlomo Dubnov, 10.1109/ICASSP43922.2022.9746312arXiv:2202.00874IEEE International Conference on Acoustics, Speech and Signal Processing -Proceedings. 2022. 2022-May (2022</p>
<p>Evaluating Large Language Models Trained on Code. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Pondé De Oliveira Pinto, Jared Kaplan, Harrison Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, arXiv:2107.03374Joshua Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech ZarembaJan Leike. 2021. 2021Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N CarrMatthias Plappert</p>
<p>TabFact: A Largescale Dataset for Table-based Fact Verification. Wenhu Chen, Hongmin Wang, Jianshu Chen, Yunkai Zhang, Hong Wang, Shiyang Li, Xiyou Zhou, William Yang, Wang , arXiv:1909.021642019. 2019</p>
<p>PI2: End-to-end Interactive Visualization Interface Generation from Queries. Yiru Chen, Eugene Wu, 10.1145/3514221.3526166arXiv:2107.08203Proceedings of the ACM SIGMOD International Conference on Management of Data. the ACM SIGMOD International Conference on Management of Data2022. 2022</p>
<p>Symphony : Towards Natural Language Query Answering over Multi-modal Data Lakes. Zui Chen, Ju Fan, Sam Madden, Nan Tang, CIDR. 2023</p>
<p>PaLM: Scaling Language Modeling with Pathways. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Emily Vinodkumar Prabhakaran, Nan Reif, Ben Du, Reiner Hutchinson, James Pope, Jacob Bradbury, Michael Austin, Guy Isard, Pengcheng Gur-Ari, Toju Yin, Anselm Duke, Sanjay Levskaya, Sunipa Ghemawat, Henryk Dev, Xavier Michalewski, Vedant Garcia, Kevin Misra, Liam Robinson, Denny Fedus, Daphne Zhou, David Ippolito, Hyeontaek Luan, Barret Lim, Alexander Zoph, Ryan Spiridonov, Sepassi, arXiv:2204.02311David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck2022Jeff Dean, Slav PetrovCoRR abs/2204.0 (2022</p>
<p>Transmed: Transformers advance multi-modal medical image classification. Yin Dai, Yifan Gao, Fayu Liu, 10.3390/diagnostics11081384arXiv:2103.05940Diagnostics. 112021. 2021</p>
<p>BERT: Pre-training of deep bidirectional transformers for language understanding. Jacob Devlin, Ming Wei Chang, Kenton Lee, Kristina Toutanova, arXiv:1810.04805NAACL. 2019</p>
<p>CSWin Transformer: A General Vision Transformer Backbone with Cross-Shaped Windows. Xiaoyi Dong, Jianmin Bao, Dongdong Chen, Weiming Zhang, Nenghai Yu, Lu Yuan, Dong Chen, Baining Guo, 10.1109/CVPR52688.2022.01181arXiv:2107.00652Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition. the IEEE Computer Society Conference on Computer Vision and Pattern Recognition2022. 2022-June (2022</p>
<p>Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity. William Fedus, Barret Zoph, Noam Shazeer, arXiv:2101.03961Journal of Machine Learning Research. 2312022. 2022</p>
<p>GPT-3: Its Nature, Scope, Limits, and Consequences. Minds and Machines. Luciano Floridi, Massimo Chiriatti, 10.1007/s11023-020-09548-12020. 202030</p>
<p>Multimodal Transformer for Video Retrieval. Valentin Gabeur, Chen Sun, Karteek Alahari, Cordelia Schmid, 10.1007/978-3-030-58548-8_13LNCS. 2020. 12349. 2020</p>
<p>Ast: Audio spectrogram transformer. Yuan Gong, Yu , An Chung, James Glass, 10.21437/Interspeech.2021-698arXiv:2104.01778Proceedings of the Annual Conference of the International Speech Communication Association. the Annual Conference of the International Speech Communication Association2021. 20211</p>
<p>SSAST: Self-Supervised Audio Spectrogram Transformer. Yuan Gong, I Jeff Cheng, Yu Lai, An Chung, James Glass, 10.1609/aaai.v36i10.21315arXiv:2110.09784Proceedings of the 36th AAAI Conference on Artificial Intelligence, AAAI 2022. the 36th AAAI Conference on Artificial Intelligence, AAAI 20222022</p>
<p>LeViT: a Vision Transformer in Con-vNet's Clothing for Faster Inference. Ben Graham, Alaaeldin El-Nouby, Hugo Touvron, Pierre Stock, Armand Joulin, Hervé Jégou, Matthijs Douze, 10.1109/ICCV48922.2021.01204arXiv:2104.01136Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer Vision2021. 2021</p>
<p>Towards Complex Text-to-SQL in Cross-Domain Database with Intermediate Representation. Jiaqi Guo, Zecheng Zhan, Yan Gao, Yan Xiao, Jian-Guang Lou, Ting Liu, Dongmei Zhang, 10.18653/v1/p19-1444arXiv:1905.082052019</p>
<p>A Survey on Vision Transformer. Kai Han, Yunhe Wang, Hanting Chen, Xinghao Chen, Jianyuan Guo, Zhenhua Liu, Yehui Tang, An Xiao, Chunjing Xu, Yixing Xu, Zhaohui Yang, Yiman Zhang, Dacheng Tao, 10.1109/TPAMI.2022.3152247arXiv:2012.12556IEEE Transactions on Pattern Analysis and Machine Intelligence. 2022. 2022</p>
<p>ClaimBuster: the first-ever end-to-end fact-checking system. Naeemul Hassan, Gensheng Zhang, Fatma Arslan, Josue Caraballo, Damian Jimenez, Siddhant Gawsane, Shohedul Hasan, Minumol Joseph, Aaditya Kulkarni, Anil Kumar Nayak, Vikas Sable, Chengkai Li, Mark Tremayne, 2017. 201710</p>
<p>Language models as knowledge bases: On entity representations, storage capacity, and paraphrased queries. Benjamin Heinzerling, Kentaro Inui, arXiv:2008.09036EACL 2021. 2021</p>
<p>Oriol Vinyals, and Laurent Sifre. 2022. Training Compute-Optimal Large Language Models. Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego De Las, Lisa Anne Casas, Johannes Hendricks, Aidan Welbl, Tom Clark, Eric Hennigan, Katie Noland, George Millican, Bogdan Van Den Driessche, Aurelia Damoc, Simon Guy, Karen Osindero, Erich Simonyan, Jack W Elsen, Rae, arXiv:2203.15556CoRR abs/2203.1 (2022</p>
<p>Parameter-efficient transfer learning for NLP. Neil Houlsby, Andrei Giurgiu, Stanisraw Jastrzçbski, Bruna Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, Sylvain Gelly, ICML.4944-4953.arXiv:1902.007512019</p>
<p>Pixel-BERT: Aligning Image Pixels with Text by Deep Multi-Modal Transformers. Zhicheng Huang, Zhaoyang Zeng, Bei Liu, Dongmei Fu, Jianlong Fu, arXiv:2004.008492020. CoRR abs/2004.0 (2020</p>
<p>From auto-tuning one size fits all to self-designed and learned data-intensive systems. Stratos Idreos, Tim Kraska, 10.1145/3299869.3314034Proceedings of the ACM SIGMOD International Conference on Management of Data. the ACM SIGMOD International Conference on Management of Data2019. 2019</p>
<p>Machine learning for cloud data systems: The progress so far and the path forward. Alekh Jindal, Matteo Interlandi, 10.14778/3476311.3476408Proceedings of the VLDB Endowment. the VLDB Endowment2021. 202114</p>
<p>Demonstration of ThalamusDB: Answering Complex SQL Queries with Natural Language Predicates on Multi-Modal Data. Saehan Jo, Immanuel Trummer, 10.1145/3555041.3589730SIGMOD. 2023</p>
<p>Verifying text summaries of relational data sets. Saehan Jo, Immanuel Trummer, Weicheng Yu, Daniel Liu, Xuezhi Wang, Cong Yu, Mehta Niyati, arXiv:/arxiv.org/abs/1804.07686201816pages</p>
<p>Verifying text summaries of relational data sets. Saehan Jo, Immanuel Trummer, Weicheng Yu, Xuezhi Wang, Cong Yu, Daniel Liu, Niyati Mehta, SIGMOD. 2019</p>
<p>AggChecker: a fact-checking system for text summaries of relational data sets. Saehan Jo, Immanuel Trummer, Weicheng Yu, Xuezhi Wang, Cong Yu, Daniel Liy, Niyati Mehta, 2019. 201912</p>
<p>Scrutinizer: A mixed-initiative approach to large-scale, data-driven claim verification. Georgios Karagiannis, Mohammed Saeed, PVLDB. 132020. 2020Paolo Papotti, and Immanuel Trummer</p>
<p>Scrutinizer: a mixed-initiative approach to large-scale, data-driven claim verification. Georgios Karagiannis, Mohammed Saeed, 2020Technical ReportPaolo Papotti, and Immanuel Trummer. Extended Technical Report</p>
<p>Scrutinizer: a system for fact-checking statistical claims. Georgios Karagiannis, Mohammed Saeed, 2020Paolo Papotti, and Immanuel Trummer</p>
<p>Scrutinizer: Fact Checking Statistical Claims. Georgios Karagiannis, Mohammed Saeed, 10.14778/3415478.3415520PVLDB. 132020. 2020Paolo Papotti, and Immanuel Trummer</p>
<p>Mining an "anti-knowledge base" from Wikipedia updates with applications to fact checking and beyond. Georgios Karagiannis, Immanuel Trummer, Saehan Jo, Shubham Khandelwal, Xuezhi Wang, Cong Yu, PVLDB. 132020. 2020</p>
<p>A Deep Dive into Deep Learning Approaches for Text-to-SQL Systems. George Katsogiannis, -Meimarakis , Georgia Koutrika, 10.1145/3448016.3457543SIGMOD. 2846-2851. 2021</p>
<p>Efficient Training of Audio Transformers with Patchout. Khaled Koutini, Jan Schlüter, Hamid Eghbal-Zadeh, Gerhard Widmer, 10.21437/Interspeech.2022-227arXiv:2110.05069Proceedings of the Annual Conference of the International Speech Communication Association. the Annual Conference of the International Speech Communication Association2022</p>
<p>A comparison of transformer and recurrent neural networks on multilingual neural machine translation. M Surafel, Mauro Lakew, Marcello Cettolo, Federico, arXiv:1806.06957COLING 2018 -27th International Conference on Computational Linguistics. 2018. 2018</p>
<p>Combating fake news: A data management and mining perspective. V S Laks, Michael Lakshmanan, Saravanan Simpson, Thirumuruganathan, 10.14778/3352063.3352117PVLDB. 122018. 2018</p>
<p>BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension. Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, Luke Zettlemoyer, 10.18653/v1/2020.acl-main.703arXiv:1910.134612020</p>
<p>NaLIR: an interactive natural language interface for querying relational databases. Fei Li, Jagadish, SIGMOD. 709-7122014</p>
<p>AI Meets Database: AI4DB and DB4AI. Guoliang Li, Xuanhe Zhou, Lei Cao, 10.1145/3448016.3457542Proceedings of the ACM SIGMOD International Conference on Management of Data. the ACM SIGMOD International Conference on Management of Data2021. 2021</p>
<p>Machine learning for databases. Guoliang Li, Xuanhe Zhou, Lei Cao, 10.14778/3476311.3476405Proceedings of the VLDB Endowment. the VLDB Endowment2021. 202114</p>
<p>VUT: Versatile UI Transformer for Multi-Modal Multi-Task User Interface Modeling. Yang Li, Gang Li, Xin Zhou, Mostafa Dehghani, Alexey Gritsenko, arXiv:2112.056922021. CoRR abs/2112.0 (2021</p>
<p>Jurassic-1: Technical details and evaluation. Opher Lieber, Or Sharir, Barak Lenz, Yoav Shoham, 2021Technical Report</p>
<p>Visformer : The Vision-friendly Transformer. Xuefeng Liu, Longhui Wei, Qi Tian, Zhengsu Chen, Lingxi Xie, Jianwei Niu, ICCV. 589-5982021</p>
<p>RoBERTa: A robustly optimized BERT pretraining approach. Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov, arXiv:1907.116922019. CoRR abs/1907.1, 1 (2019</p>
<p>Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, Baining Guo, Swin Transformer. 2021 IEEE/CVF International Conference on Computer Vision (ICCV). 2021. 2021</p>
<p>Speedup your analytics: Automatic parameter tuning for databases and big data systems. Jiaheng Lu, Yuxing Chen, Herodotos Herodotou, Shivnath Babu, 10.14778/3352063.3352112Proceedings of the VLDB Endowment. the VLDB Endowment2018. 201812</p>
<p>SpecTNT: a Time-Frequency Transformer for Music Audio. Wei-Tsung Lu, Ju-Chiang Wang, Minz Won, Keunwoo Choi, Xuchen Song, arXiv:2110.091272021. CoRR abs/2110.0 (2021</p>
<p>Towards Robust Vision Transformer. Xiaofeng Mao, Gege Qi, Yuefeng Chen, Xiaodan Li, Ranjie Duan, Shaokai Ye, Yuan He, Hui Xue, 10.1109/CVPR52688.2022.01173arXiv:2105.07926Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition. the IEEE Computer Society Conference on Computer Vision and Pattern Recognition2022. 2022-June (2022</p>
<p>Audio Captioning Transformer. Xinhao Mei, Xubo Liu, Qiushi Huang, Mark D Plumbley, Wenwu Wang, arXiv:2107.098172021. 2021</p>
<p>TextCube: Automated construction and multidimensional exploration. Yu Meng, Jiaxin Huang, Jingbo Shang, Jiawei Han, 10.14778/3352063.3352113Proceedings of the VLDB Endowment. the VLDB Endowment2018. 201812</p>
<p>Can Foundation Models Wrangle Your Data?. Avanika Narayan, Ines Chami, Laurel Orr, Christopher Ré, arXiv:2205.09911PVLDB. 162022. 2022</p>
<p>Managing ml pipelines: Feature stores and the coming wave of embedding ecosystems. Laurel Orr, Atindriyo Sanyal, Xiao Ling, Karan Goel, Megan Leszczynski, 10.14778/3476311.3476402arXiv:2108.05053Proceedings of the VLDB Endowment. the VLDB Endowment2021. 202114</p>
<p>Multi-Modal Fusion Transformer for End-to-End Autonomous Driving. Aditya Prakash, Kashyap Chitta, Andreas Geiger, 10.1109/CVPR46437.2021.00700arXiv:2104.09224Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition. the IEEE Computer Society Conference on Computer Vision and Pattern Recognition2021</p>
<p>Language Models are Unsupervised Multitask Learners. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, OpenAI Blog. 12020. 2020</p>
<p>Jack W Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, Eliza Rutherford, Tom Hennigan, Jacob Menick, Albin Cassirer, Richard Powell, George Van Den Driessche, Lisa Anne Hendricks, Maribeth Rauh, Po-Sen Huang, Amelia Glaese, Johannes Welbl, Sumanth Dathathri, Saffron Huang, Jonathan Uesato, John Mellor, Irina Higgins, Antonia Creswell, Nat Mcaleese, Amy Wu, Erich Elsen, Siddhant Jayakumar, Elena Buchatskaya, David Budden, Esme Sutherland, Karen Simonyan, Michela Paganini, Laurent Sifre, Lena Martens, Xiang , Lorraine Li, Adhiguna Kuncoro, Aida Nematzadeh, Elena Gribovskaya, Domenic Donato, Angeliki Lazaridou, Arthur Mensch, Jean-Baptiste Lespiau, Maria Tsimpoukelli, Nikolai Grigorev, Doug Fritz, Thibault Sottiaux, Mantas Pajarskas, Toby Pohlen, Zhitao Gong, Daniel Toyama, Cyprien De Masson D'autume, Yujia Li, Tayfun Terzi, Vladimir Mikulik, Igor Babuschkin, Aidan Clark, arXiv:2112.11446Scaling Language Models: Methods, Analysis and Insights from Training Gopher. Blake Hechtman, Laura Weidinger, Iason Gabriel, William Isaac, Aurelia Guy, Chris Jones, James Bradbury, Matthew Johnson; Ed Lockhart, Simon Osindero, Laura Rimell, Chris Dyer, Oriol Vinyals, Kareem Ayoub, Jeff Stanway, Lorrayne Bennett, Demis HassabisKoray Kavukcuoglu, and Geoffrey Irving2022CoRR abs/2112.1 (2022</p>
<p>Exploring the limits of transfer learning with a unified text-to-text transformer. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, arXiv:1910.10683Journal of Machine Learning Research. 2112020. 2020</p>
<p>SepTr: Separable Transformer for Audio Spectrogram Processing. Radu Nicolae Cătălin Ristea, Tudor Ionescu, Fahad Shahbaz Khan, 10.21437/Interspeech.2022-249arXiv:2203.09581Proceedings of the Annual Conference of the International Speech Communication Association. the Annual Conference of the International Speech Communication Association2022</p>
<p>Transfer Learning in Natural Language Processing. Sebastian Ruder, Matthew E Peters, Swabha Swayamdipta, Thomas Wolf, ACL: Tutorials. 2019</p>
<p>ATHENA: An ontology-driven system for natural language querying over relational data stores. Diptikalyan Saha, Avrilia Floratou, Karthik Sankaranarayanan, Umar Farooq Minhas, Ashish R Mittal, Fatma Ozcan, 2016. 20169</p>
<p>PICARD: Parsing Incrementally for Constrained Auto-Regressive Decoding from Language Models. Torsten Scholak, Nathan Schucher, Dzmitry Bahdanau, 10.18653/v1/2021.emnlp-main.779arXiv:2109.050932021In EMNLP. 9895-9901</p>
<p>AAFormer: A Multi-Modal Transformer Network for Aerial Agricultural Images. Yao Shen, Lei Wang, Yue Jin, 10.1109/CVPRW56347.2022.00177IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops. 2022. June. 1704-17102022</p>
<p>Evaluating the Text-to-SQL Capabilities of Large Language Models. Richard Shin, Benjamin Van Durme, 2021. CoRR abs/2204.0, 1 (2021</p>
<p>Everything at Once -Multi-modal Fusion Transformer for Video Retrieval. Nina Shvetsova, Brian Chen, Andrew Rouditchenko, Samuel Thomas, Brian Kingsbury, Rogerio Feris, David Harwath, James Glass, Hilde Kuehne, 10.1109/CVPR52688.2022.01939arXiv:2112.04446Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition. the IEEE Computer Society Conference on Computer Vision and Pattern Recognition2022. 19988-19997</p>
<p>Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model. Shaden Smith, Mostofa Patwary, Brandon Norick, Patrick Legresley, Samyam Rajbhandari, Jared Casper, Zhun Liu, Shrimai Prabhumoye, George Zerveas, Vijay Korthikanti, Elton Zhang, Rewon Child, Reza Yazdani Aminabadi, Julie Bernauer, Xia Song, Mohammad Shoeybi, Yuxiong He, Michael Houston, Saurabh Tiwary, Bryan Catanzaro, arXiv:2201.119902022. CoRR abs/2201.1 (2022</p>
<p>Ember: No-Code Context Enrichment via similarity-based keyless joins. Sahaana Suri, Ihab Ilyas, Christopher Re, Theodoros Rekatsinas, arXiv:arXiv:2106.01501v1PVLDB. 152021. 2021</p>
<p>Rpt: Relational pre-trained transformer is almost all you need towards democratizing data preparation. Nan Tang, Ju Fan, Fangyi Li, Jianhong Tu, Xiaoyong Du, Guoliang Li, Sam Madden, Mourad Ouzzani, 10.14778/3457390.3457391arXiv:2012.02469PVLDB. 142021. 2021</p>
<p>LaMDA: Language Models for Dialog Applications. Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze, Alicia Cheng, Taylor Jin, Leslie Bos, Yu Baker, Yaguang Du, Hongrae Li, Huaixiu Lee, Amin Steven Zheng, Marcelo Ghafouri, Yanping Menegali, Maxim Huang, Dmitry Krikun, James Lepikhin, Dehao Qin, Yuanzhong Chen, Zhifeng Xu, Adam Chen, Maarten Roberts, Vincent Bosma, Yanqi Zhao, Chung-Ching Zhou, Igor Chang, Will Krivokon, Marc Rusch, Pranesh Pickett, Laichee Srinivasan, Kathleen Man, Meier-Hellstern, arXiv:2201.08239Olson, Alejandra Molina, Erin Hoffman-John, Josh Lee, Lora Aroyo, Ravi Rajakumar, Alena Butryna, Matthew Lamm, Viktoriya Kuzmina, Joe Fenton, Aaron Cohen, Rachel Bernstein, Ray Kurzweil, Blaise Aguera-Arcas, Claire Cui, Marian Croak2022. 2022Meredith Ringel Morris, Tulsee Doshi, Renelito Delos Santos, Toju Duke, Johnny Soraker, Ben Zevenbergen, Vinodkumar Prabhakaran, Mark Diaz, Ben Hutchinson</p>
<p>From natural language processing to neural databases. James Thorne, Majid Yazdani, Marzieh Saeidi, Fabrizio Silvestri, Sebastian Riedel, Alon Halevy, 10.14778/3447689.3447706Proceedings of the VLDB Endowment. the VLDB Endowment2021. 202114</p>
<p>Can deep neural networks predict data correlations from column names. Immanuel Trummer, 2021</p>
<p>Database tuning using natural language processing. Immanuel Trummer, SIGMOD Record. 502021. 2021</p>
<p>The case for nlp-enhanced database tuning: Towards tuning tools that "read the manual. Immanuel Trummer, 10.14778/3450980.3450984PVLDB. 142021. 2021</p>
<p>Verifying text summaries of relational data sets. Immanuel Trummer, 2021</p>
<p>WebChecker: Towards an Infrastructure for Efficient Misinformation Detection at Web Scale. Immanuel Trummer, IEEE Data Eng. Bull. 442021. 2021</p>
<p>BABOONS: Black-box optimization of data summaries in natural language. Immanuel Trummer, 10.14778/3551793.3551846PVLDB. 152022. 2022</p>
<p>CodexDB: Synthesizing code for query processing from natural language instructions using GPT-3 Codex. Immanuel Trummer, 10.14778/3551793.3551841PVLDB. 152022. 2022</p>
<p>DB-BERT: a database tuning tool that "reads the manual. Immanuel Trummer, 10.1145/3514221.35178432022In SIGMOD. 190-203</p>
<p>Demonstrating DB-BERT: A Database Tuning Tool that "Reads" the Manual. Immanuel Trummer, 10.1145/3514221.3520171arXiv:2112.10925SIGMOD. Association for Computing Machinery2022</p>
<p>Towards NLP-Enhanced Data Profiling Tools. Immanuel Trummer, CIDR. 1-1. 2022</p>
<p>Demonstrating NaturalMiner: Searching Large Data Sets for Abstract Patterns Described in Natural Language. Immanuel Trummer, SIGMOD. 2023</p>
<p>Attention is all you need. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, Illia Polosukhin, arXiv:1706.03762Advances in Neural Information Processing Systems. 2017</p>
<p>91] Prateek Verma and Chris Chafe. 2021. A Generative Model for Raw Audio Using Transformer Architectures. Prateek Verma, Jonathan Berger, 10.23919/DAFx51585.2021.9768298arXiv:2105.00335arXiv:2106.16036Proceedings of the 24th International Conference on Digital Audio Effects, DAFx 2021. the authors. the 24th International Conference on Digital Audio Effects, DAFx 2021. the authors2021. CoRR abs/2105.0 (2021Adieu Convolutions</p>
<p>M2TR: Multi-modal Multi-scale Transformers for Deepfake Detection. Junke Wang, Zuxuan Wu, Wenhao Ouyang, Xintong Han, Jingjing Chen, Ser , Nam Lim, Yu Gang Jiang, 10.1145/3512527.3531415arXiv:2104.09770ICMR 2022 -Proceedings of the 2022 International Conference on Multimedia Retrieval. 2022</p>
<p>Deep Learning: Systems and Responsibility. Abdul Wasay, Subarna Chatterjee, Stratos Idreos, 10.1145/3448016.3457541SIGMOD. 2867-2875. 2021</p>
<p>Robust voice querying with muve: Optimally visualizing results of phonetically similar queries. Ziyun Wei, Immanuel Trummer, Connor Anderson, 10.14778/3476249.3476289PVLDB. 142021. 2021</p>
<p>Demonstrating Robust Voice Querying with MUVE: Optimally Visualizing Results of Phonetically Similar Queries. Ziyun Wei, Immanuel Trummer, Anderson Connor, SIGMOD. 2798-28022021</p>
<p>DBPal: Weak Supervision for Learning a Natural Language Interface to Databases. Nathaniel Weir, Andrew Crotty, Alex Galakatos, Amir Ilkhechi, Shekar Ramaswamy, Rohin Bhushan, Ugur Cetintemel, Prasetya Utama, Nadja Geisler, Benjamin Hättasch, Steffen Eger, Carsten Binnig, arXiv:1909.061822019. 2019</p>
<p>Transformers: State-of-the-Art Natural Language Processing. Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Clara Patrick Von Platen, Yacine Ma, Julien Jernite, Canwen Plu, Teven Xu, Sylvain Le Scao, Mariama Gugger, Quentin Drame, Alexander Lhoest, Rush, 10.18653/v1/2020.emnlp-demos.6arXiv:1910.03771v5EMNLP. 38-45. 2020</p>
<p>SeaD: End-to-end Text-to-SQL Generation with Schema-aware Denoising. Yongbo Kuan Xuan, Yongliang Wang, Zujie Wang, Yang Wen, Dong, arXiv:2105.07911NAACL. 1845-1853. 2022</p>
<p>SyntaxSqlnet: Syntax tree networks for complex and cross-domain text-to-SQL task. Tao Yu, Michihiro Yasunaga, Kai Yang, Rui Zhang, Dongxu Wang, Zifan Li, Dragomir R Radev, 10.18653/v1/d18-1193arXiv:1810.05237Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. the 2018 Conference on Empirical Methods in Natural Language Processing2020. 2018</p>
<p>Spider: A large-scale human-labeled dataset for complex and cross-domain semantic parsing and text-to-SQL task. Tao Yu, Rui Zhang, Kai Yang, Michihiro Yasunaga, Dongxu Wang, Zifan Li, James Ma, Irene Li, Qingning Yao, Shanelle Roman, Zilin Zhang, Dragomir R Radev, 10.18653/v1/d18-1425arXiv:1809.08887Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. the 2018 Conference on Empirical Methods in Natural Language Processing2020. 2018</p>
<p>Scaling Vision Transformers. Xiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, Lucas Beyer, 10.1109/CVPR52688.2022.01179arXiv:2106.04560Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition. the IEEE Computer Society Conference on Computer Vision and Pattern Recognition2022. 2022-June (2022</p>
<p>Multi-Scale Vision Longformer: A New Vision Transformer for High-Resolution Image Encoding. Pengchuan Zhang, Xiyang Dai, Jianwei Yang, Bin Xiao, Lu Yuan, Lei Zhang, Jianfeng Gao, 10.1109/ICCV48922.2021.00299arXiv:2103.15358Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer Vision2021. 2021</p>
<p>OPT: Open Pre-trained Transformer Language Models. Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona T Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, Luke Zettlemoyer, 10.48550/arXiv.2205.01068CoRR abs/2205.02022. 2022</p>
<p>Cat-Det: Contrastively Augmented Transformer for Multimodal 3D Object Detection. Yanan Zhang, Jiaxin Chen, Di Huang, 10.1109/CVPR52688.2022.00098arXiv:2204.00325Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition. the IEEE Computer Society Conference on Computer Vision and Pattern Recognition2022</p>
<p>Seq2SQL: Generating Structured Queries from Natural Language using Reinforcement Learning. Victor Zhong, Caiming Xiong, Richard Socher, arXiv:1709.001032017. CoRR abs/1709.0, 1 (2017</p>
<p>DeepViT: Towards Deeper Vision Transformer. Daquan Zhou, Bingyi Kang, Xiaojie Jin, Linjie Yang, Xiaochen Lian, Zihang Jiang, Qibin Hou, Jiashi Feng, arXiv:2103.118862021. CoRR abs/2103.1 (2021</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1042 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1042</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1042</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-24.html">extraction-schema-24</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <p><strong>Paper ID:</strong> paper-268524153</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2404.10179v2.pdf" target="_blank">Scaling Instructable Agents Across Many Simulated Worlds</a></p>
                <p><strong>Paper Abstract:</strong> in any 3D environment is a key challenge for creating general AI. Accomplishing this goal requires learning to ground language in perception and embodied actions, in order to accomplish complex tasks. The Scalable, Instructable, Multiworld Agent (SIMA) project tackles this by training agents to follow free-form instructions across a diverse range of virtual 3D environments, including curated research environments as well as open-ended, commercial video games. Our goal is to develop an instructable agent that can accomplish anything a human can do in any simulated 3D environment. Our approach focuses on language-driven generality while imposing minimal assumptions. Our agents interact with environments in real-time using a generic, human-like interface: the inputs are image observations and language instructions and the outputs are keyboard-and-mouse actions. This general approach is challenging, but it allows agents to ground language across many visually complex and semantically rich environments while also allowing us to readily run agents in new environments. In this paper we describe our motivation and goal, the initial progress we have made, and promising preliminary results on several diverse research environments and a variety of commercial video games.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1042.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1042.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SIMA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Scalable, Instructable, Multiworld Agent (SIMA)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multi-environment, language-conditional embodied agent trained by behavioral cloning on human gameplay data to map pixel observations and natural language instructions to keyboard-and-mouse actions across many simulated 3D environments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>SIMA agent</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A transformer-based policy that cross-attends to pretrained vision/video encoders (SPARC, Phenaki) and encoded language instructions, trained primarily with behavioral cloning (supervised imitation) on human gameplay data, with an auxiliary goal-completion objective and inference-time classifier-free guidance (CFG) to improve language conditionality.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated agent / virtual agent</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Portfolio of >10 3D environments (research environments and commercial video games; evaluated on a subset of 7)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>A broad mix of environments ranging from controlled research labs (Playhouse, WorldLab, Construction Lab, ProcTHOR) to visually-rich, open-ended commercial games (No Man's Sky, Valheim, Satisfactory, Teardown, Hydroneer, Goat Simulator 3, Wobbly Life). Complexity arises from (a) high visual fidelity and semantic richness (procedurally-generated planets, many object types), (b) hundreds of objects and many possible interactions per scene in some games, (c) asynchronous real-time dynamics, and (d) varied mechanics (navigation, crafting, destructible worlds, resource management, menu-driven actions). Variation arises from heterogeneous game mechanics, visual styles, procedurally-generated content, and multiple environment instances.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Characterized qualitatively and by environment attributes reported in the paper: 'visual complexity' and 'semantically rich' scenes; explicit mentions include 'hundreds of objects in a scene' and diverse mechanics (navigation, tool use, crafting, menus, destructible physics); tasks targeted are short-horizon (~<=10 s) for training. No single scalar metric provided, but complexity is operationalized by environment type (research vs commercial), number of object types, and richness of interactions.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>mixed — research environments: low-to-medium; commercial video games: high (paper explicitly reports research envs are simpler and commercial games more complex)</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Number of distinct environments (>10), procedural generation (e.g., No Man's Sky procedurally-generated planets), visual diversity across games, diversity of task verbs/clusters (1,485 unique evaluation tasks across 7 environments spanning 9 skill categories), and heterogeneity of mechanics (menus, asynchronous dynamics).</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>high (paper emphasizes broad distribution of diverse environments and high visual/mechanical variation)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Success rate (task completion as judged by ground-truth in research envs, OCR or human evaluation in games); also normalized relative performance (relative to environment-specialist baseline) and action log-probabilities for held-out data (not predictive).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Varies by environment; reported examples: SIMA achieved 34% success on a No Man's Sky task subset (human experts: 60% on same subset; No Language baseline: 11%). Aggregate claim: SIMA outperforms environment-specialized agents with a 67% average improvement (normalized), but per-environment absolute success rates are not all numerically listed in text.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Yes — the paper explicitly discusses relationships and trade-offs: (1) Performance is higher in comparatively simpler research environments (Playhouse, WorldLab) and lower in complex commercial games and the challenging Construction Lab; (2) Training on a diverse distribution of environments (high variation) produces positive transfer: multi-environment SIMA outperforms environment-specialists (67% average improvement), indicating that higher variation (multiple environments) can improve generalization even when environments are complex; (3) trade-offs: adopting a human-like, general interface and including highly complex commercial games makes learning harder (lower absolute performance) but yields stronger transfer and broader applicability.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Multi-environment behavioral cloning (imitation learning) from human gameplay; mixing and weighting data across environments; pretrained vision/video encoders fine-tuned where appropriate; auxiliary goal prediction objective; inference-time classifier-free guidance (CFG).</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>SIMA shows positive transfer across environments: when trained across multiple environments it outperforms environment-specialized agents (statistically significant; average 67% relative improvement). Zero-shot (held-out environment) agents show non-trivial performance on generic navigation and color-based object selection skills and sometimes match environment-specialists on some domains (not uniformly). Zero-shot performance is especially good on domain-general tasks (e.g., navigation) but poorer on environment-specific skills.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Training reported to 1.2 million training steps for evaluations; three random seeds for main SIMA runs. Exact number of environment interactions or human trajectories not reported in the text.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>1) Training an agent across a diverse set of visually and mechanically rich environments using behavioral cloning and internet-scale pretrained components yields positive transfer: SIMA substantially outperforms environment-specialized baselines on average. 2) Performance is higher in simpler research environments (Playhouse, WorldLab) and lower in complex commercial games and Construction Lab, indicating environment complexity reduces absolute success rates. 3) High environment variation (many diverse environments) improves generalization and zero-shot capabilities for domain-general skills. 4) Language inputs are essential: a No Language ablation performs poorly (11% on a No Man's Sky subset). 5) Inference-time CFG substantially improves language conditionality. 6) Human experts still outperform the agent on tested tasks (60% vs 34% in the reported No Man's Sky subset), highlighting remaining gaps.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Scaling Instructable Agents Across Many Simulated Worlds', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1042.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1042.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Zero-shot SIMA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Zero-shot held-out-environment SIMA agent</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A variant of SIMA trained on all but one environment and evaluated zero-shot on the held-out environment to measure transfer/generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Zero-shot SIMA agent</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Same model architecture and training procedure as SIMA but with one environment withheld from behavioral cloning training; evaluated without any BC training on the held-out environment to assess transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated agent / virtual agent</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Held-out environment (varied per experiment; examples include Goat Simulator 3, other games and research environments)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Same diverse set as SIMA; held-out tests included commercial games and research environments. The held-out environments differ in visual fidelity, mechanics, and affordances from training environments.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>As per SIMA: visual complexity, mechanics, number of interactions; specific held-out examples include Goat Simulator 3 which was not included in visual finetuning for encoders.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>varies by held-out environment; Goat Simulator 3 explicitly noted as high visual/mechanical mismatch relative to trained encoders.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Measured by the degree to which held-out environment shares domain-general skills with training environments; zero-shot tests focus on transfer of shared affordances like navigation and color-based object interactions.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>high (held-out environments often differ substantially from training set)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Success rate on evaluation tasks; normalized performance relative to environment-specialist baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Zero-shot agents achieve non-trivial success on domain-general skills (navigation, color-based grabbing); in some cases (notably Goat Simulator 3) zero-shot performance was comparable to the environment-specialized agent despite lack of visual finetuning. No single aggregated success rate number provided for zero-shot across all held-out envs.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Paper reports that zero-shot transfer is stronger for domain-general tasks that recur across environments (navigation, grabbing by color) and weaker for environment-specific mechanics; thus variation in training environments helps generalize to new environments, but high environment-specific complexity reduces zero-shot efficacy.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Multi-environment training excluding the held-out environment; behavioral cloning with pretrained encoders; zero-shot evaluation (no BC on held-out env).</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>Zero-shot agents perform well on general navigation and perceptual tasks shared across environments and sometimes match environment-specialists on such tasks; they perform poorly on environment-specific skills and mechanics. Goat Simulator 3 zero-shot result demonstrates transfer even without encoder finetuning in that case.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Evaluated after 1.2M training steps (same schedule as other agents); no further sample-efficiency numbers provided.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Zero-shot training demonstrates that multi-environment training yields transferable policies for domain-general skills, but environment-specific mechanics and high complexity limit zero-shot performance; training variation (diverse environments) is beneficial for transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Scaling Instructable Agents Across Many Simulated Worlds', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1042.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1042.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>No-Lang ablation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>No Language ablation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An ablated agent trained and evaluated without language inputs to quantify the contribution of language conditioning to behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>No-Language ablation</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Same architecture as SIMA but with language input removed during training and evaluation to measure how much behavior arises from language vs. behavioral priors.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated agent / virtual agent</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Subset of environments used for evaluation (including No Man's Sky subset and research environments where ablation tested)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Same diverse set as SIMA; used to test whether tasks can be solved without language conditioning (i.e., by environment affordances and default policies).</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>As per SIMA environments; no separate metric introduced.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>mixed</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Same multi-environment distribution; ablation isolates absence of language conditioning rather than environment variation per se.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>high (evaluated across varied tasks), but ablation focuses on language removal</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Success rate (task completion), reported for comparisons</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Example: On the No Man's Sky subset, No-Language baseline achieved 11% success (compared to SIMA 34% and human 60%). Overall, No-Language ablation performs very poorly (permutation tests < 0.001 compared to SIMA).</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Paper uses No-Language ablation to show that environment affordances alone are insufficient for task success across varied and complex environments; removing language drastically reduces performance especially on language-specified goals, highlighting the role of language in handling variation and disambiguating tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Behavioral cloning without language conditioning (trained on the same human datasets but with language removed).</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>No-Language ablation generalizes poorly to language-specified tasks; it is significantly outperformed by SIMA and often fails even on tasks humans find nontrivial.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Same training regime (1.2M steps for evaluation); no separate efficiency numbers.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Language conditioning is critical for solving instruction-specified tasks in diverse and complex environments; removing language input reduces success rates dramatically (e.g., 11% in reported No Man's Sky subset). This supports the claim that language helps create abstractions that aid generalization across varied environments.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Scaling Instructable Agents Across Many Simulated Worlds', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1042.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1042.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>No-pretrain ablation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>No Pretraining ablation (no pretrained encoders)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An ablation that replaces internet-scale pretrained vision/video encoders with a ResNet trained from scratch to assess benefits from pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>No-pretraining ablation</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>SIMA architecture but with SPARC/Phenaki (pretrained image-text and video models) removed and replaced by a ResNet trained from scratch; trained by behavioral cloning on the same human gameplay data.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated agent / virtual agent</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Same portfolio of environments used by SIMA (evaluations reported across environments)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Same as SIMA: varied research and commercial environments with differing visual complexity and mechanics.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Same qualitative measures (visual complexity, object count, mechanics).</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>mixed</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Varied; ablation isolates the effect of lacking pretrained representations when learning across diverse environments.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>high</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Normalized performance relative to environment-specialist; permutation tests for significance</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Overall SIMA substantially outperforms the no-pretraining baseline (permutation test p < 0.001). Environment-level p-values vary (examples: 0.0002, 0.14, 0.041, 0.0002, 0.244, 0.052, 0.032 across environments), indicating benefits of internet-scale pretraining vary by environment.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>The benefit of pretraining varies with environment complexity and variation: in some environments (especially visually diverse or complex ones) pretraining yields significant improvements, while in others the effect is smaller or not significant, implying pretrained representations help cope with visual diversity/complexity but gains depend on domain match.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Behavioral cloning with learned-from-scratch vision encoder replacing pretrained models; otherwise same training pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>No-pretraining ablation underperforms SIMA overall, indicating that pretrained components improve grounded learning and transfer across varied and complex environments; magnitude of improvement depends on environment.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Same training steps used for evaluation (1.2M steps); no separate sample-efficiency report.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Internet-scale pretrained vision/video encoders materially improve agent performance and transfer across diverse environments, particularly in visually complex or semantically rich games, though the benefit magnitude varies by environment and domain shift.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Scaling Instructable Agents Across Many Simulated Worlds', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1042.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1042.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Env-specialist</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Environment-specialized agent (expert per-environment)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An agent trained only on data from a single environment (environment specialist) used as a normalization baseline for per-environment performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Environment-specialized agent</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Agent trained only on data corresponding to one specific environment (still including pretrained encoders), used as an upper-bound baseline to assess how much multi-environment training can approach environment-specific expertise.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated agent / virtual agent</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Individual environments (one agent per environment for evaluation baselines)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Single environment specialization allows focusing on environment-specific mechanics and affordances (e.g., unique menus or actions in a particular commercial game).</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Complexity determined by the specific environment; environment-specialist aims to exploit repeated exposure to the environment to master its mechanics.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>varies per environment (used as normalization point: 100% per environment in plots)</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Low (training limited to single environment) in contrast to SIMA's high training variation across environments.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>low (single-environment training)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Absolute success rate per environment (used as 100% normalization in comparisons); environment-specialist performance used to normalize others.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>By construction environment-specialist is set as 100% relative baseline in normalization plots; absolute success rates per environment are not all listed in text. Note: for Goat Simulator 3 the environment-specialist baseline was selected at its best checkpoint (400k steps) to avoid overfitting due to limited data.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Environment-specialists can outperform multi-environment agents on highly environment-specific mechanics but may perform worse on domain-general skills that are shared across environments; multi-environment SIMA outperformed environment-specialists on average (67% improvement), indicating variation in training can compensate for environment-specific specialization in many cases.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Single-environment behavioral cloning using only data from the target environment; included pretrained encoders.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>Environment-specialists provide an upper-bound for per-environment learning but tend to be less robust to distribution shifts and often underperform multi-environment SIMA on average across environments.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Varies by environment; Goat Simulator 3 specialist selected at 400k steps as the best checkpoint (selection bias noted by authors).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Although environment-specialists can learn environment-specific mechanics, training on a diverse set of environments (SIMA) produced better average performance across environments (67% relative improvement), indicating that training variation improves robustness and transferability of language-conditioned embodied behaviors.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Scaling Instructable Agents Across Many Simulated Worlds', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Creating Multimodal Interactive Agents with Imitation and Self-Supervised Learning <em>(Rating: 2)</em></li>
                <li>Grounded Language Learning in a Simulated 3D World <em>(Rating: 2)</em></li>
                <li>Building Open-Ended Embodied Agents with Internet-Scale Knowledge <em>(Rating: 2)</em></li>
                <li>RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control <em>(Rating: 2)</em></li>
                <li>Voyager: An Open-Ended Embodied Agent with Large Language Models <em>(Rating: 2)</em></li>
                <li>STEVE-1: A Generative Model for Text-to-Behavior in Minecraft <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1042",
    "paper_id": "paper-268524153",
    "extraction_schema_id": "extraction-schema-24",
    "extracted_data": [
        {
            "name_short": "SIMA",
            "name_full": "Scalable, Instructable, Multiworld Agent (SIMA)",
            "brief_description": "A multi-environment, language-conditional embodied agent trained by behavioral cloning on human gameplay data to map pixel observations and natural language instructions to keyboard-and-mouse actions across many simulated 3D environments.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "SIMA agent",
            "agent_description": "A transformer-based policy that cross-attends to pretrained vision/video encoders (SPARC, Phenaki) and encoded language instructions, trained primarily with behavioral cloning (supervised imitation) on human gameplay data, with an auxiliary goal-completion objective and inference-time classifier-free guidance (CFG) to improve language conditionality.",
            "agent_type": "simulated agent / virtual agent",
            "environment_name": "Portfolio of &gt;10 3D environments (research environments and commercial video games; evaluated on a subset of 7)",
            "environment_description": "A broad mix of environments ranging from controlled research labs (Playhouse, WorldLab, Construction Lab, ProcTHOR) to visually-rich, open-ended commercial games (No Man's Sky, Valheim, Satisfactory, Teardown, Hydroneer, Goat Simulator 3, Wobbly Life). Complexity arises from (a) high visual fidelity and semantic richness (procedurally-generated planets, many object types), (b) hundreds of objects and many possible interactions per scene in some games, (c) asynchronous real-time dynamics, and (d) varied mechanics (navigation, crafting, destructible worlds, resource management, menu-driven actions). Variation arises from heterogeneous game mechanics, visual styles, procedurally-generated content, and multiple environment instances.",
            "complexity_measure": "Characterized qualitatively and by environment attributes reported in the paper: 'visual complexity' and 'semantically rich' scenes; explicit mentions include 'hundreds of objects in a scene' and diverse mechanics (navigation, tool use, crafting, menus, destructible physics); tasks targeted are short-horizon (~&lt;=10 s) for training. No single scalar metric provided, but complexity is operationalized by environment type (research vs commercial), number of object types, and richness of interactions.",
            "complexity_level": "mixed — research environments: low-to-medium; commercial video games: high (paper explicitly reports research envs are simpler and commercial games more complex)",
            "variation_measure": "Number of distinct environments (&gt;10), procedural generation (e.g., No Man's Sky procedurally-generated planets), visual diversity across games, diversity of task verbs/clusters (1,485 unique evaluation tasks across 7 environments spanning 9 skill categories), and heterogeneity of mechanics (menus, asynchronous dynamics).",
            "variation_level": "high (paper emphasizes broad distribution of diverse environments and high visual/mechanical variation)",
            "performance_metric": "Success rate (task completion as judged by ground-truth in research envs, OCR or human evaluation in games); also normalized relative performance (relative to environment-specialist baseline) and action log-probabilities for held-out data (not predictive).",
            "performance_value": "Varies by environment; reported examples: SIMA achieved 34% success on a No Man's Sky task subset (human experts: 60% on same subset; No Language baseline: 11%). Aggregate claim: SIMA outperforms environment-specialized agents with a 67% average improvement (normalized), but per-environment absolute success rates are not all numerically listed in text.",
            "complexity_variation_relationship": "Yes — the paper explicitly discusses relationships and trade-offs: (1) Performance is higher in comparatively simpler research environments (Playhouse, WorldLab) and lower in complex commercial games and the challenging Construction Lab; (2) Training on a diverse distribution of environments (high variation) produces positive transfer: multi-environment SIMA outperforms environment-specialists (67% average improvement), indicating that higher variation (multiple environments) can improve generalization even when environments are complex; (3) trade-offs: adopting a human-like, general interface and including highly complex commercial games makes learning harder (lower absolute performance) but yields stronger transfer and broader applicability.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Multi-environment behavioral cloning (imitation learning) from human gameplay; mixing and weighting data across environments; pretrained vision/video encoders fine-tuned where appropriate; auxiliary goal prediction objective; inference-time classifier-free guidance (CFG).",
            "generalization_tested": true,
            "generalization_results": "SIMA shows positive transfer across environments: when trained across multiple environments it outperforms environment-specialized agents (statistically significant; average 67% relative improvement). Zero-shot (held-out environment) agents show non-trivial performance on generic navigation and color-based object selection skills and sometimes match environment-specialists on some domains (not uniformly). Zero-shot performance is especially good on domain-general tasks (e.g., navigation) but poorer on environment-specific skills.",
            "sample_efficiency": "Training reported to 1.2 million training steps for evaluations; three random seeds for main SIMA runs. Exact number of environment interactions or human trajectories not reported in the text.",
            "key_findings": "1) Training an agent across a diverse set of visually and mechanically rich environments using behavioral cloning and internet-scale pretrained components yields positive transfer: SIMA substantially outperforms environment-specialized baselines on average. 2) Performance is higher in simpler research environments (Playhouse, WorldLab) and lower in complex commercial games and Construction Lab, indicating environment complexity reduces absolute success rates. 3) High environment variation (many diverse environments) improves generalization and zero-shot capabilities for domain-general skills. 4) Language inputs are essential: a No Language ablation performs poorly (11% on a No Man's Sky subset). 5) Inference-time CFG substantially improves language conditionality. 6) Human experts still outperform the agent on tested tasks (60% vs 34% in the reported No Man's Sky subset), highlighting remaining gaps.",
            "uuid": "e1042.0",
            "source_info": {
                "paper_title": "Scaling Instructable Agents Across Many Simulated Worlds",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "Zero-shot SIMA",
            "name_full": "Zero-shot held-out-environment SIMA agent",
            "brief_description": "A variant of SIMA trained on all but one environment and evaluated zero-shot on the held-out environment to measure transfer/generalization.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Zero-shot SIMA agent",
            "agent_description": "Same model architecture and training procedure as SIMA but with one environment withheld from behavioral cloning training; evaluated without any BC training on the held-out environment to assess transfer.",
            "agent_type": "simulated agent / virtual agent",
            "environment_name": "Held-out environment (varied per experiment; examples include Goat Simulator 3, other games and research environments)",
            "environment_description": "Same diverse set as SIMA; held-out tests included commercial games and research environments. The held-out environments differ in visual fidelity, mechanics, and affordances from training environments.",
            "complexity_measure": "As per SIMA: visual complexity, mechanics, number of interactions; specific held-out examples include Goat Simulator 3 which was not included in visual finetuning for encoders.",
            "complexity_level": "varies by held-out environment; Goat Simulator 3 explicitly noted as high visual/mechanical mismatch relative to trained encoders.",
            "variation_measure": "Measured by the degree to which held-out environment shares domain-general skills with training environments; zero-shot tests focus on transfer of shared affordances like navigation and color-based object interactions.",
            "variation_level": "high (held-out environments often differ substantially from training set)",
            "performance_metric": "Success rate on evaluation tasks; normalized performance relative to environment-specialist baseline.",
            "performance_value": "Zero-shot agents achieve non-trivial success on domain-general skills (navigation, color-based grabbing); in some cases (notably Goat Simulator 3) zero-shot performance was comparable to the environment-specialized agent despite lack of visual finetuning. No single aggregated success rate number provided for zero-shot across all held-out envs.",
            "complexity_variation_relationship": "Paper reports that zero-shot transfer is stronger for domain-general tasks that recur across environments (navigation, grabbing by color) and weaker for environment-specific mechanics; thus variation in training environments helps generalize to new environments, but high environment-specific complexity reduces zero-shot efficacy.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Multi-environment training excluding the held-out environment; behavioral cloning with pretrained encoders; zero-shot evaluation (no BC on held-out env).",
            "generalization_tested": true,
            "generalization_results": "Zero-shot agents perform well on general navigation and perceptual tasks shared across environments and sometimes match environment-specialists on such tasks; they perform poorly on environment-specific skills and mechanics. Goat Simulator 3 zero-shot result demonstrates transfer even without encoder finetuning in that case.",
            "sample_efficiency": "Evaluated after 1.2M training steps (same schedule as other agents); no further sample-efficiency numbers provided.",
            "key_findings": "Zero-shot training demonstrates that multi-environment training yields transferable policies for domain-general skills, but environment-specific mechanics and high complexity limit zero-shot performance; training variation (diverse environments) is beneficial for transfer.",
            "uuid": "e1042.1",
            "source_info": {
                "paper_title": "Scaling Instructable Agents Across Many Simulated Worlds",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "No-Lang ablation",
            "name_full": "No Language ablation",
            "brief_description": "An ablated agent trained and evaluated without language inputs to quantify the contribution of language conditioning to behavior.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "No-Language ablation",
            "agent_description": "Same architecture as SIMA but with language input removed during training and evaluation to measure how much behavior arises from language vs. behavioral priors.",
            "agent_type": "simulated agent / virtual agent",
            "environment_name": "Subset of environments used for evaluation (including No Man's Sky subset and research environments where ablation tested)",
            "environment_description": "Same diverse set as SIMA; used to test whether tasks can be solved without language conditioning (i.e., by environment affordances and default policies).",
            "complexity_measure": "As per SIMA environments; no separate metric introduced.",
            "complexity_level": "mixed",
            "variation_measure": "Same multi-environment distribution; ablation isolates absence of language conditioning rather than environment variation per se.",
            "variation_level": "high (evaluated across varied tasks), but ablation focuses on language removal",
            "performance_metric": "Success rate (task completion), reported for comparisons",
            "performance_value": "Example: On the No Man's Sky subset, No-Language baseline achieved 11% success (compared to SIMA 34% and human 60%). Overall, No-Language ablation performs very poorly (permutation tests &lt; 0.001 compared to SIMA).",
            "complexity_variation_relationship": "Paper uses No-Language ablation to show that environment affordances alone are insufficient for task success across varied and complex environments; removing language drastically reduces performance especially on language-specified goals, highlighting the role of language in handling variation and disambiguating tasks.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Behavioral cloning without language conditioning (trained on the same human datasets but with language removed).",
            "generalization_tested": true,
            "generalization_results": "No-Language ablation generalizes poorly to language-specified tasks; it is significantly outperformed by SIMA and often fails even on tasks humans find nontrivial.",
            "sample_efficiency": "Same training regime (1.2M steps for evaluation); no separate efficiency numbers.",
            "key_findings": "Language conditioning is critical for solving instruction-specified tasks in diverse and complex environments; removing language input reduces success rates dramatically (e.g., 11% in reported No Man's Sky subset). This supports the claim that language helps create abstractions that aid generalization across varied environments.",
            "uuid": "e1042.2",
            "source_info": {
                "paper_title": "Scaling Instructable Agents Across Many Simulated Worlds",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "No-pretrain ablation",
            "name_full": "No Pretraining ablation (no pretrained encoders)",
            "brief_description": "An ablation that replaces internet-scale pretrained vision/video encoders with a ResNet trained from scratch to assess benefits from pretraining.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "No-pretraining ablation",
            "agent_description": "SIMA architecture but with SPARC/Phenaki (pretrained image-text and video models) removed and replaced by a ResNet trained from scratch; trained by behavioral cloning on the same human gameplay data.",
            "agent_type": "simulated agent / virtual agent",
            "environment_name": "Same portfolio of environments used by SIMA (evaluations reported across environments)",
            "environment_description": "Same as SIMA: varied research and commercial environments with differing visual complexity and mechanics.",
            "complexity_measure": "Same qualitative measures (visual complexity, object count, mechanics).",
            "complexity_level": "mixed",
            "variation_measure": "Varied; ablation isolates the effect of lacking pretrained representations when learning across diverse environments.",
            "variation_level": "high",
            "performance_metric": "Normalized performance relative to environment-specialist; permutation tests for significance",
            "performance_value": "Overall SIMA substantially outperforms the no-pretraining baseline (permutation test p &lt; 0.001). Environment-level p-values vary (examples: 0.0002, 0.14, 0.041, 0.0002, 0.244, 0.052, 0.032 across environments), indicating benefits of internet-scale pretraining vary by environment.",
            "complexity_variation_relationship": "The benefit of pretraining varies with environment complexity and variation: in some environments (especially visually diverse or complex ones) pretraining yields significant improvements, while in others the effect is smaller or not significant, implying pretrained representations help cope with visual diversity/complexity but gains depend on domain match.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Behavioral cloning with learned-from-scratch vision encoder replacing pretrained models; otherwise same training pipeline.",
            "generalization_tested": true,
            "generalization_results": "No-pretraining ablation underperforms SIMA overall, indicating that pretrained components improve grounded learning and transfer across varied and complex environments; magnitude of improvement depends on environment.",
            "sample_efficiency": "Same training steps used for evaluation (1.2M steps); no separate sample-efficiency report.",
            "key_findings": "Internet-scale pretrained vision/video encoders materially improve agent performance and transfer across diverse environments, particularly in visually complex or semantically rich games, though the benefit magnitude varies by environment and domain shift.",
            "uuid": "e1042.3",
            "source_info": {
                "paper_title": "Scaling Instructable Agents Across Many Simulated Worlds",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "Env-specialist",
            "name_full": "Environment-specialized agent (expert per-environment)",
            "brief_description": "An agent trained only on data from a single environment (environment specialist) used as a normalization baseline for per-environment performance.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Environment-specialized agent",
            "agent_description": "Agent trained only on data corresponding to one specific environment (still including pretrained encoders), used as an upper-bound baseline to assess how much multi-environment training can approach environment-specific expertise.",
            "agent_type": "simulated agent / virtual agent",
            "environment_name": "Individual environments (one agent per environment for evaluation baselines)",
            "environment_description": "Single environment specialization allows focusing on environment-specific mechanics and affordances (e.g., unique menus or actions in a particular commercial game).",
            "complexity_measure": "Complexity determined by the specific environment; environment-specialist aims to exploit repeated exposure to the environment to master its mechanics.",
            "complexity_level": "varies per environment (used as normalization point: 100% per environment in plots)",
            "variation_measure": "Low (training limited to single environment) in contrast to SIMA's high training variation across environments.",
            "variation_level": "low (single-environment training)",
            "performance_metric": "Absolute success rate per environment (used as 100% normalization in comparisons); environment-specialist performance used to normalize others.",
            "performance_value": "By construction environment-specialist is set as 100% relative baseline in normalization plots; absolute success rates per environment are not all listed in text. Note: for Goat Simulator 3 the environment-specialist baseline was selected at its best checkpoint (400k steps) to avoid overfitting due to limited data.",
            "complexity_variation_relationship": "Environment-specialists can outperform multi-environment agents on highly environment-specific mechanics but may perform worse on domain-general skills that are shared across environments; multi-environment SIMA outperformed environment-specialists on average (67% improvement), indicating variation in training can compensate for environment-specific specialization in many cases.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Single-environment behavioral cloning using only data from the target environment; included pretrained encoders.",
            "generalization_tested": false,
            "generalization_results": "Environment-specialists provide an upper-bound for per-environment learning but tend to be less robust to distribution shifts and often underperform multi-environment SIMA on average across environments.",
            "sample_efficiency": "Varies by environment; Goat Simulator 3 specialist selected at 400k steps as the best checkpoint (selection bias noted by authors).",
            "key_findings": "Although environment-specialists can learn environment-specific mechanics, training on a diverse set of environments (SIMA) produced better average performance across environments (67% relative improvement), indicating that training variation improves robustness and transferability of language-conditioned embodied behaviors.",
            "uuid": "e1042.4",
            "source_info": {
                "paper_title": "Scaling Instructable Agents Across Many Simulated Worlds",
                "publication_date_yy_mm": "2024-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Creating Multimodal Interactive Agents with Imitation and Self-Supervised Learning",
            "rating": 2,
            "sanitized_title": "creating_multimodal_interactive_agents_with_imitation_and_selfsupervised_learning"
        },
        {
            "paper_title": "Grounded Language Learning in a Simulated 3D World",
            "rating": 2,
            "sanitized_title": "grounded_language_learning_in_a_simulated_3d_world"
        },
        {
            "paper_title": "Building Open-Ended Embodied Agents with Internet-Scale Knowledge",
            "rating": 2,
            "sanitized_title": "building_openended_embodied_agents_with_internetscale_knowledge"
        },
        {
            "paper_title": "RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control",
            "rating": 2,
            "sanitized_title": "rt2_visionlanguageaction_models_transfer_web_knowledge_to_robotic_control"
        },
        {
            "paper_title": "Voyager: An Open-Ended Embodied Agent with Large Language Models",
            "rating": 2,
            "sanitized_title": "voyager_an_openended_embodied_agent_with_large_language_models"
        },
        {
            "paper_title": "STEVE-1: A Generative Model for Text-to-Behavior in Minecraft",
            "rating": 1,
            "sanitized_title": "steve1_a_generative_model_for_texttobehavior_in_minecraft"
        }
    ],
    "cost": 0.01729675,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Scaling Instructable Agents Across Many Simulated Worlds</p>
<p>Sima Team 
Maria Abi Raad 
Arun Ahuja 
Catarina Barros 
Frederic Besse 
Andrew Bolt 
Adrian Bolton 
Bethanie Brownfield 
Gavin Buttimore 
Max Cant 
Sarah Chakera 
Stephanie C Y Chan 
Jeff Clune 
University of British Columbia</p>
<p>Adrian Collister 
Vikki Copeman 
Alex Cullum 
Ishita Dasgupta 
Dario De Cesare 
Julia Di Trapani 
Yani Donchev 
Emma Dunleavy 
Martin Engelcke 
Ryan Faulkner 
Frankie Garcia 
Charles Gbadamosi 
Zhitao Gong 
Lucy Gonzales 
Karol Gregor 
Arne Olav Hallingstad 
Tim Harley 
Sam Haves 
Felix Hill 
Ed Hirst 
Drew A Hudson 
Steph Hughes-Fitt 
Danilo J Rezende 
Mimi Jasarevic 
Laura Kampis 
Rosemary Ke 
Thomas Keck 
Junkyung Kim 
Oscar Knagg 
Kavya Kopparapu 
Andrew Lampinen 
Shane Legg 
Alexander Lerchner 
Marjorie Limont 
Yulan Liu 
Maria Loks-Thompson 
Joseph Marino 
Kathryn Martin Cussons 
Loic Matthey 
Siobhan Mcloughlin 
Piermaria Mendolicchio 
Hamza Merzic 
Anna Mitenkova 
Alexandre Moufarek 
Valeria Oliveira </p>
<p>Hannah Openshaw
David Reichert
Yanko Oliveira
Aneesh Pappu, Alex Platonov, Ollie PurkissRenke Pan</p>
<p>John Reid
Pierre Harvey Richemond
Tyson Roberts
Giles Ruscoe, Jaume Sanchez Elias, Tasha Sandars 2</p>
<p>Daniel P. Sawyer
Tayfun Terzi
Tim Scholtes, Guy Simmons, Daniel Slater, Hubert Soyer, Heiko Strathmann, Peter Stys, Allison C. Tam 2, Denis Teplyashin, Davide Vercelli, Bojan Vujatovic, Marcus Wainwright, Jane X. Wang, Zhengdong Wang, Daan Wierstra 2, Duncan Williams, Nathaniel Wong, Sarah YorkNick Young</p>
<p>Scaling Instructable Agents Across Many Simulated Worlds
392840F9F39414F52ABD28BC5466AB07AgentsEmbodimentFoundation ModelsLanguageVideo Games3D Environments
Building embodied AI systems that can follow arbitrary language instructions in any 3D environment is a key challenge for creating general AI.Accomplishing this goal requires learning to ground language in perception and embodied actions, in order to accomplish complex tasks.The Scalable, Instructable, Multiworld Agent (SIMA) project tackles this by training agents to follow free-form instructions across a diverse range of virtual 3D environments, including curated research environments as well as openended, commercial video games.Our goal is to develop an instructable agent that can accomplish anything a human can do in any simulated 3D environment.Our approach focuses on language-driven generality while imposing minimal assumptions.Our agents interact with environments in real-time using a generic, human-like interface: the inputs are image observations and language instructions and the outputs are keyboard-and-mouse actions.This general approach is challenging, but it allows agents to ground language across many visually complex and semantically rich environments while also allowing us to readily run agents in new environments.In this paper we describe our motivation and goal, the initial progress we have made, and promising preliminary results on several diverse research environments and a variety of commercial video games.</p>
<p>Introduction</p>
<p>Despite the impressive capabilities of large language models (Brown et al., 2020;Hoffmann et al., 2022;OpenAI, 2023;Anil et al., 2023;Gemini Team et al., 2023), connecting them to the embodied world that we inhabit remains challenging.Modern AI can write computer programs (Li et al., 2022) or play chess at super-human level (Silver et al., 2018), but the ability of AI to perceive and act in the world remains far below human level.Competence in language alone is easier for AI than grounded perception and behavior, underscoring the well-known paradox that what is easier for AI is harder for humans, and vice versa (Moravec, 1988).In SIMA, we collect a large and diverse dataset of gameplay from both curated research environments and commercial video games.This dataset is used to train agents to follow open-ended language instructions via pixel inputs and keyboard-and-mouse action outputs.</p>
<p>Agents are then evaluated in terms of their behavior across a broad range of skills.</p>
<p>Yet, language is most useful in the abstractions it conveys about the world.Language abstractions can enable efficient learning and generalization (Hill et al., 2020;Colas et al., 2020;Lampinen et al., 2022;Tam et al., 2022;Hu and Clune, 2023).Once learned, language can unlock planning, reasoning (e.g., Huang et al., 2022;Brohan et al., 2023b;Driess et al., 2023;Kim et al., 2023), and communication (Zeng et al., 2022) about grounded situations and tasks.In turn, grounding language in rich environments can make a system's understanding of the language itself more systematic and generalizable (Hill et al., 2019).Thus, several questions emerge: How can we bridge the divide between the symbols of language and their external referents (cf., Harnad, 1990)?How can we connect the abstractions and generality afforded by language to grounded perception and action, and how can we do so in a safe and scalable way?</p>
<p>Here, we draw inspiration from these questions-and the prior and concurrent research projects that have addressed them (e.g., Hermann et al., 2017;Abramson et al., 2020;Brohan et al., 2023a,b;Driess et al., 2023;Wang et al., 2023b;Tan et al., 2024)-to attempt to connect language to grounded behavior at scale.Bridging this gap is a core challenge for developing general embodied AI.</p>
<p>The Scalable, Instructable, Multiworld Agent (SIMA) project aims to build a system that can follow arbitrary language instructions to act in any virtual 3D environment via keyboard-and-mouse actions-from custom-built research environments to a broad range of commercial video games.There is a long history of research in creating agents that can interact with video games or simulated 3D environments (e.g., Mnih et al., 2015;Berner et al., 2019;Vinyals et al., 2019;Baker et al., 2022) and even follow language instructions in a limited range of environments (e.g., Abramson et al., 2020;Lifshitz et al., 2023).In SIMA, however, we are drawing inspiration from the lesson of large language models that training on a broad distribution of data is the most effective way to make progress in general AI (e.g., Brown et al., 2020;Hoffmann et al., 2022;OpenAI, 2023;Anil et al., 2023;Gemini Team et al., 2023).Thus, in contrast to prior works (e.g., Abramson et al., 2020;Vinyals et al., 2019;Berner et al., 2019;Lifshitz et al., 2023), we are attempting to tackle this problem across many simulated environments, in the most general and scalable way possible, by making few assumptions beyond interacting with the environments in the same way as humans do.</p>
<p>To this end, have made a number of design decisions that make our approach more general, but also more challenging:</p>
<p>• We incorporate many rich, visually complex, open-ended video games containing hundreds of objects in a scene and a large number of possible interactions.• These environments are asynchronous (e.g., Berner et al., 2019;Vinyals et al., 2019); unlike many research environments, they do not stop and wait while the agent computes its next action.</p>
<p>• Each instance of a commercial video game needs to run on a GPU; thus, we cannot run hundreds or thousands of actors per game per experiment as often done in RL (cf., Espeholt et al., 2018).• Agents receive the same screen observations that a human playing the game would without access to internal game state, rewards, or any other privileged information (cf., Berner et al., 2019;Vinyals et al., 2019).• To interact with the environments, agents use the same keyboard-and-mouse controls that humans do (e.g., Baker et al., 2022;Humphreys et al., 2022;Lifshitz et al., 2023), rather than handcrafted action spaces or high-level APIs.• We focus on following language instructions (e.g., Abramson et al., 2020) rather than simply playing the games to maximize a win-rate or generating plausible behavior (cf., Berner et al., 2019;Vinyals et al., 2019).• We train and test our agents using open-ended natural language, rather than simplified grammars or command sets (e.g., Abramson et al., 2020).</p>
<p>These design choices make the learning problem harder, but their generality makes expanding to new environments easier: agents use the same interface across environments without requiring a custom design of control and observation spaces for each new game.Furthermore, since the agent-environment interface is human compatible, it allows agents the potential to achieve anything that a human could, and allows direct imitation learning from human behavior.This general interface from language instructions to embodied behavior can also enable agents to transfer previously learned skills zero-shot to never-before-seen games.Doing research in generic virtual environments allows us to test our agents in a broad and challenging range of situations-where the lessons learned are likely to be more applicable to real-world applications with visually rich perception and control such as robotics-without the risks and costs of real-world testing: if the agent crashes a spaceship in a video game, we can just restart the game.</p>
<p>In the SIMA project thus far, we have created an agent that performs short-horizon tasks based on language instructions produced by a user; though instructions could also be produced by a language model (e.g., Jiang et al., 2019;Driess et al., 2023;Wang et al., 2023b;Hu et al., 2023;Ajay et al., 2023).We have a portfolio of over ten 3D environments, consisting of research environments and commercial video games.For research environments we evaluate agents using the ground truth state, but commercial video games are not designed to report on the completion of arbitrary language tasks.We have therefore developed a variety of methods for evaluation in video games, including using optical character recognition (OCR) to detect onscreen text describing task completion, and using human evaluation of recorded videos of agent behavior.In the rest of this tech report, we describe the high-level approach (illustrated in Figure 1) and our initial progress towards the ultimate goal of SIMA: developing an instructable agent that can accomplish anything a human can do in any simulated 3D environment.</p>
<p>Related work</p>
<p>SIMA builds on a long history of using games as a platform for AI research.For example, backgammon provided the initial proving ground for early deep reinforcement learning methods (Tesauro et al., 1995), and later works have achieved superhuman performance even in complex board games like Go (Silver et al., 2016(Silver et al., , 2018)).</p>
<p>Video games Over the last ten years, video games have provided an increasingly important setting for research focused on embodied agents that perform visuomotor control in rich environments.Researchers have used many video game environments, covering a wide spectrum from Atari (Bellemare et al., 2013) to DoTA (Berner et al., 2019) and StarCraft II (Vinyals et al., 2019).In SIMA, however, we restrict our focus to games that resemble 3D physical embodiment most closely, in particular games where the player interacts with a 3D world from a first or over-the-shoulder pseudo-first-person view.This focus excludes many of the games which have previously been used for research, such as the ones listed above.There has however been notable interest in first-person embodied video games as a platform for AI research (Johnson et al., 2016;Tessler et al., 2017;Guss et al., 2019;Pearce and Zhu, 2022;Hafner et al., 2023;Durante et al., 2024;Tan et al., 2024).These video game AI projects have driven the development of many innovative techniques, e.g., learning from videos by annotating them with estimated player keyboard-and-mouse actions using inverse dynamics models (Pearce and Zhu, 2022;Baker et al., 2022).More recently, games that offer API access to the environment have served as a platform for grounding large language models (Wang et al., 2023a), and some works have even considered grounding a language model in a game through direct perception and action of a lower-level controller (Wang et al., 2023b).Instead of focusing on a single game or environment, however, SIMA considers a range of diverse games to train agents on a larger variety of content.</p>
<p>Research environments</p>
<p>Other works have focused on custom, controlled environments designed for research.Many of these environments focus on particular domains of real-world knowledge.For example, AI2-THOR (Kolve et al., 2017), VirtualHome (Puig et al., 2018), ProcTHOR (Deitke et al., 2022), AI Habitat (Savva et al., 2019;Szot et al., 2021;Puig et al., 2023), ALFRED (Shridhar et al., 2020), and Behavior (Srivastava et al., 2021) simulate embodied agents behaving in naturalistic rendered scenes.CARLA (Dosovitskiy et al., 2017) provides a simulator for autonomous driving.MuJoCo (Todorov et al., 2012), PyBullet (Coumans and Bai, 2016-2023), and Isaac Gym (Makoviychuk et al., 2021) provide high quality physics simulators for learning low-level control and are used by benchmarks for robotic manipulation such as Meta-World (Yu et al., 2020) and Ravens (Zeng et al., 2021).Albrecht et al. (2022) propose a unified environment encompassing a variety of skills afforded through ecologically-inspired interactions.The Playhouse (Abramson et al., 2020;DeepMind Interactive Agents Team et al., 2021;Abramson et al., 2022a) and WorldLab (e.g., Gulcehre et al., 2019) environments are built using Unity (see Ward et al., 2020).Open Ended Learning Team et al. (2021) and Adaptive Agent Team et al. (2023) also use Unity to instantiate a broad distribution of procedurally generated tasks with shared underlying principles.For the results in this work, we also use Playhouse, WorldLab, and ProcTHOR.In addition, we introduce a new environment, called the Construction Lab.</p>
<p>Robotics Robotics is a key area for research in embodied intelligence.A variety of robotics projects have used simulations for training, to transfer efficiently to real-world robotic deployments (Höfer et al., 2021), though generally within a single, constrained setting.More recent work has focused on environment-generality, including scaling robotic learning datasets across multiple tasks and embodiments (Brohan et al., 2022(Brohan et al., , 2023a;;Stone et al., 2023;Padalkar et al., 2023)-thereby creating Vision-Language-Action (VLA) models (Brohan et al., 2023a), similar to the SIMA agent.The latter challenge of generalizing or quickly adapting to new embodiments has some parallels to acting in a new 3D environment or computer game where the mechanics are different.Moreover, a variety of recent works have applied pretrained (vision-)language models as a planner for a lower-level instruction-conditional robotic control policy (Brohan et al., 2023b;Driess et al., 2023;Vemprala et al., 2023;Hu et al., 2023).Our approach shares a similar philosophy to the many works that attempt to ground language via robotics.SIMA, however, avoids the additional challenges of costly hardware requirements, resource-intensive data collection, and the practical limitations on diversity of real-world evaluation settings.Instead, SIMA makes progress towards embodied AI by leveraging many simulated environments and commercial video games to obtain the sufficient breadth and richness that we conjecture to be necessary for effectively scaling embodied agents-with the hope that lessons learned (and possibly even the agents themselves) will be applicable to robotic embodiments in the future.</p>
<p>Learning environment models Some works attempt to leverage learned models of environments to train agents in these learned simulations (e.g., Ha and Schmidhuber, 2018;Hafner et al., 2020Hafner et al., , 2023;;Yang et al., 2023).These methods, however, tend to be difficult to scale to diverse sets of visually complex environments that need to be self-consistent across long periods of time.Nevertheless, learning imperfect models can still be valuable.In SIMA, we build on video models (Villegas et al., 2022), which we fine-tune on game environments.However, we only use the internal state representations of the video models rather than explicit rollouts-in keeping with other approaches that use generative modeling as an objective function for learning state representations (e.g., Gregor et al., 2019;Zolna et al., 2024).</p>
<p>Grounding language Another stream of work-overlapping with those above-has focused on grounding language in simulated 3D environments, through agents that are trained in controlled settings with semi-natural synthetic language (Hermann et al., 2017;Hill et al., 2019), or by imitating human interactions in a virtual house to learn a broader ability to follow natural language instructions (Abramson et al., 2020;DeepMind Interactive Agents Team et al., 2021;Abramson et al., 2022a,b).Moreover, a range of recent works develop agents that connect language to embodied action, generally as part of a hierarchy controlled by a language model (Jiang et al., 2019;Driess et al., 2023;Wang et al., 2023b;Hu et al., 2023;Ajay et al., 2023).We likewise draw inspiration from the idea that language is an ideal interface for directing an agent, but extend our scope beyond the limited affordances of a single controlled environment.In that sense, SIMA overlaps more with several recent works (Reed et al., 2022;Huang et al., 2023;Durante et al., 2024) that also explore training a single model to perform a broad range of tasks involving actions, vision, and language.However, SIMA is distinct in our focus on simultaneously (1) taking a language-first perspective, with all training experiences being language-driven; (2) adopting a unified, human-like interface across environments with language and vision to keyboard-and-mouse control; and (3) exploring a broad range of visually rich, diverse, and human-compatible environments that afford a wide range of complex skills.</p>
<p>Language supports grounded learning, and grounded learning supports language A key motivation of SIMA is the idea that learning language and learning about environments are mutually reinforcing.A variety of studies have found that even when language is not necessary for solving a task, learning language can help agents to learn generalizable representations and abstractions, or to learn more efficiently.Language abstractions can accelerate grounded learning, for example accelerating novelty-based exploration in reinforcement learning by providing better state abstractions (Tam et al., 2022;Mu et al., 2022), or composing known goals into new ones (Colas et al., 2020;Nottingham et al., 2023).Moreover, learning to predict natural-language explanations (Lampinen et al., 2022), descriptions (Kumar et al., 2022), or plans (Hu and Clune, 2023) can help agents to learn more efficiently, and to generalize better out of distribution.Language may be a powerful tool for shaping agent capabilities (Colas et al., 2022).</p>
<p>Conversely, richly grounded learning can also support language learning.Since human language use is deeply integrated with our understanding of grounded situations (McClelland et al., 2020), understanding the subtleties of human language will likely benefit from this grounding.Beyond this theoretical argument, empirical evidence shows that grounding can support even fundamental kinds of generalization- Hill et al. (2019) show that agents grounded in richer, more-embodied environments exhibit more systematic compositional generalization.These findings motivate the possibility that learning both language and its grounding will not only improve grounded actions, but improve a system's knowledge of language itself.</p>
<p>Approach</p>
<p>Many overlapping areas of previous and concurrent work share some of our philosophy, motivations, and approaches.What distinguishes the SIMA project is our focus on language-conditional behavior across a diverse range of visually and mechanically complex simulated environments that afford a rich set of skills.In this section, we provide a high-level overview of our approach: our environments, data, agents, and evaluations.</p>
<p>Environments</p>
<p>SIMA aims to ground language across many rich 3D environments (Figure 2).Thus, we selected 3D embodied environments that offer a broad range of open-ended interactions-such environments afford the possibility of rich and deep language interactions.We focus on environments that are either in a) first-person or b) third-person with the camera over the player's shoulder.To achieve diversity and depth of experience, we use a variety of commercial video games, as well as several environments created specifically for agent research.Each type of environment offers distinct advantages, ranging from open-ended diverse experiences to targeted assessments of agent skills.We have deliberately sought to build a portfolio of games that covers a wide range of settings-from mundane tasks in semi-realistic environments, to acting as a mischevious goat in a world with exaggerated physics, to exploring mythological worlds or science-fiction universes.Below, we briefly describe the environments we have used in SIMA thus far by category and in alphabetical order.</p>
<p>Commercial video games</p>
<p>Commercial video games offer exciting, open-ended worlds full of visual richness and the potential for complex interactions.In SIMA, we have partnered with games developers whose games we used for training agents, and we are continuing to develop relationships with new developers-for our full list of current partners, please see our Acknowledgements section.We focus on a variety of open-world or sandbox games that contain diverse skills, while avoiding games containing harmful content such as extreme violence or biases.We have also sought a broad diversity of worlds and stories, but with a focus on games that exhibit a depth of interesting mechanics.Accordingly, games from our portfolio offer a wide range of distinct challenges in perception and action, from flying a spaceship to mining minerals or crafting armor, as well as more common core features, such as navigation or gathering resources.Games also often include interactions that extend beyond the skillset of typical embodied research environments, such as menu use and interfaces more similar to those faced in computer control benchmarks (e.g., Humphreys et al., 2022;Koh et al., 2024).For the results in this report, we focus on single-player interactions within these games.</p>
<p>We run instances of each game in a secure Google Cloud environment, using hardware accelerated rendering to a virtual display.This display is streamed to a browser for human gameplay, or to a remote agent client process during evaluation.To instantiate repeatable evaluation or data collection scenarios within each game, we build datasets of save-game files from expert play, and use scripted processes to automate the process of installing game-files, booting the game, navigating its main menu, and loading a specific save-game.</p>
<p>We now provide a brief description of the games we used.</p>
<p>Goat Simulator 3:</p>
<p>A third-person game where the player is a goat in a world with exaggerated physics.The player can complete quests, most of which involve wreaking havoc.The goat is able to lick, headbutt, climb, drive, equip a wide range of visual and functional items, and perform various other actions.Throughout the course of the game, the goat unlocks new abilities, such as the ability to fly.</p>
<p>Hydroneer: A first-person mining and base building sandbox where the player is tasked with digging for gold and other resources to turn a profit and enhance their mining operation.To do this, they must build and upgrade their set-ups and increase the complexity and levels of automation until they have a fully automated mining system.Players can also complete quests from non-player characters to craft bespoke objects and gain extra money.Hydroneer requires careful planning and managing of resources.</p>
<p>No Man's Sky:</p>
<p>A first-or third-person survival game where the player seeks to explore a galaxy full of procedurally-generated planets.This involves flying between planets to gather resources, trade, build bases, and craft items that are needed to upgrade their equipment and spaceship while surviving a hazardous environment.No Man's Sky includes a large amount of visual diversity-which poses important challenges for agent perception-and rich interactions and skills.</p>
<p>Satisfactory: A first-person, open-world exploration and factory building game, in which players attempt to build a space elevator on an alien planet.This requires building increasingly complex production chains to extract natural resources and convert them into industrial goods, tools, and structures-whilst navigating increasingly hostile areas of a large open environment.</p>
<p>Teardown: A first-person, sandbox-puzzle game in a fully destructible voxel world where players are tasked with completing heists to gain money, acquiring better tools, and undertaking even more high-risk heists.Each heist is a unique scenario in one of a variety of locations where players must assess the situation, plan the execution of their mission, avoid triggering alarms, and escape before a timer expires.Teardown involves planning and using the environment to one's advantage to complete the tasks with precision and speed.</p>
<p>Valheim: A third-person survival and sandbox game in a world inspired by Norse mythology.Players must explore various biomes, gather resources, hunt animals, build shelter, craft equipment, sail the oceans and defeat mythological monsters to advance in the game-while surviving challenges like hunger and cold.</p>
<p>Wobbly Life: A third-person, open-world sandbox game where the player can explore the world, unlock secrets, and complete various jobs to earn money and buy items, leading up to buying their own house.They must complete these jobs whilst contending with the rag-doll physics of their characters and competing against the clock.The jobs require timing, planning, and precision to be completed.The world is extensive and varied, with a diverse range of interactive objects.</p>
<p>Playhouse</p>
<p>Commercial Video Games</p>
<p>WorldLab</p>
<p>Research Environments</p>
<p>Construction Lab</p>
<p>Figure 2 | Environments.We use over ten 3D environments in SIMA, consisting of commercial video games and research environments.The diversity of these environments is seen in their wide range of visual observations and environmental affordances.Yet, because these are all 3D environments, basic aspects of 3D embodied interaction, such as navigation, are shared.Commercial video games offer a higher degree of rich interactions and visual fidelity, while research environments serve as a useful testbed for probing agent capabilities.</p>
<p>Research environments</p>
<p>In contrast to commercial video games, AI research environments are typically more controllable, offering the ability to instill and carefully assess particular skills, and more rapid and reliable evaluations of task completion.Unlike many of the games in our portfolio, several of these research environments also tend to feature more real-world analogous-if still simplified-physical interactions.</p>
<p>We have drawn on several prior research environments and developed a new environment-the Construction Lab-that incorporates important challenges which were not otherwise well-captured by our other environments.</p>
<p>Construction Lab: A new research environment where agents need to build novel items and sculptures from interconnecting building blocks, including ramps to climb, bridges to cross, and dynamic contraptions.Construction Lab focuses on cognitive capabilities such as object manipulation and an intuitive understanding of the physical world.</p>
<p>Playhouse: An environment used in various prior works (Abramson et al., 2020;DeepMind Interactive Agents Team et al., 2021;Abramson et al., 2022a), consisting of a procedurally-generated house environment with various objects.We have augmented this environment with improved graphics and richer interactions, including skills like cooking or painting.</p>
<p>ProcTHOR: An environment consisting of procedurally-generated rooms with realistic contents, such as offices and libraries, introduced by Deitke et al. (2022).Although benchmark task sets exist in this environment, prior works have not used keyboard and mouse actions for agents; thus we focus on this environment primarily for data collection rather than evaluation.</p>
<p>WorldLab: An environment used in prior work (e.g., Gulcehre et al., 2019), further specialized for testing embodied agents by using a limited set of intuitive mechanics, such as sensors and doors, and relying primarily on the use of simulated physics on a range of objects.The SIMA dataset includes a broad range of text instructions that can be roughly clustered into a hierarchy.Due to the common 3D embodied nature of the environments that we consider, many generic tasks, such as navigation and object manipulation, are present in multiple environments.Categories were derived from a data-driven hierarchical clustering analysis of the human-generated text instructions within a fixed, pretrained word embedding space.Note that the area of each cluster in the wheel in Figure 3 does not correspond to the exact number of instructions from that cluster in the dataset.</p>
<p>Data</p>
<p>Our approach relies on training agents at scale via behavioral cloning, i.e., supervised learning of the mapping from observations to actions on data generated by humans.Thus, a major focus of our effort is on collecting and incorporating gameplay data from human experts.This includes videos, language instructions and dialogue, recorded actions, and various annotations such as descriptions or marks of success or failure.These data constitute a rich, multi-modal dataset of embodied interaction within over 10 simulated environments, with more to come. 1 Our data can be used to augment and leverage existing training data (e.g., Abramson et al., 2020), or to fine-tune pretrained models to endow them with more situated understanding.These datasets cover a broad range of instructed tasks: Figure 3 shows instruction clusters derived from hierarchically clustering the text instructions present in the data within a fixed, pretrained word embedding space.</p>
<p>Yet, collecting data at scale is not sufficient for training successful agents.Data quality processes 1 Note: Due to a limited amount of collected data and/or evaluations, we present agent evaluation results (Section 4) on a subset of 7 of these environments.are critical to ensuring an accurate and unconfounded mapping between language and behavior.This presents various technical challenges.We take care to engineer our data collections, including preprocessing and filtering the raw data, to highlight important skills and effectively train our agents.</p>
<p>Data collections</p>
<p>We collect data using a variety of methods, including allowing single players to freely play, and then annotating these trajectories with instructions post-hoc.We also perform two-player setter-solver collections (Abramson et al., 2020;DeepMind Interactive Agents Team et al., 2021), in which one player instructs another what to do in selected scenarios while sharing a single player view in order to match the single-player collections.All our data collections were performed with participants contracting with Google.The full details of our data collection protocols, including compensation rates, were reviewed and approved by an independent Human Behavioral Research Committee for ethics and privacy.All participants provided informed consent prior to completing tasks and were reimbursed for their time.</p>
<p>Preprocessing, filtering, and weighting Before training, we perform a variety of offline preprocessing steps, including resizing data for agent input, filtering out low-quality data using a variety of heuristics, and remixing and weighting data across environments and collections to prioritize the most effective learning experiences.</p>
<p>Agent</p>
<p>The SIMA agent maps visual observations and language instructions to keyboard-and-mouse actions (Figure 4).Given the complexity of this undertaking-such as the high dimensionality of the input and output spaces, and the breadth of possible instructions over long timescales-we predominantly focus on training the agent to perform instructions that can be completed in less than approximately 10 seconds.Breaking tasks into simpler sub-tasks enables their reuse across different settings and entirely different environments, given an appropriate sequence of instructions from the user.</p>
<p>Our agent architecture builds on prior related work (Abramson et al., 2020(Abramson et al., , 2022a)), but with various changes and adaptations to our more general goals.First, our agent incorporates not only trained-from-scratch components, but also several pretrained models-including a model trained on fine-grained image-text alignment, SPARC (Bica et al., 2024), and a video prediction model, Phenaki (Villegas et al., 2022)-which we further fine-tune on our data through behavioral cloning and video prediction, respectively.In preliminary experiments, we found that these models offer complementary benefits.Combining these pre-trained models with fine-tuning and from-scratch training allows the agent to utilize internet-scale pretraining while still specializing to particular aspects of the environments and the control tasks that it encounters.</p>
<p>More specifically, our agent (Figure 4) utilizes trained-from-scratch transformers that cross-attend to the different pretrained vision components, the encoded language instruction, and a Transformer-XL (Dai et al., 2019) that attends to past memory states to construct a state representation.The resulting state representation is provided as input to a policy network that produces keyboard-and-mouse actions for sequences of 8 actions.We train this agent with behavioral cloning, as well as an auxiliary objective of predicting goal completion.</p>
<p>We use Classifier-Free Guidance (CFG; Ho and Salimans, 2022;Lifshitz et al., 2023) to improve the language-conditionality of a trained agent when running it in an environment.CFG was originally proposed for strengthening text-conditioning in diffusion models (Ho and Salimans, 2022), but has also proven useful for similar purposes with language models (Sanchez et al., 2023) and languageconditioned agents (Lifshitz et al., 2023).That is, we compute the policy, , with and without language conditioning, and shift the policy logits in the direction of the difference between the two:   =  (image, language) +  ( (image, language) −  (image, •)) .</p>
<p>Evaluation methods</p>
<p>Our focus on generality in SIMA introduces challenges for evaluation.While research environments may provide automated methods for assessing whether language-following tasks have been successfully completed, such success criteria may not be generally available.That is, language instructions may not correspond to goal states recorded by an environment (e.g. a user might instruct "make a pile of rocks to mark this spot" or "see if you can jump over this chasm").</p>
<p>Evaluating agents in commercial video games poses substantial additional challenges.Video game evaluations cannot rely on access to privileged information about the state of an environment.Additionally, it is difficult to reinstate agents in precisely the same state in environments that are not designed as reproducible benchmarks, and loading each task in commercial video games is considerably slower and more costly than those in research environments.Achieving fast, stable, and reliable evaluations comparable across environments is thus challenging.We therefore use a range of distinct evaluation types that provide different trade-offs in efficiency, cost, accuracy, and coverage.Moreover, ensuring that our evaluations truly assess language conditionality, rather than environmental affordances, requires care.For instance, if a task contains a knife, a cutting board, and a carrot, the agent may ascertain the goal ("cut the carrot on the cutting board") without relying on the language instruction.Thus, task settings need to afford a diversity of actions, ideally testing multiple instructions from a single initial state, to properly evaluate whether the agent's actions are driven by language.</p>
<p>Action log-probabilities One simple approach is to evaluate agents based on their action predictions on held-out evaluation data.However, consistent with prior findings (Abramson et al., 2022b;Baker et al., 2022), we observed that agent action log-probabilities on evaluation data show at most a weak correlation with agent performance beyond the most basic skills.Thus, online evaluations, in which the agent interacts with the environment, are needed to understand agent performance in detail.</p>
<p>Static visual input</p>
<p>Similar to predicting actions on held-out data, we can provide the agent with a static visual input and a language instruction to perform a particular valid action (e.g., "jump") to assess simple responses directly mapping to particular keyboard and/or mouse actions.We have used evaluations of this form for our commercial video game environments, as they have the advantage of not requiring actually loading a game.While these evaluations can be a useful early signal, they do not reliably predict success on prolonged tasks.</p>
<p>Ground-truth Our internally-developed research environments (Construction Lab, Playhouse, and WorldLab) are capable of providing ground-truth assessments of whether language-following tasks have been successfully completed.These tasks can depend on the state of the agent ("move forward") and the surrounding environment ("lift the green cube"), as well as more complex interactions ("attach a connector point to the top of the large block" or "use the knife to chop the carrots").Such tasks enable robust testing of a range of particular skills, with a highly reliable signal of task success.Moreover, we design the task settings and evaluation to be strong tests of precision; for example, many tasks include distractor objects, for which the episode is marked as an immediate failure if the agent interacts with the distractors rather than the instruction target-even if the agent might have completed the actual task later.We also include other types of assessments, such as instructing the agent to complete one goal, and then interrupting with another goal to evaluate whether it switches appropriately-this ensures that agents are sufficiently responsive to changes in commands.A subset of our research environment tasks are used to provide a fast evaluation signal of agent progress during training.</p>
<p>Optical character recognition (OCR)</p>
<p>Many of our commercial video game environments provide on-screen text signalling the completion of tasks or quests, or even the results of lower-level actions like collecting resources or entering certain areas of a game.By detecting on-screen text using OCR in pre-defined evaluation scenarios, sometimes in combination with detecting specific keyboard-andmouse actions, we can cheaply assess whether the agent has successfully performed particular tasks.This form of automated evaluation also avoids the subjectivity of human evaluations.We make use of OCR evaluation in particular for two games, No Man's Sky and Valheim, which both feature a significant amount of on-screen text.In No Man's Sky, for example, we have developed evaluation tasks such as "mine carbon/salt/ferrite", "use the analysis visor", or "open the exosuit menu".Similarly, in Valheim we have tasks such as "collect wood/stone/raspberries", "use the workbench", or "cook food".In general, however, OCR evaluations are restricted to tasks that signal completion with game-specific text rather than arbitrary tasks that can be specified with language instructions and which we would expect a general agent to be able to solve.Other video games also have significantly less on-screen text, which makes the range of behaviors that can be evaluated in these games with OCR very narrow.</p>
<p>Human evaluation</p>
<p>In the many cases where we cannot automatically derive a signal of task success, we turn to humans to provide this assessment.While this is our most general evaluation method, it is also the slowest and most expensive.We use human judges who are game experts, i.e., they have played these specific games for at least 16 hours, and often over the course of several weeks.We ask them to review recorded agent videos, collecting multiple ratings of the same video from different judges (typically 5) to ensure reliable assessments.We also encourage strict evaluations: we instruct judges to mark an episode as a failure in cases where the agent performs irrelevant actions first, even if the agent successfully completes the instructed task afterward.</p>
<p>We curated our human-evaluation tasks by identifying a list of frequently-occurring verbs in English, and combined it with a list of verbs that naturally emerged from gameplay and interactive testing of our agents.We use this verb list as a foundation for our evaluations across all video game environments.We assign each task (save state and instruction pair) to a single, most-representative skill category (e.g."craft items"), even though most tasks require a wide range of implicit skills to succeed (e.g.crafting often requires menu use).The resulting evaluation set provides a long term challenge for agent research that spans a wide range of difficulties-from simple game agnostic tasks such as "turn left", to ones testing specialized game knowledge "compare the crafting cost of antimatter and antimatter housing", to ones utilising broader semantic knowledge such as "take the pitchfork from the person shoveling hay".Grounding our evaluation framework in the distribution of natural language allows us to test our agents in both common and adversarial scenarios, and thereby to measure our progress towards our long-term goal of developing an instructable agent that can accomplish anything a human can do in any simulated 3D environment.</p>
<p>In the results below (Section 4), we primarily report evaluation scores based on ground-truth evaluations for research environments and combined OCR and human evaluations for commercial video game environments.Across the 7 environments for which we have evaluations, we have a total of 1,485 unique tasks, spanning a range of 9 skill categories, from movement ("go ahead", "look up", "jump") to navigation ("go to the HUB terminal", "go to your ship"), resource gathering ("collect carbon", "get raspberries"), object management ("use the analysis visor", "cut the potato"), and more.(For reference, MineDojo (Fan et al., 2022), a related work investigating language-conditional agents in MineCraft, used 1,581 unique tasks spanning 4 skill categories: survival, harvest, tech-free, and combat).Given the diversity and coverage of our current evaluations, they provide a reasonable assessment of the fundamental language-conditional skills that we expect from our agent.Yet, there remains ongoing work in developing more scalable, general, and reliable evaluations, particularly as we move toward more complex and open-ended tasks.</p>
<p>Latency mitigations</p>
<p>Our agent is evaluated in several environments that run in real-time, asynchronously to the agent.This can pose challenges for the timely execution of agent-generated actions.Latencies or delays (Bratko et al., 1995) are introduced by the computation of actions and the transmission of observations and actions over the network.We account for this latency during behavioral cloning by predicting actions that are offset in time relative to the visual input to the agent, and mirror this offset during evaluation by appropriate buffering of observations and actions during neural-network inference.We additionally minimize latencies with appropriate scheduling of action computation on TPU accelerators, on-device caching of neural-network state across timesteps, and by careful choices of batch size and other implementation details.</p>
<p>Responsibility</p>
<p>We follow a structured approach to responsible model development, to identify, measure, and manage foreseeable ethics and safety challenges.These are informed by academic literature reviews, engaging with internal ethics teams, and developing comprehensive ethical assessments that document key risks with mitigation strategies.We ensure that our research projects uphold Google's AI Principles.2 SIMA was carefully assessed and reviewed to ensure that its societal benefits outweigh the risks, and that appropriate risk mitigations are incorporated.</p>
<p>No Man's Sky -"go to the spaceship"</p>
<p>Valheim -"chop down a tree" Goat Simulator 3 -"drive the car" Satisfactory -"go to the HUB" Teardown -"go through the gate"</p>
<p>Figure 5 | Agent Trajectories.The SIMA agent is capable of performing a range of language-instructed tasks across diverse 3D virtual environments.Here, we provide several representative, visually salient examples of the agent's capabilities that demonstrate basic navigation and tool use skills.</p>
<p>Benefits SIMA is a cutting-edge research initiative which focuses on how to develop instructable agents in simulated environments.This research presents interesting opportunities for the future of humans and AI collaborating together; unlike LLMs, SIMA is able to both understand natural language instructions and dynamic, interactive 3D environments.This presents a new paradigm for working with AI agents, and the potential for exciting new immersive 3D experiences with AI.Finally, simulated environments present a safer alternative for research compared to other AI deployments.</p>
<p>Risks As well as these benefits, we have reflected on potential risks associated with training on video game data.These include risks associated with training an agent on games that include violent, explicit or otherwise harmful behaviors.We have also reflected on the implications on representational harms, as the agent may learn from stereotyped depictions or actions in game settings.Besides these risks, there are also down stream risks associated with the future hypothetical deployments of SIMA, through either intentional misuse or benign action.</p>
<p>Mitigations</p>
<p>We have worked to ameliorate these risks through a holistic approach, including:</p>
<p>• Careful curation of content.We avoided a number of games that have scientifically interesting, but violent environments.We also outlined behavioral "red-lines" with our ethics and safety teams; games with content that violates these red-lines are not used.• Continuous evaluations of SIMA's safety performance.</p>
<p>• Ensuring SIMA's deployments and agreements are transparent, and for now remain in a controlled, closed environment.</p>
<p>Ultimately, given the careful training data selection and constrained deployment environment of SIMA, we are confident we can maximize the benefits while minimising the ethical risks.</p>
<p>Initial results</p>
<p>In this section, we report initial evaluation results of the SIMA agent.After presenting several qualitative examples of the SIMA agent's capabilities, we start by considering the quantitative performance of the SIMA agent, broken down by environment and skill category.We then compare these results with several baselines and ablations, allowing us to assess the generalization capabilities of the agent and the efficacy of our design choices.Finally, we investigate a subset of evaluation tasks to estimate human-level performance as an additional comparison.</p>
<p>Qualitative examples To provide a sense of the agent's general capabilities, Figure 5 displays several representative examples of the agent in our commercial video game environments.Despite the visual diversity of the environments, the agent is capable of performing these tasks, demonstrating basic navigation and tool use skills.Even when the instructed target is not in view ("go to the spaceship" and "go to the HUB"), the agent is able to find the target.For further qualitative examples, please refer to the accompanying website. 3</p>
<p>Performance across environments and skills</p>
<p>In Figure 6, we report the average performance of the SIMA agent across the seven environments for which we have quantitative evaluations.Averages are calculated across multiple episodes per task (in research environments, one episode per task in video games), multiple tasks per environment, and across three training runs with different random seeds.Error bars denote the 95% confidence intervals (CIs) across the tasks within that environment and the three training runs with different random seeds.We note that developing informative evaluation tasks is in itself an ongoing effort, and the quantitative results in this work reflect only the range of particular behaviors that are evaluated at this point in time.</p>
<p>Overall, the results show that the SIMA agent is able to complete a range of tasks across many environments, but there remains substantial room for improvement.Performance is better for Playhouse and WorldLab, which are comparatively simpler research environments.For the more complex commercial video game environments, we see that performance is, understandably, somewhat lower.Notably, performance on Construction Lab is lower as well, highlighting the relative difficulty of this research environment and its evaluation tasks.This enables the SIMA platform to serve as a useful testbed for further development of agents that can connect language to perception and action.</p>
<p>In order to better understand the performance of the SIMA agent across an increasing variety of simulated environments, we developed an evaluation framework grounded in natural language for adding and clustering evaluation tasks, as detailed in our evaluation methods.As these skill clusters are derived from our evaluation tasks rather than the training data, they are similar to, yet distinct from, those in Figure 3.As shown in Figure 7, performance varies across different skill categories, including within skill clusters such as "movement" or "game progression".Note that even seemingly simple skill clusters can involve nontrivial game interactions, e.g., some of the "look" tasks involve 3 https://deepmind.google/discover/blog/sima-generalist-ai-agent-for-3d-virtual-environments/Agents achieve notable success, but are far from perfect; their success rates vary by environment.Colors indicate the evaluation method(s) used to assess performance for that environment.(Note that humans would also find some of these tasks challenging, and thus human-level performance would not be 100%, see Section 4.3.)skills like steering a spaceship ("look at a planet") or orienting based on the surrounding terrain ("look downhill").While there are many subtleties depending on these additional interactions and the mechanics of the environment in which the skill is used, in general, skills that require more precise actions or spatial understanding ("combat", "use tools", "build") tend to be more challenging.</p>
<p>Evaluating environment generalization &amp; ablations</p>
<p>We compare our main SIMA agent to various baselines and ablations, both in aggregate (Figure 8) and broken down across our environments (Figure 9).The agents we report across all environments include:</p>
<p>• SIMA: Our main SIMA agent, which is trained across all environments except for Hydroneer and Wobbly Life, which we use for qualitative zero-shot evaluation.• Zero-shot: Separate SIMA agents trained like the main agent, but only on  − 1 of our environments, and evaluated zero-shot on the held-out environment-that is, without any BC training on it.These agents assess the transfer ability of our agent in a controlled setting.(Note that these agents use the same pretrained encoders as the main SIMA agent, which were finetuned on data from a subset of our environments; thus, in some cases the pretrained encoders will have been tuned with visual inputs from the held-out environment, even though the agent has not been trained to act in that environment.However, the encoders were not fine-tuned on data from Goat Simulator 3, thus the transfer results in that case are unconfounded.)• No pretraining ablation: An agent where we removed the pretrained encoders in the SIMA agent.We replaced these models with a ResNet vision model that is trained from scratch (as in Abramson et al., 2022a), as in preliminary experiments we found training the SPARC/Phenaki encoders through agent training resulted in poor performance.Comparing to this agent tests the benefits of pretrained models for agent performance.• No language ablation: An agent that lacks language inputs, during training as well as evaluation.</p>
<p>Comparing to this agent shows the degree to which our agent's performance can be explained by simple language-agnostic behavioral priors.• Environment-specialized: We additionally train an expert agent on each environment, which is trained only on data corresponding to that environment, but still includes the more broadly pretrained encoders.We normalize the performance of all other agents by the expert agent on Agents exhibit varying degrees of performance across the diverse skills that we evaluate, performing some skills reliably and others with more limited success.Skill categories are grouped into clusters (color), which are derived from our evaluation tasks.</p>
<p>each environment, as a measure of what is possible using our methods and the data we have for that environment.</p>
<p>Note that due to the number of comparison agents, we only ran a single seed for each, rather than the three seeds used for the main SIMA agent.Each agent is evaluated after 1.2 million training steps. 4The bars in Figure 8 and Figure 9 represent average performance (normalized relative to the environment-specialist); the errorbars are parametric 95%-CIs across tasks and seeds (where multiple seeds are available).</p>
<p>Figure 8 shows a summary of our results, while Figure 9 shows the results by environment.SIMA outperforms environment-specialized agents overall (67% average improvement over environmentspecialized agent performance), thus demonstrating positive transfer across environments.We statistically quantify this benefit by using a permutation test on the mean difference across the per-task performance of the SIMA agent and the environment-specialized agent within each domain; in every case SIMA significantly outperforms the environment-specialized agent (-values on each environment respectively: 0.001, 0.002, 0.036, 0.0002, 0.008, 0.004, and 0.0002).Furthermore, SIMA performs much better than the baselines.SIMA substantially outperforms the no-pretraining baseline overall (permutation test  &lt; 0.001), thus showing that internet-scale knowledge supports grounded learning-though the magnitude and significance of the benefit varies across the environments (permutation test -values respectively 0.0002, 0.14, 0.041, 0.0002, 0.244, 0.052, 0.032).Finally, the no-language ablation performs very poorly (all permutation tests  &lt; 0.001).Importantly, this demonstrates not only that our agent is in fact using language, but also that our evaluation tasks are effectively designed to test this capability, rather than being solvable by simply executing plausible behaviors.Comparing the SIMA agent to an ablation without classifier-free guidance (CFG), CFG substantially improves language conditionality.However, even without CFG, the agent still exhibits language-conditional behavior, outperforming the No Language ablation.Note that this evaluation was performed only on a subset of our research environments: Construction Lab, Playhouse, and WorldLab.</p>
<p>The zero-shot evaluations are also promising.Even when tested in an environment on which it has not been trained to act the agent demonstrates strong performance on general tasks, though of course it falls short in achieving environment-specific skills.Zero-shot agents are capable of performing generic navigation skills that appear across many games (e.g."go down the hill"), and show some more complex abilities like grabbing an object by its color, using the fact that color is consistent across games, and the consistent pattern that most games use left mouse to grab or interact with objects.Importantly, even on the Goat Simulator 3 environment, where the agents have not even received visual finetuning, the zero-shot agent still performs comparably to the environmentspecialized one-thus showing transfer is not driven by the visual components alone.Note that even where the numerical performance of the zero-shot and environment-specialized agents is similar, they are generally good at different skills-with the environment-specialized agent performing well on game-specific interactions, but performing more weakly on common skills that are supported across many games, and that the zero-shot agent therefore can execute.</p>
<p>Note that zero-shot performance is especially strong on the WorldLab environment for three reasons.First, the evaluation tasks for this environment contain a relatively larger proportion of domain-general skills, such as recognizing objects by color, because we use them as rapid tests of agent capabilities.Second, this environment uses the same underlying engine and shares some implementation details with the other internal research environments, which may support behavioral transfer despite their varied visual styles, asset libraries, physical mechanics, and environment affordances.Furthermore, environment-specialized agent performance may be slightly weaker on this environment because there is a non-trivial distribution shift from training to test.This is because some of our data comes from earlier versions of the environment with differences in dynamics, and task distributions.Agents trained across multiple environments may be more robust to this distribution shift.</p>
<p>Classifier-free guidance Finally, Figure 10 compares the performance of agents with and without classifier-free guidance (CFG; Lifshitz et al., 2023), evaluated on a subset of our research environments: Construction Lab, Playhouse, and WorldLab.Without CFG ( = 0), the SIMA agent performs noticeably worse.However, the No CFG agent still exhibits a high degree of language conditionality, significantly outperforming the No Language baseline.These results show the benefit of CFG, highlighting the impact that inference-time interventions can have on agent controllability.</p>
<p>Figure 11 | Comparison with Human Performance on No Man's Sky.Evaluating on a subset of tasks from No Man's Sky, human game experts outperform all agents.Yet, humans only achieve 60% success on this evaluation.This highlights the difficulty of the tasks considered in this project.</p>
<p>Human comparison</p>
<p>To provide an additional baseline comparison, we evaluated our agents against expert human performance on an additional set of tasks from No Man's Sky, which were chosen to test a focused set of skills in a diverse range of settings.These tasks range in difficulty, from simple instructions ("walk forward") to more complex instructions ("use the analysis visor to identify new animals").The humans who performed the tasks were players who participated in our data collection and had experience with the game.We evaluated human performance using the same judges and evaluation setup that was used for our agents; the judges were not told that they were evaluating human performance rather than agents.</p>
<p>Results are summarized in Figure 11 with error bars denoting parametric 95%-CIs.The human players achieved a success rate of only 60% on these tasks, demonstrating the difficulty of the tasks we considered in this project and the stringency of our evaluation criteria.For example, some human failures appear to be due to engaging in unnecessary behaviors before completing the task, like initially opening and interacting with the starship menu when instructed to "recharge the mining beam," or entering analysis mode after scanning when told to "mine oxygen."Despite these challenging evaluations, the SIMA agent achieved non-trivial performance (34% success), far exceeding that of the No Language baseline (11% success), for example.We note that 100% success may not necessarily be achievable, due to disagreement between human judges on more ambiguous tasks.Nevertheless, there is still considerable progress needed to match human performance.This underscores the utility of the entire SIMA setup for providing a challenging, yet informative, metric for assessing grounded language interactions in embodied agents.</p>
<p>Looking ahead</p>
<p>SIMA is a work in progress.In this tech report, we have described our goal and philosophy, and presented some preliminary results showing our agent's ability to ground language instructions in behavior across a variety of rich 3D environments.We see notable performance and early signs of transfer across environments, as well as zero-shot transfer of basic skills to held-out environments.Still, many skills and tasks remain out of reach.In our future work, we aim to a) scale to more environments and datasets by continuing to expand our portfolio of games, environments, and datasets; b) increase the robustness and controllability of agents; c) leverage increasingly high-quality pretrained models (Gemini Team et al., 2023); and d) develop more comprehensive and carefully controlled evaluations.</p>
<p>We believe that by doing so, we will make SIMA an ideal platform for doing cutting-edge research on grounding language and pretrained models safely in complex environments, thereby helping to tackle a fundamental challenge of AGI.Our research also has the potential to enrich the learning experiences and deployment environments of future foundation models; one of our goals is to ground the abstract capabilities of large language models in embodied environments.We hope that SIMA will help us learn how to overcome the fundamental challenge of linking language to perception and action at scale, and we are excited to share more details about our research in the future.</p>
<p>Figure 1 |
1
Figure1| Overview of SIMA.In SIMA, we collect a large and diverse dataset of gameplay from both curated research environments and commercial video games.This dataset is used to train agents to follow open-ended language instructions via pixel inputs and keyboard-and-mouse action outputs.Agents are then evaluated in terms of their behavior across a broad range of skills.</p>
<p>Figure 3 |
3
Fight Kick</p>
<p>Figure 4 |
4
Figure 4 | Setup &amp; SIMA Agent Architecture.The SIMA agent receives language instructions from a user and image observations from the environment, and maps them to keyboard-and-mouse actions.</p>
<p>Figure 6 |
6
Figure6| Average Success Rate of the SIMA Agent by Environment.Agents achieve notable success, but are far from perfect; their success rates vary by environment.Colors indicate the evaluation method(s) used to assess performance for that environment.(Note that humans would also find some of these tasks challenging, and thus human-level performance would not be 100%, see Section 4.3.)</p>
<p>Figure 7 |
7
Figure7| Average Success Rate of the SIMA Agent by Skill Category.Agents exhibit varying degrees of performance across the diverse skills that we evaluate, performing some skills reliably and others with more limited success.Skill categories are grouped into clusters (color), which are derived from our evaluation tasks.</p>
<p>Figure 10 |
10
Figure10| Evaluating the Benefit of Classifier-Free Guidance.Comparing the SIMA agent to an ablation without classifier-free guidance (CFG), CFG substantially improves language conditionality.However, even without CFG, the agent still exhibits language-conditional behavior, outperforming the No Language ablation.Note that this evaluation was performed only on a subset of our research environments: Construction Lab, Playhouse, and WorldLab.</p>
<p>https://ai.google/responsibility/principles/
With one exception: as we had a relatively small quantity of data for Goat Simulator 3, we attempted to prevent the environment-specialized baseline from overfitting by evaluating it every 200,000 training steps, then selecting the best performing number of steps, which was 400,000 steps, as our environment-specialized baseline. Although this is a biased selection process, because we are using the environment-specialized agent as a baseline, it will only lead to underestimating the advantage of SIMA.
AcknowledgementsWe thank the following games developers for partnering with us on this project: Coffee Stain, Foulball Hangover, Hello Games, Keen Software House, Rubberband Games, Saber Interactive / Tuxedo Labs, and Strange Loop Games.We also thankBica et al. (2024)for their assistance in incorporating SPARC into the SIMA agent as well asZolna et al. (2024)and Scott Reed for their assistance in incorporating Phenaki into the SIMA agent.We thank Matthew McGill, Nicholas Roy, Avraham Ruderman, Daniel Tanis, and Frank Perbet for their assistance with research environment task development.We thank Alistair Muldal for assistance with data and infrastructure from prior efforts.We also thank Timothy Lillicrap for early input into the SIMA concept and insights from prior efforts.We thank Tom Ward, Joe Stanton, David Barker, and George Thomas for their infrastructure and support for running game binaries on Google Cloud infrastructure.Finally, we thank our team of participants who generated gameplay and language annotation data, as well as performed human evaluations of our agents, without whom this work would not have been possible.Generalization + AblationsRelative Performance (%)Figure8| Aggregate Relative Performance.Bars indicate the performance of the SIMA agent as well as the baselines and ablations relative to the performance of the environment-specialized agents, aggregated equally across environments.The SIMA agent outperforms ablations that do not incorporate internet pretraining and substantially outperforms an ablation without language.The solid line shows environment-specialized relative performance, which by normalization is 100%.Proprietary + ConfidentialGeneralization + Ablations Bars indicate the performance of the SIMA agent as well as the baselines and ablations relative to the performance of the environment-specialized agents.While performance varies across the environments, the general pattern of results is largely preserved.Even when trained while holding out an environment and evaluated zero-shot on the unseen environment, our agent can achieve non-trivial performance-almost always outperforming the no-language ablation, and in some cases even matching or exceeding environment-specialized agent performance.The solid line shows the relative performance of an environment-specialized agent, which by normalization is 100%.No Language (Ablation)Human BaselineAuthor contributionsIn this section, we summarize author contributions by project area, role in the area, and then alphabetically per role.A role key is provided at the end.Agents &amp; modelsLeads
. Josh Abramson, Arun Ahuja, Iain Barr, Arthur Brussee, Federico Carnevale, Mary Cassin, Rachita Chhaparia, Stephen Clark, Bogdan Damoc, Andrew Dudzik, arXiv:2012.056722020Imitating Interactive Intelligence. arXiv preprint</p>
<p>Improving Multimodal Interactive Agents with Reinforcement Learning from Human Feedback. Josh Abramson, Arun Ahuja, Federico Carnevale, Petko Georgiev, Alex Goldin, Alden Hung, Jessica Landon, Jirka Lhotka, Timothy Lillicrap, Alistair Muldal, arXiv:2211.116022022aarXiv preprint</p>
<p>. Josh Abramson, Arun Ahuja, Federico Carnevale, Petko Georgiev, Alex Goldin, Alden Hung, Jessica Landon, Timothy Lillicrap, Alistair Muldal, Blake Richards, arXiv:2205.132742022bEvaluating Multimodal Interactive Agents. arXiv preprint</p>
<p>Human-Timescale Adaptation in an Open-Ended Task Space. Adaptive Agent Team, Jakob Bauer, Kate Baumli, Satinder Baveja, Feryal Behbahani, Avishkar Bhoopchand, Nathalie Bradley-Schmieg, Michael Chang, ; , International Conference on Machine Learning. Natalie Clay, Adrian Collister,2023</p>
<p>Akash Srivastava, and Pulkit Agrawal. Compositional Foundation Models for Hierarchical Planning. Anurag Ajay, Seungwook Han, Yilun Du, Shuang Li, Abhi Gupta, Tommi Jaakkola, Josh Tenenbaum, Leslie Kaelbling, Advances in Neural Information Processing Systems. 2023</p>
<p>Avalon: A Benchmark for RL Generalization Using Procedurally Generated Worlds. Joshua Albrecht, Abraham Fetterman, Bryden Fogelman, Ellie Kitanidis, Bartosz Wróblewski, Nicole Seo, Michael Rosenthal, Maksis Knutins, Zack Polizzi, James Simon, Advances in Neural Information Processing Systems. 2022</p>
<p>. Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, arXiv:2305.104032023arXiv preprint</p>
<p>Video PreTraining (VPT): Learning to Act by Watching Unlabeled Online Videos. Bowen Baker, Ilge Akkaya, Peter Zhokov, Joost Huizinga, Jie Tang, Adrien Ecoffet, Brandon Houghton, Raul Sampedro, Jeff Clune, Advances in Neural Information Processing Systems. 2022</p>
<p>The Arcade Learning Environment: An Evaluation Platform for General Agents. Yavar Marc G Bellemare, Joel Naddaf, Michael Veness, Bowling, Journal of Artificial Intelligence Research. 472013</p>
<p>Dota 2 with Large Scale Deep Reinforcement Learning. Christopher Berner, Greg Brockman, Brooke Chan, Vicki Cheung, Przemysław Dębiak, Christy Dennison, David Farhi, Quirin Fischer, Shariq Hashme, Chris Hesse, arXiv:1912.066802019arXiv preprint</p>
<p>Improving fine-grained understanding in image-text pre-training. Ioana Bica, Anastasija Ilić, Matthias Bauer, Goker Erdogan, Matko Bošnjak, Christos Kaplanis, Alexey A Gritsenko, Matthias Minderer, Charles Blundell, Razvan Pascanu, Jovana Mitrović, arXiv:2401.098652024arXiv preprint</p>
<p>Ivan Bratko, Tanja Urbančič, Claude Sammut, Behavioural Cloning: Phenomena, Results and Problems. IFAC Proceedings Volumes. 199528</p>
<p>RT-1: Robotics Transformer for Real. Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Joseph Dabis, Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Jasmine Hsu, arXiv:2212.068172022arXiv preprint</p>
<p>RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control. Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Xi Chen, Krzysztof Choromanski, Tianli Ding, Danny Driess, Avinava Dubey, Chelsea Finn, arXiv:2307.158182023aarXiv preprint</p>
<p>Do As I Can, Not As I Say: Grounding Language in Robotic Affordances. Anthony Brohan, Yevgen Chebotar, Chelsea Finn, Karol Hausman, Alexander Herzog, Daniel Ho, Julian Ibarz, Alex Irpan, Eric Jang, Ryan Julian, Conference on Robot Learning. 2023b</p>
<p>Language Models are Few-Shot Learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Advances in Neural Information Processing Systems. 2020</p>
<p>Language as a Cognitive Tool to Imagine Goals in Curiosity-Driven Exploration. Cédric Colas, Tristan Karch, Nicolas Lair, Jean-Michel Dussoux, Clément Moulin-Frier, Peter Dominey, Pierre-Yves Oudeyer, Advances in Neural Information Processing Systems. 2020</p>
<p>Language and culture internalization for human-like autotelic AI. Cédric Colas, Tristan Karch, Clément Moulin-Frier, Pierre-Yves Oudeyer, Nature Machine Intelligence. 4122022</p>
<p>PyBullet, a Python module for physics simulation for games, robotics and machine learning. Erwin Coumans, Yunfei Bai, </p>
<p>Transformer-XL: Attentive Language Models beyond a Fixed-Length Context. Zihang Dai, Zhilin Yang, Yiming Yang, Jaime G Carbonell, Quoc Le, Ruslan Salakhutdinov, 2019Association for Computational Linguistics</p>
<p>Creating Multimodal Interactive Agents with Imitation and Self-Supervised Learning. Deepmind Interactive, Agents Team, Josh Abramson, Arun Ahuja, Arthur Brussee, Federico Carnevale, Mary Cassin, Felix Fischer, Petko Georgiev, Alex Goldin, Mansi Gupta, arXiv:2112.037632021arXiv preprint</p>
<p>ProcTHOR: Large-Scale Embodied AI Using Procedural Generation. Matt Deitke, Eli Vanderbilt, Alvaro Herrasti, Luca Weihs, Kiana Ehsani, Jordi Salvador, Winson Han, Eric Kolve, Aniruddha Kembhavi, Roozbeh Mottaghi, Advances in Neural Information Processing Systems. 2022</p>
<p>CARLA: An Open Urban Driving Simulator. Alexey Dosovitskiy, German Ros, Felipe Codevilla, Antonio Lopez, Vladlen Koltun, Conference on Robot Learning. 2017</p>
<p>Danny Driess, Fei Xia, S M Mehdi, Corey Sajjadi, Aakanksha Lynch, Brian Chowdhery, Ayzaan Ichter, Jonathan Wahid, Quan Tompson, Tianhe Vuong, Yu, arXiv:2303.03378PaLM-E: An Embodied Multimodal Language Model. 2023arXiv preprint</p>
<p>Zane Durante, Bidipta Sarkar, Ran Gong, Rohan Taori, Yusuke Noda, Paul Tang, Ehsan Adeli, Kowshika Shrinidhi, Kevin Lakshmikanth, Arnold Schulman, Milstein, arXiv:2402.05929An Interactive Agent Foundation Model. 2024arXiv preprint</p>
<p>IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures. Lasse Espeholt, Hubert Soyer, Remi Munos, Karen Simonyan, Vlad Mnih, Tom Ward, Yotam Doron, Vlad Firoiu, Tim Harley, Iain Dunning, International Conference on Machine Learning. 2018</p>
<p>Building Open-Ended Embodied Agents with Internet-Scale Knowledge. Linxi Fan, Guanzhi Wang, Yunfan Jiang, Ajay Mandlekar, Yuncong Yang, Haoyi Zhu, Andrew Tang, De-An, Yuke Huang, Anima Zhu, Anandkumar, Minedojo, Advances in Neural Information Processing Systems. 2022</p>
<p>Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, arXiv:2312.11805A Family of Highly Capable Multimodal Models. 2023arXiv preprint</p>
<p>Shaping Belief States with Generative Environment Models for RL. Karol Gregor, Danilo Jimenez Rezende, Frederic Besse, Yan Wu, Hamza Merzic, Aaron Van Den Oord, Advances in Neural Information Processing Systems. 2019</p>
<p>Making Efficient Use of Demonstrations to Solve Hard Exploration Problems. Caglar Gulcehre, Tom Le Paine, Bobak Shahriari, Misha Denil, Matt Hoffman, Hubert Soyer, Richard Tanburn, Steven Kapturowski, Neil Rabinowitz, Duncan Williams, International Conference on Learning Representations. 2019</p>
<p>MineRL: A Large-Scale Dataset of Minecraft Demonstrations. Brandon William H Guss, Nicholay Houghton, Phillip Topin, Cayden Wang, Manuela Codel, Ruslan Veloso, Salakhutdinov, International Joint Conference on Artificial Intelligence. 2019</p>
<p>Recurrent World Models Facilitate Policy Evolution. David Ha, Jürgen Schmidhuber, Advances in Neural Information Processing Systems. 2018</p>
<p>Mastering Atari with Discrete World Models. Danijar Hafner, Timothy P Lillicrap, Mohammad Norouzi, Jimmy Ba, International Conference on Learning Representations. 2020</p>
<p>Danijar Hafner, Jurgis Pasukonis, Jimmy Ba, Timothy Lillicrap, arXiv:2301.04104Mastering Diverse Domains through World Models. 2023arXiv preprint</p>
<p>The Symbol Grounding Problem. Stevan Harnad, Physica D: Nonlinear Phenomena. 421-31990</p>
<p>Karl Moritz Hermann, Felix Hill, Simon Green, Fumin Wang, Ryan Faulkner, Hubert Soyer, David Szepesvari, Wojciech Marian Czarnecki, Max Jaderberg, Denis Teplyashin, arXiv:1706.06551Grounded Language Learning in a Simulated 3D World. 2017arXiv preprint</p>
<p>Environmental drivers of systematicity and generalization in a situated agent. Felix Hill, Andrew Lampinen, Rosalia Schneider, Stephen Clark, Matthew Botvinick, James L Mcclelland, Adam Santoro, International Conference on Learning Representations. 2019</p>
<p>Grounded Language Learning Fast and Slow. Felix Hill, Olivier Tieleman, Nathaniel Tamara Von Glehn, Hamza Wong, Stephen Merzic, Clark, International Conference on Learning Representations. 2020</p>
<p>. Jonathan Ho, Tim Salimans, arXiv:2207.125982022Classifier-Free Diffusion Guidance. arXiv preprint</p>
<p>Sim2Real in Robotics and Automation: Applications and Challenges. Sebastian Höfer, Kostas Bekris, Ankur Handa, Juan Camilo Gamboa, Melissa Mozifian, Florian Golemo, Chris Atkeson, Dieter Fox, Ken Goldberg, John Leonard, IEEE Transactions on Automation Science and Engineering. 1822021</p>
<p>Training Compute-Optimal Large Language Models. Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego De Las, Lisa Anne Casas, Johannes Hendricks, Aidan Welbl, Clark, arXiv:2203.155562022arXiv preprint</p>
<p>Shengran Hu, Jeff Clune, arXiv:2306.00323Thought Cloning: Learning to Think while Acting by Imitating Human Thinking. 2023arXiv preprint</p>
<p>Look Before You Leap: Unveiling the Power of GPT-4V in Robotic Vision-Language Planning. Yingdong Hu, Fanqi Lin, Tong Zhang, Li Yi, Yang Gao, arXiv:2311.178422023arXiv preprint</p>
<p>. Jiangyong Huang, Silong Yong, Xiaojian Ma, Xiongkun Linghu, Puhao Li, Yan Wang, Qing Li, Song-Chun Zhu, Baoxiong Jia, Siyuan Huang, arXiv:2311.12871An Embodied Generalist Agent in 3D World. 2023arXiv preprint</p>
<p>Language Models as Zero-Shot Planners: Extracting Actionable Knowledge for Embodied Agents. Wenlong Huang, Pieter Abbeel, Deepak Pathak, Igor Mordatch, International Conference on Machine Learning. 2022</p>
<p>A data-driven approach for learning to control computers. David Peter C Humphreys, Tobias Raposo, Gregory Pohlen, Rachita Thornton, Alistair Chhaparia, Josh Muldal, Petko Abramson, Adam Georgiev, Timothy Santoro, Lillicrap, International Conference on Machine Learning. 2022</p>
<p>Language as an Abstraction for Hierarchical Deep Reinforcement Learning. Yiding Jiang, Shixiang Shane Gu, Kevin P Murphy, Chelsea Finn, Advances in Neural Information Processing Systems. 2019</p>
<p>The Malmo Platform for Artificial Intelligence Experimentation. Matthew Johnson, Katja Hofmann, Tim Hutton, David Bignell, International Joint Conference on Artificial Intelligence. 2016</p>
<p>Language Models can Solve Computer Tasks. Geunwoo Kim, Pierre Baldi, Stephen Mcaleer, Advances in Neural Information Processing Systems. 2023</p>
<p>Jing Yu Koh, Robert Lo, Lawrence Jang, Vikram Duvvur, Ming Chong Lim, Po-Yu Huang, Graham Neubig, Shuyan Zhou, Ruslan Salakhutdinov, Daniel Fried, arXiv:2401.13649VisualWebArena: Evaluating Multimodal Agents on Realistic Visual Web Tasks. 2024arXiv preprint</p>
<p>Eric Kolve, Roozbeh Mottaghi, Winson Han, Eli Vanderbilt, Luca Weihs, Alvaro Herrasti, Matt Deitke, Kiana Ehsani, Daniel Gordon, arXiv:1712.05474Yuke Zhu, et al. AI2-THOR: An Interactive 3D Environment for Visual AI. 2017arXiv preprint</p>
<p>Using Natural Language and Program Abstractions to Instill Human Inductive Biases in Machines. Sreejan Kumar, Carlos G Correa, Ishita Dasgupta, Raja Marjieh, Y Michael, Robert Hu, Jonathan D Hawkins, Karthik Cohen, Tom Narasimhan, Griffiths, Advances in Neural Information Processing Systems. 2022</p>
<p>Tell me why! Explanations support learning relational and causal structure. Nicholas Andrew K Lampinen, Ishita Roy, Dasgupta, C Y Stephanie, Allison Chan, James Tam, Chen Mcclelland, Adam Yan, Neil C Santoro, Jane Rabinowitz, Wang, International Conference on Machine Learning. 2022</p>
<p>Competition-Level Code Generation with AlphaCode. Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Rémi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, Science. 37866242022</p>
<p>STEVE-1: A Generative Model for Text-to-Behavior in Minecraft. Shalev Lifshitz, Keiran Paster, Harris Chan, Jimmy Ba, Sheila Mcilraith, arXiv:2306.009372023arXiv preprint</p>
<p>Isaac Gym: High Performance GPU Based Physics Simulation For Robot Learning. Viktor Makoviychuk, Lukasz Wawrzyniak, Yunrong Guo, Michelle Lu, Kier Storey, Miles Macklin, David Hoeller, Nikita Rudin, Arthur Allshire, Ankur Handa, Advances in Neural Information Processing Systems. 2021</p>
<p>Placing language in an integrated understanding system: Next steps toward human-level performance in neural language models. Felix James L Mcclelland, Maja Hill, Jason Rudolph, Hinrich Baldridge, Schütze, Proceedings of the National Academy of Sciences. 117422020</p>
<p>Human-level control through deep reinforcement learning. Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, Nature. 51875402015</p>
<p>Mind Children: The Future of Robot and Human Intelligence. Hans Moravec, 1988Harvard University Press</p>
<p>Improving Intrinsic Exploration with Language Abstractions. Jesse Mu, Victor Zhong, Roberta Raileanu, Minqi Jiang, Noah Goodman, Tim Rocktäschel, Edward Grefenstette, Advances in Neural Information Processing Systems. 2022</p>
<p>Do embodied agents dream of pixelated sheep?: Embodied decision making using language guided world modelling. Kolby Nottingham, Prithviraj Ammanabrolu, Alane Suhr, Yejin Choi, Hannaneh Hajishirzi, Sameer Singh, Roy Fox, arXiv:2301.120502023arXiv preprint</p>
<p>Open-Ended Learning Leads to Generally Capable Agents. Adam Team, Anuj Stooke, Catarina Mahajan, Charlie Barros, Jakob Deck, Jakub Bauer, Maja Sygnowski, Max Trebacz, Michael Jaderberg, Mathieu, arXiv:2107.12808Open Ended Learning. 2021arXiv preprint</p>
<p>Abhishek Padalkar, Acorn Pooley, Ajinkya Jain, Alex Bewley, Alex Herzog, Alex Irpan, Alexander Khazatsky, Anant Rai, Anikait Singh, Anthony Brohan, arXiv:2310.08864Open X-Embodiment: Robotic Learning Datasets and RT-X Models. 2023arXiv preprint</p>
<p>Counter-Strike Deathmatch with Large-Scale Behavioural Cloning. Tim Pearce, Jun Zhu, IEEE Conference on Games. 2022</p>
<p>VirtualHome: Simulating Household Activities via Programs. Xavier Puig, Kevin Ra, Marko Boben, Jiaman Li, Tingwu Wang, Sanja Fidler, Antonio Torralba, Computer Vision and Pattern Recognition. 2018</p>
<p>Xavier Puig, Eric Undersander, Andrew Szot, Mikael Dallaire Cote, Tsung-Yen Yang, Ruslan Partsey, Ruta Desai, Alexander William Clegg, Michal Hlavac, So Yeon Min, Vladimír Vondruš, Theophile Gervet, Vincent-Pierre Berges, John M Turner, Oleksandr Maksymets, Zsolt Kira, arXiv:2310.13724Jitendra Malik, Devendra Singh Chaplot, Unnat Jain, Dhruv Batra, Akshara Rai, and Roozbeh Mottaghi. Habitat 3.0: A Co-Habitat for Humans, Avatars and Robots. 2023arXiv preprint</p>
<p>. Scott Reed, Konrad Zolna, Emilio Parisotto, Sergio Gómez Colmenarejo, Alexander Novikov, Gabriel Barth-Maron, Mai Giménez, Yury Sulsky, Jackie Kay, Jost Tobias Springenberg, A Generalist Agent. Transactions on Machine Learning Research. 2022</p>
<p>Habitat: A Platform for Embodied AI Research. Guillaume Sanchez, Honglu Fan, Alexander Spangher, Elad Levi, Pawan Sasanka Ammanamanchi, Stella Biderman, ; Manolis Savva, Abhishek Kadian, Oleksandr Maksymets, Yili Zhao, Erik Wijmans, Bhavana Jain, Julian Straub, Jia Liu, Vladlen Koltun, Jitendra Malik, arXiv:2306.17806Stay on topic with Classifier-Free Guidance. 2023. 2019arXiv preprintInternational Conference on Computer Vision</p>
<p>ALFRED: A Benchmark for Interpreting Grounded Instructions for Everyday Tasks. Mohit Shridhar, Jesse Thomason, Daniel Gordon, Yonatan Bisk, Winson Han, Roozbeh Mottaghi, Luke Zettlemoyer, Dieter Fox, Computer Vision and Pattern Recognition. 2020</p>
<p>Mastering the game of Go with deep neural networks and tree search. David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den, Julian Driessche, Ioannis Schrittwieser, Veda Antonoglou, Marc Panneershelvam, Lanctot, Nature. 52975874842016</p>
<p>A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play. David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez, Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore Graepel, Science. 36264192018</p>
<p>Sanjana Srivastava, Chengshu Li, Michael Lingelbach, Roberto Martín-Martín, Fei Xia, Kent Vainio, Zheng Lian, Cem Gokmen, Shyamal Buch, Karen Liu, Silvio Savarese, Hyowon Gweon, Jiajun Wu, Li Fei-Fei, BEHAVIOR: Benchmark for Everyday Household Activities in Virtual, Interactive, and Ecological Environments. 2021Conference in Robot Learning</p>
<p>Open-World Object Manipulation using Pre-trained Vision-Language Models. Austin Stone, Ted Xiao, Yao Lu, Keerthana Gopalakrishnan, Kuang-Huei Lee, Quan Vuong, Paul Wohlhart, Brianna Zitkovich, Fei Xia, Chelsea Finn, arXiv:2303.009052023arXiv preprint</p>
<p>Habitat 2.0: Training Home Assistants to Rearrange their Habitat. Andrew Szot, Alexander Clegg, Eric Undersander, Erik Wijmans, Yili Zhao, John Turner, Noah Maestre, Mustafa Mukadam, Devendra Singh Chaplot, Oleksandr Maksymets, Advances in Neural Information Processing Systems. 2021</p>
<p>Semantic Exploration from Language Abstractions and Pretrained Representations. Allison Tam, Neil Rabinowitz, Andrew Lampinen, Nicholas A Roy, Stephanie Chan, Jane Strouse, Andrea Wang, Felix Banino, Hill, Advances in Neural Information Processing Systems. 2022</p>
<p>Towards general computer control: A multimodal agent for red dead redemption ii as a case study. Weihao Tan, Ziluo Ding, Wentao Zhang, Boyu Li, Bohan Zhou, Junpeng Yue, Haochong Xia, Jiechuan Jiang, Longtao Zheng, Xinrun Xu, arXiv:2403.031862024arXiv preprint</p>
<p>Temporal Difference Learning and TD-Gammon. Gerald Tesauro, Communications of the ACM. 3831995</p>
<p>A Deep Hierarchical Approach to Lifelong Learning in Minecraft. Chen Tessler, Shahar Givony, Tom Zahavy, Daniel Mankowitz, Shie Mannor, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence2017</p>
<p>MuJoCo: A physics engine for model-based control. Emanuel Todorov, Tom Erez, Yuval Tassa, IEEE International Conference on Intelligent Robots and Systems. 2012</p>
<p>Sai Vemprala, Rogerio Bonatti, Arthur Bucker, Ashish Kapoor, arXiv:2306.17582ChatGPT for Robotics: Design Principles and Model Abilities. 2023arXiv preprint</p>
<p>Phenaki: Variable Length Video Generation from Open Domain Textual Descriptions. Ruben Villegas, Mohammad Babaeizadeh, Pieter-Jan Kindermans, Hernan Moraldo, Han Zhang, Mohammad Taghi Saffar, Santiago Castro, Julius Kunze, Dumitru Erhan, International Conference on Learning Representations. 2022</p>
<p>Grandmaster level in StarCraft II using multi-agent reinforcement learning. Oriol Vinyals, Igor Babuschkin, Wojciech M Czarnecki, Michaël Mathieu, Andrew Dudzik, Junyoung Chung, David H Choi, Richard Powell, Timo Ewalds, Petko Georgiev, Nature. 57577822019</p>
<p>Voyager: An Open-Ended Embodied Agent with Large Language Models. Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, Anima Anandkumar, arXiv:2305.162912023aarXiv preprint</p>
<p>Zihao Wang, Shaofei Cai, Anji Liu, Yonggang Jin, Jinbing Hou, Bowei Zhang, Haowei Lin, Zhaofeng He, Zilong Zheng, Yaodong Yang, arXiv:2311.05997JARVIS-1: Open-World Multi-task Agents with Memory-Augmented Multimodal Language Models. 2023barXiv preprint</p>
<p>. Tom Ward, Andrew Bolt, Nik Hemmings, Simon Carter, Manuel Sanchez, Ricardo Barreira, Seb Noury, Keith Anderson, Jay Lemmon, Jonathan Coe, Piotr Trochim, Tom Handley, Adrian Bolton, arXiv:2011.092942020Using Unity to Help Solve Intelligence. arXiv preprint</p>
<p>. Mengjiao Yang, Yilun Du, Kamyar Ghasemipour, Jonathan Tompson, Dale Schuurmans, Pieter Abbeel, arXiv:2310.061142023Learning Interactive Real-World Simulators. arXiv preprint</p>
<p>Meta-World: A Benchmark and Evaluation for Multi-Task and Meta Reinforcement Learning. Tianhe Yu, Deirdre Quillen, Zhanpeng He, Ryan Julian, Karol Hausman, Chelsea Finn, Sergey Levine, Conference on Robot Learning. 2020</p>
<p>Transporter Networks: Rearranging the Visual World for Robotic Manipulation. Andy Zeng, Pete Florence, Jonathan Tompson, Stefan Welker, Jonathan Chien, Maria Attarian, Travis Armstrong, Ivan Krasin, Dan Duong, Vikas Sindhwani, Conference on Robot Learning. 2021</p>
<p>Socratic Models: Composing Zero-Shot Multimodal Reasoning with Language. Andy Zeng, Maria Attarian, Marcin Krzysztof, Adrian Choromanski, Stefan Wong, Federico Welker, Aveek Tombari, Purohit, Vikas Michael S Ryoo, Johnny Sindhwani, Lee, International Conference on Learning Representations. 2022</p>
<p>Konrad Zolna, Serkan Cabi, Yutian Chen, Eric Lau, Claudio Fantacci, Jurgis Pasukonis, Jost Tobias Springenberg, Sergio Gomez Colmenarejo, arXiv:2401.08525GATS: Gather-Attend-Scatter. 2024arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
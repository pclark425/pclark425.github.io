<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9652 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9652</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9652</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-168.html">extraction-schema-168</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <p><strong>Paper ID:</strong> paper-273098798</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2410.02054v1.pdf" target="_blank">Comparing Criteria Development Across Domain Experts, Lay Users, and Models in Large Language Model Evaluation</a></p>
                <p><strong>Paper Abstract:</strong> Large Language Models (LLMs) are increasingly utilized for domain-specific tasks, yet integrating domain expertise into evaluating their outputs remains challenging. A common approach to evaluating LLMs is to use metrics, or criteria, which are assertions used to assess performance that help ensure that their outputs align with domain-specific standards. Previous efforts have involved developers, lay users, or the LLMs themselves in creating these criteria, however, evaluation particularly from a domain expertise perspective, remains understudied. This study explores how domain experts contribute to LLM evaluation by comparing their criteria with those generated by LLMs and lay users. We further investigate how the criteria-setting process evolves, analyzing changes between a priori and a posteriori stages. Our findings emphasize the importance of involving domain experts early in the evaluation process while utilizing complementary strengths of lay users and LLMs. We suggest implications for designing workflows that leverage these strengths at different evaluation stages.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9652.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9652.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Staged Criteria-Based Evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>A priori / A posteriori Staged Criteria-Based Evaluation Workflow</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A two-stage human+LLM evaluation method where evaluators first create binary/assertion-style criteria from the prompt alone (a priori) and then refine or add criteria after inspecting model outputs (a posteriori); criteria are applied as tags to outputs and consolidated via thematic coding.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td>GPT-4o; Claude 3.5 Sonnet; Gemini (outputs used in a posteriori stage)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td>Three large conversational models used to generate task outputs for human and LLM-based evaluators; GPT-4o was also used to generate/refine criteria in-parallel to human participants (criteria generation carried out July 2024).</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Applied evaluation methodology across Nutrition/Dietetics and Pedagogy (education/tutoring).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Human interviews where domain experts and lay users (N=5 per domain for experts) create initial criteria from the prompt (a priori), then inspect three LLM-generated outputs and tag parts of outputs with criteria in MAXQDA and add/refine criteria (a posteriori); an LLM was run through the same two-step criteria generation process for comparison; results consolidated via open-coding thematic analysis by three researchers.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Binary/assertion-style criteria grouped into themes including Accuracy (correctness of facts/solutions), Clarity (clear explanations, math vocabulary), Guidance (follow-up questions, hints, recommendations), Formatting/Presentation (bullets, headings, diagrams), Safety/Health Considerations (avoid harmful recommendations), Personalization (grade-level appropriateness), Decision Support (references to evidence-based guidance).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>No standardized benchmark; evaluation used three domain-specific prompt scenarios per domain and three LLM outputs per scenario (GPT-4o, Claude 3.5 Sonnet, Gemini) produced for the study.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Domain experts produced more numerous and much more specific, knowledge-based criteria at the a priori stage; all groups introduced additional criteria in the a posteriori stage (criteria drift), with experts especially adding formatting/readability and decision-support criteria; lay users emphasized formatting and accessibility; the LLM produced general, prompt-keyword-driven criteria and prioritized following prompt instructions and personalization, but lacked domain detail; experts refined and expanded criteria more after seeing outputs than lay users did.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Scope limited to two domains (dietetics and pedagogy) with small expert samples; no standard benchmark dataset; potential replicability issues because LLMs evolve over time; criteria drift may reflect overfitting to specific outputs rather than genuine shifts in evaluation standards.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>Compared to typical human/traditional evaluations, domain experts produced checklist-like, domain-grounded assertions (more rigorous than lay users or LLMs); lay users brought usability-focused criteria often absent from technical evaluations; LLM-generated criteria were more generalized and prompt-centric, suggesting LLMs cannot substitute for expert judgments on domain-specific theory accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Use a staged workflow: involve domain experts in the a priori stage to set knowledge-grounded criteria, use LLMs for lower-level or repetitive checks and to refine criteria a posteriori, and include lay users for formatting/usability criteria; support users with tooling to translate impressions into explicit binary criteria; use expert criteria as benchmarks for fine-tuning and for constructing reward models.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Comparing Criteria Development Across Domain Experts, Lay Users, and Models in Large Language Model Evaluation', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9652.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9652.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-as-Criterion-Generator</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-based Criteria Generation and Refinement (model-as-judge approach)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Using an LLM to autonomously generate initial binary evaluation criteria from a prompt and then refine or add criteria after seeing model outputs, mirroring the human two-step process to scale criterion creation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td>GPT-4o (used to generate/refine criteria); other evaluated output LLMs: Claude 3.5 Sonnet, Gemini.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td>GPT-4o configured with system prompts to emit JSON-formatted binary criteria (true/false statements) first based on instruction-only, then to refine criteria given the initial criteria plus three model outputs. No external tagging tool used; outputs generated autonomously within the study.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Methodological; applied to Nutrition/Dietetics and Pedagogy scenarios in this study.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Automated generation of assertion-style criteria from the prompt (a priori) and automated refinement after ingesting the same three outputs shown to human participants (a posteriori); criteria formatted as JSON per instructions; comparison made between LLM-generated criteria and human-generated criteria.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Generalized, prompt-aligned binary assertions (e.g., 'identifies foods to avoid' or 'avoids giving the final answer') focusing on surface-level compliance with prompt instructions and personalization; less frequent domain-specific thresholds or numerical cutoffs.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Same three domain prompt scenarios and three LLM outputs per scenario used for human participants; criteria generation performed on these artifacts (no external benchmark).</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>LLM criteria tended to be higher-level and keyword-driven, aligning with prompt objectives but lacking the fine-grained, evidence-based specificity produced by domain experts; the LLM sometimes added criteria reflecting content it observed in outputs (e.g., hydration), indicating sensitivity to output content and risk of overfitting to output idiosyncrasies.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>LLM criteria are generalized and may miss domain nuance; models may produce criteria that overfit to the specific outputs they inspect (a posteriori), leading to unreliable general evaluation standards; LLMs can reflect outdated or contaminated training data and may not safely represent domain-sensitive details (e.g., health guidance).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>LLM-generated criteria can scale criterion creation and surface obvious prompt-aligned checks, but they fall short of expert-level specificity and domain-grounded thresholds; LLMs are complementary rather than substitutive of domain experts.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Use LLMs to handle lower-level or repetitive criterion tasks (format checks, prompt-compliance), constrain their outputs with expert-curated templates, and always have domain experts vet/refine LLM-suggested criteria before adopting them as benchmarks; prefer using LLMs for a posteriori refinement only if experts cannot be engaged early.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Comparing Criteria Development Across Domain Experts, Lay Users, and Models in Large Language Model Evaluation', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9652.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9652.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Human Expert vs Lay User Protocol</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Human Criteria Development and Tagging Protocol (MAXQDA-assisted interviews and open coding)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A human-centered data collection and analysis pipeline where domain experts and lay users create initial binary criteria from prompts, tag LLM outputs using MAXQDA during a posteriori review, and then researchers consolidate criteria via open-coding into themes and sub-themes.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Human-centered evaluation methodology applied in Nutrition/Dietetics and Pedagogy.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Semi-structured interviews (1.5 hours) with counterbalanced scenarios and randomized outputs; participants entered initial criteria into MAXQDA, tagged parts of outputs with these criteria, and added/refined criteria after seeing outputs; three researchers performed independent open-coding and grouped criteria into high-level themes.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Participant-created binary/assertion criteria across themes: Accuracy, Clarity, Guidance, Formatting, Decision Support, Safety, Personalization, Follow-up questioning; expert criteria often included quantitative thresholds (e.g., nutrient DV percentages) and specific domain checks.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Three prompt scenarios per domain; three LLM outputs per scenario (GPT-4o, Claude 3.5 Sonnet, Gemini).</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Experts produced more criteria overall and more domain-specific, quantitative criteria (e.g., nutrient thresholds, explicit foods to avoid) primarily at the a priori stage; lay users focused more on formatting, readability, and explanation style; both groups expanded criteria after seeing outputs but experts expanded more, especially adding formatting and decision-support items.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Small, domain-limited participant pools; experts may be influenced by outputs (criteria drift) and may find it difficult to convert subjective impressions into explicit binary criteria during a posteriori review; human coding requires time and expert labor.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>This approach operationalizes human expert review into explicit binary assertions suitable for automated evaluation pipelines, improving over ad-hoc human review but still more costly than automated metrics; contrasts with traditional single-metric evaluations (BLEU/ROUGE) by producing domain-grounded, multi-criteria assessments.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Provide tooling (e.g., guided templates, examples) to help humans convert impressions into formal criteria; involve experts early to establish domain standards; leverage MAXQDA-style tagging for traceability of which output segments satisfy which criteria; use thematic consolidation by multiple coders to improve reliability.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Comparing Criteria Development Across Domain Experts, Lay Users, and Models in Large Language Model Evaluation', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9652.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e9652.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Evaluation Tools & Assertions</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Referenced Evaluation Tools and Assertion Synthesis (EvalLM, EvalGen, SPade, ChainForge, Promptfoo)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Set of prior systems and algorithmic approaches cited as means to support user-defined criteria creation, assertion synthesis from prompt histories, and LLM-as-judge evaluation; cited as related work and potential building blocks for staged evaluation workflows.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Tooling for LLM evaluation and prompt engineering (general NLG evaluation).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Mentioned methods include interactive creation of custom, natural-language criteria (EvalLM), assertion synthesis from prompt revision histories (EvalGen, SPADE), LLM-as-judge frameworks (ChainForge, Promptfoo), and visual analytics for side-by-side model comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>These tools support binary/assertion-style checks, ordinal ranking scales, prompt-compliance tests, bias-detection metrics, and synthesized assertions that can be applied automatically across outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Not applied as concrete benchmarks in this paper; presented as related systems that can be integrated with the staged workflow. SPade/Spade-style assertion syntheses were mentioned as approaches to automatically create data-quality assertions.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Paper positions these tools as complementary: interactive tools help non-experts craft criteria; SPade-like systems can reduce repetitive expert labor by synthesizing assertions early; LLM-as-judge tools can operationalize criteria but need expert oversight due to generalization and overfitting risks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Existing tools are often developer-focused and may lack domain-specific rigor; automated assertion synthesis can produce surface-level checks that miss domain nuance; reliance on LLM-as-judge risks misalignment with expert standards.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>These tools partially automate and scale aspects of human evaluation but cannot replace domain experts for domain-grounded criteria; recommended to be used in combination with expert involvement.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Integrate interactive tools for lay-user criteria, use SPade-like assertion synthesis to pre-populate candidate criteria for experts to vet, and retain expert review for safety-critical or domain-specialized checks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Comparing Criteria Development Across Domain Experts, Lay Users, and Models in Large Language Model Evaluation', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Evallm: Interactive evaluation of large language model prompts on user-defined criteria <em>(Rating: 2)</em></li>
                <li>Spade: Synthesizing assertions for large language model pipelines <em>(Rating: 2)</em></li>
                <li>Who Validates the Validators? Aligning LLM-Assisted Evaluation of LLM Outputs with Human Preferences <em>(Rating: 2)</em></li>
                <li>G-eval: Nlg evaluation using gpt-4 with better human alignment <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9652",
    "paper_id": "paper-273098798",
    "extraction_schema_id": "extraction-schema-168",
    "extracted_data": [
        {
            "name_short": "Staged Criteria-Based Evaluation",
            "name_full": "A priori / A posteriori Staged Criteria-Based Evaluation Workflow",
            "brief_description": "A two-stage human+LLM evaluation method where evaluators first create binary/assertion-style criteria from the prompt alone (a priori) and then refine or add criteria after inspecting model outputs (a posteriori); criteria are applied as tags to outputs and consolidated via thematic coding.",
            "citation_title": "here",
            "mention_or_use": "use",
            "llm_name": "GPT-4o; Claude 3.5 Sonnet; Gemini (outputs used in a posteriori stage)",
            "llm_description": "Three large conversational models used to generate task outputs for human and LLM-based evaluators; GPT-4o was also used to generate/refine criteria in-parallel to human participants (criteria generation carried out July 2024).",
            "scientific_domain": "Applied evaluation methodology across Nutrition/Dietetics and Pedagogy (education/tutoring).",
            "evaluation_method": "Human interviews where domain experts and lay users (N=5 per domain for experts) create initial criteria from the prompt (a priori), then inspect three LLM-generated outputs and tag parts of outputs with criteria in MAXQDA and add/refine criteria (a posteriori); an LLM was run through the same two-step criteria generation process for comparison; results consolidated via open-coding thematic analysis by three researchers.",
            "evaluation_criteria": "Binary/assertion-style criteria grouped into themes including Accuracy (correctness of facts/solutions), Clarity (clear explanations, math vocabulary), Guidance (follow-up questions, hints, recommendations), Formatting/Presentation (bullets, headings, diagrams), Safety/Health Considerations (avoid harmful recommendations), Personalization (grade-level appropriateness), Decision Support (references to evidence-based guidance).",
            "benchmark_or_dataset": "No standardized benchmark; evaluation used three domain-specific prompt scenarios per domain and three LLM outputs per scenario (GPT-4o, Claude 3.5 Sonnet, Gemini) produced for the study.",
            "results_summary": "Domain experts produced more numerous and much more specific, knowledge-based criteria at the a priori stage; all groups introduced additional criteria in the a posteriori stage (criteria drift), with experts especially adding formatting/readability and decision-support criteria; lay users emphasized formatting and accessibility; the LLM produced general, prompt-keyword-driven criteria and prioritized following prompt instructions and personalization, but lacked domain detail; experts refined and expanded criteria more after seeing outputs than lay users did.",
            "limitations_or_challenges": "Scope limited to two domains (dietetics and pedagogy) with small expert samples; no standard benchmark dataset; potential replicability issues because LLMs evolve over time; criteria drift may reflect overfitting to specific outputs rather than genuine shifts in evaluation standards.",
            "comparison_to_human_or_traditional": "Compared to typical human/traditional evaluations, domain experts produced checklist-like, domain-grounded assertions (more rigorous than lay users or LLMs); lay users brought usability-focused criteria often absent from technical evaluations; LLM-generated criteria were more generalized and prompt-centric, suggesting LLMs cannot substitute for expert judgments on domain-specific theory accuracy.",
            "recommendations_or_best_practices": "Use a staged workflow: involve domain experts in the a priori stage to set knowledge-grounded criteria, use LLMs for lower-level or repetitive checks and to refine criteria a posteriori, and include lay users for formatting/usability criteria; support users with tooling to translate impressions into explicit binary criteria; use expert criteria as benchmarks for fine-tuning and for constructing reward models.",
            "uuid": "e9652.0",
            "source_info": {
                "paper_title": "Comparing Criteria Development Across Domain Experts, Lay Users, and Models in Large Language Model Evaluation",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "LLM-as-Criterion-Generator",
            "name_full": "LLM-based Criteria Generation and Refinement (model-as-judge approach)",
            "brief_description": "Using an LLM to autonomously generate initial binary evaluation criteria from a prompt and then refine or add criteria after seeing model outputs, mirroring the human two-step process to scale criterion creation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "llm_name": "GPT-4o (used to generate/refine criteria); other evaluated output LLMs: Claude 3.5 Sonnet, Gemini.",
            "llm_description": "GPT-4o configured with system prompts to emit JSON-formatted binary criteria (true/false statements) first based on instruction-only, then to refine criteria given the initial criteria plus three model outputs. No external tagging tool used; outputs generated autonomously within the study.",
            "scientific_domain": "Methodological; applied to Nutrition/Dietetics and Pedagogy scenarios in this study.",
            "evaluation_method": "Automated generation of assertion-style criteria from the prompt (a priori) and automated refinement after ingesting the same three outputs shown to human participants (a posteriori); criteria formatted as JSON per instructions; comparison made between LLM-generated criteria and human-generated criteria.",
            "evaluation_criteria": "Generalized, prompt-aligned binary assertions (e.g., 'identifies foods to avoid' or 'avoids giving the final answer') focusing on surface-level compliance with prompt instructions and personalization; less frequent domain-specific thresholds or numerical cutoffs.",
            "benchmark_or_dataset": "Same three domain prompt scenarios and three LLM outputs per scenario used for human participants; criteria generation performed on these artifacts (no external benchmark).",
            "results_summary": "LLM criteria tended to be higher-level and keyword-driven, aligning with prompt objectives but lacking the fine-grained, evidence-based specificity produced by domain experts; the LLM sometimes added criteria reflecting content it observed in outputs (e.g., hydration), indicating sensitivity to output content and risk of overfitting to output idiosyncrasies.",
            "limitations_or_challenges": "LLM criteria are generalized and may miss domain nuance; models may produce criteria that overfit to the specific outputs they inspect (a posteriori), leading to unreliable general evaluation standards; LLMs can reflect outdated or contaminated training data and may not safely represent domain-sensitive details (e.g., health guidance).",
            "comparison_to_human_or_traditional": "LLM-generated criteria can scale criterion creation and surface obvious prompt-aligned checks, but they fall short of expert-level specificity and domain-grounded thresholds; LLMs are complementary rather than substitutive of domain experts.",
            "recommendations_or_best_practices": "Use LLMs to handle lower-level or repetitive criterion tasks (format checks, prompt-compliance), constrain their outputs with expert-curated templates, and always have domain experts vet/refine LLM-suggested criteria before adopting them as benchmarks; prefer using LLMs for a posteriori refinement only if experts cannot be engaged early.",
            "uuid": "e9652.1",
            "source_info": {
                "paper_title": "Comparing Criteria Development Across Domain Experts, Lay Users, and Models in Large Language Model Evaluation",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Human Expert vs Lay User Protocol",
            "name_full": "Human Criteria Development and Tagging Protocol (MAXQDA-assisted interviews and open coding)",
            "brief_description": "A human-centered data collection and analysis pipeline where domain experts and lay users create initial binary criteria from prompts, tag LLM outputs using MAXQDA during a posteriori review, and then researchers consolidate criteria via open-coding into themes and sub-themes.",
            "citation_title": "here",
            "mention_or_use": "use",
            "llm_name": null,
            "llm_description": null,
            "scientific_domain": "Human-centered evaluation methodology applied in Nutrition/Dietetics and Pedagogy.",
            "evaluation_method": "Semi-structured interviews (1.5 hours) with counterbalanced scenarios and randomized outputs; participants entered initial criteria into MAXQDA, tagged parts of outputs with these criteria, and added/refined criteria after seeing outputs; three researchers performed independent open-coding and grouped criteria into high-level themes.",
            "evaluation_criteria": "Participant-created binary/assertion criteria across themes: Accuracy, Clarity, Guidance, Formatting, Decision Support, Safety, Personalization, Follow-up questioning; expert criteria often included quantitative thresholds (e.g., nutrient DV percentages) and specific domain checks.",
            "benchmark_or_dataset": "Three prompt scenarios per domain; three LLM outputs per scenario (GPT-4o, Claude 3.5 Sonnet, Gemini).",
            "results_summary": "Experts produced more criteria overall and more domain-specific, quantitative criteria (e.g., nutrient thresholds, explicit foods to avoid) primarily at the a priori stage; lay users focused more on formatting, readability, and explanation style; both groups expanded criteria after seeing outputs but experts expanded more, especially adding formatting and decision-support items.",
            "limitations_or_challenges": "Small, domain-limited participant pools; experts may be influenced by outputs (criteria drift) and may find it difficult to convert subjective impressions into explicit binary criteria during a posteriori review; human coding requires time and expert labor.",
            "comparison_to_human_or_traditional": "This approach operationalizes human expert review into explicit binary assertions suitable for automated evaluation pipelines, improving over ad-hoc human review but still more costly than automated metrics; contrasts with traditional single-metric evaluations (BLEU/ROUGE) by producing domain-grounded, multi-criteria assessments.",
            "recommendations_or_best_practices": "Provide tooling (e.g., guided templates, examples) to help humans convert impressions into formal criteria; involve experts early to establish domain standards; leverage MAXQDA-style tagging for traceability of which output segments satisfy which criteria; use thematic consolidation by multiple coders to improve reliability.",
            "uuid": "e9652.2",
            "source_info": {
                "paper_title": "Comparing Criteria Development Across Domain Experts, Lay Users, and Models in Large Language Model Evaluation",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Evaluation Tools & Assertions",
            "name_full": "Referenced Evaluation Tools and Assertion Synthesis (EvalLM, EvalGen, SPade, ChainForge, Promptfoo)",
            "brief_description": "Set of prior systems and algorithmic approaches cited as means to support user-defined criteria creation, assertion synthesis from prompt histories, and LLM-as-judge evaluation; cited as related work and potential building blocks for staged evaluation workflows.",
            "citation_title": "",
            "mention_or_use": "mention",
            "llm_name": null,
            "llm_description": null,
            "scientific_domain": "Tooling for LLM evaluation and prompt engineering (general NLG evaluation).",
            "evaluation_method": "Mentioned methods include interactive creation of custom, natural-language criteria (EvalLM), assertion synthesis from prompt revision histories (EvalGen, SPADE), LLM-as-judge frameworks (ChainForge, Promptfoo), and visual analytics for side-by-side model comparisons.",
            "evaluation_criteria": "These tools support binary/assertion-style checks, ordinal ranking scales, prompt-compliance tests, bias-detection metrics, and synthesized assertions that can be applied automatically across outputs.",
            "benchmark_or_dataset": "Not applied as concrete benchmarks in this paper; presented as related systems that can be integrated with the staged workflow. SPade/Spade-style assertion syntheses were mentioned as approaches to automatically create data-quality assertions.",
            "results_summary": "Paper positions these tools as complementary: interactive tools help non-experts craft criteria; SPade-like systems can reduce repetitive expert labor by synthesizing assertions early; LLM-as-judge tools can operationalize criteria but need expert oversight due to generalization and overfitting risks.",
            "limitations_or_challenges": "Existing tools are often developer-focused and may lack domain-specific rigor; automated assertion synthesis can produce surface-level checks that miss domain nuance; reliance on LLM-as-judge risks misalignment with expert standards.",
            "comparison_to_human_or_traditional": "These tools partially automate and scale aspects of human evaluation but cannot replace domain experts for domain-grounded criteria; recommended to be used in combination with expert involvement.",
            "recommendations_or_best_practices": "Integrate interactive tools for lay-user criteria, use SPade-like assertion synthesis to pre-populate candidate criteria for experts to vet, and retain expert review for safety-critical or domain-specialized checks.",
            "uuid": "e9652.3",
            "source_info": {
                "paper_title": "Comparing Criteria Development Across Domain Experts, Lay Users, and Models in Large Language Model Evaluation",
                "publication_date_yy_mm": "2024-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Evallm: Interactive evaluation of large language model prompts on user-defined criteria",
            "rating": 2,
            "sanitized_title": "evallm_interactive_evaluation_of_large_language_model_prompts_on_userdefined_criteria"
        },
        {
            "paper_title": "Spade: Synthesizing assertions for large language model pipelines",
            "rating": 2,
            "sanitized_title": "spade_synthesizing_assertions_for_large_language_model_pipelines"
        },
        {
            "paper_title": "Who Validates the Validators? Aligning LLM-Assisted Evaluation of LLM Outputs with Human Preferences",
            "rating": 2,
            "sanitized_title": "who_validates_the_validators_aligning_llmassisted_evaluation_of_llm_outputs_with_human_preferences"
        },
        {
            "paper_title": "G-eval: Nlg evaluation using gpt-4 with better human alignment",
            "rating": 1,
            "sanitized_title": "geval_nlg_evaluation_using_gpt4_with_better_human_alignment"
        }
    ],
    "cost": 0.01362825,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Comparing Criteria Development Across Domain Experts, Lay Users, and Models in Large Language Model Evaluation
2 Oct 2024</p>
<p>Annalisa Szymanski 
Araya Simret 
Gebreegziabher 
Oghenemaro Anuyah oanuyah@nd.edu 
Ronald A Metoyer rmetoyer@nd.edu 
Toby Jia 
Jun Li </p>
<p>University of Notre Dame Notre Dame
INUSA</p>
<p>University of Notre Dame Notre Dame
INUSA</p>
<p>University of Notre Dame Notre Dame
INUSA</p>
<p>University of Notre Dame Notre Dame
INUSA</p>
<p>University of Notre Dame Notre Dame
INUSA</p>
<p>Comparing Criteria Development Across Domain Experts, Lay Users, and Models in Large Language Model Evaluation
2 Oct 2024175702A58806627A3558F885AA607B89arXiv:2410.02054v1[cs.HC]Large Language ModelsEvaluation SystemsCriteria-SettingDomain Expertise
Large Language Models (LLMs) are increasingly utilized for domainspecific tasks, yet integrating domain expertise into evaluating their outputs remains challenging.A common approach to evaluating LLMs is to use metrics, or criteria, which are assertions used to assess performance that help ensure that their outputs align with domain-specific standards.Previous efforts have involved developers, lay users, or the LLMs themselves in creating these criteria, however, evaluation particularly from a domain expertise perspective, remains understudied.This study explores how domain experts contribute to LLM evaluation by comparing their criteria with those generated by LLMs and lay users.We further investigate how the criteria-setting process evolves, analyzing changes between a priori and a posteriori stages.Our findings emphasize the importance of involving domain experts early in the evaluation process while utilizing complementary strengths of lay users and LLMs.We suggest implications for designing workflows that leverage these strengths at different evaluation stages.CCS CONCEPTS• Human-centered computing → Empirical studies in HCI; Empirical studies in HCI.</p>
<p>INTRODUCTION</p>
<p>Large Language Models (LLMs) have been increasingly utilized across various domains, such as healthcare, nutrition, education, and other specialized fields, to perform complex tasks [2,5,6,38,39].Despite their widespread use, significant concerns remain about the accuracy and reliability of LLM outputs in these areas, where errors and hallucinations can have serious implications [3,30].The subjective and context-specific nature of these tasks often requires human review, making the evaluation process more complex [35].However, human integration into the evaluation process presents many challenges, including cost, time constraints, and scalability [16].</p>
<p>To address these challenges, tools have been introduced to support developers in evaluating LLM performance, with a particular focus on prompt engineering and auditing [4,23,41,48].A promising area of exploration is the systematic evaluation of LLMs using criteria-based assertions to assess the quality of model outputs [4,25,41].Tools such as EvalLM [25], which support userdefined criteria evaluation, primarily target those with experience in LLM development.While LLM engineers or those with LLM development may have intuitions on how to evaluate models, they do not have the domain-specific knowledge needed to make reliable evaluation criteria [44].In addition, tools such as EvalGen have been used to support the concept of "criteria drift" [41], where evaluation criteria has been observed to evolve as users move from the prompt (a priori) to the output (a posteriori) in criteria setting [41].In this process, LLMs have been used to support users in generating and refining evaluation criteria based on their assessment of an output [25,41].</p>
<p>While current approaches effectively support the creation of metrics and criteria that align LLMs with the developer, there remains a gap in literature regarding how to best incorporate domain expertise into LLM evaluation processes.This gap arises from the substantial costs, time, and resources required to engage domain experts, as well as the repetitive task of having them reassess outputs to address criteria drift.This lack of domain expertise raises concerns about the depth and precision of current evaluation methods, which may fall short without expert input.We believe that domain experts are necessary in this process because of the specialized knowledge they bring to the developer to craft the precise criteria necessary to ensure that evaluations are both accurate and contextually relevant, particularly in complex domains [18].Domain experts are often needed to address ethical concerns and contribute domain-specific knowledge [22], while lay users offer perspectives from lived experiences, and LLMs apply their own evaluation logic [47].</p>
<p>In our study, we aim to determine if there is an alignment with the types of criteria that are set between domain experts, lay users, and those generated by LLMs to better understand where to involve domain experts in the evaluation workflow.Our research is guided by the following questions.</p>
<p>• (RQ1.)What are the similarities and differences in criteriasetting between domain experts, lay users, and LLMs? • (RQ2.)How do a priori (prompt-based) and a posteriori (output-based) stages of criteria-setting influence the development of evaluation criteria within each group?</p>
<p>• (RQ3.)What are the design implications of involving domain experts to improve the LLM evaluation workflow?</p>
<p>To answer our research questions, we conduct a case study in two specialized fields that require domain expertise to ensure the quality of the output: nutrition/dietetics and pedagogy.Domain experts from each field establish evaluation criteria for LLMs, which we then compare to criteria generated by lay users and LLMs.To analyze how these criteria evolve, participants create evaluation criteria both a priori and a posteriori which allows us to examine shifts in criterion development and determine the best place to incorporate expertise.</p>
<p>Based on our qualitative analysis, our findings show that domain experts set detailed and specific evaluation criteria upon reading the prompt, indicating that they are most effective in the a priori stage of the evaluation workflow.Lay users and LLMs also come up with important criteria that are different from those of domain experts, suggesting critical roles they can play to complement each other.With our analysis, we show that there are strengths for including each of these groups in the evaluation process at different stages.While domain experts bring specific knowledge in their criteria, lay users focus more on usability issues, while LLMs manage lower level tasks to make sure the output meets the task requirements.Considering the challenges of cost and time with including domain experts in the process, we discuss a staged evaluation workflow where we can best leverage complementary strengths from domain experts, lay users, and LLMs to improve the evaluation workflow.</p>
<p>Our research makes the following contributions to the development of evaluation systems in HCI:</p>
<p>• We provide an empirical understanding of the differences in evaluation metrics developed by domain experts, lay users, and LLMs in a priori and a posteriori stages.• We discuss the implications of involving these groups in the evaluation process.• We propose design recommendations for evaluation workflows that leverage the complementary strengths of domain experts, lay users, and LLMs.</p>
<p>MOTIVATION AND BACKGROUND</p>
<p>We begin this section by reviewing existing research on the systems that have been developed for LLM evaluations.Next, we discuss the current approaches to incorporating domain experts and the critical need for their involvement in the evaluation and refinement of LLMs.Through these reviews, we discuss the gaps in the literature with involving domain experts and why we focus on the evaluation process specifically.</p>
<p>Systems for LLM Evaluations</p>
<p>While recent years have brought significant progress with using Natural Language Generation (NLG) tasks, evaluating these tasks remains difficult [14,24].Automated metrics such as BLUE [36] or ROGUE [29] have been proposed to evaluate the ground truth.However, these have limitations due to the detailed and contextdependent nature of domain-specific outputs.This necessitates the establishment of tailored evaluation criteria to assess these models for these tasks effectively, given their differing objectives and requirements.These criteria are measures of assertions used to measure the quality of the LLM by ranking the criteria with true/false statements [41] or ordinal ranking scales [31,51].</p>
<p>To support both programmers and end users in developing and refining these criteria, interactive systems have been proposed that facilitate the generation and testing of evaluation criteria.These systems are primarily designed to identify bias in generated responses [7], iterate and optimize prompts [25], and select models [23] that best align with user objectives.Despite the variability in individual goals of the systems, the overarching goal remains consistent: to enhance the alignment between user preferences and the generated LLM responses [41].</p>
<p>To help align user intentions and preferences with those of an LLM-as-a-judge model, ChainForge [4] or Promptfoo [49] enables users to create their own evaluation criteria for assessing the quality of different model responses.Similarly, Desmond et al. [11] present a tool that allows users to generate evaluation outputs by providing a prompt, selecting multiple models, and defining LLM-as-a-Judge criteria with custom metrics using natural language.EvalLM also facilitates criteria-driven evaluation to aid in refining prompts [25].Additionally, EvalGen was developed to help users generate assertions based on prompt revision history using an algorithmic approach [41].These tools emphasize the importance of user involvement in refining evaluation criteria to better capture the depth of their domain-specific tasks.However, they have primarily been developed for use by a developer or designer, who may not contain the domain-specific expertise or a lay user's perspective.</p>
<p>Current State of Incorporating Domain Expertise</p>
<p>There have been ways to incorporate expertise into the development and refinement of AI tools, particularly in areas such as training data curation [20], fine-tuning models [21,50], and enhancing data collection processes [27].Human domain experts have been utilized in reinforcement learning for LLMs primarily to provide feedback and guide the training process so that the models may align more closely with expert-level understanding and decision-making [34].This is typically done through Reinforcement Learning from Human Feedback (RLHF) to improve the behavior and performance of LLMs [34,52].For example, LLM outputs have been shown to improve in alignment with desired behaviors through the use of human domain expert feedback [34].Similarly, human preferences have been used to compare different outputs generated by LLMs, and these preferences have then been used to train a reward model [9].However, there are limitations to refinement tools.Even with the use of refined or curated data, it remains questionable how LLMs will perform due to the massive diversity of their datasets, especially when encountering situations that are not well-represented in the training data [42].Rather than refining LLMs for general language tasks, summarization, or reinforcement learning, other work has used detailed validation processes by domain experts to lead to the creation of design guidelines for prompting LLMs in specialized fields [43].Involving human experts this way in the validation of LLMs has been shown to result in improved output accuracy, reduced biases, guided model refinement, and increased development of specialized tools [43,52].This direct involvement of experts is promising in that it results in LLMs that are more reliable and effective in real-world applications [43].In this study, we consider the evaluation process to be the most critical phase, as it allows developers and industry professionals a way to test their models, gather feedback on the strengths and weaknesses of specific prompt outputs, and potentially improve model performance.</p>
<p>The Need of Domain Expertise</p>
<p>Domain experts are often not included in development processes because of the costliness and time intensiveness [16], which explains why LLMs or developers are used to validate the models.There has been considerable discussion regarding the capabilities of LLMs across various domains such as finance, healthcare, and cybersecurity, that also address concerns related to trustworthiness, fairness, transparency, and ethical implications, and suggest the need for domain expert involvement [39].Others have collected expert opinions on use cases, benefits, risks, and privacy concerns related to LLMs in healthcare, and strongly implicate the importance of expert opinions in understanding the potential impact of LLMs [10].</p>
<p>There has been emphasis on the value of using domain-specific professional knowledge to guide responsible LLM behavior, suggesting that LLMs focus on aiding users to identify the right questions and relevant information rather than providing definitive answers [8].In Cheong et al., experts raised concerns about the ability of LLMs to provide context-aware, adaptive guidance that is tailored to the specific situations and environments of the user and also pointed out that the data used to train LLMs may be outdated or too generalized to address the specific details of individual cases [8].Others have also concluded that to create a high-quality LLM solution, the involvement of domain experts is crucial, particularly in providing annotated data to calibrate metrics, evaluating the quality of LLM outputs, especially in technical fields, and defining key qualities for custom metric design [45].</p>
<p>Inconsistencies in automated and LLM-based evaluations of chatbot performances have revealed and emphasized the need for human evaluation in critical areas [1].The study showed that while human evaluation is considered the gold standard, it is complex, requiring the use of factored human evaluation with multiple criteria ratings, and may depend on the level of expertise of the evaluators [1].</p>
<p>While LLMs can assist in the evaluation process by identifying areas of low confidence or suggesting criteria modifications, having humans oversee the process remains essential, particularly for subjective and complex tasks [35].Participants in the study, such as data scientists, software engineers, and AI engineers, demonstrated the need for detailed feedback on individual evaluation dimensions, rather than relying solely on high-level metrics [35].The study recommends a system where LLMs handle repetitive tasks or initial assessments, but human reviewers refine and validate criteria and outputs to ensure a more accurate and reliable evaluation process [35].However, since this study focused on development contexts, it is crucial to further test and evaluate the involvement of domain experts in the setting of evaluation criteria.This will help determine whether their input leads to more precise, contextually relevant, and reliable assessments, particularly in complex or specialized domains where LLMs and automated systems may not suffice.</p>
<p>METHODS</p>
<p>We conducted interviews with both domain experts and lay users.Each group developed and refined evaluation criteria for each of three specific tasks within their respective domains or, in the case of lay users, within the domain where they had sought expert assistance.Simultaneously, we generated criteria using an LLM for each of the same three domain-specific tasks for direct comparison.Because of the "criteria drift" phenomenon, where human evaluation criteria evolve during the process of assessing LLM outputs [41], we structured the criteria-setting process in two stages.We were interested to see how the criteria setting would evolve between first reviewing the prompt only and then after evaluation of the output.</p>
<p>The following sections provide detailed information on participant recruitment, the prompt scenarios used, the study protocol, and data analysis.</p>
<p>Participant Recruitment</p>
<p>Our study received approval from our institution's Review Board (IRB) prior to participant recruitment.Below, we outline our recruitment efforts for both domain experts and lay users.Both groups were asked to sign a consent form and fill out a pre-interview survey prior to the study.In Table 3 of the Appendix, we outline our participant information.</p>
<p>3.1.1Domain Experts.Two domains were used; nutrition and pedagogy.The domains were chosen because they are both knowledgeintensive, making them well-suited for the level of decision-making needed to develop criteria and evaluate generated outputs.For the nutrition domain, we recruited five registered dietitians (RDs) with specialized training and experience in clinical practice.For the pedagogy domain, we recruited five licensed mathematics educators who have taught middle and high school mathematics and hold certifications in specific teaching competencies.</p>
<p>Participants were recruited through email communication with local practices, schools, and university networks.We then distributed informed consent forms electronically to all of our participants, which they signed before the study.The registered dietitians had an average of nine years of professional experience, and mathematics educators averaged eight years of experience.While 56% of all participants had prior experience with LLM tools such as ChatGPT or Gemini, none had previously set evaluation criteria for LLMs before participating in our study.</p>
<p>Lay Users.</p>
<p>To evaluate the differences in criteria-setting between domain experts and lay users, we recruited participants who had previously received dietitian or nutrition services or had consulted with a tutor, teaching assistant, or other educational support for pedagogical purposes.The recruitment was carried out through social networks, university networks, and outreach within the local community.Among the dietitian users, one participant reported using GPT for nutrition services prior to the study.In the tutoring group, 50% of participants had used LLM tools, such as ChatGPT or Gemini, for tutoring purposes with only one having prior experience in setting evaluation criteria.The process followed for setting and refining evaluation criteria for LLMs.In step (1), participants (domain experts or lay users) were presented with a prompt to review.In step (2), participants established initial criteria.In step (3), participants were presented with three outputs generated by LLMs.In step (4), each participant subsequently added or refined their criteria based on their review of the outputs.</p>
<p>Scenario Creation and Output Generation</p>
<p>We crafted three prompt scenarios for each domain, nutrition and pedagogy, based on a review of relevant literature.For the nutrition domain, prompt scenarios focused on disease management, food product suitability, and meal planning.These prompt scenarios were selected based on common prompts and questions posed to dietitians that LLMs could potentially address [15,26,37,43].In the pedagogy domain, prompt scenarios were inspired by existing studies on LLMs-as-tutors for secondary education [28].These prompt scenarios focused on generating hints for students with partially completed solutions, providing feedback on student responses, and explaining math concepts related to the 9th-12th grade curriculum, particularly algebra.All of these prompt scenarios were designed to elicit tutoring or pedagogical responses from the LLM.Detailed prompt scenario descriptions can be found in Appendix A.</p>
<p>The outputs presented to participants were generated using three different LLMs: GPT-4o, Claude 3.5 Sonnet, and Gemini.These models were selected to provide a range of responses, ensuring that the criteria set by participants could be applied to variations seen across different LLMs.</p>
<p>Study Protocol</p>
<p>We conducted interviews with participants, in which they were provided with prompt scenarios related to their specific domain.</p>
<p>The participants were then asked to set evaluation criteria that would be used to assess LLM outputs.To mitigate potential learning effects and order bias, we counterbalanced the prompt scenarios across participants.This means that each participant received the scenarios in a different order, ensuring that no single scenario was consistently presented first.For example, if one participant started with Scenario 1, another participant might begin with Scenario 2 or 3. Additionally, the LLM outputs were randomized to prevent any bias associated with the sequence of outputs.The interviews lasted 1.5 hours and were conducted over Zoom.</p>
<p>Figure 1 illustrates the interview process, which involved two key phases: the a priori initial criteria-setting phase while just viewing the prompt and the a posteriori refinement phase after seeing the outputs.To facilitate the development and tracking of evaluation criteria, we utilized MAXQDA, a software tool designed for qualitative data analysis, specifically suited for coding and tagging outputs based on user-defined categories [33].MAXQDA allowed participants to create system codes representing their evaluation criteria, which they could then apply, or "tag," to sections of the output during the review process.This tagging enabled participants to systematically assess how well each output aligned with their criteria.In addition to setting initial criteria, MAXQDA provided an easy mechanism for refinement.During the a posteriori phase, participants could revise or add new criteria as they reviewed the outputs.The tool allowed participants to tag outputs with both their original and newly refined criteria, making it possible to iteratively update and expand their evaluation framework based on what they observed in the LLM outputs.This systematic tagging and refinement process ensured that participants could effectively track how the criteria applied across different outputs and make necessary adjustments to capture relevant evaluation measures.</p>
<p>Initial Criteria Setting (a priori): Participants were first presented with a prompt and asked to develop initial evaluation criteria.These initial criteria were recorded in the MaxQDA system through the code-setting tools.Each initial criterion was added to a set labeled "Initial Criteria" for documentation and review.</p>
<p>Criteria Refinement (a posteriori): After the setting of the initial criteria, participants were presented with three different LLMgenerated outputs related to the prompt scenario.They reviewed each output relative to the criteria they had initially set.If one of their criteria applied to part of the output, they would use the MaxQDA system to code the criterion to that part of the output.In addition, after the participants noted aspects of the output that they liked, disliked, or would change, they either added additional criteria or refined their initial criteria.The participants were asked to highlight parts of the output in which the revised criteria applied.Through this process, participants were able to revisit and update their criteria to identify the most relevant and effective measures for evaluating LLM outputs in their respective domains.</p>
<p>LLM Generation of Criteria</p>
<p>Similarly, a LLM was tasked with generating evaluation criteria in two steps to mirror the process followed by human participants.First, the LLM was provided with the same prompt scenario given to human participants and was asked to generate an initial set of criteria based solely on the prompt.In the second phase, the LLM was given the initial criteria it generated, along with the same three LLM-generated outputs shown to the human participants.The LLM was then instructed to refine its initial criteria or develop new ones based on its evaluation of these outputs.While the LLM did not use predefined codes or tagging tools, as the human participants did in the MAXQDA system, this approach allowed the LLM to autonomously generate criteria and refine them without being influenced by a pre-established structure.A detailed description of the instructions used to generate the criteria can be found in Appendix B.</p>
<p>Criteria Consolidation and Analysis</p>
<p>We conducted a comprehensive analysis of the generated criteria using standard open-coding procedures.Three researchers independently participated in the coding process, beginning by thoroughly reviewing the criteria from the interviews to gain familiarity with the data.The criteria generated by domain experts, lay users, and the LLM were then grouped into themes.</p>
<p>As part of the initial coding process, the research team developed sub-themes for each of the three scenarios.These sub-themes varied by scenario, reflecting the unique context and focus of each task.After developing the initial sub-themes, we grouped them into higher-level themes that were consistent across all three scenarios and both domains.Through this thematic grouping, we consolidated the data into key findings (KF).</p>
<p>We also paid particular attention to when the criteria were created or modified, either during the initial prompt-only stage a priori or after the participants reviewed the outputs a posteriori.This temporal consideration allowed us to analyze how the criteria evolved over time and to identify differences between the themes generated by different groups at different stages.By examining how criteria changed after participants were exposed to the outputs, we were able to identify areas where the initial expectations a priori were either reinforced or adapted a posteriori, providing deeper insights into the criteria-setting of domain experts, lay users, and the LLM.</p>
<p>FINDINGS</p>
<p>Our findings provide valuable insights relevant to our first research question (RQ1), which examines the similarities and differences of criteria set by domain experts, lay users, and an LLM.Our second question (RQ2) then compares the types of criteria as well as common themes and sub-themes across a priori and a posteriori stages.We have organized our analysis into key findings (KF), and we provide examples of the criteria developed in each group.We denote nutrition experts as NutExp, followed by a number indicating the participant (e.g., NutExp1 for Nutrition Expert 1).Similarly, we use PedExp for pedagogy experts (e.g., PedExp1 for Pedagogy Expert 1), NutUsr for lay users in nutrition (e.g., NutUsr1 for Nutrition User 1), and PedUsr for lay users in pedagogy (e.g., PedUsr1 for Pedagogy User 1).</p>
<p>Similarities and Differences in Domain</p>
<p>Experts, Lay Users, and LLMs (RQ1)</p>
<p>In this section, we explore the differences and similarities in the themes developed across the domain experts, lay users, and LLMs during the criteria development process.Although there are common themes that emerged across the different participant groups, we also observe notable differences in the level of specificity provided for the criteria.Domain experts, lay users, and LLMs each approached the criteria-setting task with varying levels of detail and focus.We break down these themes into sub-themes and provide examples of the specific criteria that were generated by each group to illustrate the distinctions in how they approached the criteria-setting tasks.Table 1 shows examples of the types of criteria across common themes and sub-themes among the three groups.</p>
<p>KF1: Domain experts show a greater level of detail and specificity in criteria development.</p>
<p>Nutrition/Dietetics Domain.While overlap was observed in the criteria within the Guidance theme across domain experts, lay users, and LLMs, the criteria provided by domain experts were more detailed and knowledge-based.For example, in the sub-theme of recommending healthier food product alternatives, which aligned to a request in the prompt for Scenario 1 (see Appendix A.1), all three groups recognized the importance of establishing criteria for food alternatives.However, the level of detail in their recommendations varied across the groups.The dietitians proposed more highly detailed criteria such as "give examples of high-fiber and low added sugar breakfast options that include at least 8 grams of protein per serving." (NutExp2) or "recommend cereals with less than 20% of the DV for added sugars, at least 10% DV for fiber, and at least 10% DV for protein" (NutExp3).These criteria reflect a deep understanding of nutritional guidelines and are intended to guide precise dietary recommendations.By specifying exact nutritional thresholds, expert criteria were observed to go beyond general advice to ensure that the recommendations are both practical and aligned with dietary standards.In contrast, the criteria generated by users were found to be broader and less detailed.For example, users proposed criteria such as, "suggested foods should contain high fiber foods" (NutUsr2) or "include price comparisons for meal alternatives" (NutUsr3).While these suggestions align with the overall goal of promoting healthier choices, they lack the specificity observed among the dietitians.</p>
<p>Similarly, the criteria produced by the LLM were also general in nature.For example, the LLM suggested, "Explanation includes potential cereal substitutions" (LLM).Although this aligns with the theme of recommending healthier alternatives, it does not specify how these substitutions should meet dietary goals, leaving room for broad interpretation.Another example can be found in Scenario 3, which involved responding to a prompt related to Crohn's disease ((refer to Appendix A.1).All domain experts set specific evaluation criteria, such as avoiding high fiber, lactose, and processed foods.These criteria differed from those generated by lay users, who were primarily concerned with more generalized information, such as recommending foods that would not cause flare-ups.Similarly, the LLM provided general criteria, such as to "identify foods that should be increased or avoided to prevent flare-ups" (LLM).While both users and the LLM acknowledged the need for a list of items, the domain experts were observed to go further in the criteria by specifying the exact types of foods to avoid, reflecting their deeper understanding and expertise.</p>
<p>In the sub-theme of dietary management, domain experts created criteria for blood sugar and health considerations, responding to the prompt that identified the individual's goal to manage blood sugar levels (refer to Appendix A.1). Dietitians expected the output to include detailed criteria such as "eating more meals or snacks throughout the day" (NutExp2) and "include protein at breakfast for ideal blood glucose management" (NutExp3).In contrast, the LLM provided a much more general response such as "The explanation addresses the user's goal of managing blood sugar levels" (LLM).This demonstrates a significant difference in the depth of evaluation criteria, as the domain experts were highly specific in their answers while the LLM appeared to respond simply to a keyword in the prompt.</p>
<p>Additionally, domain experts placed more emphasis on criteria related related to follow-up questions relative to the other two groups.For example, in Scenario 2 (refer to Appendix A.1), three out of five dietitians developed evaluation criteria that suggested that the model ask specific follow-up questions regarding the individual's weight loss goals, such as "the answer should ask the user how much weight they want to lose, and what their desired timeline is for weight loss" (NutExp4).Follow-up evaluation criteria in the domain expert group were consistently present across all three dietary scenarios.For example, in Scenario 1 "The model should query the user's gender and age to provide more personalized recommendations (NutExp4)" and in Scenario 3 "should query for medications and dietary supplements; if absent, recommend speaking with a medical professional (NutExp4)".Within these criteria, we observed that dietitians sought more detailed information about the client and emphasized the need to prompt for relevant information to generate the most accurate recommendations.</p>
<p>In the Accuracy theme, there was a shared concern about the accuracy of the output between the three groups.However, the dietitians provided highly specific criteria to define accuracy in their evaluations.For example, they emphasized the importance of health considerations with stating that "Managing blood sugar levels is crucial for preventing energy crashes and reducing the risk of developing insulin resistance" (NutExp1).Similarly, another dietitian made criteria to "Include vitamins and minerals which are present at at least 20% of the DV as benefits" (NutExp5).In contrast, lay users focused more on how the information was presented rather than the specific details themselves.For instance, users noted the importance of "Output lists specific product information in relation to nutrients I am seeking to gain or avoid" (NutUsr4) or to "Detail every ingredient in this cereal and detail all of the nutrition facts like protein % or fat %" (NutUsr1).This highlights a key difference in the expectations for accuracy, with dietitians focusing on specific health-related content, while lay users placed greater emphasis on how the information was presented and explained.The LLM also addressed accuracy but in a more general manner, providing criteria such as "Explanation includes assessment of added sugars in the cereal." This further highlights the varying levels of depth and specificity across the groups.</p>
<p>Pedagogy Domain.A similar emphasis on specificity was observed by domain experts in the tutoring domain.Within the theme of Clarity, domain experts used their subject matter expertise to establish precise criteria that focused on the output by not only providing clear mathematical explanations but also listing specific core concepts and justifications.While all three groups recognized the importance of delivering explanations across the scenarios, the degree of specificity differed notably between them.For instance, domain experts set more instructional criteria such as "include additional Considerations: Domain Restrictions, Discriminant Implications, negative coefficients versus positive coefficients."(PedExp3) and "should explain WHY the speeds are added together" (PedExp2), which we found to demonstrate their depth of understanding and focus on pedagogical clarity.These criteria often focused on the reasoning behind mathematical processes to ensure that the explanations provided did more than solve the problem; they were also designed to offer insight into why specific methods were used.In contrast, lay users tended to focus on broader criteria, for example, "the solution should highlight or point out any specific conditions that are critical to the question given" (PedUsr3) or "establish what is given in the problem (values known) &amp; unknown values" (PedUsr2).This approach, while useful, did not emphasize that the criteria provide the specific explanation or reasoning behind the steps as much as the domain expert criteria.Meanwhile, the LLM's criteria, while generally clear, were even less detailed than those of the lay users.An example from the LLM's evaluation was "the corrected solution provides clear reasoning and verification of each step taken" (LLM).Although this highlights the importance of clarity and verification, it lacks the specificity of explaining why a particular method was chosen, which was a key distinction in the experts' criteria.</p>
<p>In the theme of Accuracy, we observed that both domain experts and the LLM provided criteria focused on ensuring correct solutions, equations, mathematical concepts, and mathematical terms.For instance, the domain experts emphasized precision in the use of specific terms, setting criteria such as "use specific math vocabulary such as discriminant, variable, and coefficient" (PedExp1) or requiring that the output "should include the standard form of the equation (i.e., ax 2 + bx + c)" (PedExp2).In contrast, the LLM set more generalized criteria regarding accuracy, such as "the response is free from mathematical errors and inaccuracies" (LLM), reflecting a broader approach without emphasizing specific mathematical terminology or structures.While the lay users contributed fewer criteria related to accuracy, they did offer some unique perspectives.One lay user, for example, highlighted the importance of sequencing in their evaluation, specifying that the "correct approach should come after discussing errors" (PedUsr1).Another lay user added a practical criterion, suggesting the system should "solve for distance using the t that was solved for" (PedUsr4), indicating an interest in ensuring that intermediate steps are carried through to final solutions.These examples illustrate how lay users, although less focused on technical accuracy, still contributed meaningful criteria that emphasized the process and the logical progression in solving problems.This demonstrates a clear difference in the way accuracy is approached by domain experts, who prioritize detailed correctness and mathematical terminology, compared to the LLM, which focuses on broader error prevention, and the lay users, who emphasize practical, process-oriented solutions.KF2: Lay users placed greater emphasis on presenting information in an understandable manner, with a stronger focus on layout and format relative to domain experts and LLMs.</p>
<p>Nutrition/Dietetics Domain.We observed that lay users placed significantly more emphasis on the Formatting of the output compared to domain experts.Lay users frequently introduced criteria aimed at providing bullet points or clear headings/titles to structure the output.For instance, in the dietary scenarios, they suggested formatting preferences such as "the response should have a bulleted list with highlighted text" (NutUsr2) and "the output should include a well-organized format of different diet options (bullets, numbers, spacing, etc.)" (NutUsr4).As the evaluation process continued, lay users refined these criteria, adding more specific preferences for structuring the information.For example, they called for the inclusion of clearly defined sections with titles, as reflected in the criterion "the output includes more defining titles for key takeaways/health best practices" (NutUsr4).This highlights how lay users prioritize readability and ease of navigation in the output, an aspect that neither the domain experts nor the LLM focused on.</p>
<p>In addition to formatting, lay users also set more criteria related to Clarity.They expressed preferences for concise and clear, which was overlooked by domain experts.For example, lay users proposed criteria, such as "Response should be short" (NutUsr2) and "output includes summary in layman terms at conclusion"(NutUsr4).This reflects their desire for outputs that are easy to understand, especially for non-experts, contrasting with the more detail-oriented approach of domain experts.Lay users also placed greater emphasis on ensuring the output provided explanations for recommendations.For instance, criteria such as "include reasons for avoiding/including certain foods" (NutUsr3) and "output should include information about why this is a good option for the user's individual needs" (Nu-tUsr4) illustrate the users' need for context and rationale behind the suggestions.Similarities were observed with the LLM, whereas domain experts tended to focus more on the accuracy or guidance of the information.While users did set criteria related to the accuracy and depth of content, such as the inclusion of specific dietary details, they did not emphasize how the information was presented or explained.This is similar to what was observed by the LLM.This contrast reveals that lay users are more concerned with ensuring that the output is accessible and comprehensible, while domain experts focus primarily on precision and domain-specific details.</p>
<p>Pedagogy Domain.In the pedagogy domain, we observed that lay users set more criteria related to the theme of Guidance, particularly focusing on providing teacher support and offering hints.For example, lay users created criteria such as "helps in recognizing where they might have misunderstood, [is] really helpful in fixing their thought process" (PedUsr1) or "Lead the student to try and make both sides to be under the same denominator of 4" (PedUsr1).This contrasts with domain experts, who focused on broader encouragement, with criteria such as "should encourage students in their math work."(PedExp2)</p>
<p>Additionally, lay users were more concerned with identifying and addressing incorrect answers.They emphasized criteria such as "If something is wrong, tell the user and prompt [the user] with questions to re-teach" (PedUsr1) and "point out what the user did incorrectly" (PedUsr2).This focus on error correction and understanding differs from domain experts, who did not prioritize this theme.</p>
<p>We also observed that lay users set criteria under the Formatting theme, emphasizing the need for relatable examples and visual aids to help them grasp concepts.Lay users focused on making content accessible by including visual aids, diagrams, and straightforward language, in contrast to domain experts and LLMs, who often relied on more technical terms and abstract explanations.For instance, one lay user suggested that the output "should provide figures or diagrams to aid understanding" (PedUsr2).Another emphasized the need to "use diagrams to accommodate visual learners." (PedUsr3).In contrast, domain experts set no criteria related to visual aids.</p>
<p>KF3:</p>
<p>The LLM creates criterion by taking keywords directly from the prompt that are more generalized and focus more on following prompt instructions relative to the users and the domain experts.</p>
<p>Nutrition/Dietetics Domain.We observed that many of the criteria established by the LLM fell under the same themes as those of the users and domain experts, but the LLM's criteria tended to be more generalized.The LLM provided broad surface-level details related to the prompt or the output, a pattern observed across multiple scenarios when comparing its criteria to those of domain experts and users.For example, in Scenario 2 (see Appendix A.1), which involved a prompt related to weight loss diets, there was overlap between the domain expert and the LLM in discussing Accuracy related to the diet safety.Both the LLM and the domain experts included criteria addressing the safety of diets.The LLM's criteria were broad, such as "avoid promoting overly restrictive fad diets" (LLM) and "avoid harmful or unsafe dieting practices" (LLM), which aligned with the concerns of domain experts about fad diets and the safety of the outputs.However, the domain experts were more specific, naming particular diets to avoid, such as intermittent fasting, carb cycling, paleo, or ketogenic diets.</p>
<p>There were also instances of disagreement between the LLM and domain experts regarding the evaluation criteria.For example, in the theme of Clarity in one scenario, the LLM set criteria that was non-decisive, such as "Evaluation seems balanced and objective without overtly favoring or disfavoring the product" (LLM).This differed from domain experts, who set firm recommendations, such as "[State] this food is not a good choice for managing blood sugar levels" (NutExp1) based on scientific understanding.</p>
<p>Additionally, some of the LLM's criteria were highly promptspecific, reflecting the details of the given prompt.For instance, in the theme of Guidance, when the prompt stated to lower blood sugar and find healthier alternatives, the LLM created criteria focusing on potential substitutions or dietary restrictions relevant to blood sugar management.Similarly, for prompts focused on weight loss and muscle gain, the LLM generated criteria directly aligned with the prompt, suggesting dietary recommendations for those specific goals.This demonstrates how the LLM's evaluation criteria are often shaped by the prompt provided.</p>
<p>Pedagogy Domain.The LLM aligns with both domain experts and lay users in the theme of Guidance, particularly in encouraging the use of examples and offering hints.For instance, in Scenario 1, which asked for hints without revealing the full solution, the LLM generated the criteria: "The response avoids directly giving the final answer to the math problem" (LLM).While this is more generalized than the criteria set by experts, who might specify the exact step at which to stop, it achieves the same objective.</p>
<p>Additionally, we observed that only the LLM prioritized personalization of the user compared to the other groups.The LLM set criteria such as "The hints provided align with the level of understanding expected from a freshman in high school studying Algebra 1" (LLM) or "The response uses appropriate and accessible language for a high school student" (LLM).Neither domain experts nor lay users considered the student's grade level in their criteria.</p>
<p>In the tutoring domain, we found similar trends between the LLM, domain experts, and users around the themes of Accuracy and Clarity.For accuracy, the LLM's criteria were more generalized, such as "The explanation correctly identifies the steps that have been taken so far, including subtracting 5 from both sides" (LLM).Although this is broader than the specific criteria set by domain experts or users, it still aligns with the objectives of the prompt.Similarly, in the theme of clarity, the LLM generated generalized criteria with more descriptive substance, such as "The response suggests specific operations to perform, such as multiplying to eliminate fractions, and reaffirms the logic behind these operations" (LLM) or "The response clarifies when it is appropriate to use the quadratic formula compared to other methods of solving quadratic equations" (LLM).Interestingly, the LLM did not set any criteria around the theme of Formatting in the pedagogy domain, although this was considered important by both domain experts and lay users.</p>
<p>Criteria Evolution Over Time (RQ2)</p>
<p>In this section, we will discuss how the evaluation criteria differ between the domain experts and users between the a priori stage and the a posteriori stages.To do this, we analyzed the different types of criteria themes that were present.We discuss the types of criterion that were refined between stages, then we will discuss the new themes that emerged in the a posteriori stage.Figure 2 shows the average number of criteria that was set for each domain across the themes.</p>
<p>KF4: In the a posteriori phase, groups introduce additional criteria, with domain experts particularly emphasizing new themes related to formatting.</p>
<p>Nutrition/Dietetics Domain.For the dietitian experts, new criteria emerged in the formatting theme, such as adding bullet points or providing clear headings/titles.As some outputs featured these different layouts, experts adapted by incorporating criteria that improved readability.Additionally, the sub-theme of encouragement was introduced by domain experts, who appreciated the supportive tone of the outputs.For instance, one expert suggested the criterion to "always encourage patience and consistency" (PedExp2) in response to a specific output.</p>
<p>New criteria also emerged under the theme of Clarity, focusing on specific wording and Decision Support.Dietitian experts added criteria to ensure outputs used precise medical language, such as "Remove opinionated text" (PedExp5) or "Avoid giving medical advice" (PedExp4).Furthermore, when the output lacked guidance to help make informed decisions, experts introduced criteria such as "When recommending to 'limit' a nutrient, provide evidence-based guidance, e.g., from the DGA or AHA" (PedExp4).This shift highlights how participants transitioned from conceptualizing ideal outputs to critiquing model performance and suggesting improvements.</p>
<p>Many themes remained consistent for domain experts, but they added new criteria based on what was absent in the outputs.For instance, if allergens were missing, an expert added the criterion "Should include allergens in cereal" (PedExp3), or when specific nutrient information was not present, they added "List nutrition label" (PedExp5).For lay users, fewer new themes emerged, but they added criteria based on their preferences in the output.Some users, for example, introduced criteria related to staying hydrated after seeing it highlighted in the output which suggests that exposure to certain content can influence their evaluation criteria.</p>
<p>The LLM presented new sub-themes related to decision support and staying hydrated.For example, after the model was fed the output that mentioned hydration, the LLM added the criteria of "The response includes recommendations to stay hydrated" (LLM).In addition, since the output addressed foods that were quick for the user, a new criteria of "Explanation clearly addresses the quick and easy preparation requirement" (LLM) was added.By adapting its criteria based on the output, the LLM not only mirrors human  evaluative processes, but also refines its ability to meet user needs more effectively.</p>
<p>Pedagogy Domain.In the a posteriori phase, new criteria related to formatting also emerged among the pedagogy experts.Once the domain experts were exposed to the outputs, they introduced criteria for formatting the results using numbers or bullets, and criteria regarding how to show equations.For example, one expert introduced the criterion: "Should provide the quadratic formula as a clear fraction with a numerator and denominator" (PedExp2).</p>
<p>Additionally, pedagogy experts created criteria for formatting outputs with numbers and bullets.Interestingly, the lay users also emphasized the importance of presenting equations in the a posteriori phase.However, unlike the pedagogy experts, lay users set criteria focused on the inclusion of visual aids.For example, one lay user added the criterion: "A picture or diagram that represents the question" (PedUsr2).This suggests that while both groups valued proper equation presentation, lay users placed greater emphasis on visual representation.The LLM, in contrast, did not provide any criteria related to formatting.In KF1, we observed a focus on the themes of accuracy, clarity, and guidance among pedagogy experts.For the theme of accuracy, criteria were predominantly set in the a priori phase, where all 5 pedagogy experts established some criteria related to accuracy.In the a posteriori phase, experts refined their accuracy-related criteria.In contrast, only 1 lay user set accuracy-related criteria in the a priori phase, with another user adding accuracy criteria in the a posteriori phase.The LLM also set accuracy-related criteria in the a priori phase but showed minimal refinement in the a posteriori phase.For example, a pedagogy expert introduced the criterion: "Explain the importance of keeping the equation balanced" (PedExp1), highlighting the focus on mathematical accuracy early on.</p>
<p>For the theme of clarity, both pedagogy experts and lay users consistently set criteria to ensure clear mathematical explanations.In both a priori and a posteriori phases, all domain experts and lay users set criteria related to clarity.However, pedagogy experts placed additional emphasis on breaking down mathematical steps and mentioning core concepts.For instance, one expert added the criterion: "Categorize algebraic process steps" (PedExp1), reflecting their focus on clarity and structure.This contrasts with the LLM, which did not introduce as many criteria specific to providing a breakdown of steps or core concepts.</p>
<p>In the guidance theme, lay users introduced new criteria in the a posteriori phase related to providing check-ins for students and offering guiding questions for teachers.One lay user added the criterion: "Ask guided questions to aide in solving the next step" (PedUsr4).The domain experts instead introduced criteria around providing hints in the a posteriori phase, likely in response to the LLM often giving full answers instead of gradual hints.Additionally, more criteria related to teacher support emerged in the a posteriori phase for both domain experts and lay users.</p>
<p>The LLM did not present any new emerging themes between the a priori and a posteriori phases.Instead, the model refined existing criteria within the same themes.For example, in the a priori phase of Scenario 3, the LLM set the criterion: "The response clearly defines what a quadratic equation is" (LLM).After seeing the output, the criterion evolved into: "The response explains why the quadratic formula is a useful method for solving quadratic equations".Additionally, new criteria emerged based on what was explained in the output.For instance, the LLM added: "The response clearly differentiates between real and complex solutions when using the quadratic formula, " reflecting the content of the output.KF5: Experts in both domains created more criteria than lay users, with a notable increase in the a posteriori phase, indicating refinement after reviewing outputs.</p>
<p>Table 2 compares the average number of criteria created by domain experts and lay users in both the a priori and a posteriori phases across the dietetics and pedagogy domains.As the data demonstrates, experts in both domains generally set more criteria than lay users, particularly in the a posteriori phase, suggesting that exposure to outputs encouraged experts to refine and expand their evaluation criteria.Lay users, while more consistent between phases, also increased the number of criteria slightly after reviewing outputs, indicating some degree of refinement.Interestingly, both experts and lay users in the dietetics domain created more criteria in both phases compared to those in pedagogy.This may suggest that tasks in the dietetics domain prompt a greater need for detailed and specific criteria, likely due to the complex nature of health-related decisions, compared to the more structured and instructional nature of the pedagogy domain.While both groups followed a similar refinement process, the extent of criteria development varied between domains, with a stronger focus on specific details in dietetics.</p>
<p>DISCUSSION</p>
<p>This paper contributes new insights into the strengths and weaknesses of domain experts, lay users, and LLMs in creating evaluation criteria for domain-specific tasks.Our findings suggest to HCI developers that we leverage the complementary strengths of the three groups in evaluation workflows.We show that the domain expert is needed because of the value of the complex and domain-specific criteria they generate.In addition, there are also benefits to using lay users and LLMs in the evaluation process.In this section, we will first discuss our findings from the interviews (RQ1 and RQ2) and suggest design implications (RQ3) for evaluation workflows to ensure the outputs meet the standards of both lay users and domain experts.</p>
<p>The Importance of Experts, Lay Users, and LLMs in the Evaluation Process</p>
<p>Our results show that domain experts can effectively apply their knowledge in setting LLM evaluation criteria by providing wellreasoned criteria for the model's output.Experts approach the task by answering the prompt as they would for a client or student, offering detailed guidance and outlining the key information that should be included in the output, almost as if they are creating a checklist of what the response should contain.This aligns with previous literature where domain experts use specialized knowledge to refine and improve model outcomes [43].</p>
<p>In contrast, the lay users were observed to set their criteria with more of a focus on inquiry, and prioritizing clear and understandable presentations, often emphasizing formatting or visuals to enhance readability.Our findings indicate that lay users contribute unique perspectives to the evaluation criteria, focusing on aspects such as clarity and format for easier comprehension.This user-centric approach highlights the importance of incorporating diverse viewpoints in the evaluation process, as lay users may prioritize different aspects of the outputs compared to domain experts.This study differs from previous approaches to LLM evaluation, which have often focused on developers or designers setting criteria and testing prompt outputs for alignment [4,41].However, we show a need to design evaluation systems that integrate both the domain expert and the lay users evaluation criteria in the process.</p>
<p>As we see in our study, the LLM occasionally provided evaluation criteria that aligned with the same themes as domain experts or lay users, however, its criteria tended to be overly generalized.Unlike domain experts, who drew on their specialized knowledge, the LLM appeared to focus primarily on keywords within the prompt to establish the criteria for its output.In HCI research, LLMs are increasingly used to replace human participants for cost-effectiveness [17,19].While LLMs have outperformed humans in tasks such as text annotation [12], relying on them for text generation and criteria setting introduces reliability concerns.LLMs often pull from outdated or contaminated data sources, which is particularly problematic in fields such as nutrition and pedagogy, where relying on non-credible sources can spread inaccurate information [43].In addition, LLMs struggle to accurately represent human demographic identities, introducing biases that can be harmful in sensitive areas such as health and education [46].These risks demonstrate the importance of integrating domain experts into the criteria-setting process to prevent the dissemination of inaccurate or biased information.Allowing LLMs to set evaluation criteria without expert oversight can be harmful, as these models tend to generalize or simplify complex topics, overlooking crucial context and domain-specific knowledge.Therefore, domain experts are essential to ensure that the evaluation criteria are accurate, credible, and free from bias.</p>
<p>Do We Experience Criteria Drift or</p>
<p>Overfitting to the Data?</p>
<p>Our findings align with other literature regarding the concept of the criteria drift as seen in Shankar et al. in that we also observed refinement and addition of new criteria following evaluation of the output [41].Interestingly, we observed that domain experts in the a priori stage added more heavy knowledge-based evaluation criteria.This is notable as it shows that the domain experts set domainspecific criteria prior to seeing the output, further displaying their priority in applying knowledge-based criteria at the onset upon reading the prompt.We also observed that additional criteria were created in the a posteriori phase in each domain.Domain experts were more inclined to introduce criteria related to formatting and readability during the a posteriori phase.Lay users and domain experts rarely changed their initial criteria.Instead, they expanded upon the original criteria by developing new criteria within the same themes established during the a priori phase.This process involved tagging specific aspects of the output they liked or disliked and using these impressions to set additional evaluation criteria.These findings suggest that the outputs had some influence on the evaluation criteria, as participants adapted and expanded their criteria based on their reactions to the content.This influence could potentially alter domain experts' mental models of the criteria needed, even when they have already established specific criteria from their knowledge.While lay users exhibited similar behavior, it is less concerning in their case, as they may rely more on the outputs to set accurate evaluation criteria in the a posteriori phase.Moreover, this issue is not exclusive to human evaluators.The tendency of LLMs to set criteria during the a posteriori phase mirrors the concept of "overfitting" found in machine learning, where models become too tailored to their training data and fail to generalize effectively [13].This raises significant questions regarding the authenticity of criteria drift observed in both human participants and LLMs.An important consideration is: to what extent are we seeing genuine criteria drift, and to what extent are participants and the LLM responding to the specific characteristics of the outputs?This type of concern should be taken into account when developing tools used for the generation of criteria and the evaluation of the results.</p>
<p>Implications For Evaluation Workflows (RQ3)</p>
<p>In this section, we draw on the insights learned from our study and address our third research question that discusses the implications for evaluation workflows.</p>
<p>Designing a Staged Evaluation Approach.</p>
<p>A key implication of our findings is that the evaluation process should be broken down into stages, with domain experts, lay users, and LLMs engaged at different points to maximize their complementary strengths.Domain experts should be involved in the a priori stage to set detailed, knowledge-based evaluation criteria that are rooted in professional standards.Their expertise ensures that complex, domain-specific considerations are addressed early to reduce the risk of bias in later stages.In addition, by involving the domain expert earlier on in the process, such as by having domain experts provide feedback on the prompt or create evaluation criteria, this could be saving the time and cost of domain expertise in output evaluation.Lay users, who prioritize formatting and usability and tend to provide more generalized criteria, may not require data annotation.Our findings suggest that their priorities can be effectively incorporated into prompt instructions by ensuring clarity, readability, and the inclusion of visual elements.These considerations are relevant across both domains, and developers can use this insight to refine model training or prompt instructions, potentially saving time and costs.However, if budget allows or usability testing is being conducted, involving lay users in the a posteriori phase could further enhance user experience without compromising the technical rigor established by domain experts.</p>
<p>Given the potential for overfitting, LLMs should primarily be used to create evaluation criteria during the a posteriori phase.However, there is an opportunity to leverage LLMs to collaborate with domain experts or lay users to refine and align criteria during the a priori phase.If involving domain experts is not feasible, we recommend using LLMs to manage lower-level tasks, such as ensuring the output meets the prompt's requirements.Additionally, training LLMs on domain-specific criteria could help scale evaluation processes across various scenarios and domains.By assigning routine tasks to LLMs and reserving more complex evaluations for domain experts, the evaluation process becomes more cost-effective and balanced.Tools such as spade, which automatically synthesize data quality assertions based on prompt version histories, can further streamline the process by identifying potential errors early, helping domain experts focus on high-priority aspects of the evaluation [40].This approach ensures both comprehensive depth and breadth in evaluations, utilizing the strengths of each group to produce high-quality, usable outputs.</p>
<p>Supporting the Development of Evaluation Criteria.</p>
<p>As we also saw in our results, domain experts and lay users sometimes struggled to create evaluation criteria in the a posteriori stage, acting more as evaluators than criteria-setters.We observed that participants would comment on aspects of the output they liked or disliked but found it challenging to establish criteria to support these preferences.This process sometimes limited the creation of definitive evaluation criteria that could be expressed as clear assertion statements.This suggests that future evaluation systems should support the development of criteria, perhaps by offering guidance or tools that help users articulate their evaluation standards more clearly.As seen with Shankar et al. [40], criteria can be generated based on the evaluation of the output.This would streamline the evaluation process and ensure that both user groups contribute to the refinement of LLM outputs.</p>
<p>Using</p>
<p>Criteria for LLM Refinement.Our findings also highlight the potential to utilize domain expert-established criteria as benchmarks not only to evaluate, but also to refine LLM outputs.The developed criteria can serve as foundational elements in the LLM training and fine-tuning processes.By integrating these criteria into the training data or prompt, designers can work toward aligning LLMs more closely with the standards and expectations of both domain experts and end-users.Previous literature has used comprehensive criteria to generate natural language feedback for task execution [32].For example, the LLMCRIT framework demonstrates how using criteria derived from expert guidelines can help models generate more structured and actionable feedback, making it a promising approach for fine-tuning LLM outputs toward expert-level expectations [32].Similarly, in our study, these criteria have shown that they can not only serve as evaluation tools, but also play a crucial role in refining and improving LLMs to better meet the requirements of domain-specific tasks.</p>
<p>LIMITATIONS AND FUTURE WORK</p>
<p>Our study, while instrumental in investigating the criteria development process, exhibits several limitations that warrant acknowledgment.First, we conducted this study within two specific domains, dietetics and pedagogy, which may limit the generalizability of our findings.Different domains, especially those with varying levels of technical complexity or unique evaluation needs, could yield different criteria and evaluation workflows.Future work could explore how these workflows apply to a broader range of domains to better understand the flexibility and adaptability of the proposed evaluation processes.</p>
<p>Second, while our findings suggest a new evaluation workflow, it has yet to be tested in practical settings.Future work could focus on building systems and tools that implement and support this workflow, ensuring that it is practical and scalable for real-world applications.Implementing our proposed evaluation strategies in live systems will allow further refinement based on practical challenges and insights gathered during usage.</p>
<p>Additionally, we recognize the limitations surrounding the replicability of our findings due to potential changes in LLMs over time.The use of LLMs in our study to generate criteria and output introduces the challenge of evolving models, which may affect the consistency of the results if the study is replicated at a later date.The results and evaluation criteria used in our study were generated in July 2024, and updates or changes in the LLMs may lead to variations in performance or outcomes.</p>
<p>Finally, while this study focused on the evaluation process, there may be other opportunities to investigate how criteria-setting can be further optimized by integrating domain experts, lay users, and LLMs into other stages of model development, such as working to use this criteria to improve the model during prompt generation or model fine-tuning.All of these approaches could significantly improve the models to incorporate domain expertise to improve the model.</p>
<p>CONCLUSION</p>
<p>This paper examined the strengths and weaknesses of domain experts, lay users, and LLMs in creating evaluation criteria for specialized tasks, providing an empirical understanding of how these groups contribute to the evaluation process in both the a priori and a posteriori stages.Our findings highlight the complementary strengths of domain experts and lay users, each offering unique perspectives, as well as the potential and limitations of using LLMs to develop evaluation criteria.We observed significant differences in the criteria set by each group, revealing both benefits and challenges for improving evaluation workflows.Our analysis suggests design implications that emphasize leveraging the complementary strengths of different groups to establish complex, knowledge-based criteria, while also considering the cost and time involved in human annotations, particularly with domain experts.This study not only emphasizes the critical role that domain experts can play in enhancing evaluation processes, but also provides valuable insights into how different types of users can contribute to the development of more effective evaluation workflows.Ultimately, our work contributes to the creation of efficient workflows that can help designers better assess the quality of LLM output.</p>
<p>A PROMPTS</p>
<p>This section presents the three scenarios used to develop evaluation criteria in both the Nutrition and Dietetics domain as well as the Education and Tutoring domain.The same prompts were provided to the Domain Expert, LLM, and User, ensuring consistency in developing criteria across different perspectives.</p>
<p>A.1 Nutrition and Dietetics Domain</p>
<p>Scenario 1.I'm aiming to cut down on added sugars and refined carbs to help manage my blood sugar levels, while also trying to increase my intake of fiber-rich foods.I need something quick and easy in the morning, so I am considering starting the day with Great Value Cinnamon Crunch Breakfast Cereal.Please provide an explanation as to why this product is a good or bad food choice for me, including any health benefits, allergens, dietary restrictions, or potential cereal substitutions.</p>
<p>Scenario 2.</p>
<p>I am a 25-year-old woman with Crohn's disease.I have a lot of flare ups with the foods that I am eating.Please discuss which foods I should be increasing in my diet and which ones I should be avoiding that could cause flare ups.If I follow these restrictions, please discuss what benefits I should be looking for.I am a freshman in high school studying Algebra 1, and I'm working through a homework problem that's giving me some trouble.Here's the problem I'm working on: Solve for  in the equation:
3𝑥 − 2 4 + 5 =7 2
.</p>
<p>So far, I have subtracted 5 from both sides of the equation to get: My Solution: First, I set the distances they travel equal to each other since they meet at the same point: 40t = 60t Then, I solve for t by dividing both sides by their respective speeds, so t = 40/60 Therefore, they will meet at t = 2/3 hours.
3𝑥 − 2 4 =7
Scenario 3 I am a high school student in a math class.Can you help me understand how to solve quadratic equations using the quadratic formula?I'm not sure when or why to use it.</p>
<p>B LLM CRITERIA GENERATION PROMPTS</p>
<p>This section presents the single prompt used to develop the evaluation criteria.It includes one prompt for generating criteria based solely on the initial instruction, and another for refining the criteria after considering the model's outputs.Blue text indicates content that is programmatically filled in.</p>
<p>B.1 Criteria Generation Based on Instruction Only</p>
<p>System Prompt You are a helpful and precise assistant that can create binary evaluation criteria for a given user instruction.Your task is to generate evaluation criteria for assessing a large language model's performance.Each criterion should be a statement in which you would answer true/false.The criteria should not be in the form of a one sentence statement, not a question.You should return your final answer as a valid JSON object.User Prompt Create evaluation criteria for the given prompt instruction.</p>
<p>Instruction</p>
<p>B.2 Criteria Refinement Incorporating Model Outputs</p>
<p>System Prompt You are a helpful and precise assistant that will refine or add new binary evaluation criteria for a given user instruction and set out outputs.Your task is to refine or create new evaluation criteria for assessing a large language model's performance given a set of initial evaluation criteria.Each criterion should be a statement in which you would answer true/false.The criteria should not be in the form of a one sentence statement, not a question.The response should separate out the revised criteria and added criteria from the initial criteria.You should return your final answer as a valid JSON object.</p>
<p>User Prompt Consider the initial evaluation criteria provided.After reviewing the outputs attached, either refine the intial criteria or create new evaluation criteria for the given prompt instruction and three outputs: After reviewing the outputs attached, either refine the initial criteria or create new evaluation criteria for the given prompt instruction and three outputs.</p>
<p>[</p>
<p>4 Figure 1 :
41
Figure1: The process followed for setting and refining evaluation criteria for LLMs.In step(1), participants (domain experts or lay users) were presented with a prompt to review.In step (2), participants established initial criteria.In step (3), participants were presented with three outputs generated by LLMs.In step (4), each participant subsequently added or refined their criteria based on their review of the outputs.</p>
<p>(a) Average Number of Criteria Set for the Nutrition Domain (b) Average Number of Criteria Set for the Pedagogy Domain</p>
<p>Figure 2 :
2
Figure 2: Heat map of the average number of criteria for each domain set by experts and lay users at both the a priori and a posteriori stages.The checkmarks indicate where a LLM (GPT-4o) generated criteria at each stage within a specific theme.</p>
<p>Scenario 3 .
3
I am a 35-year-old, 230 pound man that would like to lose weight and gain muscle.I have been going to the gym every day to workout.What are some diets that I should be considering to lose weight?A.2 Education and Tutoring Domain Scenario 1.</p>
<p>[</p>
<p>The Start of Criteria] Evaluation Criteria [The End of Criteria].</p>
<p>Table 1 :
1Evaluation Criteria Examples
Examples of the types of criteria in the themes and sub-themes between domain experts, lay users, and LLMs.</p>
<p>Table 2 :
2
Table showing totals of the criteria created by both domain experts and lay users in the a priori and a posteriori stages.
NutritionPedagogyScenarioa prioria posterioria prioria posterioriExpert1785629856361047Total (Average)22261419Lay User 176482986636756Total (Average)21211520</p>
<p>2 − 5. I'm confused about how to deal with the fractions and what to do next.Could you please give me some hints on how to continue without directly giving me the answer?Scenario 2. I am a 9th-grade student and currently learning about relative speed and rate problems in my Algebra class.Could you please review my solution and provide feedback?I'd appreciate it if you could point out any errors in my reasoning or suggest alternative approaches to solving this problem.I'm really trying to grasp this concept fully.Question: Car A is traveling from City X to City Y, a distance of 120 miles, at a constant speed of 40 miles per hour.Car B is traveling from City Y to City X, a distance of 120 miles, at a constant speed of 60 miles per hour.Both cars start at the same time.At what point along the road between City X and City Y will they meet?</p>
<p>The Start of Instructions] Instruction [The End of Instructions] [The Start of Assistant 1's Response] Output 1 [The End of Assistant 1's Response] [The Start of Assistant 2's Response] Output 2 [The End of Assistant 2's Response] [The Start of Assistant 3's Response] Output 3 [The End of Assistant 3's Response]</p>
<p>Bhashithe Abeysinghe, Ruhan Circi, arXiv:2406.03339The Challenges of Evaluating LLM Applications: An Analysis of Automated, Human, and LLM-Based Approaches. 2024. 2024arXiv preprint</p>
<p>Creating trustworthy llms: Dealing with hallucinations in healthcare ai. Muhammad Aurangzeb, Ahmad , Ilker Yaramis, Taposh Dutta, Roy , arXiv:2311.014632023. 2023arXiv preprint</p>
<p>Augmenting LLMs with Knowledge: A survey on hallucination prevention. Konstantinos Andriopoulos, Johan Pouwelse, arXiv:2309.164592023. 2023arXiv preprint</p>
<p>ChainForge: A Visual Toolkit for Prompt Engineering and LLM Hypothesis Testing. Ian Arawjo, Chelse Swoopes, Priyan Vaithilingam, Martin Wattenberg, Elena L Glassman, Proceedings of the CHI Conference on Human Factors in Computing Systems. the CHI Conference on Human Factors in Computing Systems2024</p>
<p>Comparing physician and artificial intelligence chatbot responses to patient questions posted to a public social media forum. John W Ayers, Adam Poliak, Mark Dredze, Eric C Leas, Zechariah Zhu, Jessica B Kelley, Dennis J Faix, Aaron M Goodman, Christopher A Longhurst, Michael Hogarth, JAMA internal medicine. 1832023. 2023</p>
<p>ChatGPT and future artificial intelligence chatbots: what may be the influence on credentialed nutrition and dietetics practitioners. Angeline Chatelan, Aurélien Clerc, Pierre-Alexandre Fonta, Journal of the Academy of Nutrition and Dietetics. 1232023. 2023</p>
<p>Yida Chen, Aoyu Wu, Trevor Depodesta, Catherine Yeh, Kenneth Li, Nicholas Castillo Marin, Oam Patel, Jan Riecke, Shivam Raval, Olivia Seow, arXiv:2406.07882Designing a Dashboard for Transparency and Control of Conversational AI. 2024. 2024arXiv preprint</p>
<p>Engaging Legal Experts towards Responsible LLM Policies for Legal Advice. Inyoung Cheong, King Xia, Kevin Kj, Quan Feng, Amy X Ze Chen, Zhang, The 2024 ACM Conference on Fairness, Accountability, and Transparency. 2024A) I Am Not a Lawyer</p>
<p>Deep reinforcement learning from human preferences. Jan Paul F Christiano, Tom Leike, Miljan Brown, Shane Martic, Dario Legg, Amodei, 2017. 201730Advances in neural information processing systems</p>
<p>Potential of Large Language Models in Health Care: Delphi Study. Kerstin Denecke, Richard May, Octavio Rivera Llmhealthgroup, Romero, Journal of Medical Internet Research. 26e523992024. 2024</p>
<p>EvaluLLM: LLM assisted evaluation of generative outputs. Michael Desmond, Zahra Ashktorab, Qian Pan, Casey Dugan, James M Johnson, Companion Proceedings of the 29th International Conference on Intelligent User Interfaces. 2024</p>
<p>Yijiang River, Dong , Tiancheng Hu, Nigel Collier, arXiv:2406.11657Can LLM be a Personalized Judge?. 2024. 2024arXiv preprint</p>
<p>Determinants of llm-assisted decisionmaking. Eva Eigner, Thorsten Händler, arXiv:2402.173852024. 2024arXiv preprint</p>
<p>Mingqi Gao, Xinyu Hu, Jie Ruan, Xiao Pu, Xiaojun Wan, arXiv:2402.01383Llm-based nlg evaluation: Current status and challenges. 2024. 2024arXiv preprint</p>
<p>ChatGPT as a virtual dietitian: Exploring its potential as a tool for improving nutrition knowledge. Garcia Manuel, Applied System Innovation. 6962023. 2023</p>
<p>Repairing the cracked foundation: A survey of obstacles in evaluation practices for generated text. Sebastian Gehrmann, Elizabeth Clark, Thibault Sellam, Journal of Artificial Intelligence Research. 772023. 2023</p>
<p>ChatGPT outperforms crowd workers for text-annotation tasks. Fabrizio Gilardi, Meysam Alizadeh, Maël Kubli, Proceedings of the National Academy of Sciences. 120e23050161202023. 2023</p>
<p>Don't stop pretraining: Adapt language models to domains and tasks. Suchin Gururangan, Ana Marasović, Swabha Swayamdipta, Kyle Lo, Iz Beltagy, Doug Downey, Noah A Smith, arXiv:2004.109642020. 2020arXiv preprint</p>
<p>Evaluating large language models in generating synthetic hci research data: a case study. Perttu Hämäläinen, Mikke Tavast, Anton Kunnari, Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems. the 2023 CHI Conference on Human Factors in Computing Systems2023</p>
<p>Shaomang Huang, Jianfeng Pan, Hanzhong Zheng, arXiv:2407.11686CCoE: A Compact LLM with Collaboration of Experts. 2024. 2024arXiv preprint</p>
<p>Comparative Analysis of Finetuning Strategies and Automated Evaluation Metrics for Large Language Models in Customer Service Chatbots. Benjamin Ilse, Frederick Blackwood, 2024. 2024</p>
<p>Junfeng Jiao, Saleh Afroogh, Yiming Xu, Connor Phillips, arXiv:2406.18841Navigating llm ethics: Advancements, challenges, and future directions. 2024. 2024arXiv preprint</p>
<p>Llm comparator: Visual analytics for side-by-side evaluation of large language models. Minsuk Kahng, Ian Tenney, Mahima Pushkarna, Xieyang Michael, James Liu, Emily Wexler, Krystal Reif, Minsuk Kallarackal, Michael Chang, Lucas Terry, Dixon, Extended Abstracts of the CHI Conference on Human Factors in Computing Systems. 2024</p>
<p>Daniel Khashabi, Gabriel Stanovsky, Jonathan Bragg, Nicholas Lourie, Jungo Kasai, Yejin Choi, Noah A Smith, Daniel S Weld, arXiv:2101.06561GENIE: Toward reproducible and standardized human evaluation for text generation. 2021. 2021arXiv preprint</p>
<p>Evallm: Interactive evaluation of large language model prompts on user-defined criteria. Tae Soo, Kim , Yoonjoo Lee, Jamin Shin, Young-Ho Kim, Juho Kim, Proceedings of the CHI Conference on Human Factors in Computing Systems. the CHI Conference on Human Factors in Computing Systems2024</p>
<p>Comparison of answers between ChatGPT and human dieticians to common nutrition questions. Daniel Kirk, Elise Van Eijnatten, Guido Camps, Journal of Nutrition and Metabolism. 202355486842023. 2023</p>
<p>Optimization of hepatological clinical guidelines interpretation by large language models: a retrieval augmented generation-based framework. Simone Kresevic, Mauro Giuffrè, Milos Ajcevic, Agostino Accardo, Lory S Crocè, Dennis L Shung, NPJ Digital Medicine. 71022024. 2024</p>
<p>Student Interaction with NewtBot: An LLMas-tutor Chatbot for Secondary Physics Education. Anna Lieb, Toshali Goel, Extended Abstracts of the CHI Conference on Human Factors in Computing Systems. 2024</p>
<p>Rouge: A package for automatic evaluation of summaries. Chin-Yew Lin, Text summarization branches out. 2004</p>
<p>Exploring and evaluating hallucinations in llm-powered code generation. Fang Liu, Yang Liu, Lin Shi, Houkun Huang, Ruifeng Wang, Zhen Yang, Li Zhang, arXiv:2404.009712024. 2024arXiv preprint</p>
<p>Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, Chenguang Zhu, arXiv:2303.16634G-eval: Nlg evaluation using gpt-4 with better human alignment. 2023. 2023arXiv preprint</p>
<p>Leveraging LLMs for the Quality Assurance of Software Requirements. Sebastian Lubos, Alexander Felfernig, Thi Ngoc, Trang Tran, Damian Garber, Merfat El Mansi, Seda Polat Erdeniz, Viet-Man Le, 2024 IEEE 32nd International Requirements Engineering Conference (RE). IEEE2024</p>
<p>MAXQDA and its Application to LIS Research. Seyedhadi Marjaei, Ahmadian Fahimeh, Yazdi, Chandrashekara, Library Philosophy and Practice. 2019. 2019</p>
<p>Training language models to follow instructions with human feedback. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, Advances in neural information processing systems. 352022. 2022</p>
<p>Qian Pan, Zahra Ashktorab, Michael Desmond, Martin Santillan Cooper, James Johnson, Rahul Nair, Elizabeth Daly, Werner Geyer, arXiv:2407.03479Human-Centered Design Recommendations for LLM-as-a-Judge. 2024. 2024arXiv preprint</p>
<p>Bleu: a method for automatic evaluation of machine translation. Kishore Papineni, Salim Roukos, Todd Ward, Wei-Jing Zhu, Proceedings of the 40th annual meeting of the Association for Computational Linguistics. the 40th annual meeting of the Association for Computational Linguistics2002</p>
<p>Is ChatGPT an Effective Tool for Providing Dietary Advice?. Valentina Ponzo, Ilaria Goitre, Enrica Favaro, Fabio Dario Merlo, Maria Vittoria Mancino, Sergio Riso, Simona Bo, Nutrients. 164692024. 2024</p>
<p>ChatGPT utility in healthcare education, research, and practice: systematic review on the promising perspectives and valid concerns. Malik Sallam, In Healthcare. 118872023MDPI</p>
<p>Souvika Sarkar, Mohammad Fakhruddin Babar, Monowar Hasan, Shubhra Kanti, Karmaker , arXiv:2401.16577LLMs as On-demand Customizable Service. 2024. 2024arXiv preprint</p>
<p>Shreya Shankar, Haotian Li, Parth Asawa, Madelon Hulsebos, Yiming Lin, Harrison Zamfirescu-Pereira, Will Chase, Aditya G Fu-Hinthorn, Eugene Parameswaran, Wu, arXiv:2401.03038Spade: Synthesizing assertions for large language model pipelines. 2024. 2024arXiv preprint</p>
<p>Shreya Shankar, Björn Zamfirescu-Pereira, Aditya G Hartmann, Ian Parameswaran, Arawjo, arXiv:2404.12272Who Validates the Validators? Aligning LLM-Assisted Evaluation of LLM Outputs with Human Preferences. 2024. 2024arXiv preprint</p>
<p>Bias in reinforcement learning: A review in healthcare applications. Benjamin Smith, Anahita Khojandi, Rama Vasudevan, Comput. Surveys. 562023. 2023</p>
<p>Integrating Expertise in LLMs: Crafting a Customized Nutrition Assistant with Refined Template Instructions. Annalisa Szymanski, Brianna L Wimer, Oghenemaro Anuyah, Heather A Eicher-Miller, Ronald A Metoyer, Proceedings of the CHI Conference on Human Factors in Computing Systems. the CHI Conference on Human Factors in Computing Systems2024</p>
<p>Ningzhi Tang, Meng Chen, Zheng Ning, Aakash Bansal, Yu Huang, Collin Mcmillan, Toby Jia-Jun Li, arXiv:2405.16081A Study on Developer Behaviors for Validating and Repairing LLM-Generated Code Using Eye Tracking and IDE Actions. 2024. 2024arXiv preprint</p>
<p>A Field Guide to Automatic Evaluation of LLM-Generated Summaries. A Tempest, Brittany Van Schaik, Pugh, Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval. the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval2024</p>
<p>Large language models cannot replace human participants because they cannot portray identity groups. Angelina Wang, Jamie Morgenstern, John P Dickerson, arXiv:2402.019082024. 2024arXiv preprint</p>
<p>Jiayin Wang, Weizhi Ma, Peijie Sun, Min Zhang, Jian-Yun Nie, arXiv:2401.08329Understanding User Experience in Large Language Model Interactions. 2024. 2024arXiv preprint</p>
<p>Prompt engineering in consistency and reliability with the evidence-based guideline for LLMs. Li Wang, Xi Chen, Xiangwen Deng, Hao Wen, Mingke You, Weizhi Liu, Qi Li, Jian Li, Digital Medicine. 7412024. 2024</p>
<p>Ian Webster, promptfoo: Test your prompts. 2023</p>
<p>Chainof-Experts: When LLMs Meet Complex Operations Research Problems. Ziyang Xiao, Dongxiang Zhang, Yangjun Wu, Lilin Xu, Jessica Yuan, Xiongwei Wang, Xiaojin Han, Tao Fu, Jia Zhong, Mingli Zeng, Song, The Twelfth International Conference on Learning Representations. 2023</p>
<p>Llmeval: A preliminary study on how to evaluate large language models. Yue Zhang, Ming Zhang, Haipeng Yuan, Shichun Liu, Yongyao Shi, Tao Gui, Qi Zhang, Xuanjing Huang, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence2024. 19615-1962238</p>
<p>Nisan Daniel M Ziegler, Jeffrey Stiennon, Tom B Wu, Alec Brown, Dario Radford, Paul Amodei, Geoffrey Christiano, Irving, arXiv:1909.08593Fine-tuning language models from human preferences. 2019. 2019arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
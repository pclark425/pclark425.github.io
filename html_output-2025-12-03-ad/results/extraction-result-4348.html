<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4348 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4348</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4348</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-99.html">extraction-schema-99</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or approaches that use LLMs (or other AI models) to extract, distill, or discover quantitative laws, patterns, relationships, or principles from scientific papers or scholarly literature.</div>
                <p><strong>Paper ID:</strong> paper-270371249</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2406.05348v3.pdf" target="_blank">Toward Reliable Ad-hoc Scientific Information Extraction: A Case Study on Two Materials Datasets</a></p>
                <p><strong>Paper Abstract:</strong> We explore the ability of GPT-4 to perform ad-hoc schema based information extraction from scientific literature. We assess specifically whether it can, with a basic prompting approach, replicate two existing material science datasets, given the manuscripts from which they were originally manually extracted. We employ materials scientists to perform a detailed manual error analysis to assess where the model struggles to faithfully extract the desired information, and draw on their insights to suggest research directions to address this broadly important task.</p>
                <p><strong>Cost:</strong> 0.02</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4348.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4348.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or approaches that use LLMs (or other AI models) to extract, distill, or discover quantitative laws, patterns, relationships, or principles from scientific papers or scholarly literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4 ODIE pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>One-shot schema-based extraction pipeline using GPT-4 for on-demand information extraction (ODIE)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A pipeline that converts PDFs to XML (via GROBID), sends full-paper XML to GPT-4 with role-setting and a one-shot exemplar prompt encoding a JSON schema, and postprocesses the model's JSON output to produce structured rows of quantitative materials data.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>One-shot schema-based extraction with GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Papers (PDFs) are parsed to XML using GROBID; the XML for a full paper (including tables/captions when parsed) is provided to GPT-4 (gpt-4-1106-preview / GPT-4 Turbo variant) along with role-setting, an explicit JSON schema enumerating desired fields, and a single exemplar (one-shot). The model is prompted to return a JSON list of schema entries (one per unique composition/experiment). Minimal postprocessing normalizes slight JSON-format differences and removes JS-style comments; extracted rows are then aligned to ground-truth dataset rows using a greedy matching requiring key columns (e.g., formula + yield strength or diffusing element + D). Variants tested include zero-shot, one-shot, and a LangChain chunking workflow.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>GPT-4 (gpt-4-1106-preview / GPT-4 Turbo variant)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Materials science (multi-principal element alloys and silicate melt diffusion)</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>128 MPEA PDFs attempted (of 164), 55 diffusion PDFs attempted (of 71); manual annotation sample: 60 MPEA papers, 40 diffusion papers</td>
                        </tr>
                        <tr>
                            <td><strong>type_of_quantitative_law</strong></td>
                            <td>Extraction of numerical experimental measurements and property values (yield strength, hardness, diffusion coefficients) rather than discovery of closed-form laws; supports building structured datasets for downstream model-driven discovery</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_output_format</strong></td>
                            <td>Structured JSON rows corresponding to dataset schema (tabular rows of numerical and categorical fields)</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Automated alignment and comparison to two externally curated ground-truth datasets (MPEA dataset and Zhang et al. diffusion dataset) plus detailed manual expert error analysis</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>MPEA (evaluated subset): 368 ground-truth entries evaluated, 107 matched (29.5% match rate / recall on evaluated set); diffusion: 1714 ground-truth entries evaluated, 23 matched (~1.34% match rate). Qualitative: high hallucination rate (approx. hallucinated rows similar in number to matches for MPEA); many misses due to parsing/figures/tables.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared prompting variants (0-shot, 1-shot, LangChain chunking). One-shot generally outperformed zero-shot; LangChain increased number of relevant extractions but reduced precision (more hallucinations). No external non-LLM extraction baselines reported for head-to-head quantitative comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Major failure modes: PDF/XML parsing (GROBID failures), inability to read figures/plots (figure-bound quantitative data inaccessible), complex/nonstandard table comprehension, unit mismatches and missing unit conversion, narrative-vs-table context ambiguity, generic LM comprehension errors leading to missed values or hallucinations, and hallucination of rows (including extracting secondary-source values). API limitation: no programmatic PDF upload; needed intermediate XML.</td>
                        </tr>
                        <tr>
                            <td><strong>requires_human_in_loop</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>fully_automated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Toward Reliable Ad-hoc Scientific Information Extraction: A Case Study on Two Materials Datasets', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4348.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4348.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or approaches that use LLMs (or other AI models) to extract, distill, or discover quantitative laws, patterns, relationships, or principles from scientific papers or scholarly literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LangChain chunking workflow</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LangChain-based chunked schema extraction workflow</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A LangChain implementation that chunks long-paper text into overlapping token windows, provides an entity/schema description to an LLM, and aggregates per-chunk extractions into full-paper structured outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>LangChain chunked schema extraction</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Papers are split into 2000-token chunks with 1% overlap; each chunk and a schema of entities (dataset columns) is passed to the LLM (same GPT-4 variant) via the LangChain framework, and results are merged to produce JSON rows. This approach reduces context-length issues by localizing extraction to smaller windows and then aggregating.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>GPT-4 (same model family as used in the main pipeline)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Materials science (same datasets as main study)</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Same attempted set as main pipeline (128 MPEA, 55 diffusion PDFs attempted)</td>
                        </tr>
                        <tr>
                            <td><strong>type_of_quantitative_law</strong></td>
                            <td>Extraction of numerical experimental measurements and associated metadata (material properties, diffusion coefficients)</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_output_format</strong></td>
                            <td>Structured JSON rows</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Comparison to ground-truth datasets and manual expert annotation (same evaluation pipeline as main study)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Qualitative: LangChain extracted more relevant entries (higher recall) in some cases but reduced precision (more hallucinations) compared to one-shot full-document prompting; exact numeric metrics not provided in text.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared to zero-shot and one-shot full-document prompting within the study; LangChain offered recall gains at cost of precision.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Still subject to upstream PDF/XML parsing failures and figure-bound data loss; chunking can increase hallucinations and introduces aggregation/alignment challenges.</td>
                        </tr>
                        <tr>
                            <td><strong>requires_human_in_loop</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>fully_automated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Toward Reliable Ad-hoc Scientific Information Extraction: A Case Study on Two Materials Datasets', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4348.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4348.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or approaches that use LLMs (or other AI models) to extract, distill, or discover quantitative laws, patterns, relationships, or principles from scientific papers or scholarly literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ODIE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>On-Demand Information Extraction (ODIE)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A paradigm in which an LLM is provided a corpus, a schema, and a few exemplars to extract complex structured information ad-hoc without fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Instruct and Extract: Instruction Tuning for On-Demand Information Extraction</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>On-demand information extraction (ODIE)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>ODIE refers to using large pre-trained LMs in a zero- or few-shot manner to extract complex schema-aligned structured information from text on demand (i.e., one-off extraction tasks) by providing instructions and exemplars rather than fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>General LLMs (ODIE literature explores multiple LLMs; specific models depend on cited works)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Broad (scientific literature, clinical notes, etc.) — discussed as a general paradigm</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>type_of_quantitative_law</strong></td>
                            <td>Structured extraction of numerical experimental measurements and associated metadata (supports building datasets for downstream discovery rather than direct symbolic law discovery)</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_output_format</strong></td>
                            <td>Structured tables/JSON following user schema</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Varies by work; Jiao et al. (ODIE) focuses on instruction tuning and evaluation datasets/metrics for IE</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Same general limitations as few-shot LLM IE: sensitivity to prompt/exemplar selection, hallucination, difficulty with multimodal content (figures), and reliance on parsed text quality.</td>
                        </tr>
                        <tr>
                            <td><strong>requires_human_in_loop</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fully_automated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Toward Reliable Ad-hoc Scientific Information Extraction: A Case Study on Two Materials Datasets', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4348.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4348.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or approaches that use LLMs (or other AI models) to extract, distill, or discover quantitative laws, patterns, relationships, or principles from scientific papers or scholarly literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SciDaSynth</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SciDaSynth: Interactive Structured Knowledge Extraction and Synthesis from Scientific Literature with Large Language Model</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An interactive QA-based interface leveraging LLMs to extract structured scientific knowledge; evaluated via a 12-scientist user study.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>SciDaSynth: Interactive Structured Knowledge Extraction and Synthesis from Scientific Literature with Large Language Model</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>SciDaSynth interactive QA-based extraction</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>An interactive system that uses question-answering interactions with an LLM to extract structured knowledge from papers; designed to let domain scientists iteratively query and refine extractions. The paper evaluated the interface in a user study with 12 scientists.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Scientific literature broadly (user study with domain scientists)</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>type_of_quantitative_law</strong></td>
                            <td>General structured knowledge extraction (tables/relations) rather than explicit symbolic law discovery</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_output_format</strong></td>
                            <td>Interactive QA outputs that can be synthesized into structured representations</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>12-scientist user study (human evaluation of system utility)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Interactive approach requires human-in-the-loop; multimodal/figure data and PDF parsing remain challenges in general.</td>
                        </tr>
                        <tr>
                            <td><strong>requires_human_in_loop</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>fully_automated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Toward Reliable Ad-hoc Scientific Information Extraction: A Case Study on Two Materials Datasets', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4348.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e4348.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or approaches that use LLMs (or other AI models) to extract, distill, or discover quantitative laws, patterns, relationships, or principles from scientific papers or scholarly literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Polak et al. GPT-4 extraction</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Flexible, Model-Agnostic Method for Materials Data Extraction from Text Using General Purpose Language Models (Polak et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Works that apply GPT-4 (and conversational LLMs) to extract specific material properties (e.g., bulk modulus) from sentences in papers, using prompt engineering and verification strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Flexible, Model-Agnostic Method for Materials Data Extraction from Text Using General Purpose Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>GPT-4 / conversational LLM extraction for material properties</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Approaches that use LLMs (GPT-4 / conversational models) with targeted prompts (and sometimes additional verification) to extract numeric materials properties at sentence-level granularity; emphasis on model-agnostic pipelines and prompt engineering.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>GPT-4 and other conversational LLMs (per cited works)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Materials science (bulk modulus, phase-property relationships, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>type_of_quantitative_law</strong></td>
                            <td>Extraction of numerical property values and phase-property relationships (structured numerical data)</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_output_format</strong></td>
                            <td>Structured tables/JSON</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Comparison to curated sentence-level datasets and dataset creators' annotations (per cited works)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Sentence-level approaches may miss table/figure-bound data; need for verification/standardization of units and schema alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>requires_human_in_loop</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fully_automated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Toward Reliable Ad-hoc Scientific Information Extraction: A Case Study on Two Materials Datasets', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4348.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e4348.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or approaches that use LLMs (or other AI models) to extract, distill, or discover quantitative laws, patterns, relationships, or principles from scientific papers or scholarly literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Yang et al. band-gap extraction</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Accurate Prediction of Experimental Band Gaps from Large Language Model-Based Data Extraction</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A reported workflow using zero-shot ChatGPT with an added verification step to extract band-gap values from sentence-level data.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Accurate Prediction of Experimental Band Gaps from Large Language Model-Based Data Extraction</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Zero-shot ChatGPT with verification for band-gap extraction</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Zero-shot prompting of ChatGPT to extract band gap information from sentences, supplemented by a verification step to improve reliability of extracted values.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>ChatGPT (zero-shot)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Materials / semiconductor band gaps</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>type_of_quantitative_law</strong></td>
                            <td>Numerical material property extraction (band gaps)</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_output_format</strong></td>
                            <td>Structured numerical dataset (tables/JSON)</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Verification step plus comparison to curated datasets (Dong & Cole dataset referenced)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Zero-shot extraction requires verification to mitigate hallucination; sentence-level focus may omit tabular/figure data.</td>
                        </tr>
                        <tr>
                            <td><strong>requires_human_in_loop</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fully_automated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Toward Reliable Ad-hoc Scientific Information Extraction: A Case Study on Two Materials Datasets', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4348.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e4348.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or approaches that use LLMs (or other AI models) to extract, distill, or discover quantitative laws, patterns, relationships, or principles from scientific papers or scholarly literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Walker et al. fine-tuned GPT-3.5</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Extracting structured seed-mediated gold nanorod growth procedures from scientific text with LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A workflow that fine-tunes GPT-3.5 to extract experimental growth procedures and outcomes for gold nanorods from paper text.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Extracting structured seed-mediated gold nanorod growth procedures from scientific text with LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Fine-tuned GPT-3.5 for procedural and outcome extraction</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Fine-tune GPT-3.5 on labeled examples of experimental procedures and outcomes to extract structured protocols and quantitative outcomes from scientific text.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>GPT-3.5 (fine-tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Nanomaterials / materials synthesis</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>type_of_quantitative_law</strong></td>
                            <td>Extraction of experimental procedures and quantitative outcomes (not explicit discovery of laws)</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_output_format</strong></td>
                            <td>Structured procedural records and outcome fields</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Fine-tuning requires labeled data and may not generalize to ad-hoc schemas without retraining.</td>
                        </tr>
                        <tr>
                            <td><strong>requires_human_in_loop</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fully_automated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Toward Reliable Ad-hoc Scientific Information Extraction: A Case Study on Two Materials Datasets', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4348.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e4348.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or approaches that use LLMs (or other AI models) to extract, distill, or discover quantitative laws, patterns, relationships, or principles from scientific papers or scholarly literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Montanelli Cohere extraction</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>High-Throughput Extraction of Phase-Property Relationships from Literature Using Natural Language Processing and Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Use of the Cohere language model to extract phase–property relationships at scale from full-text materials papers.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>High-Throughput Extraction of Phase-Property Relationships from Literature Using Natural Language Processing and Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Cohere-based phase-property extraction</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Apply the Cohere model on corpora of full-text papers to extract relationships between phases and material properties in a high-throughput pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Cohere model (unspecified variant)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Materials science (phase-property relationships)</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>type_of_quantitative_law</strong></td>
                            <td>Phase-property relationships (structured associations; supports downstream modeling)</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_output_format</strong></td>
                            <td>Structured relationship records / tables</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Full-text and table/figure parsing limitations likely relevant; specifics not provided in this paper's discussion.</td>
                        </tr>
                        <tr>
                            <td><strong>requires_human_in_loop</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fully_automated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Toward Reliable Ad-hoc Scientific Information Extraction: A Case Study on Two Materials Datasets', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4348.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e4348.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or approaches that use LLMs (or other AI models) to extract, distill, or discover quantitative laws, patterns, relationships, or principles from scientific papers or scholarly literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>A-Lab autonomous-experiment pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>An autonomous laboratory for the accelerated synthesis of novel materials (A-Lab)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A closed-loop system that incorporates extracted synthesis recipes into AI-guided experimentation, linking IE outputs to robotic execution and optimization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>An autonomous laboratory for the accelerated synthesis of novel materials</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>AI-guided experimentation using extracted recipes</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Extract synthesis recipes/parameters from literature using information extraction methods and feed those structured recipes into an autonomous experimentation pipeline to plan and run experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Materials synthesis and autonomous experimentation</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>type_of_quantitative_law</strong></td>
                            <td>Extraction of procedural and parameter relationships to guide experiments (not explicit symbolic laws)</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_output_format</strong></td>
                            <td>Structured recipes/parameters integrated with experimental planners</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Experimental execution in an autonomous lab (per A-Lab publication)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Relies on high-quality extraction of protocol parameters; extraction errors propagate to experiments; integration challenges and need for ground-truth validation.</td>
                        </tr>
                        <tr>
                            <td><strong>requires_human_in_loop</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fully_automated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Toward Reliable Ad-hoc Scientific Information Extraction: A Case Study on Two Materials Datasets', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4348.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e4348.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or approaches that use LLMs (or other AI models) to extract, distill, or discover quantitative laws, patterns, relationships, or principles from scientific papers or scholarly literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ToolFormer</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ToolFormer: Language Models Can Teach Themselves to Use Tools</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method enabling LMs to learn to call external tools (e.g., unit converters) to augment abilities such as unit normalization during extraction.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Toolformer: Language Models Can Teach Themselves to Use Tools</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Toolformer-style tool-augmented LM</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Train or prompt LMs to detect when external tool calls (e.g., unit conversion APIs, calculators) are needed and invoke those tools to normalize or transform extracted numeric values, addressing unit-mismatch and conversion errors.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>General LLMs in ToolFormer work (not a single specific model here)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>General — applicable to scientific information extraction tasks requiring unit conversions and computations</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>type_of_quantitative_law</strong></td>
                            <td>Unit-normalized numerical data extraction (supports downstream law discovery by normalizing values)</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_output_format</strong></td>
                            <td>Structured numerical values with normalized units</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Requires reliable tool-integration APIs and model coordination; not directly evaluated in the present study but suggested as a remedy for unit-related errors.</td>
                        </tr>
                        <tr>
                            <td><strong>requires_human_in_loop</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fully_automated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Toward Reliable Ad-hoc Scientific Information Extraction: A Case Study on Two Materials Datasets', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4348.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e4348.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or approaches that use LLMs (or other AI models) to extract, distill, or discover quantitative laws, patterns, relationships, or principles from scientific papers or scholarly literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4V OCR / vision references</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Exploring OCR Capabilities of GPT-4V(ision)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Work evaluating GPT-4V's ability to read and extract information from figures and plots, relevant to addressing figure-bound scientific data extraction.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Exploring OCR Capabilities of GPT-4V(ision) : A Quantitative and In-depth Evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Vision-enabled LLM figure/plot reading (GPT-4V)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Evaluate and apply multimodal LLMs (GPT-4V) to read OCR text in figures and to interpret plots so that numerical values presented visually can be extracted—potentially enabling extraction of values like yield strength from curves.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>GPT-4V (vision-capable GPT-4 variant) as evaluated in cited work</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>General scientific documents with figures/plots (materials science examples discussed)</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>type_of_quantitative_law</strong></td>
                            <td>Extraction of figure-bound numerical data (points on plots), enabling retrieval of experimental numerical values</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_output_format</strong></td>
                            <td>Numerical values and annotations extracted from images/plots (structured JSON)</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Quantitative OCR/vision evaluation in cited work</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Vision models may need specialized tuning to read curves precisely (e.g., read single point from load curve); integrating vision outputs with textual schema alignment is nontrivial.</td>
                        </tr>
                        <tr>
                            <td><strong>requires_human_in_loop</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fully_automated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Toward Reliable Ad-hoc Scientific Information Extraction: A Case Study on Two Materials Datasets', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4348.11">
                <h3 class="extraction-instance">Extracted Data Instance 11 (e4348.11)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or approaches that use LLMs (or other AI models) to extract, distill, or discover quantitative laws, patterns, relationships, or principles from scientific papers or scholarly literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Collage</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Collage: Decomposable rapid prototyping for information extraction on scientific PDFs</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A system for decomposable rapid prototyping of information extraction on scientific PDFs that can help address PDF parsing and IE pipeline modularity.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Collage: Decomposable rapid prototyping for information extraction on scientific pdfs</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Collage decomposable IE prototyping</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>A decomposable framework to prototype IE components on scientific PDFs (parsing, decomposition, and integration), aimed at improving robustness of PDF parsing and downstream extraction pipelines.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Scientific PDFs generally (addresses parsing and IE tooling challenges)</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>type_of_quantitative_law</strong></td>
                            <td>Facilitates reliable extraction of numerical and tabular information from PDFs (prerequisite to law discovery)</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_output_format</strong></td>
                            <td>Intermediate parsed representations feeding into IE models</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Helps address parsing but does not itself solve multimodal figure extraction or LM comprehension/hallucination.</td>
                        </tr>
                        <tr>
                            <td><strong>requires_human_in_loop</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fully_automated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Toward Reliable Ad-hoc Scientific Information Extraction: A Case Study on Two Materials Datasets', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Instruct and Extract: Instruction Tuning for On-Demand Information Extraction <em>(Rating: 2)</em></li>
                <li>SciDaSynth: Interactive Structured Knowledge Extraction and Synthesis from Scientific Literature with Large Language Model <em>(Rating: 2)</em></li>
                <li>Flexible, Model-Agnostic Method for Materials Data Extraction from Text Using General Purpose Language Models <em>(Rating: 2)</em></li>
                <li>Extracting accurate materials data from research papers with conversational language models and prompt engineering <em>(Rating: 2)</em></li>
                <li>High-Throughput Extraction of Phase-Property Relationships from Literature Using Natural Language Processing and Large Language Models <em>(Rating: 2)</em></li>
                <li>Extracting structured seed-mediated gold nanorod growth procedures from scientific text with LLMs <em>(Rating: 2)</em></li>
                <li>Toolformer: Language Models Can Teach Themselves to Use Tools <em>(Rating: 2)</em></li>
                <li>Exploring OCR Capabilities of GPT-4V(ision) : A Quantitative and In-depth Evaluation <em>(Rating: 2)</em></li>
                <li>An autonomous laboratory for the accelerated synthesis of novel materials <em>(Rating: 2)</em></li>
                <li>Collage: Decomposable rapid prototyping for information extraction on scientific pdfs <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4348",
    "paper_id": "paper-270371249",
    "extraction_schema_id": "extraction-schema-99",
    "extracted_data": [
        {
            "name_short": "GPT-4 ODIE pipeline",
            "name_full": "One-shot schema-based extraction pipeline using GPT-4 for on-demand information extraction (ODIE)",
            "brief_description": "A pipeline that converts PDFs to XML (via GROBID), sends full-paper XML to GPT-4 with role-setting and a one-shot exemplar prompt encoding a JSON schema, and postprocesses the model's JSON output to produce structured rows of quantitative materials data.",
            "citation_title": "here",
            "mention_or_use": "use",
            "method_name": "One-shot schema-based extraction with GPT-4",
            "method_description": "Papers (PDFs) are parsed to XML using GROBID; the XML for a full paper (including tables/captions when parsed) is provided to GPT-4 (gpt-4-1106-preview / GPT-4 Turbo variant) along with role-setting, an explicit JSON schema enumerating desired fields, and a single exemplar (one-shot). The model is prompted to return a JSON list of schema entries (one per unique composition/experiment). Minimal postprocessing normalizes slight JSON-format differences and removes JS-style comments; extracted rows are then aligned to ground-truth dataset rows using a greedy matching requiring key columns (e.g., formula + yield strength or diffusing element + D). Variants tested include zero-shot, one-shot, and a LangChain chunking workflow.",
            "llm_model_used": "GPT-4 (gpt-4-1106-preview / GPT-4 Turbo variant)",
            "scientific_domain": "Materials science (multi-principal element alloys and silicate melt diffusion)",
            "number_of_papers": "128 MPEA PDFs attempted (of 164), 55 diffusion PDFs attempted (of 71); manual annotation sample: 60 MPEA papers, 40 diffusion papers",
            "type_of_quantitative_law": "Extraction of numerical experimental measurements and property values (yield strength, hardness, diffusion coefficients) rather than discovery of closed-form laws; supports building structured datasets for downstream model-driven discovery",
            "extraction_output_format": "Structured JSON rows corresponding to dataset schema (tabular rows of numerical and categorical fields)",
            "validation_method": "Automated alignment and comparison to two externally curated ground-truth datasets (MPEA dataset and Zhang et al. diffusion dataset) plus detailed manual expert error analysis",
            "performance_metrics": "MPEA (evaluated subset): 368 ground-truth entries evaluated, 107 matched (29.5% match rate / recall on evaluated set); diffusion: 1714 ground-truth entries evaluated, 23 matched (~1.34% match rate). Qualitative: high hallucination rate (approx. hallucinated rows similar in number to matches for MPEA); many misses due to parsing/figures/tables.",
            "baseline_comparison": "Compared prompting variants (0-shot, 1-shot, LangChain chunking). One-shot generally outperformed zero-shot; LangChain increased number of relevant extractions but reduced precision (more hallucinations). No external non-LLM extraction baselines reported for head-to-head quantitative comparison.",
            "challenges_limitations": "Major failure modes: PDF/XML parsing (GROBID failures), inability to read figures/plots (figure-bound quantitative data inaccessible), complex/nonstandard table comprehension, unit mismatches and missing unit conversion, narrative-vs-table context ambiguity, generic LM comprehension errors leading to missed values or hallucinations, and hallucination of rows (including extracting secondary-source values). API limitation: no programmatic PDF upload; needed intermediate XML.",
            "requires_human_in_loop": true,
            "fully_automated": false,
            "uuid": "e4348.0",
            "source_info": {
                "paper_title": "Toward Reliable Ad-hoc Scientific Information Extraction: A Case Study on Two Materials Datasets",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "LangChain chunking workflow",
            "name_full": "LangChain-based chunked schema extraction workflow",
            "brief_description": "A LangChain implementation that chunks long-paper text into overlapping token windows, provides an entity/schema description to an LLM, and aggregates per-chunk extractions into full-paper structured outputs.",
            "citation_title": "here",
            "mention_or_use": "use",
            "method_name": "LangChain chunked schema extraction",
            "method_description": "Papers are split into 2000-token chunks with 1% overlap; each chunk and a schema of entities (dataset columns) is passed to the LLM (same GPT-4 variant) via the LangChain framework, and results are merged to produce JSON rows. This approach reduces context-length issues by localizing extraction to smaller windows and then aggregating.",
            "llm_model_used": "GPT-4 (same model family as used in the main pipeline)",
            "scientific_domain": "Materials science (same datasets as main study)",
            "number_of_papers": "Same attempted set as main pipeline (128 MPEA, 55 diffusion PDFs attempted)",
            "type_of_quantitative_law": "Extraction of numerical experimental measurements and associated metadata (material properties, diffusion coefficients)",
            "extraction_output_format": "Structured JSON rows",
            "validation_method": "Comparison to ground-truth datasets and manual expert annotation (same evaluation pipeline as main study)",
            "performance_metrics": "Qualitative: LangChain extracted more relevant entries (higher recall) in some cases but reduced precision (more hallucinations) compared to one-shot full-document prompting; exact numeric metrics not provided in text.",
            "baseline_comparison": "Compared to zero-shot and one-shot full-document prompting within the study; LangChain offered recall gains at cost of precision.",
            "challenges_limitations": "Still subject to upstream PDF/XML parsing failures and figure-bound data loss; chunking can increase hallucinations and introduces aggregation/alignment challenges.",
            "requires_human_in_loop": true,
            "fully_automated": false,
            "uuid": "e4348.1",
            "source_info": {
                "paper_title": "Toward Reliable Ad-hoc Scientific Information Extraction: A Case Study on Two Materials Datasets",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "ODIE",
            "name_full": "On-Demand Information Extraction (ODIE)",
            "brief_description": "A paradigm in which an LLM is provided a corpus, a schema, and a few exemplars to extract complex structured information ad-hoc without fine-tuning.",
            "citation_title": "Instruct and Extract: Instruction Tuning for On-Demand Information Extraction",
            "mention_or_use": "mention",
            "method_name": "On-demand information extraction (ODIE)",
            "method_description": "ODIE refers to using large pre-trained LMs in a zero- or few-shot manner to extract complex schema-aligned structured information from text on demand (i.e., one-off extraction tasks) by providing instructions and exemplars rather than fine-tuning.",
            "llm_model_used": "General LLMs (ODIE literature explores multiple LLMs; specific models depend on cited works)",
            "scientific_domain": "Broad (scientific literature, clinical notes, etc.) — discussed as a general paradigm",
            "number_of_papers": null,
            "type_of_quantitative_law": "Structured extraction of numerical experimental measurements and associated metadata (supports building datasets for downstream discovery rather than direct symbolic law discovery)",
            "extraction_output_format": "Structured tables/JSON following user schema",
            "validation_method": "Varies by work; Jiao et al. (ODIE) focuses on instruction tuning and evaluation datasets/metrics for IE",
            "performance_metrics": null,
            "baseline_comparison": null,
            "challenges_limitations": "Same general limitations as few-shot LLM IE: sensitivity to prompt/exemplar selection, hallucination, difficulty with multimodal content (figures), and reliance on parsed text quality.",
            "requires_human_in_loop": null,
            "fully_automated": null,
            "uuid": "e4348.2",
            "source_info": {
                "paper_title": "Toward Reliable Ad-hoc Scientific Information Extraction: A Case Study on Two Materials Datasets",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "SciDaSynth",
            "name_full": "SciDaSynth: Interactive Structured Knowledge Extraction and Synthesis from Scientific Literature with Large Language Model",
            "brief_description": "An interactive QA-based interface leveraging LLMs to extract structured scientific knowledge; evaluated via a 12-scientist user study.",
            "citation_title": "SciDaSynth: Interactive Structured Knowledge Extraction and Synthesis from Scientific Literature with Large Language Model",
            "mention_or_use": "mention",
            "method_name": "SciDaSynth interactive QA-based extraction",
            "method_description": "An interactive system that uses question-answering interactions with an LLM to extract structured knowledge from papers; designed to let domain scientists iteratively query and refine extractions. The paper evaluated the interface in a user study with 12 scientists.",
            "llm_model_used": null,
            "scientific_domain": "Scientific literature broadly (user study with domain scientists)",
            "number_of_papers": null,
            "type_of_quantitative_law": "General structured knowledge extraction (tables/relations) rather than explicit symbolic law discovery",
            "extraction_output_format": "Interactive QA outputs that can be synthesized into structured representations",
            "validation_method": "12-scientist user study (human evaluation of system utility)",
            "performance_metrics": null,
            "baseline_comparison": null,
            "challenges_limitations": "Interactive approach requires human-in-the-loop; multimodal/figure data and PDF parsing remain challenges in general.",
            "requires_human_in_loop": true,
            "fully_automated": false,
            "uuid": "e4348.3",
            "source_info": {
                "paper_title": "Toward Reliable Ad-hoc Scientific Information Extraction: A Case Study on Two Materials Datasets",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Polak et al. GPT-4 extraction",
            "name_full": "Flexible, Model-Agnostic Method for Materials Data Extraction from Text Using General Purpose Language Models (Polak et al.)",
            "brief_description": "Works that apply GPT-4 (and conversational LLMs) to extract specific material properties (e.g., bulk modulus) from sentences in papers, using prompt engineering and verification strategies.",
            "citation_title": "Flexible, Model-Agnostic Method for Materials Data Extraction from Text Using General Purpose Language Models",
            "mention_or_use": "mention",
            "method_name": "GPT-4 / conversational LLM extraction for material properties",
            "method_description": "Approaches that use LLMs (GPT-4 / conversational models) with targeted prompts (and sometimes additional verification) to extract numeric materials properties at sentence-level granularity; emphasis on model-agnostic pipelines and prompt engineering.",
            "llm_model_used": "GPT-4 and other conversational LLMs (per cited works)",
            "scientific_domain": "Materials science (bulk modulus, phase-property relationships, etc.)",
            "number_of_papers": null,
            "type_of_quantitative_law": "Extraction of numerical property values and phase-property relationships (structured numerical data)",
            "extraction_output_format": "Structured tables/JSON",
            "validation_method": "Comparison to curated sentence-level datasets and dataset creators' annotations (per cited works)",
            "performance_metrics": null,
            "baseline_comparison": null,
            "challenges_limitations": "Sentence-level approaches may miss table/figure-bound data; need for verification/standardization of units and schema alignment.",
            "requires_human_in_loop": null,
            "fully_automated": null,
            "uuid": "e4348.4",
            "source_info": {
                "paper_title": "Toward Reliable Ad-hoc Scientific Information Extraction: A Case Study on Two Materials Datasets",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Yang et al. band-gap extraction",
            "name_full": "Accurate Prediction of Experimental Band Gaps from Large Language Model-Based Data Extraction",
            "brief_description": "A reported workflow using zero-shot ChatGPT with an added verification step to extract band-gap values from sentence-level data.",
            "citation_title": "Accurate Prediction of Experimental Band Gaps from Large Language Model-Based Data Extraction",
            "mention_or_use": "mention",
            "method_name": "Zero-shot ChatGPT with verification for band-gap extraction",
            "method_description": "Zero-shot prompting of ChatGPT to extract band gap information from sentences, supplemented by a verification step to improve reliability of extracted values.",
            "llm_model_used": "ChatGPT (zero-shot)",
            "scientific_domain": "Materials / semiconductor band gaps",
            "number_of_papers": null,
            "type_of_quantitative_law": "Numerical material property extraction (band gaps)",
            "extraction_output_format": "Structured numerical dataset (tables/JSON)",
            "validation_method": "Verification step plus comparison to curated datasets (Dong & Cole dataset referenced)",
            "performance_metrics": null,
            "baseline_comparison": null,
            "challenges_limitations": "Zero-shot extraction requires verification to mitigate hallucination; sentence-level focus may omit tabular/figure data.",
            "requires_human_in_loop": null,
            "fully_automated": null,
            "uuid": "e4348.5",
            "source_info": {
                "paper_title": "Toward Reliable Ad-hoc Scientific Information Extraction: A Case Study on Two Materials Datasets",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Walker et al. fine-tuned GPT-3.5",
            "name_full": "Extracting structured seed-mediated gold nanorod growth procedures from scientific text with LLMs",
            "brief_description": "A workflow that fine-tunes GPT-3.5 to extract experimental growth procedures and outcomes for gold nanorods from paper text.",
            "citation_title": "Extracting structured seed-mediated gold nanorod growth procedures from scientific text with LLMs",
            "mention_or_use": "mention",
            "method_name": "Fine-tuned GPT-3.5 for procedural and outcome extraction",
            "method_description": "Fine-tune GPT-3.5 on labeled examples of experimental procedures and outcomes to extract structured protocols and quantitative outcomes from scientific text.",
            "llm_model_used": "GPT-3.5 (fine-tuned)",
            "scientific_domain": "Nanomaterials / materials synthesis",
            "number_of_papers": null,
            "type_of_quantitative_law": "Extraction of experimental procedures and quantitative outcomes (not explicit discovery of laws)",
            "extraction_output_format": "Structured procedural records and outcome fields",
            "validation_method": null,
            "performance_metrics": null,
            "baseline_comparison": null,
            "challenges_limitations": "Fine-tuning requires labeled data and may not generalize to ad-hoc schemas without retraining.",
            "requires_human_in_loop": null,
            "fully_automated": null,
            "uuid": "e4348.6",
            "source_info": {
                "paper_title": "Toward Reliable Ad-hoc Scientific Information Extraction: A Case Study on Two Materials Datasets",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Montanelli Cohere extraction",
            "name_full": "High-Throughput Extraction of Phase-Property Relationships from Literature Using Natural Language Processing and Large Language Models",
            "brief_description": "Use of the Cohere language model to extract phase–property relationships at scale from full-text materials papers.",
            "citation_title": "High-Throughput Extraction of Phase-Property Relationships from Literature Using Natural Language Processing and Large Language Models",
            "mention_or_use": "mention",
            "method_name": "Cohere-based phase-property extraction",
            "method_description": "Apply the Cohere model on corpora of full-text papers to extract relationships between phases and material properties in a high-throughput pipeline.",
            "llm_model_used": "Cohere model (unspecified variant)",
            "scientific_domain": "Materials science (phase-property relationships)",
            "number_of_papers": null,
            "type_of_quantitative_law": "Phase-property relationships (structured associations; supports downstream modeling)",
            "extraction_output_format": "Structured relationship records / tables",
            "validation_method": null,
            "performance_metrics": null,
            "baseline_comparison": null,
            "challenges_limitations": "Full-text and table/figure parsing limitations likely relevant; specifics not provided in this paper's discussion.",
            "requires_human_in_loop": null,
            "fully_automated": null,
            "uuid": "e4348.7",
            "source_info": {
                "paper_title": "Toward Reliable Ad-hoc Scientific Information Extraction: A Case Study on Two Materials Datasets",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "A-Lab autonomous-experiment pipeline",
            "name_full": "An autonomous laboratory for the accelerated synthesis of novel materials (A-Lab)",
            "brief_description": "A closed-loop system that incorporates extracted synthesis recipes into AI-guided experimentation, linking IE outputs to robotic execution and optimization.",
            "citation_title": "An autonomous laboratory for the accelerated synthesis of novel materials",
            "mention_or_use": "mention",
            "method_name": "AI-guided experimentation using extracted recipes",
            "method_description": "Extract synthesis recipes/parameters from literature using information extraction methods and feed those structured recipes into an autonomous experimentation pipeline to plan and run experiments.",
            "llm_model_used": null,
            "scientific_domain": "Materials synthesis and autonomous experimentation",
            "number_of_papers": null,
            "type_of_quantitative_law": "Extraction of procedural and parameter relationships to guide experiments (not explicit symbolic laws)",
            "extraction_output_format": "Structured recipes/parameters integrated with experimental planners",
            "validation_method": "Experimental execution in an autonomous lab (per A-Lab publication)",
            "performance_metrics": null,
            "baseline_comparison": null,
            "challenges_limitations": "Relies on high-quality extraction of protocol parameters; extraction errors propagate to experiments; integration challenges and need for ground-truth validation.",
            "requires_human_in_loop": null,
            "fully_automated": null,
            "uuid": "e4348.8",
            "source_info": {
                "paper_title": "Toward Reliable Ad-hoc Scientific Information Extraction: A Case Study on Two Materials Datasets",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "ToolFormer",
            "name_full": "ToolFormer: Language Models Can Teach Themselves to Use Tools",
            "brief_description": "A method enabling LMs to learn to call external tools (e.g., unit converters) to augment abilities such as unit normalization during extraction.",
            "citation_title": "Toolformer: Language Models Can Teach Themselves to Use Tools",
            "mention_or_use": "mention",
            "method_name": "Toolformer-style tool-augmented LM",
            "method_description": "Train or prompt LMs to detect when external tool calls (e.g., unit conversion APIs, calculators) are needed and invoke those tools to normalize or transform extracted numeric values, addressing unit-mismatch and conversion errors.",
            "llm_model_used": "General LLMs in ToolFormer work (not a single specific model here)",
            "scientific_domain": "General — applicable to scientific information extraction tasks requiring unit conversions and computations",
            "number_of_papers": null,
            "type_of_quantitative_law": "Unit-normalized numerical data extraction (supports downstream law discovery by normalizing values)",
            "extraction_output_format": "Structured numerical values with normalized units",
            "validation_method": null,
            "performance_metrics": null,
            "baseline_comparison": null,
            "challenges_limitations": "Requires reliable tool-integration APIs and model coordination; not directly evaluated in the present study but suggested as a remedy for unit-related errors.",
            "requires_human_in_loop": null,
            "fully_automated": null,
            "uuid": "e4348.9",
            "source_info": {
                "paper_title": "Toward Reliable Ad-hoc Scientific Information Extraction: A Case Study on Two Materials Datasets",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "GPT-4V OCR / vision references",
            "name_full": "Exploring OCR Capabilities of GPT-4V(ision)",
            "brief_description": "Work evaluating GPT-4V's ability to read and extract information from figures and plots, relevant to addressing figure-bound scientific data extraction.",
            "citation_title": "Exploring OCR Capabilities of GPT-4V(ision) : A Quantitative and In-depth Evaluation",
            "mention_or_use": "mention",
            "method_name": "Vision-enabled LLM figure/plot reading (GPT-4V)",
            "method_description": "Evaluate and apply multimodal LLMs (GPT-4V) to read OCR text in figures and to interpret plots so that numerical values presented visually can be extracted—potentially enabling extraction of values like yield strength from curves.",
            "llm_model_used": "GPT-4V (vision-capable GPT-4 variant) as evaluated in cited work",
            "scientific_domain": "General scientific documents with figures/plots (materials science examples discussed)",
            "number_of_papers": null,
            "type_of_quantitative_law": "Extraction of figure-bound numerical data (points on plots), enabling retrieval of experimental numerical values",
            "extraction_output_format": "Numerical values and annotations extracted from images/plots (structured JSON)",
            "validation_method": "Quantitative OCR/vision evaluation in cited work",
            "performance_metrics": null,
            "baseline_comparison": null,
            "challenges_limitations": "Vision models may need specialized tuning to read curves precisely (e.g., read single point from load curve); integrating vision outputs with textual schema alignment is nontrivial.",
            "requires_human_in_loop": null,
            "fully_automated": null,
            "uuid": "e4348.10",
            "source_info": {
                "paper_title": "Toward Reliable Ad-hoc Scientific Information Extraction: A Case Study on Two Materials Datasets",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Collage",
            "name_full": "Collage: Decomposable rapid prototyping for information extraction on scientific PDFs",
            "brief_description": "A system for decomposable rapid prototyping of information extraction on scientific PDFs that can help address PDF parsing and IE pipeline modularity.",
            "citation_title": "Collage: Decomposable rapid prototyping for information extraction on scientific pdfs",
            "mention_or_use": "mention",
            "method_name": "Collage decomposable IE prototyping",
            "method_description": "A decomposable framework to prototype IE components on scientific PDFs (parsing, decomposition, and integration), aimed at improving robustness of PDF parsing and downstream extraction pipelines.",
            "llm_model_used": null,
            "scientific_domain": "Scientific PDFs generally (addresses parsing and IE tooling challenges)",
            "number_of_papers": null,
            "type_of_quantitative_law": "Facilitates reliable extraction of numerical and tabular information from PDFs (prerequisite to law discovery)",
            "extraction_output_format": "Intermediate parsed representations feeding into IE models",
            "validation_method": null,
            "performance_metrics": null,
            "baseline_comparison": null,
            "challenges_limitations": "Helps address parsing but does not itself solve multimodal figure extraction or LM comprehension/hallucination.",
            "requires_human_in_loop": null,
            "fully_automated": null,
            "uuid": "e4348.11",
            "source_info": {
                "paper_title": "Toward Reliable Ad-hoc Scientific Information Extraction: A Case Study on Two Materials Datasets",
                "publication_date_yy_mm": "2024-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Instruct and Extract: Instruction Tuning for On-Demand Information Extraction",
            "rating": 2,
            "sanitized_title": "instruct_and_extract_instruction_tuning_for_ondemand_information_extraction"
        },
        {
            "paper_title": "SciDaSynth: Interactive Structured Knowledge Extraction and Synthesis from Scientific Literature with Large Language Model",
            "rating": 2,
            "sanitized_title": "scidasynth_interactive_structured_knowledge_extraction_and_synthesis_from_scientific_literature_with_large_language_model"
        },
        {
            "paper_title": "Flexible, Model-Agnostic Method for Materials Data Extraction from Text Using General Purpose Language Models",
            "rating": 2,
            "sanitized_title": "flexible_modelagnostic_method_for_materials_data_extraction_from_text_using_general_purpose_language_models"
        },
        {
            "paper_title": "Extracting accurate materials data from research papers with conversational language models and prompt engineering",
            "rating": 2,
            "sanitized_title": "extracting_accurate_materials_data_from_research_papers_with_conversational_language_models_and_prompt_engineering"
        },
        {
            "paper_title": "High-Throughput Extraction of Phase-Property Relationships from Literature Using Natural Language Processing and Large Language Models",
            "rating": 2,
            "sanitized_title": "highthroughput_extraction_of_phaseproperty_relationships_from_literature_using_natural_language_processing_and_large_language_models"
        },
        {
            "paper_title": "Extracting structured seed-mediated gold nanorod growth procedures from scientific text with LLMs",
            "rating": 2,
            "sanitized_title": "extracting_structured_seedmediated_gold_nanorod_growth_procedures_from_scientific_text_with_llms"
        },
        {
            "paper_title": "Toolformer: Language Models Can Teach Themselves to Use Tools",
            "rating": 2,
            "sanitized_title": "toolformer_language_models_can_teach_themselves_to_use_tools"
        },
        {
            "paper_title": "Exploring OCR Capabilities of GPT-4V(ision) : A Quantitative and In-depth Evaluation",
            "rating": 2,
            "sanitized_title": "exploring_ocr_capabilities_of_gpt4vision_a_quantitative_and_indepth_evaluation"
        },
        {
            "paper_title": "An autonomous laboratory for the accelerated synthesis of novel materials",
            "rating": 2,
            "sanitized_title": "an_autonomous_laboratory_for_the_accelerated_synthesis_of_novel_materials"
        },
        {
            "paper_title": "Collage: Decomposable rapid prototyping for information extraction on scientific pdfs",
            "rating": 1,
            "sanitized_title": "collage_decomposable_rapid_prototyping_for_information_extraction_on_scientific_pdfs"
        }
    ],
    "cost": 0.0198005,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Toward Reliable Ad-hoc Scientific Information Extraction: A Case Study on Two Materials Datasets
25 May 2025</p>
<p>Satanu Ghosh satanu.ghosh@unh.edu 
University of New Hampshire</p>
<p>Neal R Brodnik nrbodnik@ucsb.edu 
University of California
Santa Barbara</p>
<p>Carolina Frey cfrey@ucsb.edu 
University of California
Santa Barbara</p>
<p>Collin Holgate holgate@ucsb.edu 
University of California
Santa Barbara</p>
<p>Tresa M Pollock tresap@ucsb.edu 
University of California
Santa Barbara</p>
<p>Samantha Daly samdaly@ucsb.edu 
University of California
Santa Barbara</p>
<p>Samuel Carton samuel.carton@unh.edu 
University of New Hampshire</p>
<p>Toward Reliable Ad-hoc Scientific Information Extraction: A Case Study on Two Materials Datasets
25 May 2025AE36554ED190CDC377E8DFF32B9234A0arXiv:2406.05348v3[cs.CL]
We explore the ability of GPT-4 to perform ad-hoc schema based information extraction from scientific literature.We assess specifically whether it can, with a basic prompting approach, replicate two existing material science datasets, given the manuscripts from which they were originally manually extracted.We employ materials scientists to perform a detailed manual error analysis to assess where the model struggles to faithfully extract the desired information, and draw on their insights to suggest research directions to address this broadly important task.</p>
<p>Introduction</p>
<p>A key use case for large language models in science is ad-hoc schema-based information extraction.In this scenario, a scientist has a specific information need, such as the compressive yield strength of known multi-principal element alloys (MPEAs), and wants to (1) identify papers containing relevant information instances, and (2) comprehensively extract these instances according to a desired schema, which might include secondary information such as testing conditions and synthesis details (Fig 1).</p>
<p>Such datasets are a key element of scientific informatics, transforming information reported inconsistently throughout the literature into a structured format appropriate for use in machine learning.In materials science, such datasets are used in a wide range of applications, including modeldriven materials discovery, where they can be used to train or warm-start surrogate models of material properties which are then incorporated into various kinds of discovery algorithms (Pilania, 2021).This is a challenging extraction task.The materials science literature contains a plethora of experimental results, often spanning multiple decades, in a multitude of different formats with varying physical units.Prior generations of models needed large amounts of data to fine-tune, and were brittle and non-transferable, limiting their practicality for this type of ad-hoc, one-off extraction task.To date, manual annotation is still the standard approach for extraction of this type of unstructured data (Xu et al., 2023b).Contemporary large language models such as GPT-4 (OpenAI, 2023) and Gemini (Team et al., 2023) have the potential to conquer this barrier.With the emergent few-shot capabilities of these models, a scientist can potentially present a LLM with a (1) collection of manuscripts, (2) a schema defining what types of data they want to extract, and (3) a few exemplars of performing this extraction, and receive back a tabular dataset of information extracted from those manuscripts, ready to use for their chosen purpose.This ad-hoc paradigm has been referred to as "on-demand" information extraction (ODIE) (Jiao et al., 2023) and it promises to speed up the search for new materials with unique properties.</p>
<p>We argue that this very ambitious extraction setting should be a key target for LLM-driven scientific information extraction, as it can potentially allow scientists to extract a maximum of clinically relevant data with a minimum of effort, both in materials science and other domains such as biomedicine.Two significant questions, then, are (1) to what extent can contemporary LLMs perform this type of ad-hoc extraction on scientific text, and (2) what are the major barriers impeding their effectiveness?</p>
<p>In this paper, we explore these questions using two manually-extracted material property datasets, one focused on multi-principal element alloys (MPEAs), sometimes known as high-entropy alloys (HEAs) (Borg et al., 2020), and one focused on elemental diffusion in silicate melts (Zhang et al., 2010).We use these existing datasets as sources of schema structure and exemplars, and evaluate whether a representative contemporary LLM (GPT-4) is capable of replicating them given the manuscripts from which they were originally extracted.We perform a detailed error analysis, including annotation by domain experts, to understand where and why the model falls short, to guide further development of LLMs to advance extraction paradigms for materials data.</p>
<p>Generally, we find that the model shows great potential for extracting information described narratively or in the form of a conventionally-formatted table.The majority of errors are attributable to figures, which our current pipeline does not address, and PDF parsing issues.Other sources of error include non-standard table formats, the need for additional postprocessing of extracted values, as well as true reading comprehension errors which could be addressed via improved prompt engineering.All relevant code and data can be found in our github repository1 .</p>
<p>Related work</p>
<p>Information extraction (IE) is a core NLP task with a long history, and one of many which contemporary large language models (LLMs) such as GPT-4 (OpenAI, 2023) and Gemini (Team et al., 2023) are able to perform in a zero-or few-shot setting (Xu et al., 2023a).This few-shot extraction ability has enabled a new extraction paradigm, involving extracting a complex schema with little or no training data, termed "on-demand information extraction" (ODIE) by (Jiao et al., 2023).Our task setting falls into this paradigm.</p>
<p>Zero-and few-shot scientific information extraction forms a major aspect of the excitement over LLMs' potential for accelerating scientific discovery, because of its potential to create structured information from diffuse unstructured data sources such as scientific papers and clinical notes (Hope et al., 2022;Morris, 2023).Ad-hoc extraction from scientific literature is particularly challenging because the amount of manually-annotated data needed to fine-tune a model to perform a particular extraction task would constitute a significant fraction of all the data in existence, obviating the benefit of training such a model.Scientific literature is also highly multimodal, with key information presented in narrative, tabular, and visual formats.SciDaSynth (Wang et al., 2024) approaches the problem with an interactive QA-based interface, evaluating based on a 12-scientist user study.LLM-powered IE has attracted attention in materials science specifically (Olivetti et al., 2020;Xie et al., 2023;Polak and Morgan, 2024).Polak et al. (2024) explore the use of GPT-4 to extract bulk modulus information from sentences extracted from scientific papers, while Yang et al. (2023) uses zero-shot ChatGPT with a verification step to extract band gap information from sentences collected in a dataset by (Dong and Cole, 2022).Walker et al. (2023) fine-tunes GPT-3.5 to extract growth procedures and outcomes for gold nanorods from paper text.Montanelli et al. (2024) uses the Cohere model to extract phase-property relationships from a corpus of full-text materials papers.Finally, the A-Lab (Szymanski et al., 2023) incorporates extracted synthesis recipes into an AIguided experimentation protocol.Our work differs from these recent approaches in seeking to extract a rich, complex schema from non-curated data in one pass with no fine-tuning, instead of narrowlydefined tasks over carefully-curated corpora.This is a highly ambitious and difficult extraction task, but one that places a minimum of burden on the scientist, and we see it as a key milestone for scientific IE.2004-2022 1964-2009 Table 1: Basic statistics for the two datasets, publication year range.For both datasets, a subset of total papers were retrieved, parsed and analyzed in this work.</p>
<p>MPEA Diffusion</p>
<p>Datasets</p>
<p>We experiment with two material properties datasets, one pertaining to multi-principal element alloys (MPEAs) (Borg et al., 2020), and one to elemental diffusion in silicate melts (Zhang et al., 2010).(Miracle and Senkov, 2017;Miracle et al., 2020).</p>
<p>The MPEA dataset used for this analysis specifically aims to record key properties reported for MPEAs derived primarily from the refractory metals (Cr, Hf, Mo, Nb, Ru, Ta, Ti V, W, Zr).These properties include yield strength, which represents the force necessary to make a metal permanently deform, and elongation, which represents the degree to which a material can deform prior to failure.Hardness, which represents the ability of the material resist localized deformation, can be used to estimate the yield strength of a material.Other useful details recorded in the MPEA dataset where available include the concentration of interstitial elements present (i.e.oxygen, nitrogen, and carbon concentrations), the method by which the material was processed , the test temperature, and the method of testing.The MPEA dataset used in this study is an expansion of the dataset published in (Borg et al., 2020), which has been continually updated .Records were taken from experimental literature published in 2004-2022 and are identified by a Digital Object Identifier (doi) that was assigned by the publisher.</p>
<p>Diffusion in Silicate Melts Dataset Our second dataset (Zhang et al., 2010) covers the diffusion (transport of elements) inside silicate glasses and melts (e.g., magma), which affects many processes in geology and materials science.The key value in this dataset is diffusion coefficient, which is often symbolized as D and has units of m 2 /s.The diffusion coefficient is a proportionality constant that describes how rapidly one element is transported within a substance.As one may expect, the value of D depends on what the diffusing element is, the medium (magma) it is moving in (i.e., its composition), and also environmental conditions like temperature, pressure, and how much water is present (which greatly affects magmatic structure).The diversity of magmas found in the earth and the elements being transported within them motivates building and maintaining databases of diffusion coefficients to help elucidate underlying truths.</p>
<p>Outside of its fundamental value, we chose this dataset for our study because (1) diffusion as a process is heavily sensitive to experimental conditions, meaning value extraction is only useful with the associated large quantity of metadata; (2) it is a large dataset, affording sufficient measurements and papers to draw meaningful conclusions; (3) the multi-decade span of publication dates offers exposure to a range of scientific conventions and pdf quality levels from the perspective of typesetting, character recognition, and table format; and (4) it represents a field where updates occur on a decadal basis, thus valuing a tool for automatic extraction of new measurements.</p>
<p>Method</p>
<p>Our extraction pipeline consists of the following steps: (1) retrieving and parsing source PDFs; (2) prompting and response postprocessing; and (3) extracted row alignment.</p>
<p>Retrieving and parsing source PDFs</p>
<p>The vast majority of PDFs available in both datasets are not open-access, and are referred to only by their DOI within each dataset.As a first stage, we manually download as many PDFs as we can easily access, resulting in 128 PDFs of the original 164 for the MPEA dataset, and 55 out of 71 for the diffusion dataset.Our github repository includes the DOIs of the PDFs we downloaded but not, for copyright reasons, the PDFs of these papers.</p>
<p>As of June 2024, none of the major LLM APIs allow programmatic uploading of PDF files.Therefore, to process PDFs at scale, they must be converted to an intermediate readable format.We use GROBID2 to convert paper PDFs to XMLs.When parsed correctly, the XML contains all textual information from the paper, including tables and their respective captions.However, GROBID cannot process figures, leaving any figure-bound information inaccessible by definition.The error analysis presented in Section 6.2 assesses the magnitude of this problem.</p>
<p>Prompting</p>
<p>We experiment with three basic prompting techniques: zero-and one-shot prompting, and LangChain.For each technique, we try both a "complex" schema including all columns from the original dataset, and a reduced "simple" schema including only a subset of column.</p>
<p>Role Setting:</p>
<p>I am a helpful assistant capable of extracting information from text.I will not generate any new tokens... return the information in format of the schema.</p>
<p>Exemplar: {{Prompt instructions}} {{Exemplar XML}} {{Exemplar output JSON}}</p>
<p>Prompt:</p>
<p>Extract all information relating to every High Entropy Alloys (HEA) from the following text: {{Paper XML}} using the following schema: {{JSON schema}}.Return a list of schemas in JSON format for every unique compound.Zero-and one-shot prompting Working with the gpt-4-1106-preview GPT-4 Turbo model variant via the OpenAI API, we use a very basic and general prompting approach, illustrated in Fig. 2. Our goal is to extract all columns from a single full paper XML at once, requesting them in the form of a JSON.In the one-shot condition, the prompt includes a single exemplar, selected randomly to be (Long et al., 2019) and (Rüssel and Wiedenroth, 2004) for the MPEA and diffusion dataset respectively.</p>
<p>We additionally make use both role-setting and task instructions in our prompt.Role-setting has been found to be generally helpful in prompt engineering (Shanahan et al., 2023;Kong et al., 2023), so we set various desired aspects of the LLM's behavior via this mechanism, i.e., extracting information from the given text without generating new tokens and following the provided schema (if any).The task instructions are specific and detailed, customized to each dataset based on expert input and include a schema that should be followed.We add additional context with background knowledge and special instructions.A more complete version of the prompt is available in the appendix (Section A.2.</p>
<p>LangChain LangChain3 is a popular opensource framework for developing solutions that combine LLMs with other tools.Information extraction is one of the many use cases that can be performed with LangChain.We follow the guidelines on their website to perform structured information extraction for long text.In short, the LLM is provided with a schema of various entities and a short description of each entity.In our use case, these entities are the column headers in the two datases.An important distinction of this method from basic prompting methods was chunking the papers into smaller blocks of 2000 tokens, with each block having a 1% overlap with the previous one.</p>
<p>Postprocessing Minimal postprocessing is needed for the model's JSON output, which varies slightly in format from run to run.This includes wrapping JSON-formatted output in ''' 'json' tags.The model also sometimes produces Javascript-style '//' comments explaining its extraction decisions; therefore, we use the jstyleson Python library4 to parse this JSON output rather than the default Python json library.</p>
<p>Extracted row alignment</p>
<p>In order to evaluate our success in replicating the original datasets, including which rows were matched, which original rows were missed, and which extracted rows were hallucinated, we first need to determine which extracted row corresponds with which original dataset row.This is nontrivial, because the columns that define a unique record will vary from paper to paper.One paper might test multiple different compositions, while another might test the same composition under different testing conditions, while a third might perform multiple trials of the same experiment.Given that the model will miss or hallucinate many individual cell values, this becomes a nuanced matching task.</p>
<p>Our approach is to combine domain-specific minimum criteria for a possible match with a greedy matching process within each paper.An extracted row is a potential match for an original row if it shares values for hand-chosen required columns, which we chose to be formula and yield strength for the MPEA dataset and diffusing element and diffusion coefficient for the diffusion dataset.Beyond this minimum, match strength is determined by proportion of shared column values between the two rows, after which rows are greedily paired with each other until no possible pairs remain for a given paper.Paired rows are classified as a 'match', unpaired rows from the original dataset are classified as a 'miss', and unpaired extracted rows are classified as a 'hallucination'.</p>
<p>Automated evaluation</p>
<p>After aligning extracted rows with dataset rows as described above, we report the number of matches and hallucinations in Table 2 along with the recall and precision.For both the datasets, two variations of schema and three types of prompting.</p>
<p>The only difference between simple and complex schema is the number of properties mentioned in each schema.</p>
<p>The raw performance is underwhelming.On MPEA, the model hallucinates roughly as many rows as it matches, and misses twice as many.We are able to match significantly more entries using the simple schema than the complex schema.While using Langchain we extracted more relevant entries, but we can see a drop of precision (more hallucination).On the Diffusion dataset, we are able to match no rows at all for 0-shot and, though the model hallucinates at a proportionally lower rate than for the MPEA dataset.Using Langchain clearly improved the performance on this dataset but the hallucinations also increased significantly.</p>
<p>Table 3 gives a detailed breakdown of extraction performance for related properties (all properties other than the ones used to match the entries), properties of high variance (experimental properties that were subjected to change in a paper) and properties of low variance (experimental constants).The two different schemas present the two different notions of information extraction: maximal and minimal.Tasks that require less exploration of contextually related information extraction can follow the minimal approach.From the results, we can see that the extraction of fewer properties aids the model, as it allows more useful extractions with higher recall (on the primary property).The precision value gives us an idea about the hallucinated extractions, and the simpler schema resulted in lesser hallucination compared to the complex schema.The 0-shot prompting did not perform well for low-variance properties, but performed much better at extracting high-variance properties.Langchain performs better than 0-shot in most cases but outperformed by the 1-shot, considering that we cannot draw any conclusion from the Diffusion dataset because the number of extractions are so low.</p>
<p>Manual error analysis</p>
<p>To elucidate what formats key information occurs in, and under what circumstances the model struggles to extract that information, two materials science experts hand-annotated a sample of papers and attempted LLM extractions from each dataset, using the 1-shot prompting variant.60 papers' worth of attempted extractions were annotated from the MPEA dataset, constituting a total of 428 rows, and 40 papers from the Diffusion dataset, constituting   1714 rows.Both sets were sampled randomly from the set of retrieved and parsed papers for which extractions were attempted.Each row was labeled for the format in which the key information occurred (table, figure, narrative text, etc).Each failure, either missing or hallucinated rows, was annotated for its failure reason, to the best of the annotator's ability to diagnose the model's behavior by looking at the PDF, parsed XML, and extracted JSON.</p>
<p>In any given paper, different values will be reported in different forms, such as yield strengths in table and testing temperature in narrative text.Our annotators were instructed to note the format of the majority of the "key information" for each paper, and based their failure evaluations based on that information.Figure 3A shows that, while a plurality of relevant MPEA data is located in tables, a significant percentage of the data is located in figures, which is inaccessible to any text-only extraction method.A significant percentage of relevant information is also located in written narrative form.Looking at the distribution of data format for rows the LLM failed to extract, we can see that it trivially misses 100% of figure-formatted data, 2/3 of table-formatted data, and a relatively smaller 1/3 of narratively-described data.Figure 3B shows the same numbers for the diffusion dataset, where the vast majority of relevant data was reported in tables, which the LLM in almost every case failed to extract.</p>
<p>Data format</p>
<p>Error analysis</p>
<p>Figure 4 summarizes annotated causes of error between the two datasets.It includes a separate plot for misses and matches (covering all the original rows in the dataset covered by the annotator), and one for hallucinations, indicating extracted rows that could not be matched to an original dataset row.Reported error types include XML parsing errors, where GROBID failed to extract key information from the PDF; figure errors where key information was stored in figures invisible to the model; dataset errors where the original dataset had missing or incorrect values which the model successfully extracted; generalized comprehension errors where the LM overlooked key information; unit conversion errors where values were reported in a different unit from that recorded in the dataset; confusion errors where the model confused one value for another; secondary source, where the model picked up values from other sources referred to in passing; and alignment errors where the alignment algorithm described above failed to match extracted to original rows.</p>
<p>MPEA dataset.Figure 4A summarizes error causes reasons for MPEA original dataset rows (both matched and missed).For the 368 groundtruth entries evaluated from the MPEA dataset, a total of 107 were matched (29.5%).Of the missed 70.5%, major sources of error were model comprehension problems (18.5%),key information being presented in figures (30.9%), XML parsing problems (13.2%), and errors in the original dataset (6%).Thus for this dataset the model's performance on the extraction task was hindered more by the form of data presentation than by limitations in comprehension ability.The majority of annotated entries in the MPEA set that were missed by the model (160 of 256, or 62.5%) were entries that were located in either a figure or a table that did not parse properly into the XML input.For both of these cases, it is unreasonable to expect the extraction of information that was never presented to the model in the first place, so even though these database values could not be recreated, the failure does not provide much insight from a text comprehension perspective.For the 67 missed MPEA entries that were comprehension-related (annotators manually determined that relevant information was present in text or a table that properly parsed into XML), 2/3 of them were values that appeared only in tables.Cases of missed values in a table often occurred with tables that were relatively complex in structure, such as in Senkov et al. ( 2020), where information on yield strength is merged in with information on flow stress and categorized by temperature and relative contribution from material constituent structure-making it difficult to directly extract the yield strength as demanded by the dataset schema.</p>
<p>For values missed in the text itself, some of these errors can be attributed to cases where the value was mentioned in discussion around specific figure or table, making the text itself more difficult to interpret.For example, in Hu et al. (2019), the discussion of MPEA yield strength in the text occurs in the excerpt "[t]he maximum compressive strength is... for the NbZrTi and NbHfZrTi alloys, respectively," which is stated as part of a discussion around a related figure showing that this "maximum compressive strength" corresponds to the yield strength.However, without the associated figure, the text alone is difficult to properly interpret.Still, there were apparent cases of comprehension failure, such as in (Tseng et al., 2018), where, for one of their target test temperatures, the authors plainly state "the compressive yield strength was 1005 MPa," a point which the model failed to detect.</p>
<p>Figure 4B summarizes reasons for hallucinated rows in the MPEA dataset.The most common form of hallucination on the MPEA dataset were generalized miscomprehensions (59 cases, 52.7%), often cases where the model identified a valid composition but none of the other relevant properties.The next most common form of reported hallucination (27, 25.6%) were not actually hallucinations, but rather the model finding accurate compositions and property values that were not collected in the construction of the original dataset.The third most common type of "hallucination" were cases where composition/yield strength combinations reported in prior work were mentioned in the paper at hand, and then extracted by the model.XML parsing was not a major source of error for the MPEA dataset, where most of the included papers are relatively recent.</p>
<p>Diffusion dataset Figure 4C summarizes the percentage of expert-annotated error reasons for diffusion dataset rows (both matched and missed).</p>
<p>The overall performance of the model was distinctly different for this dataset.Out of a total of 1714 dataset entries, only 23 entries were able to be automatically aligned with original records, and 15 of these were missing so many other values as to be effectively useless.Thus, extraction almost completely failed for this model.By far the largest source of error was XML parsing issues, which were responsible for 965 misses.Errors in XML parsing were often related to the age of the publication.Many of the older diffusion publications in the set have outdated typesetting practices and and non-standard table structures with multiply-merged cells, both of which created numerous issues in the model input information.</p>
<p>After XML issues, the next largest error source was generalized model comprehension, which was responsible for 387 (24.2%) of misses.The model had significant difficulty correctly reporting textual values like melt descriptions, as these often required domain knowledge connections that were difficult to make in the context of the prompt.For example, Alletti et al. (2007) studied diffusion in a Hawaiitic melt, but during database compilation, this 'Hawaiitic' description was instead reported as 'basalt' as part of a standardizing effort by Zhang et al. (2010) to reduce the number of unique mineral terms in the database.When the model retrieved summarizing values from the abstract of Alletti et al. (2007), the melt was labeled as "Hawaiitic melt from Mt. Etna", which is factually correct, but does not align with the database description of basalt.Other key forms of miscomprehension included misreading complex table formats like in the MPEA dataset, and erroneously preferring narratively-presented information to tabular information.In processing (MacKenzie and Canil, 2008), for instance, the model seems to have extracted values presented in the abstract rather than in the centerpiece tables.</p>
<p>Finally, unit mismatches were a significant source of error, which were related to 17.1% of all rows.When the database was compiled by Zhang et al. (2010), the standard units of m 2 /s were chosen for the diffusion coefficient, but it is not uncommon for this coefficient to also be reported in cm 2 /s, µm 2 /s, or even as a logarithm (log(D)), depending on the context of the investigation.Unitbased comprehension issues often involved failure to report diffusion coefficient values in the correct units, and failure to correctly convert logarithmform diffusion coefficients.Without access to an external conversion tool, it is unreasonable to expect the LLM to perform these unit conversions.Figure 4D summarizes hallucinated rows for the diffusion dataset.Of the 203 reported hallucinations reported on the annotated portion of the Diffusion dataset, 109 (53.6%) were annotated as comprehension issues, including "confusion" instances where the model confused the desired value for something else.An example is (Roselieb and Jambon, 2002), where it incorrectly took Do values from the abstract.Another reported type hallucination was the retrieval of simplified or summarizing values.The remaining hallucinations were a combination of XML issues, assignment comprehension issues, and entries not collected during dataset construction.</p>
<p>Discussion</p>
<p>The manual error analysis identifies several major barriers to effective ad-hoc schema-based information extraction from scientific articles.PDF parsing.PDF parsing is a problem (Gururaja et al., 2024) for older PDFs which, for some areas such as geology and certain subfields of materials science, can still contain valuable information worth extracting.This suggest a need for native LM support for PDFs, which, while present in major models such as Claude and ChatGPT via their interfaces, is not yet available in programmatic API form at the time of writing.</p>
<p>Figure comprehension.</p>
<p>The MPEA dataset demonstrates that a considerable amount of valuable quantitative scientific information is presented visually rather than in tabular form, suggesting a need for vision-language models capable of reading values from plots and figures.For tasks like determining yield strength from a load curve, a vision model would need to be able to recognize the curve itself and accurately read a singular specific point off of that curve, which may require specialized tuning.Recent work like Shi et al. (2023) provides a starting point for this.</p>
<p>Unit conversion.</p>
<p>A characteristic problem in the diffusion dataset is the presentation of results in units like log(m 2 /s) or cm 2 /s when the schema requires m 2 /s.A LM capable of normalizing across these different units would need access to external tools capable of doing so, like ToolFormer (Schick et al., 2023).</p>
<p>Table comprehension</p>
<p>. By far the most common format for experimental results is tables, and table miscomprehensions, especially those associated with complex or non-standard table formats, formed the bulk of true comprehension errors made by the model on both datasets.Thus, a key research direction needs to be improved table comprehension specifically.</p>
<p>Narrative context for tabular information.While tabular data is typically (though not always) the centerpiece of a given paper, narrative information provides important context for understanding it.Constant values such as the temperature at which experiments were run is often presented narratively (which we want to extract), and selected values from a given table are often summarized narratively (which we don't want to extract).Secondarilyreported values from prior work also falls into this category, as it is often presented as context for the primary set of results.Thus, another important research direction toward ad-hoc scientific information extraction is to correctly parse the relationship between tabular and narratively-presented data, perhaps by explicitly treating tabular data as higher priority than narrative data when performing holistic extraction.</p>
<p>Deeper schemas.There are also significant challenges associated with the use of scientific terminology across different fields.This is particularly apparent for the diffusion dataset, which represents a significant effort by an experienced geologist to compile silicate diffusion information and present it in a form that translates across disciplines.The result of this effort shows through particularly in the melt and experiment descriptions in the compiled dataset, both of which have been standardized as much as possible.For an IE model to accurately recreate these types of standardized descriptions, it would need to understand the full schema asso-ciated with the standardization, and also be sufficiently flexible to generate the unique tokens associated with these schema labels, even when those tokens do not occur within the text itself.Creating a role and prompt for the model that permits this flexibility while also mitigating hallucination is a nontrivial task.</p>
<p>Conclusion</p>
<p>We perform an analysis of whether a representative contemporary large language model, GPT-4, is capable of performing complex ad-hoc scientific information extraction sufficient for replicating two materials property datasets.A detailed manual expert error analysis shows that, while the model shows promise for narrative and tabular data, there are significant barriers in both modality and comprehension ability that prevent the model from being adequate to the task using a baseline one-shot prompting approach.Nevertheless, our initial results on this only-recently-recognized information extraction paradigm and our careful analysis will help guide further work in this area.</p>
<p>Limitations</p>
<p>Our goal in this paper is to study the limitations of a representative contemporary large language model (GPT-4) in performing ad-hoc schema-based information extraction.Our analysis approach has its own limitations, however, including the assumption of representativeness in the one model we evaluate and the two materials datasets we evaluate it on.GPT-4's behavior may not generalize to other models, nor do other manually-curated scientific datasets necessarily conform to the structure of the two that we study.There will need to be extensive further experimentation with a wider variety of model and datasets to fully characterize the research frontier for this topic.</p>
<p>A Appendix</p>
<p>A.1 Dataset column descriptions Table 4 describes each major column in each dataset.</p>
<p>A.2 Full prompt</p>
<p>The prompts used for both extraction tasks are given below.</p>
<p>A.2.1 Role setting for both tasks I am a helpful assistant capable of extracting information from text.I will not generate any new tokens, only extract tokens containing information from the text you provide.If provided with a schema, then I will follow it and return the information in format of the schema.</p>
<p>A.2.2 Prompt for MPEA extraction</p>
<p>Extract all information relating to every High Entropy Alloys (HEA) from the following text: {exemplar text / other text to extract from} using the following schema: schema.Return a list of schemas in JSON format for every unique compound.</p>
<p>Additional context: HEA composition sometimes has variable ratio denoted by 'x', if so, then replace x with a float or integer as applicable by the information in the paper.</p>
<p>Restriction: Do not extract any tokens if a HEA material does not have a property mentioned in the schema.If a property is not available then return 'No information'.Do not violate the schema.</p>
<p>A.2.3 Schema for MPEA extraction</p>
<p>{ "high entropy alloy formula": {"type": "string"}, "microstructure": {"type": "string"}, "processing method": {"type": "string"}, "BCC/FCC/other": {"type": "string"}, "grain size": {"type": "float"}, "experimental density": {"type": "float"}, "hardness": {"type": "float"}, "type of test": {"type": "string"}, "test temperature": {"type": "float"}, "yield strength": {"type": "float"}, "ultimate tensile strength": {"type": "float"}, "elongation": {"type": "float"}, "elongation plastic": {"type": "float"}, "experimental young modulus": {"type": "float"}, "oxygen content": {"type": "float"}, "nitrogen content": {"type": "float"}, "carbon content": {"type": "float"} }</p>
<p>A.2.4 Exemplar output for MPEA extraction</p>
<p>One entry from the one-shot output exemplar that we used is given below.The actual output contains 5 of these entries in a JString list.</p>
<p>{ "high entropy alloy formula": 'NbMoTaWVCr', "microstructure":'BCC+Laves+Sec.',"processing method":'POWDER', "BCC\/FCC\/other":'other', "grain size": 0.54, "experimental density": 'No information', "hardness": 1072.0,"type of test":'C', "test temperature": 25.0, "yield strength": 'No information', "ultimate tensile strength": 'No information', "elongation": 'No information', "elongation plastic": 'No information', "experimental young modulus": 'No information', "oxygen content": 7946.0,"nitrogen content": 'No information', "carbon content": 'No information' }</p>
<p>A.2.5 Prompt for Diffusion extraction</p>
<p>Extract all information relating to diffusion of elements into a silicate melt from the following text: {one shot text / other text to extract from} using the following schema: {schema}.Return a list of schemas in JSON format for every unique compound.</p>
<p>Additional context: there will generally be one or multiple liquids, and one or more elements that move through that liquid at a certain speed.That speed is called the "diffusivity" or the "diffusion coefficient" D(m 2/s).Each element will have a unique diffusivity in each melt and temperature and pressure, so it is important to keep track of every combination.</p>
<p>Restriction: Do not extract any tokens if no relevant property is present as mentioned in the schema.If a property is not available then return 'No information'.Do not violate the schema.</p>
<p>A.2.6 Schema for Diffusion extraction</p>
<p>A.2.7 Exemplar output for Diffusion extraction</p>
<p>The following is one of 21 entries in the one-shot exemplar output.The actual output is a list of such entries given to the model as a JString.</p>
<p>{</p>
<p>"melt": "NCMAS6", "diffusing species": "Fe", "type of experiment": "electrochemistry", "test temperature": 1573.15,"pressure": "No information", "diffusivity": 1.35e-07, "SiO2": 80.6793201360426, "TiO2": "No information", "Al2O3": 0.0, "FeOt": "No information", "MnO": "No information", "MgO": 0.0, "CaO": 14.11921335907197, "Na2O": 5.201466504885413, "K2O": "No information", "P2O5": "No information", "H2Ot": "No information" }</p>
<p>A.2.8 Exemplar text for both tasks</p>
<p>The text is too long to be presented here but can be viewed in (Long et al., 2019) (MPEA) and (Rüssel and Wiedenroth, 2004) (Diffusion).Input text comprised of the abstract along with other sections and tables (including their captions) sequentially as present in the paper.</p>
<p>A.3 Illustrative examples of errors</p>
<p>As mentioned in the discussion section we found errors occurring primarily due to some reasons like table comprehension, text comprehension, groundtruth information present in plots/images, unit conversion problems, poor XML parsing, simplification of information, and partial information extraction from secondary sources.We wanted to show what those problems look like so in the subsections below we will give an example for each of the errors.We also show an example of an instance when the model was actually correct, in-spite not being matched with the human annotator.</p>
<p>A.3.1 Table comprehension</p>
<p>In Figure 6</p>
<p>A.3.2 Text comprehension</p>
<p>The text in Figure 7 is a paragraph of text taken from Couzinié et al. (2015), where the model could not understand that the text is mentioning information about yield strength.</p>
<p>A.3.3 Information in plots/images</p>
<p>A lot of missed extraction were due to the information being present in plots.As the XML parsing cannot process the image this information is  skipped.In Figure 8, we can see such a plot where the information about the yield strength was present in the plot (Chen et al., 2018).</p>
<p>A.3.4 Unit conversion</p>
<p>In multiple situation of the diffusion dataset we saw that the model extracted the correct information as present in the paper but as the units are not converted as the gold-extractions therefore we could not match them.An example this error can be found in Figure 9 from Zhang et al. (1989).</p>
<p>A.3.5 XML parsing</p>
<p>XML parsing of some older papers from the diffusion dataset was not of good quality and this contributed to some failed extraction.One such example can be found in Figure 10 from Baker et al. (2002).The XML created from the PDF file was not parsed properly and the actual values in the cells were just missed during the parsing process.The actual table is provided in Figure 11.</p>
<p>A.4 Simplification</p>
<p>In Figure 12, we can see the abstract from Balcone-Boissard et al. (2009), that mentions different val-  ues of diffusion rates at different temperatures.The model extracted all the diffusion rates correctly but missed the varying temperatures associated with them and associated them with one temperature value, thereby simplifying the text.</p>
<p>A.5 Secondary source</p>
<p>In Lilensten et al. (2018), we can can find values of yield strength reported from other papers that were  reviewed by the author.The authors reported these values as a summary of the entire paper without going into the details and that can be seen in Figure 13.We considered these extraction as errors as they are from secondary sources.In the given example the model extracted the value 880 MPa along with the generic formula of TiZrNbHf as mentioned in the text without any other information.</p>
<p>A.6 Actually correct</p>
<p>In this example taken from Ge et al. (2020) the model extracted correct yield strength value from table (Figure 14) and formula as reported.The gold extractions reported by the expert had rounded off yield strength values and the formula was reported as ratio of elements, leading to matching failure.</p>
<p>Figure 1 :
1
Figure 1: Extracting large-scale structured data from scientific literature should be as simple as specifying a schema, a corpus of manuscripts, and a few exemplars, and letting the LLM perform the extraction.</p>
<p>Additional context: HEA composition sometimes have variable ratio... not available then return 'No information'.Restriction: Do not extract any tokens if a HEA... not available then return 'No information'.Do not violate the schema.</p>
<p>Figure 2 :
2
Figure 2: Abbreviated one-shot prompt.The prompt begins with role-setting, includes a single exemplar with prompt instructions, then repeated prompt instructions with additional context and clarifications.</p>
<p>Figure 3 :
3
Figure 3: Counts of rows with key information in different formats.</p>
<p>Figure 3
3
Figure 3 summarizes the data format of key information in both datasets, including both original rows in the dataset and rows the model failed to extract.Possible data formats include table, figures, narrative text, calculated, meaning manually calculated by the creator of the dataset from other information within the paper, and other, a miscellaneous category.Figure3Ashows that, while a plurality of relevant MPEA data is located in tables, a significant percentage of the data is located in figures, which is inaccessible to any text-only extraction method.A significant percentage of relevant information is also located in written narrative form.Looking at the distribution of data format for rows the LLM failed to extract, we can see that it trivially misses 100% of figure-formatted data, 2/3 of table-formatted data, and a relatively smaller 1/3 of narratively-described data.Figure3Bshows the same numbers for the diffusion dataset, where the vast majority of relevant data was reported in tables, which the LLM in almost every case failed to extract.</p>
<p>Figure 4 :
4
Figure 4: Proportions of differing error reasons, divided by error type (missed row vs. hallucination) and dataset.</p>
<p>Figure 5 :
5
Figure 5: Plot to visualize the variance of properties found in two datasets.High variance means properties that are altered in experiments and low variance show experimental constants.</p>
<p>taken fromSenkov et al. (2018) we found that the model was able to extract the yield strength of three out of eight yield strength value even when the caption clearly mentioned which column corresponds to the true yield strength.</p>
<p>Figure 6 :
6
Figure 6: This is an example table that the model failed to comprehend even when the XML parsing was perfect.</p>
<p>Figure 7 :
7
Figure 7: This image shows the paragraph which the model failed to comprehend and did not extract the yield strength value (highlighted text).</p>
<p>Figure 8 :
8
Figure 8: Figure of a plot that contains information about the relationship between yield strength and temperature.</p>
<p>Figure 9 :
9
Figure 9: Example of atable with failed extraction due to unit conversion error.The table presented rate of diffusion in µ/s 2 whereas the gold extraction where in m/s 2 .</p>
<p>Figure 10 :
10
Figure 10: The given XML text was the parsed text from the PDF file.</p>
<p>Figure 11 :
11
Figure 11:Table that was parsed incorrectly leading to information loss.</p>
<p>Figure 12 :
12
Figure 12: Example of text simplification performed by the model.</p>
<p>Figure 13 :
13
Figure 13: An example of secondary information that was collected by the model without a detailed context around it.</p>
<p>Figure 14 :
14
Figure 14: Model extracted values from this table as found in the paper.</p>
<p>Table 1
1reports basic statistics of eachdataset, including the number of unique papers,number of unique records (since a single paper canyield multiple records), and publication year range.Table 4 describes the individual columns in eachdataset.MPEA Mechanical Properties Dataset Multi-principal element alloys (MPEAs) are a new classof chemically complex metallic alloys of interestfor applications in energy, defense, and transporta-tion, distinguished from conventional alloys by hav-ing relatively even proportions of many elements</p>
<p>Table 3 :
3
Extraction results for both the datasets can be found in this table.Recall and precision are calculated based on the matched entries found in Table.2 (if no match is found in required properties, then we cannot report any other results).</p>
<p>Table 4 :
4
Descriptions of individual columns in MPEA and diffusion datasets.</p>
<p>https://github.com/SatanuG/ad_hoc_ information_extraction
https://github.com/kermitt2/grobid
https://python.langchain.com/v0.2/docs/ introduction/
https://github.com/linjackson78/jstyleson
AcknowledgmentsThis work was supported by the OpenAI Researcher Access Program.
Halogen diffusion in a basaltic melt. Marina Alletti, Don R Baker, Carmela Freda, Geochimica et Cosmochimica Acta. 71142007</p>
<p>The effect of halogens on zr diffusion and zircon dissolution in hydrous metaluminous granitic melts. Don R Baker, Aida Conte, Carmela Freda, Luisa Ottolini, Contributions to Mineralogy and Petrology. 14262002</p>
<p>Benoît Villemant, and Georges Boudon. 2009. F and cl diffusion in phonolitic melts: Influence of the na/k ratio. Hélène Balcone-Boissard, Baker, Chemical Geology. 2631-4</p>
<p>Expanded dataset of mechanical properties and observed phases of multi-principal element alloys. K H Christopher, Carolina Borg, Jasper Frey, Tresa M Moh, Stéphane Pollock, Daniel B Gorsse, Oleg N Miracle, Bryce Senkov, James E Meredig, Saal, 10.1038/s41597-020-00768-9Scientific Data. 714302020Nature Publishing Group: 1 Publisher</p>
<p>Contribution of lattice distortion to solid solution strengthening in a series of refractory high entropy alloys. Chen, Kauffmann, I-C Laube, R Choi, Schwaiger, Huang, Lichtenberg, Müller, H-J Gorr, Christ, Metallurgical and Materials Transactions A. 492018</p>
<p>On the room temperature deformation mechanisms of a tizrhfnbta refractory high-entropy alloy. J-Ph Couzinié, Lilensten, Champion, Dirras, Perrière, Guillot, Materials Science and Engineering: A. 6452015</p>
<p>Autogenerated database of semiconductor band gaps using ChemDataExtractor. Qingyang Dong, Jacqueline M Cole, 10.1038/s41597-022-01294-6Number: 1 Publisher. Nature Publishing Group20229193Scientific Data</p>
<p>Effects of al addition on the microstructures and properties of monbtativ refractory high entropy alloy. Shaofan Ge, Huameng Fu, Long Zhang, Huahai Mao, Hong Li, Aimin Wang, Weirong Li, Haifeng Zhang, Materials Science and Engineering: A. 7841392752020</p>
<p>Sireesh Gururaja, Yueheng Zhang, Guannan Tang, Tianhao Zhang, Kevin Murphy, Yu-Tsen Yi, Junwon Seo, Anthony Rollett, Emma Strubell, arXiv:2410.23478Collage: Decomposable rapid prototyping for information extraction on scientific pdfs. 2024arXiv preprint</p>
<p>A Computational Inflection for Scientific Discovery. Tom Hope, Doug Downey, Oren Etzioni, Daniel S Weld, Eric Horvitz, arXiv:2205.02007arXiv:2205.02007 [cs] type: article2022Technical Report</p>
<p>Microstructure and mechanical properties of nbzrti and nbhfzrti alloys. Yu-Min Hu, Xiao-Dong Liu, Na-Na Guo, Liang Wang, Yan-Qing Su, Jing-Jie Guo, Rare Metals. 382019</p>
<p>Instruct and Extract: Instruction Tuning for On-Demand Information Extraction. Yizhu Jiao, Ming Zhong, Sha Li, Ruining Zhao, Siru Ouyang, Ji Heng, Jiawei Han, ArXiv:2310.160402023</p>
<p>Better Zero-Shot Reasoning with Role-Play Prompting. Aobo Kong, Shiwan Zhao, Hao Chen, Qicheng Li, Yong Qin, Ruiqi Sun, Xin Zhou, 10.48550/arXiv.2308.07702ArXiv:2308.077022023</p>
<p>Study of a bcc multi-principal element alloy: Tensile and simple shear properties and underlying deformation mechanisms. J-P Lilensten, Couzinié, Perrière, Clément Hocini, Keller, Dirras, Guillot, Acta Materialia. 1422018</p>
<p>A fine-grained nbmotawvcr refractory high-entropy alloy with ultra-high strength: Microstructural evolution and mechanical properties. Yan Long, Xiaobiao Liang, Kai Su, Haiyan Peng, Xiaozhen Li, Journal of Alloys and Compounds. 7802019</p>
<p>Volatile heavy metal mobility in silicate liquids: Implications for volcanic degassing and eruption prediction. Jason M Mackenzie, Dante Canil, 10.1016/j.epsl.2008.03.005Earth and Planetary Science Letters. 26932008</p>
<p>A critical review of high entropy alloys and related concepts. D B Miracle, O N Senkov, 10.1016/j.actamat.2016.08.081Acta Materialia. 1222017</p>
<p>Refractory high entropy superalloys (RSAs). B Daniel, Ming-Hung Miracle, Oleg N Tsai, Vishal Senkov, Rajarshi Soni, Banerjee, 10.1016/j.scriptamat.2020.06.048Scripta Materialia. 1872020</p>
<p>High-Throughput Extraction of Phase-Property Relationships from Literature Using Natural Language Processing and Large Language Models. Integrating Materials and Manufacturing Innovation. Luca Montanelli, Vineeth Venugopal, Elsa A Olivetti, Marat I Latypov, 10.1007/s40192-024-00344-82024</p>
<p>Scientists' Perspectives on the Potential for Generative AI in their Fields. Meredith Ringel, Morris , ArXiv:2304.014202023</p>
<p>Data-driven materials research enabled by natural language processing and information extraction. Elsa A Olivetti, Jacqueline M Cole, Edward Kim, Olga Kononova, Gerbrand Ceder, Thomas , Yong-Jin Han, Anna M Hiszpanski, 10.1063/5.0021106Applied Physics Reviews. 74413172020</p>
<p>. 10.48550/arXiv.2303.08774ArXiv:2303.087742023OpenAIGPT-4 Technical Report</p>
<p>Machine learning in materials science: From explainable predictions to autonomous design. Ghanshyam Pilania, 10.1016/j.commatsci.2021.110360Computational Materials Science. 1931103602021</p>
<p>Flexible, Model-Agnostic Method for Materials Data Extraction from Text Using General Purpose Language Models. P Maciej, Shrey Polak, Anna Modi, Jinming Latosinska, Ching-Wen Zhang, Shanonan Wang, Ayan Deep Wang, Dane Hazra, Morgan, 10.48550/arXiv.2302.04914ArXiv:2302.049142024cond-mat</p>
<p>Extracting accurate materials data from research papers with conversational language models and prompt engineering. P Maciej, Dane Polak, Morgan, Nature Communications. 15115692024</p>
<p>Tracer diffusion of Mg, Ca, Sr, and Ba in Na-aluminosilicate melts. Knut Roselieb, Albert Jambon, 10.1016/S0016-7037(01)00754-266..109RGeochimica et Cosmochimica Acta. 662002. 2002GeCoAADS Bibcode</p>
<p>The effect of glass composition on the thermodynamics of the fe2+/fe3+ equilibrium and the iron diffusivity in na2o/mgo/cao/al2o3/sio2 melts. Christian Rüssel, Achim Wiedenroth, Chemical Geology. 2131-32004</p>
<p>Toolformer: Language Models Can Teach Themselves to Use Tools. Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, Thomas Scialom, 10.48550/arXiv.2302.04761ArXiv:2302.047612023</p>
<p>Temperature dependent deformation behavior and strengthening mechanisms in a low density refractory high entropy alloy al10nb15ta5ti30zr40. On, J-P Senkov, S I Couzinie, Rao, Soni, Banerjee, 2020100627Materialia, 9</p>
<p>Effect of cold deformation and annealing on the microstructure and tensile properties of a hfnbtatizr refractory high entropy alloy. Metallurgical and Materials Transactions A. On, Senkov, Al Pilchak, Semiatin, 201849</p>
<p>Role play with large language models. Murray Shanahan, Kyle Mcdonell, Laria Reynolds, 10.1038/s41586-023-06647-8Nature. 62379872023Nature Publishing Group</p>
<p>Exploring OCR Capabilities of GPT-4V(ision) : A Quantitative and In-depth Evaluation. Yongxin Shi, Dezhi Peng, Wenhui Liao, Zening Lin, Xinhong Chen, Chongyu Liu, Yuyi Zhang, Lianwen Jin, ArXiv:2310.168092023</p>
<p>An autonomous laboratory for the accelerated synthesis of novel materials. Nathan J Szymanski, Bernardus Rendy, Yuxing Fei, Rishi E Kumar, Tanjin He, David Milsted, Matthew J Mcdermott, Max Gallant, Ekin Dogus Cubuk, Amil Merchant, Haegyeom Kim, Anubhav Jain, Christopher J Bartel, Kristin Persson, Yan Zeng, Gerbrand Ceder, 10.1038/s41586-023-06734-wNature. 62479902023Nature Publishing Group</p>
<p>Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, arXiv:2312.11805Anja Hauth, and others. 2023. Gemini: a family of highly capable multimodal models. arXiv preprint</p>
<p>Effects of mo, nb, ta, ti, and zr on mechanical properties of equiatomic hf-mo-nb-ta-ti-zr alloys. Ko-Kai Tseng, Chien-Chang Juan, Shuen Tso, Hsuan-Chu Chen, Che-Wei Tsai, Jien-Wei Yeh, Entropy. 211152018</p>
<p>Extracting structured seedmediated gold nanorod growth procedures from scientific text with LLMs. Nicholas Walker, Sanghoon Lee, John Dagdelen, Kevin Cruse, Samuel Gleason, Alexander Dunn, Gerbrand Ceder, A Paul Alivisatos, Kristin A Persson, Anubhav Jain, 10.1039/D3DD00019BDigital Discovery. 262023Royal Society of Chemistry</p>
<p>SciDaSynth: Interactive Structured Knowledge Extraction and Synthesis from Scientific Literature with Large Language Model. Xingbo Wang, Samantha L Huey, Rui Sheng, Saurabh Mehta, Fei Wang, 10.48550/arXiv.2404.13765ArXiv:2404.137652024</p>
<p>Tong Xie, Yuwei Wan, Wei Huang, Yufei Zhou, Yixuan Liu, Qingyuan Linghu, Shaozhou Wang, Chunyu Kit, Clara Grazian, Wenjie Zhang, Bram Hoex, ArXiv:2304.02213Large Language Models as Master Key: Unlocking the Secrets of Materials Science with GPT. 2023</p>
<p>Large Language Models for Generative Information Extraction: A Survey. Derong Xu, Wei Chen, Wenjun Peng, Chao Zhang, Tong Xu, Xiangyu Zhao, Xian Wu, Yefeng Zheng, Enhong Chen, 10.48550/arXiv.2312.17617ArXiv:2312.176172023a</p>
<p>Small data machine learning in materials science. Pengcheng Xu, Xiaobo Ji, Minjie Li, Wencong Lu, 10.1038/s41524-023-01000-znpj Computational Materials. 912023bNature Publishing Group</p>
<p>Accurate Prediction of Experimental Band Gaps from Large Language Model-Based Data Extraction. J Samuel, Shutong Yang, Subhashini Li, Vahe Venugopalan, Muratahan Tshitoyan, Aykol, 2023Amil Merchant, Ekin Dogus Cubuk, and Gowoon Cheon</p>
<p>Diffusion Data in Silicate Melts. Youxue Zhang, Huaiwei Ni, Yang Chen, 10.2138/rmg.2010.72.8Reviews in Mineralogy and Geochemistry. 7212010</p>
<p>Diffusive crystal dissolution. Youxue Zhang, David Walker, Charles E Lesher, Contributions to Mineralogy and Petrology. 1021989</p>            </div>
        </div>

    </div>
</body>
</html>
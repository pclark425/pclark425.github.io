<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4555 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4555</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4555</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-100.html">extraction-schema-100</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <p><strong>Paper ID:</strong> paper-39983a80f111f7f6e793f02c5725a14bca76b32d</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/39983a80f111f7f6e793f02c5725a14bca76b32d" target="_blank">The AI Scientist-v2: Workshop-Level Automated Scientific Discovery via Agentic Tree Search</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> The AI Scientist-v2 is introduced, an end-to-end agentic system capable of producing the first entirely AI generated peer-review-accepted workshop paper, marking the first instance of a fully AI-generated paper successfully navigating a peer review.</p>
                <p><strong>Paper Abstract:</strong> AI is increasingly playing a pivotal role in transforming how scientific discoveries are made. We introduce The AI Scientist-v2, an end-to-end agentic system capable of producing the first entirely AI generated peer-review-accepted workshop paper. This system iteratively formulates scientific hypotheses, designs and executes experiments, analyzes and visualizes data, and autonomously authors scientific manuscripts. Compared to its predecessor (v1, Lu et al., 2024 arXiv:2408.06292), The AI Scientist-v2 eliminates the reliance on human-authored code templates, generalizes effectively across diverse machine learning domains, and leverages a novel progressive agentic tree-search methodology managed by a dedicated experiment manager agent. Additionally, we enhance the AI reviewer component by integrating a Vision-Language Model (VLM) feedback loop for iterative refinement of content and aesthetics of the figures. We evaluated The AI Scientist-v2 by submitting three fully autonomous manuscripts to a peer-reviewed ICLR workshop. Notably, one manuscript achieved high enough scores to exceed the average human acceptance threshold, marking the first instance of a fully AI-generated paper successfully navigating a peer review. This accomplishment highlights the growing capability of AI in conducting all aspects of scientific research. We anticipate that further advancements in autonomous scientific discovery technologies will profoundly impact human knowledge generation, enabling unprecedented scalability in research productivity and significantly accelerating scientific breakthroughs, greatly benefiting society at large. We have open-sourced the code at https://github.com/SakanaAI/AI-Scientist-v2 to foster the future development of this transformative technology. We also discuss the role of AI in science, including AI safety.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4555.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4555.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AI Scientist-v2</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>The AI Scientist-v2</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An end-to-end agentic system that uses large language models to autonomously generate research ideas, write experiment code, execute experiments via parallel agentic tree search, critique visualizations with a VLM, and author complete manuscripts; it integrates literature-query tools (e.g., Semantic Scholar) during idea generation to assess novelty.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>The AI Scientist-v2</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>An integrated automated scientific discovery pipeline composed of: (1) a generalized idea-generation stage that queries literature tools (Semantic Scholar) to assess novelty; (2) an Experiment Progress Manager agent that coordinates four experiment stages (preliminary prototype, hyperparameter tuning, research agenda execution, ablations) with explicit stopping criteria; (3) a parallelized agentic tree-search mechanism where each tree node is an experiment (script, plan, metrics, visualization, execution trace) and LLMs generate/refine Python experiment code for nodes which are executed, debugged, and selected via LLM evaluators; (4) Vision-Language Model (VLM) feedback integrated into plotting and manuscript-reflection stages to critique figures/captions; (5) single-pass manuscript generation followed by a reflection stage using reasoning models. The system logs checkpoints, replicates experiments for statistics, and selects best-performing nodes to seed later stages.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Claude 3.5 Sonnet (v2) for code generation; GPT-4o for LLM/VLM feedback and summary/report agents (as reported in the paper's hyperparameters table)</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Literature querying via integrated tools (Semantic Scholar) during idea generation plus structured prompting of LLMs to assess novelty and retrieve relevant prior work; extraction of figure-text pairs for VLM-based checking.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Multi-stage LLM-driven synthesis combining literature-query results with internally generated ideas, hierarchical experiment-tree aggregation (best-node selection across stages), and LLM-based reflection to produce a coherent manuscript; uses aggregation nodes to combine replication outputs into summary visualizations.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Machine learning research (ML experiments, compositional generalization, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Complete research manuscripts (papers), experiment code, visualizations, aggregated experimental summaries</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Peer-review scores (human reviewer ratings), internal LLM evaluators, experiment performance metrics (e.g., validation accuracy/loss), and VLM figure-clarity checks</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Produced three fully autonomous manuscripts submitted to an ICLR workshop; one paper received reviewer scores 6, 6, and 7 (average 6.33) and qualified for workshop-level acceptance before withdrawal per authors' protocol; runtime per paper typically several hours up to 15 hours.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>The AI Scientist-v1 (template-based, linear experiment flow)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Increased autonomy (removed template dependency), enabled parallel experiments via agentic tree search, and integrated VLM review; qualitatively improved exploration depth and out-of-the-box deployability compared to v1 (no numeric benchmark provided).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Combining LLM-driven code generation with structured, parallel tree-search and a stage-wise experiment manager enables more systematic and deeper exploration than a linear, template-driven pipeline; integrating literature queries during idea generation grounds novelty checks; VLM feedback improves figure clarity.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>LLM citation hallucinations and occasional inaccuracies; limits in methodological rigor compared to top-tier human-authored work; computational costs and runtime limits (per-node runtime up to 1 hour; total up to 15 hours); dataset handling ad-hoc (Hugging Face where available).</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Paper reports hyperparameters for node allocations per stage (e.g., Stage 1: 21 nodes) and runtime limits; authors note more seeds/runs increases chance of producing a peer-review-surviving manuscript, but no formal scaling curve across number of papers or model size is provided.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The AI Scientist-v2: Workshop-Level Automated Scientific Discovery via Agentic Tree Search', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4555.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4555.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AIDE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AIDE (AI-driven exploration in the space of code)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A system combining LLM-based code generation with tree-search where nodes represent candidate code solutions scored by scalar evaluations (e.g., validation accuracy); nodes are iteratively selected and refined to improve performance on ML engineering tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Aide: Ai-driven exploration in the space of code</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>AIDE</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>An approach that treats code generation as the action space for LLMs and couples it with tree-search: each tree node encodes a candidate solution state (code + scalar evaluation), and nodes are selected for debugging or refinement guided by their evaluation scores; demonstrated on the MLEBench benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Not reported in detail in this paper; described primarily as LLM-driven code generation with evaluation feedback rather than literature extraction.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Tree-search-based iterative refinement of code solutions guided by scalar evaluation metrics (best-first selection), producing improved implementations across candidate nodes.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Machine learning engineering / code synthesis tasks</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Executable code solutions and improved implementations</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Benchmark performance on MLEBench (task-specific metrics, e.g., validation accuracy), scalar node evaluation scores</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Reported as state-of-the-art performance on MLEBench (per citation in paper), exact numeric gains not reproduced in this manuscript.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Prior code-generation or search methods on MLEBench</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Claimed SOTA on MLEBench per referenced work; no direct numeric comparison in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Integrating tree-search with LLM code generation can systematically explore and improve candidate implementations; nodes scored by task-specific metrics guide productive refinements.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Increased complexity and computational demand; scalability challenges noted for tree-search with LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Paper cites increased complexity and computational demand as tree-search scales; no empirical scaling curves provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The AI Scientist-v2: Workshop-Level Automated Scientific Discovery via Agentic Tree Search', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4555.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4555.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GraphEval</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GraphEval (Grapheval)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A graph-based LLM framework designed to evaluate research ideas using structured graph representations to support idea assessment.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Grapheval: A lightweight graph-based LLM framework for idea evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>GraphEval</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A framework that uses graph-based methods to structure and evaluate research ideas with LLM assistance; aimed at lightweight evaluation of novelty and feasibility via graph representations (as described in the related work summary).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Graph-structured representation of idea components combined with LLM-based evaluation prompts (specific extraction mechanisms not detailed in this manuscript).</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Graph-based aggregation and evaluation of idea elements to produce an overall assessment of a proposed research idea.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Research idea evaluation (general scientific/ML idea space)</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Evaluations/scores of research ideas; structured idea assessments</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Human evaluations of idea novelty/feasibility per referenced study (specific metrics not reproduced in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Graph-based structuring can help LLMs assess and organize research ideas for evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Not detailed in this paper; potential issues include LLM reliability and feasibility assessment accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The AI Scientist-v2: Workshop-Level Automated Scientific Discovery via Agentic Tree Search', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4555.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4555.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Agent Laboratory</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Agent Laboratory</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LLM-agent framework that uses LLMs as research assistants to automate parts of the research workflow, including idea generation and experimentation assistance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Agent laboratory: Using llm agents as research assistants</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Agent Laboratory</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A system that deploys LLM-based agents to act as research assistants across research tasks; paper references it as a concurrent work that leverages LLM agents for research assistance, though implementation specifics are not provided in this manuscript.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Not specified in this manuscript; described generally as LLM agents operating on research tasks (likely using prompting and tool use).</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Agent-based coordination of LLMs to perform multi-step research activities (details not provided here).</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>General research assistance (ML research focus implied)</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Research assistance outputs (ideas, code, analyses) depending on agent roles</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>LLM agents can serve as modular research assistants; cited as part of rapid developments in agentic research tools.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Not discussed in detail in this manuscript.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The AI Scientist-v2: Workshop-Level Automated Scientific Discovery via Agentic Tree Search', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4555.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e4555.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>agentrxiv</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>agentrxiv</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A framed effort toward collaborative autonomous research using LLM agents, described as a concurrent work exploring agentic autonomy in research workflows.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Agentrxiv: Towards collaborative autonomous research</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>agentrxiv</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Presented as a concurrent approach emphasizing collaborative autonomous research with LLM agents; specifics are only referenced in related work and not detailed in this manuscript.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Agent collaboration to support research tasks (paper does not detail mechanisms).</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Autonomous collaborative research (general)</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Collaborative research artifacts (unspecified)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Cited to highlight rapid concurrent work exploring agentic LLMs for research.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The AI Scientist-v2: Workshop-Level Automated Scientific Discovery via Agentic Tree Search', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4555.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e4555.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AI-Researcher</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AI-Researcher</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A referenced fully-automated scientific discovery system that uses LLM agents to navigate the research pipeline end-to-end.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Ai-researcher: Fully-automated scientific discovery with llm agents</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>AI-Researcher</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Mentioned as an end-to-end automated framework that leverages LLM agents to perform stages of the research pipeline autonomously; the present manuscript cites it as a related industry/academic effort but does not reproduce its internals.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>LLM-agent pipeline spanning idea generation, experiments, and writeup (details not provided here).</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>General automated scientific discovery (LLM agents)</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Automated research outputs (papers, experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Cited as an example of fully-automated LLM-agent research frameworks in related work.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The AI Scientist-v2: Workshop-Level Automated Scientific Discovery via Agentic Tree Search', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4555.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e4555.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Reflexion</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reflexion (Language agents with verbal reinforcement learning)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An agentic scaffolding method where models iteratively reflect on past outputs to self-criticize and improve future responses.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Reflexion: Language agents with verbal reinforcement learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Reflexion</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A framework that enables LLM agents to iteratively reflect on previous responses (verbal reinforcement learning) to improve robustness and performance on complex reasoning tasks; cited as an agentic scaffolding approach to enhance LLM reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Iterative self-reflection on generated outputs (not specific to paper extraction).</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Self-reflection and iterative refinement of internal outputs; can be combined with other agentic approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>General LLM agent reasoning and robustness</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Improved LLM responses and agent behavior</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Iterative reflection improves agent robustness, though with computational overhead.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Can introduce increased computational overhead and slower inference.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The AI Scientist-v2: Workshop-Level Automated Scientific Discovery via Agentic Tree Search', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4555.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e4555.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Si et al. (2025) study</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Can LLMs generate novel research ideas? a large-scale human study with 100+ NLP researchers</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A human-subject study evaluating whether LLMs can generate novel research ideas; found LLM-generated ideas were often more novel but less feasible than human experts' ideas.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Can LLMs generate novel research ideas? a large-scale human study with 100+ NLP researchers</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Si et al. (LLM idea-generation evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A large-scale evaluation study where LLMs generate scientific ideas which are then judged by human researchers for novelty and feasibility; included as evidence of LLMs' strengths and limitations in idea generation.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Not a system for extracting from papers; uses human evaluation of LLM-generated ideas and likely LLM prompting for idea generation.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Human-in-the-loop evaluation of aggregated LLM outputs to assess novelty/feasibility.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Scientific idea generation (NLP research community focus)</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Evaluated research ideas (study outcomes)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Human novelty and feasibility ratings</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Reported that LLM-generated ideas tended to be more novel but less feasible than human-generated ideas (per cited study summary).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Human expert-generated ideas</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>More novel vs. humans but often less feasible.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>LLMs can propose novel ideas at scale but feasibility and depth remain concerns.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Feasibility and practical grounding of LLM-generated ideas; human evaluation required.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The AI Scientist-v2: Workshop-Level Automated Scientific Discovery via Agentic Tree Search', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4555.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e4555.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Eger et al. survey</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Transforming science with large language models: A survey on ai-assisted scientific discovery, experimentation, content generation, and evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A comprehensive survey summarizing how LLMs are applied across scientific workflows including discovery, experimentation, content generation, and evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Transforming science with large language models: A survey on ai-assisted scientific discovery, experimentation, content generation, and evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Eger et al. (survey)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A survey paper that catalogs and analyzes numerous roles LLMs play in scientific workflows, including idea generation, experiment design, content generation, and evaluation; cited in related work to contextualize the field.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Survey/meta-analysis methods (compiles published approaches rather than a single extraction technique).</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Literature synthesis and categorization of LLM applications in science.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Cross-domain survey of LLM applications in scientific research</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Survey synthesis, taxonomy of methods</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>LLMs are increasingly integrated across many parts of scientific workflows, but challenges (e.g., hallucinations, evaluation standards) remain.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Survey highlights open challenges like evaluation standards, hallucination, and integration into rigorous scientific practice.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The AI Scientist-v2: Workshop-Level Automated Scientific Discovery via Agentic Tree Search', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Aide: Ai-driven exploration in the space of code <em>(Rating: 2)</em></li>
                <li>Grapheval: A lightweight graph-based LLM framework for idea evaluation <em>(Rating: 2)</em></li>
                <li>Agent laboratory: Using llm agents as research assistants <em>(Rating: 2)</em></li>
                <li>Agentrxiv: Towards collaborative autonomous research <em>(Rating: 2)</em></li>
                <li>Ai-researcher: Fully-automated scientific discovery with llm agents <em>(Rating: 2)</em></li>
                <li>Reflexion: Language agents with verbal reinforcement learning <em>(Rating: 1)</em></li>
                <li>Can LLMs generate novel research ideas? a large-scale human study with 100+ NLP researchers <em>(Rating: 2)</em></li>
                <li>Transforming science with large language models: A survey on ai-assisted scientific discovery, experimentation, content generation, and evaluation <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4555",
    "paper_id": "paper-39983a80f111f7f6e793f02c5725a14bca76b32d",
    "extraction_schema_id": "extraction-schema-100",
    "extracted_data": [
        {
            "name_short": "AI Scientist-v2",
            "name_full": "The AI Scientist-v2",
            "brief_description": "An end-to-end agentic system that uses large language models to autonomously generate research ideas, write experiment code, execute experiments via parallel agentic tree search, critique visualizations with a VLM, and author complete manuscripts; it integrates literature-query tools (e.g., Semantic Scholar) during idea generation to assess novelty.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "The AI Scientist-v2",
            "system_description": "An integrated automated scientific discovery pipeline composed of: (1) a generalized idea-generation stage that queries literature tools (Semantic Scholar) to assess novelty; (2) an Experiment Progress Manager agent that coordinates four experiment stages (preliminary prototype, hyperparameter tuning, research agenda execution, ablations) with explicit stopping criteria; (3) a parallelized agentic tree-search mechanism where each tree node is an experiment (script, plan, metrics, visualization, execution trace) and LLMs generate/refine Python experiment code for nodes which are executed, debugged, and selected via LLM evaluators; (4) Vision-Language Model (VLM) feedback integrated into plotting and manuscript-reflection stages to critique figures/captions; (5) single-pass manuscript generation followed by a reflection stage using reasoning models. The system logs checkpoints, replicates experiments for statistics, and selects best-performing nodes to seed later stages.",
            "llm_model_used": "Claude 3.5 Sonnet (v2) for code generation; GPT-4o for LLM/VLM feedback and summary/report agents (as reported in the paper's hyperparameters table)",
            "extraction_technique": "Literature querying via integrated tools (Semantic Scholar) during idea generation plus structured prompting of LLMs to assess novelty and retrieve relevant prior work; extraction of figure-text pairs for VLM-based checking.",
            "synthesis_technique": "Multi-stage LLM-driven synthesis combining literature-query results with internally generated ideas, hierarchical experiment-tree aggregation (best-node selection across stages), and LLM-based reflection to produce a coherent manuscript; uses aggregation nodes to combine replication outputs into summary visualizations.",
            "number_of_papers": null,
            "domain_or_topic": "Machine learning research (ML experiments, compositional generalization, etc.)",
            "output_type": "Complete research manuscripts (papers), experiment code, visualizations, aggregated experimental summaries",
            "evaluation_metrics": "Peer-review scores (human reviewer ratings), internal LLM evaluators, experiment performance metrics (e.g., validation accuracy/loss), and VLM figure-clarity checks",
            "performance_results": "Produced three fully autonomous manuscripts submitted to an ICLR workshop; one paper received reviewer scores 6, 6, and 7 (average 6.33) and qualified for workshop-level acceptance before withdrawal per authors' protocol; runtime per paper typically several hours up to 15 hours.",
            "comparison_baseline": "The AI Scientist-v1 (template-based, linear experiment flow)",
            "performance_vs_baseline": "Increased autonomy (removed template dependency), enabled parallel experiments via agentic tree search, and integrated VLM review; qualitatively improved exploration depth and out-of-the-box deployability compared to v1 (no numeric benchmark provided).",
            "key_findings": "Combining LLM-driven code generation with structured, parallel tree-search and a stage-wise experiment manager enables more systematic and deeper exploration than a linear, template-driven pipeline; integrating literature queries during idea generation grounds novelty checks; VLM feedback improves figure clarity.",
            "limitations_challenges": "LLM citation hallucinations and occasional inaccuracies; limits in methodological rigor compared to top-tier human-authored work; computational costs and runtime limits (per-node runtime up to 1 hour; total up to 15 hours); dataset handling ad-hoc (Hugging Face where available).",
            "scaling_behavior": "Paper reports hyperparameters for node allocations per stage (e.g., Stage 1: 21 nodes) and runtime limits; authors note more seeds/runs increases chance of producing a peer-review-surviving manuscript, but no formal scaling curve across number of papers or model size is provided.",
            "uuid": "e4555.0",
            "source_info": {
                "paper_title": "The AI Scientist-v2: Workshop-Level Automated Scientific Discovery via Agentic Tree Search",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "AIDE",
            "name_full": "AIDE (AI-driven exploration in the space of code)",
            "brief_description": "A system combining LLM-based code generation with tree-search where nodes represent candidate code solutions scored by scalar evaluations (e.g., validation accuracy); nodes are iteratively selected and refined to improve performance on ML engineering tasks.",
            "citation_title": "Aide: Ai-driven exploration in the space of code",
            "mention_or_use": "mention",
            "system_name": "AIDE",
            "system_description": "An approach that treats code generation as the action space for LLMs and couples it with tree-search: each tree node encodes a candidate solution state (code + scalar evaluation), and nodes are selected for debugging or refinement guided by their evaluation scores; demonstrated on the MLEBench benchmark.",
            "llm_model_used": null,
            "extraction_technique": "Not reported in detail in this paper; described primarily as LLM-driven code generation with evaluation feedback rather than literature extraction.",
            "synthesis_technique": "Tree-search-based iterative refinement of code solutions guided by scalar evaluation metrics (best-first selection), producing improved implementations across candidate nodes.",
            "number_of_papers": null,
            "domain_or_topic": "Machine learning engineering / code synthesis tasks",
            "output_type": "Executable code solutions and improved implementations",
            "evaluation_metrics": "Benchmark performance on MLEBench (task-specific metrics, e.g., validation accuracy), scalar node evaluation scores",
            "performance_results": "Reported as state-of-the-art performance on MLEBench (per citation in paper), exact numeric gains not reproduced in this manuscript.",
            "comparison_baseline": "Prior code-generation or search methods on MLEBench",
            "performance_vs_baseline": "Claimed SOTA on MLEBench per referenced work; no direct numeric comparison in this paper.",
            "key_findings": "Integrating tree-search with LLM code generation can systematically explore and improve candidate implementations; nodes scored by task-specific metrics guide productive refinements.",
            "limitations_challenges": "Increased complexity and computational demand; scalability challenges noted for tree-search with LLMs.",
            "scaling_behavior": "Paper cites increased complexity and computational demand as tree-search scales; no empirical scaling curves provided here.",
            "uuid": "e4555.1",
            "source_info": {
                "paper_title": "The AI Scientist-v2: Workshop-Level Automated Scientific Discovery via Agentic Tree Search",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "GraphEval",
            "name_full": "GraphEval (Grapheval)",
            "brief_description": "A graph-based LLM framework designed to evaluate research ideas using structured graph representations to support idea assessment.",
            "citation_title": "Grapheval: A lightweight graph-based LLM framework for idea evaluation",
            "mention_or_use": "mention",
            "system_name": "GraphEval",
            "system_description": "A framework that uses graph-based methods to structure and evaluate research ideas with LLM assistance; aimed at lightweight evaluation of novelty and feasibility via graph representations (as described in the related work summary).",
            "llm_model_used": null,
            "extraction_technique": "Graph-structured representation of idea components combined with LLM-based evaluation prompts (specific extraction mechanisms not detailed in this manuscript).",
            "synthesis_technique": "Graph-based aggregation and evaluation of idea elements to produce an overall assessment of a proposed research idea.",
            "number_of_papers": null,
            "domain_or_topic": "Research idea evaluation (general scientific/ML idea space)",
            "output_type": "Evaluations/scores of research ideas; structured idea assessments",
            "evaluation_metrics": "Human evaluations of idea novelty/feasibility per referenced study (specific metrics not reproduced in this paper).",
            "performance_results": null,
            "comparison_baseline": null,
            "performance_vs_baseline": null,
            "key_findings": "Graph-based structuring can help LLMs assess and organize research ideas for evaluation.",
            "limitations_challenges": "Not detailed in this paper; potential issues include LLM reliability and feasibility assessment accuracy.",
            "scaling_behavior": null,
            "uuid": "e4555.2",
            "source_info": {
                "paper_title": "The AI Scientist-v2: Workshop-Level Automated Scientific Discovery via Agentic Tree Search",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "Agent Laboratory",
            "name_full": "Agent Laboratory",
            "brief_description": "An LLM-agent framework that uses LLMs as research assistants to automate parts of the research workflow, including idea generation and experimentation assistance.",
            "citation_title": "Agent laboratory: Using llm agents as research assistants",
            "mention_or_use": "mention",
            "system_name": "Agent Laboratory",
            "system_description": "A system that deploys LLM-based agents to act as research assistants across research tasks; paper references it as a concurrent work that leverages LLM agents for research assistance, though implementation specifics are not provided in this manuscript.",
            "llm_model_used": null,
            "extraction_technique": "Not specified in this manuscript; described generally as LLM agents operating on research tasks (likely using prompting and tool use).",
            "synthesis_technique": "Agent-based coordination of LLMs to perform multi-step research activities (details not provided here).",
            "number_of_papers": null,
            "domain_or_topic": "General research assistance (ML research focus implied)",
            "output_type": "Research assistance outputs (ideas, code, analyses) depending on agent roles",
            "evaluation_metrics": null,
            "performance_results": null,
            "comparison_baseline": null,
            "performance_vs_baseline": null,
            "key_findings": "LLM agents can serve as modular research assistants; cited as part of rapid developments in agentic research tools.",
            "limitations_challenges": "Not discussed in detail in this manuscript.",
            "scaling_behavior": null,
            "uuid": "e4555.3",
            "source_info": {
                "paper_title": "The AI Scientist-v2: Workshop-Level Automated Scientific Discovery via Agentic Tree Search",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "agentrxiv",
            "name_full": "agentrxiv",
            "brief_description": "A framed effort toward collaborative autonomous research using LLM agents, described as a concurrent work exploring agentic autonomy in research workflows.",
            "citation_title": "Agentrxiv: Towards collaborative autonomous research",
            "mention_or_use": "mention",
            "system_name": "agentrxiv",
            "system_description": "Presented as a concurrent approach emphasizing collaborative autonomous research with LLM agents; specifics are only referenced in related work and not detailed in this manuscript.",
            "llm_model_used": null,
            "extraction_technique": null,
            "synthesis_technique": "Agent collaboration to support research tasks (paper does not detail mechanisms).",
            "number_of_papers": null,
            "domain_or_topic": "Autonomous collaborative research (general)",
            "output_type": "Collaborative research artifacts (unspecified)",
            "evaluation_metrics": null,
            "performance_results": null,
            "comparison_baseline": null,
            "performance_vs_baseline": null,
            "key_findings": "Cited to highlight rapid concurrent work exploring agentic LLMs for research.",
            "limitations_challenges": null,
            "scaling_behavior": null,
            "uuid": "e4555.4",
            "source_info": {
                "paper_title": "The AI Scientist-v2: Workshop-Level Automated Scientific Discovery via Agentic Tree Search",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "AI-Researcher",
            "name_full": "AI-Researcher",
            "brief_description": "A referenced fully-automated scientific discovery system that uses LLM agents to navigate the research pipeline end-to-end.",
            "citation_title": "Ai-researcher: Fully-automated scientific discovery with llm agents",
            "mention_or_use": "mention",
            "system_name": "AI-Researcher",
            "system_description": "Mentioned as an end-to-end automated framework that leverages LLM agents to perform stages of the research pipeline autonomously; the present manuscript cites it as a related industry/academic effort but does not reproduce its internals.",
            "llm_model_used": null,
            "extraction_technique": null,
            "synthesis_technique": "LLM-agent pipeline spanning idea generation, experiments, and writeup (details not provided here).",
            "number_of_papers": null,
            "domain_or_topic": "General automated scientific discovery (LLM agents)",
            "output_type": "Automated research outputs (papers, experiments)",
            "evaluation_metrics": null,
            "performance_results": null,
            "comparison_baseline": null,
            "performance_vs_baseline": null,
            "key_findings": "Cited as an example of fully-automated LLM-agent research frameworks in related work.",
            "limitations_challenges": null,
            "scaling_behavior": null,
            "uuid": "e4555.5",
            "source_info": {
                "paper_title": "The AI Scientist-v2: Workshop-Level Automated Scientific Discovery via Agentic Tree Search",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "Reflexion",
            "name_full": "Reflexion (Language agents with verbal reinforcement learning)",
            "brief_description": "An agentic scaffolding method where models iteratively reflect on past outputs to self-criticize and improve future responses.",
            "citation_title": "Reflexion: Language agents with verbal reinforcement learning",
            "mention_or_use": "mention",
            "system_name": "Reflexion",
            "system_description": "A framework that enables LLM agents to iteratively reflect on previous responses (verbal reinforcement learning) to improve robustness and performance on complex reasoning tasks; cited as an agentic scaffolding approach to enhance LLM reasoning.",
            "llm_model_used": null,
            "extraction_technique": "Iterative self-reflection on generated outputs (not specific to paper extraction).",
            "synthesis_technique": "Self-reflection and iterative refinement of internal outputs; can be combined with other agentic approaches.",
            "number_of_papers": null,
            "domain_or_topic": "General LLM agent reasoning and robustness",
            "output_type": "Improved LLM responses and agent behavior",
            "evaluation_metrics": null,
            "performance_results": null,
            "comparison_baseline": null,
            "performance_vs_baseline": null,
            "key_findings": "Iterative reflection improves agent robustness, though with computational overhead.",
            "limitations_challenges": "Can introduce increased computational overhead and slower inference.",
            "scaling_behavior": null,
            "uuid": "e4555.6",
            "source_info": {
                "paper_title": "The AI Scientist-v2: Workshop-Level Automated Scientific Discovery via Agentic Tree Search",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "Si et al. (2025) study",
            "name_full": "Can LLMs generate novel research ideas? a large-scale human study with 100+ NLP researchers",
            "brief_description": "A human-subject study evaluating whether LLMs can generate novel research ideas; found LLM-generated ideas were often more novel but less feasible than human experts' ideas.",
            "citation_title": "Can LLMs generate novel research ideas? a large-scale human study with 100+ NLP researchers",
            "mention_or_use": "mention",
            "system_name": "Si et al. (LLM idea-generation evaluation)",
            "system_description": "A large-scale evaluation study where LLMs generate scientific ideas which are then judged by human researchers for novelty and feasibility; included as evidence of LLMs' strengths and limitations in idea generation.",
            "llm_model_used": null,
            "extraction_technique": "Not a system for extracting from papers; uses human evaluation of LLM-generated ideas and likely LLM prompting for idea generation.",
            "synthesis_technique": "Human-in-the-loop evaluation of aggregated LLM outputs to assess novelty/feasibility.",
            "number_of_papers": null,
            "domain_or_topic": "Scientific idea generation (NLP research community focus)",
            "output_type": "Evaluated research ideas (study outcomes)",
            "evaluation_metrics": "Human novelty and feasibility ratings",
            "performance_results": "Reported that LLM-generated ideas tended to be more novel but less feasible than human-generated ideas (per cited study summary).",
            "comparison_baseline": "Human expert-generated ideas",
            "performance_vs_baseline": "More novel vs. humans but often less feasible.",
            "key_findings": "LLMs can propose novel ideas at scale but feasibility and depth remain concerns.",
            "limitations_challenges": "Feasibility and practical grounding of LLM-generated ideas; human evaluation required.",
            "scaling_behavior": null,
            "uuid": "e4555.7",
            "source_info": {
                "paper_title": "The AI Scientist-v2: Workshop-Level Automated Scientific Discovery via Agentic Tree Search",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "Eger et al. survey",
            "name_full": "Transforming science with large language models: A survey on ai-assisted scientific discovery, experimentation, content generation, and evaluation",
            "brief_description": "A comprehensive survey summarizing how LLMs are applied across scientific workflows including discovery, experimentation, content generation, and evaluation.",
            "citation_title": "Transforming science with large language models: A survey on ai-assisted scientific discovery, experimentation, content generation, and evaluation",
            "mention_or_use": "mention",
            "system_name": "Eger et al. (survey)",
            "system_description": "A survey paper that catalogs and analyzes numerous roles LLMs play in scientific workflows, including idea generation, experiment design, content generation, and evaluation; cited in related work to contextualize the field.",
            "llm_model_used": null,
            "extraction_technique": "Survey/meta-analysis methods (compiles published approaches rather than a single extraction technique).",
            "synthesis_technique": "Literature synthesis and categorization of LLM applications in science.",
            "number_of_papers": null,
            "domain_or_topic": "Cross-domain survey of LLM applications in scientific research",
            "output_type": "Survey synthesis, taxonomy of methods",
            "evaluation_metrics": null,
            "performance_results": null,
            "comparison_baseline": null,
            "performance_vs_baseline": null,
            "key_findings": "LLMs are increasingly integrated across many parts of scientific workflows, but challenges (e.g., hallucinations, evaluation standards) remain.",
            "limitations_challenges": "Survey highlights open challenges like evaluation standards, hallucination, and integration into rigorous scientific practice.",
            "scaling_behavior": null,
            "uuid": "e4555.8",
            "source_info": {
                "paper_title": "The AI Scientist-v2: Workshop-Level Automated Scientific Discovery via Agentic Tree Search",
                "publication_date_yy_mm": "2025-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Aide: Ai-driven exploration in the space of code",
            "rating": 2
        },
        {
            "paper_title": "Grapheval: A lightweight graph-based LLM framework for idea evaluation",
            "rating": 2
        },
        {
            "paper_title": "Agent laboratory: Using llm agents as research assistants",
            "rating": 2
        },
        {
            "paper_title": "Agentrxiv: Towards collaborative autonomous research",
            "rating": 2
        },
        {
            "paper_title": "Ai-researcher: Fully-automated scientific discovery with llm agents",
            "rating": 2
        },
        {
            "paper_title": "Reflexion: Language agents with verbal reinforcement learning",
            "rating": 1
        },
        {
            "paper_title": "Can LLMs generate novel research ideas? a large-scale human study with 100+ NLP researchers",
            "rating": 2
        },
        {
            "paper_title": "Transforming science with large language models: A survey on ai-assisted scientific discovery, experimentation, content generation, and evaluation",
            "rating": 2
        }
    ],
    "cost": 0.016344749999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>The AI Scientist-v2: Workshop-Level Automated Scientific Discovery via Agentic Tree Search</h1>
<p>Yutaro Yamada ${ }^{1, <em>}$, Robert Tjarko Lange ${ }^{1, </em>}$, Cong Lu ${ }^{1,2,3, <em>}$, Shengran Hu ${ }^{1,2,3}$, Chris Lu ${ }^{4}$, Jakob Foerster ${ }^{4}$, Jeff Clune ${ }^{2,3,5, \dagger}$ and David Ha ${ }^{1, \dagger}$<br></em>Equal Contribution, ${ }^{1}$ Sakana AI, ${ }^{2}$ University of British Columbia, ${ }^{3}$ Vector Institute, ${ }^{4}$ FLAIR, University of Oxford, ${ }^{5}$ Canada CIFAR AI Chair, ${ }^{\dagger}$ Equal Advising</p>
<h4>Abstract</h4>
<p>AI is increasingly playing a pivotal role in transforming how scientific discoveries are made. We introduce The AI Scientist-v2, an end-to-end agentic system capable of producing the first entirely AIgenerated peer-review-accepted workshop paper. This system iteratively formulates scientific hypotheses, designs and executes experiments, analyzes and visualizes data, and autonomously authors scientific manuscripts. Compared to its predecessor (v1, Lu et al., 2024), The AI Scientist-v2 eliminates the reliance on human-authored code templates, generalizes effectively across diverse machine learning domains, and leverages a novel progressive agentic tree-search methodology managed by a dedicated experiment manager agent. Additionally, we enhance the AI reviewer component by integrating a Vision-Language Model (VLM) feedback loop for iterative refinement of content and aesthetics of the figures. We evaluated The AI Scientist-v2 by submitting three fully autonomous manuscripts to a peer-reviewed ICLR workshop. Notably, one manuscript achieved high enough scores to exceed the average human acceptance threshold, marking the first instance of a fully AI-generated paper successfully navigating a peer review. This accomplishment highlights the growing capability of AI in conducting all aspects of scientific research. We anticipate that further advancements in autonomous scientific discovery technologies will profoundly impact human knowledge generation, enabling unprecedented scalability in research productivity and significantly accelerating scientific breakthroughs, greatly benefiting society at large. We have open-sourced the code at https://github.com/SakanaAI/AI-Scientist-v2 to foster the future development of this transformative technology. We also discuss the role of AI in science, including AI safety.</p>
<h2>1. Introduction</h2>
<p>Automated scientific discovery empowered by artificial intelligence (AI) has garnered considerable attention in recent years (Cornelio et al., 2023; Gil et al., 2014; King et al., 2009; Kitano, 2021; Wang et al., 2023; Xu et al., 2021). The development of end-to-end frameworks capable of autonomously formulating hypotheses, performing experiments, analyzing results, and authoring manuscripts could fundamentally transform the scientific process. A notable recent advance in this direction is The AI Scientist-v1 (Lu et al., 2024), which demonstrated the feasibility of a fully automated scientific workflow and downstream manuscript production. However, significant limitations constrained its broad applicability and autonomy. Specifically, it relied heavily on human-authored code templates requiring manual effort to create a new template for each new topic area. Furthermore, its linear and shallow experimentation approach prevented deeper exploration of scientific hypotheses.</p>
<p>In this paper, we introduce The AI Scientist-v2, a substantially improved successor that directly addresses these limitations. Our contributions are threefold. First, we eliminate the dependency on human-provided code templates, significantly increasing the system's autonomy and ability to be deployed out of the box across multiple machine learning domains. Second, we introduce an experiment manager agent coupled with a novel agentic tree-search algorithm, enabling deeper and</p>
<p>Table 1 | Comparison of AI Scientist Versions. Comparison highlights key advancements in THE AI SCIENTIST-v2, including autonomous code generation via tree search, enhanced VLM integration for feedback during experiments and manuscript review, and evaluation through formal peer review.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Feature</th>
<th style="text-align: center;">Codebase <br> Drafting</th>
<th style="text-align: center;">Execution <br> Planning</th>
<th style="text-align: center;">Parallel <br> Experiments</th>
<th style="text-align: center;">VLM <br> Reviewer</th>
<th style="text-align: center;">Human Result <br> Evaluation</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">THE AI SCIENTIST-V1</td>
<td style="text-align: center;">Topic-Specific</td>
<td style="text-align: center;">Linear</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">Not Submitted</td>
</tr>
<tr>
<td style="text-align: left;">THE AI SCIENTIST-V2</td>
<td style="text-align: center;">Domain-General</td>
<td style="text-align: center;">Tree-Based</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">Workshop Acceptance-Worthy</td>
</tr>
</tbody>
</table>
<p>more systematic exploration of complex hypotheses. Third, we enhance the reviewing and refinement stages by integrating a Vision-Language Model (VLM)-based feedback mechanism, improving the quality, clarity, and alignment of generated figures, captions, and text interpretation. To rigorously evaluate the capabilities and limitations of fully autonomous manuscript generation, we conducted a controlled experiment: three manuscripts entirely generated by The AI Scientist-v2 were submitted to a peer-reviewed workshop at ICLR. Remarkably, one manuscript achieved an average reviewer score of 6.33 (placing it roughly in the top $45 \%$ of submissions) and would have been accepted after meta-review were it human-generated, thus becoming the first fully AI-generated manuscript to successfully pass a peer-review process.</p>
<p>The accepted paper investigates whether incorporating an explicit compositional regularization term into neural network training can improve compositional generalization. Specifically, it penalizes large deviations between embeddings of successive time steps in sequence models, hypothesizing that this encourages compositionality. The approach is evaluated using synthetic arithmetic expression datasets, but it is found that compositional regularization does not yield significant improvements and occasionally harms performance. The workshop reviewers appreciated the paper for clearly identifying the challenges of effective compositional regularization and reporting on negative results. However, they collectively highlighted shortcomings, including insufficient justification and intuitive explanations for why the chosen regularization method would enhance compositionality. Our personal assessment (detailed further in 4) highlights several additional potential improvements in method description (e.g., making clear exactly which component of the network is being regularized), potential dataset overlap issues, and inaccuracies in figure captions. Overall, reviewers viewed the paper as an interesting and technically sound workshop contribution that needs further development and broader experimentation to reach conference-level rigor.</p>
<p>This report provides an in-depth outline of the developed methodological advances, analysis of the workshop-submitted papers, and a discussion on the ethical and safety considerations of systems like The AI Scientist-v2. Our overall contributions are as follows:</p>
<ol>
<li>We introduce The AI Scientist-v2, an automated scientific discovery framework enhanced by agentic tree search, VLM feedback, and parallel experiment execution. It thereby significantly improves the autonomy, flexibility, and scientific exploration depth of previous systems.</li>
<li>We demonstrate, for the first time, that an AI-generated manuscript can successfully pass peer review at a recognized machine learning workshop, marking a critical milestone for AI science.</li>
<li>We conduct comprehensive internal evaluations and analyses of both peer-review feedback and our system's outputs, providing insights into the strengths, weaknesses, and current status of AI-generated manuscripts relative to traditional human-authored scientific publications.</li>
<li>We open-source the full codebase for The AI Scientist-v2 and the ICLR 2025 workshop experiment data, encouraging further exploration by the research community and advancing a discussion regarding AI's evolving role in science-in the open.</li>
</ol>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1 | The AI Scientist-v2 Workflow. The workflow consists of several phases covering automated idea generation, experiment execution, figure visualization, manuscript writing, and reviewing. Unlike the initial version, The AI Scientist-v2 removes the dependency on humancoded templates. Instead, it employs agentic tree search (managed by an Experiment Progress Manager across several stages, orange) to generate and refine code implementations. Subsequent experimentation leverages the best-performing code checkpoints (nodes) from the tree search to iteratively test various research hypotheses.</p>
<h1>2. Background</h1>
<p>The AI Scientist-v1 (Lu et al., 2024) introduced the first AI system that entirely automates scientific discovery and the presentation of its results. Given a baseline code template, it autonomously wrote code, executed experiments, visualized outcomes, and produced a complete scientific manuscript. However, despite representing a significant step forward, The AI Scientist-v1 was subject to limitations. Foremost among these was its reliance on human-crafted baseline code templates, significantly constraining its autonomy and hindering unconstrained out-of-the-box deployability. Instead, human effort was still required to draft an initial base experiment outline in code. Additionally, the experimentation process followed a strictly linear hypothesis-testing routine, limiting depth and exploration flexibility, especially when addressing complex research questions.</p>
<p>Language Model Agent Scaffolding. To further enhance LLM performance on complex reasoning tasks, researchers have developed agentic scaffolding frameworks, each with distinct advantages and limitations. For example, Reflexion (Shinn et al., 2024) enables models to iteratively reflect on previous responses, encouraging self-improvement through critical evaluation of past outputs; it improves robustness, but can introduce computational overhead and slower inference. Another promising direction is the integration of tree-search strategies with LLMs (Jiang et al., 2025), allowing structured exploration of reasoning paths. This approach enhances systematic reasoning and comprehensiveness, though at the cost of increased complexity, higher computational demands, and challenges in scalability.</p>
<p>Tree Search with Large Language Models. We empirically observed that automated research conducted by The AI Scientist-v1 often resulted in short-sighted experimentation. The humandriven scientific process, on the other hand, relies on open-ended hypothesis generation, steppingstone collection, and iterative hypothesis refinement. Recent advances using code generation as an action space have opened new opportunities for LLM-driven automated workflows (Wang et al., 2024). AIDE (Jiang et al., 2025) combines LLM-based code generation with tree search, demon-</p>
<p>strating state-of-the-art performance on the MLEBench benchmark (Chan et al., 2025), designed for machine learning engineering tasks. In AIDE, each node represents a potential solution state with a corresponding scalar evaluation score (e.g., validation accuracy). Nodes are iteratively selected for further debugging or refinement based on these scores. Inspired by this approach, we integrate a similar tree search-based exploration strategy within our automated scientific discovery framework, adapting it specifically to the multi-stage nature of scientific experimentation, as detailed in $\S 3$.</p>
<h1>3. The AI Scientist-v2</h1>
<p>We now describe the major innovations introduced in The AI Scientist-v2 relative to The AI SCIENTIST-v1 (Lu et al., 2024). The most significant improvement is the move towards greater autonomy and generalization, starting a more general idea generation phase ( $\S 3.1$ ) and eliminating the reliance on fixed, human-authored template code for experimentation. This process begins with generalized idea generation, producing an initial concept, which then feeds into the experimentation phase (3.2). To manage this, we introduce two critical features in the experimentation phase: coarse-grained experiment management and agentic tree search-based exploration. Additionally, we integrate Vision Language Models (VLMs) into the experimental and review phases (3.4). Finally, we streamline the manuscript writing phase by replacing the incremental, Aider-based (Gauthier, 2024) iterative writing approach of The AI Scientist-v1 with a simpler, single-pass generation followed by a separate reflection stage powered by reasoning models such as ol (OpenAI, 2024). We include a full list of sampling hyperparameters and models used in Appendix A and the prompts used for The AI Scientist-v2 in Appendix B.</p>
<h3>3.1. More General Idea Generation</h3>
<p>A key conceptual shift in The AI Scientist-v2 is the approach to research idea generation. Unlike the predecessor system, which primarily focused on proposing incremental modifications or extensions based on an existing codebase, The AI Scientist-v2 adopts a process that begins at a higher level of abstraction. The system is prompted to engage in more open-ended thinking about potential research directions, hypotheses, and experimental designs, akin to formulating a research abstract or grant proposal before committing to a specific implementation.</p>
<p>This approach encourages the exploration of potentially more novel or foundational ideas, rather than being constrained by the structure and topics of pre-existing code. It aligns more closely with how researchers often develop broader research visions, starting with abstract concepts and assessing novelty and feasibility before diving into specific implementations. Crucially, this generalized idea generation phase integrates literature review tools, such as Semantic Scholar, in the loop. The system can query the literature database during the idea formulation process to assess the novelty of a proposed concept and identify relevant prior work. This allows for more informed decisions about pursuing a particular research avenue, ensuring ideas are grounded in the existing scientific landscape from the outset, rather than relying solely on post-hoc checks.</p>
<h3>3.2. Removing Template Dependency</h3>
<p>Following the improved idea generation phase, The AI Scientist-v2 proceeds with experimentation. Beyond the code-conditioned idea generation, The AI Scientist-v1 also depended on the predefined template code as a starting baseline implementation. The LLM-driven code changes were then limited to sequential code adaptations. We now outline our strategy for eliminating this limitation, thus improving the system's flexibility and autonomy.</p>
<h3>3.2.1. Experiment Progress Manager</h3>
<p>Real-world scientific experimentation typically proceeds through distinct stages, from initial feasibility assessments to detailed ablation analyses. To emulate this structured approach, we introduce an</p>
<p>experiment progress manager agent that coordinates four clearly defined stages of scientific experimentation:</p>
<p>Stage 1 Preliminary Investigation: Establishing initial feasibility and correctness through a minimal working prototype based on the generated research idea.
Stage 2 Hyperparameter Tuning: Refining the initial implementation by optimizing critical hyperparameters (e.g., learning rate, epochs) to create a robust experimental baseline.
Stage 3 Research Agenda Execution: Systematically implementing the core research agenda based on the tuned baseline.
Stage 4 Ablation Studies: Systematically assessing the importance of various research components, providing rigorous support for the main experimental findings.</p>
<p>Each stage has explicit stopping criteria. Stage 1 concludes when a basic working prototype is successfully executed. Stage 2 ends when experiments stabilize, as indicated by convergence in training curves and successful execution across at least two datasets. Stages 3 and 4 conclude when the allocated computational budget is exhausted. Stage 3 also includes a check for experiment duration-if runs finish much faster than the pre-allocated runtime, the system suggests increasing the complexity of experiments.</p>
<p>After each stage, the experiment manager selects the best-performing node using a dedicated LLM evaluator (see next section) based on clearly articulated criteria. This selected node is then carried forward to seed the subsequent experimentation stage. The manager also records checkpoints at each stage's completion. To ensure scientific rigor and reproducibility, the experiment manager launches multiple replications of the selected best experiments at the conclusion of each stage. These repeated runs provide statistics (mean and standard deviation) for figures and reported results.</p>
<h1>3.2.2. Parallelized Agentic Tree Search</h1>
<p>The AI Scientist-v1 operated strictly linearly, where each code refinement directly built on the immediately preceding experiment. In contrast, The AI Scientist-v2 adopts a significantly more flexible and exploratory approach inspired by recent successes in integrating tree search with LLM-driven workflows (Chan et al., 2025; Jiang et al., 2025; Wijk et al., 2024) and research on openendedness (Clune, 2019; Mouret and Clune, 2015). We incorporate this agentic tree search approach across all four experimentation stages outlined in 3.2.1, enabling deeper and more systematic exploration of scientific hypotheses.</p>
<p>Each experimental node within our tree-based framework undergoes the following execution cycle: An LLM first generates both a concrete experimentation plan and the associated Python code to implement the experiment. The generated code is immediately executed in a Python interpreter. If execution encounters an error, the error message is recorded, and the node is marked as buggy, ending the current execution cycle for that node. If execution succeeds, the experiment proceeds to the plotting phase.</p>
<p>During each experiment, the system is instructed to save all relevant experimental outputs (training and validation metrics, losses, etc.) into structured numpy files. In the plotting phase, The AI Scientist-v2 reads these stored results and the code, generating visualizations that summarize and illustrate the findings clearly. These visualizations are subsequently passed to a Vision-Language Model (VLM) for critique. Any issues flagged by the VLM (such as unclear labels, missing legends, or misleading visualizations) result in the node being marked as buggy, and this feedback is recorded for future debugging. Nodes that successfully execute and pass the VLM review without issue are designated as non-buggy.</p>
<p>We define each node as a collection comprising an experiment script (e.g., a Python file), a textual</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2 | The AI Scientist-v2 workflow showing different stages of tree-based experimentation. Stage 1 begins at the root node, where initial experiment code is generated in parallel. After running the experiment code and visualization scripts, each node is classified based on the outcome: if an error occurs, it is marked as a buggy node; otherwise, it is labeled as a non-buggy node. New child nodes are created differently depending on their parent node's status: For non-buggy nodes, refinement is applied to improve the experiment code for better performance. For buggy nodes, the system attempts to debug them using stored error information. A best-performing node, selected by LLM-based evaluation, is passed down as the root node of Stage 2. From this root node, child nodes are created for hyperparameter tuning. The top-performing node from Stage 2 is then used to initialize Stage 3 , where the system executes the research agenda, applies refinements, and performs debugging as needed. In Stage 4, similar to Stage 2, the root node generates ablation nodes. Additionally, replication nodes repeat the same experiment as their parent node, while aggregation nodes collect results from replication nodes to generate combined visualizations and summaries.</p>
<p>description of the high-level plan implemented in the script, an execution error trace (if applicable), experiment runtime, performance metrics recorded during the experiment, feedback from an LLM after running the script, a visualization script, file paths to the generated figures, feedback from a VLM on those figures, and the node's final status (either buggy or non-buggy).</p>
<p>At each iteration, the system selects several nodes from the existing tree to expand in parallel. With a predefined probability, a buggy node is chosen (thus prioritizing error resolution and debugging); otherwise, a non-buggy node is selected for further refinement and improvement. When choosing between non-buggy nodes, the system uses a best-first search strategy, guided by an LLM that evaluates candidates based on factors like performance metrics, training dynamics, and the quality of generated plots. The selected nodes are expanded by creating a new child node that may either attempt debugging if the parent node was buggy, or refine and improve upon the previous experiment if the parent was non-buggy. An LLM is used to generate the plan and experiment code for each new child node, after which all new nodes are executed concurrently in parallel, significantly accelerating the exploration process. In addition to buggy and non-buggy nodes, we introduce specialized node variants tailored to specific experimental needs:</p>
<ul>
<li>Hyperparameter nodes systematically explore alternative hyperparameter configurations during Stage 2. The system maintains careful records of previously tested hyperparameters, preventing redundant experiments. Errors encountered during hyperparameter tuning trigger the creation of corresponding debug nodes.</li>
<li>Ablation nodes evaluate crucial ablation studies during Stage 4, assessing the importance of various components or assumptions underlying the experiment. Similar to hyperparameter nodes, previously tested ablation conditions are tracked to avoid repetition, and debugging nodes are created in response to any encountered errors.</li>
<li>Replication nodes execute replicates of their parent experiments using different random seeds. Typically, several replication nodes are created to enable the calculation of statistical measures (mean and standard deviation) of experimental outcomes, enhancing result robustness.</li>
<li>Aggregation nodes are special nodes created to consolidate and visualize the combined results of replication nodes. Unlike other node types, aggregation nodes do not conduct new experiments but simply generate a Python script to aggregate and summarize prior results, producing figures that explicitly show means and standard deviations.</li>
</ul>
<p>The structured design of experiment stages and tailored node types facilitates systematic exploration across all stages. Unlike some LLM agents that rigidly follow predefined, fine-grained workflow graphs, our approach adopts a looser structure that guides the entire empirical research cycle, enabling flexible system behavior while maintaining coherence across iterative stages.</p>
<h1>3.3. Dataset Loading via Hugging Face</h1>
<p>Most empirical machine learning research relies heavily on publicly available datasets. Hugging Face Hub provides a convenient and unified framework for accessing a wide variety of commonly used datasets, complete with predefined train, validation, and test splits. In The AI Scientist-v2, we prompt the system to leverage Hugging Face Hub whenever possible, automatically downloading required datasets using the standard one-line function (datasets.load_dataset). While this standardized approach greatly simplifies dataset handling, we acknowledge it is somewhat ad-hoc, as not all dataset repositories support this method.</p>
<h3>3.4. Vision-Language Model Reviewer</h3>
<p>Unlike The AI Scientist-v1, which did not leverage Vision Language Models (VLMs), The AI Scientist-v2 incorporates VLMs at two phases of the research workflow: First, during the tree-based experimentation phase, VLMs provide immediate feedback on generated figures, ensuring</p>
<p>that these visualizations effectively and accurately communicate experimental results. Second, during the manuscript writing reflection stage, VLMs evaluate figures and their captions, enhancing the visual clarity and coherence of the resulting paper.</p>
<p>In the paper-writing process, we extract screenshots of figures alongside their captions and the corresponding text from the paper that references them (identified by the keyword "Figure X"). These images and textual references are then provided to the VLM, which performs multiple quality checks, including verifying the alignment between figures and captions, identifying issues with visual clarity (e.g., missing legends, unclear labels), and detecting potential duplication of figures in the main text and appendix. Through the iterative integration of VLM feedback, we significantly enhance the visual quality and clarity of manuscripts generated by The AI Scientist-v2.</p>
<h1>4. Human Evaluation of Manuscripts Generated by The AI Scientist-v2</h1>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3 | Peer-reviewed ICBINB workshop paper generated by The AI Scientist-v2. The paper investigates the usage of a temporal consistency regularizer on the embeddings of an LSTMbased sequence model. The results discuss the effect of the regularizer on compositional regularization and highlight the difficulty of training models capable of improved generalization. It received peerreview scores of 6 (weak accept), 7 (accept), and 6 (weak accept) before meta-review and ranked among the top $45 \%$ submitted workshop papers.</p>
<p>To rigorously evaluate the capabilities and limitations of our automated scientific discovery system, we conducted a human evaluation study in collaboration with the organizers of the ICLR 2025 workshop, "I Can't Believe It's Not Better" (ICBINB). As detailed below, our evaluation included submitting fully automated manuscripts generated by The AI SCIENTIST-v2 to the official peer-review process of the workshop.</p>
<h3>4.1. Evaluation Methodology</h3>
<p>Our evaluation process involved the following carefully controlled steps:</p>
<ol>
<li>AI-Generated Submissions: We generated three complete manuscripts using only THE AI Scientist-v2, starting from broad topical prompts aligned with the workshop's scope via the generalized idea generation process ( $\S 3.1$ ). After the initial topic definition, the entire scientific workflow-hypothesis formulation, experiment design, coding, data analysis, visualizations, and manuscript writing-was executed autonomously, without any human intervention or editing.</li>
<li>Blind Peer Review: The three AI-generated submissions were included among the 43 total submissions received by the ICBINB workshop. Reviewers were informed in advance that some submissions might be AI-generated, but were not told which submissions were produced by THE AI Scientist-v2. Reviewers could also opt out of reviewing potentially AI-generated manuscripts.</li>
<li>Review Outcomes and Acceptance Decisions: Among the three manuscripts produced by THE AI Scientist-v2, one manuscript achieved a sufficiently high average reviewer score ( 6.33 out of 10 , with individual scores of 6,6 , and 7 ) to surpass the workshop's acceptance threshold. The remaining two submissions received lower scores and were not accepted.</li>
<li>Post-Review Withdrawal: Prior to the workshop submission, we arranged with the workshop organizers and ICLR leadership that any accepted AI-generated manuscripts would be withdrawn after the review process. This decision was made to avoid prematurely setting a precedent for purely AI-generated research entering the official scientific record without broader community discussion and consensus. Reviewers were informed of the experiment only after peer review.</li>
</ol>
<p>In addition to the official workshop peer reviews, we also conducted a thorough internal evaluation of all three AI-generated manuscripts. Our internal review carefully examined the experimental rigor, clarity of presentation, methodological soundness, and novelty of the generated manuscripts. We concluded that none of the manuscripts met the quality standards typical of top-tier main-track conferences. However, we thought that one submission was indeed sufficiently compelling to receive acceptance at the workshop level, and this is the same manuscript the workshop peer review process accepted. This outcome provides encouraging evidence that manuscripts autonomously generated by The AI Scientist-v2 can produce research on par with top-tier Machine Learning workshop papers (see detailed internal analyses in 4.2).
Observations and Insights. Our internal inspection of the generated experiments and code revealed several noteworthy limitations. First, The AI Scientist-v2 occasionally introduced inaccuracies in citations, similar to the well-known "hallucination" issue encountered in large language models. Second, while the system successfully executed standard experimental pipelines, it sometimes lacked the detailed methodological rigor and in-depth analysis typically required for acceptance at leading main conferences. However, such limitations did not prevent acceptance at the workshop level.</p>
<p>Transparency and Ethical Considerations. We believe it is crucial for the scientific community to engage openly and transparently with AI-generated research, subjecting it to the same rigorous peer-review processes applied to human-authored work. However, responsible oversight is essential. In conducting this evaluation, we obtained IRB approval from the University of British Columbia (H24-02652). We ensured full transparency and coordination with ICLR leadership and the workshop organizers. Before the review process, reviewers were explicitly informed that some submissions could be AI-generated and offered the option to opt out. Following acceptance, we withdrew the AI-generated manuscript prior to publication, which is consistent with our commitment to avoid prematurely inserting purely AI-generated works into the official scientific record without broader community discussion. We emphasize that the community has not yet reached a consensus on integrating AI-generated research into formal scientific publications, making careful and transparent experimentation essential at this preliminary stage. Additionally, we believe that all AI-generated papers should be clearly labeled as such in any public arena, and in The AI Scientist-v1 and The AI Scientist-v2 always make sure to do so.</p>
<h1>4.2. The first AI-generated peer-reviewed workshop paper.</h1>
<p>Paper Generation Process. The generation process for the workshop-accepted paper began with the generalized idea generation phase (\$3.1), prompted with the workshop's theme (ICBINB's focus on negative results and unexpected findings) extracted from the official website. In this phase, the system generated around twenty potential research ideas, all centered on core machine learning topics. To encourage a more applied perspective, we then modified the system prompt to focus on ideas involving the use of deep learning in real-world domains such as finance, psychology, agriculture, environmental science, and public health. This second phase produced another set of roughly twenty research ideas. From this combined AI-generated pool, we selected the three most promising initial ideas-two from the first batch, and one from the second batch-based on alignment with the workshop theme and potential interest, focusing on topics representing distinct research directions. This initial idea selection step allowed us to manage computational resources by choosing which distinct, AIgenerated starting points to explore further with the full system. It did not involve modifying the ideas themselves. All three generated ideas resulted in a workshop-submitted paper (included in full in Appendix C). For each selected idea, the system autonomously executed the full experimental pipeline using the parallelized agentic tree search (\$3.2.2) multiple times, each initiated with a different random seed. From the multiple complete manuscripts generated for each initial idea (i.e., one manuscript per seed), we selected the single best-resulting manuscript for submission based on a careful inspection of its overall coherence and scientific quality. This process mimics a professor reviewing the work of many students or teams and deciding which work is ready to be submitted for peer review. Our current study aims to see whether The AI Scientist-v2 can produce at least one paper that survives peer review, and not what fraction of the time it can do so. That is an interesting question for future work and is likely best done after additional improvements are made in the next generation of The AI Scientist. In the reflection stage of the writeup for each run, The AI Scientist-v2 is prompted with the target page lengths (e.g., the 4-page limit for the workshop) alongside the current length of the compiled PDF. This allowed the system to ensure that the final output adhered to submission guidelines without manual text editing within that specific run.</p>
<p>Crucially, while humans initiated the process by providing the high-level workshop theme and selected which initial AI-generated ideas to run multiple times through the full pipeline (akin to deciding which experiments to fund or prioritize), and subsequently selected the most promising complete output from those multiple runs, the entire process within any single run-hypothesis refinement, code generation, execution, analysis, visualization, and writing-was performed autonomously by The AI Scientist-v2. No human edited the generated code, experimental results, figures, or manuscript text of the selected final manuscript. The selection of initial ideas from the AI's output, the execution of multiple seeds, the subsequent selection of the best complete run, and the automated handling of length constraints represent high-level experimental setup and process management (meta-selection from fully autonomous outputs), not human-in-the-loop intervention in the scientific content generation of the chosen manuscript. The system, if run for sufficiently many seeds, would have generated similar outputs, requiring only the final selection step to be performed by humans. Even this could have been avoided were we willing to send all generated papers to peer review, which we did not want to do. Therefore, all submitted content was entirely generated by THE AI SCIENTIST-v2.</p>
<p>Workshop-Accepted Paper Content. The paper investigates the use of compositional regularization to improve generalization in neural networks. The AI Scientist-v2 proposes adding an explicit regularization term to the training loss function, encouraging networks to develop compositional representations to encourage representations to not change much over time while processing inputs. However, contrary to its expectations, experiments using synthetic arithmetic expression datasets revealed that this approach did not significantly enhance generalization performance. In fact, com-</p>
<p>positional regularization sometimes hindered model training. Furthermore, increasing arithmetic expression complexity made generalization even worse, irrespective of regularization. The paper concludes that explicitly enforcing compositional structures via regularization alone may not be sufficient and highlights potential conflicts between compositional regularization and the primary learning objective. It recommends future exploration of alternative regularization methods and different architectural approaches to better address compositional generalization issues. We provide the full annotated paper in Appendix C.</p>
<h1>Initial Idea for the Workshop-Accepted Paper</h1>
<div class="codehilite"><pre><span></span><code><span class="s2">&quot;Title&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;Enhancing Compositional Generalization in Neural Networks via Compositional</span>
<span class="n">Regularization</span><span class="s2">&quot;,</span>
<span class="s2">&quot;Short Hypothesis&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;Introducing a compositional regularization term during training can</span>
<span class="n">encourage</span><span class="w"> </span><span class="n">neural</span><span class="w"> </span><span class="n">networks</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">develop</span><span class="w"> </span><span class="n">compositional</span><span class="w"> </span><span class="n">representations</span><span class="p">,</span><span class="w"> </span><span class="n">thereby</span><span class="w"> </span><span class="n">improving</span><span class="w"> </span><span class="n">their</span>
<span class="n">ability</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">generalize</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">novel</span><span class="w"> </span><span class="n">combinations</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">known</span><span class="w"> </span><span class="n">components</span><span class="o">.</span><span class="s2">&quot;,</span>
<span class="s2">&quot;Experiments&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span>
<span class="w">    </span><span class="s2">&quot;Implement the compositional regularization term and integrate it into the loss function of</span>
<span class="w">    </span><span class="n">standard</span><span class="w"> </span><span class="n">sequence</span><span class="o">-</span><span class="n">to</span><span class="o">-</span><span class="n">sequence</span><span class="w"> </span><span class="n">neural</span><span class="w"> </span><span class="n">network</span><span class="w"> </span><span class="n">architectures</span><span class="w"> </span><span class="n">with</span><span class="w"> </span><span class="n">attention</span><span class="w"> </span><span class="n">mechanisms</span><span class="o">.</span><span class="s2">&quot;,</span>
<span class="w">    </span><span class="s2">&quot;Train models on synthetic datasets like SCAN and COGS, evaluating performance on</span>
<span class="w">    </span><span class="n">compositional</span><span class="w"> </span><span class="n">generalization</span><span class="w"> </span><span class="n">tasks</span><span class="w"> </span><span class="n">with</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="n">without</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">regularization</span><span class="w"> </span><span class="n">term</span><span class="o">.</span><span class="s2">&quot;,</span>
<span class="w">    </span><span class="s2">&quot;Apply the method to real-world tasks such as machine translation using the IWOLT dataset</span>
<span class="w">    </span><span class="ow">and</span><span class="w"> </span><span class="n">semantic</span><span class="w"> </span><span class="n">parsing</span><span class="w"> </span><span class="n">with</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">GeoQuery</span><span class="w"> </span><span class="n">dataset</span><span class="p">,</span><span class="w"> </span><span class="n">assessing</span><span class="w"> </span><span class="n">improvements</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">generalization</span><span class="w"> </span><span class="n">to</span>
<span class="w">    </span><span class="n">new</span><span class="w"> </span><span class="n">language</span><span class="w"> </span><span class="n">constructs</span><span class="o">.</span><span class="s2">&quot;,</span>
<span class="w">    </span><span class="s2">&quot;Analyze the learned representations by visualizing embedding spaces and utilizing</span>
<span class="w">    </span><span class="n">compositionality</span><span class="w"> </span><span class="n">metrics</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">assess</span><span class="w"> </span><span class="n">how</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">regularization</span><span class="w"> </span><span class="n">affects</span><span class="w"> </span><span class="n">internal</span>
<span class="w">    </span><span class="n">representations</span><span class="o">.</span><span class="s2">&quot;,</span>
<span class="w">    </span><span class="s2">&quot;Conduct ablation studies to determine the impact of different strengths of the</span>
<span class="w">    </span><span class="n">regularization</span><span class="w"> </span><span class="n">term</span><span class="p">,</span><span class="w"> </span><span class="n">identifying</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">optimal</span><span class="w"> </span><span class="n">balance</span><span class="w"> </span><span class="n">between</span><span class="w"> </span><span class="n">enforcing</span><span class="w"> </span><span class="n">compositionality</span><span class="w"> </span><span class="ow">and</span>
<span class="w">    </span><span class="n">maintaining</span><span class="w"> </span><span class="n">overall</span><span class="w"> </span><span class="n">performance</span><span class="o">.</span><span class="s2">&quot;,</span>
<span class="w">    </span><span class="s2">&quot;Compare the proposed method against other approaches aimed at improving compositional</span>
<span class="w">    </span><span class="n">generalization</span><span class="p">,</span><span class="w"> </span><span class="n">such</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="n">meta</span><span class="o">-</span><span class="n">learning</span><span class="w"> </span><span class="n">techniques</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="n">specialized</span><span class="w"> </span><span class="n">architectures</span><span class="o">.</span><span class="s2">&quot;</span>
<span class="p">],</span>
<span class="p">]</span>
</code></pre></div>

<p>Paper Assessment by the Authors. In our review, we evaluated the technical aspects of this paper and identified several strengths and weaknesses. We appreciated the exploration of temporal consistency regularization-penalizing large changes in embedding representations between successive tokens-as an interesting method to enhance compositional generalization. The synthetic arithmetic task chosen by the authors was appropriate, providing a suitable setting to test their hypothesis across varying levels of complexity. However, we noted several areas requiring improvement. First, the description of the regularization term was unclear and potentially misleading, as readers might incorrectly assume it was applied to the LSTM hidden states rather than input embeddings. We recommended clarifying this explicitly by adding a code appendix or conducting additional ablations applying the regularization to LSTM hidden states. Second, the paper omitted key references, notably Hochreiter and Schmidhuber (1997), and instead relied on general textbook citations. Additionally, we found inaccuracies in some figures and descriptions: specifically, the caption of Figure 3 incorrectly interpreted validation loss, and Figure 5's attention-based model clearly outperformed the LSTM model, contradicting the authors' claims. Furthermore, we found the experimental evaluation limited, as the tasks were restricted to short sequences and synthetic data. We suggested extending the evaluation to include real-world tasks, longer sequences, larger models, and a deeper analysis.</p>
<p>Our examination of the code revealed potential issues with dataset overlapapproximately 57\% overlap between training and test sets-which could significantly affect the reliability of the results. Additionally, we identified confusion in the paper's terminology regarding "embedding states" versus "hidden states," which should be clarified for precision. We also questioned the reported $100 \%$ accuracy of the attention-augmented LSTM model, as our additional tests indicated that this performance was primarily due to task simplicity and significantly decreased when task complexity increased. Overall,</p>
<p>we considered the paper technically sound and a borderline accept for the workshop, acknowledging its valuable insights and intriguing ideas. However, we concluded it lacks sufficient depth and rigor for acceptance into a full conference without addressing the highlighted concerns.</p>
<p>Paper Assessment by Human Workshop Reviewers. The reviewers generally agree that the paper addresses an important topic-compositional generalization in neural networks and appreciate the authors' proposed compositional regularization method, as well as their detailed analysis of unexpected results. All reviewers recognize the paper's strength in clearly presenting why the regularization term does not yield the anticipated improvements, emphasizing its informative negative results. However, the reviewers highlight several areas for improvement:</p>
<p>Justification and Intuition: All reviewers suggest the need for clearer justification or intuition behind why penalizing large changes between successive hidden states might improve compositionality. They recommend adding references to related works, theoretical motivations, or visual explanations to strengthen the rationale.
Network Architecture Generalization: Reviewers emphasize that since only the LSTM architecture was evaluated, the findings should not be generalized across all neural network types. They suggest experimenting with additional architectures, such as transformers, to better understand the impact of the regularization across different neural network models.
Experimental Breadth: Reviewers suggest extending the evaluation to other tasks or datasets beyond synthetic arithmetic expressions to further validate the generalizability of the conclusions.
Overall: The reviewers recommend acceptance to the workshop due to the paper's insightful exploration and clear analysis despite its negative results. They encourage further elaboration on methodological motivations, additional experimental evaluations, and clearer connections between compositional regularization and the complexity of compositional tasks. The paper received scores of 6 (weak accept), 7 (accept), and 6 (weak accept). Below, we include two of the reviews for which we obtained explicit permission from the reviewers to include them in our report. The remaining reviewer did not respond to our request.</p>
<h1>Reviewer # 1: A good paper analyzing the effectiveness of a compositional regularization term for LSTMs</h1>
<p>Summary: The authors propose a regularization term to enhance compositional regularization in neural networks. The idea is to penalize large deviations between subsequent time steps of the hidden state, thus "squeezing" the hidden state to encourage composition and preventing a dominating representation. The authors test their approach on synthetic arithmetic expression with varying operator complexity and length. They show that although the regularization term appears to be working, it counterintuitively does not improve test accuracy. Furthermore, the authors identify a bottleneck regarding network capacity with increasing arithmetic operators.</p>
<p>Strengths:
I find the idea of regularizing or squeezing the hidden representations to encourage compositionally an interesting idea. The authors define a good baseline and ablate their method well against it, revealing why the regularization term does not work as expected. I think the insight that operator complexity is a bottleneck for the neural network is important, as it raises the question whether architectural changes might be more effective for compositionally than regularization.</p>
<p>Weaknesses:
The paper would benefit from more intuition as to why the proposed regularization term should encourage compositionality. This could be either an experiment or simply a visualization for the reader. Only one architecture (LSTM) was tested. It would be interesting to see if transformer architectures fare better with compositionality due to the attention mechanism.
I think the connection between compositional regularization and operator complexity needs to be made more explicit. From reading the introduction both arguments seem a bit disconnected although I can infer the authors intentions.</p>
<p>Conclusion:
Overall, I would accept this paper to the workshop, since it proposes a simple and interesting</p>
<div class="codehilite"><pre><span></span><code><span class="n">idea</span><span class="w"> </span><span class="n">with</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">authors</span><span class="w"> </span><span class="n">providing</span><span class="w"> </span><span class="n">ablations</span><span class="w"> </span><span class="n">that</span><span class="w"> </span><span class="n">encourage</span><span class="w"> </span><span class="n">further</span><span class="w"> </span><span class="n">analysis</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">problem</span><span class="p">.</span><span class="w"> </span><span class="n">As</span><span class="w"> </span><span class="n">a</span>
<span class="n">suggestion</span><span class="w"> </span><span class="n">I</span><span class="w"> </span><span class="n">would</span><span class="w"> </span><span class="n">encourage</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">authors</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">give</span><span class="w"> </span><span class="n">more</span><span class="w"> </span><span class="n">intuition</span><span class="w"> </span><span class="n">on</span><span class="w"> </span><span class="n">why</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">proposed</span>
<span class="n">regularization</span><span class="w"> </span><span class="n">term</span><span class="w"> </span><span class="n">should</span><span class="w"> </span><span class="n">improve</span><span class="w"> </span><span class="n">compositionality</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">proposed</span><span class="w"> </span><span class="n">network</span><span class="p">.</span><span class="w"> </span><span class="n">I</span><span class="w"> </span><span class="n">would</span><span class="w"> </span><span class="n">suggest</span>
<span class="n">either</span><span class="w"> </span><span class="n">adding</span><span class="w"> </span><span class="n">more</span><span class="w"> </span><span class="n">related</span><span class="w"> </span><span class="n">work</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">support</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">regularisation</span><span class="w"> </span><span class="n">term</span><span class="w"> </span><span class="k">or</span><span class="w"> </span><span class="n">elaborating</span><span class="w"> </span><span class="n">on</span><span class="w"> </span><span class="n">the</span>
<span class="n">intuition</span><span class="w"> </span><span class="n">behind</span><span class="w"> </span><span class="n">penalising</span><span class="w"> </span><span class="n">subsequent</span><span class="w"> </span><span class="n">steps</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">hidden</span><span class="w"> </span><span class="n">state</span><span class="p">.</span>
<span class="nl">Rating:</span><span class="w"> </span><span class="mh">7</span><span class="o">:</span><span class="w"> </span><span class="n">Good</span><span class="w"> </span><span class="n">paper</span><span class="p">,</span><span class="w"> </span><span class="n">accept</span>
<span class="nl">Award:</span><span class="w"> </span><span class="n">No</span><span class="w"> </span><span class="n">Award</span>
<span class="nl">Confidence:</span><span class="w"> </span><span class="mh">4</span><span class="o">:</span><span class="w"> </span><span class="n">The</span><span class="w"> </span><span class="n">reviewer</span><span class="w"> </span><span class="n">is</span><span class="w"> </span><span class="n">confident</span><span class="w"> </span><span class="n">but</span><span class="w"> </span><span class="k">not</span><span class="w"> </span><span class="n">absolutely</span><span class="w"> </span><span class="n">certain</span><span class="w"> </span><span class="n">that</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">evaluation</span><span class="w"> </span><span class="n">is</span>
<span class="n">correct</span>
</code></pre></div>

<h1>Reviewer #2: Compositional Regularization: Unexpected Obstacles in Enhancing Neural Network Generalization</h1>
<p>This paper investigates the effectiveness of incorporating a compositional regularization term into the loss function of neural networks to improve compositional generalization. The authors hypothesized that penalizing deviations from compositional structures would enhance the model's ability to generalize to unseen arithmetic expressions. However, their results on synthetic arithmetic datasets showed that compositional regularization did not lead to significant improvements and, in some cases, even hindered learning.</p>
<p>I think this paper greatly contributes to the workshops theme and fits into the scope. Moreover, it is a great example of challenges that occur during such approaches and could be interesting to discuss in the workshop setting. While I think that the authors should further broaden the experiments to other tasks in order to increase the generalizability of the findings, I would still recommend to accept the paper.</p>
<p>Rating: 6: Marginally above acceptance threshold
Award: No Award
Confidence: 2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper</p>
<h2>5. Limitations \&amp; Ethical Considerations</h2>
<p>While The AI Scientist-v2 demonstrates significant progress by successfully generating a peer-reviewed workshop paper, it is important to contextualize this achievement clearly. First, the acceptance occurred at a workshop level rather than at the main conference track, and only one of the three AI-generated submissions was accepted. Workshop papers generally report preliminary results and exploratory work, and acceptance rates at workshops (typically 60-80\%) are notably higher than at main conference tracks ( $20-30 \%$ for leading machine learning venues such as ICLR, ICML, and NeurIPS). Thus, the current version of The AI Scientist-v2 does not yet consistently reach the rigorous standard required for top-tier conference publications, nor does it even reach workshop-level consistently.</p>
<p>Moreover, despite the structured agentic tree search and enhanced autonomy introduced in The AI SCIENTIST-v2, certain aspects of scientific inquiry-such as formulating genuinely novel, highimpact hypotheses, designing truly innovative experimental methodologies, or rigorously justifying design choices with deep domain expertise-remain challenging for purely automated systems. Addressing these limitations in future iterations will be essential to move beyond preliminary or incremental scientific results toward consistently high-quality, conference-level contributions.</p>
<p>As LLMs rapidly advance, future versions of our system will likely overcome many current limitations. Therefore, we believe it is important for the scientific community to study the quality of AI-generated research, and one of the best ways to do so is to submit (with appropriate permissions) a small sample of it to the same peer-review processes used to evaluate human work. We conducted this study with full cooperation from both ICLR leadership and the workshop organizers, and received IRB approval from the University of British Columbia (H24-02652). Per agreement with ICLR workshop organizers, our AI-generated papers will not appear on OpenReview's public forum and have already</p>
<p>been withdrawn. As a community, we need to establish norms for AI-generated science-including disclosure requirements and timing. We advocate for transparency about AI-generated content, though questions remain about whether work should first be judged on merit to avoid bias. Going forward, we will continue to exchange opinions with the research community on the state of this technology to ensure it does not evolve solely to game peer review or artificially inflate the CVs of unscrupulous scientists, which would undermine the meaning of the scientific peer review and evaluation processes.</p>
<h1>6. Related Work</h1>
<p>Recent advancements have substantially expanded the field of automated scientific discovery, particularly through approaches leveraging artificial intelligence (AI). Early end-to-end approaches, exemplified by The AI Scientist-v1 (Lu et al., 2024), introduced fully automated frameworks, such as AI-Researcher (Data Intelligence Lab, 2025), capable of autonomously navigating the entire research pipeline. Subsequent works, however, often incorporate varying degrees of human oversight, as demonstrated by Intology (Intology AI, 2025) and Carl (AutoScience AI, 2025). Other systems narrow the scope; for example, CycleResearcher (Weng et al., 2025) focuses specifically on the path from idea generation to manuscript drafting, explicitly excluding experimental execution. Alternative approaches include protocol designs for experiments in self-driving laboratories that do not rely on large language models (LLMs) or use them in complementary roles (Shi et al., 2025). Several concurrent works explore similar territories, including Agent Laboratory (Schmidgall et al., 2025) and agentRxiv (Schmidgall and Moor, 2025), highlighting the rapid development in this area.</p>
<p>LLM-based scientific idea generation has been explicitly investigated in recent studies. Notably, Si et al. (2025) examined the capabilities of LLMs to generate human-level scientific ideas, finding through human evaluations that LLM-generated ideas were typically more novel but often less feasible than those proposed by human experts. GraphEval (Feng et al., 2025) offers graph-based methods for evaluating research ideas, further highlighting the current limitations of LLMs in accurate idea assessment.</p>
<p>Several benchmarks have been established to systematically evaluate AI performance in scientific tasks. MLEBench (Chan et al., 2025) and Aide (Jiang et al., 2025) provide structured environments to assess model capabilities on tasks representative of research engineering workloads. The METR Research Engineer benchmark (Wijk et al., 2024), for instance, demonstrates AI superiority in executing short-duration tasks (sub-2-hour tasks). Comprehensive reviews, such as the one by Eger et al. (2025), document the role and effectiveness of LLMs in scientific workflows. Coding-specific benchmarks such as SciCode (Tian et al., 2024), curated explicitly by domain scientists, address problems across physics, chemistry, and biology, encompassing structured sub-problems to rigorously evaluate research-related programming skills. Similarly, BixBench focuses on computational biology, providing comprehensive evaluations of LLM-based agents (Mitchener et al., 2025). Additionally, independent evaluations specifically target AI scientist frameworks, like the evaluation of The AI Scientist-v1 by Beel et al. (2025), further delineate AI capabilities in this domain.</p>
<p>Industry efforts, including Google's AI Research Copilot (also known as AI Co-Scientist, Gottweis et al., 2025), exemplify contributions from major technology companies to this growing field. Conceptually, Bengio et al. (2025) draws a distinction between agentic AI systems and Scientist AIs, emphasizing that the latter focus primarily on deepening the understanding of data rather than pursuing goaldirected interactions with the world. This distinction underscores the varying philosophical and methodological perspectives driving contemporary automated scientific discovery efforts.</p>
<h1>7. Conclusion</h1>
<p>In this work, we introduced The AI Scientist-v2, a significantly improved automated scientific discovery system featuring enhanced autonomy and exploration capabilities. Compared to its predecessor, The AI Scientist-v1, our system removes reliance on human-crafted templates, incorporates a structured and exploratory agentic tree search methodology supervised by an experiment manager agent, and integrates Vision-Language Model (VLM) feedback loops for iterative refinement of visualizations and manuscript quality. We demonstrated that The AI Scientist-v2 is capable of autonomously generating manuscripts that successfully pass peer review at a workshop of a major machine learning conference.</p>
<p>This achievement, the first instance of a fully AI-generated paper navigating peer review, marks a notable milestone and shows promising early signs of progress, even considering the limitations discussed regarding workshop versus conference standards (5). While significant challenges remain in consistently achieving top-tier quality and generating truly groundbreaking hypotheses, the capabilities demonstrated here suggest a clear trajectory. We believe that such advancements signal that next-generation AI Scientists will herald a new era in science. This is just the beginning; we expect AI capabilities to continue improving, potentially at an exponential rate. At some point in the future, AI will likely generate papers that match or exceed human quality, even at the highest levels of scientific publishing.</p>
<p>Ultimately, overcoming current limitations and scaling these systems holds immense potential. We believe what matters most is not simply how AI science compares to human science, but whether its discoveries aid in human flourishing, such as curing diseases or expanding our knowledge of the laws that govern our universe. By developing systems like The AI Scientist-v2 and sharing them openly, we look forward to helping usher in this era of AI science contributing to the betterment of humanity, fostering collaboration and accelerating the pace of discovery.</p>
<h2>References</h2>
<p>AutoScience AI. Meet Carl: The First AI System to Produce Academically Peer-Reviewed Research, 2025. URL https://www.autoscience.ai/blog/meet-carl-the-first-ai-system-t o-produce-academically-peer-reviewed-research. Accessed: 2025-03-21.</p>
<p>Joeran Beel, Min-Yen Kan, and Moritz Baumgart. An evaluation of sakana's ai scientist for autonomous research: Wishful thinking or an emerging reality towards' artificial general research intelligence'(agri)? arXiv preprint arXiv:2502.14297, 2025.</p>
<p>Yoshua Bengio, Michael Cohen, Damiano Fornasiere, Joumana Ghosn, Pietro Greiner, Matt MacDermott, Sren Mindermann, Adam Oberman, Jesse Richardson, Oliver Richardson, Marc-Antoine Rondeau, Pierre-Luc St-Charles, and David Williams-King. Superintelligent agents pose catastrophic risks: Can scientist ai offer a safer path?, 2025. URL https://arxiv.org/abs/2502.15657.</p>
<p>Jun Shern Chan, Neil Chowdhury, Oliver Jaffe, James Aung, Dane Sherburn, Evan Mays, Giulio Starace, Kevin Liu, Leon Maksin, Tejal Patwardhan, Aleksander Madry, and Lilian Weng. MLEbench: Evaluating machine learning agents on machine learning engineering. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/f orum?id=6s5uXNWGIh.</p>
<p>Jeff Clune. Ai-gas: Ai-generating algorithms, an alternate paradigm for producing general artificial intelligence. CoRR, abs/1905.10985, 2019. URL http://arxiv.org/abs/1905.10985.</p>
<p>Cristina Cornelio, Sanjeeb Dash, Vernon Austel, Tyler R. Josephson, Joao Goncalves, Kenneth L. Clarkson, Nimrod Megiddo, Bachir El Khadir, and Lior Horesh. Combining data and theory for</p>
<p>derivable scientific discovery with ai-descartes. Nature Communications, 14(1):1777, Apr 2023. ISSN 2041-1723. doi: 10.1038/s41467-023-37236-y. URL https://doi.org/10.1038/s414 $67-023-37236-y$.</p>
<p>The University of Hong Kong Data Intelligence Lab. Ai-researcher: Fully-automated scientific discovery with llm agents, 2025. URL https://github.com/HKUDS/AI-Researcher.</p>
<p>Steffen Eger, Yong Cao, Jennifer D'Souza, Andreas Geiger, Christian Greisinger, Stephanie Gross, Yufang Hou, Brigitte Krenn, Anne Lauscher, Yizhi Li, et al. Transforming science with large language models: A survey on ai-assisted scientific discovery, experimentation, content generation, and evaluation. arXiv preprint arXiv:2502.05151, 2025.</p>
<p>Tao Feng, Yihang Sun, and Jiaxuan You. Grapheval: A lightweight graph-based LLM framework for idea evaluation. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/forum?id=5RUM1aIdok.</p>
<p>Paul Gauthier. Aider is ai pair programming in your terminal, 2024. URL https://aider. chat/. https://aider.chat/.</p>
<p>Yolanda Gil, Mark Greaves, James Hendler, and Haym Hirsh. Amplify scientific discovery with artificial intelligence. Science, 346(6206):171-172, 2014. doi: 10.1126/science.1259439. URL https://www.science.org/doi/abs/10.1126/science. 1259439.</p>
<p>Juraj Gottweis, Wei-Hung Weng, Alexander Daryin, Tao Tu, Anil Palepu, Petar Sirkovic, Artiom Myaskovsky, Felix Weissenberger, Keran Rong, Ryutaro Tanno, et al. Towards an ai co-scientist. arXiv preprint arXiv:2502.18864, 2025.</p>
<p>Intology AI. Zochi Tech Report, 2025. URL https://www.intology.ai/blog/zochi-tech-r eport. Accessed: 2025-03-21.</p>
<p>Zhengyao Jiang, Dominik Schmidt, Dhruv Srikanth, Dixing Xu, Ian Kaplan, Deniss Jacenko, and Yuxiang Wu. Aide: Ai-driven exploration in the space of code, 2025. URL https://arxiv.org/ abs/2502.13138.</p>
<p>Ross D. King, Jem Rowland, Stephen G. Oliver, Michael Young, Wayne Aubrey, Emma Byrne, Maria Liakata, Magdalena Markham, Pinar Pir, Larisa N. Soldatova, Andrew Sparkes, Kenneth E. Whelan, and Amanda Clare. The automation of science. Science, 324(5923):85-89, 2009. doi: 10.1126/sc ience.1165620. URL https://www.science.org/doi/abs/10.1126/science. 1165620.</p>
<p>Hiroaki Kitano. Nobel turing challenge: creating the engine for scientific discovery. npj Systems Biology and Applications, 7(1):29, Jun 2021. ISSN 2056-7189. doi: 10.1038/s41540-021-00189-3. URL https://doi.org/10.1038/s41540-021-00189-3.</p>
<p>Chris Lu, Cong Lu, Robert Tjarko Lange, Jakob Foerster, Jeff Clune, and David Ha. The ai scientist: Towards fully automated open-ended scientific discovery. arXiv preprint arXiv:2408.06292, 2024.</p>
<p>Ludovico Mitchener, Jon M Laurent, Benjamin Tenmann, Siddharth Narayanan, Geemi P Wellawatte, Andrew White, Lorenzo Sani, and Samuel G Rodriques. Bixbench: a comprehensive benchmark for llm-based agents in computational biology. arXiv preprint arXiv:2503.00096, 2025.</p>
<p>Jean-Baptiste Mouret and Jeff Clune. Illuminating search spaces by mapping elites. ArXiv, abs/1504.04909, 2015. URL https://api.semanticscholar.org/CorpusID:14759751.</p>
<p>OpenAI. Openai o1 system card, 2024. URL https://api.semanticscholar.org/CorpusID: 272648256 .</p>
<p>Samuel Schmidgall and Michael Moor. Agentrxiv: Towards collaborative autonomous research. arXiv preprint arXiv:2503.18102, 2025.</p>
<p>Samuel Schmidgall, Yusheng Su, Ze Wang, Ximeng Sun, Jialian Wu, Xiaodong Yu, Jiang Liu, Zicheng Liu, and Emad Barsoum. Agent laboratory: Using llm agents as research assistants. arXiv preprint arXiv:2501.04227, 2025.</p>
<p>Yu-Zhe Shi, Mingchen Liu, Fanxu Meng, Qiao Xu, Zhangqian Bi, Kun He, Lecheng Ruan, and Qining Wang. Hierarchically encapsulated representation for protocol design in self-driving labs. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openrevi ew.net/forum?id=9nUBh4V6SA.</p>
<p>Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. Reflexion: Language agents with verbal reinforcement learning. Advances in Neural Information Processing Systems, 36, 2024.</p>
<p>Chenglei Si, Diyi Yang, and Tatsunori Hashimoto. Can LLMs generate novel research ideas? a large-scale human study with 100+ NLP researchers. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/forum?id=M23dTGWCZy.</p>
<p>Minyang Tian, Luyu Gao, Shizhuo Dylan Zhang, Xinan Chen, Cunwei Fan, Xuefei Guo, Roland Haas, Pan Ji, Kittithat Krongchon, Yao Li, Shengyan Liu, Di Luo, Yutao Ma, Hao Tong, Kha Trinh, Chenyu Tian, Zihan Wang, Bohao Wu, Yanyu Xiong, Shengzhu Yin, Minhui Zhu, Kilian Lieret, Yanxin Lu, Genglin Liu, Yufeng Du, Tianhua Tao, Ofir Press, Jamie Callan, Eliu Huerta, and Hao Peng. Scicode: A research coding benchmark curated by scientists, 2024.</p>
<p>Hanchen Wang, Tianfan Fu, Yuanqi Du, Wenhao Gao, Kexin Huang, Ziming Liu, Payal Chandak, Shengchao Liu, Peter Van Katwyk, Andreea Deac, Anima Anandkumar, Karianne Bergen, Carla P. Gomes, Shirley Ho, Pushmeet Kohli, Joan Lasenby, Jure Leskovec, Tie-Yan Liu, Arjun Manrai, Debora Marks, Bharath Ramsundar, Le Song, Jimeng Sun, Jian Tang, Petar Velikovi, Max Welling, Linfeng Zhang, Connor W. Coley, Yoshua Bengio, and Marinka Zitnik. Scientific discovery in the age of artificial intelligence. Nature, 620(7972):47-60, Aug 2023. ISSN 1476-4687. doi: 10.1038/s41586-023-06221-2. URL https://doi.org/10.1038/s41586-023-06221-2.</p>
<p>Xingyao Wang, Yangyi Chen, Lifan Yuan, Yizhe Zhang, Yunzhu Li, Hao Peng, and Heng Ji. Executable code actions elicit better llm agents. In Proceedings of the 41st International Conference on Machine Learning, ICML'24. JMLR.org, 2024.</p>
<p>Yixuan Weng, Minjun Zhu, Guangsheng Bao, Hongbo Zhang, Jindong Wang, Yue Zhang, and Linyi Yang. Cycleresearcher: Improving automated research via automated review. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/f orum?id=bjcsVLoHYs.</p>
<p>Hjalmar Wijk, Tao R. Lin, Joel Becker, Sami Jawhar, Neev Parikh, Thomas Broadley, Lawrence Chan, Michael Chen, Josh Clymer, Jai Dhyani, Elena Ericheva, Katharyn Garcia, Brian Goodrich, Nikola Jurkovic, Megan Kinniment, Aron Lajko, Seraphina Nix, Lucas Jun Koba Sato, William Saunders, Maksym Taran, Ben West, and Elizabeth Barnes. Re-bench: Evaluating frontier ai r\&amp;d capabilities of language model agents against human experts. ArXiv, abs/2411.15114, 2024. URL https://api.semanticscholar.org/CorpusID:274192262.</p>
<p>Yongjun Xu, Xin Liu, Xin Cao, Changping Huang, Enke Liu, Sen Qian, Xingchen Liu, Yanjun Wu, Fengliang Dong, Cheng-Wei Qiu, Junjun Qiu, Keqin Hua, Wentao Su, Jian Wu, Huiyu Xu, Yong Han, Chenguang Fu, Zhigang Yin, Miao Liu, Ronald Roepman, Sabine Dietmann, Marko Virta,</p>
<p>Fredrick Kengara, Ze Zhang, Lifu Zhang, Taolan Zhao, Ji Dai, Jialiang Yang, Liang Lan, Ming Luo, Zhaofeng Liu, Tao An, Bin Zhang, Xiao He, Shan Cong, Xiaohong Liu, Wei Zhang, James P. Lewis, James M. Tiedje, Qi Wang, Zhulin An, Fei Wang, Libo Zhang, Tao Huang, Chuan Lu, Zhipeng Cai, Fang Wang, and Jiabao Zhang. Artificial intelligence: A powerful paradigm for scientific research. The Innovation, 2(4), Nov 2021. ISSN 2666-6758. doi: 10.1016/j.xinn.2021.100179. URL https://doi.org/10.1016/j.xinn.2021.100179.</p>
<h1>Author Contributions</h1>
<p>Yutaro Yamada (shared first author): Co-led the project and contributed core ideas. Coded the core tree-search and template-free version of the AI Scientist v2. Ran paper generation experiments. Read and validated the work of many AI-generated papers to select submissions and checked the paper code implementations. Led the writing of the paper. Wrote detailed analyses of the submitted papers for our manuscript.</p>
<p>Robert Tjarko Lange (shared first author): Co-initiated, co-led the project and contributed core ideas. Coded core parts of VLM AI Reviewer, tailored the paper generation pipeline to the workshop and ran the paper generation experiments. Organized the workshop communication process. Read and validated the work of many AI-generated papers to select submissions and checked the paper code implementations. Led the writing of the paper. Wrote detailed analyses of the submitted papers for our manuscript.</p>
<p>Cong Lu (shared first author): Co-initiated, co-led the project and contributed core ideas. Coded core parts of the improved idea generation, tool use, experiment aggregation, and paper writing framework. Evaluated AI-generated paper submissions. Wrote and led the IRB approval process. Led the writing of the paper.</p>
<p>Shengran Hu: Enhanced the iterative AI reviewer with VLM feedback, contributed to the experiment and paper writing framework, helped run paper generation experiments, read and validated the work of many AI-generated papers to select submissions, and checked the paper code implementations. Helped writing and iterating over drafts of the paper. Helped write the IRB approval.</p>
<p>Chris Lu: Co-initiated the project. Provided advice, feedback, and writing.
Jakob Foerster: Provided advice, feedback, and writing.
Jeff Clune (equal advising): Provided overarching guidance for the research project, offering technical insight, advice, feedback, and writing. Oversaw the IRB application process. Evaluated AI-generated paper submissions.</p>
<p>David Ha (equal advising): Provided overarching guidance for the research project, offering technical insight, advice, feedback, and writing. Oversaw the public communication process.</p>
<h1>Supplementary Material</h1>
<p>Table of Contents
A Hyperparameters ..... 21
B Prompts ..... 21
C AI Generated Papers ..... 31
C. 1 Compositional Regularization: Unexpected Obstacles in Enhancing Neural Network Generalization ..... 32
C.1.1 AI Scientist Team Review ..... 41
C.1.2 AI Scientist Team Code Review ..... 42
C. 2 Unveiling the Impact of Label Noise on Model Calibration in Deep Learning ..... 45
C.2.1 The AI Scientist-v2 Idea ..... 45
C.2.2 AI Scientist Team Review ..... 53
C.2.3 AI Scientist Team Code Review ..... 55
C.2.4 Workshop Reviews ..... 57
C. 3 Real-world Challenges in Pest Detection using Deep Learning: an Investigation into Failures and Solutions ..... 58
C.3.1 The AI Scientist-v2 Idea ..... 58
C.3.2 AI Scientist Team Review ..... 65
C.3.3 AI Scientist Team Code Review ..... 66
C.3.4 Workshop Reviews ..... 67</p>
<h1>A. Hyperparameters</h1>
<p>This section details the key hyperparameters used in The AI Scientist-v2. Model configurations for language and vision-language models are listed in Table 2. The hyperparameters governing the agentic tree search (\$3.2.2) and experiment stage (\$3.2.1) progression, including node execution limits, are shown in Table 3.</p>
<p>Table 2 | LLM and VLM Hyperparameters.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Component/Task</th>
<th style="text-align: left;">Model Used</th>
<th style="text-align: center;">Max Tokens</th>
<th style="text-align: center;">Temperature</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Code Generation (\$3.2)</td>
<td style="text-align: left;">Claude 3.5 Sonnet (v2)</td>
<td style="text-align: center;">8,192</td>
<td style="text-align: center;">0.5</td>
</tr>
<tr>
<td style="text-align: left;">LLM/VLM Feedback Agents (\$3.4)</td>
<td style="text-align: left;">GPT-4o</td>
<td style="text-align: center;">8,192</td>
<td style="text-align: center;">0.5</td>
</tr>
<tr>
<td style="text-align: left;">Summary Report Agent (\$3)</td>
<td style="text-align: left;">GPT-4o</td>
<td style="text-align: center;">8,192</td>
<td style="text-align: center;">1.0</td>
</tr>
</tbody>
</table>
<p>Table 3 | Agentic Tree Search \&amp; Execution Hyperparameters (\$3.2.2, 3.2.1).</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Hyperparameter</th>
<th style="text-align: center;">Value</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Debug Probability</td>
<td style="text-align: center;">1.0</td>
</tr>
<tr>
<td style="text-align: left;">Maximum Debug Depth</td>
<td style="text-align: center;">3</td>
</tr>
<tr>
<td style="text-align: left;">Maximum Experiment Runtime per Node</td>
<td style="text-align: center;">1 hour</td>
</tr>
<tr>
<td style="text-align: left;">Node Allocation per Stage:</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Stage 1: Preliminary Investigation</td>
<td style="text-align: center;">21 nodes</td>
</tr>
<tr>
<td style="text-align: left;">Stage 2: Hyperparameter Tuning</td>
<td style="text-align: center;">12 nodes</td>
</tr>
<tr>
<td style="text-align: left;">Stage 3: Research Agenda Execution</td>
<td style="text-align: center;">12 nodes</td>
</tr>
<tr>
<td style="text-align: left;">Stage 4: Ablation Studies</td>
<td style="text-align: center;">12 nodes</td>
</tr>
</tbody>
</table>
<p>The total time required for The AI Scientist-v2 to generate a single paper depends on the complexity of the problems. Based on our experience, this process usually takes anywhere from several hours to a maximum of 15 hours, which is the runtime limit we have set.</p>
<h2>B. Prompts</h2>
<p>In this section, we include the prompts used in all phases of The AI Scientist-v2.</p>
<h2>Idea Generation Prompt</h2>
<p># System prompt
You are an experienced AI researcher who aims to propose high-impact research ideas resembling exciting grant proposals. Feel free to propose any novel ideas or experiments; make sure they are novel. Be very creative and think out of the box. Each proposal should stem from a simple and elegant question, observation, or hypothesis about the topic. For example, they could involve very interesting and simple interventions or investigations that explore new possibilities or challenge existing assumptions. Clearly clarify how the proposal distinguishes from the existing literature.</p>
<p>Ensure that the proposal can be done starting from the provided codebase, and does not require resources beyond what an academic lab could afford. These proposals should lead to papers that are publishable at top ML conferences.</p>            </div>
        </div>

    </div>
</body>
</html>
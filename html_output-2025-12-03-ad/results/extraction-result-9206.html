<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9206 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9206</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9206</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-162.html">extraction-schema-162</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using language models for detecting anomalies in lists, sequences, or tabular data, including details of the models, data types, methods, results, comparisons, and limitations.</div>
                <p><strong>Paper ID:</strong> paper-265317916</p>
                <p><strong>Paper Title:</strong> Anomaly Detection in Time Series Data Using Reversible Instance Normalized Anomaly Transformer</p>
                <p><strong>Paper Abstract:</strong> Anomalies are infrequent in nature, but detecting these anomalies could be crucial for the proper functioning of any system. The rarity of anomalies could be a challenge for their detection as detection models are required to depend on the relations of the datapoints with their adjacent datapoints. In this work, we use the rarity of anomalies to detect them. For this, we introduce the reversible instance normalized anomaly transformer (RINAT). Rooted in the foundational principles of the anomaly transformer, RINAT incorporates both prior and series associations for each time point. The prior association uses a learnable Gaussian kernel to ensure a thorough understanding of the adjacent concentration inductive bias. In contrast, the series association method uses self-attention techniques to specifically focus on the original raw data. Furthermore, because anomalies are rare in nature, we utilize normalized data to identify series associations and employ non-normalized data to uncover prior associations. This approach enhances the modelled series associations and, consequently, improves the association discrepancies.</p>
                <p><strong>Cost:</strong> 0.008</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9206.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9206.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using language models for detecting anomalies in lists, sequences, or tabular data, including details of the models, data types, methods, results, comparisons, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RINAT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reversible Instance Normalized Anomaly Transformer</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A transformer-based unsupervised anomaly detection model for multivariate time series that combines learnable Gaussian prior associations, self-attention-based series associations, reversible instance normalization applied only to the series-association branch, and a minimax-driven association-discrepancy objective plus reconstruction loss to score anomalies.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Reversible Instance Normalized Anomaly Transformer (RINAT)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>transformer (anomaly-transformer variant)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>multivariate time series (sequence data)</td>
                        </tr>
                        <tr>
                            <td><strong>data_domain</strong></td>
                            <td>server telemetry (SMD, PSM) and satellite telemetry/spacecraft monitoring (MSL, SMAP)</td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_type</strong></td>
                            <td>rare/outlier anomalous time points and anomalous segments in time series</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>An encoder-only transformer that computes two association distributions per time point: (1) prior associations via a learnable Gaussian kernel over relative temporal distances to capture adjacent-concentration inductive bias, using raw (non-normalized) inputs; (2) series associations via self-attention computed on instance-normalized embeddings (RevIN applied only on this branch). Association discrepancy is measured by symmetric KL divergence and combined with reconstruction loss; a minimax strategy alternately minimizes and maximizes association discrepancy to amplify separability of anomalies; final anomaly score combines negative (softmaxed) association-discrepancy and reconstruction error.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_methods</strong></td>
                            <td>Compared against 16 baselines including Anomaly Transformer (replicated), InterFusion, Beat-GAN, OmniAnomaly, LSTM-VAE, DAGMM, MPPCACD, LOF, ITAD, THOC, Deep-SVDD, CL-MPPCCA, LSTM, VAR, OC-SVM, IsolationForest</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>precision, recall, F1 score, ROC/AUC</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Reported performance is comparable to or slightly better than the anomaly transformer overall; RINAT achieved higher precision consistently across datasets but showed reduced recall on some datasets (MSL and SMD). The authors report an F1 value cited in the discussion (98.28) while emphasizing dataset-dependent variability. AUC (ROC) for RINAT was reported higher than the anomaly transformer on SMAP and PSM and comparable on MSL and SMD.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baseline</strong></td>
                            <td>RINAT performs similarly to the anomaly transformer overall, outperforms most other baselines, has a small edge over the anomaly transformer on the PSM dataset, and higher precision but sometimes lower recall compared to the anomaly transformer (worse false-negative rate on MSL and SMD).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Performance is dataset-dependent; RINAT showed decreased recall on MSL and SMD (potentially more false negatives). The paper notes normalization can nullify anomalies if applied naively, motivating the partial (series-branch-only) RevIN; still the approach requires careful tuning of normalization and loss weighting (λ). No runtime/scale (parameter-count) limitations are reported. Code and data availability require contacting authors.</td>
                        </tr>
                        <tr>
                            <td><strong>unique_insights</strong></td>
                            <td>Partial reversible instance normalization (apply RevIN only to the series-association computation) can emphasize association discrepancies between learned self-attention (series associations) and temporal-distance priors, thereby highlighting anomalies; combining a learnable Gaussian prior for adjacent-concentration bias with a minimax association-discrepancy objective is effective for distinguishing rare anomalous points in multivariate time series.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Anomaly Detection in Time Series Data Using Reversible Instance Normalized Anomaly Transformer', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9206.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9206.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using language models for detecting anomalies in lists, sequences, or tabular data, including details of the models, data types, methods, results, comparisons, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Anomaly Transformer</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Anomaly Transformer: Time Series Anomaly Detection with Association Discrepancy</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A transformer-based unsupervised method that defines series associations via self-attention and prior associations via learnable Gaussian kernels over temporal distance, and detects anomalies by measuring association discrepancy (KL divergence) between them, combined with reconstruction loss.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Anomaly transformer: Time series anomaly detection with association discrepancy.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Anomaly Transformer</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>transformer (self-attention)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>multivariate time series (sequence data)</td>
                        </tr>
                        <tr>
                            <td><strong>data_domain</strong></td>
                            <td>server and satellite telemetry datasets used as benchmarks (SMD, PSM, MSL, SMAP)</td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_type</strong></td>
                            <td>rare/outlier anomalous time points and anomalous segments in time series</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Compute series associations from self-attention and prior associations from learnable Gaussian kernels over relative temporal distance; measure association discrepancy (KL divergence) between prior and series associations and combine with reconstruction loss; use association discrepancy as a pointwise anomaly score.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_methods</strong></td>
                            <td>Used as a primary baseline and compared to many classical and deep baselines (see RINAT entry).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>precision, recall, F1 score, ROC/AUC</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Reported as state-of-the-art baseline in the paper; RINAT's results are comparable with slight dataset-dependent differences (anomaly transformer sometimes attains higher F1/recall on MSL and SMD). The paper replicates anomaly transformer results for comparison but does not reprint full numeric tables in the text excerpt.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baseline</strong></td>
                            <td>Within this paper, the anomaly transformer is the strongest baseline; RINAT is comparable and sometimes slightly worse (MSL, SMD) or slightly better (PSM) depending on metric and dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>The anomaly transformer can be affected by instance-level distribution shifts and by naive normalization (which may reduce anomaly signal); these concerns motivate RINAT's reversible-normalization modification. No explicit model-size or compute limitations are reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>unique_insights</strong></td>
                            <td>The anomaly transformer formalizes the idea of association discrepancy (between learned self-attention and temporal-distance priors) as a signal for anomalies and benefits from a minimax training strategy to make this discrepancy more discriminative.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Anomaly Detection in Time Series Data Using Reversible Instance Normalized Anomaly Transformer', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Anomaly transformer: Time series anomaly detection with association discrepancy. <em>(Rating: 2)</em></li>
                <li>Reversible instance normalization for accurate time-series forecasting against distribution shift. <em>(Rating: 2)</em></li>
                <li>Attention is all you need <em>(Rating: 1)</em></li>
                <li>Reversible instance normalized anomaly transformer <em>(Rating: 1)</em></li>
                <li>Language models are few-shot learners <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9206",
    "paper_id": "paper-265317916",
    "extraction_schema_id": "extraction-schema-162",
    "extracted_data": [
        {
            "name_short": "RINAT",
            "name_full": "Reversible Instance Normalized Anomaly Transformer",
            "brief_description": "A transformer-based unsupervised anomaly detection model for multivariate time series that combines learnable Gaussian prior associations, self-attention-based series associations, reversible instance normalization applied only to the series-association branch, and a minimax-driven association-discrepancy objective plus reconstruction loss to score anomalies.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Reversible Instance Normalized Anomaly Transformer (RINAT)",
            "model_type": "transformer (anomaly-transformer variant)",
            "model_size": null,
            "data_type": "multivariate time series (sequence data)",
            "data_domain": "server telemetry (SMD, PSM) and satellite telemetry/spacecraft monitoring (MSL, SMAP)",
            "anomaly_type": "rare/outlier anomalous time points and anomalous segments in time series",
            "method_description": "An encoder-only transformer that computes two association distributions per time point: (1) prior associations via a learnable Gaussian kernel over relative temporal distances to capture adjacent-concentration inductive bias, using raw (non-normalized) inputs; (2) series associations via self-attention computed on instance-normalized embeddings (RevIN applied only on this branch). Association discrepancy is measured by symmetric KL divergence and combined with reconstruction loss; a minimax strategy alternately minimizes and maximizes association discrepancy to amplify separability of anomalies; final anomaly score combines negative (softmaxed) association-discrepancy and reconstruction error.",
            "baseline_methods": "Compared against 16 baselines including Anomaly Transformer (replicated), InterFusion, Beat-GAN, OmniAnomaly, LSTM-VAE, DAGMM, MPPCACD, LOF, ITAD, THOC, Deep-SVDD, CL-MPPCCA, LSTM, VAR, OC-SVM, IsolationForest",
            "performance_metrics": "precision, recall, F1 score, ROC/AUC",
            "performance_results": "Reported performance is comparable to or slightly better than the anomaly transformer overall; RINAT achieved higher precision consistently across datasets but showed reduced recall on some datasets (MSL and SMD). The authors report an F1 value cited in the discussion (98.28) while emphasizing dataset-dependent variability. AUC (ROC) for RINAT was reported higher than the anomaly transformer on SMAP and PSM and comparable on MSL and SMD.",
            "comparison_to_baseline": "RINAT performs similarly to the anomaly transformer overall, outperforms most other baselines, has a small edge over the anomaly transformer on the PSM dataset, and higher precision but sometimes lower recall compared to the anomaly transformer (worse false-negative rate on MSL and SMD).",
            "limitations_or_failure_cases": "Performance is dataset-dependent; RINAT showed decreased recall on MSL and SMD (potentially more false negatives). The paper notes normalization can nullify anomalies if applied naively, motivating the partial (series-branch-only) RevIN; still the approach requires careful tuning of normalization and loss weighting (λ). No runtime/scale (parameter-count) limitations are reported. Code and data availability require contacting authors.",
            "unique_insights": "Partial reversible instance normalization (apply RevIN only to the series-association computation) can emphasize association discrepancies between learned self-attention (series associations) and temporal-distance priors, thereby highlighting anomalies; combining a learnable Gaussian prior for adjacent-concentration bias with a minimax association-discrepancy objective is effective for distinguishing rare anomalous points in multivariate time series.",
            "uuid": "e9206.0",
            "source_info": {
                "paper_title": "Anomaly Detection in Time Series Data Using Reversible Instance Normalized Anomaly Transformer",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "Anomaly Transformer",
            "name_full": "Anomaly Transformer: Time Series Anomaly Detection with Association Discrepancy",
            "brief_description": "A transformer-based unsupervised method that defines series associations via self-attention and prior associations via learnable Gaussian kernels over temporal distance, and detects anomalies by measuring association discrepancy (KL divergence) between them, combined with reconstruction loss.",
            "citation_title": "Anomaly transformer: Time series anomaly detection with association discrepancy.",
            "mention_or_use": "use",
            "model_name": "Anomaly Transformer",
            "model_type": "transformer (self-attention)",
            "model_size": null,
            "data_type": "multivariate time series (sequence data)",
            "data_domain": "server and satellite telemetry datasets used as benchmarks (SMD, PSM, MSL, SMAP)",
            "anomaly_type": "rare/outlier anomalous time points and anomalous segments in time series",
            "method_description": "Compute series associations from self-attention and prior associations from learnable Gaussian kernels over relative temporal distance; measure association discrepancy (KL divergence) between prior and series associations and combine with reconstruction loss; use association discrepancy as a pointwise anomaly score.",
            "baseline_methods": "Used as a primary baseline and compared to many classical and deep baselines (see RINAT entry).",
            "performance_metrics": "precision, recall, F1 score, ROC/AUC",
            "performance_results": "Reported as state-of-the-art baseline in the paper; RINAT's results are comparable with slight dataset-dependent differences (anomaly transformer sometimes attains higher F1/recall on MSL and SMD). The paper replicates anomaly transformer results for comparison but does not reprint full numeric tables in the text excerpt.",
            "comparison_to_baseline": "Within this paper, the anomaly transformer is the strongest baseline; RINAT is comparable and sometimes slightly worse (MSL, SMD) or slightly better (PSM) depending on metric and dataset.",
            "limitations_or_failure_cases": "The anomaly transformer can be affected by instance-level distribution shifts and by naive normalization (which may reduce anomaly signal); these concerns motivate RINAT's reversible-normalization modification. No explicit model-size or compute limitations are reported in this paper.",
            "unique_insights": "The anomaly transformer formalizes the idea of association discrepancy (between learned self-attention and temporal-distance priors) as a signal for anomalies and benefits from a minimax training strategy to make this discrepancy more discriminative.",
            "uuid": "e9206.1",
            "source_info": {
                "paper_title": "Anomaly Detection in Time Series Data Using Reversible Instance Normalized Anomaly Transformer",
                "publication_date_yy_mm": "2023-11"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Anomaly transformer: Time series anomaly detection with association discrepancy.",
            "rating": 2,
            "sanitized_title": "anomaly_transformer_time_series_anomaly_detection_with_association_discrepancy"
        },
        {
            "paper_title": "Reversible instance normalization for accurate time-series forecasting against distribution shift.",
            "rating": 2,
            "sanitized_title": "reversible_instance_normalization_for_accurate_timeseries_forecasting_against_distribution_shift"
        },
        {
            "paper_title": "Attention is all you need",
            "rating": 1,
            "sanitized_title": "attention_is_all_you_need"
        },
        {
            "paper_title": "Reversible instance normalized anomaly transformer",
            "rating": 1,
            "sanitized_title": "reversible_instance_normalized_anomaly_transformer"
        },
        {
            "paper_title": "Language models are few-shot learners",
            "rating": 1,
            "sanitized_title": "language_models_are_fewshot_learners"
        }
    ],
    "cost": 0.00844025,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Anomaly Detection in Time Series Data Using Reversible Instance Normalized Anomaly Transformer
19 November 2023</p>
<p>Chang Choi 
Kiho Lim 
Gyuho Choi 
Ranjai Baidya ranjai123baidya@gmail.com 
Kpro System
673-1 Dogok-ri, Wabu-eupNamyangju-si 12270, GyeonggiRepublic of Korea</p>
<p>Heon Jeong hjeong@cdu.ac.kr 
Department of Fire Service Administration
Chodang University
80, Jeollanam-do58530MuaneupMuanro, MuangunRepublic of Korea</p>
<p>Anomaly Detection in Time Series Data Using Reversible Instance Normalized Anomaly Transformer
19 November 20235EA58F61A28C9BF655F3CB23AAECCD3B10.3390/s23229272Received: 21 September 2023 Revised: 4 November 2023 Accepted: 13 November 2023Baidya, R.Jeong, H. Anomaly Detection in Time Series Data Using Reversible Instance Normalized Anomaly Transformer time series dataanomaly detectionattention mechanismtransformernormalization
Anomalies are infrequent in nature, but detecting these anomalies could be crucial for the proper functioning of any system.The rarity of anomalies could be a challenge for their detection as detection models are required to depend on the relations of the datapoints with their adjacent datapoints.In this work, we use the rarity of anomalies to detect them.For this, we introduce the reversible instance normalized anomaly transformer (RINAT).Rooted in the foundational principles of the anomaly transformer, RINAT incorporates both prior and series associations for each time point.The prior association uses a learnable Gaussian kernel to ensure a thorough understanding of the adjacent concentration inductive bias.In contrast, the series association method uses self-attention techniques to specifically focus on the original raw data.Furthermore, because anomalies are rare in nature, we utilize normalized data to identify series associations and employ non-normalized data to uncover prior associations.This approach enhances the modelled series associations and, consequently, improves the association discrepancies.</p>
<p>Introduction</p>
<p>Anomaly detection in time series data is pivotal in modern data analysis [1] and involves identifying rare patterns or discrepancies that deviate from expected behaviors.This form of detection has a broad range of applications in industries, such as manufacturing, healthcare, and finance [2][3][4].As technological advancements continue, we are producing and collecting more data than ever before.This influx is not just about the sheer volume of data but also its complexity.Complex relationships and patterns embedded within data grow more nuanced as data expand.Given that many modern systems and processes are data-driven, even minor irregularities can lead to significant consequences [5].</p>
<p>Not all anomalies are of concern; some might be benign outliers without substantial impacts.However, others could indicate severe issues, such as critical system failures.In industries, like system operations, finance, and healthcare, distinguishing between these types of anomalies can be of paramount importance [6][7][8].</p>
<p>We analyze time series data either as univariate or multivariate [9].These data can be further decomposed into four distinct types [10].The secular trend represents the consistent, long-term direction of a dataset.Seasonal variations are predictable patterns that recur at regular intervals, like sales spikes during holidays.Cyclical fluctuations refer to longerterm changes without a fixed pattern, often influenced by broader conditions, like economic recessions.Irregular variations represent unpredictable changes due to unforeseen events or outliers, with irregular anomalies being these sudden, unexpected variations.</p>
<p>In this paper, we suggest the reversible instance normalized anomaly transformer for unsupervised anomaly detection in real-life time series data.First, we considered the transformer [11] architecture, following the success of the anomaly transformer [12] Sensors 2023, 23, 9272 2 of 15 in anomaly detection.Transformers have also achieved positive results in the areas of natural language processing [13], machine vision [14,15], and time series [16].These successes can be attributed to the ability of self-attention in transformers to obtain longrange individual relationships.Furthermore, following the observation in [17,18], it is evident that time series data undergo distribution shifts.Addressing distribution shifts while performing time series forecasting has led to significantly improved results [17,18].However, attempting to normalize time series data during anomaly detection could further degrade the performance of the model owing to the sparsity of anomalies in actual data.Also, normalizing time series data, like in [17], could nullify the anomalies as anomalous datapoints will be closer in value to the normal datapoints.We normalize the used time series dataset for anomaly detection such that anomalies are highlighted as compared to normal datapoints.In anomaly transformers [12], the idea of using prior associations and series-associations seems to be highly effective.Here, series associations are calculated using the self-attention of transformers, and prior associations are calculated using learnable Gaussian kernels to calculate priors with respect to the relative temporal distance.To that end, reversibly normalized data should be used for determining series associations, and regular datapoints should be passed to determine prior associations.In this way, when a minimax-strategy-based association discrepancy is used for anomaly detection, the anomalies are highlighted more as compared to normal datapoints.The contributions of the paper can be summarized as follows:</p>
<p>• the suggestion of the reversible instance normalized anomaly transformer to highlight anomalies better than normal datapoints; • the achievement of comparable or better results in four actual datasets.</p>
<p>Related Works</p>
<p>Anomalies in time series data can occur in various ways and can broadly be categorized into temporal, intermetric, or a combination of temporal-intermetric anomalies.Temporal anomalies [19] can be global, where singular or multiple points in a series have values significantly different from the rest.Contextual anomalies are variations relative to neighboring data points.Anomalies in one context might be normal in another.Seasonal anomalies deviate from the regular seasonality of a series.Trend anomalies cause a persistent shift in the data's mean, leading to a change in the time series trend without affecting its cyclical and seasonal patterns.Shapelets pertain to subsequences in data for which cycles or shapes deviate from the usual pattern, influenced by external factors.In analyzing time series data, several algorithms have been proposed for anomaly detection.Based on intrinsic model characteristics, these anomaly detection algorithms can be systematically classified into five distinct categories.</p>
<p>Stochastic Models</p>
<p>Although modern machine-learning-based methods are increasingly popular for this task, there are several traditional techniques and categories that have been used over the years.These models operate on the assumption that data follow a specific statistical pattern or distribution.Anomalies are identified when observed data points deviate significantly from this expected pattern.Examples include autoregressive integrated moving average (ARIMA) [20], the exponential smoothing state space model (ETS) [21], and the seasonal decomposition of time series (STL) [22].</p>
<p>Distance-Based Models</p>
<p>The core idea of these models is that anomalies are data points that are far away from other points.Examples include the k-nearest neighbor (k-NN) algorithm [23], where a point is considered an anomaly if its distance from its k th nearest neighbor exceeds some threshold.Density-based methods, like (DBSCAN) [24], can also be considered in this category, where sparse regions with a low density of data points can be indicative of anomalies.</p>
<p>Information-Theoretic Models</p>
<p>These models are based on concepts from information theory, such as entropy.The idea is to measure the randomness or unpredictability in the data [25].High or low entropy regions, depending on the context, can be indicative of anomalies.A sudden spike in entropy in time series data might indicate an anomaly.</p>
<p>Machine Learning and Deep Learning Models</p>
<p>These models are trained on historical time series data to learn data patterns.Anomalies are detected when new data points significantly differ from the model's prediction.We can further divide machine learning and deep learning models into two categories, namely, forecasting-based models and reconstruction-based models.</p>
<p>Forecasting-Based Models</p>
<p>Forecasting-based models learn the usual patterns from past data, predict future patterns, and then label anomalies if real future data are too different from their predictions.Recurrent neural networks (RNNs) are the commonly used approach as they are designed to handle sequences of data, making them naturally suited for time series.RNNs are trained on a sequence of data points to learn the pattern.When predicting future data points, if the actual data deviate significantly from their predictions, the data are labeled as anomalous.Long short-term memory (LSTM) is an advanced type of RNN that is designed to remember patterns over long sequences and avoid long-term dependency issues found in traditional RNNs.LSTMs are particularly good at capturing long-term patterns in time series data.If the LSTM's prediction for a future data point does not match the actual observed data, it is an indication of an anomaly.Owing to their long memory, they can be particularly useful for spotting anomalies that are based on long-term patterns [26,27].Convolutional neural networks (CNNs) are primarily designed for image processing to identify spatial hierarchies in data.However, they can be adapted for time series data by treating segments of time series as local patterns.A CNN can slide over a time series and learn local patterns [28].After training, if a new pattern appears that does not match any learned pattern, the CNN can label this as an anomaly.It is effective for capturing local anomalies in a dataset.Transformer-based models [29] use attention mechanisms to weigh the importance of different data points in a sequence.Introduced for natural language-processing tasks, their adaptability has extended their usage for time series forecasting.Transformers can give attention to significant patterns in a time series dataset.When trained, if the model encounters a data point or sequence that significantly deviates from the patterns it gave attention to, the model can label that as an anomaly.The capacity to handle long sequences with varied attention spans makes transformers robust for complex anomaly detection scenarios.Graph neural networks (GNNs) are designed for graph-structured data.Graphs consist of nodes and edges, and GNNs process these data by propagating and aggregating information from neighboring nodes to enhance the feature representation of each node or edge.Time series data are transformed into a graph format, especially when there is a relationship or correlation between different time series.For instance, in multivariate time series, where different series influence each other, or in scenarios where temporal patterns form a network of relationships [30], GNNs learn the underlying structure and relationships in the data.When a deviation from the learned graph structure or relationship pattern occurs, it is an indication of an anomaly.</p>
<p>Reconstruction-Based Models</p>
<p>This type of model aims to learn a compressed representation of the data and then reconstruct it.Anomalies are often identified based on how well the model can reconstruct a particular data point or sequence.Autoencoder-based models [31] aim to copy their inputs to their outputs and consist of an encoder, which compresses the input into a latent-space representation, and a decoder, which reconstructs the input data from this representation.For anomaly detection, this model trains the autoencoder on normal data so that it learns to reconstruct the input data well.When an anomalous data point is passed through, the reconstruction error (difference between the original data point and its reconstruction) tends to be high, signaling an anomaly.Variational autoencoder (VAE)-based models [32] are a type of autoencoder with added constraints on the encoded representations and are designed to generate new data points and, hence, are often used in generative tasks.For anomaly detection, like standard autoencoders, VAEs are trained on normal data to learn the data structure.Anomalies are data points that are difficult for the VAE to reconstruct, leading to high reconstruction errors.Additionally, the latent space of a VAE (where data are compressed) follows a specific distribution, and deviations from this can also signal anomalies.Generative adversarial network (GAN)-based models [33] consist of two networks: a generator that produces data and a discriminator that evaluates them.The generator tries to produce data that the discriminator cannot distinguish from real data.GANs can be trained on normal data, where the generator learns to produce normal data samples.When a real data point is fed to the discriminator and is deemed as "fake" (or different from the learned distribution), it can be an indication of an anomaly [34].</p>
<p>Proposed Method</p>
<p>We propose an anomaly detection method that combines a transformer architecture with an autoencoder structure.Transformer-based models are originally designed for natural language-processing tasks [11].These models use an attention mechanism to weigh the importance of different data points in a sequence, enabling the models to capture longrange dependencies in data.Transformers can be trained in a reconstruction manner similar to autoencoders and can learn to predict or reconstruct a segment of a time series based on its context.A high reconstruction error indicates an anomaly.Given the transformer's ability to handle long sequences and varied attention spans, it can capture both local and global anomalies in data.In the majority of existing time series anomaly detection methods, there is a prevalent emphasis on understanding predominant temporal patterns.However, these traditional approaches prioritize either pointwise representations focusing on individual data points or pairwise associations examining relationships between pairs.Thus, these models often hesitate in comprehensively capturing the adjacent concentration inductive bias of each time point in time series data.This inductive bias suggests that for each time point in a time series, its immediate neighbors are more relevant or influential for its representation than distant points.Furthermore, these models can be susceptible to distribution shifts in the data, meaning that the models might struggle when the underlying statistical properties of the time series change over time.</p>
<p>To address the challenges faced by traditional time series anomaly detection methods, a two-fold solution is proposed.First, the learnable Gaussian kernel is introduced to effectively handle the adjacent concentration inductive bias, ensuring that each data point in the series adequately emphasizes its immediate neighbors.Second, the integration of reversible instance normalization (RevIN) is suggested, incorporating both normalization and denormalization with a learnable affine transformation.This approach provides a robust mechanism to counteract distribution shifts, ensuring consistent model performance even as the underlying statistical properties of the data evolve.</p>
<p>Anomaly Transformer</p>
<p>The anomaly transformer is an adaptation of the transformer architecture designed for unsupervised time series anomaly detection.In anomaly transformers, the temporal association between data from each time point is obtained using a self-attention map and is termed as 'series association'.The series association is more significant for nonanomalous time points and less so for anomalous time points.As anomalous time points are less frequent, their associations with the adjacent time points are much higher, where these disruptions are more likely to appear.This is termed as 'prior association'.Based on the series association and prior association, a new criterion called the 'association discrepancy' is introduced for anomaly classification.The self-attention is modified to separately obtain the prior association and series association for each time point.Although series associations are obtained using the conventional self-attention, prior associations are obtained using learnable Gaussian kernels.A minimax approach is implemented to enhance the differentiation between normal and abnormal patterns in the association discrepancy.</p>
<p>Reversible Instance Normalization</p>
<p>Time-series forecasting models frequently encounter challenges related to distribution shifts, where statistical properties in training and test data evolve over time, leading to performance issues.Although removing non-stationary information from input sequences can mitigate these discrepancies, it may compromise the model's ability to capture the original data distribution.To address this issue, reversible instance normalization (RevIN) was introduced, a method that normalizes input sequences and then denormalizes the model's output sequences using normalization statistics [17].This approach maintains the performance while effectively handling distribution shifts in time-series forecasting.Suppose we have a set of input and output time series data, X = x (i) N i=1 and
Y = y (i) N i=1
, respectively, where N is the number of sequences, K is the number of variables, T x is the length of the input, and T y is the length of the output.Then, given the mean and standard deviation of each instance, x (i) k ∈ R T x , the data are normalized as follows:
x(i) kt = γ k     x (i) kt − µ t x (i) kt Var x (i) kt + ε     + β k(1)
where µ t x (i) kt and Var x
(i)
kt are the mean and standard deviation (Var), respectively, and γ, β ∈ R K are learnable affine parameters.The mean and standard deviation are given as follows:
µ t x (i) kt = 1 T x ∑ T x j=1 x (i) kj and Var x (i) kt = 1 T x ∑ T x j=1 (x (i) kj − µ t x (i) kt 2 (2)
Similarly, the forecasting-model output is denormalized as follows:
ŷ(i) kt = Var x (i) kt + ε   ∼ y (i) kt − β k γ k   + µ t x (i) kt(3)
In this work, we intentionally used the concept of normalization to further emphasize the differences between the anomalous and non-anomalous datapoints by normalizing the data.Because anomalies are rare, it is difficult for them to build series associations, and their associations with their neighboring datapoints are stronger.When input data are normalized, anomalies in the data are less significant.Considering this, we propose to find series associations using normalized data and prior associations using the original (non-normalized) data.We hypothesize that this way, stronger prior associations can be observed, which will help us to obtain better association discrepancies.In our architecture, we do not use learnable parameters, β, as it has previously been determined that the difference between using them and not using them is negligible [18].</p>
<p>Reversible Instance Normalized Anomaly Transformer (RINAT)</p>
<p>By focusing on the constraints of transformers and the achievement of the anomaly transformer in unsupervised anomaly detection, we enhanced the anomaly transformer to the reversible instance normalized anomaly transformer.We adopted the anomaly transformer [12] as it addresses the challenge of the adjacent inductive by introducing the prior association and series association of each time point.We also leveraged the concept of the reversible normalization and rethought the anomaly transformer for the same application.This architecture estimates the anomaly score based on the association discrepancy and reconstruction error.The association discrepancy considers the prior association and series association of each time point.The prior association employs the learnable Gaussian kernel to present the adjacent concentration inductive bias of each time point.The series association corresponds to the self-attention weights learned from raw series.We renovated the anomaly transformer by adding the reversible instance learnable normalization to input time series data because anomalies are rare and normalization might reduce the impact of anomalies.Thus, normalization was only applied to the series association part, as shown in Figure 1.This partial application of the reversible instance normalization brings to light the variations between the series associations and the prior associations while determining the association discrepancies.As in the anomaly transformer, we utilized an encoder-only design, with stacks of specially designed attention blocks and feedforward layers.These stacks are repeated multiple times.However, the attention block is different from the anomaly attention block.</p>
<p>is negligible [18].</p>
<p>Reversible Instance Normalized Anomaly Transformer (RINAT)</p>
<p>By focusing on the constraints of transformers and the achievement of the anomaly transformer in unsupervised anomaly detection, we enhanced the anomaly transformer to the reversible instance normalized anomaly transformer.We adopted the anomaly transformer [12] as it addresses the challenge of the adjacent inductive by introducing the prior association and series association of each time point.We also leveraged the concept of the reversible normalization and rethought the anomaly transformer for the same application.This architecture estimates the anomaly score based on the association discrepancy and reconstruction error.The association discrepancy considers the prior association and series association of each time point.The prior association employs the learnable Gaussian kernel to present the adjacent concentration inductive bias of each time point.The series association corresponds to the self-attention weights learned from raw series.We renovated the anomaly transformer by adding the reversible instance learnable normalization to input time series data because anomalies are rare and normalization might reduce the impact of anomalies.Thus, normalization was only applied to the series association part, as shown in Figure 1.This partial application of the reversible instance normalization brings to light the variations between the series associations and the prior associations while determining the association discrepancies.As in the anomaly transformer, we utilized an encoder-only design, with stacks of specially designed attention blocks and feedforward layers.These stacks are repeated multiple times.However, the attention block is different from the anomaly attention block.Given the time series data, X ∈ R T , with T time steps, and each time-step value, x i , in the sequence, we perform embedding on the given time series data.For the input layer, we take layer l = 0.
X l=0 Out = emb(X)(4)
For X l=0 Out ∈ R T×D , D represents the embedding dimension, effectively capturing both the time series length and the embedded feature dimensions.The proposed transformer architecture for anomaly detection integrates the power of the traditional transformer with additional steps.These steps include the reversible normalization, semi-stationary anomaly attention, as well as strategic placements of the layer normalization and denormalization.A salient feature of this architecture is the semi-stationary anomaly attention, which intakes two distinct inputs.The first one is the normalized data from the reversible normalization stage and second one is the raw embedded data directly from the embedding phase.The reversible normalization stage normalizes the given data by subtracting their mean, µ, and dividing by their standard deviation, σ.
X l=0 norm = X l=0 Out − µ σ (5)
Sensors 2023, 23, 9272 7 of 15</p>
<p>Given two distinct inputs to the semi-stationary anomaly attention, this stage estimates the anomaly discrepancy using the two-branch structure.One branch estimates the prior association to address the challenge of the adjacent inductive.The relationship between two temporal points, i and j, with respect to the relative temporal distance within the series is quantified using the Gaussian kernel, represented by the following equation:
P l = rescale   1 √ 2πσ i exp − |j − i| 2 2σ 2 i i,j∈{1,2...N}  (6)
Benefiting from the unimodal property of the Gaussian kernel, essentially, this design can pay more attention to the adjacent time points.The learnable scale parameter, σ, for the Gaussian kernel makes prior associations adapt to various time series patterns, such as different lengths of anomaly segments.</p>
<p>Next, a branch of the normalized anomaly attention estimates the series association.The series association corresponds to the self-attention weights learned from raw series.Given the embedded data, X l=0 norm , self-attention weights are computed using a scaled dotproduct between query Q l , and key K l , followed by a SoftMax operation.We compute Q l , K l , and V l as follows:
Q l = X l=0 norm W l Q K l = X l=0 norm W l K V l = X l=0 norm W l V (7)
where
W l Q ∈ R d v ×d q , W l K ∈ R d v ×d k , and W l V ∈ R d v ×d v
are weights for layer l.Then, the series association coefficient, S l , is derived as follows:
S l = So f tMax Q l K lT √ d k(8)X l Out = S l × V l(9)
The series association coefficient and prior association coefficient both represent the probability distribution.The disparity between the prior and series associations is measured using the Kullback-Leibler (KL) divergence.
AssDisp(P, S; X) = 1 L L ∑ l=1 KL P l i ||S l i + KL(S l i ||P l i )) i=1,2,3...N(10)
After the attention mechanism, the output is normalized using the layer normalization.This step improves the model convergence and ensures stable activations.The normalized output is then directed to a feedforward neural network, which further extracts high-level features and representations from the data.Once processed, the output undergoes another layer normalization step to maintain a stabilized activation range.To preserve the original time series scale and pattern, a denormalization step is employed, reversing the effects of the initial normalization and ensuring the final output remains intricately tied to the original series dynamics.
X Rev = X rec norm − β γ (11)
In the proposed architecture, the process for learning or training to achieve the desired performance is guided using two loss functions simultaneously.This dual-loss approach helps the network to learn and adapt based on two different objectives or criteria.The primary component is the reconstruction loss, measuring the disparity between the original series and the decoded output, essentially guiding the series association to recognize the most pivotal associations.Complementing this is the association discrepancy loss, which Sensors 2023, 23, 9272 8 of 15 highlights the differences between typical patterns and unusual patterns in time series data.The loss function for input series is as follows:</p>
<p>Loss Final (X, P, S, λ; X Rev ) = X − X Rev −λ AssDisp(P, S; X) (12) The value of λ determines the influence of the association discrepancy within the broader context of the loss function.Additionally, we implemented the minimax strategy to make the association discrepancy more distinguishable.This approach is employed between the series association and prior association in two phases.In the minimize phase, the model adjusts the prior association, P l , to reflect the series association, S l .The prior association serves as an initial model or understanding, which is then refined or updated based on the actual patterns observed in the series association.This enables the prior association to become more adaptable to a variety of temporal patterns found in the data.Conversely, in the maximize phase, the objective is to increase the association discrepancy, pushing the series association to focus more on non-adjacent data points.The model pays extra attention to data points that are separated by significant time intervals.A score, AS(X), is assigned for each data point in the series to quantify the deviation of the data point from the norm.
AS(X) = So f tMax(−AssDisp(P, S; X norm )) X − X Rev(13)
This gives the pointwise anomaly criteria based on the association discrepancy.</p>
<p>Experiments</p>
<p>We extensively evaluated the proposed RINAT with different publicly available datasets in three practical applications.</p>
<p>Datasets</p>
<p>We used the following four datasets in our experiments: (1) the server machine dataset (SMD) [35], which is a dataset collected from a large internet company and consists of five-week-long data with 38 dimensions; (2) pooled server metrics (PSMs) [36], which are a collection of internally collected data from multiple application server nodes at eBay and have 26 dimensions; and the (3) Mars Science Laboratory (MSL) rover [37] and (4) Soil Moisture Active Passive (SMAP) [37] satellite datasets, which are public datasets made available by NASA, contain telemetry anomaly data derived from Incident Surprise Anomaly (ISA) reports of spacecraft monitoring systems, and have 55 and 25 dimensions, respectively.</p>
<p>Implementation Details</p>
<p>The overall experiments were performed in a system with a single Nvidia Geforce RTX 3090, and the implemented code was written in the Pytorch framework of version 1.13.The overall setup was implemented in a fashion similar to that in the work of the anomaly transformer [12].A non-overlapping sliding window was used to obtain a set of sub-series, just like in [38].For all the datasets, the sliding window was set to a fixed size of 100.Time points were labeled as anomalies if their anomaly scores were higher than a certain threshold, δ.The threshold, δ, was determined such that a proportion, r, of the data in the validation dataset would be labeled as anomalies.For the SMD dataset, we set r = 0.5% and 1% for the rest.For anomaly detection, if a single time point in a certain segment of an anomalous time series was detected, it was considered that the whole anomalous segment was detected.This adjustment strategy has previously been widely adopted [35,38,39] and, similar to the adjustment strategy for the anomaly transformer [12], contains three layers.We set the number of channels in the hidden-state model at 512 and the number of heads, h, at 8. The hyperparameter, λ, (Equation ( 4)) was set at 3 for all the datasets to tradeoff two parts of the loss function.We used the ADAM optimizer [40] at an initial learning rate of 10 −4 .The training process was stopped early, within 10 epochs, with a batch size of 32.</p>
<p>Sensors 2023, 23, 9272 9 of 15</p>
<p>Baselines</p>
<p>We compared our model with 16 other baseline models, namely, InterFusion [41], Beat-GAN [42], OmniAnomaly [35], LSTM-VAE [32], DAGMM [43], MPPCACD [44], LOF [45], ITAD [46], THOC [38], Deep-SVDD [47], CL-MPPCCA [48], LSTM [37], VAR [49], OC-SVM [50], IsolationForest [51], and the anomaly transformer [12].</p>
<p>Results</p>
<p>Table 1 shows the quantitative comparison of the precision, recall, and F1 scores for the 16 other baseline models and the suggested model.We can see that although the performance of the suggested model is comparable to that of the anomaly transformer in the SMD, MSL, and SMAP datasets, it is better than that of the state-of-the-art anomaly transformer in the PSM dataset.Figures 2-5 show the comparisons of the precision, recall, and F1 scores, respectively.The proposed model outperforms almost all the existing algorithms except for the anomaly transformer.</p>
<p>Table 1.Quantitative results for the suggested model and 16 other models in four actual datasets.The metrics used for comparison are precision (P), recall (R), and F1 scores.Higher values represent better performance in each of these metrics.The results of anomaly transformer was replicated using their provided code while for the rest of models the results were copied from the anomaly transformer paper [12].With the MSL data, the proposed model shows a slightly lower performance compared to that of the anomaly transformer, especially in terms of the precision and F1 scores.Although the performance of the proposed model is impressive and slightly better than that of the anomaly transformer, with an F1 score of 98.28, we can see that the performance of the proposed model is very comparable to that of the anomaly transformer.The F1 scores are almost the same, indicating a similar overall performance in this dataset.The proposed model seems to show a drop in performance, especially in terms of the recall and F1 scores, compared to those of the anomaly transformer.The anomaly transformer tends to perform better than the proposed model in the MSL and SMD datasets in terms of the F1 score, while the proposed model has a slight edge over the anomaly transformer in the PSM dataset.Both models perform similarly in the SMAP dataset.The proposed model consistently shows higher precision than the anomaly transformer in all the datasets, but it tends to have a lower recall score than the anomaly transformer in the MSL and SMD datasets.Figure 6 shows the ROC curves for the suggested model architecture alongside the ROC curves of the anomaly transformer and BeatGAN architectures.The AUC values of the suggested model architecture in the SMAP and PSM datasets seem to be better than those of even the anomaly transformer architecture.Additionally, for the MSL and SMD datasets, even though the proposed model architecture does not outshine that of the anomaly transformer, the AUC values are comparable.With the MSL data, the proposed model shows a slightly lower performance compared to that of the anomaly transformer, especially in terms of the precision and F1 scores.Although the performance of the proposed model is impressive and slightly better than that of the anomaly transformer, with an F1 score of 98.28, we can see that the performance of the proposed model is very comparable to that of the anomaly transformer.The F1 scores are almost the same, indicating a similar overall performance in this dataset.The proposed model seems to show a drop in performance, especially in terms of the recall and F1 scores, compared to those of the anomaly transformer.The anomaly transformer tends to perform better than the proposed model in the MSL and SMD datasets in terms of the F1 score, while the proposed model has a slight edge over the anomaly transformer in the PSM dataset.Both models perform similarly in the SMAP dataset.The proposed model consistently shows higher precision than the anomaly transformer in all the datasets, but it tends to have a lower recall score than the anomaly transformer in the MSL and SMD datasets.Figure 6 shows the ROC curves for the suggested model architecture alongside the ROC curves of the anomaly transformer and BeatGAN architectures.The AUC values of the suggested model architecture in the SMAP and PSM datasets seem to be better than those of even the anomaly transformer architecture.Additionally, for the MSL and SMD datasets, even though the proposed model architecture does not outshine that of the anomaly transformer, the AUC values are comparable.</p>
<p>Dataset</p>
<p>Conclusions</p>
<p>In conclusion, our paper introduced the reversible instance normalized anomaly transformer, building upon the fundamental principles of the anomaly transformer.Through a comprehensive evaluation of well-established benchmarks, including those of the anomaly transformer and 16 other baseline models across multiple datasets, we have gained valuable insights.Although our model demonstrates commendable performance, it is crucial to recognize that the model's strengths and limitations are contextdependent, varying across datasets.</p>
<p>Figure 1 .
1
Figure 1.Reversible instance normalized anomaly transformer (RINAT).</p>
<p>Figure 2 .
2
Figure 2. Comparison of the proposed model with four different models in group1 classifiers using four different datasets: (a) SMD; (b) MSL; (c) SMAP; (d) PSM.</p>
<p>Figure 2 .
2
Figure 2. Comparison of the proposed model with four different models in group1 classifiers using four different datasets: (a) SMD; (b) MSL; (c) SMAP; (d) PSM.</p>
<p>Figure 2 .Figure 3 .
23
Figure 2. Comparison of the proposed model with four different models in group1 classifiers using four different datasets: (a) SMD; (b) MSL; (c) SMAP; (d) PSM.</p>
<p>Figure 4 .
4
Figure 4. Comparison of the proposed model with four different models in group3 classifiers using four different datasets: (a) SMD; (b) MSL; (c) SMAP; (d) PSM.</p>
<p>Figure 3 .Figure 3 .
33
Figure 3.Comparison of the proposed model with four different models in group2 classifiers using four different datasets: (a) SMD; (b) MSL; (c) SMAP; (d) PSM.</p>
<p>Figure 4 .
4
Figure 4. Comparison of the proposed model with four different models in group3 classifiers using four different datasets: (a) SMD; (b) MSL; (c) SMAP; (d) PSM.</p>
<p>Figure 4 .
4
Figure 4. Comparison of the proposed model with four different models in group3 classifiers using four different datasets: (a) SMD; (b) MSL; (c) SMAP; (d) PSM.</p>
<p>Figure 5 .
5
Figure 5.Comparison of proposed model with four different models in group4 classifiers using four different datasets: (a) SMD; (b) MSL; (c) SMAP; (d) PSM.</p>
<p>Figure 5 .Figure 6 .
56
Figure 5.Comparison of proposed model with four different models in group4 classifiers using four different datasets: (a) SMD; (b) MSL; (c) SMAP; (d) PSM.Sensors 2023, 23, x FOR PEER REVIEW 15 of 18</p>
<p>Figure 6 .
6
Figure 6.ROC curves (horizontal axis: false-positive rate; vertical axis: true-positive rate) for four different datasets: (a) SMD; (b) MSL; (c) SMAP; (d) PSM.A higher AUC value (area under the ROC curve) indicates a better performance.The predefined threshold proportion, r, is in {0.5%, 1.0%, 1.5%, 2.0%, 10%, 20%, and 30%}.</p>
<p>ConclusionsIn conclusion, our paper introduced the reversible instance normalized anomaly transformer, building upon the fundamental principles of the anomaly transformer.Through a comprehensive evaluation of well-established benchmarks, including those of the anomaly transformer and 16 other baseline models across multiple datasets, we have gained valuable insights.Although our model demonstrates commendable performance, it is crucial to recognize that the model's strengths and limitations are context-dependent, varying across datasets.This variability in performance underscores the importance for considering the specific characteristics and complexities of each dataset.Notably, the proposed model exhibits a decline in the recall score in comparison to that of the anomaly transformer in the MSL and SMD datasets, which suggests a potentially higher rate of false negatives for these datasets.However, when considering the F1 score, which provides a balanced view by combining precision and recall scores, our proposed model holds a slight advantage over the anomaly transformer in the PSM dataset.On the other hand, the anomaly transformer outperforms the proposed model in the MSL and SMD datasets.Interestingly, the performance remains comparable for both models in the SMAP dataset.Although our research contributes a valuable approach to anomaly detection, its effectiveness is subject to the unique characteristics of each dataset.These findings underscore the need for further research to adapt and fine-tune anomaly detection models for specific domains, thereby enhancing their applicability in a diverse range of actual scenarios.Future work should focus on addressing the limitations of our model, particularly in datasets where recall is a critical metric, and optimizing it for broader applicability.Additionally, a deeper theoretical exploration of the model's validity and further refinement may open doors for improving its performance in challenging datasets, like MSL and SMD.Acknowledgments: We would like to thank Ramesh Lama for his immense help with the manuscript and suggestions during the development of the idea for this work.Data Availability Statement:The code and dataset shall be made available upon request via email to the corresponding author.Funding: This work was supported by the "Regional Innovation Strategy" (RIS) through the National Research Foundation (NRF) of Korea and was funded by the Ministry of Education (MOE) (No. 2021RIS-002) and an Institute of Information and Communications Technology Planning and Evaluation (IITP) grant funded by the Government of Korea (MIST) (No. 2022-0-00530).Conflicts of Interest:Author Ranjai Baidya was employed by the company Kpro System.The remaining authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.
Time Series Analysis. J D Hamilton, 2020Princeton University PressPrinceton, NJ, USA</p>
<p>Anomaly detection in COVID-19 time-series data. H Homayouni, I Ray, S Ghosh, S Gondalia, M G Kahn, 10.1007/s42979-021-00658-wSN Comput. Sci. 2021279</p>
<p>Anomaly Detection in Financial Time Series by Principal Component Analysis and Neural Networks. S Crépey, N Lehdili, N Madhar, M Thomas, 10.3390/a15100385Algorithms. 153852022</p>
<p>Detecting anomalies in time series data from a manufacturing system using recurrent neural networks. Y Wang, M Perry, D Whitlock, J W Sutherland, 10.1016/j.jmsy.2020.12.007J. Manuf. Syst. 622022</p>
<p>A new perspective towards the development of robust data-driven intrusion detection for industrial control systems. A Ayodeji, Y K Liu, N Chao, L Q Yang, 10.1016/j.net.2020.05.012Nucl. Eng. Technol. 522020</p>
<p>Real-time big data processing for anomaly detection: A survey. R A A Habeeb, F Nasaruddin, A Gani, I A T Hashem, E Ahmed, M Imran, 10.1016/j.ijinfomgt.2018.08.006Int. J. Inf. Manag. 452019</p>
<p>Artificial intelligence based anomaly detection of energy consumption in buildings: A review, current trends and new perspectives. Y Himeur, K Ghanem, A Alsalemi, F Bensaali, A Amira, 10.1016/j.apenergy.2021.116601Appl. Energy. 2872021. 116601</p>
<p>Detecting regions of maximal divergence for spatio-temporal anomaly detection. B Barz, E Rodner, Y G Garcia, J Denzler, 10.1109/TPAMI.2018.2823766IEEE Trans. Pattern Anal. Mach. Intell. 412018</p>
<p>Time series analysis on univariate and multivariate variables: A comprehensive survey. S R Beeram, S Kuchibhotla, Commun. Softw. Netw. Proc. 20202019</p>
<p>The Concise Encyclopedia of Statistics. Y Dodge, 2008Springer Science &amp; Business Media: Berlin/HeidelbergGermany</p>
<p>Polosukhin, I. Attention is all you need. A Vaswani, N Shazeer, N Parmar, J Uszkoreit, L Jones, A N Gomez, Adv. Neural Inf. Process. Syst. 302017</p>
<p>Anomaly transformer: Time series anomaly detection with association discrepancy. J Xu, H Wu, J Wang, M Long, arXiv:2110.026422021</p>
<p>Language models are few-shot learners. T Brown, B Mann, N Ryder, M Subbiah, J D Kaplan, P Dhariwal, D Amodei, Adv. Neural Inf. Process. Syst. 332020</p>
<p>An image is worth 16x16 words: Transformers for image recognition at scale. A Dosovitskiy, L Beyer, A Kolesnikov, D Weissenborn, X Zhai, T Unterthiner, N Houlsby, arXiv:2010.119292020</p>
<p>Swin transformer: Hierarchical vision transformer using shifted windows. Z Liu, Y Lin, Y Cao, H Hu, Y Wei, Z Zhang, B Guo, Proceedings of the IEEE/CVF International Conference on Computer Vision, Virtual. the IEEE/CVF International Conference on Computer Vision, VirtualOctober 2021</p>
<p>Informer: Beyond efficient transformer for long sequence time-series forecasting. H Zhou, S Zhang, J Peng, S Zhang, J Li, H Xiong, W Zhang, Proceedings of the AAAI Conference on Artificial Intelligence, Virtual. the AAAI Conference on Artificial Intelligence, Virtual2-9 February 202135</p>
<p>Reversible instance normalization for accurate time-series forecasting against distribution shift. T Kim, J Kim, Y Tae, C Park, J H Choi, J Choo, Proceedings of the International Conference on Learning Representations. the International Conference on Learning RepresentationsVienna, Austria4 May 2021</p>
<p>Non-stationary transformers: Exploring the stationarity in time series forecasting. Y Liu, H Wu, J Wang, M Long, Adv. Neural Inf. Process. Syst. 352022</p>
<p>TFAD: A decomposition time series anomaly detection architecture with time-frequency analysis. C Zhang, T Zhou, Q Wen, L Sun, Proceedings of the 31st ACM International Conference on Information &amp; Knowledge Management. the 31st ACM International Conference on Information &amp; Knowledge ManagementAtlanta GA, USAOctober 2022</p>
<p>A Review of ARIMA vs. V I Kontopoulou, A D Panagopoulos, I Kakkos, G K Matsopoulos, 10.3390/fi15080255Machine Learning Approaches for Time Series Forecasting in Data Driven Networks. 202315</p>
<p>A state space framework for automatic forecasting using exponential smoothing methods. R J Hyndman, A B Koehler, R D Snyder, S Grose, 10.1016/S0169-2070(01)00110-8Int. J. Forecast. 182002</p>
<p>STL: A seasonal-trend decomposition. R B Cleveland, W S Cleveland, J E Mcrae, I Terpenning, J. Off. Stat. 61990</p>
<p>Statistical analysis of nearest neighbor methods for anomaly detection. X Gu, L Akoglu, A Rinaldo, Adv. Neural Inf. Process. Syst. 322019</p>
<p>An Empirical Study on Anomaly Detection Using Density-Based and Representative-Based Clustering Algorithms. G S Fuhnwi, J O Agbaje, K Oshinubi, O J Peter, 10.46481/jnsps.2023.1364J. Niger. Soc. Phys. Sci. 2023, 5, 1364</p>
<p>Unsupervised outlier detection for time series by entropy and dynamic time warping. S E Benkabou, K Benabdeslem, B Canitia, 10.1007/s10115-017-1067-8Knowl. Inf. Syst. 542018</p>
<p>Multivariate industrial time series with cyber-attack simulation: Fault detection using an lstm-based predictive data model. P Filonov, A Lavrentyev, A Vorontsov, arXiv:1612.066762016</p>
<p>Anomaly detection in ECG time signals via deep long short-term memory networks. S Chauhan, L Vig, Proceedings of the 2015 IEEE International Conference on Data Science and Advanced Analytics (DSAA). the 2015 IEEE International Conference on Data Science and Advanced Analytics (DSAA)Paris, FranceIEEE19-21 October 2015</p>
<p>Time-series anomaly detection service at microsoft. H Ren, B Xu, Y Wang, C Yi, C Huang, X Kou, Q Zhang, Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining. the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data MiningAnchorage, AK, USA4-8 July 2019</p>
<p>Attend and diagnose: Clinical time series analysis using attention models. H Song, D Rajan, J Thiagarajan, A Spanias, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial IntelligenceNew Orleans, LA, USA2-7 February 201832</p>
<p>Graph neural network-based anomaly detection in multivariate time series. A Deng, B Hooi, Proceedings of the AAAI Conference on Artificial Intelligence, Virtual. the AAAI Conference on Artificial Intelligence, Virtual2-9 February 202135</p>
<p>LSTM-based encoder-decoder for multi-sensor anomaly detection. P Malhotra, A Ramakrishnan, G Anand, L Vig, P Agarwal, G Shroff, arXiv:1607.001482016</p>
<p>A multimodal anomaly detector for robot-assisted feeding using an lstm-based variational autoencoder. D Park, Y Hoshi, C C Kemp, 10.1109/LRA.2018.2801475IEEE Robot. Autom. Lett. 32018</p>
<p>LSTM-based VAE-GAN for time-series anomaly detection. Z Niu, K Yu, X Wu, 10.3390/s20133738Sensors. 202020</p>
<p>Deep transformer networks for anomaly detection in multivariate time series data. S Tuli, G Casale, N R Jennings, Tranad, 10.14778/3514061.3514067arXiv:2201.072842022</p>
<p>Robust anomaly detection for multivariate time series through stochastic recurrent neural network. Y Su, Y Zhao, C Niu, R Liu, W Sun, D Pei, Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining. the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data MiningAnchorage, AK, USA4-8 August 2019</p>
<p>Practical approach to asynchronous multivariate time series anomaly detection and localization. A Abdulaal, Z Liu, T Lancewicki, Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining, Virtual. the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining, VirtualAugust 2021</p>
<p>Detecting spacecraft anomalies using lstms and nonparametric dynamic thresholding. K Hundman, V Constantinou, C Laporte, I Colwell, T Soderstrom, Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining. the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data MiningLondon, UKAugust 2018</p>
<p>Timeseries anomaly detection using temporal hierarchical one-class network. L Shen, Z Li, J Kwok, Adv. Neural Inf. Process. Syst. 332020</p>
<p>Unsupervised anomaly detection via variational auto-encoder for seasonal kpis in web applications. H Xu, W Chen, N Zhao, Z Li, J Bu, Z Li, H Qiao, Proceedings of the 2018 World Wide Web Conference. the 2018 World Wide Web ConferenceLyon, FranceApril 2018</p>
<p>D P Kingma, J Ba, Adam, arXiv:1412.6980A method for stochastic optimization. 2014</p>
<p>Multivariate time series anomaly detection and interpretation using hierarchical inter-metric and temporal embedding. Z Li, Y Zhao, J Han, Y Su, R Jiao, X Wen, D Pei, Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining, Virtual. the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining, VirtualAugust 2021</p>
<p>Anomalous rhythm detection using adversarially generated time series. B Zhou, S Liu, B Hooi, X Cheng, J Ye, Beatgan, Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence (IJCAI-19). the Twenty-Eighth International Joint Conference on Artificial Intelligence (IJCAI-19)Macao, China10-16 August 20192019</p>
<p>Deep autoencoding gaussian mixture model for unsupervised anomaly detection. B Zong, Q Song, M R Min, W Cheng, C Lumezanu, D Cho, H Chen, Proceedings of the International Conference on Learning Representations. the International Conference on Learning RepresentationsVancouver, BC, Canada30 April-3 May 2018</p>
<p>A data-driven health monitoring method for satellite housekeeping data based on probabilistic clustering and dimensionality reduction. T Yairi, N Takeishi, T Oda, Y Nakajima, N Nishimura, N Takata, 10.1109/TAES.2017.2671247IEEE Trans. Aerosp. Electron. Syst. 532017</p>
<p>LOF: Identifying density-based local outliers. M M Breunig, H P Kriegel, R T Ng, J Sander, Proceedings of the 2000 ACM SIGMOD International Conference on Management of Data. the 2000 ACM SIGMOD International Conference on Management of DataDallas, TX, USAMay 2000</p>
<p>Itad: Integrative tensor-based anomaly detection system for reducing false positives of satellite systems. Y Shin, S Lee, S Tariq, M S Lee, O Jung, D Chung, S S Woo, Proceedings of the 29th ACM International Conference on Information &amp; Knowledge Management, Virtual. the 29th ACM International Conference on Information &amp; Knowledge Management, Virtual19-23 October 2020</p>
<p>Deep one-class classification. L Ruff, R Vandermeulen, N Goernitz, L Deecke, S A Siddiqui, A Binder, M Kloft, Proceedings of the International Conference on Machine Learning. the International Conference on Machine LearningStockholm, Sweden10-15 July 2018</p>
<p>Detecting anomalies in space using multivariate convolutional LSTM with mixtures of probabilistic PCA. S Tariq, S Lee, Y Shin, M S Lee, O Jung, D Chung, S S Woo, Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining. the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data MiningAnchorage, AK, USA4-8 August 2019</p>
<p>. O Anderson, M Kendall, Time-Series. J. R. Stat. Soc. Ser. D. 1976</p>
<p>Support vector data description. D M Tax, R P Duin, 10.1023/B:MACH.0000008084.60811.49Mach. Learn. 542004</p>
<p>Isolation forest. F T Liu, K M Ting, Z H Zhou, Proceedings of the 2008 Eighth IEEE International Conference on Data Mining. the 2008 Eighth IEEE International Conference on Data MiningPisa, Italy15-19 December 2008</p>
<p>Disclaimer/Publisher's Note: The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods. instructions or products referred to in the content</p>            </div>
        </div>

    </div>
</body>
</html>
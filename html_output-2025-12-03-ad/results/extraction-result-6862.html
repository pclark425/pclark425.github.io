<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6862 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6862</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6862</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-132.html">extraction-schema-132</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to synthesize novel chemical compounds for specific applications, including model details, generation methods, target applications, chemical representations, evaluation metrics, constraints, integration with external tools, datasets, reported performance, experimental validation, and reported challenges or limitations.</div>
                <p><strong>Paper ID:</strong> paper-276632282</p>
                <p><strong>Paper Title:</strong> Large language models for scientific discovery in molecular property prediction</p>
                <p><strong>Paper Abstract:</strong> Large language models (LLMs) are a form of artificial intelligence system encapsulating vast knowledge in the form of natural language. These systems are adept at numerous complex tasks including creative writing, storytelling, translation, question-answering, summarization and computer code generation. Although LLMs have seen initial applications in natural sciences, their potential for driving scientific discovery remains largely unexplored. In this work, we introduce LLM4SD, a framework designed to harness LLMs for driving scientific discovery in molecular property prediction by synthesizing knowledge from literature and inferring knowledge from scientific data. LLMs synthesize knowledge by extracting established information from scientific literature, such as molecular weight being key to predicting solubility. For inference, LLMs identify patterns in molecular data, particularly in Simplified Molecular Input Line Entry System-encoded structures, such as halogen-containing molecules being more likely to cross the blood–brain barrier. This information is presented as interpretable knowledge, enabling the transformation of molecules into feature vectors. By using these features with interpretable models such as random forest, LLM4SD can outperform the current state of the art across a range of benchmark tasks for predicting molecular properties. We foresee it providing interpretable and potentially new insights, aiding scientific discovery in molecular property prediction. Zheng et al. developed LLM4SD, a framework using large language models to predict molecular properties. The method leverages the ability of large language models to synthesize knowledge from literature and to reason about scientific data with domain expertise.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6862.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6862.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to synthesize novel chemical compounds for specific applications, including model details, generation methods, target applications, chemical representations, evaluation metrics, constraints, integration with external tools, datasets, reported performance, experimental validation, and reported challenges or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM4SD</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLMs for Scientific Discovery (LLM4SD) pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A pipeline that leverages large language models to (1) synthesize rules from scientific literature and (2) infer rules from labelled molecular data, converts those rules into executable RDKit-based feature functions, vectorizes molecules (SMILES → feature vectors) and trains interpretable models (e.g., random forest) for molecular property prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLM4SD (framework using LLM backbones: GPT-4, Galactica-6.7b/30b, Falcon-7b/40b, ChemLLM-7b, ChemDFM-13b)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>pipeline / prompt-driven rule synthesis + code-generation (tool-using pipeline)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>varies by backbone (examples: Galactica-6.7b, Galactica-30b, Falcon-7b, Falcon-40b, GPT-4)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Does not train a single model end-to-end; leverages pretrained LLM backbones (general and domain-specific) that were themselves pretrained on large text corpora including scientific literature (Galactica: scientific literature tokens; ChemLLM/ChemDFM: chemical/scientific corpora; GPT-4: large multi-domain corpora).</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Prompt-based rule synthesis from literature + prompt-driven inference from sampled labelled SMILES batches; deduplication/summarization via LLM; conversion of human-readable rules into Python/RDKit functions (code generation by GPT-4) that compute numeric/categorical features for each molecule (feature-engineering rather than direct molecule generation).</td>
                        </tr>
                        <tr>
                            <td><strong>chemical_representation</strong></td>
                            <td>SMILES strings (input format for rule inference and feature extraction); rules map SMILES → numeric/categorical features via RDKit functions.</td>
                        </tr>
                        <tr>
                            <td><strong>target_application</strong></td>
                            <td>Molecular property prediction across domains: physiology (BBBP, ClinTox, Tox21, SIDER), biophysics (HIV, BACE), physical chemistry (ESOL, FreeSolv, Lipophilicity) and quantum mechanics (QM9 tasks: μ, α, R2, ZPVE, Cv, Δϵ, εHOMO, εLUMO, U0, U, H, G).</td>
                        </tr>
                        <tr>
                            <td><strong>constraints_used</strong></td>
                            <td>Requirement that generated rules have an associated numeric or categorical measure to allow deterministic conversion to code; during inference LLMs were provided sampled labelled instances and required to produce measurable rules; experimental setup used 30 and 50 sampling iterations for rule inference; scaffold splits for train/val/test for most tasks; no explicit synthetic-accessibility/toxicity filters applied to generation because the pipeline generates features/rules not molecules.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_with_external_tools</strong></td>
                            <td>RDKit (rdMolDescriptors and other modules) for calculating features from SMILES; GPT-4 used to generate Python code implementing rules; scikit-learn random-forest for downstream interpretable modelling; standard libraries used (Hugging Face, numpy, pytorch, scipy, bitsandbytes, accelerate) for tooling and experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_used</strong></td>
                            <td>MoleculeNet benchmark datasets (presplit scaffold datasets provided by authors): BBBP, ClinTox, Tox21 (12 subtasks), SIDER (27 subtasks), HIV, BACE, ESOL, FreeSolv, Lipophilicity, QM9 (12 subtasks).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Classification: AUC-ROC; Regression (physical chemistry): RMSE; Regression (quantum mechanics): MAE. Additional evaluation: statistical tests for rule significance (two-sided Mann–Whitney U-test for classification, two-sided linear regression t-test for regression; p < 0.05 threshold), literature review to label rules as previously reported or novel.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Across 58 tasks LLM4SD outperformed nine specialized baselines on average. Examples reported in the paper: physiology (AUC-ROC) improved to 76.60% from prior best 74.53% (gain 2.07%); quantum mechanics average MAE reported as 5.8233 vs baseline 11.2450 (≈48.2% improvement claimed); physical chemistry RMSE reported as 1.28 vs baseline RMSE 1.47 (≈12.9% improvement). Component-combination results (averaged across backbones/domains) reported: physiology combined (synthesis+inference) AUC-ROC 73.62 vs synthesis-only 70.46 and inference-only 69.25; biophysics combined AUC-ROC 79.10 vs 76.30 and 75.76; physical chemistry combined RMSE 1.54 vs synthesis 1.99 and inference 1.77; quantum mechanics combined MAE 10.42 vs synthesis 37.67 and inference 68.17. (All numbers reported in the manuscript; many are dataset/domain-averages.)</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Pipeline produces interpretable rules rather than directly proposing synthesizable novel molecules; no wet-lab synthesis or physical validation of generated molecules reported. Limitations noted: smaller LLM backbones can produce gibberish or fail at inference (model scale and pretraining data impact capability); reliance on pretrained LLM knowledge which can bias results toward literature-known rules; distinguishing memorized knowledge from genuine inference (some inferred rules lacked prior literature support and need further validation); handling long biological sequences (proteins/genes) is highlighted as a future challenge due to context-window limits and tokenization; the method requires that rules be expressible as numeric/categorical functions, limiting discovery of non-measurable heuristics. Authors also note a need for human review of generated rules (though experimental results were obtained without human intervention).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large language models for scientific discovery in molecular property prediction', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6862.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6862.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to synthesize novel chemical compounds for specific applications, including model details, generation methods, target applications, chemical representations, evaluation metrics, constraints, integration with external tools, datasets, reported performance, experimental validation, and reported challenges or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generative Pre-trained Transformer 4</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large, general-purpose transformer-based language model used in this work primarily to generate executable Python (RDKit) functions implementing human- or LLM-synthesized chemical rules and as one of the LLM backbones for rule synthesis/inference.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>decoder-only large language model (prompted; used for code generation and rule synthesis/inference)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>very large (paper states ~1.76 trillion training tokens; parameter count not specified in the manuscript)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Pretrained on a very large multi-domain corpus including web data and scientific text (paper cites large-scale pretraining but does not list proprietary chemical corpora specific to GPT-4).</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Prompting / role-playing (instructed to act as chemist) to (a) synthesize rules from literature and (b) infer rules from sampled SMILES-labelled data; specifically used to generate Python code (RDKit) implementing the rules.</td>
                        </tr>
                        <tr>
                            <td><strong>chemical_representation</strong></td>
                            <td>SMILES (LLM is given SMILES strings with labels to infer rules; generated code operates on SMILES via RDKit).</td>
                        </tr>
                        <tr>
                            <td><strong>target_application</strong></td>
                            <td>Feature/rule synthesis for molecular property prediction tasks (e.g., BBBP, toxicity, solubility, QM9 properties).</td>
                        </tr>
                        <tr>
                            <td><strong>constraints_used</strong></td>
                            <td>Prompts require rules to be convertible to numeric/categorical measures; sampling-based inference (30/50 iterations) to generate stable rule sets; no molecule-generation constraints because model was not used to propose novel molecules.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_with_external_tools</strong></td>
                            <td>GPT-4 generated Python code that uses RDKit functions (rdMolDescriptors, etc.) to implement rules; produced features used as inputs to interpretable models (random forest).</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_used</strong></td>
                            <td>MoleculeNet-derived datasets for rule inference and validation (BBBP, ClinTox, Tox21, SIDER, HIV, BACE, ESOL, FreeSolv, Lipophilicity, QM9).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Performance of downstream classifiers/regressors measured by AUC-ROC, RMSE, MAE; statistical significance of generated rules assessed with Mann–Whitney U-test or linear regression t-test (p < 0.05).</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>GPT-4 backbone consistently performs well across all tasks in the study; exact per-backbone numbers not individually enumerated beyond aggregate/backbone comparisons, but GPT-4 is reported as the top-performing backbone in many tasks (no wet-lab validation).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>GPT-4 is used for code generation and rule extraction but may reflect literature bias; generated rules require statistical testing and expert literature review to confirm novelty; cost and access to GPT-4 not discussed but implied (large-scale model).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large language models for scientific discovery in molecular property prediction', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6862.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6862.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to synthesize novel chemical compounds for specific applications, including model details, generation methods, target applications, chemical representations, evaluation metrics, constraints, integration with external tools, datasets, reported performance, experimental validation, and reported challenges or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Galactica-6.7b</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Galactica (6.7 billion parameters)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A science-specialized LLM used as a backbone in LLM4SD; authors found Galactica-6.7b to offer strong scientific-rule synthesis and inference performance despite smaller parameter count compared to some larger models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Galactica-6.7b</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>decoder-only LLM, domain-specific (scientific literature pretraining)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>6.7B parameters</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Pretrained extensively on scientific literature (paper states Galactica was built from scratch using ~106 billion tokens of scientific literature).</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Prompt-based knowledge synthesis from literature and inference from data (sampling batches of labelled SMILES) to produce measurable rules; summarization/deduplication of rules across batches.</td>
                        </tr>
                        <tr>
                            <td><strong>chemical_representation</strong></td>
                            <td>SMILES provided as input when inferring from datasets; generated rules translate SMILES → features via RDKit code.</td>
                        </tr>
                        <tr>
                            <td><strong>target_application</strong></td>
                            <td>Molecular property prediction (same MoleculeNet tasks as LLM4SD pipeline).</td>
                        </tr>
                        <tr>
                            <td><strong>constraints_used</strong></td>
                            <td>Rules required to be numeric/categorical; sampling-based inference; used as one of multiple backbone models (30/50 sampling iterations).</td>
                        </tr>
                        <tr>
                            <td><strong>integration_with_external_tools</strong></td>
                            <td>Output rules converted to RDKit-based functions; downstream random forest training; statistical tests and literature review for rule validation.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_used</strong></td>
                            <td>MoleculeNet datasets (BBBP, ClinTox, Tox21, SIDER, HIV, BACE, ESOL, FreeSolv, Lipophilicity, QM9).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>AUC-ROC (physiology, biophysics), RMSE (physical chemistry), MAE (quantum mechanics); rule significance via Mann–Whitney U-test and regression t-test.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Galactica-6.7b was chosen for detailed rule validation due to superior performance and reproducibility; authors report that Galactica-6.7b often matched or exceeded larger Galactica-30b on several domains (exception: quantum mechanics where 30b performed better by ~14%). Specific numeric backbone-level metrics are plotted in the paper but domain-averaged performance contributions are reported (see LLM4SD reported results).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Although domain-specific pretraining yielded strong results, larger model size did not always improve performance within the Galactica series; some inferred rules lacked literature support and require further validation; potential for producing second-order features that need domain-expert interpretation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large language models for scientific discovery in molecular property prediction', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6862.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6862.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to synthesize novel chemical compounds for specific applications, including model details, generation methods, target applications, chemical representations, evaluation metrics, constraints, integration with external tools, datasets, reported performance, experimental validation, and reported challenges or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Galactica-30b</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Galactica (30 billion parameters)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A larger Galactica variant pretrained on scientific literature; used as a backbone for rule synthesis/inference in LLM4SD with improved performance in some domains (notably quantum mechanics).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Galactica-30b</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>decoder-only LLM, domain-specific (scientific literature pretraining)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>30B parameters</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Pretrained on scientific literature (paper indicates Galactica models were pretraining on ~106B tokens overall, and larger models have broader stored knowledge).</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Prompt-based literature synthesis and data inference to produce numeric/categorical rules; summarization across batches.</td>
                        </tr>
                        <tr>
                            <td><strong>chemical_representation</strong></td>
                            <td>SMILES</td>
                        </tr>
                        <tr>
                            <td><strong>target_application</strong></td>
                            <td>Molecular property prediction (MoleculeNet tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>constraints_used</strong></td>
                            <td>Numeric/categorical rule requirement; sampling iterations; same pipeline constraints as other backbones.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_with_external_tools</strong></td>
                            <td>RDKit, random forest, statistical testing and literature review.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_used</strong></td>
                            <td>MoleculeNet datasets as above.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>AUC-ROC, RMSE, MAE; statistical tests for rules.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Galactica-30b outperformed Galactica-6.7b in quantum mechanics by ~14% according to the paper; otherwise performance comparable in many domains.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Larger model did not universally outperform smaller variant, indicating pretraining corpus and model calibration can be as important as parameter count.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large language models for scientific discovery in molecular property prediction', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6862.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e6862.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to synthesize novel chemical compounds for specific applications, including model details, generation methods, target applications, chemical representations, evaluation metrics, constraints, integration with external tools, datasets, reported performance, experimental validation, and reported challenges or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Falcon-7b</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Falcon (7 billion parameters)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A general-purpose open LLM used as a backbone in LLM4SD; smaller Falcon model struggled on scientific inference tasks in this study (produced gibberish for some inference prompts).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Falcon-7b</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>decoder-only general-purpose LLM (prompted)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B parameters</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>General pretraining corpus (broad web-based text, not specialized to scientific literature per authors' description).</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Prompt-based literature synthesis and data inference; however in practice the small Falcon-7b often failed at inference tasks in this work.</td>
                        </tr>
                        <tr>
                            <td><strong>chemical_representation</strong></td>
                            <td>SMILES</td>
                        </tr>
                        <tr>
                            <td><strong>target_application</strong></td>
                            <td>Molecular property prediction via rule synthesis/inference (when functioning), but underperformed on physiology and quantum mechanics in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>constraints_used</strong></td>
                            <td>Same numeric/categorical rule requirement; scaffold split and sampling iterations applied in experimental protocol.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_with_external_tools</strong></td>
                            <td>RDKit, random forest used downstream; model used as LLM backbone in pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_used</strong></td>
                            <td>MoleculeNet datasets (same as other backbones).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>AUC-ROC, RMSE, MAE as appropriate per task.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Falcon-7b underperformed relative to Falcon-40b and other domain-specific models; failed to produce valid inference output on some domains (physiology, quantum mechanics).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>General models of small scale may lack domain understanding and can produce non-sensical outputs for scientific inference prompts; scale and domain pretraining matter.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large language models for scientific discovery in molecular property prediction', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6862.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e6862.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to synthesize novel chemical compounds for specific applications, including model details, generation methods, target applications, chemical representations, evaluation metrics, constraints, integration with external tools, datasets, reported performance, experimental validation, and reported challenges or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Falcon-40b</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Falcon (40 billion parameters)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A larger Falcon variant used as an LLM backbone; performed better than Falcon-7b and could bridge some domain gaps due to increased scale, but still generally trailed domain-specialized Galactica in this study unless scale afforded emergent capabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Falcon-40b</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>decoder-only general-purpose LLM (prompt-based)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>40B parameters</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Pretrained on large general-purpose corpora (not specifically on scientific literature according to the manuscript).</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Prompted literature synthesis and data inference to output measurable rules; summarization/deduplication across batches.</td>
                        </tr>
                        <tr>
                            <td><strong>chemical_representation</strong></td>
                            <td>SMILES</td>
                        </tr>
                        <tr>
                            <td><strong>target_application</strong></td>
                            <td>Rule/feature extraction for molecular property prediction tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>constraints_used</strong></td>
                            <td>Numeric/categorical rule requirement; sampling strategy; scaffold split on datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_with_external_tools</strong></td>
                            <td>RDKit, random forest, statistics for rule validation.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_used</strong></td>
                            <td>MoleculeNet family datasets used in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>AUC-ROC / RMSE / MAE; statistical tests for rules.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Falcon-40b improved over Falcon-7b and could perform scientific inference across domains due to emergent capabilities enabled by scale; performance still compared against Galactica and GPT-4 in backbone studies.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>General-purpose pretraining requires more scale to match domain-specific models; scale has computational and access costs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large language models for scientific discovery in molecular property prediction', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6862.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e6862.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to synthesize novel chemical compounds for specific applications, including model details, generation methods, target applications, chemical representations, evaluation metrics, constraints, integration with external tools, datasets, reported performance, experimental validation, and reported challenges or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChemLLM-7b</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChemLLM (7 billion parameters)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A chemical-domain-adapted LLM (fine-tuned from a general model) evaluated as a backbone; in this work ChemLLM-7b underperformed compared to Galactica-6.7b, likely due to limited chemical token/data scale during fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChemLLM-7b</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>fine-tuned domain-adapted LLM (decoder-only)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B parameters</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Adapted from a general LLM (InternLM2-7b) and fine-tuned on chemical data; paper states ChemLLM used only ~7 million tokens of chemical data (very small scale compared to other models).</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Prompt-based rule synthesis and inference (same LLM4SD pipeline), but performance was limited in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>chemical_representation</strong></td>
                            <td>SMILES</td>
                        </tr>
                        <tr>
                            <td><strong>target_application</strong></td>
                            <td>Molecular property prediction (via rule generation into RDKit features).</td>
                        </tr>
                        <tr>
                            <td><strong>constraints_used</strong></td>
                            <td>Numeric/categorical rule requirement; sampling-based inference.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_with_external_tools</strong></td>
                            <td>RDKit and downstream random forest; statistical rule validation.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_used</strong></td>
                            <td>MoleculeNet datasets for rule inference/validation.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>AUC-ROC, RMSE, MAE, statistical tests on rules.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>ChemLLM-7b substantially underperformed compared to Galactica-6.7b in the authors' backbone comparison; authors attribute underperformance to scale and training-data-size differences.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Small fine-tuning corpus (7M tokens) limited the model's domain knowledge despite chemical-focused training; adaptation-from-general-model approach may be inferior to large-scale domain pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large language models for scientific discovery in molecular property prediction', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6862.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e6862.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to synthesize novel chemical compounds for specific applications, including model details, generation methods, target applications, chemical representations, evaluation metrics, constraints, integration with external tools, datasets, reported performance, experimental validation, and reported challenges or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChemDFM-13b</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChemDFM (13 billion parameters)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A chemical foundation model (based on LLaMa-13b fine-tuning) evaluated as a backbone; in this study ChemDFM-13b underperformed relative to Galactica-6.7b, possibly due to differences in pretraining approach and data scale.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChemDFM-13b</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>fine-tuned domain-adapted LLM (decoder-only)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>13B parameters</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Adapted from a general LLM (LLaMa-13b) and trained/fine-tuned on chemical/scientific tokens; paper states ChemDFM used ~34 billion tokens (training data scale noted).</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Prompt-based literature synthesis and data inference per LLM4SD protocol; produced measurable rules for conversion to RDKit code.</td>
                        </tr>
                        <tr>
                            <td><strong>chemical_representation</strong></td>
                            <td>SMILES</td>
                        </tr>
                        <tr>
                            <td><strong>target_application</strong></td>
                            <td>Feature/rule extraction for molecular property prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>constraints_used</strong></td>
                            <td>Numeric/categorical rule requirement; sampling iterations for inference.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_with_external_tools</strong></td>
                            <td>RDKit, random forest, statistical testing, literature review.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_used</strong></td>
                            <td>MoleculeNet datasets used in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>AUC-ROC, RMSE, MAE; statistical tests to validate rules.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>ChemDFM-13b underperformed compared to Galactica-6.7b in this study despite larger fine-tuning token counts; authors discuss differences in adaptation strategy and datasets as possible causes.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Fine-tuning from a general LLM may not match models trained from-scratch on scientific corpora; data composition and pretraining approach crucial for downstream scientific inference.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large language models for scientific discovery in molecular property prediction', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6862.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e6862.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to synthesize novel chemical compounds for specific applications, including model details, generation methods, target applications, chemical representations, evaluation metrics, constraints, integration with external tools, datasets, reported performance, experimental validation, and reported challenges or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RDKit</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RDKit cheminformatics toolkit</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Open-source cheminformatics library used to compute molecular descriptors and implement the executable functions that realize LLM-generated rules (e.g., molecular weight, topological polar surface area, H-bond donors/acceptors).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>RDKit (rdMolDescriptors and other modules)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>cheminformatics software library (used via generated Python code)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>n/a (software library)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>n/a (tool for computing molecule descriptors from SMILES/internally coded algorithms)</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>n/a (used as runtime library invoked by generated Python functions that implement LLM-specified rules)</td>
                        </tr>
                        <tr>
                            <td><strong>chemical_representation</strong></td>
                            <td>SMILES (RDKit parses SMILES into molecular graphs and computes descriptors)</td>
                        </tr>
                        <tr>
                            <td><strong>target_application</strong></td>
                            <td>Compute numeric/categorical features from SMILES per LLM-generated rules to be used as inputs to interpretable ML models for property prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>constraints_used</strong></td>
                            <td>Rules must map to RDKit-computable descriptors or boolean tests (authors required numeric/categorical measurability to ensure RDKit implementation).</td>
                        </tr>
                        <tr>
                            <td><strong>integration_with_external_tools</strong></td>
                            <td>Called by Python code generated by GPT-4; outputs used by scikit-learn random-forest models; used alongside statistical testing libraries for rule validation.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_used</strong></td>
                            <td>Applied to MoleculeNet SMILES datasets to produce features for training/prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Not applicable (tool used for feature computation); downstream model metrics: AUC-ROC, RMSE, MAE.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>RDKit used to implement LLM-derived rules; no separate RDKit-specific results reported.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Requires that LLM-specified rules be convertible to RDKit-implementable computations; some abstract or complex hypothesized features may not be directly implementable.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large language models for scientific discovery in molecular property prediction', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Galactica: a large language model for science <em>(Rating: 2)</em></li>
                <li>Chemllm: a chemical large language model <em>(Rating: 2)</em></li>
                <li>ChemDFM: a large language foundation model for chemistry <em>(Rating: 2)</em></li>
                <li>MoleculeNet: a benchmark for molecular machine learning <em>(Rating: 2)</em></li>
                <li>Are large language models superhuman chemists? <em>(Rating: 1)</em></li>
                <li>Gpt-4 technical report <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6862",
    "paper_id": "paper-276632282",
    "extraction_schema_id": "extraction-schema-132",
    "extracted_data": [
        {
            "name_short": "LLM4SD",
            "name_full": "LLMs for Scientific Discovery (LLM4SD) pipeline",
            "brief_description": "A pipeline that leverages large language models to (1) synthesize rules from scientific literature and (2) infer rules from labelled molecular data, converts those rules into executable RDKit-based feature functions, vectorizes molecules (SMILES → feature vectors) and trains interpretable models (e.g., random forest) for molecular property prediction.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LLM4SD (framework using LLM backbones: GPT-4, Galactica-6.7b/30b, Falcon-7b/40b, ChemLLM-7b, ChemDFM-13b)",
            "model_type": "pipeline / prompt-driven rule synthesis + code-generation (tool-using pipeline)",
            "model_size": "varies by backbone (examples: Galactica-6.7b, Galactica-30b, Falcon-7b, Falcon-40b, GPT-4)",
            "training_data_description": "Does not train a single model end-to-end; leverages pretrained LLM backbones (general and domain-specific) that were themselves pretrained on large text corpora including scientific literature (Galactica: scientific literature tokens; ChemLLM/ChemDFM: chemical/scientific corpora; GPT-4: large multi-domain corpora).",
            "generation_method": "Prompt-based rule synthesis from literature + prompt-driven inference from sampled labelled SMILES batches; deduplication/summarization via LLM; conversion of human-readable rules into Python/RDKit functions (code generation by GPT-4) that compute numeric/categorical features for each molecule (feature-engineering rather than direct molecule generation).",
            "chemical_representation": "SMILES strings (input format for rule inference and feature extraction); rules map SMILES → numeric/categorical features via RDKit functions.",
            "target_application": "Molecular property prediction across domains: physiology (BBBP, ClinTox, Tox21, SIDER), biophysics (HIV, BACE), physical chemistry (ESOL, FreeSolv, Lipophilicity) and quantum mechanics (QM9 tasks: μ, α, R2, ZPVE, Cv, Δϵ, εHOMO, εLUMO, U0, U, H, G).",
            "constraints_used": "Requirement that generated rules have an associated numeric or categorical measure to allow deterministic conversion to code; during inference LLMs were provided sampled labelled instances and required to produce measurable rules; experimental setup used 30 and 50 sampling iterations for rule inference; scaffold splits for train/val/test for most tasks; no explicit synthetic-accessibility/toxicity filters applied to generation because the pipeline generates features/rules not molecules.",
            "integration_with_external_tools": "RDKit (rdMolDescriptors and other modules) for calculating features from SMILES; GPT-4 used to generate Python code implementing rules; scikit-learn random-forest for downstream interpretable modelling; standard libraries used (Hugging Face, numpy, pytorch, scipy, bitsandbytes, accelerate) for tooling and experiments.",
            "dataset_used": "MoleculeNet benchmark datasets (presplit scaffold datasets provided by authors): BBBP, ClinTox, Tox21 (12 subtasks), SIDER (27 subtasks), HIV, BACE, ESOL, FreeSolv, Lipophilicity, QM9 (12 subtasks).",
            "evaluation_metrics": "Classification: AUC-ROC; Regression (physical chemistry): RMSE; Regression (quantum mechanics): MAE. Additional evaluation: statistical tests for rule significance (two-sided Mann–Whitney U-test for classification, two-sided linear regression t-test for regression; p &lt; 0.05 threshold), literature review to label rules as previously reported or novel.",
            "reported_results": "Across 58 tasks LLM4SD outperformed nine specialized baselines on average. Examples reported in the paper: physiology (AUC-ROC) improved to 76.60% from prior best 74.53% (gain 2.07%); quantum mechanics average MAE reported as 5.8233 vs baseline 11.2450 (≈48.2% improvement claimed); physical chemistry RMSE reported as 1.28 vs baseline RMSE 1.47 (≈12.9% improvement). Component-combination results (averaged across backbones/domains) reported: physiology combined (synthesis+inference) AUC-ROC 73.62 vs synthesis-only 70.46 and inference-only 69.25; biophysics combined AUC-ROC 79.10 vs 76.30 and 75.76; physical chemistry combined RMSE 1.54 vs synthesis 1.99 and inference 1.77; quantum mechanics combined MAE 10.42 vs synthesis 37.67 and inference 68.17. (All numbers reported in the manuscript; many are dataset/domain-averages.)",
            "experimental_validation": false,
            "challenges_or_limitations": "Pipeline produces interpretable rules rather than directly proposing synthesizable novel molecules; no wet-lab synthesis or physical validation of generated molecules reported. Limitations noted: smaller LLM backbones can produce gibberish or fail at inference (model scale and pretraining data impact capability); reliance on pretrained LLM knowledge which can bias results toward literature-known rules; distinguishing memorized knowledge from genuine inference (some inferred rules lacked prior literature support and need further validation); handling long biological sequences (proteins/genes) is highlighted as a future challenge due to context-window limits and tokenization; the method requires that rules be expressible as numeric/categorical functions, limiting discovery of non-measurable heuristics. Authors also note a need for human review of generated rules (though experimental results were obtained without human intervention).",
            "uuid": "e6862.0",
            "source_info": {
                "paper_title": "Large language models for scientific discovery in molecular property prediction",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "GPT-4",
            "name_full": "Generative Pre-trained Transformer 4",
            "brief_description": "A large, general-purpose transformer-based language model used in this work primarily to generate executable Python (RDKit) functions implementing human- or LLM-synthesized chemical rules and as one of the LLM backbones for rule synthesis/inference.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_type": "decoder-only large language model (prompted; used for code generation and rule synthesis/inference)",
            "model_size": "very large (paper states ~1.76 trillion training tokens; parameter count not specified in the manuscript)",
            "training_data_description": "Pretrained on a very large multi-domain corpus including web data and scientific text (paper cites large-scale pretraining but does not list proprietary chemical corpora specific to GPT-4).",
            "generation_method": "Prompting / role-playing (instructed to act as chemist) to (a) synthesize rules from literature and (b) infer rules from sampled SMILES-labelled data; specifically used to generate Python code (RDKit) implementing the rules.",
            "chemical_representation": "SMILES (LLM is given SMILES strings with labels to infer rules; generated code operates on SMILES via RDKit).",
            "target_application": "Feature/rule synthesis for molecular property prediction tasks (e.g., BBBP, toxicity, solubility, QM9 properties).",
            "constraints_used": "Prompts require rules to be convertible to numeric/categorical measures; sampling-based inference (30/50 iterations) to generate stable rule sets; no molecule-generation constraints because model was not used to propose novel molecules.",
            "integration_with_external_tools": "GPT-4 generated Python code that uses RDKit functions (rdMolDescriptors, etc.) to implement rules; produced features used as inputs to interpretable models (random forest).",
            "dataset_used": "MoleculeNet-derived datasets for rule inference and validation (BBBP, ClinTox, Tox21, SIDER, HIV, BACE, ESOL, FreeSolv, Lipophilicity, QM9).",
            "evaluation_metrics": "Performance of downstream classifiers/regressors measured by AUC-ROC, RMSE, MAE; statistical significance of generated rules assessed with Mann–Whitney U-test or linear regression t-test (p &lt; 0.05).",
            "reported_results": "GPT-4 backbone consistently performs well across all tasks in the study; exact per-backbone numbers not individually enumerated beyond aggregate/backbone comparisons, but GPT-4 is reported as the top-performing backbone in many tasks (no wet-lab validation).",
            "experimental_validation": false,
            "challenges_or_limitations": "GPT-4 is used for code generation and rule extraction but may reflect literature bias; generated rules require statistical testing and expert literature review to confirm novelty; cost and access to GPT-4 not discussed but implied (large-scale model).",
            "uuid": "e6862.1",
            "source_info": {
                "paper_title": "Large language models for scientific discovery in molecular property prediction",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "Galactica-6.7b",
            "name_full": "Galactica (6.7 billion parameters)",
            "brief_description": "A science-specialized LLM used as a backbone in LLM4SD; authors found Galactica-6.7b to offer strong scientific-rule synthesis and inference performance despite smaller parameter count compared to some larger models.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Galactica-6.7b",
            "model_type": "decoder-only LLM, domain-specific (scientific literature pretraining)",
            "model_size": "6.7B parameters",
            "training_data_description": "Pretrained extensively on scientific literature (paper states Galactica was built from scratch using ~106 billion tokens of scientific literature).",
            "generation_method": "Prompt-based knowledge synthesis from literature and inference from data (sampling batches of labelled SMILES) to produce measurable rules; summarization/deduplication of rules across batches.",
            "chemical_representation": "SMILES provided as input when inferring from datasets; generated rules translate SMILES → features via RDKit code.",
            "target_application": "Molecular property prediction (same MoleculeNet tasks as LLM4SD pipeline).",
            "constraints_used": "Rules required to be numeric/categorical; sampling-based inference; used as one of multiple backbone models (30/50 sampling iterations).",
            "integration_with_external_tools": "Output rules converted to RDKit-based functions; downstream random forest training; statistical tests and literature review for rule validation.",
            "dataset_used": "MoleculeNet datasets (BBBP, ClinTox, Tox21, SIDER, HIV, BACE, ESOL, FreeSolv, Lipophilicity, QM9).",
            "evaluation_metrics": "AUC-ROC (physiology, biophysics), RMSE (physical chemistry), MAE (quantum mechanics); rule significance via Mann–Whitney U-test and regression t-test.",
            "reported_results": "Galactica-6.7b was chosen for detailed rule validation due to superior performance and reproducibility; authors report that Galactica-6.7b often matched or exceeded larger Galactica-30b on several domains (exception: quantum mechanics where 30b performed better by ~14%). Specific numeric backbone-level metrics are plotted in the paper but domain-averaged performance contributions are reported (see LLM4SD reported results).",
            "experimental_validation": false,
            "challenges_or_limitations": "Although domain-specific pretraining yielded strong results, larger model size did not always improve performance within the Galactica series; some inferred rules lacked literature support and require further validation; potential for producing second-order features that need domain-expert interpretation.",
            "uuid": "e6862.2",
            "source_info": {
                "paper_title": "Large language models for scientific discovery in molecular property prediction",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "Galactica-30b",
            "name_full": "Galactica (30 billion parameters)",
            "brief_description": "A larger Galactica variant pretrained on scientific literature; used as a backbone for rule synthesis/inference in LLM4SD with improved performance in some domains (notably quantum mechanics).",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Galactica-30b",
            "model_type": "decoder-only LLM, domain-specific (scientific literature pretraining)",
            "model_size": "30B parameters",
            "training_data_description": "Pretrained on scientific literature (paper indicates Galactica models were pretraining on ~106B tokens overall, and larger models have broader stored knowledge).",
            "generation_method": "Prompt-based literature synthesis and data inference to produce numeric/categorical rules; summarization across batches.",
            "chemical_representation": "SMILES",
            "target_application": "Molecular property prediction (MoleculeNet tasks).",
            "constraints_used": "Numeric/categorical rule requirement; sampling iterations; same pipeline constraints as other backbones.",
            "integration_with_external_tools": "RDKit, random forest, statistical testing and literature review.",
            "dataset_used": "MoleculeNet datasets as above.",
            "evaluation_metrics": "AUC-ROC, RMSE, MAE; statistical tests for rules.",
            "reported_results": "Galactica-30b outperformed Galactica-6.7b in quantum mechanics by ~14% according to the paper; otherwise performance comparable in many domains.",
            "experimental_validation": false,
            "challenges_or_limitations": "Larger model did not universally outperform smaller variant, indicating pretraining corpus and model calibration can be as important as parameter count.",
            "uuid": "e6862.3",
            "source_info": {
                "paper_title": "Large language models for scientific discovery in molecular property prediction",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "Falcon-7b",
            "name_full": "Falcon (7 billion parameters)",
            "brief_description": "A general-purpose open LLM used as a backbone in LLM4SD; smaller Falcon model struggled on scientific inference tasks in this study (produced gibberish for some inference prompts).",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Falcon-7b",
            "model_type": "decoder-only general-purpose LLM (prompted)",
            "model_size": "7B parameters",
            "training_data_description": "General pretraining corpus (broad web-based text, not specialized to scientific literature per authors' description).",
            "generation_method": "Prompt-based literature synthesis and data inference; however in practice the small Falcon-7b often failed at inference tasks in this work.",
            "chemical_representation": "SMILES",
            "target_application": "Molecular property prediction via rule synthesis/inference (when functioning), but underperformed on physiology and quantum mechanics in experiments.",
            "constraints_used": "Same numeric/categorical rule requirement; scaffold split and sampling iterations applied in experimental protocol.",
            "integration_with_external_tools": "RDKit, random forest used downstream; model used as LLM backbone in pipeline.",
            "dataset_used": "MoleculeNet datasets (same as other backbones).",
            "evaluation_metrics": "AUC-ROC, RMSE, MAE as appropriate per task.",
            "reported_results": "Falcon-7b underperformed relative to Falcon-40b and other domain-specific models; failed to produce valid inference output on some domains (physiology, quantum mechanics).",
            "experimental_validation": false,
            "challenges_or_limitations": "General models of small scale may lack domain understanding and can produce non-sensical outputs for scientific inference prompts; scale and domain pretraining matter.",
            "uuid": "e6862.4",
            "source_info": {
                "paper_title": "Large language models for scientific discovery in molecular property prediction",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "Falcon-40b",
            "name_full": "Falcon (40 billion parameters)",
            "brief_description": "A larger Falcon variant used as an LLM backbone; performed better than Falcon-7b and could bridge some domain gaps due to increased scale, but still generally trailed domain-specialized Galactica in this study unless scale afforded emergent capabilities.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Falcon-40b",
            "model_type": "decoder-only general-purpose LLM (prompt-based)",
            "model_size": "40B parameters",
            "training_data_description": "Pretrained on large general-purpose corpora (not specifically on scientific literature according to the manuscript).",
            "generation_method": "Prompted literature synthesis and data inference to output measurable rules; summarization/deduplication across batches.",
            "chemical_representation": "SMILES",
            "target_application": "Rule/feature extraction for molecular property prediction tasks.",
            "constraints_used": "Numeric/categorical rule requirement; sampling strategy; scaffold split on datasets.",
            "integration_with_external_tools": "RDKit, random forest, statistics for rule validation.",
            "dataset_used": "MoleculeNet family datasets used in the paper.",
            "evaluation_metrics": "AUC-ROC / RMSE / MAE; statistical tests for rules.",
            "reported_results": "Falcon-40b improved over Falcon-7b and could perform scientific inference across domains due to emergent capabilities enabled by scale; performance still compared against Galactica and GPT-4 in backbone studies.",
            "experimental_validation": false,
            "challenges_or_limitations": "General-purpose pretraining requires more scale to match domain-specific models; scale has computational and access costs.",
            "uuid": "e6862.5",
            "source_info": {
                "paper_title": "Large language models for scientific discovery in molecular property prediction",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "ChemLLM-7b",
            "name_full": "ChemLLM (7 billion parameters)",
            "brief_description": "A chemical-domain-adapted LLM (fine-tuned from a general model) evaluated as a backbone; in this work ChemLLM-7b underperformed compared to Galactica-6.7b, likely due to limited chemical token/data scale during fine-tuning.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "ChemLLM-7b",
            "model_type": "fine-tuned domain-adapted LLM (decoder-only)",
            "model_size": "7B parameters",
            "training_data_description": "Adapted from a general LLM (InternLM2-7b) and fine-tuned on chemical data; paper states ChemLLM used only ~7 million tokens of chemical data (very small scale compared to other models).",
            "generation_method": "Prompt-based rule synthesis and inference (same LLM4SD pipeline), but performance was limited in experiments.",
            "chemical_representation": "SMILES",
            "target_application": "Molecular property prediction (via rule generation into RDKit features).",
            "constraints_used": "Numeric/categorical rule requirement; sampling-based inference.",
            "integration_with_external_tools": "RDKit and downstream random forest; statistical rule validation.",
            "dataset_used": "MoleculeNet datasets for rule inference/validation.",
            "evaluation_metrics": "AUC-ROC, RMSE, MAE, statistical tests on rules.",
            "reported_results": "ChemLLM-7b substantially underperformed compared to Galactica-6.7b in the authors' backbone comparison; authors attribute underperformance to scale and training-data-size differences.",
            "experimental_validation": false,
            "challenges_or_limitations": "Small fine-tuning corpus (7M tokens) limited the model's domain knowledge despite chemical-focused training; adaptation-from-general-model approach may be inferior to large-scale domain pretraining.",
            "uuid": "e6862.6",
            "source_info": {
                "paper_title": "Large language models for scientific discovery in molecular property prediction",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "ChemDFM-13b",
            "name_full": "ChemDFM (13 billion parameters)",
            "brief_description": "A chemical foundation model (based on LLaMa-13b fine-tuning) evaluated as a backbone; in this study ChemDFM-13b underperformed relative to Galactica-6.7b, possibly due to differences in pretraining approach and data scale.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "ChemDFM-13b",
            "model_type": "fine-tuned domain-adapted LLM (decoder-only)",
            "model_size": "13B parameters",
            "training_data_description": "Adapted from a general LLM (LLaMa-13b) and trained/fine-tuned on chemical/scientific tokens; paper states ChemDFM used ~34 billion tokens (training data scale noted).",
            "generation_method": "Prompt-based literature synthesis and data inference per LLM4SD protocol; produced measurable rules for conversion to RDKit code.",
            "chemical_representation": "SMILES",
            "target_application": "Feature/rule extraction for molecular property prediction.",
            "constraints_used": "Numeric/categorical rule requirement; sampling iterations for inference.",
            "integration_with_external_tools": "RDKit, random forest, statistical testing, literature review.",
            "dataset_used": "MoleculeNet datasets used in experiments.",
            "evaluation_metrics": "AUC-ROC, RMSE, MAE; statistical tests to validate rules.",
            "reported_results": "ChemDFM-13b underperformed compared to Galactica-6.7b in this study despite larger fine-tuning token counts; authors discuss differences in adaptation strategy and datasets as possible causes.",
            "experimental_validation": false,
            "challenges_or_limitations": "Fine-tuning from a general LLM may not match models trained from-scratch on scientific corpora; data composition and pretraining approach crucial for downstream scientific inference.",
            "uuid": "e6862.7",
            "source_info": {
                "paper_title": "Large language models for scientific discovery in molecular property prediction",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "RDKit",
            "name_full": "RDKit cheminformatics toolkit",
            "brief_description": "Open-source cheminformatics library used to compute molecular descriptors and implement the executable functions that realize LLM-generated rules (e.g., molecular weight, topological polar surface area, H-bond donors/acceptors).",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "RDKit (rdMolDescriptors and other modules)",
            "model_type": "cheminformatics software library (used via generated Python code)",
            "model_size": "n/a (software library)",
            "training_data_description": "n/a (tool for computing molecule descriptors from SMILES/internally coded algorithms)",
            "generation_method": "n/a (used as runtime library invoked by generated Python functions that implement LLM-specified rules)",
            "chemical_representation": "SMILES (RDKit parses SMILES into molecular graphs and computes descriptors)",
            "target_application": "Compute numeric/categorical features from SMILES per LLM-generated rules to be used as inputs to interpretable ML models for property prediction.",
            "constraints_used": "Rules must map to RDKit-computable descriptors or boolean tests (authors required numeric/categorical measurability to ensure RDKit implementation).",
            "integration_with_external_tools": "Called by Python code generated by GPT-4; outputs used by scikit-learn random-forest models; used alongside statistical testing libraries for rule validation.",
            "dataset_used": "Applied to MoleculeNet SMILES datasets to produce features for training/prediction.",
            "evaluation_metrics": "Not applicable (tool used for feature computation); downstream model metrics: AUC-ROC, RMSE, MAE.",
            "reported_results": "RDKit used to implement LLM-derived rules; no separate RDKit-specific results reported.",
            "experimental_validation": false,
            "challenges_or_limitations": "Requires that LLM-specified rules be convertible to RDKit-implementable computations; some abstract or complex hypothesized features may not be directly implementable.",
            "uuid": "e6862.8",
            "source_info": {
                "paper_title": "Large language models for scientific discovery in molecular property prediction",
                "publication_date_yy_mm": "2025-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Galactica: a large language model for science",
            "rating": 2,
            "sanitized_title": "galactica_a_large_language_model_for_science"
        },
        {
            "paper_title": "Chemllm: a chemical large language model",
            "rating": 2,
            "sanitized_title": "chemllm_a_chemical_large_language_model"
        },
        {
            "paper_title": "ChemDFM: a large language foundation model for chemistry",
            "rating": 2,
            "sanitized_title": "chemdfm_a_large_language_foundation_model_for_chemistry"
        },
        {
            "paper_title": "MoleculeNet: a benchmark for molecular machine learning",
            "rating": 2,
            "sanitized_title": "moleculenet_a_benchmark_for_molecular_machine_learning"
        },
        {
            "paper_title": "Are large language models superhuman chemists?",
            "rating": 1,
            "sanitized_title": "are_large_language_models_superhuman_chemists"
        },
        {
            "paper_title": "Gpt-4 technical report",
            "rating": 1,
            "sanitized_title": "gpt4_technical_report"
        }
    ],
    "cost": 0.01857125,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>nature machine intelligence</p>
<p>Yizhen Zheng 
Huan Yee Koh 0000-0002-0488-2616
Jiaxin Ju 0000-0003-3503-5708
An T N Nguyen 0000-0003-0528-9416
Lauren T May 0000-0002-4412-1707
Geoffrey I Webb 0000-0001-9963-5169
&amp; Shirui Pan 0000-0003-0794-527X
nature machine intelligence
C1D42EBB99874670B140BC3D2A2925D710.1038/s42256-025-00994-z
Large language models (LLMs) are a form of artificial intelligence system encapsulating vast knowledge in the form of natural language.These systems are adept at numerous complex tasks including creative writing, storytelling, translation, question-answering, summarization and computer code generation.Although LLMs have seen initial applications in natural sciences, their potential for driving scientific discovery remains largely unexplored.In this work, we introduce LLM4SD, a framework designed to harness LLMs for driving scientific discovery in molecular property prediction by synthesizing knowledge from literature and inferring knowledge from scientific data.LLMs synthesize knowledge by extracting established information from scientific literature, such as molecular weight being key to predicting solubility.For inference, LLMs identify patterns in molecular data, particularly in Simplified Molecular Input Line Entry System-encoded structures, such as halogen-containing molecules being more likely to cross the blood-brain barrier.This information is presented as interpretable knowledge, enabling the transformation of molecules into feature vectors.By using these features with interpretable models such as random forest, LLM4SD can outperform the current state of the art across a range of benchmark tasks for predicting molecular properties.We foresee it providing interpretable and potentially new insights, aiding scientific discovery in molecular property prediction.</p>
<p>Article</p>
<p>https://doi.org/10.1038/s42256-025-00994-zCalcExactMolWt (mol) &lt; 500', which will return a binary result.Here, rdMolDescriptors is a module in the RDKit 13 library.If the result is 0, it means the molecule has a weight greater than 500 Da and vice versa.In practice, human experts can review these generated rules to drop duplicate or non-functional parts.However, the experimental results presented were obtained without any human intervention or modification.</p>
<p>With these rules and functions, molecules can be transformed into vectorized representations: that is, features.These rule-based features can then be used to train an interpretable model, such as a random forest or linear classifier (Fig. 1c).Remarkably, we noted that when enhanced with LLM4SD, these traditional interpretable models can surpass state-of-the-art baselines.</p>
<p>Once the interpretable models are trained (Fig. 1d), we can gain valuable insights.In the BBBP prediction task, we can see the prediction results, the rules content, how each molecule aligns with those rules by looking into their vector representation and how important each rule is for the final prediction.For example, a random-forest model builds decision trees, and if these trees rely heavily on certain features, those features are assessed to have higher importance for the task.This helps us understand which rules are most critical in the model's predictions.As shown in Fig. 1d, the input molecule has a molecular weight under 500 Da, and molecular weight is a key factor in predicting BBBP.To improve usability for researchers, we have created a web-based application that offers knowledge synthesis, inference and prediction with interpretation (Supplementary Information Section 3).</p>
<p>Experimental results</p>
<p>In this section, we offer a synopsis of LLM4SD's performance spanning the four domains of physiology, biophysics, quantum mechanics and physical chemistry.Subsequently, we investigate the key components of LLM4SD, examining its performance across various LLM backbones of differing scales and pretraining datasets.</p>
<p>Overall performance on four domains.To evaluate LLM4SD's versatility, we conducted a comprehensive analysis of its performance across 58 molecular prediction tasks across four domains (Fig. 2).We compared LLM4SD's performance with nine specialized, state-of-the-art supervised machine learning models.These are advanced graph or geometric neural networks (GNNs): AttrMask 14 , GraphCL 15 , MolCLR 16 , 3DInfomax 17 , GraphMVP 18 , MoleBERT 19 , Grover 20 and UniMol 21 .Each model was pretrained on large datasets with diverse molecular knowledge and then fine-tuned for specific tasks (Methods).As a standard baseline, we implemented random forest with ECFP4 (ref.22) as input set features.</p>
<p>Benchmarking LLM4SD against the baseline, LLM4SD demonstrated its superior efficacy and performance (Fig. 2).This performance spanned 58 diverse tasks, from physiology (Extended Data Figs.1-3) and biophysics (Extended Data Fig. 4) to physical chemistry (Extended Data Fig. 5) and quantum mechanics (Extended Data Fig. 6).The detailed descriptions of these tasks is illustrated in the Methods section (Methods, 'Datasets').</p>
<p>In both physiology and biophysics, our model outperformed all existing baselines (Fig. 2a).Notably, we attained state-of-the-art results in physiology, raising the area under the receiver operating characteristic curve (AUC-ROC) from a previous best of 74.53% to 76.60%, a gain of 2.07%.In biophysics, our model also achieved the best performance.These advancements in physiology and biophysics emphasize the robustness and precision of LLM4SD in tasks that demand intricate biological understanding and modelling.</p>
<p>On tasks in quantum mechanics and physical chemistry, LLM4SD demonstrated substantial advancements (Fig. 2b).In the domain of quantum mechanics, it showed a profound improvement of 48.2% over the best performing baseline, registering an average mean absolute error (MAE) of 5.8233 across 12 tasks as opposed to 11.2450 achieved by factors-knowledge derived from the literature they are trained on.Additionally, LLMs can understand formal scientific languages, such as the Simplified Molecular Input Line Entry System (SMILES), which describes molecular structures and is used for storing molecular property data.For example, to represent a molecule's solubility, its SMILES string and logP score can be stored together.Given these two key abilities-understanding molecular property prediction tasks and interpreting SMILES-we are motivated to explore two questions: can LLMs leverage their prior knowledge and reasoning abilities to facilitate scientific discovery?Can LLMs be effectively used to help predicting the properties of molecules?</p>
<p>In this work, we propose LLM4SD (LLMs for Scientific Discovery).LLM4SD functions by performing two main tasks: synthesizing knowledge from existing literature and inferring knowledge by observing experimental data.First, LLM4SD retrieves known rules to predict molecular properties based on its pretrained literature, such as molecules with molecular weight under 500 Da being more likely to pass the blood-brain barrier (BBB).Second, using its understanding of SMILES notation and chemistry knowledge, LLM4SD identifies patterns from experimental data, such as molecules containing halogens being more likely to pass the BBB.These rules are then used to create interpretable feature vectors for each molecule.By training an interpretable machine learning model using these vectors, we show that this pipeline achieves the current state of the art on molecular property prediction across 58 benchmark tasks from the MoleculeNet dataset curated by the Stanford PANDE group 9 .These tasks, encompassing both classification and regression, span four domains: physiology, biophysics, physical chemistry and quantum mechanics.</p>
<p>Although these molecular property predictions address complex challenges, such as predicting a molecule's BBB permeability, they represent only a small corner of the breadth of scientific endeavour.The findings of LLM4SD highlight the broader potential of using LLMs for scientific discovery.</p>
<p>Results</p>
<p>LLM4SD pipeline</p>
<p>Our scientific discovery pipeline, LLM4SD, shown in Fig. 1, consists of four main components: (1) knowledge synthesis from the scientific literature, (2) knowledge inference from data, (3) model training and (4) interpretable insights.</p>
<p>In the knowledge synthesis from literature phase (Fig. 1a), LLMs use their pretrained understanding from extensive scientific literature 5,10,11 to synthesize rules for predicting molecular properties.For example, in predicting BBB permeability (BBBP), LLMs can apply established knowledge, such as the Lipinski rule of five 12 , which assesses drug-likeness based on rules like molecular weight being under 500 Da or there being fewer than five hydrogen bond donors.In the knowledge inference from data phase (Fig. 1b), LLMs can utilize their inferential and analytical abilities to identify patterns in scientific data: for example, SMILES strings and their corresponding labels.For instance, LLMs can detect that molecules containing halogens are more likely to pass the BBB, as this trait is frequently observed in the provided data for molecules that successfully cross the barrier.</p>
<p>In both the knowledge synthesis and inference stages, we require that the identified rules have either a numerical or categorical measure associated with them.This ensures that the rules can be readily transformed into corresponding code functions, which in turn can convert each molecule into a vector of values.Prompts used for both knowledge synthesis and inference in LLM4SD are shown in Extended Data Tables 1-3.</p>
<p>To convert these rules into corresponding executable code functions, we utilize GPT-4 to generate Python code using cheminformatics software such as RDKit 13 .For example, the rule 'if the molecular weight is smaller than 500 Da, the molecule is likely more permeable to the blood-brain barrier' can be converted into code as 'rdMolDescriptors.</p>
<p>Article</p>
<p>https://doi.org/10.1038/s42256-025-00994-z the second-best baseline GraphMVP.Similarly, in physical chemistry, LLM4SD observed a noteworthy enhancement, with the model reaching an MAE of 1.28, marking an 12.9% advancement over the baseline root mean square error (RMSE) of 1.47.These substantial improvements in regression tasks affirm the refined capability of our approach in continuous prediction.</p>
<p>Compared to these advanced GNN baselines, LLM4SD has two primary advantages.First, it leverages a wealth of prior knowledge accumulated from decades of scientific literature.Although GNNs can learn general patterns from extensive molecular datasets through pretraining, incorporating specific scientific knowledge typically requires explicit effort, such as the careful curation and integration of domain-specific features into the model's input, unless that knowledge is hand-coded for their use.Developers must manually decide both what knowledge to include and how to integrate it effectively into the model 23,24 .In contrast, LLMs, pretrained on vast amounts of literature across fields like chemistry, inherently embed substantial amounts of scientific knowledge that can be leveraged directly without additional intervention beyond natural language interaction.Second, GNNs, although effective in encoding molecules into embeddings, often lack interpretability.This limits their utility in generating clear scientific hypotheses because their interpretive mechanisms, such as attention mechanisms, tend to be opaque.</p>
<p>Study of key components.</p>
<p>To delve deeper into the intricacies of the LLM4SD pipeline, we studied the influence of scale and pretraining datasets on its performance.In addition, we assessed the relative contributions of knowledge synthesis and inference.Our evaluation spanned across a spectrum of foundational LLM backbones, notably the GPT-4 (ref.ChemDFM-13b (ref.26).These backbones can be categorized into two classes, general LLMs and domain-specific LLMs.In particular, GPT-4 and the Falcon models are trained for a broad range of applications, imbibing a more general context during their pretraining phase, whereas the Galactica 10 models, ChemLLM 25 and ChemDFM 26 are pretrained or fine-tuned on mainly scientific literature.</p>
<p>Effect of scale.</p>
<p>The comparison of seven LLM4SD backbones revealed substantial differences among the different LLMs (Fig. 3a,b).Within the Falcon series, performance disparities were conspicuous.Falcon-7b, a smaller model, fell short compared to Falcon-40b in its range of domain expertise.Notably, it failed to conduct tasks in two key areas-physiology and quantum mechanics-indicating a weaker understanding of scientific challenges and data interpretation.Specifically, it produced gibberish responses when asked to conduct inference.</p>
<p>Conversely, the Galactica series painted a more nuanced picture.Unlike the Falcon series, a larger model did not necessarily translate to superior performance.In disciplines such as physiology, biophysics and physical chemistry, Galactica-6.7brivalled the performance of Galactica-30b, despite the latter having more than four times the number of parameters.However, in the domain of quantum mechanics, the larger Galactica-30b surged ahead, outperforming Galactica-6.7bby a margin of 14%.This variance could be attributed to the intricate and abstract Assume you are an experienced chemist.Please come up with 20/30 rules that are important to predict blood-brain barrier permeability.</p>
<p>Prompt for LLMs:</p>
<p>Prompt for LLMs:</p>
<p>Article</p>
<p>https://doi.org/10.1038/s42256-025-00994-znature of quantum mechanics, where the depth and breadth of knowledge encapsulated in the larger model might offer a discernible advantage.Surprisingly, ChemLLM-7b and ChemDFM-13b substantially underperform compared to Galactica-6.7b,despite all of them being domain-specific models.Unlike Galactica-6.7b,which was built from scratch using 106 billion tokens of scientific literature, ChemLLM-7b and ChemDFM-13b are adapted from general LLMs-InternLM2-7b (ref.27) and LLaMa-13b (ref.28), respectively-through fine-tuning.Moreover, Galactica benefits from a rich training dataset, whereas ChemLLM-7b utilizes only 7 million tokens of chemical data and ChemDFM-13b is trained on 34 billion tokens.We conjecture that this discrepancy in data scale and training approach leads to the performance difference.</p>
<p>GPT-4 consistently performs well on all tasks, which is reasonable considering its enormous scale.It is said to have been trained on approximately 1.76 trillion tokens 29 , which is more than 100 times larger than all baselines.</p>
<p>Effect of pretraining datasets of LLMs.It becomes evident that an LLM steeped in scientific literature, even if smaller in scale, exhibits a commendable prowess in scientific tasks (Fig. 3a,b).Conversely, the Falcon series, designed for general utility, necessitates a more substantial scale to effectively navigate scientific challenges.We postulate that this phenomenon is underpinned by the emergent capabilities 30 inherent in large-scale LLMs.These capabilities empower the more expansive Falcon-40b to bridge the knowledge gap and adapt to scientific tasks.In a broader perspective, despite their relatively modest scale, the Galactica models consistently outperformed the Falcon series, underscoring the pivotal role of proper domain-specific pretraining.This is further supported by the fact that, despite its smaller scale, the Galactica series achieves performance comparable to that of GPT-4.</p>
<p>Contributions of knowledge synthesis and inference.</p>
<p>It is important to assess the relative contributions of the features synthesized from literature and those inferred from data.To this end, we trained each of the classifiers using just one or the other or both forms of feature.Overall values for these three types of models were obtained by averaging over results for all tasks in a domain (Fig. 3c).</p>
<p>Across the 58 tasks, the combination of synthesis and inference features consistently outperformed individual methods.Specifically, in the field of physiology, an average AUC-ROC of 73.62 was achieved using both methods, compared to 70.46 with synthesis alone and 69.25 with inference.Similarly, in biophysics, combining both methods yielded an average AUC-ROC of 79.10, surpassing the scores of 76.30 and 75.76 obtained from synthesis and inference features, respectively.In physical chemistry, the combined approach resulted in an average RMSE of 1.54, which is notably better than the 1.99 from synthesis features and 1.77 from inference features.Finally, in quantum mechanics, the use of both synthesis and inference features produced an MAE of 10.42, improving on the values of 37.67 and 68.17 recorded with synthesis and inference alone.</p>
<p>These observations highlight the value of combining knowledge synthesis from scientific literature with inference from data.Literature imparts foundational theoretical insights, whereas empirical data identifies further regularities.The fusion of these knowledge facets equips the models with a comprehensive understanding, empowering them to excel across varied tasks and domains.</p>
<p>Validation of established rules</p>
<p>With LLM4SD outperforming specialized, state-of-the-art methods, we further validated the rules generated by Galactica-6.7bdue to its superior performance and ease of reproducibility.Individual rules were validated in two ways: statistical tests to confirm their association with
A t t r M a s k G r a p h C L G r a p h M V P M o l C L R M o l e B E R T R F + E C F P 4 G r o v e r U n iM o l L L M 4 S D 3 D I n f o m a x A t t r M a s k G r a p h C L G r a p h M V P M o l C L R M o l e B E R T R F + E C F P 4 G r o v e r U n iM o l L L M 4 S D 3 D I n f o m a x A t t r M a s k G r a p h C L G r a p h M V P M o l C L R M o l e B E R T R F + E C F P 4 G r o v e r U n iM o l LLM4SD Avg result 3DInfomax AttrMask GraphCL GraphMVP MolCLR MoleBERT RF + ECFP4 Grover UniMol LLM4SD Avg result 3DInfomax AttrMask GraphCL GraphMVP MolCLR MoleBERT RF + ECFP4 Grover UniMol L L M 4 S D MAE RMSE G r a p h C L G r a p h M V P M o l C L R M o l e B E R T R F + E C F P 4 G r o v e r U n iM o l 3 D I n f o m a x A t t r M a s k</p>
<p>Article</p>
<p>https://doi.org/10.1038/s42256-025-00994-z the target molecular attribute and a literature review to assess whether they are discussed in existing scientific literature.We employed the Mann-Whitney U-test 31 of association for classification tasks and the linear correlation t-test for regression tasks.The Mann-Whitney U-test 31 compared the distributions of a chosen rule across the two classes of the target variable, thereby evaluating the statistical relevance of the rule's ability to distinguish classes.Conversely, the linear regression t-test treated the chosen rule as the independent variable and examined whether its coefficient significantly deviated from 0, reflecting whether the rule contributes to regression prediction.</p>
<p>We further carried out a comprehensive review with in-domain experts of statistically significant rules to evaluate whether they are already identified in existing literature (Supplementary Information Section 4).Specifically, the evaluation process follows a two-step approach.First, two pharmacologists independently perform a literature review using Google Scholar for any studies related to the identified statistically significant rules for the downstream task.After that, if both experts found no related articles, we categorize the rule as 'statistically significant and not found in literature'.If either identified the rule in the literature, it would be classified as 'statistically significant and found in literature'.Following this process, we categorized each rule into one of three classes: statistically significant and literature supported; statistically significant and not found in literature; or statistically insignificant (Fig. 4).</p>
<p>Knowledge synthesis from scientific literature.We discovered that most of the synthesized rules we examined are readily available in existing scholarly works.Notably, across all selected tasks, an overwhelming majority (85%) of these rules had statistically significant association with the target labels, affirming our pipeline's ability to summarize rules from scientific literature that were the most important for different tasks and domains.</p>
<p>Importantly, except for BACE 32 and Tox21-NR-Ahr9, we found no instances where statistically significant synthesized rules were absent from existing literature (Fig. 4).This aligns with the design of our pipeline: without analysing the data, LLMs tend to aggregate and summarize existing knowledge.To illustrate, in the context of BBBP, the rules generated by our pipeline were consistent with well-established determinants such as molecular weight, lipophilicity, distribution coefficient, topological polar surface area and hydrogen bonds [33][34][35] .These findings validate the robustness and reliability of our pipeline in leveraging LLMs to summarize existing scientific literature.</p>
<p>Biophysics</p>
<p>Quantum mechanics Physical chemistry Fig. 3 | Study of LLM4SD's components.a, Performance comparison of LLM4SD using seven LLM backbones for physiology and biophysics.b, Performance comparison of LLM4SD using seven LLM backbones for physical chemistry and quantum mechanics.c, Examining the influence of both synthesized and inferred knowledge on the average (across all backbones) model performance across all four domains.The triangle's colour signifies the metric employed for domainspecific tasks.A (+) next to the metric name indicates that higher values yield better results, whereas a (−) suggests the contrary.</p>
<p>Article</p>
<p>https://doi.org/10.1038/s42256-025-00994-zKnowledge inference from data.We found that an average of 91.3% of the inferred rules were statistically significant, higher than synthesized rules (Fig. 4).Of these, an average of 74% rules were already documented in existing scientific literature, and we were unable to find prior mention of 17.3%.These latter rules were primarily associated with data patterns that are not widely discussed in the literature but can be inferred from our task dataset, such as obscure molecular substructures that influence target labels like the Gibbs free energy (ΔG) of a molecule.</p>
<p>In contrast to the knowledge synthesized from literature, we found that six out of eight tasks have statistically significant rules that we could not identify in the existing literature.This suggests that the inferred rules produced by LLM4SD are not merely a result of the LLM's textual memorization during pretraining.Instead, the inferred rules reflect a genuine capability to derive meaningful rules from data based on the specific task.</p>
<p>Our case studies further substantiated the utility of these unidentified but statistically significant rules.For instance, in BBBP, where 38% of rules are statistically significant but unidentified, Galactica-6.7bpinpointed the carbonyl functional group and fragment rings as key determinants of a molecule's BBBP.We hypothesize that these features are crucial for calculating a molecule's cross-sectional area, which in turn influences its orientation in lipid-water interfaces-factors vital for membrane partitioning and permeation 35 .Intriguingly, this suggests that our pipeline enables LLMs to infer 'second-order features'.</p>
<p>These are features that may not be immediately obvious or widely recognized but are consistent with established scientific principles in literature.In doing so, LLMs not only corroborate existing knowledge but also apply existing knowledge in interpreting data, thereby enriching the current scientific discourse.An additional case study was conducted to assess the effectiveness of several lesser-explored rules (Supplementary Information Section 6).</p>
<p>By leveraging LLMs, our pipeline not only validates well-established scientific principles but also uncovers less documented and even potentially new rules.This facilitates a more effective and transparent interaction between scientists and the artificial intelligence (AI) system, enhancing both the quality and trustworthiness of the research output.Moreover, the statistically robust but underrepresented rules we identified could serve as promising avenues for future scientific exploration, thereby advancing the frontier of collaborative, AI-assisted research.</p>
<p>Discussion</p>
<p>Our exploration unveiled unexpected capabilities of LLM4SD through our specially designed pipeline, enabling LLMs to excel in scientific discovery in molecular prediction.Through seamless integration with our proposed architecture, LLMs exhibit state-of-the-art performance across 58 downstream tasks drawn from four domains in the Mole-culeNet benchmark.The inherent versatility of LLM4SD stands as a testament to its potential for broader molecular applications across varied domains.Fig. 4 | Literature review and statistical analysis of LLM rules.a-d, We conducted statistical analysis and a comprehensive literature review on rules generated by Galactica-6.7bacross all four scientific domains, with two tasks evaluated for each domain: quantum mechanics (a), physical chemistry (b), physiology (c) and biophysics (d).In the statistical analysis, the significance of a rule is determined based on the task type: for classification, the two-sided Mann-Whitney U-test 31 compares the difference in distributions of chosen rule across the two classes of the target variable; and for regression, the two-sided linear regression t-test 44 treats the chosen rule as the independent variable and examined whether its coefficient significantly deviated from 0, reflecting whether the rule contributes to prediction.In both cases, we used a 0.05 P value threshold to determine rule significance.In the literature review, we assessed the prevalence of a rule in existing literature.With statistical analysis and literature review, each rule is categorized into one of three classes: statistically significant and literature supported; statistically significant and not found in literature; or statistically insignificant.Across all tasks, literature-synthesized knowledge rules were generally both prevalent in existing literature and statistically significant.</p>
<p>In contrast, empirically inferred data rules yielded mixed results, with some easily found in existing literature and others not identified by the researchers.</p>
<p>Article</p>
<p>https://doi.org/10.1038/s42256-025-00994-zScientific discovery, vast in scope, is constantly evolving with our expanding understanding of the universe.Our study only focused on molecular property prediction.However, LLM4SD's promising results in molecular property prediction hold promise for the direct application of LLMs in more advanced applications, such as protein sequence or gene sequence analysis.Protein and gene sequences are much more complicated than the SMILES strings in our study, which typically comprise up to dozens of characters.For example, protein sequences normally have 300-500 amino acids, whereas gene sequences typically contain many thousands of nucleotides.The complexity of these data poses great challenges for LLMs to understand long context domain-specific information, which requires extensive prior knowledge.In addition, LLMs need a longer and effective context window to process this data.Even for models that support large input, they often cannot use the long context input effectively in practice 36,37 .To enhance LLMs' ability to handle intricate biological data, pretraining them on vast, diverse datasets of protein or gene sequences may help the models understand the complex patterns more effectively.Additionally, incorporating retrieval-augmented generation with specialized biological knowledge bases, such as UniProt 38 and GenBank 39 , can provide additional knowledge to improve understanding and contextual accuracy.Furthermore, developing efficient tokenization methods specifically tailored for biological sequences may enhance the model's ability to process and analyse this type of data, leading to more accurate and insightful results.We envision further expansion, integrating more tasks and domains, pushing LLMs to their full potential and reshaping the boundaries of scientific inquiry.</p>
<p>As we gaze towards the horizon, the potential trajectory for LLM4SD is compelling.We anticipate a future where the nexus between LLMs and advanced scientific toolkits deepens.As computational capabilities grow and scientific knowledge expands, our pipeline stands poised for evolutionary enhancements.Our steadfast goal is to harmoniously incorporate LLMs with myriad scientific arenas, unlocking insights and pioneering avenues previously unimagined.</p>
<p>Methods</p>
<p>Datasets</p>
<p>We conducted a thorough evaluation of LLM4SD, covering 58 subtasks across four unique domains for a robust assessment.The physiology domain included 41 tasks like BBBP, ClinTox and the 12-task Tox21, ranging from NR-AR to SR-p53, along with the 27-task SIDER suite covering various medical conditions.Biophysics offered two classification tasks: BACE and HIV.In physical chemistry, we addressed three regression tasks: ESOL, FreeSolv and Lipophilicity; and the quantum mechanics domain presented 12 regression tasks within the QM9 dataset, exploring properties from mu to G, providing a comprehensive insight into LLM4SD's capabilities.</p>
<p>Physiology</p>
<p>BBBP.</p>
<p>The BBBP dataset contains 2,039 instances, each representing unique compounds labelled based on their permeability properties.Predicting which molecules can cross this barrier is paramount for drug development, especially for neurological conditions.</p>
<p>ClinTox.The ClinTox dataset, with 1,478 instances, provides comprehensive information on the toxicological properties of various compounds.</p>
<p>Tox21.With 7,831 instances, the Tox21 dataset is a collaborative effort to identify environmental toxicants.Its 12 classification tasks focus on specific biological targets or pathways.The nuclear receptor (NR) tasks, namely NR-AhR, NR-AR, NR-AR-LBD, NR-Aromatase, NR-ER, NR-ER-LBD and NR-PPAR-gamma, examine interactions with intracellular proteins influencing gene expression and potential toxic effects.The stress response (SR) tasks, including SR-ARE, SR-ATAD5, SR-HSE, SR-MMP and SR-p53, explore the impact of chemicals on stress-related cellular pathways.</p>
<p>SIDER.</p>
<p>The SIDER dataset, with 1,427 instances, offers detailed data on medication side effects.Each task in this dataset relates to a specific adverse drug reaction, aiding researchers in understanding and predicting potential drug side effects.The 27 classification tasks are (1) hepatobiliary disorders; (2) metabolism and nutrition disorders;</p>
<p>(3) product issues; (4) eye disorders; (5)</p>
<p>Biophysics</p>
<p>HIV.With a collection of 17,930 instances, the HIV dataset offers a comprehensive repository of molecules represented in the SMILES format.This dataset is instrumental in the classification of compounds based on their potential inhibitory effects against HIV.</p>
<p>BACE.</p>
<p>The BACE dataset, comprising 11,908 instances, is a curated collection of molecules, each represented in the SMILES format.This dataset is tailored for classification tasks, aiming to discern molecules that can inhibit the BACE-1 enzyme.By analysing the molecules within this dataset, researchers can glean insights into the structural features that confer inhibitory properties against BACE-1.</p>
<p>Physical chemistry</p>
<p>ESOL.The ESOL dataset, comprising 1,128 instances, is a curated collection that delves into the solubility of molecules in water.By analysing the ESOL dataset, researchers can gain a deeper understanding of the molecular features that dictate solubility, thereby aiding in the design of compounds with optimal solubility profiles.Each entry in this dataset is represented using the SMILES notation, a universal language for describing the structure of chemical species.</p>
<p>FreeSolv.With 642 instances, the FreeSolv dataset provides comprehensive data on the hydration free energy of molecules.This dataset is pivotal for researchers aiming to predict how molecules interact with water, which has implications for drug solubility and stability.Each molecule in the FreeSolv dataset is also represented using the SMILES notation.</p>
<p>Lipophilicity.Lipophilicity is a fundamental property that influences the absorption, distribution, metabolism and excretion of drugs.The Lipophilicity dataset, with 4,200 compounds, offers a rich resource for understanding this property.Analysing this dataset allows researchers to discern the molecular attributes that contribute to a compound's lipophilicity, guiding the synthesis of molecules with desired pharmacokinetic properties.Like the other datasets in this domain, each entry is denoted using the SMILES notation.</p>
<p>Quantum mechanics</p>
<p>QM9.The quantum mechanics domain, central to understanding the fundamental properties of matter, is exemplified in our evaluation through the QM9 dataset.Comprising 133,885 instances, the QM9 Article https://doi.org/10.1038/s42256-025-00994-zdataset provides a comprehensive exploration of molecules' quantum mechanical attributes, essential for diverse applications from material science to pharmaceuticals.It includes 12 tasks: μ (dipole moment), α (polarizability), R 2 (squared radius), ZPVE (zero-point vibrational energy), C v (heat capacity at constant volume), Δϵ (energy gap), ϵ HOMO (highest occupied molecular orbital energy), ϵ LUMO (lowest unoccupied molecular orbital energy), U 0 (internal energy at 0 Kelvin), U (internal energy at standard state), H (enthalpy) and G (Gibbs free energy).</p>
<p>Baselines</p>
<p>We rigorously assessed our pipeline in comparison to specialized, state-of-the-art supervised machine learning methods.For conventional approaches, we employed random forest 40 , using ECFP4 (ref.22) as the input feature set.We also considered state-of-the-art GNNs, including attribute masking (AttrMask) 14 , GraphCL 15 , MolCLR 16 , 3DInfomax 17 , GraphMVP 18 , MoleBERT 19 , Grover 20 and UniMol 21 .Each of these models was initialized with pretrained weights and subsequently fine-tuned for specific tasks.</p>
<p>In summary, AttrMask pretraining involves teaching the GNN to predict randomly masked atom and bond attributes within molecular graphs.GROVER pretraining focuses on contextual predictions of an atom's surroundings and predicts graph-level motifs.GraphCL and MolCLR use graph augmentations for a contrastive learning objective, aimed at maximizing the similarity between augmentations originating from the same molecule while minimizing similarity between augmentations from different molecules.GraphMVP and 3DInfomax leverage existing three-dimensional (3D) molecular datasets to pretrain models capable of deducing 3D molecular geometry from two-dimensional graphs, by optimizing mutual information between 3D summary vectors and GNN graph representations.MoleBERT, the recent state-of-the-art method, employs a vector quantized variational autoencoder-based tokenizer to diversify atom vocabulary, thereby balancing dominant and rare atoms.It uses masked atoms modelling and triplet masked contrastive learning for node and graph-level pretraining, respectively.Finally, UniMol pioneers a universal 3D molecular representation learning method, which pretrained with 3D position recovery and masked atom prediction.</p>
<p>LLM4SD in the molecular prediction pipeline</p>
<p>In this section, we detail the proposed pipeline and the techniques used to align with the requirements of molecular property prediction tasks.Instead of merely prompting LLMs to generate scientific hypotheses 41 or training them for direct predictions 42 , LLM4SD emulates how human experts conduct scientific research.This includes synthesizing knowledge from literature, inferring hypotheses from datasets, validating findings through experiments and elucidating the rationale behind predictions.</p>
<p>Knowledge synthesis from the scientific literature.LLMs are usually pretrained on large corpora of text data that include books, articles, websites and other written content.This extensive pretraining helps LLMs to learn the structure of the language, recognize patterns, understand context and acquire a wide-ranging knowledge of facts and concepts.Thus, the goal of the knowledge synthesis process is to extract relevant features from the vast pool of the knowledge that an LLM possesses from the pretraining stage.</p>
<p>To achieve this, we first instruct the LLM to adopt the persona of an experienced chemist and then engage it to identify pertinent features based on its existing knowledge.This form of role-playing prompt facilitates the knowledge mining process to mimic how human experts solve real-world challenges.For example, when a chemist needs to predict the bioactivity or BBBP of a molecule, they often apply feature-related rules such as number of hydrogen bond donors/acceptors, molecular weight and logP.We require that the features identified by LLMs can be measured with a numerical or categorical measure to enable their transcription into corresponding functions.</p>
<p>Knowledge inference from data.The objective of knowledge inference form data is to harness the powerful reasoning skills of LLMs to identify relevant features by analysing the given data.Given their impressive ability to solve mathematical problems and identify patterns, we conjecture that LLMs have the capacity to discern common patterns within groups of molecules based on their scientific understanding.To validate this hypothesis, we provide LLMs with an instruction and several batches of sampled instances with their corresponding class labels or instance property values.In the instruction, the LLM is tasked with analysing patterns from provided data to identify features that effectively discriminate between two classes of instances or predict their property values.As a result, LLMs will come up with rules distilled from the analysis for each batch.Because the generated rules in different batches may contain duplicates, we ultimately employ the LLMs' summarization capability to condense the rules and eliminate duplicates, resulting in the final list of features.</p>
<p>Model training.In this stage, all the features identified in the first two stages are transcribed into corresponding functions.All these functions take a scientific instance as input-for example, a SMILES string for molecules-and return a feature value.Consequently, the final representation of an instance resides in an r-dimensional space, where r is the number of features that have been identified.</p>
<p>These vector representations function as the feature vectors for the model training.Employing interpretable models like a linear layer or random forest enables quantification of each rule's importance in prediction, thus elucidating their contribution to the model's final decision.This transparency fosters an intuitive comprehension of the decision-making process, enhancing trust and usability among domain experts.</p>
<p>Interpretable insights.Following interpretable model training, our pipeline delivers clear insights, including prediction results, vector representations of molecules, rules and their corresponding importance scores.This information helps users identify key factors influencing the prediction, thereby improving interpretability.</p>
<p>Metrics</p>
<p>We assessed LLM4SD across 58 molecular property prediction tasks spanning four domains, utilizing distinct evaluation metrics tailored to each task's nature.For the domains of physiology and biophysics, the AUC-ROC metric was employed.AUC-ROC measures the ability of the model to distinguish between classes, with a range from 0 to 1, where a higher value indicates better performance.In the domain of physical chemistry, the RMSE was used.RMSE quantifies the difference between predicted and observed values, with a lower value indicating a closer fit to the true data.For quantum mechanics, we utilized the MAE metric.MAE measures the average magnitude of errors between predicted and true values, with smaller values denoting better accuracy.</p>
<p>Related work</p>
<p>Molecular property prediction.</p>
<p>Research in molecular property prediction has explored a range of computational methods.Preliminary attempts use extended connectivity fingerprint 22 to encode molecular characteristics for traditional machine learning methods such as random forest 40 .Transitioning into neural network approaches, GROVER 20 , GraphCL 15 and AttrMask 14 employ pretraining on large-scale unlabelled molecular graphs and then fine-tune for property prediction.Specifically, GROVER 20 focuses on contextual predictions of an atom's surroundings and predicts graph-level motifs, whereas GraphCL 15 uses a contrastive learning approach to maximize the similarity between Article https://doi.org/10.1038/s42256-025-00994-zdifferent augmented views of a molecule.AttrMask masks and predicts the attributes of nodes, such as atom type.</p>
<p>Recent advancements have introduced more sophisticated frameworks.MolCLR 16 , for instance, extends contrastive learning with three molecular graph augmentations, atom masking, bond deletion and subgraph removal.Meanwhile, GraphMVP 18 and 3Dinfomax 17 enhance contrastive learning by incorporating 3D information about the molecules, contrasting two-dimensional and 3D views of molecules.UniMol 21 pioneers a universal 3D molecular representation learning method, which pretrains with 3D position recovery and masked atom prediction.Additionally, MolBERT 19 employs a vector quantized variational autoencoders framework to encode atoms into chemically meaningful discrete codes and trains through masked atom prediction.</p>
<p>Although previous methods like GNN that encode information into vectors are effective, they often lack interpretability.This makes them less useful for generating clear hypotheses, as their interpretative mechanisms, like attention, offer only a vague understanding.In contrast, our LLM4SD method produces comprehensible rules akin to human analytical processes, offering clearer and more actionable insights.Additionally, previous models cannot effectively harness prior knowledge of chemistry, which remains a challenging task.LLMs pretrained on vast amounts of knowledge, including chemistry, possess the potential to act as human domain-specific experts.Despite their promise, the application of LLMs in scientific discovery is still underexplored.Our proposed LLM4SD method leverages LLMs to drive scientific discovery in molecular property prediction, demonstrating improvements by synthesizing prior knowledge and inferring principles from data.</p>
<p>Experimental settings</p>
<p>The experimental setting consists of splitting datasets and evaluation.Splitting datasets.We followed the MolCLR setup by using the code from MolCLR's GitHub repository to partition the dataset into an 80/10/10 split for training, validation and test sets.Specifically, for the physiology, biophysics and physical chemistry tasks, we employed a scaffold split for molecular compounds, whereas for quantum mechanics tasks, we used a random split.To ensure a fair comparison across all baselines, we reran all baseline methods using the exact same data split (that is, all models share the same SMILES-label pairs in the train, validation and test sets).This guarantees consistency in the training, validation and test sets across all baselines.</p>
<p>Evaluation.After constructing the dataset, molecules were converted into numerical features using the LLM4SD framework, leveraging four distinct LLM backbones: Falcon-7b, Falcon-40b, Galactica-6.7band Galactica-30b.For each backbone, two separate sets of inferred rules were generated, corresponding to 30 and 50 iterations of data sampling, respectively.These numerical features served as input for training a random-forest model.To optimize the model's performance, a grid search was performed to identify the best hyperparameters.The optimized random-forest model was then employed to predict molecular properties on the test set.Each prediction was repeated ten times, and the final test results reflect the backbone and sampling times combination that achieved the best performance on the validation set.</p>
<p>Reporting summary</p>
<p>Further information on research design is available in the Nature Portfolio Reporting Summary linked to this article.</p>
<p>a</p>
<p>Knowledge</p>
<p>Fig. 1 |
1
Fig. 1 | LLMs for scientific discovery in molecular prediction pipeline.a, Knowledge synthesis from the literature.In this phase, LLMs synthesized knowledge based on their pretrained literature for tasks like predicting BBBP.For example, molecules with a molecular weight under 500 Da are more likely to pass through the BBB.b, Knowledge inference from data.Here, LLMs analyse data, such as SMILES strings with labels (1 for BBB permeable, 0 for non-BBB permeable), to identify patterns.For instance, they may observe that molecules containing halogens have a higher chance of crossing the BBB.c, Model</p>
<p>Fig. 2 |
2
Fig. 2 | Comparison between LLM4SD and baselines across four domains.The red dashed line represents the average performance of all baselines.a, Comparative analysis of model performance versus baselines in physiology and biophysics.b, Comparative analysis of regression performance: LLM4SD versus baselines in quantum mechanics and physical chemistry.</p>
<p>4 F
4
a l c o n -7 b F a l c o n -4 0 b G a l a c t i c a -6 .7 b G a l a c t i c a -3</p>
<p>Extended Data Fig. 1 |
1
Detailed performance comparison between 'LLM4SD' and nine baselines in the physiology domain.The red dashed line shows the average result across all methods.Each marker's error bar denotes the method's standard deviation, which is obtained via 10 runs.LLM4SD outperformed other models in 3 out of 4 datasets using the AUC-ROC metric and consistently surpassing the average across all datasets.The results for Tox21 and SIDER are average scores from 12 and 27 tasks respectively (see Extended Data Figs. 2 and 3 for detailed breakdown).Extended Data Fig. 2 | Detailed performance comparison between 'LLM4SD' and nine baselines on Tox21 Dataset.The red dashed line shows the average result across all methods.Each marker's error bar denotes the method's standard deviation, which is obtained via 10 runs.LLM4SD ranks among the top three methods in 8 out of 12 tasks and consistently outperformed the average in all tasks.Extended Data Fig. 4 | Detailed performance comparison between 'LLM4SD' and nine baselines in the biophysics domain.The red dashed line shows the average result across all methods, in terms of AUC-ROC.Each marker's error bar denotes the method's standard deviation, which is obtained via 10 runs.LLM4SD outperformed the top-performing baseline by roughly 1% on the HIV dataset and closely matched the best performing method, UniMol.In both cases, LLM4SD delivered a visibly superior outcome compared to the average performance.Extended Data Fig. 5 | Detailed performance comparison between 'LLM4SD' and nine baselines in the physical chemistry domain.The red dashed line shows the average result across all methods.The physical chemistry domain encompasses three datasets: the ESOL dataset with 1,128 instances, the FreeSolv dataset with 642 instances, and the Lipophilicity dataset comprising 4,200 compounds.Each marker's error bar represents the method's standard deviation, calculated based on 10 independent runs (n=10).These data points are overlaid on the plot in grey colour.LLM4SD substantially outperformed all baseline methods on ESOL, demonstrating a 57% improvement over the average outcome for that dataset, and achieved state-of-the-art results on the additional datasets, FreeSolv and Lipophilicity.Extended Data Fig. 6 | Detailed performance comparison between 'LLM4SD' and nine baselines in the quantum mechanics domain.The red dashed line shows the average result across all methods.The quantum mechanics domain includes the QM9 datasets with 12 subtasks, comprising 133,885 instances.Each marker's error bar denotes the method's standard deviation, which is obtained via 10 runs (n=10).These data points are overlaid on the plot in grey colour.LLM4SD excelled in predicting properties such as U0, U, H, and G, showing substantial enhancements.In other tasks, the results from LLM4SD were comparable to the average of all methods.</p>
<p>Extended Data Table 1 | General prompt for Knowledge Synthesis from the Scientific Literature and Knowledge Inference from Data https</p>
<p>://doi.org/10.1038/s42256-025-00994-z</p>
<p>Extended Data Table 2 | Classification task descriptions for the general prompt in Extended Data Table 1 Extended Data Table 3 | Regression task descriptions for the general prompt in Extended Data Table 1</p>
<p>Scientific productivity is facing a notable decline, with progress in many fields roughly halving every 13 years 1 . As scientific discovery becomes increasingly complex and challenging, traditional methodologies struggle to keep pace, necessitating innovative approaches. Scientific discovery relies on building on existing knowledge to analyse experimental data, recognize data patterns and formulate well-reasoned hypotheses2 . This process requires two essential abilities: prior knowledge understanding and reasoning abilities. Large language models
Extended Data Fig.3| Detailed performance comparison between 'LLM4SD' and nine baselines on Sider Dataset. The red dashed line shows the average result across all methods. Each marker's error bar denotes the method's standard deviation, which is obtained via 10 runs. LLM4SD ranks among the top three methods in 22 out of 27 tasks, and consistently outperforms the average in all tasks with the exception of the 'Psychiatric disorders' task.
Acknowledgements H.Y.K.'s scholarship is supported by the Australian Government Research Training Programme (RTP) Scholarship and Monash University as a cocontribution to Australian Research Council grant no.ARC DP210100072.L.T.M.'s, G.I.W.'s and A.T.N.N.'s research into AI applications for drug discovery is supported by a National Health and Medical Research Council (NHMRC) of Australia Ideas grant (grant no.APP2013629).L.T.M.'s research is also supported by the National Heart Foundation of Australia (grant no.101857).L.T.M.'s and A.T.N.N.'s research is also funded by the NHMRC of Australia and the Department of Health and Aged Care through the Medical Research Future Fund (MRFF) Stem Cell Therapies Mission (grant no.MRF2015957). Computational resources were generously provided by the Nectar Research Cloud, a collaborative Australian research platform supported by the NCRIS-funded Australian Research Data Commons (ARDC) and the MASSIVE HPC facility.We also gratefully acknowledge the support of the Griffith University eResearch Service &amp; Specialized Platforms Team and the use of the High-Performance Computing Cluster 'Gowonda'.S.P. is supported by ARC Future Fellowship (grant no.FT210100097) and ARC grant no.DP240101547.Data availabilityThe split datasets utilized in this study are entirely open source and have been made publicly available to ensure straightforward replication of our findings.We have provided presplit datasets for a variety of tasks, including BBBP, ClinTox, Tox21 (12 subtasks), SIDER (27 subtasks), HIV, BACE, ESOL, Lipophilicity, FreeSolv and QM9 (12 subtasks).These datasets are divided into training, validation and test sets based on our experimental settings.You can access them at the following GitHub repository: https://github.com/zyzisastudyreallyhardguy/LLM4SD/tree/main/scaffold_datasets.For the original raw datasets provided by MoleculeNet 9 , the corresponding links are as follows: BBBP, https:// deepchemdata.s3-us-west-1.amazonaws.com/datasets/BBBP.csv;Clin-Tox, https://deepchemdata.s3-us-west-1.amazonaws.com/datasets/clintox.csv.gz;Tox21, https://deepchemdata.s3-us-west-1.amazonaws.com/datasets/tox21.csv.gz;SIDER, https://deepchemdata.s3-us-west-1.amazonaws.com/datasets/sider.csv.gz;HIV, https://deepchemdata. s3-us-west-1.amazonaws.com/datasets/HIV.csv;BACE, https://deepchemdata.s3-us-west-1.amazonaws.com/datasets/bace.csv;ESOL, https://deepchemdata.s3-us-west-1.amazonaws.com/datasets/delaney-processed.csv;FreeSolv, https://deepchemdata.s3-us-west-1.amazonaws.com/datasets/SAMPL.csv; Lipophilicity, https://deepchemdata.s3-us-west-1.amazonaws.com/datasets/Lipophilicity.csv;QM9, https://deepchemdata.s3-us-west-1.amazonaws.com/datasets/qm9.csv.Source data are provided with this paper.Code availabilityIn our commitment to transparency and reproducibility, we have released our code showing our implementation.This encompasses methodologies for literature knowledge mining, knowledge inference rule mining and interpretable model training.Throughout this work, we have employed several open-source libraries, including Hugging Face, numpy, rdkit, pytorch, scipy, bitsandbytes and accelerate.The GitHub link of the model is https://github.com/zyzisastudyreallyhardguy/LLM4SD (https://doi.org/10.5281/zenodo.13986921)43.Furthermore, we are in the process of deploying a website to facilitate scientists in utilizing LLM4SD.The site features three core functionalities for scientific users: knowledge synthesis, knowledge inference and prediction with explanations.Examples of user interactions with the website can be found in the Supplementary Information.Author contributionsS.P. and G.I.W. supervised the project.Y.Z., H.Y.K. and J.J. contributed to the conception and design of the work.Y.Z., H.Y.K. and J.J. contributed to the technical implementation.Y.Z., H.Y.K. and J.J. prepared the figures.Y.Z.contributed to the design of the web-based application.A.T.N.N. and L.T.M. provided domain expertise for the literature review and validation of rules.Y.Z., H.Y.K., A.T.N.N. and L.T.M. contributed to the design of the rule validation test.All authors edited and revised the manuscript.Competing interestsThe authors declare no competing interests.Additional informationExtended data is available for this paper at https://doi.org/10.1038/s42256-025-00994-z.Supplementary informationThe online version contains supplementary material available at https://doi.org/10.1038/s42256-025-00994-z.Articlehttps://doi.org/10.1038/s42256-025-00994-zPublisher's note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.Springer Nature or its licensor (e.g. a society or other partner) holds exclusive rights to this article under a publishing agreement with the author(s) or other rightsholder(s); author self-archiving of the accepted manuscript version of this article is solely governed by the terms of such publishing agreement and applicable law.
Are ideas getting harder to find?. N Bloom, C I Jones, J Reenen, M Webb, Am. Econ. Rev. 1102020</p>
<p>Scientific discovery in the age of artificial intelligence. H Wang, Nature. 6202023</p>
<p>Baby steps in evaluating the capacities of large language models. M Frank, Nat. Rev. Psychol. 22023</p>
<p>Language models are few-shot learners. T Brown, Adv. Neural Inf. Process Syst. 332020</p>
<p>Gpt-4 technical report. J Achiam, 2023Preprint at</p>
<p>Health system-scale language models are all-purpose prediction engines. L Y Jiang, Nature. 6192023</p>
<p>Large language models encode clinical knowledge. K Singhal, Nature. 6202023</p>
<p>Are large language models superhuman chemists?. A Mirza, 2024</p>
<p>MoleculeNet: a benchmark for molecular machine learning. Z Wu, Chem. Sci. 92018</p>
<p>Galactica: a large language model for science. R Taylor, 2022</p>
<p>The Falcon series of open language models. E Almazrouei, 2023Preprint at</p>
<p>Experimental and computational approaches to estimate solubility and permeability in drug discovery and development settings. C A Lipinski, F Lombardo, B W Dominy, P J Feeney, Adv. Drug Deliv. Rev. 642012</p>
<p>G Landrum, 10.5281/zenodo.14779836rdkit/rdkit: 2024_09_5 (Q3 2024) Release. Release_2024_09_5. 2025</p>
<p>10.1038/s42256-025-00994-zArticle. </p>
<p>Strategies for pre-training graph neural networks. W Hu, Proc. International Conference on Learning Representations. International Conference on Learning Representations2020</p>
<p>Graph contrastive learning with augmentations. Y You, Adv. Neural Inf. Process. Sys. 332020</p>
<p>Molecular contrastive learning of representations via graph neural networks. Y Wang, J Wang, Z Cao, A B Farimani, Nat. Mach. Intell. 42022</p>
<p>3D Infomax improves GNNs for molecular property prediction. H Stärk, Proc. International Conference on Machine Learning. K Chaudhuri, International Conference on Machine LearningPMLR2022</p>
<p>Pre-training molecular graph representation with 3D geometry. S Liu, Proc. 10th International Conference on Learning Representations. 10th International Conference on Learning Representations2022</p>
<p>Mole-bert: rethinking pre-training graph neural networks for molecules. J Xia, Proc. 11th International Conference on Learning Representations. 11th International Conference on Learning Representations2023</p>
<p>Self-supervised graph transformer on large-scale molecular data. Y Rong, Adv. Neural Inf. Process. Sys. 332020</p>
<p>Uni-mol: a universal 3D molecular representation learning framework. G Zhou, Proc. 11th International Conference on Learning Representations. 11th International Conference on Learning Representations2023</p>
<p>Extended-connectivity fingerprints. D Rogers, M Hahn, J. Chem. Inf. Model. 502010</p>
<p>A deep learning approach to antibiotic discovery. J M Stokes, Cell. 1802020</p>
<p>Discovery of a structural class of antibiotics with explainable deep learning. F Wong, Nature. 6262024</p>
<p>Chemllm: a chemical large language model. D Zhang, 2024Preprint at</p>
<p>ChemDFM: a large language foundation model for chemistry. Z Zhao, 38th Conference on Neural Information Processing Systems, Foundation Models for Science: Progress, Opportunities, and Challenges. NeurIPS2024</p>
<p>. Z Cai, 2024Internlm2 technical report. Preprint at</p>
<p>Llama: open and efficient foundation language models. H Touvron, 2023Preprint at</p>
<p>Exploring ChatGPT and its impact on society. M Haque, S Li, 10.1007/s43681-024-00435-4AI Ethics. 2024</p>
<p>Emergent abilities of large language models. J Wei, Transact. Mach. Learn. Res. 2022</p>
<p>P E Mcknight, J Najab, The Corsini Encyclopedia of Psychology. I B Weiner, W E Craighead, Wiley2010</p>
<p>Computational modeling of β-secretase 1 (BACE-1) inhibitors using ligand based approaches. G Subramanian, B Ramsundar, V Pande, R A Denny, J. Chem. Inf. Model. 562016</p>
<p>Defining desirable central nervous system drug space through the alignment of molecular properties, in vitro adme, and safety attributes. T Wager, ACS Chem. Neurosci. 12010</p>
<p>Moving beyond rules: the development of a central nervous system multiparameter optimization (cns mpo) approach to enable alignment of druglike properties. T Wager, X Hou, P Verhoest, A Villalobos, ACS Chem. Neurosci. 12010</p>
<p>Molecular determinants of blood-brain barrier permeation. W Geldenhuys, A Mohammad, C Adkins, P Lockman, Ther. Deliv. 62015</p>
<p>Lost in the middle: how language models use long contexts. N F Liu, Trans. Assoc. Comput. Linguist. 122024</p>
<p>The NLP task effectiveness of long-range transformers. G Qin, Y Feng, B Van Durme, Proc. 17th Conference of the European Chapter. A Vlachos, I Augenstein, 17th Conference of the European ChapterACL2023</p>
<p>UniProt: a worldwide hub of protein knowledge. The UniProt Consortium. 201947</p>
<p>. D A Benson, Nucleic Acids Res. 412013</p>
<p>Random forests. L Breiman, Mach. Learn. 452001</p>
<p>Can chatgpt be used to generate scientific hypotheses?. Y J Park, J. Materiomics. 102024</p>
<p>Smiles transformer: pre-trained molecular fingerprint for low data drug discovery. S Honda, S Shi, H R Ueda, 2019Preprint at</p>
<p>Code repository LLM4SD: release v. J Ju, 10.5281/zenodo.139869212024</p>
<p>The probable error of a mean. Student, Biometrika. 61908</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1153 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1153</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1153</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-25.html">extraction-schema-25</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <p><strong>Paper ID:</strong> paper-259502013</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2303.11582v4.pdf" target="_blank">Adaptive Experimentation at Scale: A Computational Framework for Flexible Batches</a></p>
                <p><strong>Paper Abstract:</strong> Standard bandit algorithms that assume continual reallocation of measurement effort are challenging to implement due to delayed feedback and infrastructural/organizational difficulties. Motivated by practical instances involving a handful of reallocation epochs in which outcomes are measured in batches, we develop a computation-driven adaptive experimentation framework that can flexibly handle batching. Our main observation is that normal approximations, which are universal in statistical inference, can also guide the design of adaptive algorithms. By deriving a Gaussian sequential experiment, we formulate a dynamic program that can leverage prior information on average rewards. Instead of the typical theory-driven paradigm, we leverage computational tools and empirical benchmarking for algorithm development. In particular, our empirical analysis highlights a simple yet effective algorithm, Residual Horizon Optimization, which iteratively solves a planning problem using stochastic gradient descent. Our approach significantly improves statistical power over standard methods, even when compared to Bayesian bandit algorithms (e.g., Thompson sampling) that require full distributional knowledge of individual rewards. Overall, we expand the scope of adaptive experimentation to settings that are difficult for standard methods, involving limited adaptivity, low signal-to-noise ratio, and unknown reward distributions.</p>
                <p><strong>Cost:</strong> 0.026</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1153.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1153.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RHO</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Residual Horizon Optimization</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A model-predictive-control style Bayesian batch adaptive-experimentation policy that at each epoch solves an open-loop planning problem (constant allocation over remaining horizon) under a Gaussian sequential experiment approximation using stochastic gradient optimization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Residual Horizon Optimization (RHO)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>An adaptive experimentation agent that maintains Gaussian posterior means and variances over arm average rewards and at each reallocation epoch solves a stochastic optimization (open-loop planning/MPC) over a constant allocation for the remaining horizon; the planning objective is the expected terminal posterior max mean (Bayes simple regret minimization). It uses the Gaussian sequential experiment approximation, Monte Carlo sampling (Sobol/QMC), and gradient-based optimization (softmax param, Adam) to compute sampling probabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>Bayesian adaptive experimental design / model predictive control (open-loop planning) with Gaussian-sequential approximation</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>At each epoch, RHO uses current posterior (µ,σ) to solve a planning optimization that assumes a constant allocation for all remaining batches (equivalently accumulates remaining budget), maximizing expected terminal max posterior mean; it updates posteriors after observing batch aggregate rewards and repeats (iterative MPC). The optimization is performed via pathwise Monte Carlo gradients (reparameterization) and stochastic gradient ascent.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Finite K-armed batched bandit (Beta-Bernoulli and Gamma-Gumbel experiments in paper)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Unknown arm means drawn from a prior; stochastic, batched feedback (few reallocation epochs), possibly non-Gaussian per-unit rewards (Bernoulli, Gumbel), low signal-to-noise regimes, unknown/partially known measurement variance, limited adaptivity (T small), no contextual information.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>Discrete action space of size K (experiments use K={10,100}); horizon T up to 10 reallocation epochs; per-epoch batch sizes considered bt*n ∈ {100, 10,000}; state is K-dimensional posterior means and variances.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>Quantitative summary from paper: RHO achieved best Bayes simple-regret performance in 493 out of 640 benchmark instances (77.0%). Empirically outperforms uniform allocation, successive elimination and several Gaussian-batch versions of Thompson-sampling variants across a wide range of settings (small/large batches, high/low noise, K up to 100). (Metric: Bayes simple regret; plots report relative gains vs uniform.)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td>Baseline uniform allocation and successive elimination have higher Bayes simple regret across many tested instances; exact numeric baselines vary by instance and are reported in the paper's empirical plots and tables (no single scalar given).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Empirically sample-efficient: yields substantial reductions in Bayes simple regret even for small per-epoch batch sizes (bt*n = 100) and large K, leveraging adaptive reallocation in a few epochs; the paper reports consistent gains across batch sizes and noise regimes but does not give a single sample-count threshold.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Calibrates exploration based on residual horizon: for long remaining horizon it assigns more exploratory mass to many arms (aggressive exploration early); as horizon shortens it concentrates on promising arms (exploitation). This calibration emerges from optimizing terminal expected max posterior mean under Gaussian-sequential model.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>Uniform allocation, Successive Elimination, Gaussian-limit Thompson Sampling, Gaussian-limit Top-Two Thompson Sampling, Myopic (one-step KG-like), TS+ (policy iteration on TS), Policy Gradient (PG) variants.</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>RHO, a simple MPC-style policy derived from Gaussian sequential approximations, consistently outperforms competing batch policies across diverse settings (best in 77% of 640 instances). It handles flexible batch sizes, low SNR, unknown reward distributions, and few reallocation epochs; planning via pathwise gradients is tractable and effective. RHO provably dominates any policy that chooses future allocations only based on current information (it outperforms any open-loop constant plan and the uniform allocation).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Requires solving an optimization per epoch (computational cost), though authors propose distilled offline variants; relies on Gaussian CLT approximation (though empirically robust even for small batches), and theoretical convergence rates are slower (O(n^-1/6)) without overlap assumptions; performance guarantee compares favorably to uniform but not a full optimality proof for all settings. No hard failure cases reported, but complex training/optimization methods (policy gradient) can outperform RHO on small-K high-noise instances.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1153.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1153.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Gaussian-TS</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Gaussian Limit Thompson Sampling</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A Thompson-sampling adaptation that operates on the Gaussian sequential experiment: sampling from the Gaussian posterior on arm average rewards induced by CLT approximations of batch means.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Gaussian Limit Thompson Sampling</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A Bayesian sampling agent that maintains a Gaussian conjugate posterior over the per-arm average reward (using CLT-based Gaussian likelihood for batch means) and at each decision epoch samples from this Gaussian posterior to form a sampling probability (approximate one-hot argmax expectation or softmax surrogate is used for differentiability).</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>Thompson sampling adapted to Gaussian-sequential batch observations</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>Updates Gaussian posterior (mean and variance) after each batch; selects arms by sampling from the posterior over arm means and allocating probability mass according to sampled argmax (or softmax surrogate) for the next batch.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Finite K-armed batched bandit (Beta-Bernoulli and Gamma-Gumbel experiments, evaluated in pre-limit using actual batch sample means)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Unknown arm means drawn from a prior; batched stochastic feedback; possibly non-Gaussian per-unit rewards, limited reallocation epochs, variable measurement noise.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>K up to 100, T up to 10, batch sizes bt*n considered 100 and 10,000.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>Per paper: Gaussian-TS is effective when measurement noise is low; it is outperformed by RHO in many hard (low SNR, few epochs) instances. No single scalar performance given; included in empirical benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Reasonable in moderate-noise regimes; less effective in underpowered/high-noise settings according to experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Implicit Bayesian exploration via posterior sampling (Thompson sampling) which tends to allocate more to uncertain arms but may concentrate effort as posterior concentrates.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>RHO, Top-Two TS, TS+, Myopic, Policy Gradient, Uniform, Successive Elimination</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>Gaussian-limit TS (using conjugate Gaussian approximations) is a natural baseline; performs well in low-noise settings but is outperformed by RHO in many realistic settings with limited adaptivity or low SNR. The paper also uses a differentiable softmax surrogate for TS for use in policy iteration.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Tends to oversample the apparent best arm in low-uncertainty regimes, which can be suboptimal for best-arm identification under small budgets; suffers in high-noise or very underpowered settings.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1153.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1153.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TopTwo-TS</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Gaussian Limit Top-Two Thompson Sampling</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A batched adaptation of Top-Two Thompson Sampling applied to the Gaussian sequential experiment; mixes between top two sampled arms to improve exploration for best-arm identification.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Gaussian Limit Top-Two Thompson Sampling</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>An adaptation of Top-Two Thompson Sampling to the Gaussian-sequential model that uses the Gaussian posterior to implement top-two sampling heuristics in batched settings (allocations derived from posterior probabilities or softmax surrogates).</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>Top-two Thompson sampling (Bayesian pure-exploration variant) under Gaussian sequential approximation</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>At each epoch uses posterior to identify the top and second-best candidates (via posterior draws) and allocates sampling mass to them, promoting exploration among the leading arms.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Finite K-armed batched bandit (same experimental setups)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Unknown arm means, batched stochastic feedback, possibly heavy-tailed or discrete per-unit distributions, limited epochs.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>K up to 100, T up to 10, batch sizes considered 100 and 10,000.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>Included as a baseline; can outperform plain TS in some cases but overall RHO achieved better performance across the benchmark suite. No single numeric aggregate aside from inclusion in empirical plots.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Improved exploration compared to plain TS in some regimes, but not as consistently effective as RHO across the many tested instances.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Explicitly biases exploration toward the second-best alternative to avoid premature exploitation of a possibly suboptimal best-arm estimate.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>RHO, Gaussian-TS, Myopic, TS+, Policy Gradient, Uniform</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>Top-Two TS is a sensible Bayesian heuristic for best-arm identification; in the Gaussian-batch experiments it helps avoid over-concentration but is still outperformed by RHO in many low-SNR/few-epoch settings.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Requires distributional model (handled here via Gaussian-sequential approximation); performance sensitive to noise level and batch structure.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1153.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1153.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Myopic</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Myopic (one-step lookahead / Knowledge Gradient-like)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A randomized one-step lookahead policy (a randomized Knowledge Gradient variant) that maximizes expected immediate improvement in the terminal metric assuming no future adaptivity.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Myopic (one-step KG-like)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Computes allocations to maximize one-step expected improvement (or one-step expected terminal max posterior mean) ignoring future information; implemented as a randomized one-step knowledge-gradient style heuristic in the Gaussian sequential model.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>One-step lookahead (Knowledge Gradient style) Bayesian design</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>At each epoch chooses allocation maximizing immediate expected gain in terminal objective under the Gaussian likelihood; updates posterior and repeats.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Finite K-armed batched bandit</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Unknown arm means, batched observations, stochastic rewards, limited number of reallocation epochs.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>K up to 100, T up to 10, batch sizes 100 or 10,000.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>Per paper: performs well for very short horizons (small T) where one-step lookahead is near-optimal, but worsens for longer horizons due to insufficient exploration; included in benchmarks but generally outperformed by RHO in longer-horizon experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Good in short-horizon tasks (quickly improves terminal metric using local information) but less sample-efficient in longer-horizon/underpowered problems.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Implicitly trades off exploration/exploitation via one-step value of information; tends to exploit in longer horizons since it does not plan for future learning.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>RHO, Gaussian-TS, Top-Two TS, TS+, Policy Gradient, Uniform</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>Myopic is competitive for very short horizons but fails to schedule exploration appropriately when additional adaptivity would be beneficial; demonstrates value of non-myopic planning.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Insufficient exploration for longer horizons; performance degrades as horizon or number of arms grows.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1153.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1153.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TS+</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>TS+ (Policy Iteration on Approximate Thompson Sampling)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A pathwise policy-iteration (rollout) method that uses a differentiable soft surrogate of Thompson sampling as a base policy and performs gradient-based optimization of a single-step Q-function (pathwise gradients) to improve allocations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>TS+ (Policy-iteration on approximate TS)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Starts from an approximate (differentiable) Thompson-sampling policy (softmax surrogate) and applies pathwise policy iteration: Monte Carlo sample Q-function values and optimize allocation via autodiff and stochastic gradient ascent to obtain an improved policy.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>Policy iteration / rollout with pathwise gradients using Gaussian sequential experiment</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>At each epoch, computes Qπ(ρ) under the base policy π (approximate TS) by simulating sample paths and then optimizes the current allocation ρ to maximize this Q via auto-differentiation; future allocations then follow the base policy.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Finite K-armed batched bandit (benchmarks in paper)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Unknown arm means, batched feedback, stochastic, limited reallocation epochs.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>K up to 100, T up to 10, batch sizes 100 and 10,000.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>Improves upon plain Thompson sampling in some regimes (notably where TS is unstable), but in general shows optimization/training instability and is outperformed by RHO in many settings according to empirical results.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Potentially sample-efficient when optimization/training succeeds, but training/optimization instabilities can reduce practical efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Improved over base TS by optimizing the immediate Q-function; still relies on base TS behavior for future allocations.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>RHO, Gaussian-TS, Top-Two TS, Myopic, Policy Gradient, Uniform</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>Policy-iteration on approximate TS can yield improvements but is sensitive to optimization instability and generally underperforms RHO across the benchmark suite.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Optimization/training instabilities; sensitive to choice of differentiable surrogate and gradient-based optimization failures.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1153.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e1153.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PolicyGradient</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Pathwise Policy Gradient (PG)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A parametric policy (neural network) trained by pathwise stochastic gradients of the Gaussian-sequential value using reparameterization (auto-differentiation) to directly optimize Bayes simple regret.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Policy Gradient (PG / PG-m variants)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A differentiable parameterized policy (feedforward NN with softmax output over allocations) trained by direct pathwise gradients of the sample-path value (terminal max posterior mean) under the Gaussian sequential experiment; m-lookahead variants optimize shorter-horizon objectives to mitigate vanishing gradients.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>Policy-gradient reinforcement learning applied to the Gaussian-sequential MDP</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>Uses simulated Gaussian-sequential trajectories (reparameterized) to compute exact pathwise gradients of the terminal objective with respect to policy parameters and applies stochastic gradient ascent (Adam). m-lookahead versions optimize truncated horizons to reduce vanishing gradients.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Finite K-armed batched bandit (benchmarks in paper)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Unknown arm means, batched stochastic feedback, variable noise, limited epochs, large K possible.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>NN input includes K posterior means and variances plus epoch and measurement variance; experiments used K up to 100 and T up to 10; network has 2 hidden layers of 512 units in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>Policy gradient matches or beats RHO in a subset of instances (notably small K = 10 and high measurement noise); overall PG was best in 76 of 640 instances (≈11.8%) whereas RHO was best in 493 instances (77%).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Training is computationally intensive; PG can be sample-efficient after training on simulated priors but suffers from vanishing gradients for long horizons making learning difficult.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Learned implicitly via optimizing terminal objective; m-lookahead addresses credit assignment by shortening horizon used during training.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>RHO, Gaussian-TS, Top-Two TS, TS+, Myopic, Uniform</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>Policy-gradient methods can (after training) match RHO in some regimes (small K, high noise) but are unstable and generally underperform RHO on large-K/low-noise/long-horizon settings; training difficulties (vanishing gradients) limit practical effectiveness.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Vanishing gradients for long-horizon optimization, instability in training, poor scaling to large K or long residual horizons without careful design (PG-m ameliorates but does not fully resolve).</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1153.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e1153.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Uniform</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Uniform allocation (Randomized design / A/B testing)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Non-adaptive baseline that assigns samples uniformly across arms in every batch (standard randomized design).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Uniform allocation</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A static experimental design that divides sampling budget equally among all arms each batch; serves as a baseline for comparison in all experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>None (non-adaptive randomized design)</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>No adaptation; fixed uniform sampling probabilities independent of observed data.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Same finite K-armed batched bandit experiments</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Unknown arm means, batched stochastic feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>K up to 100, fixed per-batch allocations.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td>Baseline performance used to compute relative gains; RHO and other adaptive methods report relative Bayes simple-regret improvements over uniform (figures and tables in paper).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Low in regimes with many arms or low SNR compared to adaptive methods.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>None (pure exploration but inefficient for best-arm identification).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>RHO, Gaussian-TS, Top-Two TS, Myopic, TS+, Policy Gradient, Successive Elimination</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>Uniform is a strong simple baseline but RHO guarantees to outperform any open-loop constant allocation including uniform; empirical results show substantial relative gains of RHO over uniform in many settings.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Inefficient when arm gaps are small or number of arms is large; cannot adapt to observed information.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1153.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e1153.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SuccElim</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Successive Elimination</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A standard batched sequential-elimination algorithm that repeatedly eliminates arms whose confidence intervals are dominated by others and reallocates uniformly among remaining arms.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Successive Elimination (batched)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>At each batch, assigns samples uniformly to non-eliminated arms, computes confidence intervals using empirical means and an uncertainty width β_a(n_a) calibrated via grid search, and eliminates arms whose UCB < some other arm's LCB; repeats until end.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>Sequential elimination (frequentist adaptive design via confidence intervals)</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>Uses confidence intervals derived from empirical means and known/estimated variances to eliminate arms and reduce the action set; remaining budget allocated uniformly among survivors.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Finite K-armed batched bandit (used as a non-Bayesian adaptive baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Unknown reward distributions (paper uses it especially when reward model unknown, e.g. Gumbel), batched feedback, limited epochs.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>K up to 100; performance depends on batch size and confidence tuning constants.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>Per paper: Successive elimination struggles particularly when batch sizes are small (bt*n = 100) and in low-sample regimes; RHO outperforms successive elimination across many tested instances.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Elimination can be sample-inefficient when batches are small and confidence intervals are wide; performance sensitive to tuning parameters.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Eliminate arms aggressively when confident, otherwise explore survivors uniformly; risk of premature elimination when underpowered.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>RHO, Uniform, Gaussian-TS, Top-Two TS, Myopic, Policy Gradient</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>Sequential elimination is a useful classical baseline but is outperformed by RHO in many realistic batched settings, especially small-batch/high-noise cases.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Performs poorly with small batch sizes and underpowered experiments; requires tuning of confidence parameters and assumes some model knowledge for interval calibration.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Paradoxes in learning and the marginal value of information <em>(Rating: 2)</em></li>
                <li>Batched bandit problems <em>(Rating: 2)</em></li>
                <li>Batched thompson sampling <em>(Rating: 2)</em></li>
                <li>Asymptotic representations for sequential decisions, adaptive experiments, and batched bandits <em>(Rating: 2)</em></li>
                <li>Inference for batched bandits <em>(Rating: 1)</em></li>
                <li>A Knowledge-Gradient policy for sequential information collection <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1153",
    "paper_id": "paper-259502013",
    "extraction_schema_id": "extraction-schema-25",
    "extracted_data": [
        {
            "name_short": "RHO",
            "name_full": "Residual Horizon Optimization",
            "brief_description": "A model-predictive-control style Bayesian batch adaptive-experimentation policy that at each epoch solves an open-loop planning problem (constant allocation over remaining horizon) under a Gaussian sequential experiment approximation using stochastic gradient optimization.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Residual Horizon Optimization (RHO)",
            "agent_description": "An adaptive experimentation agent that maintains Gaussian posterior means and variances over arm average rewards and at each reallocation epoch solves a stochastic optimization (open-loop planning/MPC) over a constant allocation for the remaining horizon; the planning objective is the expected terminal posterior max mean (Bayes simple regret minimization). It uses the Gaussian sequential experiment approximation, Monte Carlo sampling (Sobol/QMC), and gradient-based optimization (softmax param, Adam) to compute sampling probabilities.",
            "adaptive_design_method": "Bayesian adaptive experimental design / model predictive control (open-loop planning) with Gaussian-sequential approximation",
            "adaptation_strategy_description": "At each epoch, RHO uses current posterior (µ,σ) to solve a planning optimization that assumes a constant allocation for all remaining batches (equivalently accumulates remaining budget), maximizing expected terminal max posterior mean; it updates posteriors after observing batch aggregate rewards and repeats (iterative MPC). The optimization is performed via pathwise Monte Carlo gradients (reparameterization) and stochastic gradient ascent.",
            "environment_name": "Finite K-armed batched bandit (Beta-Bernoulli and Gamma-Gumbel experiments in paper)",
            "environment_characteristics": "Unknown arm means drawn from a prior; stochastic, batched feedback (few reallocation epochs), possibly non-Gaussian per-unit rewards (Bernoulli, Gumbel), low signal-to-noise regimes, unknown/partially known measurement variance, limited adaptivity (T small), no contextual information.",
            "environment_complexity": "Discrete action space of size K (experiments use K={10,100}); horizon T up to 10 reallocation epochs; per-epoch batch sizes considered bt*n ∈ {100, 10,000}; state is K-dimensional posterior means and variances.",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "Quantitative summary from paper: RHO achieved best Bayes simple-regret performance in 493 out of 640 benchmark instances (77.0%). Empirically outperforms uniform allocation, successive elimination and several Gaussian-batch versions of Thompson-sampling variants across a wide range of settings (small/large batches, high/low noise, K up to 100). (Metric: Bayes simple regret; plots report relative gains vs uniform.)",
            "performance_without_adaptation": "Baseline uniform allocation and successive elimination have higher Bayes simple regret across many tested instances; exact numeric baselines vary by instance and are reported in the paper's empirical plots and tables (no single scalar given).",
            "sample_efficiency": "Empirically sample-efficient: yields substantial reductions in Bayes simple regret even for small per-epoch batch sizes (bt*n = 100) and large K, leveraging adaptive reallocation in a few epochs; the paper reports consistent gains across batch sizes and noise regimes but does not give a single sample-count threshold.",
            "exploration_exploitation_tradeoff": "Calibrates exploration based on residual horizon: for long remaining horizon it assigns more exploratory mass to many arms (aggressive exploration early); as horizon shortens it concentrates on promising arms (exploitation). This calibration emerges from optimizing terminal expected max posterior mean under Gaussian-sequential model.",
            "comparison_methods": "Uniform allocation, Successive Elimination, Gaussian-limit Thompson Sampling, Gaussian-limit Top-Two Thompson Sampling, Myopic (one-step KG-like), TS+ (policy iteration on TS), Policy Gradient (PG) variants.",
            "key_results": "RHO, a simple MPC-style policy derived from Gaussian sequential approximations, consistently outperforms competing batch policies across diverse settings (best in 77% of 640 instances). It handles flexible batch sizes, low SNR, unknown reward distributions, and few reallocation epochs; planning via pathwise gradients is tractable and effective. RHO provably dominates any policy that chooses future allocations only based on current information (it outperforms any open-loop constant plan and the uniform allocation).",
            "limitations_or_failures": "Requires solving an optimization per epoch (computational cost), though authors propose distilled offline variants; relies on Gaussian CLT approximation (though empirically robust even for small batches), and theoretical convergence rates are slower (O(n^-1/6)) without overlap assumptions; performance guarantee compares favorably to uniform but not a full optimality proof for all settings. No hard failure cases reported, but complex training/optimization methods (policy gradient) can outperform RHO on small-K high-noise instances.",
            "uuid": "e1153.0"
        },
        {
            "name_short": "Gaussian-TS",
            "name_full": "Gaussian Limit Thompson Sampling",
            "brief_description": "A Thompson-sampling adaptation that operates on the Gaussian sequential experiment: sampling from the Gaussian posterior on arm average rewards induced by CLT approximations of batch means.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Gaussian Limit Thompson Sampling",
            "agent_description": "A Bayesian sampling agent that maintains a Gaussian conjugate posterior over the per-arm average reward (using CLT-based Gaussian likelihood for batch means) and at each decision epoch samples from this Gaussian posterior to form a sampling probability (approximate one-hot argmax expectation or softmax surrogate is used for differentiability).",
            "adaptive_design_method": "Thompson sampling adapted to Gaussian-sequential batch observations",
            "adaptation_strategy_description": "Updates Gaussian posterior (mean and variance) after each batch; selects arms by sampling from the posterior over arm means and allocating probability mass according to sampled argmax (or softmax surrogate) for the next batch.",
            "environment_name": "Finite K-armed batched bandit (Beta-Bernoulli and Gamma-Gumbel experiments, evaluated in pre-limit using actual batch sample means)",
            "environment_characteristics": "Unknown arm means drawn from a prior; batched stochastic feedback; possibly non-Gaussian per-unit rewards, limited reallocation epochs, variable measurement noise.",
            "environment_complexity": "K up to 100, T up to 10, batch sizes bt*n considered 100 and 10,000.",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "Per paper: Gaussian-TS is effective when measurement noise is low; it is outperformed by RHO in many hard (low SNR, few epochs) instances. No single scalar performance given; included in empirical benchmarks.",
            "performance_without_adaptation": null,
            "sample_efficiency": "Reasonable in moderate-noise regimes; less effective in underpowered/high-noise settings according to experiments.",
            "exploration_exploitation_tradeoff": "Implicit Bayesian exploration via posterior sampling (Thompson sampling) which tends to allocate more to uncertain arms but may concentrate effort as posterior concentrates.",
            "comparison_methods": "RHO, Top-Two TS, TS+, Myopic, Policy Gradient, Uniform, Successive Elimination",
            "key_results": "Gaussian-limit TS (using conjugate Gaussian approximations) is a natural baseline; performs well in low-noise settings but is outperformed by RHO in many realistic settings with limited adaptivity or low SNR. The paper also uses a differentiable softmax surrogate for TS for use in policy iteration.",
            "limitations_or_failures": "Tends to oversample the apparent best arm in low-uncertainty regimes, which can be suboptimal for best-arm identification under small budgets; suffers in high-noise or very underpowered settings.",
            "uuid": "e1153.1"
        },
        {
            "name_short": "TopTwo-TS",
            "name_full": "Gaussian Limit Top-Two Thompson Sampling",
            "brief_description": "A batched adaptation of Top-Two Thompson Sampling applied to the Gaussian sequential experiment; mixes between top two sampled arms to improve exploration for best-arm identification.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Gaussian Limit Top-Two Thompson Sampling",
            "agent_description": "An adaptation of Top-Two Thompson Sampling to the Gaussian-sequential model that uses the Gaussian posterior to implement top-two sampling heuristics in batched settings (allocations derived from posterior probabilities or softmax surrogates).",
            "adaptive_design_method": "Top-two Thompson sampling (Bayesian pure-exploration variant) under Gaussian sequential approximation",
            "adaptation_strategy_description": "At each epoch uses posterior to identify the top and second-best candidates (via posterior draws) and allocates sampling mass to them, promoting exploration among the leading arms.",
            "environment_name": "Finite K-armed batched bandit (same experimental setups)",
            "environment_characteristics": "Unknown arm means, batched stochastic feedback, possibly heavy-tailed or discrete per-unit distributions, limited epochs.",
            "environment_complexity": "K up to 100, T up to 10, batch sizes considered 100 and 10,000.",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "Included as a baseline; can outperform plain TS in some cases but overall RHO achieved better performance across the benchmark suite. No single numeric aggregate aside from inclusion in empirical plots.",
            "performance_without_adaptation": null,
            "sample_efficiency": "Improved exploration compared to plain TS in some regimes, but not as consistently effective as RHO across the many tested instances.",
            "exploration_exploitation_tradeoff": "Explicitly biases exploration toward the second-best alternative to avoid premature exploitation of a possibly suboptimal best-arm estimate.",
            "comparison_methods": "RHO, Gaussian-TS, Myopic, TS+, Policy Gradient, Uniform",
            "key_results": "Top-Two TS is a sensible Bayesian heuristic for best-arm identification; in the Gaussian-batch experiments it helps avoid over-concentration but is still outperformed by RHO in many low-SNR/few-epoch settings.",
            "limitations_or_failures": "Requires distributional model (handled here via Gaussian-sequential approximation); performance sensitive to noise level and batch structure.",
            "uuid": "e1153.2"
        },
        {
            "name_short": "Myopic",
            "name_full": "Myopic (one-step lookahead / Knowledge Gradient-like)",
            "brief_description": "A randomized one-step lookahead policy (a randomized Knowledge Gradient variant) that maximizes expected immediate improvement in the terminal metric assuming no future adaptivity.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Myopic (one-step KG-like)",
            "agent_description": "Computes allocations to maximize one-step expected improvement (or one-step expected terminal max posterior mean) ignoring future information; implemented as a randomized one-step knowledge-gradient style heuristic in the Gaussian sequential model.",
            "adaptive_design_method": "One-step lookahead (Knowledge Gradient style) Bayesian design",
            "adaptation_strategy_description": "At each epoch chooses allocation maximizing immediate expected gain in terminal objective under the Gaussian likelihood; updates posterior and repeats.",
            "environment_name": "Finite K-armed batched bandit",
            "environment_characteristics": "Unknown arm means, batched observations, stochastic rewards, limited number of reallocation epochs.",
            "environment_complexity": "K up to 100, T up to 10, batch sizes 100 or 10,000.",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "Per paper: performs well for very short horizons (small T) where one-step lookahead is near-optimal, but worsens for longer horizons due to insufficient exploration; included in benchmarks but generally outperformed by RHO in longer-horizon experiments.",
            "performance_without_adaptation": null,
            "sample_efficiency": "Good in short-horizon tasks (quickly improves terminal metric using local information) but less sample-efficient in longer-horizon/underpowered problems.",
            "exploration_exploitation_tradeoff": "Implicitly trades off exploration/exploitation via one-step value of information; tends to exploit in longer horizons since it does not plan for future learning.",
            "comparison_methods": "RHO, Gaussian-TS, Top-Two TS, TS+, Policy Gradient, Uniform",
            "key_results": "Myopic is competitive for very short horizons but fails to schedule exploration appropriately when additional adaptivity would be beneficial; demonstrates value of non-myopic planning.",
            "limitations_or_failures": "Insufficient exploration for longer horizons; performance degrades as horizon or number of arms grows.",
            "uuid": "e1153.3"
        },
        {
            "name_short": "TS+",
            "name_full": "TS+ (Policy Iteration on Approximate Thompson Sampling)",
            "brief_description": "A pathwise policy-iteration (rollout) method that uses a differentiable soft surrogate of Thompson sampling as a base policy and performs gradient-based optimization of a single-step Q-function (pathwise gradients) to improve allocations.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "TS+ (Policy-iteration on approximate TS)",
            "agent_description": "Starts from an approximate (differentiable) Thompson-sampling policy (softmax surrogate) and applies pathwise policy iteration: Monte Carlo sample Q-function values and optimize allocation via autodiff and stochastic gradient ascent to obtain an improved policy.",
            "adaptive_design_method": "Policy iteration / rollout with pathwise gradients using Gaussian sequential experiment",
            "adaptation_strategy_description": "At each epoch, computes Qπ(ρ) under the base policy π (approximate TS) by simulating sample paths and then optimizes the current allocation ρ to maximize this Q via auto-differentiation; future allocations then follow the base policy.",
            "environment_name": "Finite K-armed batched bandit (benchmarks in paper)",
            "environment_characteristics": "Unknown arm means, batched feedback, stochastic, limited reallocation epochs.",
            "environment_complexity": "K up to 100, T up to 10, batch sizes 100 and 10,000.",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "Improves upon plain Thompson sampling in some regimes (notably where TS is unstable), but in general shows optimization/training instability and is outperformed by RHO in many settings according to empirical results.",
            "performance_without_adaptation": null,
            "sample_efficiency": "Potentially sample-efficient when optimization/training succeeds, but training/optimization instabilities can reduce practical efficiency.",
            "exploration_exploitation_tradeoff": "Improved over base TS by optimizing the immediate Q-function; still relies on base TS behavior for future allocations.",
            "comparison_methods": "RHO, Gaussian-TS, Top-Two TS, Myopic, Policy Gradient, Uniform",
            "key_results": "Policy-iteration on approximate TS can yield improvements but is sensitive to optimization instability and generally underperforms RHO across the benchmark suite.",
            "limitations_or_failures": "Optimization/training instabilities; sensitive to choice of differentiable surrogate and gradient-based optimization failures.",
            "uuid": "e1153.4"
        },
        {
            "name_short": "PolicyGradient",
            "name_full": "Pathwise Policy Gradient (PG)",
            "brief_description": "A parametric policy (neural network) trained by pathwise stochastic gradients of the Gaussian-sequential value using reparameterization (auto-differentiation) to directly optimize Bayes simple regret.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Policy Gradient (PG / PG-m variants)",
            "agent_description": "A differentiable parameterized policy (feedforward NN with softmax output over allocations) trained by direct pathwise gradients of the sample-path value (terminal max posterior mean) under the Gaussian sequential experiment; m-lookahead variants optimize shorter-horizon objectives to mitigate vanishing gradients.",
            "adaptive_design_method": "Policy-gradient reinforcement learning applied to the Gaussian-sequential MDP",
            "adaptation_strategy_description": "Uses simulated Gaussian-sequential trajectories (reparameterized) to compute exact pathwise gradients of the terminal objective with respect to policy parameters and applies stochastic gradient ascent (Adam). m-lookahead versions optimize truncated horizons to reduce vanishing gradients.",
            "environment_name": "Finite K-armed batched bandit (benchmarks in paper)",
            "environment_characteristics": "Unknown arm means, batched stochastic feedback, variable noise, limited epochs, large K possible.",
            "environment_complexity": "NN input includes K posterior means and variances plus epoch and measurement variance; experiments used K up to 100 and T up to 10; network has 2 hidden layers of 512 units in experiments.",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "Policy gradient matches or beats RHO in a subset of instances (notably small K = 10 and high measurement noise); overall PG was best in 76 of 640 instances (≈11.8%) whereas RHO was best in 493 instances (77%).",
            "performance_without_adaptation": null,
            "sample_efficiency": "Training is computationally intensive; PG can be sample-efficient after training on simulated priors but suffers from vanishing gradients for long horizons making learning difficult.",
            "exploration_exploitation_tradeoff": "Learned implicitly via optimizing terminal objective; m-lookahead addresses credit assignment by shortening horizon used during training.",
            "comparison_methods": "RHO, Gaussian-TS, Top-Two TS, TS+, Myopic, Uniform",
            "key_results": "Policy-gradient methods can (after training) match RHO in some regimes (small K, high noise) but are unstable and generally underperform RHO on large-K/low-noise/long-horizon settings; training difficulties (vanishing gradients) limit practical effectiveness.",
            "limitations_or_failures": "Vanishing gradients for long-horizon optimization, instability in training, poor scaling to large K or long residual horizons without careful design (PG-m ameliorates but does not fully resolve).",
            "uuid": "e1153.5"
        },
        {
            "name_short": "Uniform",
            "name_full": "Uniform allocation (Randomized design / A/B testing)",
            "brief_description": "Non-adaptive baseline that assigns samples uniformly across arms in every batch (standard randomized design).",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Uniform allocation",
            "agent_description": "A static experimental design that divides sampling budget equally among all arms each batch; serves as a baseline for comparison in all experiments.",
            "adaptive_design_method": "None (non-adaptive randomized design)",
            "adaptation_strategy_description": "No adaptation; fixed uniform sampling probabilities independent of observed data.",
            "environment_name": "Same finite K-armed batched bandit experiments",
            "environment_characteristics": "Unknown arm means, batched stochastic feedback.",
            "environment_complexity": "K up to 100, fixed per-batch allocations.",
            "uses_adaptive_design": false,
            "performance_with_adaptation": null,
            "performance_without_adaptation": "Baseline performance used to compute relative gains; RHO and other adaptive methods report relative Bayes simple-regret improvements over uniform (figures and tables in paper).",
            "sample_efficiency": "Low in regimes with many arms or low SNR compared to adaptive methods.",
            "exploration_exploitation_tradeoff": "None (pure exploration but inefficient for best-arm identification).",
            "comparison_methods": "RHO, Gaussian-TS, Top-Two TS, Myopic, TS+, Policy Gradient, Successive Elimination",
            "key_results": "Uniform is a strong simple baseline but RHO guarantees to outperform any open-loop constant allocation including uniform; empirical results show substantial relative gains of RHO over uniform in many settings.",
            "limitations_or_failures": "Inefficient when arm gaps are small or number of arms is large; cannot adapt to observed information.",
            "uuid": "e1153.6"
        },
        {
            "name_short": "SuccElim",
            "name_full": "Successive Elimination",
            "brief_description": "A standard batched sequential-elimination algorithm that repeatedly eliminates arms whose confidence intervals are dominated by others and reallocates uniformly among remaining arms.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Successive Elimination (batched)",
            "agent_description": "At each batch, assigns samples uniformly to non-eliminated arms, computes confidence intervals using empirical means and an uncertainty width β_a(n_a) calibrated via grid search, and eliminates arms whose UCB &lt; some other arm's LCB; repeats until end.",
            "adaptive_design_method": "Sequential elimination (frequentist adaptive design via confidence intervals)",
            "adaptation_strategy_description": "Uses confidence intervals derived from empirical means and known/estimated variances to eliminate arms and reduce the action set; remaining budget allocated uniformly among survivors.",
            "environment_name": "Finite K-armed batched bandit (used as a non-Bayesian adaptive baseline)",
            "environment_characteristics": "Unknown reward distributions (paper uses it especially when reward model unknown, e.g. Gumbel), batched feedback, limited epochs.",
            "environment_complexity": "K up to 100; performance depends on batch size and confidence tuning constants.",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "Per paper: Successive elimination struggles particularly when batch sizes are small (bt*n = 100) and in low-sample regimes; RHO outperforms successive elimination across many tested instances.",
            "performance_without_adaptation": null,
            "sample_efficiency": "Elimination can be sample-inefficient when batches are small and confidence intervals are wide; performance sensitive to tuning parameters.",
            "exploration_exploitation_tradeoff": "Eliminate arms aggressively when confident, otherwise explore survivors uniformly; risk of premature elimination when underpowered.",
            "comparison_methods": "RHO, Uniform, Gaussian-TS, Top-Two TS, Myopic, Policy Gradient",
            "key_results": "Sequential elimination is a useful classical baseline but is outperformed by RHO in many realistic batched settings, especially small-batch/high-noise cases.",
            "limitations_or_failures": "Performs poorly with small batch sizes and underpowered experiments; requires tuning of confidence parameters and assumes some model knowledge for interval calibration.",
            "uuid": "e1153.7"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Paradoxes in learning and the marginal value of information",
            "rating": 2,
            "sanitized_title": "paradoxes_in_learning_and_the_marginal_value_of_information"
        },
        {
            "paper_title": "Batched bandit problems",
            "rating": 2,
            "sanitized_title": "batched_bandit_problems"
        },
        {
            "paper_title": "Batched thompson sampling",
            "rating": 2,
            "sanitized_title": "batched_thompson_sampling"
        },
        {
            "paper_title": "Asymptotic representations for sequential decisions, adaptive experiments, and batched bandits",
            "rating": 2,
            "sanitized_title": "asymptotic_representations_for_sequential_decisions_adaptive_experiments_and_batched_bandits"
        },
        {
            "paper_title": "Inference for batched bandits",
            "rating": 1,
            "sanitized_title": "inference_for_batched_bandits"
        },
        {
            "paper_title": "A Knowledge-Gradient policy for sequential information collection",
            "rating": 1,
            "sanitized_title": "a_knowledgegradient_policy_for_sequential_information_collection"
        }
    ],
    "cost": 0.02558325,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Adaptive Experimentation at Scale: A Computational Framework for Flexible Batches</p>
<p>Ethan Che 
Decision, Risk, and Operations Division
Columbia Business School</p>
<p>Hongseok Namkoong namkoong@gsb.columbia.edu 
Decision, Risk, and Operations Division
Columbia Business School</p>
<p>Adaptive Experimentation at Scale: A Computational Framework for Flexible Batches</p>
<p>Standard bandit algorithms that assume continual reallocation of measurement effort are challenging to implement due to delayed feedback and infrastructural/organizational difficulties. Motivated by practical instances involving a handful of reallocation opportunities in which outcomes are measured in batches, we develop a computation-driven adaptive experimentation framework that can flexibly handle batching. Our main observation is that normal approximations, which are universal in statistical inference, can also guide the design of adaptive algorithms. By deriving a Gaussian sequential experiment, we formulate a dynamic program that can leverage prior information on average rewards. Instead of the typical theory-driven paradigm, we leverage computational tools and empirical benchmarking for algorithm development. Our empirical analysis highlights a simple yet effective algorithm, Residual Horizon Optimization, which iteratively solves a planning problem using stochastic gradient descent. Our approach significantly improves power over standard methods, even when compared to Bayesian algorithms (e.g., Thompson sampling) that require full distributional knowledge of individual rewards. Overall, we expand the scope of adaptive experimentation to settings standard methods struggle with, involving limited adaptivity, low signal-to-noise ratio, and unknown reward distributions.</p>
<p>Introduction</p>
<p>Experimentation is the basis of scientific decision-making for medical treatments, engineering solutions, policy-making, and business products alike. Since experimenting is typically expensive or risky (e.g., clinical trials), the cost of collecting data poses a central operational constraint. Simultaneously, as policy interventions and engineering solutions become more sophisticated, modern experiments increasingly involve many treatment arms. When the differences in average rewards across arms (average treatment effects) are small relative to the sample size, statistical power is of fundamental concern [71, 16,28]. Even for online platforms that can automatically deploy experiments to millions to billions of users, typical A/B tests are underpowered as they involve incremental changes to a product that have a small relative impact on key business metrics such as revenue or user satisfaction [71,72,73]. When there is interference across individuals, treatments may be randomized over entire markets or geographic regions severely limiting statistical power [51].</p>
<p>Adaptive allocation of measurement effort can significantly improve statistical power and allow reliable identification of the optimal decision/treatment. Accordingly, adaptive methods-dubbed pure-exploration multi-armed bandit (MAB) algorithms-have received tremendous attention since the foundational works of Thompson, Chernoff, Robbins, and Lai [106,22,96,75]. Most of these algorithms are specifically designed to enjoy strong theoretical guarantees as the number of reallocation epochs grows to infinity [14,77,104]. However, standard frameworks cannot model typical experimentation paradigms where adaptive reallocation incurs high operational cost. Although a universal assumption in the MAB literature, unit-level continual reallocation of sampling effort is often expensive or infeasible due to organizational cost and delayed feedback. Even in online platforms with advanced experimentation infrastructures, engineering difficulties and lack of organizational incentives deter continual reallocation at the unit level [101,4,10,90].</p>
<p>Due to the challenges associated with reallocating measurement effort, typical real-world adaptive experiments employ a few reallocation epochs in which outcomes are measured for many units in parallel ("batches") [10,91,56,26,65,31]. Motivated by these operational considerations, we develop and analyze batched adaptive experimentation methods tailored to a handful of reallocation opportunities. Our main conceptual contribution is the formulation of a dynamic program that allows designing adaptive methods that are near-optimal for the fixed number of reallocation epochs. Algorithms designed from our framework can flexibly handle any batch size, and are automatically tailored to the instance-specific measurement noise and statistical power. Specifically, we use a normal approximation for aggregate rewards to formulate a Gaussian sequential experiment where each experiment epoch consists of draws from a Gaussian distribution for each arm. The dynamic program solves for the best adaptive allocation where noisy Gaussian draws become more accurate with increased sampling allocation (see Figure 1).</p>
<p>Although we use (frequentist) central limit-based normal approximations to derive the Gaussian sequential experiment, our proposed dynamic programming framework solves for adaptive allocations using Bayesian approaches. Unlike standard Bayesian bandit formulations (e.g., [36,65,98]) that require distributional knowledge of individual rewards, we only use a prior over the average rewards and our likelihood functions over average rewards are Gaussian from the CLT. Our formulation thus allows leveraging prior information on average rewards constructed using the rich reservoir of previous experiments, but retains some of the advantages of model-free frequentist methods. Computationally, sampling allocations derived from our Gaussian sequential experiment can be computed offline after each epoch, in contrast to typical Bayesian bandit algorithms that require real-time posterior inference (e.g., top-two Thompson sampling [98]).</p>
<p>Our formulation provides a computation-driven framework for algorithm design. Despite continuous and high-dimensional state/action spaces, our limiting dynamic program offers a novel computational advantage: the Gaussian sequential experiment provides a smoothed model of partial feedback for which sample paths are fully differentiable with respect to the sampling probabilities chosen by the experimenter. By simulating a trajectories of the experiment, we can calculate per-  formance metrics (e.g., cumulative or simple regret) and compute policy gradients through modern auto-differentiation softwares such as Tensorflow [1] or PyTorch [92]. These gradients can be used to directly optimize a planning problem over allocations, e.g., policy gradient or policy iteration. We demonstrate our framework by providing efficient implementations of approximate dynamic programming (ADP) and reinforcement learning (RL) methods, and evaluating them through empirical benchmarking. Our algorithm development and evaluation approach is in stark contrast to the bandit literature that largely operates on mathematical insight and researcher ingenuity [88].</p>
<p>Our empirical analysis highlights a simple yet effective heuristic, Residual Horizon Optimization, which iteratively solves for the optimal static allocation that only uses currently available information. Similar to model predictive control (MPC), the allocation is used in the corresponding period and newly collected observations are used to update the planning problem in the next period. Through extensive empirical validation, we demonstrate RHO consistently provides major gains in decision-making performance over a rich set of benchmark methods. Out of 640 problem instances we consider-varying across number of reallocation opportunities, number of arms, reward distributions, priors, etc.-RHO achieves the best performance in 493 (77%) of them despite its simplicity. As a summary of our findings, Figure 2 compares the performance of RHO against standard batch policies such as oracle Thompson sampling-based policies that have full distributional knowledge of individual rewards. Despite relying on Gaussian approximations, RHO provides significant power gains in hard instances with high measurement noise. Overall, our approach expands the scope of adaptive experimentation to settings standard adaptive policies struggle with, involving few reallocation epochs, low signal-to-noise ratio, and unknown reward distributions.</p>
<p>Paper outline We begin our discussion by showing that the Gaussian sequential experiment is a valid approximation as the batch size becomes large (Section 2). We study the admissible regime where differences in average rewards scale as 1/ √ n, where n is the batch size at each epoch. The scaling is standard in formalizing statistical power in inferential scenarios [107,78]. By extending this conceptual framework to sequential experiments, we show that normal approximations uni-versal in statistical inference is also useful for designing adaptive experimentation methods. In Section 3, we observe the Gaussian sequential experiment can be represented by a Markov decision process over the experimenter's beliefs on average rewards. At every reallocation epoch, state transitions in the MDP are governed by posterior updates based on new observations. We illustrate our computation-driven paradigm to algorithm development in Section 4 and 5. Our MDP formulation gives rise to several ADP and RL-based policies, which we detail in Section 4.1. Through empirical benchmarking, we find that a simple MPC-style policy, RHO, consistently outperforms even carefully tuned RL methods (Section 4.2). Motivated by its strong empirical performance, we provide basic theoretical insights on RHO in Section 4.3. We show RHO is optimal among those that do not optimize for potential future feedback and in particular, is always better than the uniform allocation-the de facto standard in practice. By noting that RHO can be seen as a dynamic extension of the single-batch problem analyzed by Frazier and Powell [35], we also characterize how RHO calibrates exploration when the remaining sampling budget is large. In Section 5, we perform a thorough empirical comparison with standard batched bandit policies under diverse settings, e.g., low vs high measurement noise, large vs small batches, flat vs concentrated priors. We summarize our empirical benchmarking in the interactive app (18).</p>
<p>We defer a full discussion of the literature to the end of the paper to maximize clarity. Our asymptotic formulation and resulting algorithms are new, but intimately connected to the vast body of work on adaptive allocation of measurement effort. In Section 6, we situate the current paper across several fields, such as pure-exploration MABs, batched MABs, ranking &amp; selection in simulation optimization, diffusion limits for MABs, as well as various works studying Gaussian environments related to the one we outline in Figure 1.</p>
<p>Gaussian Sequential Experiment</p>
<p>Our goal is to select the best treatment arm out of K alternatives, using a small (known) number of reallocation epochs (T ). Each experimentation epoch t = 0, ..., T − 1 involves a batch of b t n samples, where b t &gt; 0 is a fixed and known constant and n is a scaling parameter. The constants b t can be thought of as the length of time between experiment epochs, which may vary across epochs t. Solving for the optimal adaptive allocation is challenging as it involves a dynamic program over a combinatorially large action space that depends on unknown reward distributions. Any solution to such "finite batch" problem will depend heavily on the batch size and need to be re-solved if the batch size changes, that is if more/less samples are collected than expected.</p>
<p>To circumvent such complexity, we formulate an asymptotic approximation of the adaptive experimentation problem using normal approximation of aggregate rewards. The sequential Gaussian experiment we derive in this section provides tractable policies that can deal with any flexible batch size. To derive this CLT-based large batch limit, we must first choose a natural scaling for the average rewards. If the gap in the average reward of each arm is ≫ 1/ √ n, adaptive experimentation is unnecessary as the best arm can be found after only a single epoch. Conversely, if the gaps are ≪ 1/ √ n, then we cannot reliably learn even after many experimentation epochs; one cannot improve upon the uniform allocation (a.k.a. randomized design or A/B testing). We thus focus on the admissible regime where the gap between average rewards are Θ(1/ √ n). Using this scaling for average rewards, we observe i.i.d. rewards for each unit allocated to a treatment arm a
R a = h a √ n + ϵ a ,(1)
where h a is an unknown "local parameter" that determines the difference in average rewards across arms. Without loss of generality, we set the baseline reward to zero; since we assume the noise ϵ a has mean zero, henceforth we abuse notation and refer to h as both the average rewards and the gaps in average rewards. We assume that the variance of the noise Var(ϵ a ) = s 2 a is known and constant. In particular, s 2 a does not scale with n, so it is crucial to sample arms many times to discern differences between their means h a / √ n. Although reward variances are typically unknown, they can be estimated from a small initial batch in practice; empirically, the policies we consider are robust to estimation error in s 2 a and a rough estimate suffices (see Section A.1).</p>
<p>Gaussian sequential experiment as a limit adaptive experiment</p>
<p>Our goal is to use the information collected until the beginning of epoch t to select an adaptive allocation (policy) π t ∈ ∆ K , the fraction of samples allocated to each of the K treatment arms. Let {R t a,j } K a=1 denote the potential rewards for unit j ∈ [b t n] at time t. We use ξ t a,j to denote an indicator for whether arm a was pulled for unit j at time t. Our starting point is the observation that the (scaled) estimator for average rewards converges in distribution from the central limit theorem √ nR n t,a :=
1 b t √ n btn j=1 ξ t a,j R t a,j d ⇝ N π t,a h a , π t,a s 2 a .(2)
From the normal approximation (2), √ nR n t,a /π t,a can be seen as an approximate draw from the distribution N (h a , s 2 a /π t,a ), giving a noisy observation of the average reward h a . The allocation π t controls the effective sample size and the ability to distinguish signal from noise, a.k.a. statistical power.</p>
<p>Using successive normal approximations (2) at each epoch, we arrive at a Gaussian sequential experiment that provides an asymptotic approximation to the adaptive experimentation problem. Definition 1. A Gaussian sequential experiment is characterized by observations G 0 , . . . , G T −1 with conditional distributions
G t |G 0 , . . . , G t−1 ∼ N {π t,a h a } K a=1 , diag π t,a s 2 a b t K a=1
.</p>
<p>In this asymptotic experiment, the experimenter chooses π t at each epoch t and observes an independent Gaussian measurement distributed as N π t,a h a , π t,a s 2 a for each arm a. We use the asymptotic Gaussian sequential experiment as an approximation to the original batched adaptive epochs and derive near-optimal adaptive experimentation methods for this asymptotic problem.</p>
<p>Building on the observation (2), our first theoretical result shows that the Gaussian sequential experiment (Definition 1) provides an accurate approximation in the large batch limit n → ∞. Our limit sequential experiment extends classical local asymptotic normality results in statistics [107,78]. Our result relies on the following basic conditions on the reward and policy π t 's.</p>
<p>Assumption A.</p>
<ol>
<li>Ignorability: rewards are independent from sampling decisions, conditional on past observations:
ξ t+1 j ⊥ R t+1 j |R n 0 , . . . ,R n t .</li>
<li>Moment restriction: there exists C &gt; 0 such that E ∥ϵ∥ 4 2 ≤ C.</li>
</ol>
<p>For the sequential Gaussian experiment to provide a valid approximation of the objective (9), we further need regularity conditions on the policy.</p>
<p>Assumption B. 1. Sufficiency: the policy depends on the data only through aggregate rewards:
π t = π t √ nR n 0 , . . . , √ nR n t−1
for all t = 0, · · · , T − 1 2. Continuity: discontinuity points of π t are measure zero with respect to (G 0 , ..., G t−1 ) for all t Assumption B1 is unavoidable in our setup as we restrict attention to coarser information represented by the aggregate rewards (2). Note however, that we do not require sampling probabilities to be bounded away from zero, as is commonly assumed for inferential procedures.</p>
<p>We are now ready to give our main asymptotic result, which we prove in Section B.1. The key difficulty of this result is to show convergence of the batch sample means to their Gaussian limit under sampling probabilities that are themselves stochastic, as they are selected by the policy which is influenced by previous measurements. The sampling probabilities are arbitrary and are allowed to even be zero, so we require showing uniform convergence across all possible sampling probabilities.</p>
<p>Theorem 1. Let Assumptions A, B hold. Then, the Gaussian sequential experiment in Definition 1 provides a valid asymptotic approximation as n → ∞
( √ nR n 0 , . . . , √ nR n T −1 ) d ⇝ (G 0 , ..., G T −1 ).(3)
Our proof also provides a bound for the rate of convergence in Theorem 1. To metrize weak convergence, consider the bounded Lipschitz distance d BL
d BL (µ, ν) := sup |E R∼µf (R) − E R∼νf (R)| :f ∈ Lip(R K ), sup x,y∈R K |f (x) −f (y)| ≤ 1 .
Since we measure convergence through bounded Lipschitz test functions, we require smoothness of the policy with respect to observations
L := max 0≤t≤T −1 max a max{∥π t,a ∥ Lip , √ π t,a Lip } &lt; ∞.(4)
Corollary 1. Let Assumptions A, B and the bound (4) hold. Let M := 1 +L(s 2 * /b * K 1/2 + max a |h a |) with s 2 * := max a s 2 a , b * := min 0≤t≤T −1 b t . Then,
d BL (( √ nR n 0 , . . . , √ nR n T −1 ), (G 0 , . . . , G T −1 )) ≤ CM T n −1/6 (5) where C ∈ (0, ∞) is a constant that depends polynomially on K, M , h, s 2 * , E ∥ϵ∥ 3 2 , E ∥ϵ∥ 4 2 and b −1 * .
Unlike other asymptotic normality results for adaptive sampling policies, we do not assume anything beyond continuity of the sampling probabilities. In particular, we do not assume that sampling probabilities are lower bounded, as is typically done for proving central limit theorems under adaptive sampling. As a result, we obtain a slower rate of convergence than the standard n −1/2 rate expected for the CLT (e.g., Berry-Esseen bound) as the batch size grows large; a minor modification to our proof gives the usual O(n −1/2 ) rate if the sampling probabilities are bounded from below. We obtain an O(n −1/6 ) rate through a multivariate Stein's method; since we do not assume a lower bound on sampling probabilities the corresponding Stein's operator has weaker (a) Simple regret for Gumbel rewards (Gamma prior) (b) Simple regret for Bernoulli rewards (Beta prior) Figure 3. Histograms of simple regret after T = 10 epochs involving K = 100 arms, across instances h drawn from the prior ν. Each histogram corresponds to a different batch size, with the green histogram corresponding to the Gaussian sequential experiment (n = ∞). Even when the batch size is small (b t n = 100), the performance of the policy in these non-Gaussian environments closely matches the performance of the policy in the Gaussian sequential experiment. smoothing properties than in standard cases and we require an additional Gaussian smoothing argument to apply the result to Lipschitz test functions.</p>
<p>If sampling probabilities are bounded away from zero, we can obtain the usual n −1/2 rate of weak convergence. We do not require that the sampling probabilities are almost surely greater than some threshold, but a milder condition on integrability of the inverse sampling probabilities.
Assumption C. Overlap: there exists a constant C o &gt; 0 such that sup n,t E max a 1 π t,a ( √ nR n 0 , . . . , √ nR n t−1 ) 3 ≤ C o(6)
We prove the following convergence rate in Section B.3.</p>
<p>Corollary 2. Let Assumptions A, B, C and the bound (4) hold. Let M := 1 +L(s 2
* /b * K 1/2 + max a |h a |) with s 2 * := max a s 2 a , b * := min 0≤t≤T −1 b t . Then, d BL (( √ nR n 0 , . . . , √ nR n T −1 ), (G 0 , . . . , G T −1 )) ≤CM T n −1/2 (7) whereC ∈ (0, ∞) depends on K, C o , M , h, s 2 * , E ∥ϵ∥ 3 2 , E ∥ϵ∥ 4 2 , and b −1 * polynomially.
The bounds (5) and (7) suffer an exponential dependence on the time horizon T , which is perhaps unsurprising as the result involves joint weak convergence of correlated random variables. The smoothness of the policy also enters, which suggests that convergence may be slower for policies that are very sensitive to changes in the measurement. Nevertheless, T is very small compared to the batch size for the settings we consider. Empirically, we observe below that the asymptotic limit offer an accurate approximation even when the batch sizes are small.</p>
<p>Validity of normal approximations even for small batch sizes</p>
<p>Empirically, we observe that the Gaussian sequential experiment is a good approximation even when the batch sizes are exceedingly small. While Theorem 1 and Corollary 3 only guarantees validity of our approximation in the large batch limit, this observation impels us to apply policies derived from the problem (11) to problems with small batches. Overall, our observations are consistent with the longstanding folklore in statistics that the central limit theorem often provides a practical approximation even for small samples.</p>
<p>We illustrate that the sequential Gaussian experiment provides an accurate prediction of performance in finite batch systems. Our primary metric of performance is simple regret, the difference in the average reward between the best arm and the arm selected by the policy at the end of experiment (according to a policy denoted as π T ). Since modern experimental platforms typically run many experiments, it is natural for the experimenter to have a prior ν over the average rewards h of the treatment arms, so we focus on simple regret among instances drawn from this prior. For different batch scalings n, we compare the distribution of the simple regret
E a [h a ⋆ − h a ] where a ∼ π T √ nR 0 , . . . , √ nR T −1(8)
over the prior h ∼ ν, to its limiting object E a∼π T (G 0 ,...,
G T −1 ) [h a ⋆ − h a ].
For simplicity, we consider a large batch limit version of the Thompson sampling policy; we observe similar trends for different policies π. As we detail in Section 3, this Bayesian policy maintains a Gaussian belief over the average rewards h and updates posteriors using observed aggregate rewards √ nR n . Figure 3 displays histograms of the simple regret (8) incurred by this policy across experiments with T = 10 reallocation epochs and K = 100 treatment arms. Each histogram depicts the distribution of the simple regret (8) over the prior h ∼ ν for a particular batch scaling n, including under the Gaussian sequential experiment corresponding to n = ∞. Even for exceptionally small batch sizes relative to the number of arms (batch size of b t n = 100 across K = 100 arms), the Bayes simple regret closely matches that predicted by the Gaussian sequential experiment.</p>
<p>Bayesian Adaptive Experimentation</p>
<p>It is natural for the experimenter to have a prior distribution h ∼ ν over the relative gap between average rewards. For example, modern online platforms run thousands of experiments from which a rich reservoir of previously collected data is available [71,72,73]. In this work, we focus on minimizing the Bayes simple regret at the end of the experiment
BSR T (π, ν,R) := E h∼ν E[h a ⋆ − h a ] where a ∼ π T and a ⋆ ∈ argmax a h a ,(9)
the scaled optimality gap between the final selection a and the optimal arm a ⋆ averaged over the prior. The notation BSR T (π, ν,R) considers adaptive policies π = {π t } T t=0 , prior ν over h, and observation processR = (R 0 , . . . ,R T −1 ), which is the set of aggregate rewards used by the policy to determine the sampling allocations.</p>
<p>Instead of optimizing the Bayes simple regret for each finite batch size, we use Theorem 1 to derive an asymptotic approximation under the Gaussian sequential experiment.</p>
<p>Corollary 3. Let ν be a prior over average rewards h satisfying E h∼ν ∥h∥ 1 &lt; ∞ and let Assumptions A, B hold. Then, the Bayes simple regret underR n can be approximated by that under the Gaussian sequential experiment G in Definition 1:
BSR T (π, ν, √ nR n ) → BSR T (π, ν, G).(10)
See Section B.4 for a proof of the corollary. Using the approximation of Bayes simple regret, we optimize the asymptotic Bayesian objective
minimize π {BSR T (π, ν, G) = E h∼ν E[h a ⋆ − h a ]}(11)
over policies π = {π t (G 0 , . . . , G t−1 )} T t=0 adapted to the sequential observations G 0 , . . . , G T −1 of the Gaussian sequential experiment. Policies derived from the optimization problem (11) has marked modeling flexibility and computational advantages compared to typical Bayesian sequential sampling algorithms (e.g., variants of Thompson sampling [98]). Although reward distributions are unknown in general, Bayesian sequential sampling approaches require a distributional model of individual rewards comprising of prior and likelihood function [99]. In contrast, our framework does not assume restrictive distributional assumptions on rewards and allows naturally incorporating prior information over average rewards h a .</p>
<p>Computationally, policies derived from the optimization problem (11) can be efficiently updated offline. As these offline updates give a fixed sampling probability over the K arms at each epoch, the policies we propose in subsequent sections can be deployed to millions of units at ease. This is in stark contrast to Bayesian sequential sampling algorithms designed for unit-level feedback. While their policy updates are also often performed offline in batches in practice [91,31], when deployed these policies require real-time posterior inference and action optimization to generate treatment assignments for each unit. Implementing such methods at scale is highly challenging even for the largest online platforms with mature engineering infrastructure [3,90]. Methods derived from our formulation provides a scalable alternative as deploying the resulting adaptive policies only involves sampling from a fixed sampling probability π t for every unit in a batch, regardless of the batch size.</p>
<p>Markov decision process</p>
<p>Using the Gaussian sequential experiment as an asymptotic approximation, we derive a dynamic program that solves for the adaptive allocation minimizing the Bayes simple regret (11). Policies derived from this dynamic program-which we call the (limiting) Bayesian adaptive experimentare tailored to the signal-to-noise ratio in each problem instance and the number of reallocation opportunities T . In the Bayesian adaptive experiment, we observe a Gaussian draw G t at each epoch and use it to perform posterior updates over the experimenter's belief on the average rewards h. We formulate the sequential updates using a Markov decision process where state transitions are governed by changes in the posterior mean and variance. In Section 4, we introduce a range of adaptive sampling algorithms that (approximately) solve the Bayesian dynamic program, and benchmark them empirically. Our empirical analysis highlights a particularly effective policy, Residual Horizon Optimization, which solves an open-loop problem over future allocations that only use currently available information.</p>
<p>The normal approximations in the previous section gives Gaussian likelihood functions in our Bayesian adaptive experiment
Likelihood function: G | h ∼ N πh, diag πs 2 b ,
where we abuse notation to describe elementwise algebraic operations over K-dimensional vectors.</p>
<p>(Recall that s 2 ∈ R K is the measurement variance on the raw rewards (1).) To achieve conjugacy, we assume that the experimenter has a Gaussian prior over the average reward
Prior: h ∼ N µ 0 , diag(σ 2 0 ) =: ν,(12)
where µ 0 , σ 2 0 ∈ R K are the prior mean and variance. Standard posterior updates for Gaussian conjugate priors give the following recursive formula for the posterior mean and variance Posterior variance: σ −2 t+1,a := σ −2 t,a + s −2 a b t π t,a (13a)</p>
<p>Posterior mean: µ t+1,a := σ 2 t+1,a σ −2 t,a µ t,a + s −2
a b t G t,a .(13b)
The posterior variance decreases as a deterministic function of the sampling allocation π t , and in particular does not depend on the observation G t . Under the posterior updates (13), our goal is to optimize the Bayes simple regret (11) at the end of the experiment (time t = T ). We consider the final allocation that simply selects the arm with the highest posterior mean; formally, if the argmax a µ T,a is unique, this is equivalent to
π T,a = 1 if a = argmax a µ T,a 0 if a ̸ = argmax a µ T,a .
(More generally, we choose an arm randomly from argmax a µ T,a ; this does not change any of the results below.) Then, minimizing the Bayes simple regret under the asymptotic Gaussian sequential experiment (11) is equivalent to the following reward maximization problem
maximize π 0 ,...,π T −1 V π 0 (µ 0 , σ 0 ) = E π max a µ T,a .(14)
Here, we write E π to denote the expectation under the stochastic transitions (13) induced by the policy π 0 , . . . , π T −1 adapted to the observation sequence G 0 , . . . , G T −1 . Although we focus on the Bayes simple regret in this work, our MDP formulation can accomodate any objective function or constraint that depends on the posterior states, such as the cumulative regret, probability of correct selection, simple regret among the top-k arms, and constraints on the sampling allocations.</p>
<p>To simplify things further, we use a change of variable to reparameterize state transitions (13) as a random walk. 
σ −2 t+1,a := σ −2 t,a + s −2 a b t π t,a (µ t , σ t ) (15a) µ t+1,a := µ t,a + σ t,a b t π t,a (µ t , σ t )σ 2 t,a s 2 a + b t π t,a (µ t , σ t )σ 2 t,a Z t,a .(15b)
We defer derivation details to Section C.1. For the Markov decision process defined by the reparameterized state transitions (15), the value function for the Bayes simple regret E π [max a µ T,a ] has the following succinct representation
V π t (µ t , σ t ) = E π max a µ T,a | µ t , σ t = E π max a µ t,a + T −1 v=t σ v,a b v π v,a (µ v , σ v )σ 2 v,a s 2 a + b v π v,a (µ v , σ v )σ 2 v,a Z v,a µ t , σ t .(16)
In what follows, we use the notational shorthand
E π t [·] = E π [· | µ t , σ 2 t ] so that V π t (µ t , σ t ) = E π t [max a µ T,a ].</p>
<p>Asymptotic validity</p>
<p>The state transitions (13) are derived under the idealized the Gaussian sequential experiment. In practice, any policy π derived in this asymptotic regime will be applied to a finite batch problem.</p>
<p>Recalling the asymptotic approximation (3), the finite batch problem will involve states (µ n,t , σ n,t ) updated according to the same dynamics (13), but using the sample mean estimator √ nR n t instead of the Gaussian approximation G t Pre-limit posterior variance: σ −2 n,t+1,a := σ −2 n,t,a + s −2 a b t π t,a (µ n,t , σ n,t ) (17a)</p>
<p>Pre-limit posterior mean: µ n,t+1,a := σ 2 n,t+1,a σ −2 n,t,a µ n,t,a + s −2
a b t √ nR n t,a .(17b)
Given any policy π t (µ t , σ t ) derived from the Bayesian dynamic program (14) and average rewards h, the pre-limit posterior state (µ n,t , σ n,t ) evolves as a Markov chain and each adaptive allocation to be used in the original finite batch problem is given by π t (µ n,t , σ n,t ).</p>
<p>For policies π that is continuous in the states-satisfying conditions in Assumption B-Corollary 3 implies that the Bayesian dynamic program (14) is an accurate approximation for measuring and optimizing the performance of the policy as n grows large. Moreover, we can show that the trajectories of pre-limit posterior beliefs (µ n,t , σ n,t ) (17) can be approximated by trajectories of the Markov decision process derived using the Gaussian sequential experiment (15).</p>
<p>Corollary 4.</p>
<p>Let π t (µ t , σ t ) be a policy derived under the limit Bayesian dynamic program (14) that is continuous as a function of (µ t , σ t ). Let Assumption A hold and consider any fixed average reward h and prior (µ 0 , σ 0 ). The posterior states (17) converge to the states (13) as n → ∞
(µ n,t , σ n,t , . . . , µ n,T −1 , σ n,T −1 ) d ⇝ (µ t , σ t , . . . , µ T −1 , σ T −1 )
See Section C.2 for the proof.</p>
<p>Algorithm design through empirical benchmarking</p>
<p>We now derive algorithms for solving the limit Bayesian dynamic program (14), which arise naturally from reinforcement learning (RL) and approximate dynamic programming (ADP). Breaking from the typical theory-driven paradigm in bandit algorithms that compare regret bounds, we empirically benchmark adaptive experimentation methods to assess their performance. To ensure empirical rigor, we present a comprehensive set of experiments in the following Streamlit app:</p>
<p>https://aes-batch.streamlit.app/.</p>
<p>Our empirical approach allows us to study performance among a large class of algorithms, and analyze factors that impact performance (e.g. measurement noise s 2 a , horizons length T ). Our focus on empirical analysis reveals practical aspects of implementation that are critical for performance, but are difficult to observe from a purely theoretical approach.</p>
<p>Adaptive experimentation as approximate dynamic programming</p>
<p>Since the states (µ t , σ t ) and actions π t are both continuous, using dynamic programming to directly solve the policy optimization problem (14) is computationally intractable even for a moderate number of arms and reallocation epochs. However, a key property of this MDP is that the state transitions are differentiable with respect to the sampling allocations along any sample path Z 0 , ..., Z T −1 , Algorithm 1 Residual Horizon Optimization 1: Input: prior mean and variance on average rewards (µ 0 , σ 0 ) 2: Initialize pre-limit states (µ n,0 , σ n,0 ) = (µ 0 , σ 0 ) 3: for each epoch t ∈ 0, . . . , T − 1 do</p>
<p>4:</p>
<p>Lettingb t := T −1 v=t b v , solve the following (e.g., using stochastic gradient methods)
ρ t (µ t , σ t ) ∈ argmax ρ∈∆ K Vρ t (µ t , σ t ) := E t max a µ t,a + σ 4 t,aρ abt s 2 a + σ 2 t,aρ abt Z t,a(20)</p>
<p>5:</p>
<p>For each unit j = 1, . . . , b t n, sample arms according to ρ t (µ t , σ t ) and observe reward R t a,j if arm a was sampled (ξ t a,j = 1)</p>
<p>6:</p>
<p>Use aggregate rewards √ nR n t,a = 1 bt √ n btn j=1 ξ t a,j R t a,j and the formula (17) to compute the next state transition (µ n,t+1 , σ n,t+1 ) 7: end for 8: return argmax a∈[K] µ n,T,a which is enabled by the reparameterization trick for Gaussian random variables [69]. As a result, standard Monte Carlo approximations of the value function are differentiable with respect to the sampling allocations, allowing the use of gradient-based methods for planning and (approximately) solving the DP. In this section, we explore algorithms that utilize this auto-differentiability [1,92].</p>
<p>Residual Horizon Optimization (RHO)</p>
<p>We propose a simple yet effective method that solves a computationally cheaper approximation of the dynamic program based on model predictive control (MPC). RHO iteratively solves an open-loop planning problem, optimizing over a sequence of future sampling allocations based on currently available information (µ t , σ t ) at time t. At each epoch t, the allocation ρ t (µ t , σ t ) is derived by assuming that a fixed sequence of allocations will be used for the remaining periods horizon regardless of new information obtained in the future
ρ t (µ t , σ t ) ∈ argmax ρt,...,ρ T −1 ∈∆ K E t max a µ t,a + T −1 v=t σ v,a b vρv,a σ 2 v,a s 2 a + b vρv,a σ 2 v,a Z v,a µ t , σ t ,(19)
where ρ t (µ t , σ t ) = ρ * t for the sequence ρ * t , . . . , ρ * T −1 that maximizes the planning objective. The planning problem (19) can be simplified to consider a constant allocationρ ∈ ∆ K to be deployed for every remaining period. Intuitively, since the allocation does not change as the experiment progresses, we can accumulate updates to the posterior beliefs at each epoch and think of it as a single update to the current belief, resulting in the terminal posterior belief. In this sense, the policy can be seen as a dynamic extension of the single-batch problem studied in [35]. We summarize how our procedure will be applied to finite batch problems in Algorithm 1, and formalize this insight in the below result. See Section C.3 for its proof.
Lemma 2. Letb t := T −1 v=t b v .
For any sequence of future allocationsρ t , · · · ,ρ T −1 ∈ ∆ K that only depends on (µ t , σ t ), there is a constant allocationρ(µ t , σ t ) achieving the same Bayes simple regret
Vρ t:T −1 t (µ t , σ t ) = Vρ t (µ t , σ t ) where Vρ t (µ t , σ t ) is defined in Eq. (20).
Thus, it is sufficient to solve the constant allocation planning problem (20) to achieve the same optimal value as the original problem (19).</p>
<p>In the empirical benchmarks to come, we find that among Bayesian policies that utilize batch Gaussian approximations, Residual Horizon Optimization achieves the largest performance gain across a wide variety of settings. When the number of reallocation epochs is small, RHO calibrates the level of exploration to the time horizon by iteratively planning with the Gaussian sequential experiment (20).</p>
<p>Pathwise Policy Iteration</p>
<p>We also consider reinforcement learning/approximate DP methods for solving the full horizon problem (14). Our framework enables the use of policy iteration [105] to improve upon a standard sampling policy, such as Thompson Sampling or Top-Two Thompson Sampling. Policy iteration (also referred to as 'Rollout') takes a base policy π and returns an improved policy π ′ that selects the sampling allocation at each epoch t that maximizes the Qfunction of π, which gives the expected reward of selecting sampling allocationρ at state (µ t , σ t ) and epoch t given that all future sampling allocations are determined by the base policy,
Q π t (ρ; µ t , σ t ) = E t V π t+1 µ t + σ t b tρ σ 2 t s 2 + b tρ σ 2 t Z t , (σ −2 t + s −2 b tρ ) −1 .
Optimizing Q π t would typically require discretizing the state and action spaces or using function approximation for the Q-function. However, since the dynamics of the MDP are differentiable, one can simply draw a Monte Carlo sample of the Q-function and directly compute the gradient with respect toρ through auto-differentiation [1,92], as long as the base policy π is differentiable with respect to the state (µ, σ). One can then use stochastic gradient ascent to optimize the Qfunction. Although differentiability is not guaranteed for all policies, we observe there is often reliable differentiable surrogates. For example, for Thompson Sampling we have the following approximation
π TS (µ, σ) = E onehot(argmax a µ a + σ a Z a ) ≈ E [softmax(µ a + σ a Z a )] =:π TS (µ, σ)
where Z a iid ∼ N (0, 1), onehot maps an index to its corresponding one-hot vector, and softmax(v) := exp(v a )/ a ′ exp(v a ′ ). We define the policy TS+ to be policy iteration on the approximate Thompson sampling policy, in which at every state (µ t , σ t ) the allocation is determined by solving
π TS+ t (µ t , σ t ) ∈ argmax ρ∈∆ K Qπ TS t (ρ; µ t , σ t )(21)
with stochastic gradient ascent.</p>
<p>Pathwise Policy Gradient We apply policy gradient (PG) computed through black-box ML models, as well as their limited lookahead variants [105]. We consider an differentiable parameterized policy π θ = {π θ t } T −1 t=0 (e.g., neural networks) and aim to directly optimize the value function (16) using stochastic approximation methods. We use stochastic gradient ascent over sample paths Z 0 , . . . , Z T −1 to update policy parameters θ:
θ ← θ + α∇ θ V π θ 0 (µ 0 , σ 0 ),(22)
for step-size α and prior (µ 0 , σ 0 ). As for policy iteration, that the gradients of the value function can be computed exactly through auto-differentiation along every sample path. This is in contrast with standard RL methods for continuous state and action spaces, such as Deep Deterministic Policy Gradient (DDPG) [80], that require function approximation for the Q-functions, a step which can introduce significant approximation errors. For long horizons, training a policy with gradient descent is more difficult due to vanishing gradients. Policies are evaluated solely on the simple regret incurred at the end of the experiment, which makes it difficult to do credit assignment for sampling allocations near the beginning of the experiment. For policy gradient, we consider m-lookahead policies (denoted PG-m) trained to optimize the shorter time horizon objective V π θ T −m (µ 0 , σ 0 ).</p>
<p>Empirical comparison of Gaussian batch policies</p>
<p>There are many policies that use the Gaussian sequential experiment (Definition 1) as an approximation for batch rewards. To identify performant policies among them, we turn to empirical benchmarking on realistic examples. In total, we consider 640 problem instances across different number of reallocation opportunities, number of arms, reward distributions, and priors.</p>
<p>Setup Since we are interested in the performance of these policies in the pre-limit problem, we simulate state transitions (17) based on observed aggregate rewards √ nR n t,a . As a concrete illustration of our main findings, consider a setting with K = {10, 100} arms with up to T = 10 batches. All batches have the same number of observations in each epoch and we consider two batch sizes: b t n = 100 samples and b t n = 10, 000 observations per epoch. We evaluate each policy using the Bayes simple regret (9) under the true prior. For policies that use the Gaussian sequential experiment, we train them by approximating the true prior with a Gaussian prior with the same mean and standard deviation in order to preserve conjugacy and utilize the MDP formulation (15). All policies are evaluated according to the pre-limit finite batch problem under the true prior.</p>
<p>We consider two data-generating distributions. We use the Beta-Bernoulli experiment as our primary vehicle, where rewards for each arm are drawn from independent Bernoulli(θ a ) distributions. Here, the parameters θ a are drawn independently from a known Beta(α, β) prior. For each batch size b t n = 100 or 10, 000, we scale the prior parameters α = β = b t n to preserve the difficulty. Concretely, when b t n = 100, the prior mean for each parameter θ a is 0.5 and the prior standard deviation is ≈ 0.03, so typically each treatment arm has an average reward of 0.5 ± 0.03. We focus on small differences in average rewards as this is often the case in real-world experiments [71,72,73].</p>
<p>Since we cannot vary the measurement noise s 2 a = Var(ϵ a ) (1) under the Beta-Bernoulli experiment, we also consider an alternative setting which we call the Gamma-Gumbel experiment. Here, rewards for each arm are drawn from independent Gumbel(µ a , β) distributions, with a known, fixed scale parameter β that determines the measurement variance s 2 a = π 2 6 β. The location parameters µ a are drawn independently from a known Gamma(α, β) prior, which determines the differences in the average reward between the arms. For each batch size b t n, we set α = b t n and β = 1/b t n to preserve the difficulty of the problem as measured by the gap between average rewards.</p>
<p>Algorithms In addition to the policies mentioned in Section 3.1 that use the dynamic programming structure of the model, we also implement policies such as Gaussian Thompson Sampling which are natural adaptations of standard bandit policies to the Gaussian sequential experiment. We summarize the the list of methods we compare below.</p>
<p>• Residual Horizon Optimization: Solves the planning problem (20) Table 1. Comparison of empirical performance of Gaussian batch policies. Simple regret of various policies as a percentage of the simple regret of the uniform allocation policy (lower is better). Baseline column displays results for the Gamma-Gumbel experiment with K = 10 arms, batch size b t n = 100, s 2 = 1 with a flat prior. The other columns display results for alternative settings, where one aspect of the baseline setting is changed (e.g. K = 100 arms instead of K = 10 arms) with all else remaining the same. Out of 640 problem instances we study, RHO achieves the best performance in 493 (77.0%) settings and policy gradient in 76 (11.8%) settings, among a selection of 10 policies including Gaussian policies in Section 4 and standard batch experimentation policies in Section 5.</p>
<p>• Gaussian Limit Thompson Sampling: TS policy for the Gaussian sequential experiment</p>
<p>• Gaussian Limit Top-Two Thompson Sampling: Top-Two TS [98] for the Gaussian sequential experiment.</p>
<p>• Myopic: Maximizes one-step lookahead value function for the problem (14); a randomized version of the Knowledge Gradient method</p>
<p>• TS+: Policy iteration on the approximate TS policy (21)</p>
<p>• Policy Gradient: Heuristically solves the dynamic program (14) using a policy parameterized by a feed-forward neural network; trained through policy gradient (22) with episode length 5</p>
<p>Overview of empirical results We refer the reader to the interactive plot (18) for a comprehensive presentation on our benchmarking results. In Figure 4, we provide representative results where we consider K = 100 arms and batches of size b t n = 10, 000. Although all of these policies use Gaussian batch approximations, we evaluate them in a pre-limit problem with Gumbel distributed rewards and a Gamma prior on the mean rewards, which we refer to as the Gamma-Gumbel experiment. Figure 4a focuses on a fixed measurement variance (s 2 a = 1) across reallocation epochs, whereas Figure 4b considers different measurement noises for a fixed number of reallocations T = 10. Finally, Table 1 presents ablation studies over different reward distributions, number of arms, batch size, measurement noise, and prior specifications.</p>
<p>RHO exhibits consistently strong performance across a wide array of settings; other policies are effective in some instances and less effective in others. For example, the myopic policy performs well for short horizon experiments as expected, but worsens in longer horizon settings due to insufficient exploration. Thompson sampling, which tends to explore more than other policies, is an effective heuristic when the noise level is low, but suffers in more underpowered experiments. The policy gradient method achieves equivalent performance to RHO when there are a small number of alternatives (K = 10) and measurement noise is high. TS+ improves upon Thompson Sampling  under similar conditions, but in general suffers from instabilities related to optimization/training. Compared to RHO, policy gradient/iteration methods (heuristically) solve more sophisticated planning problems accounting for future adaptivity. However, we observe empirically that they perform worse in settings with more arms, lower measurement noise, and longer horizon experiments.</p>
<p>Discussion of Residual Horizon Optimization</p>
<p>The strong empirical performance of Residual Horizon Optimization impels us to carefully study the advantages of solving the planning problem (20). In this subsection, we show that RHO enjoys several desirable properties. First and foremost, implementing this policy only requires solving the above optimization problem (20) at the currently observed state, allowing it to remain adaptive while being computationally efficient; we use stochastic gradient methods for this purpose. Second, the objective (20) encourages the policy to explore more when the remaining horizon is long, which leads it to explore aggressively early on in the experiment while focusing on strong alternatives as the experiment winds down (see Figure 5). At each epoch RHO iteratively computes the optimal allocation among those that assume future allocations will only use currently available information. It is thus guaranteed to outperform any open-loop allocation including static designs; in particular, RHO achieves smaller Bayes simple regret than the uniform allocation. This gives a practical performance guarantee even for small T , the regime which is relevant for most real-world experimental settings, for which the uniform allocation is a highly competitive policy. Proposition 2. Let ρ t be given by Algorithm 1. For any policyπ t:T = (π t (µ t , σ t ), . . . ,π T −1 (µ t , σ t )) that only depends on currently available information
(µ t , σ t ), we have V ρ t (µ t , σ t ) ≥ Vπ t (µ t , σ t ).
See Section C.4 for the proof. In cases where it is computationally expensive to compute the allocation ρ t (µ t , σ t ) as the experiment progresses, one can learn the policy offline via standard ML methodologies (see Section C.5). Although we are primarily interested in short horizons T in practice, we can theoretically characterize the behavior of RHO in the infinite horizon limit T − t → ∞. We show that the optimization problem (20) becomes strongly concave for a large enough residual horizon T − t, which was observed for the single-batch problem in [35]. Then, by using the KKT conditions of the optimization problem (20), we also characterize the asymptotic sampling policy ρ t converges to as T − t grows large. We find that it converges to a novel posterior sampling policy we denote as Density Thompson Sampling (DTS), as it samples arms proportionally to the partial derivatives of the Gaussian Thompson Sampling policy. , where c depends only on (µ, σ).</p>
<p>There exists
T 0 such that ∀T − t &gt; T 0 ,ρ →b t Vρ t (µ, σ) is strongly concave on ∆ ϵ K .
3. Suppose there exists T 1 such that for ∀T − t &gt; T 1 , the RHO allocation satisfies ρ t,a (µ, σ) &gt; ϵ.
Then as T − t → ∞ ρ t,a (µ, σ) → π DTS a (µ, σ), where π DTS a (µ, σ) ∝ s a ∂ ∂µ a π TS a (µ, σ) 1/2
and π TS a (µ, σ) := P(a * = a|µ, σ) is the Thompson Sampling probability.</p>
<p>See Section D.1 for the proof. Theorem 3 shows that the planning problem becomes more amenable to optimization as the residual horizon increases, although the gradient of the objective decays at rate 1/b t as T − t → ∞. Moreover, although we would expect the open-loop planning problem to be better calibrated under short time horizons, even in the infinite horizon limit it gives rise to a natural, stationary posterior sampling policy, establishing a novel connection between posterior sampling and model predictive control. The limiting policy DTS is highly exploring and does not over-sample the best arm, which  makes it better suited for best-arm identification tasks than standard Gaussian Thompson Sampling (see Section D.2 for more details).</p>
<p>In contrast, we empirically observe that the more sophisticated approaches , i.e. policy gradient, suffer optimization difficulties due to vanishing gradients. This is alluded to in Theorem 3: the gradient of the static allocation value function scales as (s 2 a /b t ) and thus vanishes under small measurement variance and long residual horizon.</p>
<p>Comparison with standard bandit approaches</p>
<p>Our empirical and theoretical analysis in the previous section shows that RHO is an exceedingly simple yet effective heuristic. Impelled by these benefits, we now provide a careful empirical comparison between RHO and other standard multi-armed bandit algorithms. Overall, we find that although RHO relies on Gaussian approximations for the rewards and the prior distribution, performance gains from planning with the Gaussian sequential experiment greatly outweigh approximation errors. In total, we find that across 640 settings, RHO outperforms Gaussian policies discussed in Section 4 and standard batch bandit policies in 493 (77.0%) of them. Our empirical results suggest that RHO particularly excels in settings that are difficult for standard adaptive policies, including those a limited number of reallocation epochs, unknown reward distributions, low signal-to-noise ratios, and a large number of treatment arms. Its versatile performance across different horizon and noise levels shows the utility of calibrating exploration using the Gaussian sequential experiment. In addition to the key dimensions we highlight in this section, we provide a comprehensive set of experiments in the Streamlit app (18).</p>
<p>Algorithms To benchmark performance, we consider a suite of standard batch bandit policies proposed in the literature.</p>
<p>• Uniform: For each batch, assign samples uniformly across treatment arms. • Successive Elimination [32,38,93]: For each batch, assign samples uniformly across treatment arms that are not eliminated. Eliminate all arms whose upper confidence bound is less than the lower confidence bound of some other arm. Given K arms, n a samples drawn for arm a, measurement variance s 2 a , the confidence interval isμ a ±β a (n a ), whereμ a is the empirical mean reward of arm a and the width of the confidence bound β a (n a ) is
β a (n a ) = cs a log(n 2 a K/δ) n a ,
where c, δ are constants that are chosen via grid search to minimize regret in each instance. For the Gamma-Gumbel experiment, the Gumbel distribution does not have a known conjugate prior distribution and updating the posterior after each batch is more involved. For this reason, when in this environment we restrict our focus to successive elimination as the main baseline. This captures settings in which the reward model is unknown to the experimenter, where it is difficult to use Thompson sampling or other Bayesian policies that require the exact reward distribution.</p>
<p>Number of alternative K (treatment arms) In order to study how performance changes with the number of alternatives, we fix the batch size to be b t n = 100 and evaluate our policies for K = 10 and K = 100 alternatives. Figure 6 shows that RHO achieves strong performance for large and small number of alternatives alike, and the performance gains are larger when there is a large number of arms. In these experiments and others presented in the interactive app (18), we observe that the performance gains of RHO persist across all time horizons, particularly for short horizons.</p>
<p>Measurement variance s 2 a Next, we study how the signal-to-noise of the problem instance affects the performance of adaptive policies. To study a scenario where we can flexibly control  different measurement variances s 2 a = Varϵ a (1), we move away from the Beta-Bernoulli experiment and instead consider the Gamma-Gumbel experiment. In Figure 7, we observe that RHO outperforms uniform allocation and successive elimination in both high and low signal-to-noise regimes. When the signal-to-noise ratio is high (i.e., measurement noise s 2 a is small), RHO is able to rapidly hone in on the highest reward treatment arms. Even when the signal-to-noise is low, RHO is able to make substantial progress over uniform while other adaptive policies struggle to learn enough to improve sampling efficiency.</p>
<p>Batch size b t n In Figure 2 we presented in Section 1, we compared the regret incurred by different policies for large and small batches in the Beta-Bernoulli experiment. We use the Gamma-Gumbel experiment as an additional test of whether the Gaussian approximations hold for batched rewards. Although the Gumbel distribution has a high excess kurtosis, we still observe that the Gaussian sequential experiment serves as a useful approximation even for small batches. In Figure 8, we see that RHO greatly outperforms uniform allocation and successive elimination for small and large batches alike. In particular, successive elimination especially struggles when the batch size is small (b t n = 100).</p>
<p>Non-uniform prior distributions The previous numerical experiments focused on the case in which the experimenter's prior distribution is identical across treatment arms. Yet, modern experimental platforms typically run many experiments with similar treatments, so the experimenter may have different prior beliefs across treatments. These beliefs can be incorporated into the experimental design to improve sampling efficiency. To study the effect of non-uniform priors, we consider the Beta-Bernoulli experiment under different Beta priors for each arm. Fixing the batch size to be b t n = 100 and the number of arms to be K = 100, we consider the following prior distributions.</p>
<p>• Flat: All arms have an identical prior of Beta(100, 100). • Top One: There is a single treatment with a Beta (110,100) prior such that it has a higher prior mean than other arms. All other arms have Beta(100, 100) priors. • Top Half: Half of the arms have Beta (110,100) priors so that they have a higher prior mean than the rest which have Beta(100, 100) priors. • Descending: The first arm has the highest prior mean (Beta(100, 100)) and the prior means decrease for each arm i ∈ [K]. Each arm has a Beta(100 − (i − 1), 100) prior. Figure 9 compares the performance of the sampling policies across different prior distributions. Unsurprisingly, policies which use prior information outperform non-Bayesian policies when the prior is more informative. We observe that despite using a Gaussian prior to approximate the true prior distribution, RHO obtains significantly larger performance gains compared to Thompson sampling policies that use the true prior.</p>
<p>Discussion</p>
<p>In this work, we use Gaussian approximations to derive an MDP framework that describes how posterior beliefs evolve under the experimenter's sampling policy in batch experiments. With tools from optimal control and reinforcement learning, this framework guides the development of policies that can plan ahead and calibrate exploration to the size of the remaining sampling budget.</p>
<p>Related work</p>
<p>This paper is situated in the extensive body of work studying adaptive methods for instancedependent pure-exploration problems. Since this literature spans multiple communities including operations research, statistics, and machine learning, we provide a necessarily abridged review centered around the following characteristic aspects of our approach.</p>
<ol>
<li>
<p>Our proposed adaptive framework focuses on a small number of reallocation epochs (horizon T ). The algorithms we derive optimize instance-dependent factors that do not grow with T , which are often ignored as "constants" in the literature. 2. Using sequential Gaussian approximations, our algorithms can handle flexible batch sizes.</p>
</li>
<li>
<p>The Gaussian sequential experiment we derive is based on the admissible regime where the gaps in average rewards scale as Θ(1/ √ n). 4. When endowed with a Gaussian prior over average rewards, the Gaussian sequential experiment gives rise to a Markov decision process with fractional measurement allocations. Our MDP is connected to previous works in simulation optimization that study problems with Gaussian observations, as well as (adaptive) Bayesian experimental design methods.</p>
</li>
</ol>
<p>Pure-exploration MABs and ranking &amp; selection The problem of identifying the top-k arms out of a set of alternatives has been extensively studied in the simulation optimization and multi-armed bandit literatures. As a detailed review of this vast literature is outside the scope of the work, we refer the reader to Hong et al. [50], Chen et al. [21], Lattimore and Szepesvári [77, Ch 33.5], Russo [98, Section 1.2], and references therein for a complete review. In light of the small-horizon perspective (Point 1), we discuss these works by dividing them into two broad categories.</p>
<p>The first category studies fully sequential procedures and study the behavior of the problem when the number of reallocation epochs (horizon T ) is expected to be large. Several authors explicitly study the limiting regime T → ∞ [22,23,20,42,98], though the multi-armed bandit (MAB) literature primarily focuses on finite horizon guarantees. These works are often classified into two regimes: in the fixed-confidence setting [85,32,64,53,52,66,39,67], the objective is to find the best arm with a fixed level of confidence using the smallest number of samples, where as the fixed budget setting [15,9,37,64,17] aims to find the best arm under a fixed sampling budget. Our setting can be viewed as a Bayesian fixed-budget problem where the sampling budget is split into fixed batches. The main conceptual difference between this paper and the above body of work is that we explicitly optimize performance for each problem instance and a fixed horizon, where "constant terms" that do not depend on the horizon T play an outsize role in determining statistical performance.</p>
<p>From an algorithmic viewpoint, the majority of the works in this category study sequential elimination schemes or modifications of the upper confidence bound algorithm (UCB). Our Bayesian adaptive algorithms are mostly closely related to Bayesian algorithms for best-arm identification [95,99,103,98,65]. When viewed as algorithms for the original pre-limit problem, these algorithms require a full distributional model of individual rewards, in contrast to our framework that only requires priors over average rewards as we belabored in Section 3. Empirically, we observe in Figure 2 that a simple algorithm derived from the limiting Gaussian sequential experiment significantly outperforms these oracle Bayesian sampling methods and these gains hold even when the batch size is small.</p>
<p>Most of the aforementioned works study the probability of correct selection, with notable recent exceptions that consider the Bayes simple regret [74,8]. In this paper, we focus on the Bayes simple regret for ease of exposition, and as we believe this is the more relevant objective in practice. However, our asymptotic formulation can incorporate alternative objectives such as the probability of correct selection, and the adaptive algorithms we develop can be appropriately modified.</p>
<p>The second category of works develops one-step or two-step heuristics for selecting the next alternative, such as Knowledge Gradient or expected opportunity cost [46,36,25,24,48] or probability of correct selection [25]. Of particular relevance to our work is Frazier and Powell [35], who consider the problem of allocating a single batch of samples across several alternatives with normally distributed rewards to minimize the Bayes simple regret. Their setting is equivalent to a single-stage version of the Gaussian sequential experiment we consider. Empirically, we observe in Section 5 that our proposed planning-based policies can perform much better than these single-batch or one-step lookahead procedures.</p>
<p>Batching and delayed feedback From the perspective of handling flexible batch sizes (Point 2), several authors have adapted bandit algorithms to problems with batch evaluations and delayed feedback. In the standard exploration-exploitation MAB setting with fixed batch sizes, Perchet et al. [93], Gao et al. [38] find that even when the number of reallocation epochs scales logarithmically or even sub-lograthmically in the total sample size, one can recover the same rates for cumulative regret as in the fully-sequential setting. Esfandiari et al. [30] show these rates can be made tighter under adaptive batch sizes, and Kalkanli and Ozgur [60], Karbasi et al. [63] show that Thompson sampling with an adaptive batch scheme also achieves rates equivalent to the fully sequential case up to similar logarithmic terms. The growing literature on bandits with delayed feedback can also be seen as a bandit problem with stochastic batch sizes [58,45,94,108].</p>
<p>We focus on pure-exploration when the number of reallocation epochs is small compared to the total sample size. In this regard, Jun et al. [59] propose fixed confidence and fixed budget policies that obtain the top-k arms and find that the batch complexity scales logarithmically in the total sample size. Agarwal et al. [5] show that a sequential elimination policy obtains the top-k arms with fixed confidence with a number of reallocation epochs that only grows iterated logarithmically in the total sample size. Most of the policies proposed in the batched bandit literature are either sequential elimination based policies, or batch variants of Thompson Sampling that know the true reward distribution. Unlike these works, we study a limiting regime and propose Bayesian algorithms that use Gaussian approximations for the likelihoods. As we further detail in the next discussion point, the policies we derive for the Gaussian sequential experiment can be directly applied to the original pre-limit problem, and can thus handle any batch size flexibly. Empirically, our methods outperform standard batch methods across a range of settings, as we outline in Figure 2 and further expand in Section 5.</p>
<p>Gaussian approximations and asymptotics Bhat et al. [12] study optimal experimental design for binary treatments, under a linear specification for treatments and covariates. Their approach has philosophical similarities to ours, which optimizes a final stage outcome by using DP methods to sequentially assign treatments to subjects. They analyze an fully online version of the problem where subjects arrive stochastically and the experimenter sequentially allocates them to the treatment or control group. They balance covariates in the two groups to minimize the variance of the OLS estimator at the end of the experiment; when covariates are drawn i.i.d. from an elliptical distribution, they show the dimensionality of the DP collapses. Recently, Xiong et al. [114] consider a similar problem for panel data, maximizing the precision of a generalized least squares (GLS) estimator of the treatment effect. On the other hand, our DP crystallizes how the adaptive experimentation problem simplifies in the large-batch approximation; our formulation relies on Bayesian posterior beliefs over average rewards, rather than an estimate of the uncertainty. We consider multiple treatment arms where an effective adaptive design must sample strategically to reduce uncertainty for treatments which are likely to be the best one.</p>
<p>Our work is also connected to recent works that use Gaussian approximations for statistical inference on data produced by bandit policies [47,84,83]. Our work is most related to Zhang et al. [115] who derive inferential procedures based on Gaussian approximations in a batched bandit setting. They construct a modified ordinary least squares (OLS) estimator and show asymptotic normality as the batch size grows large. Although our setting does not feature any contextual information, we similarly consider a fixed horizon experiment with batches and our main asymptotic result considers the regime where the batch size grows large while the horizon remains fixed. In this regard, we show that asymptotic approximations used for inference are also highly effective in guiding adaptive experimentation procedures. However, unlike their work, we are able to derive our result without assuming that the propensity scores are bounded from below or clipped.</p>
<p>Concurrent and independent to the present paper, Hirano and Porter [49] consider data produced from a parametric model and derive Gaussian approximations for batched data produced by adaptive sampling policies. Their work extends the classical framework of local asymptotic normality of parametric models [107] to adaptive data. On the other hand, we do not assume a parametric model over the reward distribution and instead derive Gaussian limits over average rewards. The two approaches are complementary: we focus on deriving a formulation on which modern computational tools can be used to derive new Bayesian batch adaptive sampling policies, whereas Hirano and Porter [49] study power calculations and inferential questions.</p>
<p>There is a nascent literature on deriving diffusion limits for bandit policies [61,109,33,2,6]. These works typically let the number of reallocation epochs grow to infinity, while scaling down the gaps in average rewards like in our setting (Point 3). Wager and Xu [109] consider any Markovian policy, and Fan and Glynn [33] study Thompson sampling with extensions to batched settings where the batch size is small compared to the number of reallocation epochs. Kalvit and Zeevi [61] find a sharp discrepancy between the behavior of UCB and Thompson Sampling in a related regime. Adusumilli [2] considers an arbitrary Bayesian policy and derive an HJB equation for the optimal policy in the limit, which can be solved by PDE methods. Araman and Caldentey [6] consider a sequential testing environment with two hypotheses involving experiments that arrive according to an exogenous Poisson process. They derive diffusion process approximations in the limit as intensity of arrivals grows large and the informativeness of experiments shrinks.</p>
<p>In contrast, our diffusion limit is motivated by the practical difficulty of reallocating sampling effort and we consider a fixed number of reallocation epochs. Our formulation is derived by letting the batch size n grow large, while scaling down the difference in average rewards as Θ(1/ √ n). This setting is particularly appropriate for modeling experimentation on online platforms that deal with many units and interventions with small treatment effects. Unlike the above line of work that study limiting diffusion processes, our limiting process is a discrete time MDP which allows us to use approximate DP methods to develop new adaptive experimentation policies.</p>
<p>Gaussian observations and priors Several research communities have taken interest in sequential decision-making problems with Gaussian observations and priors due to its tractability. While the algorithmic connections are salient as we discuss shortly, these literatures do not provide formal justifications of normality unlike the present work, although we have found some nice heuristic discussions (e.g., see Kim and Nelson [68, Section 3.1]). Another key difference between our asymptotic framework and others is that we consider normal distributions over the gaps between average rewards, rather than individual rewards (Point 4).</p>
<p>Taking a decision-theoretic framework in problems with Gaussian observations, a number of authors [46,25,35] have studied simple settings that yield an exact solution, e.g., when there is a single measurement. While the formal settings are different, the spirit of our algorithmic development is similar to these approaches which heuristically extend the solution derived in the simple setting to build adaptive policies. Our use of the Gaussian sequential experiment to improve statistical power appears new; a similar Gaussian MDP problem has been recently studied in the context of robust control [89] and attention allocation [79]. Most recently, Liu et al. [82] provide bounds on the cumulative regret of a Bayesian agent that maintains a misspecified Gaussian posterior state in a (fully sequential) Bernoulli reward environment.</p>
<p>The Bayesian optimization literature uses Gaussian processes to model the world. Many authors have proposed acquisition functions for batch evaluations [40,102,110,43,113] and some derive regret bounds for batched versions UCB and Thompson sampling in Gaussian process environments [27,29,62]. The primary focus of this literature is problems with a continuous set of arms. In contrast, we focus on the allocation of measurement effort over a finite number of arms under limited statistical power, and our setting is characterized by limited extrapolation between arms and a fixed, finite exploration horizon. Our approach of using a MDP for posterior beliefs to design non-myopic adaptive experimentation policies conceptually builds on works that design non-myopic acquisition functions for Bayesian optimization [44,54,76,55,7].</p>
<p>As our methods maximize an expected utility function, it is conceptually related to Bayesian experimental design methods [18,100,34]. Instead of optimizing expected information gain (EIG), we minimize expected simple regret at the terminal period, a more tractable objective. In particular, we consider policy gradient based methods to guide the experimental design similar to the works [34,57]. Our work is also similar to [88], who propose a policy gradient algorithm for shaping the posterior used by Thompson Sampling as a computational tool to improve its performance. But unlike these works, the methods we consider at the end of Section 3.1 use pathwise policy gradients of the value function enabled by the reparameterization trick for Gaussian random vectors [70]. This is in contrast with the score function or REINFORCE gradient estimator [111] commonly used in reinforcement learning, which is known to have higher variance and is difficult to apply in continuous state and action settings. Our method allows us to use standard auto-differentiation frameworks to compute exact gradients of the value function without having to fit the value function or the model separately, as is required for other RL methods such as Deep Deterministic Policy Gradient [80]. While the reparameterization trick has been used before to maximize one-step or two-step acquisition functions in Bayesian optimization (e.g. [112,113,11]), our work differs in that we compute pathwise gradients of entire sample paths with respect to sampling allocations, and use these gradients to optimize sampling policies. </p>
<p>A Further Experimental Results</p>
<p>A.1 Unknown measurement variance</p>
<p>We consider the case where the measurement variance is unknown to the experimenter. We fix K = 100 and n = 10, 000 in the Gamma-Gumbel experiment. The experimenter believes the measurement variance for all arms is identically equal to s 2 = 1, but the true measurement variance is Intuitively, when ς = 1.0, the actual measurement variance can easily range from 50% to 196% of the variance assumed by the experimenter. We observe in Figure 10 that despite large mismatch between the experimenter's belief of the measurement variance and the true measurement variance, RHO is still able to retain much of its performance benefits over other policies. This illustrates that the method is rather robust to estimation errors of the measurement variance.
s 2 = Y i where Y i ∼ Lognormal(0,</p>
<p>B Derivations of Gaussian sequential experiment B.1 Proof of Theorem 1</p>
<p>Let s 2 t,a := s 2 a /b t be the rescaled measurement variance. Recall the Gaussian sequential experiment given in Definition 1: for all 0 &lt; t ≤ T − 1
G 0 ∼ N (hπ 0 , Σ 0 ), G t |G t−1 , . . . , G 0 ∼ N (hπ t (G 1 , ..., G t−1 ), Σ t )
where Σ t = diag(s 2 t,a π t,a (G 1 , ..., G t−1 )) and Σ 0 = diag(s 2 t,a π 0,a ).</p>
<p>Induction We use an inductive argument to prove the weak convergence (3) for a policy π and reward process R a,t satisfying Assumptions A, B. For the base case, let I 0 := {a ∈ [K] : π 0,a &gt; 0} denote the arms with a positive sampling probability (this is a deterministic set). For the arms not in I 0 , √ nR n 0 = 0 = N (h a π 0,a , s 2 t,a π 0,a ). For the remaining arms, by the Lindeberg CLT, we have that
√ nR n 0 d ⇝ N (π 0 h, Σ 0 ). Next, suppose ( √ nR n 0 , . . . , √ nR n t ) d ⇝ (G 0 , ..., G t ).(23)
To show weak convergence to G t+1 , it is sufficient to show
E[f ( √ nR n 0 , . . . , √ nR n t+1 )] → E[f (G 0 , ..., G t+1 )] for any bounded Lipschitz function f(24)
Fixing a bounded Lipschitz f , assume without loss of generality that
sup x,y∈R (t+1)×k |f (x) − f (y)| ≤ 1 and f ∈ Lip(R (t+1)×K ).
We first set up some basic notation. For any r 0:t ∈ R (t+1)×K , define the conditional expectation operator on a random variable W
E r 0:t [W ] := E W √ n R n s t s=0 = r 0:t .
Then, conditional on realizations of previous estimators up to time t, we define a shorthand for the conditional expectation of f and its limiting counterpart. Suppressing the dependence on f , let g n , g : R t×K → R be 
g n (r 0:t ) := E r 0:t f √ nR n 0 , . . . , √ nR n t , √ nR n t+1 = E f r 0:t , √ nR n
where Z ∼ N (0, I) and the conditional covariance is determined by the allocation π t+1,a (r 0:t ) Σ t+1 (r 0:t ) := diag π t+1,a (r 0:t )s 2 t+1,a .</p>
<p>Conditional on the history r 0:t , √ nR n t+1 depends on r 0:t only through the sampling probabilities π t+1,a (r 0:t ).</p>
<p>To show the weak convergence (24), decompose the difference between E[f ( √ nR n 0 , . . . , √ nR n t )] and E[f (G 0 , . . . , G t )] in terms of g n and g
|E[f ( √ nR n 0 , ..., √ nR n t+1 )] − E[f (G 0 , ..., G t+1 )]| (27) = |E[g n ( √ nR n 0 , ..., √ nR n t )] − E[g(G 0 , ..., G t )]| = |E[g n ( √ nR n 0 , ..., √ nR n t )] − E[g( √ nR n 0 , ..., √ nR n t )]| + |E[g( √ nR n 0 , ..., √ nR n t )] − E[g(G 0 , .
.., G t )]| By Assumption B2 and dominated convergence, g is continuous almost surely under (G 0 , . . . , G t ). From the inductive hypothesis (23), the continuous mapping theorem implies
E[g( √ nR n 0 , ..., √ nR n t )] → E[g(G 1 , ..., G t )].
Uniform convergence of g n → g It remains to show the convergence E[g n ( √ nR n 0 , ...,
√ nR n t )] → E[g( √ nR n 0 , ..., √ nR n t )]
. While one would expect pointwise convergence of g n (r 0:t ) to g(r 0:t ) by the CLT, proving the above requires controlling the convergence across random realizations of the sampling probabilities π t+1,a ( √ nR n 0 , ..., √ nR n t ). This is complicated by the fact that we allow the sampling probabilities to be zero. To this end, we use the Stein's method for multivariate distributions to provide rates of convergence for the CLT that are uniform across realizations r 0:t . We use the bounded Lipschitz distance d BL to metrize weak convergence.</p>
<p>To bound |E[g n ( √ nR n 0 , . . . ,
√ nR n t )] − g( √ nR n 0 , . . . , √ nR n t )|, decompose |E[g n ( √ nR n 0 , . . . , √ nR n t ) − g( √ nR n 0 , . . . , √ nR n t )]| = |E[E r 0:t [f (r 0:t , √ nR n n,t+1 ) − f (r 0:t , Σ t+1 (r 0:t ) 1/2 Z + π t+1 (r 0:t )h)]]| ≤ E[E r 0:t [d BL (Q n,t+1 (r 0:t ), Σ 1/2 t+1 (r 0:t )Z)]] ≤ E<a href="28">E r 0:t [d BL (Q n,t+1 (r 0:t ), Σ 1/2 n,t+1 (r 0:t )Z)] + E r 0:t [d BL (Σ 1/2 n,t+1 (r 0:t )Z, Σ 1/2 t+1 (r 0:t )Z)]</a>
where Q n,t+1 (r 0:t ) is the demeaned estimator
Q n,t+1 (r 0:t ) := √ nR n t+1 (r 0:t ) − π t+1 (r 0:t )h = 1 b t+1 n b t+1 n j=1 ξ t+1 j R t+1 j b t+1 − π t+1 (r 0:t )h b t+1 n and the covariance Σ n,t+1 (r 0:t ) is Σ n,t+1 (r 0:t ) := Cov ξ t+1 j R t+1 j b t+1 (29) = Σ t+1 (r 0:t ) + 1 b t+1 n diag(π t+1,a (r 0:t )h 2 a ) − 1 b t+1 n (π t+1 (r 0:t )h)(π t+1 (r 0:t )h) ⊤ .
To ease notation, we often omit the dependence on r 0:t . Note that Σ n,t+1 is not quite equal to the covariance matrix Σ t+1 appearing in the limiting Gaussian sequential experiment (26). This is because for any finite n, the covariance of this Gaussian Σ n,t+1 (r 0:t ) not only includes the variance of the arm rewards but also correlations that emerge from sampling arms according to a random multinomial vector (rather than a deterministic allocation). This gap vanishes as n → ∞ and the covariance matrix converges to the diagonal covariance matrix Σ t+1 (r 0:t ).</p>
<p>Bounding the first term in inequality (28) The first term in inequality (28) measures the distance between the sample mean Q n,t+1 and its Gaussian limit. Before we proceed, it is helpful to define the following quantities, which describe smoothness of the derivatives of any function f ∈ C 3
M 1 (f ) = sup x ∥∇f ∥ 2 M 2 (f ) = sup x ∇ 2 f op M 3 (f ) = sup x ∇ 3 f op
The following bound, which we prove in Section B.1.1, quantifies the rate of convergence for the CLT using the multivariate Stein's method Meckes [86]. Recall that ϵ is the noise in the rewards (1).
Proposition 4. For any f ∈ C 3 , |Ef (Q n,t+1 ) − Ef (Σ 1/2 n,t+1 Z)| ≤ C 1 n −1/2 M 2 (f ) + C 2 n −1/2 M 3 (f )
where C 1 and C 2 depend polynomially on K, b t+1 , h, s 2 , ∥ϵ∥ 3 2 , ∥ϵ∥ 4 2 . It remains to show convergence of Q n,t+1 to Σ n,t+1 Z for Lipschitz test functions, which is required to control the bounded Lipschitz distance. We use standard Gaussian smoothing arguments found in Meckes [87]: by convolving the test function with a Gaussian density, one obtains a smoother function for which the result of Proposition 4 is applicable. At the same time, the amount of smoothing is controlled to ensure the bias with the original test function is small.</p>
<p>Lemma 3 (Meckes [87, Corollary 3.5]). For any 1-Lipschitz function f , consider the Gaussian convolution
(f * ϕ δ )(x) := E[f (x + δZ)], where Z ∼ N (0, I). M 2 (f * ϕ δ ) ≤ M 1 (f ) sup θ:∥θ∥ 2 =1 ∇ϕ ⊤ δ θ ≤ 2 π 1 δ M 3 (f * ϕ δ ) ≤ M 1 (f ) sup θ:∥θ∥ 2 =1 θ ⊤ ∇ 2 ϕ δ θ ≤ √ 2 δ 2
Moreover, for any random vector X,
E[(f * ϕ δ )(X) − f (X)] ≤ E[δ ∥Z∥ 2 ] ≤ δ √ K
Thus, for any 1-Lipschitz function f , we have the bound
|Ef (Q n,t+1 ) − Ef (Σ 1/2 n,t+1 Z)| ≤ |Ef (Q n,t+1 ) − E(f * ϕ δ )(Q n,t+1 )| + |E(f * ϕ δ )(Q n,t+1 ) − E(f * ϕ δ )(Σ 1/2 n,t+1 Z)| + |E(f * ϕ δ )(Σ 1/2 n,t+1 Z) − Ef (Σ 1/2 n,t+1 Z)| ≤ 2δ √ K + 2 π 1 δ C 1 n −1/2 + √ 2 δ 2 C 2 n −1/2 Optimizing over δ, we obtain d BL (Q n,t+1 , Σ 1/2 n,t+1 Z) ≤ C t+1 n −1/6
for some constant C t+1 that depends only on K, b t+1 , E ∥ϵ∥ 3 2 , E ∥ϵ∥ 4 2 but not on n or r 0:t .</p>
<p>Bounding the second term in inequality (28) Finally, it remains to show uniform convergence of the second term in the bound (28). That is, the limiting Gaussian N (0, Σ 1/2 n,t+1 (r 0:t )) converges uniformly to N (0, Σ 1/2 t+1 (r 0:t )) as n → ∞:
E E r 0:t d BL Σ 1/2 n,t+1 (r 0:t )Z, Σ 1/2 t+1 (r 0:t )Z → 0.
First, for any two measures (µ, ν), d BL (µ, ν) is upper bounded by the total variation distnace 2d TV (µ, ν), which is in turn bounded above by KL divergence 1 2 d KL (µ, ν) by Pinsker's inequality. We derive the following concrete bound on the KL divergence, whose proof we defer to Section B.1.2.</p>
<p>Lemma 4. The KL-divergence between N (0, Σ n,t+1 (r 0:t )) and N (0, Σ t+1 (r 0:t )) is bounded above as follows
D kl (N (0, Σ n,t+1 (r 0:t ))||N (0, Σ t+1 (r 0:t ))) ≤ 1 b t+1 n max a h 2 a /s 2 t+1,a 1 − max a h 2 a /(s 2 a + h 2 a ) + a h 2 a s 2 t+1,a
This guarantees that Conclusion Altogether, we have shown
|E[g n ( √ nR n 0 , . . . , √ nR n t ) − g( √ nR n 0 , . . . ,
√ nR n t )]| ≤ C t+1 n −1/6 + D t+1 n −1/2 for constants C t+1 , D t+1 that depend polynomially on h a , s 2 a and higher order moments of ϵ a as well as on K and b t+1 . Thus, this shows that E[f ( √ nR n 0 , . . . ,
√ nR n t+1 )] → E[f (G 0 , .
. . , G t+1 )] as n → ∞ for any bounded Lipschitz function f , which implies the desired weak convergence.</p>
<p>B.1.1 Proof of Proposition 4</p>
<p>In order to quantify the rate of the CLT, we use the following result by Meckes [86] which provides a characterization of Stein's method for random vectors with arbitrary covariance matrices.</p>
<p>Lemma 5 (Meckes [86,Theorem 3]). Let (W, W ′ ) be an exchangeable pair of random vectors in R K . Suppose that there exists λ &gt; 0, a positive semi-definite matrix Σ, a random matrix E such that
E[W ′ − W |W ] = −λW E[(W ′ − W )(W ′ − W ) ⊤ |W ] = 2λΣ + E[E|W ]
Then for any f ∈ C 3 ,
|Ef (W ) − Ef (Σ 1/2 Z)| ≤ 1 λ √ K 4 M 2 (f )E ∥E∥ H.S. + 1 9 M 3 (f )E W ′ − W 3
where ∥·∥ H.S. is the Hilbert-Schmidt norm.</p>
<p>The rest of the proof is similar to that of Chatterjee and Meckes [19,Theorem 7], with slight modifications due to the fact that we have a non-identity covariance. For simplicity, define X j :=
ξ t+1 j R t+1 j √ b t+1 − πh √ b t+1 n .
For any index j, we construct an independent copy Y j of X j . We construct an exchangeable pair (Q n,t+1 , Q ′ n,t+1 ) by selecting an random index I ∈ {1, ..., b t+1 n} chosen uniformly and independently from Q n,t+1 and letting
Q ′ n,t+1 = Q n,t+1 − X I b t+1 n + Y I b t+1 n .
We can observe then that
E[Q ′ n,t+1 − Q n,t+1 |Q n,t+1 ] = 1 b t+1 n E[Y I − X I |Q n,t+1 ] = 1 (b t+1 n) 3/2 b t+1 n j=1 E[Y j − X j |Q n,t+1 ] = − 1 b t+1 n Q n,t+1
by independence of Y j and Q n,t+1 . This pair satisfies the first condition of Theorem 5 with λ = 1/(b t+1 n).</p>
<p>It also satisfies the second condition of Theorem 5 with:
E a,a ′ = 1 (b t+1 n) 2 b t+1 n j=1
X j,a X j,a ′ − (Σ n,t+1 ) a,a ′ by independence of X i and Y i . Thus, we have that
EE 2 a,a ′ = 1 (b t+1 n) 4 b t+1 n j=1 E E[X j,a X j,a ′ − (Σ n,t+1 ) a,a ′ |Q n,t+1 ] 2 ≤ 1 (b t+1 n) 4 b t+1 n j=1 E X j,a X j,a ′ − (Σ n,t+1 ) a,a ′ 2 = 1 (b t+1 n) 3 E X 1,a X 1,a ′ 2 − (Σ n,t+1 ) 2 a,a ′
This gives us a bound on the Hilbert-Schmidt norm:
E ∥E∥ H.S. ≤ a,a ′ EE 2 a,a ′ ≤ 1 (b t+1 n) 3/2 a,a ′ E X 1,a X 1,a ′ 2 − (Σ n,t+1 ) 2 a,a ′ ≤ 1 (b t+1 n) 3/2 E ∥X 1 ∥ 4 2
We can further bound E ∥X 1 ∥ 4 2 by a constant C 1 that is polynomial in h, b −1 t+1 , s 2 , E ∥ϵ∥ 3 2 , and E ∥ϵ∥ 4 2 . Finally, we can bound E Q ′ n,t+1 − Q n,t+1 3 2 as follows:
E Q ′ n,t+1 − Q n,t+1 3 2 = 1 (b t+1 n) 3/2 E ∥Y I − X I ∥ 3 2 ≤ 1 (b t+1 n) 3/2 8E ∥X 1 ∥ 3 2
where the final inequality uses independence of Y j and X j as well as Holder's inequality. We can bound E ∥X 1 ∥ 3 2 by another constant C 2 that is h, b −1 t+1 , s 2 , and E ∥ϵ∥ 4 2 . Plugging these bounds into the statement of Theorem 5, we obtain the stated result.</p>
<p>B.1.2 Proof of Lemma 4</p>
<p>We use the expression for the KL-divergence between two multivariate normal distributions D kl (N (0, Σ n,t+1 (r 0:t ))||N (0, Σ t+1 (r 0:t ))) = tr (Σ t+1 (r 0:t )) −1 Σ n,t+1 (r 0:t ) − m + log det Σ t+1 (r 0:t ) det Σ n,t+1 (r 0:t ) = a s 2 t+1,a π t+1,a (r 0:t ) −1 s 2 t+1,a π t+1,a (r 0:t ) + π t+1,a (r 0:t )(1 − π t+1,a (r 0:t ))
h 2 a b t+1 n − m + log det Σ t+1 (r 0:t ) − log det Σ n,t+1 (r 0:t ) ≤ a 1 b t+1 n (1 − π t+1,a (r 0:t )) h 2 a s 2 t+1,a + a log s 2 t+1,a π t+1,a (r 0:t ) − a log s 2 t+1,a π t+1,a (r 0:t ) + 1 b t+1 n h 2 a π t+1,a (r 0:t ) + 1 b t+1 n max a h 2 a /s 2 t+1,a 1 − max a h 2 a /(s 2 t+1,a + h 2 a ) ≤ 1 b t+1 n a h 2 a s 2 t+1,a + max a h 2 a /s 2 t+1,a 1 − max a h 2 a /(s 2 a + h 2 a )
We obtain det Σ n,t+1 (r 0:t ) ⊤ using the fact that Σ n,t+1 (r 0:t ) is a rank-one perturbation of a diagonal matrix log det Σ n,t+1
= log det Σ t+1 + 1 b t+1 n diag π t+1,a h 2 a + log det I + Σ ϵ,t + 1 b t+1 n diag π t+1,a h 2 a ⊤ −1 − 1 b t+1 n (π t+1 h)(π t+1 h) ⊤ = a log s 2 t+1,a π t+1,a + 1 b t+1 n h 2 a π t+1,a + log det   I − 1 b t+1 n (π t+1 h) ⊤ Σ t+1 + 1 b t+1 n diag π t+1,a h 2 a ⊤ −1 (π t+1 h)   = a log π t+1,a s 2 t+1,a + 1 b t+1 n π t+1,a h 2 a + log 1 − 1 b t+1 n a π t+1,a h 2 a s 2 t+1,a + (1/b t+1 n)h 2 a ≥ a log π t+1,a s 2 t+1,a + 1 b t+1 n π t+1,a h 2 a + log 1 − 1 b t+1 n max a h 2 a s 2 t+1,a + (1/b t+1 n)h 2 a Finally, we use the bound log(1 − x) ≥ −x 1−x to obtain log det Σ n,t+1 ≥ a log π t+1,a s 2 t+1,a + 1 b t+1 n π t+1,a h 2 a − 1 b t+1 n max a h 2 a s 2 t+1,a +(1/b t+1 n)h 2 a 1 − max a h 2 a b t+1 ns 2 t+1,a +h 2 a ≥ a log π t+1,a s 2 t+1,a + 1 b t+1 n π t+1,a h 2 a − 1 b t+1 n max a h 2 a /s 2 t+1,a 1 − max a h 2 a /(s 2 a + h 2 a )</p>
<p>B.2 Proof of Corollary 1</p>
<p>We show that for any horizon T d BL ( √ nR n 0 , . . . ,
√ nR n T −1 ), (G 0 , . . . , G T −1 ) ≤ C M T − 1 M − 1 n −1/6 , where M = 1 +L(s 2 * /b * √ K + max a |h a |)
,L is the bound on the Lipschitz constant for π 1/2 and π, and s 2 * = max a s 2 a and b * = min 0≤t≤T −1 b t . We prove the statement through induction. For the base case, we have from Proposition 4, Lemma 3, and Lemma 4 that d BL ( √ nR n 0 , N (π 0 h, Σ 0 )) is bounded by C 0 n −1/6 , for some constant C 0 that depends on K, b 0 , h, s 2 , and higher moments of ∥ϵ∥ 2 . Suppose now that this holds up to t
d BL ( √ nR n 0 , . . . , √ nR n t ), (G 0 , . . . , G t ) ≤ C t s=0 M s n −1/6 .
We proceed to show that this inequality then holds for t + 1.</p>
<p>Recall the decomposition (27) for any bounded Lipschitz function f and corresponding g, g n defined as in Eq. (25). From Proposition 4, Lemma 3, and Lemma 4
|E[g n ( √ nR n 0 , ..., √ nR n t )] − E[g( √ nR n 0 , ..., √ nR n t )]| ≤ C t+1 n −1/6 + D t+1 n −1/2
Thus, there exists some C ′ t+1 such that the above is bounded by C ′ t+1 n −1/6 . Note that the dependence of C ′ t on t is only through a polynomial on 1/b t+1 . Thus we can bound this by a constant C which will depends on T only through polynomial factors of b * = min 0≤t≤T −1 b t .</p>
<p>To bound the second term in the decomposition (27), we use the fact that
g(r 0:t ) = E f r 0:t , Σ 1/2 t+1 (r 0:t )Z + hπ t+1 (r 0:t )
is a bounded Lipschitz function. We seek to identify the Lipschitz constant for g, as we can then utilize the rate for bounded Lipschitz functions in the induction hypothesis. For any two r 0:t , r ′ 0:t ,
|g(r 0:t ) − g(r ′ 0:t )| ≤ E (r 0:t , Σ 1/2 t+1 (r 0:t )Z + hπ t+1 (r 0:t )) − (r ′ 0:t , Σ 1/2 t+1 (r ′ 0:t )Z + hπ t+1 (r ′ 0:t )) 2 ≤ E r 0:t − r ′ 0:t 2 2 + Σ 1/2 t+1 (r 0:t )Z − Σ 1/2 t+1 (r ′ 0:t )Z 2 2 + hπ t+1 (r 0:t ) − hπ t+1 (r ′ 0:t ) 2 2 1 2 ≤ r 0:t − r ′ 0:t 2 + E Σ 1/2 t+1 (r 0:t )Z − Σ 1/2 t+1 (r ′ 0:t )Z 2 + hπ t+1 (r 0:t ) − hπ t+1 (r ′ 0:t ) 2 ≤ 1 +L(s 2 * /b * E ∥Z∥ 2 + max a |h a |) r 0:t − r ′ 0:t 2 using √ a + b ≤ √ a + √ b and a − b = ( √ a + √ b)( √ a − √ b). This implies |E[f ( √ nR n 0 , ..., √ nR n t+1 )] − E[f (G 0 , ..., G t+1 )]| ≤ |E[g n ( √ nR n 0 , ..., √ nR n t )] − E[g( √ nR n 0 , ..., √ nR n t )]| + |E[g( √ nR n 0 , ..., √ nR n t )] − E[g(G 0 , ..., G t )]| ≤ Cn −1/6 + M · C t s=0 M s n −1/6 = C t+1 s=0 M s n −1/6
Noting that T −1 s=0 M s = M T −1 M −1 , we have the desired inequality.</p>
<p>B.3 Proof of Corollary 2</p>
<p>The proof proceeds as in the proof of Corollary 1. We show that for any horizon T d BL ( √ nR n 0 , . . . ,
√ nR n T −1 ), (G 0 , . . . , G T −1 ) ≤C M T − 1 M − 1 n −1/2 ,
where M = 1 +L(s 2 * /b * K 1/2 + max a |h a |) ,L is the bound on the Lipschitz constant for π 1/2 and π, and s 2 * = max a s 2 a and b * = min 0≤t≤T −1 b t . For the base case, note that by Assumption C, max a π −3 0,a (µ 0 , σ 0 ) ≤ C o . Applying standard results for rates of weak convergence of the central limit theorem (see Lemma 7), we have that d BL ( √ nR n 0 , N (π 0 h, Σ 0 )) is bounded by C 0 n −1/2 , for some constant C 0 that depends on K and polynomially on C o , b 0 , h, and s 2 . For the induction step, assume that up to t, the following inequality holds
d BL ( √ nR n 0 , . . . , √ nR n t ), (G 0 , . . . , G t ) ≤C t s=0 M s n −1/2 .
for M = 1 +L(s 2 * /b * K 1/2 + max a |h a |) and a constantC that depends on K, s 2 a , b t+1 , and h a . We proceed to show that this inequality then holds for t + 1.</p>
<p>Under Assumption C, we can obtain a n −1/2 rate for the central limit theorem specialized for bounded Lipschitz distance. First, we establish some facts about the covariance matrix Σ n,t+1 (r 0:t ) defined in Eq. (29) for the sample average Q n,t+1 (r 0:t ). We omit the dependence on r 0:t to ease notation. Lemma 6. Let λ max (·) and λ min (·) denote the maximum and minimum eigenvalues respectively. For any realization of r 0:t , we have
λ max (Σ n,t+1 ) ≤ λ max Σ t+1 + 1 b t+1 n diag π t+1,a h 2 a ≤ max a s 2 a + h 2 a b t+1 λ min (Σ n,t+1 ) ≥ λ min (Σ t+1 ) + 1 b t+1 n λ min diag(π t+1,a h 2 a ) − (π t+1 h)(π t+1 h) ⊤ ≥ min a π t+1,a (r 0:t ) min a s 2 t+1,a .</p>
<p>Proof of Lemma</p>
<p>The first inequality follows since λ max (A + B) ≤ λ max (A) + λ max (B) for symmetric matrices A, B ∈ R m×m . The second inequality uses λ min (A + B) ≥ λ min (A) + λ min (B).
Moreover, for any b ∈ R m b ⊤ diag(π t+1,a (r 0:t )h 2 a ) − (π t+1 (r 0:t )h)(π t+1 (r 0:t )h) ⊤ b = a π t+1,a (r 0:t )h 2 a b 2 a − a π t+1,a (r 0:t )h a b a 2 ≥ 0
as by Cauchy-Schwartz:
a π t+1,a (r 0:t )h a b a 2 ≤ a π t+1,a (r 0:t ) a π t+1,a (r 0:t )h 2 a b 2 a ≤ a π t+1,a (r 0:t )h 2 a b 2 a
so the matrix is positive semi-definite Next, we show convergence of Q n,t+1 (r 0:t ) to a standard normal random vector Z ∼ N (0, I m ) via a classical result, which is a corollary of Bhattacharya [13,Theorem 1]. The following result explicitly quantifies how the rate of convergence depends on the sampling probabilities, allowing us to guarantee uniform convergence.</p>
<p>Lemma 7 (Bhattacharya [13, Theorem 1]). . Let {X i } n i=1 be a sequence of n independent random vectors taking values in R K with covariance matrix Σ n . Assume that for some δ &gt; 0
M := sup n 1 n n i=1 E Σ −1/2 n X i 3+δ 2 &lt; ∞.
Then, the normalized partial sumZ n = 1
√ n n i=1 Σ −1/2 n X i satisfies d BL (Z n , N (0, I)) ≤ A K,δ M 3+3δ 3+δ n −1/2 + B K,δ M 3 3+δ n −1/2
where A K,δ , B K,δ are constants that only depends on K and δ.</p>
<p>In order to guarantee uniform convergence over r 0:t using Lemma 7, we use Assumptions A2 to control the moment term M . Since we assume the existence of a fourth moment, we let δ = 1.</p>
<p>Using C K to denote a constant that only depends on K and polynomially on s 2 , b t+1 , h (that may differ line by line), use lower bound on the minimum eigenvalue of Σ n,t+1 given in Lemma 6 to get M (r 0:t ) := sup
n 1 b t+1 n b t+1 n j=1 E (Σ n,t+1 (r 0:t )) −1/2 ξ t+1 j R t+1 j − π t+1,a (r 0:t )h a √ n 4 2 ≤ min a π t+1,a (r 0:t ) min a s 2 t+1,a −2 sup n 1 b t+1 n b t+1 n j=1 E ξ t+1 j R t+1 j − π t+1 (r 0:t )h √ n 4 2 ≤ C K max a π −2 t+1,a (r 0:t ) max a E ξ t+1 a,j R a,j − π t+1,a (r 0:t )h a √ n 4 &lt; ∞ Using Assumption A2, conclude M (r 0:t ) ≤ C K max a π −2 t+1,a (r 0:t ) C + max a |h a | √ n 4 &lt; ∞
where C is the moment bound in ϵ a given in Assumption A2. Apply Lemma 7 to get
E[E r 0:t d BL (Q n,t+1 , Σ 1/2 n,t+1 (r 0:t )Z) ] ≤ max a s 2 a + h 2 a b t+1 1/2 E[E r 0:t d BL (Σ −1/2 n,t+1 Q n,t+1 , Z) ] ≤ max a s 2 a + h 2 a b t+1 1/2 E[E r 0:t A K,δ M (r 0:t ) 3 2 (b t+1 n) −1/2 + B K,δ M (r 0:t ) 3 4 (b t+1 n) −1/2 ].
Taking expectation over r 0:t and using the preceding bound on M (r 0:t ), we have that
E[E r 0:t d BL (Q n,t+1 , Σ 1/2 n,t+1 (r 0:t )Z) ] ≤ max a s 2 a + h 2 a b t+1 1/2 · E[E r 0:t A K,δ M (r 0:t ) 3 2 (b t+1 n) −1/2 + B K,δ M (r 0:t ) 3 4 (b t+1 n) −1/2 ] ≤ n −1/2 C K E max a π −3 t+1,a ( √ nR n 0:t ) C 3 2 + max a |h a | 6 n 3 + n −1/2 C K E max a π −3/2 t+1,a ( √ nR n 0:t ) C 3 4 + max a |h a | 3 n 3/2 .
Due to Assumption C, the moment E[max a π −3 t+1,a ( of the Lipschitz constants for π 1/2 and π, s * := max a s a , and b * := min t b t . We can then use the induction hypothesis, which gives us
|E[g( √ nR n 0 , ..., √ nR n t )] − E[g(G 0 , ..., G t )]| ≤ M ·C t s=0 M s n −1/2 .
This implies
|E[f ( √ nR n 0 , ..., √ nR n t+1 )] − E[f (G 0 , ..., G t+1 )]| ≤ |E[g n ( √ nR n 0 , ..., √ nR n t )] − E[g( √ nR n 0 , ..., √ nR n t )]| + |E[g( √ nR n 0 , ..., √ nR n t )] − E[g(G 0 , ..., G t )]| ≤Cn −1/2 + M ·C t s=0 M s n −1/2 =C t+1 s=0 M s n −1/2 Noting that T −1 s=0 M s = M T −1 M −1 , we have the desired inequality.</p>
<p>B.4 Proof of Corollary 3</p>
<p>From the proof of Theorem 1, the convergence of the Bayes simple regret immediately follows from the fact that the weak convergence (3)  For any prior distribution ν over h such that E ν ||h|| &lt; ∞, dominated convergence gives
BSR T (π, ν, √ nR n ) = E h∼ν [h ⊤ π T ( √ nR n 0 , . . . , √ nR n t+1 )] = E h∼ν [h ⊤ E[π T ( √ nR n 0 , . . . , √ nR n t+1 )|h]] → E h∼µ [h ⊤ E[π T (G 0 , . . . , G T −1 )|h]] = BSR T (π, ν, G).</p>
<p>C Derivations for Bayesian adaptive experiment C.1 Proof of Lemma 1</p>
<p>In the posterior updates (13), the result (15) follows by noting
G t,a | µ t , σ t d = E[G t,a | µ t , σ t ] + Var(G t,a | µ t , σ t )Z t,a ,
and plugging in expressions for the conditional mean and variance of G t,a . Instead of this change of variables argument, we give a more evocative derivation below that shows
µ t+1 | µ t , σ t ∼ N µ t , diag(σ 2 t − σ 2 t+1 ) .(30)
Gaussianity of µ t+1 | µ t , σ t follows since µ t+1 is a linear function of G t,a given µ t , σ t µ t+1,a = σ 2 t+1,a σ −2 t,a µ t,a + s −2
a b t G t,a = σ −2 t,a + s −2 a b t π t,a (µ t , σ t ) −1 σ −2 t,a µ t,a + s −2 a b t G t,a .
We now derive expressions for the conditional mean and variance of µ t+1 . Noting that
E[G t,a | µ t , σ t ] = E[E[G t,a | h, µ t , σ t ] | µ t , σ t ] = E[π t,a (µ t , σ t )h a | µ t , σ t ] = π t,a (µ t , σ t )µ t,a ,
we conclude E[µ t+1,a | µ t , σ t ] = σ 2 t+1,a σ −2 t,a µ t,a + s −2 a b t π t,a (µ t , σ t )µ t,a = µ t,a . Next, use the law of total variance to derive
Var(µ t+1,a | µ t , σ t ) = σ 2 t+1,a s −2 a b t 2 Var(G t,a | µ t , σ t ) = σ 2 t+1,a s −2 a b t 2 E[Var(G t,a | h, µ t , σ t ) | µ t , σ t ] + Var(E[G t,a | h, µ t , σ t ] | µ t , σ t ) = σ 2 t+1,a s −2 a b t 2 π t,a (µ t , σ t )s 2 a b t + π t,a (µ t , σ t ) 2 σ 2 t,a = σ 4 t,a b t π t,a (µ t , σ t ) s 2 a + σ 2 t,a b t π t,a (µ t , σ t )
.</p>
<p>We arrive at the desired result (30) since
σ 2 t,a − σ 2 t+1,a = σ 2 t,a − σ −2 t,a + s −2 a b t π t,a (µ t , σ t ) −1 = σ 4 t,a b t π t,a (µ t , σ t ) s 2 a + σ 2 t,a b t π t,a (µ t , σ t ) .(31)</p>
<p>C.2 Proof of Corollary 4</p>
<p>The posterior states (µ n,t , σ n,t ) can be expressed as a function of the sample mean estimators and the propensity scores
σ −2 n,t+1 = σ −2 0 + t v=0 b v π v (µ n,v , σ n,v )s −2 µ n,t+1 = σ 2 n,t+1 σ −2 0 µ 0 + t v=0 b v s −2 √ nR n v .
where the operations are vector-wise. Since the allocations π v are assumed to be continuous in the posterior state (µ n,v , σ n,v ), the states (µ n,t+1 , σ n,t+1 ) are continuous functions of the sample mean estimators √ nR n v . By the continuous mapping theorem, we conclude (µ n,0 , σ n,0 , . . . , µ n,T −1 , σ n,T −1 ) d ⇝ (µ 0 , σ 0 , . . . , µ T −1 , σ T −1 ).</p>
<p>C.3 Proof of Lemma 2</p>
<p>At each fixed epoch t ∈ [T ], we consider imagined counterfactual state transitions if one follows future allocationsρ t,a , . . . ,ρ T −1,a that only depend on currently available information (µ t , σ t ). Using the same notation for counterfactual states to simplify the exposition, Lemma 1 shows that subsequent counterfactual states will be governed by
σ −2 v+1,a := σ −2 v,a + s −2 a b vρv,a (32a) µ v+1,a := µ v,a + σ 2 v,a − σ 2 v+1,a 1 2 Z v,a = µ v,a + σ v,a b vρv,a (µ t , σ t )σ 2 v,a s 2 a + b vρv,a (µ t , σ t )σ 2 v,a Z v,a .(32b)
First, we rewrite the objective (20) as a function of a single Gaussian variableZ. Recalling the identity (31)
σ 2 v,a − σ 2 v+1,a 1 2 Z v,a = σ v,a b vρv,a (µ t , σ t )σ 2 v,a s 2 a + b vρv,a (µ t , σ t )σ 2 v,a Z v,a ,
use the recursive relation (32a) to write
Var T −1 v=t σ v,a b vρv,a (µ t , σ t )σ 2 v,a s 2 a + b vρv,a (µ t , σ t )σ 2 v,a Z v,a = T −1 v=t σ 2 v,a − σ 2 v+1,a = σ 2 t,a − σ 2 T,a = σ 2 t,a − σ −2 t,a + T −1 v=t b vρv,a (µ t , σ t ) s 2 a −1 = σ 4 t,a T −1 v=t b vρv,a (µ t , σ t ) s 2 a + σ 2 t,a T −1 v=t b vρv,a (µ t , σ t )
.
Thus, we conclude E max a µ t,a + T −1 v=t σ v,a σ 2 v,a b vρv,a (µ t , σ t ) s 2 a + σ 2 v,a b vρv,a (µ t , σ t ) Z v,a µ t , σ t = E   max a    µ t,a + σ 4 t,a T −1 v=tρ v,a (µ t , σ t )b v s 2 a + σ 2 t,a T −1 v=tρ v,a (µ t , σ t )b vZ a    µ t , σ t   .
Abusing notation, we replaceZ with Z t in the final expectation. Note that for any sequence of future allocationsρ t , . . . ,ρ T −1 that only depend on (µ t , σ t ), the
change of variablesρ a = T −1 v=tρ v,abv T −1 v=t bv give Vρ t:T −1 t (µ t , σ t ) = E   max a    µ t,a + σ 4 t,a T −1 v=tρ v,a b v s 2 a + σ 2 t,a T −1 v=tρ v,a b vZ a    µ t , σ t   = E max a µ t,a + σ 4 t,aρ abt s 2 a + σ 2 t,aρ abtZ a µ t , σ t = Vρ t (µ t , σ t ) whereb t = T −1 v=t b v and Vρ t (µ t , σ t )
is the value function of the constant allocationρ. Thus, for any sequence of future allocations that only depend on (µ t , σ t ), there exists a constant allocation ρ(µ t , σ t ) that achieves the same performance.</p>
<p>C.4 Proof of Proposition 2</p>
<p>By Lemma 2, for any future allocationπ t:T = (π t (µ t , σ t ), . . . ,π T −1 (µ t , σ t )) that only depends on currently available information (µ t , σ t )), there is a constant allocation that matches the same performance. Thus, it is sufficient to show that the value function for ρ dominates the value function of any constant allocationπ v (µ t , σ t ) ≡ρ(µ t , σ t ) for v ≥ t. Proceeding by induction, observe for the base case that when t = T − 1, the definition (20) of the policy ρ t implies
V ρ T −1 (µ T −1 , σ T −1 ) = max ρ∈∆ K Vρ T −1 (µ T −1 , σ T −1 )
for all (µ T −1 , σ T −1 ). Next, as an inductive hypothesis, suppose
V ρ t+1 (µ t+1 , σ t+1 ) ≥ max ρ∈∆ K Vρ t+1 (µ t+1 , σ t+1 ) for all (µ t+1 , σ t+1 ).
Then, for any (µ t , σ t )
V ρ t (µ t , σ t ) = E t V ρ t+1 (µ t+1 , σ t+1 ) ≥ E t max ρ∈∆ K Vρ t+1 (µ t+1 , σ t+1 ) ≥ max ρ∈∆ K E t Vρ t+1 (µ t+1 , σ t+1 )(33)
where we abuse notation to denote by (µ t+1 , σ t+1 ) the state transition under the policy ρ t (15)
σ −2 t+1,a := σ −2 t,a + s −2 a b t ρ t,a (µ t , σ t ) (34a) µ t+1,a := µ t,a + σ 2 t,a − σ 2 t+1,a 1 2 Z t,a = µ t,a + σ t,a b t ρ t,a (µ t , σ t )σ 2 t,a s 2 a + b t ρ t,a (µ t , σ t )σ 2 t,a Z t,a .(34b)
We now derive a more explicit representation for E t Vρ t+1 (µ t+1 , σ t+1 ) , which is the value function for a policy that follows ρ t at time t and the constant policyρ onwards. By the expression (20) in Lemma 2, we have 
E t Vρ t+1 (µ t+1 , σ t+1 ) = E t max a µ t+1Z t+1,a = σ 4 t,a ρ abt+1 + ρ t,a (µ t , σ t )b t s 2 a + σ 2 t,a ρ abt+1 + ρ t,a (µ t , σ t )b t ,
we arrive at the identity
E t Vρ t+1 (µ t+1 , σ t+1 ) = E t max a µ t,a + σ 4 t,a ρ abt+1 + ρ t (µ t , σ t )b t s 2 a + σ 2 t,a ρ abt+1 + ρ t (µ t , σ t )b t Z t,a .
Recalling the bound (33), useρ = ρ t (µ t , σ t ) to conclude
V ρ t (µ t , σ t ) ≥ max ρ∈∆ K E t max a µ t,a + σ 4 t,a ρ abt+1 + ρ t (µ t , σ t )b t s 2 a + σ 2 t,a ρ abt+1 + ρ t (µ t , σ t )b t Z t,a ≥ E t max a µ t,a + σ 4 t,a ρ t,a (µ t , σ t )b t s 2 a + σ 2 t,a ρ t,a (µ t , σ t )b t Z t,a = max ρ∈∆ K Vρ t (µ t , σ t ),
where we used the definition of ρ t in expression (20) in the final line.</p>
<p>C.5 Distilled Residual Horizon Optimization</p>
<p>Instead of computing ρ t (µ t , σ t ) as the experiment progresses, it can sometimes be convenient to pre-compute the mapping ρ t (·, ·) so that it can be readily applied for any observed current state. By reformulating the optimization problem (20) as a stochastic optimization problem over functions of (µ t , σ t ), we propose a distilled variant of the RHO policy that can learn the mapping ρ t (·, ·) in a fully offline manner. 
(µ t , σ t ) ∈ ∆ K maximize ρ(·) measurable E (µt,σt)∼Pt max a µ t,a + σ 4 t,a ρ(µ t , σ t )b t s 2 a + σ 2 t,a ρ(µ t , σ t )b t Z t,a ,(35)
where P t is any probability measure with support R K × (0, σ 2 0 ] K andb t := T −1 v=t b v . Proposition 5 allows us to use standard ML best practices to (approximately) solve the reward maximization problem (20). We parameterize the policy using black-box ML models
{ρ θ,t (µ t , σ t ) : θ ∈ Θ t } ,
and use stochastic gradient-based optimization algorithms to solve for the optimal parameterization (Algorithm 1). In particular, the model training problem (35) can be approximated by neural networks and optimized through standard auto-differentiation frameworks [1,92] for computing stochastic (sub)gradients. By the envelope theorem, the stochastic (sub)gradient at the sample (µ t , σ t ) ∼ P t is given by
Z t,a ⋆ ∂ θ σ 4 t,a ⋆ ρ θ,t (µ t , σ t )b t s 2 a ⋆ + σ 2 t,a ⋆ ρ θ,t (µ t , σ t )b t ,
where a ⋆ ∈ argmax a µ t,a + σ 4 t,a ρ θ,t (µt,σt)bt s 2 a +σ 2 t,a ρ θ,t (µt,σt)bt Z t,a . Proof It remains to show that minimizing over all measurable functions ρ(µ t , σ t ) ∈ ∆ K gives pointwise suprema for each (µ t , σ t ) almost surely
sup ρ(·) measurable E (µt,σt)∼Pt max a µ t,a + σ 4 t,a ρ(µ t , σ t )b t s 2 a + σ 2 t,a ρ(µ t , σ t )b t Z t,a = E (µt,σt)∼Pt sup ρ E max a µ t,a + σ 4 t,a ρb t s 2 a + σ 2 t,a ρb t Z t,a µ t , σ t .
We use normal integrand theory [97,Section 14.D] to interchange the integral and supremum over measurable mappings. Recall that a map f : ∆ K × (R K × (0, σ 0 ] K ) →R is a normal integrand if its epigraphical mapping-viewed as a set-valued mapping-S f :
R K × (0, σ 0 ] K → R × R, (µ t , σ t ) → epi f (·; µ t , σ t ) = {(ρ, α) ∈ ∆ K × R : f (ρ; µ t , σ t ) ≤ α} is closed-valued. That is, S f (µ t , σ t )
is closed for all (µ t , σ t ) ∈ R K × (0, σ 0 ] K and measurable (i.e., for any open set O ⊆ R K × (0, σ 0 ] K , [97,Theorem 14.60]). If f : ∆ K × (R K × (0, σ 0 ] K ) →R is a normal integrand, and R K ×(0,σ 0 ] K f (ρ(µ t , σ t ); µ t , σ t ) dP t (µ t , σ t ) &lt; ∞ for some measurable ρ(·), then
S −1 f (O) := ∪ o∈O S −1 f (o) is measurable).</p>
<p>Lemma 8 (Rockafellar and Wets
inf ρ(·) R K ×(0,σ 0 ] K f (ρ(µ t , σ t ); µ t , σ t ) dP t (µ t , σ t ) ρ : R K × (0, σ 0 ] K → R measurable = R K ×(0,σ 0 ] K inf ρ∈∆ K f (ρ; µ t , σ t ) dP t (µ t , σ t ).
If this common value is not −∞, a measurable function ρ * : R K × (0, σ 0 ] K → R attains the minimum of the left-hand side iff ρ * (µ t , σ t ) ∈ argmin ρ∈∆ K f (ρ; µ t , σ t ) for P t -almost every (µ t , σ t ) ∈
R K × (0, σ 0 ] K .
Define the function
f (ρ; µ t , σ t ) := −E Zt∼N (0,I) max a µ t,a + σ 4 t,a ρ abt s 2 a + σ 2 t,a ρ abt Z t,a .
Since f is continuous in (ρ, µ t , σ t ) by dominated convergence, f is a normal integrand [97,Examples 14.31]. Applying Lemma 8, we obtain the desired result.</p>
<p>D Proofs of large horizon results</p>
<p>D.1 Proof of Theorem 3</p>
<p>The proof of Theorem 3 first relies on a characterization of the gradient of the objective as T − t → ∞.</p>
<p>Proposition 6. Consider any fixed epoch t and state (µ, σ). Let θ a := µ a + σ a Z a be the random variable representing the posterior belief of the average reward for arm a, and let θ * a := max a ′ ̸ =a θ a ′ . Suppose that as T − t grows large, the residual sample budgetb t = T −1 s=t b s → ∞. Then gradient and Hessian of the planning objective Vρ t (µ, σ) with respect to the allocationρ converge as follows:
lim T →∞b t ∇Vρ t (µ, σ) = diag s 2 a 2ρ 2 a E 1 σ a ϕ θ * a − µ a σ a lim T →∞b t ∇ 2 Vρ t (µ, σ) = −diag s 2 a 2ρ 3 a E 1 σ a ϕ θ * a − µ a σ a
where ϕ(·) is the standard normal density.</p>
<p>Proof</p>
<p>We characterize the asymptotic behavior of the gradient and hessian of Vρ t (µ, σ) = E t [max a µ T,a ] with respect toρ as the residual horizon T − t grows large. The residual horizon affects the planning problem through the residual batch sizeb t , which is assumed to converge to infinity as T − t → ∞. Rewrite the planning objective Vρ t (µ, σ) with a rescaled measurement variance s 2 a /b t
Vρ t (µ, σ) = E t max a µ t,a + σ 4 t,aρ a s 2 a /b t + σ 2 t,aρ a Z t,a ,
It is sufficient to understand the behavior of Vρ t (µ, σ) as the measurement variance goes to zero. For ease of notation we define α := 1/b t , and consider α as a scaling parameter of the measurement variance that converges to zero. Observe Vρ t (µ, σ) can be expressed as the composition Vρ t (µ, σ) = g φ ρ, σ 2 , αs 2 where
g(v) := E[max a µ a + v a Z a ]
φ a (ρ a , σ 2 a , s 2 a ) := σ a ρ a σ 2 a s 2 a +ρ a σ 2 a and φ, σ 2 , s 2 refer to the vectorized versions. The function g(v) describes the behavior of the max of Gaussian random variables under standard deviations v, and φ characterizes how the standard deviation of the update changes with the sampling probabilitiesρ. Through the chain rule, it is sufficient to characterize the gradient and hessian of g, φ. Frazier and Powell [35] derived expressions for the gradient and hessian of g(v).</p>
<p>Lemma 9 (Frazier and Powell [35]).
Letting W a := max a ′ ̸ =a µ a ′ + v a ′ Z a ′ , ∂ ∂v a g(v) = E Z a 1 a = arg max a ′ µ a ′ + v a ′ Z a ′ = E ϕ W a − µ a v a ∂ 2 ∂v 2 a g(v) = E W a − µ a v 3 a ϕ W a − µ a v a ∂ 2 ∂v a ∂v a ′ g(v) = E − W a − µ a v a ϕ W a − µ a v a Z a ′ 1 a ′ = argmax a̸ =a µâ + vâZâ
The first and second derivatives of φ are given by .</p>
<p>We first study the behavior of φ and its derivatives as α → 0 φ a (ρ a , σ 2 a , αs 2 a ) → σ a , ∂ ∂ρ a φ a (ρ a , σ 2 a , αs 2 a ) ∼
αs 2 a 2ρ 2 a σ a , ∂ 2 ∂ρ 2 a φ a (ρ a , σ 2 a , αs 2 a ) ∼ − αs 2 ā ρ 3 a σ a .
To analyze g(φ(ρ, σ 2 , αs 2 )), recall that θ a = µ a + σ a Z a is the random variable representing the posterior belief of the average reward of arm a. For any realization of Z ∼ N (0, I), we have
W a = max a ′ ̸ =a µ a ′ + φ a ′ (ρ a ′ , σ 2 a ′ , αs 2 a ′ )Z a ′ → θ * a ,
so dominated convergence implies lim α→0 ∂ ∂v a g(φ(ρ, σ 2 , αs 2 )) = E ϕ θ * a − µ a σ a lim α→0 ∂ 2 ∂v 2 a g(φ(ρ, σ 2 , αs 2 )) = E θ * a − µ a σ 3 a · ϕ θ * a − µ a σ a lim α→0 ∂ 2 ∂v a ∂v a ′ g(φ(ρ, σ 2 , αs 2 )) = E − θ * a − µ a σ a · ϕ θ * a − µ a σ a Z a ′ 1 a ′ = arg max a̸ =a θâ By the chain rule, ∇ρVρ t (µ, σ) = ∇ v g(φ(µ, σ 2 , αs 2 ))∇ρφ(ρ, σ 2 , αs 2 ) where the Jacobian ∇ρφ is ∇ρφ(ρ, σ 2 , αs 2 ) := diag ∂ ∂ρ a φ a (ρ a , σ 2 a , αs 2 a ) .</p>
<p>Rescaling the planning objective Vρ t by 1/α, we obtain the following limit
lim α→∞ α −1 ∇ρVρ t (µ, σ) = s 2 a 2ρ 2 a σ a E ϕ θ * a − µ a σ a
As for the Hessian, the chain rule implies
∂ 2 Vρ t ∂ρ a ∂ρ a ′ = i ∂g(φ(ρ)) ∂v i ∂ 2 φ i ∂ρ a ∂ρ a ′ + i,j ∂g(φ(ρ)) ∂v i ∂v j ∂φ i ∂ρ a ∂φ j ∂ρ a ′ = ∂g(φ(ρ)) ∂v a ∂ 2 φ a ∂ρ 2 a 1{a = a ′ } + ∂g(φ(ρ)) ∂v a ∂v a ′ ∂φ a ∂ρ a ∂φ a ′ ∂ρ a ′ ,
where we suppress the dependence of φ on other arguments for succinctness. Rewriting this using ∇ 2 ρ φ := diag ∂ 2 φa ∂ρ 2 a , we get ∇ 2 ρ Vρ t (µ, σ) = diag(∇ v g(φ(ρ)))∇ 2 ρ φ + ∇ρφ∇ 2 v g(φ(ρ))∇ρφ. Collecting the above results, we have This leads us to the following corollary, which uses the characterization of the Hessian to show that the objective becomes strongly concave as T − t → ∞.
lim α→0 α −1 diag(∇ v g(φ(ρ)))∇ 2 ρ φ = −diag s 2 ā ρ 3 a σ a E ϕ θ * a − µ a σ a lim α→0 α −1 ∇ρφ∇ 2 v g(φ(ρ))∇ρφ = lim α→0 α −1 diag αs 2 a 2ρ a 2 σ a ∇ 2 v g(φ(ρ))diag
Corollary 5. Let ∆ ϵ K = ∆ K ∩ {p : p ≥ ϵ} be the truncated simplex. For a fixed epoch t and state (µ, σ), there exists T 0 (ϵ) &gt; 0 such that ∀T − t &gt; T 0 ,ρ →b t Vρ t (µ, σ) is strongly concave on ∆ ϵ K . Proof In the proof of Proposition 6, we showed that as the (scaled) measurement variance αs 2 a converges to zero, the Hessian α −1 ∇ 2 ρ Vρ t (µ, σ) can be expressed as a negative-definite diagonal matrix plus a matrix that converges to zero. As long as the sampling probabilities are bounded below, the convergence is uniform. We have the following bound for the largest eigenvalue of the Hessian λ max ∇ 2 ρ α −1 Vρ t (µ, σ) ≤ λ max α −1 diag(∇ v g(φ(ρ)))∇ 2 ρ φ + λ max α −1 ∇ρφ∇ 2 v g(φ(ρ))∇ρφ
≤ − min a s 2 a 2σ a ϵ 3 E ϕ θ * a − µ a σ a + αC/ϵ 4 ,
where C is a constant that depends only on µ, σ, s. So as α → 0, the Hessian will be negativedefinite for allρ ∈ ∆ ϵ K . Thus, for a large enough residual batch sizeb t the planning objective will be strongly concave on the truncated simplex ∆ ϵ K for some threshold ϵ.</p>
<p>Finally, by strong concavity, the KKT conditions of the planning problem have a unique solution. By the implicit function theorem, we can characterize the limit of this solution.</p>
<p>Proposition 7. Consider a fixed epoch t and state (µ, σ). Suppose there exists T 1 such that for ∀T − t &gt; T 1 , the RHO allocation satisfies ρ t,a (µ, σ) &gt; ϵ. Then as T − t → ∞, ρ t,a (µ, σ) → π DTS a (µ, σ), where π DTS a (µ, σ) ∝ s a ∂ ∂µ a π TS a (µ, σ)</p>
<p>1/2</p>
<p>Proof As in the proof of Proposition 6, we let α =b −1 t . Let α 0 to be the threshold such that for α &lt; α 0 the objective is strongly concave (such a threshold exists from Corollary 5), and let α 1 &lt; α 0 to be the threshold such that for α &lt; α 1 the unique solution is in the interior of ∆ ϵ K (such a threshold exists by hypothesis).</p>
<p>For α small enough, the (scaled) KKT conditions give
α −1 ∇ρVρ t (µ, σ) − λ1 = 0, 1 ⊤ ρ t = 1,(36)
where λ is the optimal dual variable for the equality constraint. The Jacobian of this equation with respect to (ρ, λ) is the following block matrix
∇ 2 ρ α −1 Vρ t (µ, σ) −1 1 ⊤ 0
Since α &lt; α 0 , the Hessian of α −1 Vρ t (µ, σ) is negative definite and invertible in a neighborhood of α = 0. The Jacobian is also invertible since the Schur complement 0 − 1 ⊤ α −1 ∇ 2 ρ E max a µ T,a (−1) &lt; 0 is invertible. Moreover, α −1 ∇ρVρ t (µ, σ) is continuously differentiable in α at α = 0. So by the Implicit Function Theorem, there exists a α 2 such that for all α &lt; α 2 , the solution satisfying the KKT conditions (36),ρ(α) and λ(α)), is continuous in α. Thus, as α → 0, the unique maximizer of α −1 Vρ t (µ, σ) converges toρ ⋆ satisfying the limiting version of the KKT conditions (36). Using the explicit expression for α −1 ∇ρVρ t (µ, σ) obtained in Proposition 6, we concludē</p>
<p>D.2 More details on Density Thompson Sampling</p>
<p>Similar to TS, DTS samples from the posterior distribution; while TS asymptotically assigns all sampling effort to the best arm, DTS does not oversample the best arm, as it is based on the gap between the second best arm. Concretely, the relative sampling proportions π DTS a /π DTS a ′ can be expressed as ratios of the 'index' E 1 σa ϕ θ * a −µa σa 1/2</p>
<p>; the higher the index, the more sampling effort allocated to that arm during the epoch. This index decreases at an exponential rate for all arms as the experiment progresses and the posterior uncertainty shrinks σ a → 0.</p>
<p>Proposition 8. Consider a fixed state (µ, σ) and suppose arms are sorted in decreasing order of their means µ 1 &gt; µ 2 &gt; . . . &gt; µ K . Then we have that:
   − (µ 1 −µ 2 ) 2 2(σ 2 1 +σ 2 2 )
top two arms a ∈ 1, 2
− (µ 1 −µa) 2 2σ 2 a otherwise ≲ log E 1 σ a ϕ θ * a − µ a σ a ≲ − min a ′ ̸ =a (µ a ′ − µ a ) 2 2(σ 2 a + σ 2 a ′ )
where ≲ contains additive terms that are logarithmic in min a σ a .</p>
<p>If an arm is excessively under-sampled, its index will eventually be larger than others and so sampling effort will be spread out across arms rather than concentrating on the best one. This makes DTS better suited for best-arm identification and closer to Top-Two Thompson Sampling [98]. Proof Suppose that the arms are sorted so that µ 1 &gt; ... &gt; µ K . Since θ * a is the max of independent random variables, P(θ * a ≤ x) = a ′ ̸ =a P(θ a ′ ≤ x) = a ′ ̸ =a Φ</p>
<p>x−µ a ′ σ a ′ . This means that the density of θ * a , denoted as f * a (x), can be expressed as (by the Leibniz product rule)
f * a (x) = d dx a ′ ̸ =a Φ x − µ a ′ σ a ′ = a ′ ̸ =a d dx Φ x − µ a ′ σ a ′ â / ∈{a,a ′ } Φ x − µâ σâ = a ′ ̸ =a 1 σ a ′ ϕ x − µ a ′ σ a ′ â / ∈{a,a ′ } Φ x − µâ σâ .
Thus, we can write the expectation as
E 1 σ a ϕ θ * a − µ a σ a = ∞ −∞ 1 σ a ϕ x − µ a σ a f * a (x)dx = ∞ −∞ 1 σ a ϕ x − µ a σ a a ′ ̸ =a   1 σ a ′ ϕ x − µ a ′ σ a ′ â / ∈{a,a ′ } Φ x − µâ σâ   dx = a ′ ̸ =a ∞ −∞ 1 σ a ϕ x − µ a σ a 1 σ a ′ ϕ x − µ a ′ σ a ′ â / ∈{a,a ′ } Φ x − µâ σâ dx.
For any arm a, we can obtain an upper bound for each term in the sum by bounding Φ x−µâ σâ ≤ 1 and directly integrating
∞ −∞ 1 σ a ϕ x − µ a σ a 1 σ a ′ ϕ x − µ a ′ σ a ′ â / ∈{a,a ′ } Φ x − µâ σâ dx ≤ ∞ −∞ 1 σ a ϕ x − µ a σ a 1 σ a ′ ϕ x − µ a ′ σ a ′ dx = 1 2π(σ 2 a ′ + σ 2 a ) e − (µ a ′ −µa) 2 2(σ 2 a ′ +σ 2 a ) .
Finally, taking a max and summing up the terms gives
E 1 σ a ϕ θ * a − µ a σ a ≤ (K − 1) max a ′ ̸ =a 1 2π(σ 2 a ′ + σ 2 a ) e − (µ a ′ −µa) 2 2(σ 2 a ′ +σ 2 a ) 1 σ 1 ϕ x − µ 1 σ 1 dx = e − (µa−µ 1 ) 2 2(σ 2 a +σ 2 1 ) 2π(σ 2 a + σ 2 1 )Φ σ 1 σ a µ 1 − µ a σ 2 a + σ 2 1
Using the Gaussian tail boundΦ(x) ≥ ϕ(x) 2x for x &gt; 0,
E 1 σ a ϕ θ * a − µ a σ a ≥ 1 2 K−2 ∞ µ 1 1 σ a ϕ x − µ a σ a 1 σ 1 ϕ x − µ 1 σ 1 dx ≥ 1
2π(σ 2 a + σ 2 1 ) e − (µa−µ 1 ) 2 2(σ 2 a +σ 2 1 ) For a = 2, we can evaluate this integral explicitly
∞ µ 2 1 σ 2 ϕ x − µ 2 σ 2 1 σ 1 ϕ x − µ 1 σ 1 dx = 1 2 2π(σ 2 1 + σ 2 2 ) e − (µ 1 −µ 2 ) 2 2(σ 2 1 +σ 2 2 ) 1 + erf (µ 1 − µ 2 )σ 2 √ 2σ 1 σ 2 1 + σ 2 2 .
Note that for positive values of the error function it is bounded below by zero. This gives Repeating these steps for a = 1 (taking a ′ = 2) results in the same lower bound.</p>
<p>E Implementation Details</p>
<p>We first discuss the implementation details for solving the Residual Horizon Optimization planning problem (20). Recall that the problem involves maximizing the expectation of future posterior means over constant sampling allocations in the simplex ∆ K : maximizē ρ∈∆ K Vρ t (µ t , σ t ) = E t max a µ t,a + σ 4 t,aρ abt s 2 a + σ 2 t,aρ abt Z t,a .</p>
<p>We approximate the expectation in the objective function by a sample average approximation with N standard normal random vectors. where Z t,a,1 , ..., Z t,a,N are iid draws of N (0, 1) random variables. We use quasi-Monte Carlo methods for variance reduction and draw the normal random variables from a Sobol sequence, which is widely used in practice in Bayesian Optimizaton [11]. There are many methods for solving this constrained optimization problem (e.g. projected gradient descent). We use a softmax parameterization of the simplexρ a ∝ e va , and use unconstrained stochastic gradient methods to optimize over v. We observe that vanilla stochastic gradient descent gets stuck at sub-optimal allocations that allocate all sampling effort to one treatment arm. We obtain much better performance from approximate second-order methods such as Adam [69] or L-BFGS [81], and use Adam for the experimental evaluation.</p>
<p>For the policy gradient method, we parameterize the policy as a feed-forward neural network with 2 hidden layers with 512 units in each layer. The network uses the rectified non-linearity [41] for all hidden layers. We pass the posterior means µ ∈ R K , the posterior variances σ 2 ∈ R K , the current epoch t ∈ N, and the measurement variance s 2 ∈ R K as inputs. The output of the network is passed through a softmax layer and so the final output is a sampling allocationρ ∈ ∆ K . We train the network with the Adam optimizer with learning rate 5.0 × 10 −6 and (β 1 , β 2 ) = (0.9, 0.999) in minibatches of size 50. Minibatches are drawn by randomly generating priors (µ 0 , σ 0 ).</p>
<p>Figure 1 .
1Gaussian sequential experiment with T = 3 reallocation epochs. Bar visualizes sampling allocations at each epoch and bell curves depict normal approximations of the aggregated reward (sample mean) distribution.</p>
<p>Figure 2 .
2Relative gains over the uniform allocation as measured by the Bayes simple regret for the finite batch problem with K = 100 treatment arms. Individual rewards are Bernoulli with a Beta prior. Despite relying on normal approximation of aggregate rewards over a batch, RHO delivers substantial performance gains even over oracle algorithms that know the true reward model. These gains persist even for small batch sizes inFigure 2awhere batch size is equal to the number of arms.</p>
<p>Lemma 1 .
1Let h ∼ N (µ 0 , σ 2 0 ) and Z 0 , . . . , Z T −1 iid ∼ N (0, I K ) be standard normal variables. The system governed by the posterior updates (13) has the same joint distribution as that with the following state transitions</p>
<p>Figure 4 .
4Comparison of Gaussian batch policies. Relative gains over the uniform allocation as measured by the Bayes simple regret for the finite batch problem with K = 100 treatment arms and batches of size b t n = 10, 000. Gamma-Gumbel experiment where individual rewards are Gumbel with a Gamma prior. RHO maintains strong performance for small and long horizon experiments, as well as for low and high noise levels. Myopic performs well in short horizon experiments, but worsens in longer horizon experiments due to insufficient exploration. The Thompson sampling policies are effective for low measurement noise but their performance degrades in underpowered settings.</p>
<p>Figure 5 .
5Comparison of sampling allocations. (a) displays posterior means µ and standard deviations σ (the length of each whisker) for K = 5 arms. The other graphs illustrate the sampling allocation computed by different policies given posterior belief (µ, σ). (b) shows the sampling allocation produced by the Gaussian Thompson sampling policy, (c) is RHO when there is only 1 epoch left, and (d) is RHO when the residual horizon is T − t = 10. RHO calibrates exploration to the length of the remaining horizon; when there is only 1 epoch left, the policy gives the Bayes optimal allocation focusing solely on the top two arms. When there are many reallocation epochs left, the policy explores other arms more and resembles the Thompson sampling allocation.</p>
<p>Theorem 3 .
3Consider any fixed epoch t and state (µ, σ). Let ∆ ϵ K = ∆ K ∩ {p : p ≥ ϵ} be the truncated simplex. Suppose that as T −t grows large, the residual sample budgetb t = T −1 s=t b s → ∞.1. Asb t → ∞, the gradient scales as ∇ρVρ t (µ, σ) ∼ c</p>
<p>Figure 6 .
6Comparison of performance across different number of treatment arms. Relative gains over the uniform allocation as measured by the Bayes simple regret for the finite batch problem with a batch size of b t n = 100. Beta-Bernoulli experiment where individual rewards are Bernoulli with a Beta prior.</p>
<p>Figure 7 .
7Comparison of performance across different measurement noise levels. Relative gains over the uniform allocation as measured by the Bayes simple regret. Gamma-Gumbel experiment where rewards follow Gumbel distributions with measurement variances s 2 a ∈ {0.2, 1, 5}. There are K = 100 treatment arms with T = 10 batches of size b t n = 100.</p>
<p>•
Oracle Thompson Sampling [60, 62]: Beta-Bernoulli TS with batch updates. • Oracle Top-Two Thompson Sampling [98]: Beta-Bernoulli Top-Two TS with batch updates. • Residual Horizon Optimization: Selects the allocation by solving the planning problem (20), as in Algorithm 1.</p>
<p>size btn = 10, 000.</p>
<p>Figure 8 .
8Comparison of performance across batch sizes. Relative gains over the uniform allocation as measured by the Bayes simple regret for the finite batch problem with K = 100 treatment arms. Gamma-Gumbel experiment where individual rewards are Gumbel with a Gamma prior.</p>
<p>Figure 9 .
9Comparison of performance across different priors. Relative gains over the uniform allocation as measured by the Bayes simple regret for the finite batch problem for K = 100 arms and T = 10 batches of size b t n = 100 samples. Beta-Bernoulli experiment where individual rewards are Bernoulli with various Beta priors.</p>
<p>Figure 10 .
10Comparison of performance across variance perturbations. Relative gains over the uniform allocation as measured by the Bayes simple regret for the finite batch problem with K = 100 treatment arms, batch size n = 10, 000 in the Gamma-Gumbel experiment. Despite drastic variance mis-measurement (ς = 1), RHO maintains performance benefits over other Bayesian policies (except Policy Gradient).</p>
<p>ς 2 ) for ς ∈ {0.25, 0.5, 1.0}. The 50% confidence intervals for Y i under these distributions are ς = 0.25 : 50% CI [0.84, 1.18] ς = 0.50 : 50% CI [0.75, 1.40] ς = 1.00 : 50% CI [0.50, 1.96]</p>
<p>r 0:t )Z + hπ t+1 (r 0:t )</p>
<p>≤
(b t+1 n) −1/2 √ 2 max a h 2 a /s 2 t+1,a 1 − max a h 2 a /(s 2 a + h</p>
<p>√ nR n 0
0:t )] is bounded by C o , which gives us a n −1/2 upper bound on E[E r 0:t d BL (Q t+1 n , Σ1/2 n,t+1 (r 0:t )Z) ]. Finally, combining with the result in Lemma 4, there exists a constantC t+1 depending on K and polynomially on C o , s 2 , b t+1 , and h E[E r 0:t d BL ( r 0:t )Z + hπ t+1 (r 0:t )) ] ≤C t+1 n −1/2 uniformly over all r 0:t . Recall the decomposition (27) for any bounded Lipschitz function f and corresponding g, g n defined as in Eq. (25). The above result implies that |E[g n ( √ nR n 0 , ..., √ nR n t )] − E[g( √ nR n 0 , ..., √ nR n t )]| ≤Cn −1/2 for some constantC = max tCt . It remains to show convergence of E[bounded Lipschitz function. From the proof of Corollary B.2 we have that the Lipschitz constant for g is bounded by M = 1 +L(s 2 * /b * √ K + max a |h a |) whereL is the upper bound</p>
<p>is uniform in h under the hypothesized moment condition on h. The discontinuity points of π T are of measure zero with respect to (G 0 , . . . , G T −1 ), the convergence (3) and the Portmanteau theorem implies that for any fixed h, as n → ∞ E[π T ( √ nR n 0 , . . . , √ nR n T −1 )|h] → E[π T (G 0 , . . . , G T −1 )|h]</p>
<p>Proposition 5 .
5The solution to the Residual Horizon Optimization problem(20) coincides with the maximizer of the following reward maximization problem over measurable functions ρ</p>
<p>as in Algorithm 1Policy </p>
<p>Baseline </p>
<p>Reward 
Number Batch size 
Measurement 
Prior 
Distribution of Arms 
Noise 
Bernoulli 
100 
10,000 
0.2 
5 
Top-One Descending 
TS 
70.1 
80.9 
73.4 
69.1 
51.6 
89.1 
60.0 
62.8 
Top-Two TS 
68.7 
82.1 
74.1 
71.8 
46.8 
88.9 
59.3 
65.0 
TS+ 
70.9 
82.4 
76.6 
81.4 
63.3 
83.3 
63.0 
67.9 
Myopic 
66.5 
76.0 
64.3 
69.1 
49.7 
80.2 
60.0 
63.6 
Policy Gradient 
63.9 
79.1 
61.4 
63.6 
49.3 
79.8 
55.3 
59.6 
RHO 
58.8 
73.1 
55.8 
64.2 
41.5 78.6 
52.6 
56.6 </p>
<p>Plugging in the state transition (34b), we haveE t Vρ t+1 (µ t+1 , σ t+1 ) = E t max,a + 
σ 4 </p>
<p>t+1,aρ abt+1 </p>
<p>s 2 
a + σ 2 </p>
<p>t+1,aρ abt+1 </p>
<p>Z t+1,a 
. </p>
<p>a </p>
<p>µ t,a + σ 2 
t,a − σ 2 </p>
<p>t+1,a </p>
<p>1 </p>
<p>2 Z t,a + 
σ 4 </p>
<p>t+1,aρ abt+1 </p>
<p>s 2 
a + σ 2 </p>
<p>t+1,aρ abt+1 </p>
<p>Z t+1,a 
. </p>
<p>Noting that </p>
<p>Var t σ 2 
t,a − σ 2 </p>
<p>t+1,a </p>
<p>1 </p>
<p>2 Z t,a + 
σ 4 </p>
<p>t+1,aρ abt+1 </p>
<p>s 2 
a + σ 2 </p>
<p>t+1,aρ abt+1 </p>
<p>Acknowledgement We are indebted to Omar Besbes, Lin Fan and Daniel Russo for constructive feedback and helpful discussions. This research was partially supported by the Digital Futures Initiative.To get the lower bound, it suffices to lower bound a single a ′ term in the sum, since all terms are positive. We select the term involving a ′ = 1 (a ′ = 2 if a = 1). Since the integrand is positive, we obtain a further lower bound by truncating the integration fromOn the domain of integration,Evaluating the integral explicitly gives
TensorFlow: Large-scale machine learning on heterogeneous systems. M Abadi, A Agarwal, P Barham, E Brevdo, Z Chen, C Citro, G S Corrado, A Davis, J Dean, M Devin, S Ghemawat, I Goodfellow, A Harp, G Irving, M Isard, Y Jia, R Jozefowicz, L Kaiser, M Kudlur, J Levenberg, D Mané, R Monga, S Moore, D Murray, C Olah, M Schuster, J Shlens, B Steiner, I Sutskever, K Talwar, P Tucker, V Vanhoucke, V Vasudevan, F Viégas, O Vinyals, P Warden, M Wattenberg, M Wicke, Y Yu, X Zheng, M. Abadi, A. Agarwal, P. Barham, E. Brevdo, Z. Chen, C. Citro, G. S. Corrado, A. Davis, J. Dean, M. Devin, S. Ghemawat, I. Goodfellow, A. Harp, G. Irving, M. Isard, Y. Jia, R. Jozefowicz, L. Kaiser, M. Kudlur, J. Levenberg, D. Mané, R. Monga, S. Moore, D. Murray, C. Olah, M. Schuster, J. Shlens, B. Steiner, I. Sutskever, K. Talwar, P. Tucker, V. Vanhoucke, V. Vasudevan, F. Viégas, O. Vinyals, P. Warden, M. Wattenberg, M. Wicke, Y. Yu, and X. Zheng. TensorFlow: Large-scale machine learning on heterogeneous systems, 2015. URL https://www.tensorflow.org/. Software available from tensorflow.org.</p>
<p>Risk and optimal policies in bandit experiments. K Adusumilli, arXiv:2112.063632021econ.EMK. Adusumilli. Risk and optimal policies in bandit experiments. arXiv:2112.06363 [econ.EM], 2021.</p>
<p>Making contextual decisions with low technical debt. A Agarwal, S Bird, M Cozowicz, L Hoang, J Langford, S Lee, J Li, D Melamed, G Oshri, O Ribas, S Sen, A Slivkins, arXiv:1606.03966cs.LGA. Agarwal, S. Bird, M. Cozowicz, L. Hoang, J. Langford, S. Lee, J. Li, D. Melamed, G. Oshri, O. Ribas, S. Sen, and A. Slivkins. Making contextual decisions with low technical debt. arXiv:1606.03966 [cs.LG], 2016.</p>
<p>Making contextual decisions with low technical debt. A Agarwal, S Bird, M Cozowicz, L Hoang, J Langford, S Lee, J Li, D Melamed, G Oshri, O Ribas, arXiv:1606.03966arXiv preprintA. Agarwal, S. Bird, M. Cozowicz, L. Hoang, J. Langford, S. Lee, J. Li, D. Melamed, G. Oshri, O. Ribas, et al. Making contextual decisions with low technical debt. arXiv preprint arXiv:1606.03966, 2016.</p>
<p>Learning with limited rounds of adaptivity: Coin tossing, multi-armed bandits, and ranking from pairwise comparisons. A Agarwal, S Agarwal, S Assadi, S Khanna, Proceedings of the Thirtieth Annual Conference on Computational Learning Theory. the Thirtieth Annual Conference on Computational Learning TheoryPMLRA. Agarwal, S. Agarwal, S. Assadi, and S. Khanna. Learning with limited rounds of adaptiv- ity: Coin tossing, multi-armed bandits, and ranking from pairwise comparisons. In Proceed- ings of the Thirtieth Annual Conference on Computational Learning Theory, pages 39-75. PMLR, 2017.</p>
<p>Diffusion approximations for a class of sequential experimentation problems. V F Araman, R Caldentey, Management Science. 688V. F. Araman and R. Caldentey. Diffusion approximations for a class of sequential experi- mentation problems. Management Science, 68(8):5958-5979, 2022.</p>
<p>Multi-step budgeted bayesian optimization with unknown evaluation costs. R Astudillo, D Jiang, M Balandat, E Bakshy, P Frazier, Advances in Neural Information Processing Systems 21. R. Astudillo, D. Jiang, M. Balandat, E. Bakshy, and P. Frazier. Multi-step budgeted bayesian optimization with unknown evaluation costs. Advances in Neural Information Processing Systems 21, 2021.</p>
<p>A Atsidakou, S Katariya, S Sanghavi, B Kveton, arXiv:2211.08572Bayesian fixed-budget best-arm identification. 2022cs.LGA. Atsidakou, S. Katariya, S. Sanghavi, and B. Kveton. Bayesian fixed-budget best-arm identification. arXiv:2211.08572 [cs.LG], 2022.</p>
<p>Best arm identification in multi-armed bandits. J.-Y Audibert, S Bubeck, R Munos, Proceedings of the Twenty Third Annual Conference on Computational Learning Theory. the Twenty Third Annual Conference on Computational Learning TheoryJ.-Y. Audibert, S. Bubeck, and R. Munos. Best arm identification in multi-armed bandits. In Proceedings of the Twenty Third Annual Conference on Computational Learning Theory, pages 41-53, 2010.</p>
<p>Ae: A domain-agnostic platform for adaptive experimentation. E Bakshy, L Dworkin, B Karrer, K Kashin, B Letham, A Murthy, S Singh, Neural Information Processing Systems Workshop on Systems for Machine Learning. E. Bakshy, L. Dworkin, B. Karrer, K. Kashin, B. Letham, A. Murthy, and S. Singh. Ae: A domain-agnostic platform for adaptive experimentation. In Neural Information Processing Systems Workshop on Systems for Machine Learning, pages 1-8, 2018.</p>
<p>Botorch: A framework for efficient monte-carlo bayesian optimization. M Balandat, B Karrer, D R Jiang, S Daulton, B Letham, A G Wilson, E Bakshy, Advances in Neural Information Processing Systems 20. M. Balandat, B. Karrer, D. R. Jiang, S. Daulton, B. Letham, A. G. Wilson, and E. Bakshy. Botorch: A framework for efficient monte-carlo bayesian optimization. In Advances in Neural Information Processing Systems 20, 2020.</p>
<p>Near-optimal a-b testing. N Bhat, V F Farias, C C Moallemi, D Sinha, Management Science. 6610N. Bhat, V. F. Farias, C. C. Moallemi, and D. Sinha. Near-optimal a-b testing. Management Science, 66(10):4359-4919, 2020.</p>
<p>Rates of weak convergence for the multidimensional central limit theorem. R N Bhattacharya, Theory of Probability &amp; Its Applications. 15R. N. Bhattacharya. Rates of weak convergence for the multidimensional central limit theo- rem. Theory of Probability &amp; Its Applications, 15(1):68-86, 1970.</p>
<p>Regret analysis of stochastic and nonstochastic multi-armed bandit problems. Foundations and Trends in Machine Learning. S Bubeck, N Cesa-Bianchi, 5S. Bubeck and N. Cesa-Bianchi. Regret analysis of stochastic and nonstochastic multi-armed bandit problems. Foundations and Trends in Machine Learning, 5(1):1-122, 2012.</p>
<p>Pure exploration in multi-armed bandits problems. S Bubeck, R Munos, G Stoltz, International Conference on Algorithmic Learning Theory. SpringerS. Bubeck, R. Munos, and G. Stoltz. Pure exploration in multi-armed bandits problems. In International Conference on Algorithmic Learning Theory, pages 23-37. Springer, 2009.</p>
<p>Power failure: why small sample size undermines the reliability of neuroscience. K S Button, J Ioannidis, C Mokrysz, B A Nosek, J Flint, E S Robinson, M R Munafò, Nature Reviews Neuroscience. 145K. S. Button, J. Ioannidis, C. Mokrysz, B. A. Nosek, J. Flint, E. S. Robinson, and M. R. Munafò. Power failure: why small sample size undermines the reliability of neuroscience. Nature Reviews Neuroscience, 14(5):365-376, 2013.</p>
<p>Tight (lower) bounds for the fixed budget best arm identification bandit problem. A Carpentier, A Locatelli, Proceedings of the Twenty Ninth Annual Conference on Computational Learning Theory. the Twenty Ninth Annual Conference on Computational Learning TheoryPMLRA. Carpentier and A. Locatelli. Tight (lower) bounds for the fixed budget best arm identifi- cation bandit problem. In Proceedings of the Twenty Ninth Annual Conference on Computa- tional Learning Theory, pages 590-604. PMLR, 2016.</p>
<p>Bayesian experimental design: A review. K Chaloner, I Verdinelli, Statistical Science. 103K. Chaloner and I. Verdinelli. Bayesian experimental design: A review. Statistical Science, 10(3):273-304, 1995.</p>
<p>S Chatterjee, E Meckes, arXiv:0701464Multivariate normal approximation using exchangeable pairs. math.PRS. Chatterjee and E. Meckes. Multivariate normal approximation using exchangeable pairs. arXiv:0701464 [math.PR], 2008.</p>
<p>Simulation budget allocation for further enhancing the efficiency of ordinal optimization. C.-H Chen, J Lin, E Yucesan, S E Chick, Discrete Event Dynamic Systems. 10C.-H. Chen, J. Lin, E. Yucesan, and S. E. Chick. Simulation budget allocation for further enhancing the efficiency of ordinal optimization. Discrete Event Dynamic Systems, 10:251- 270, 2000.</p>
<p>Ranking and selection: efficient simulation budget allocation. Handbook of Simulation Optimization. C.-H Chen, S E Chick, L H Lee, N A Pujowidianto, C.-H. Chen, S. E. Chick, L. H. Lee, and N. A. Pujowidianto. Ranking and selection: efficient simulation budget allocation. Handbook of Simulation Optimization, pages 45-80, 2015.</p>
<p>Sequential design of experiments. H Chernoff, Annals of Mathematical Statistics. 303H. Chernoff. Sequential design of experiments. Annals of Mathematical Statistics, 30(3): 755-770, 1959.</p>
<p>Approaches in sequential design of experiments. H Chernoff, C A Stanford Univ, Dept, Statistics, Technical reportH. Chernoff. Approaches in sequential design of experiments. Technical report, STANFORD UNIV CA DEPT OF STATISTICS, 1973.</p>
<p>New two-stage and sequential procedures for selecting the best simulated system. S E Chick, K Inoue, Operations Research. 495S. E. Chick and K. Inoue. New two-stage and sequential procedures for selecting the best simulated system. Operations Research, 49(5):732-743, 2001.</p>
<p>Sequential sampling to myopically maximize the expected value of information. S E Chick, J Branke, C Schmidt, INFORMS Journal on Computing. 221S. E. Chick, J. Branke, and C. Schmidt. Sequential sampling to myopically maximize the expected value of information. INFORMS Journal on Computing, 22(1):71-80, 2010.</p>
<p>Adaptive design methods in clinical trials -a review. S.-C Chow, M Chang, Orphanet Journal of Rare Diseases. 311S.-C. Chow and M. Chang. Adaptive design methods in clinical trials -a review. Orphanet Journal of Rare Diseases, 3(11), 2008.</p>
<p>Parallel gaussian process optimization with upper confidence bound and pure exploration. E Contal, D Buffoni, A Robicquet, N Vayatis, Joint European Conference on Machine Learning and Knowledge Discovery in Databases. SpringerE. Contal, D. Buffoni, A. Robicquet, and N. Vayatis. Parallel gaussian process optimization with upper confidence bound and pure exploration. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, pages 225-240. Springer, 2013.</p>
<p>The dozen things experimental economists should do (more of). E Czibor, D Jimenez-Gomez, J A List, Southern Economic Journal. 862E. Czibor, D. Jimenez-Gomez, and J. A. List. The dozen things experimental economists should do (more of). Southern Economic Journal, 86(2):371-432, 2019.</p>
<p>Parallelizing exploration-exploitation tradeoffs in gaussian process bandit optimization. T Desautels, A Krause, J W Burdick, Journal of Machine Learning Research. 15T. Desautels, A. Krause, and J. W. Burdick. Parallelizing exploration-exploitation tradeoffs in gaussian process bandit optimization. Journal of Machine Learning Research, 15:3873-3923, 2014.</p>
<p>Regret bounds for batched bandits. H Esfandiari, A Karbasi, A Mehrabian, V Mirrokni, Proceedings of the Thirty-Eighth AAAI Conference on Artificial Intelligence. the Thirty-Eighth AAAI Conference on Artificial Intelligence35H. Esfandiari, A. Karbasi, A. Mehrabian, and V. Mirrokni. Regret bounds for batched bandits. In Proceedings of the Thirty-Eighth AAAI Conference on Artificial Intelligence, volume 35, pages 7340-7348, 2021.</p>
<p>Adaptive experiments for policy choice: Phone calls for home reading in kenya. B Esposito, A Sautmann, World Bank Policy Research Working Paper WPS10098. B. Esposito and A. Sautmann. Adaptive experiments for policy choice: Phone calls for home reading in kenya. World Bank Policy Research Working Paper WPS10098, 2022.</p>
<p>Action elimination and stopping conditions for the multi-armed bandit and reinforcement learning problems. E Even-Dar, S Mannor, Y Mansour, Journal of Machine Learning Research. 7E. Even-Dar, S. Mannor, and Y. Mansour. Action elimination and stopping conditions for the multi-armed bandit and reinforcement learning problems. Journal of Machine Learning Research, 7:1079-1105, 2006.</p>
<p>L Fan, P W Glynn, arXiv:2105.09232Diffusion approximations for thompson sampling. 2021cs.LGL. Fan and P. W. Glynn. Diffusion approximations for thompson sampling. arXiv:2105.09232 [cs.LG], 2021.</p>
<p>Deep adaptive design: Amortizing sequential bayesian experimental design. A Foster, D R Ivanova, I Malik, T Rainforth, Proceedings of the 38th International Conference on Machine Learning. the 38th International Conference on Machine LearningA. Foster, D. R. Ivanova, I. Malik, and T. Rainforth. Deep adaptive design: Amortizing sequential bayesian experimental design. In Proceedings of the 38th International Conference on Machine Learning, 2021.</p>
<p>Paradoxes in learning and the marginal value of information. P I Frazier, W B Powell, Decision Analysis. 74P. I. Frazier and W. B. Powell. Paradoxes in learning and the marginal value of information. Decision Analysis, 7(4):378-403, 2010.</p>
<p>A knowledge-gradient policy for sequential information collection. P I Frazier, W B Powell, S Dayanik, 10.1137/070693424SIAM Journal on Control and Optimization. 475P. I. Frazier, W. B. Powell, and S. Dayanik. A knowledge-gradient policy for sequential information collection. SIAM Journal on Control and Optimization, 47(5):2410-2439, 2008. doi: 10.1137/070693424.</p>
<p>Best arm identification: A unified approach to fixed budget and fixed confidence. V Gabillon, M Ghavamzadeh, A Lazaric, Advances in Neural Information Processing Systems. 12V. Gabillon, M. Ghavamzadeh, and A. Lazaric. Best arm identification: A unified approach to fixed budget and fixed confidence. In Advances in Neural Information Processing Systems 12, volume 25, 2012.</p>
<p>Batched multi-armed bandits problem. Z Gao, Y Han, Z Ren, Z Zhou, Advances in Neural Information Processing Systems. 19Z. Gao, Y. Han, Z. Ren, and Z. Zhou. Batched multi-armed bandits problem. Advances in Neural Information Processing Systems 19, 2019.</p>
<p>Optimal best arm identification with fixed confidence. A Garivier, E Kaufmann, Conference on Learning Theory. PMLRA. Garivier and E. Kaufmann. Optimal best arm identification with fixed confidence. In Conference on Learning Theory, pages 998-1027. PMLR, 2016.</p>
<p>A multi-points criterion for deterministic parallel global optimization based on gaussian processes. D Ginsbourger, R L Riche, L Carraro, HAL-00260579HALTechnical ReportD. Ginsbourger, R. L. Riche, and L. Carraro. A multi-points criterion for deterministic parallel global optimization based on gaussian processes. Technical Report HAL-00260579, HAL, 2008.</p>
<p>Deep sparse rectifier neural networks. X Glorot, A Bordes, Y Bengio, Proceedings of the 14th International Conference on Artificial Intelligence and Statistics. the 14th International Conference on Artificial Intelligence and StatisticsX. Glorot, A. Bordes, and Y. Bengio. Deep sparse rectifier neural networks. In Proceedings of the 14th International Conference on Artificial Intelligence and Statistics, pages 315-323, 2011.</p>
<p>A large deviations perspective on ordinal optimization. P W Glynn, S Juneja, Proceedings of the 2004 Winter Simulation Conference. the 2004 Winter Simulation ConferenceIEEEP. W. Glynn and S. Juneja. A large deviations perspective on ordinal optimization. In Proceedings of the 2004 Winter Simulation Conference, pages 577-586. IEEE, 2004.</p>
<p>Batch bayesian optimization via local penalization. J Gonzalez, Z Dai, P Hennig, N Lawrence, Proceedings of the 19th International Conference on Artificial Intelligence and Statistics. the 19th International Conference on Artificial Intelligence and StatisticsJ. Gonzalez, Z. Dai, P. Hennig, and N. Lawrence. Batch bayesian optimization via local penalization. In Proceedings of the 19th International Conference on Artificial Intelligence and Statistics, 2016.</p>
<p>Glasses: Relieving the myopia of bayesian optimisation. J Gonzalez, M Osborne, N Lawrence, Proceedings of the 19th International Conference on Artificial Intelligence and Statistics. the 19th International Conference on Artificial Intelligence and StatisticsJ. Gonzalez, M. Osborne, and N. Lawrence. Glasses: Relieving the myopia of bayesian optimisation. In Proceedings of the 19th International Conference on Artificial Intelligence and Statistics, 2016.</p>
<p>Best arm identification in multi-armed bandits with delayed feedback. A Grover, T Markov, P Attia, N Jin, N Perkins, B Cheong, M Chen, Z Yang, S Harris, W Chueh, S Ermon, Proceedings of the 21st International Conference on Artificial Intelligence and Statistics. the 21st International Conference on Artificial Intelligence and StatisticsPMLRA. Grover, T. Markov, P. Attia, N. Jin, N. Perkins, B. Cheong, M. Chen, Z. Yang, S. Harris, W. Chueh, and S. Ermon. Best arm identification in multi-armed bandits with delayed feedback. In Proceedings of the 21st International Conference on Artificial Intelligence and Statistics, pages 833-842. PMLR, 2018.</p>
<p>Bayesian look ahead one-stage sampling allocations for selection of the best population. S S Gupta, K J Miescke, Journal of Statistical Planning and Inference. 542S. S. Gupta and K. J. Miescke. Bayesian look ahead one-stage sampling allocations for selection of the best population. Journal of Statistical Planning and Inference, 54(2):229- 244, 1996.</p>
<p>Confidence intervals for policy evaluation in adaptive experiments. V Hadad, D A Hirshberg, R Zhan, S Wager, S Athey, Proceedings of the National Academy of Sciences. 118152014602118V. Hadad, D. A. Hirshberg, R. Zhan, S. Wager, and S. Athey. Confidence intervals for policy evaluation in adaptive experiments. Proceedings of the National Academy of Sciences, 118 (15):e2014602118, 2021.</p>
<p>Opportunity cost and ocba selection procedures in ordinal optimization for a fixed number of alternative systems. D He, S E Chick, C.-H Chen, IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews). 375D. He, S. E. Chick, , and C.-H. Chen. Opportunity cost and ocba selection procedures in ordinal optimization for a fixed number of alternative systems. IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews), 37(5):951-961, 2007.</p>
<p>K Hirano, J R Porter, arXiv:2302.03117Asymptotic representations for sequential decisions, adaptive experiments, and batched bandits. 2023econ.EMK. Hirano and J. R. Porter. Asymptotic representations for sequential decisions, adaptive experiments, and batched bandits. arXiv:2302.03117 [econ.EM], 2023. URL https://arxiv. org/abs/2302.03117.</p>
<p>Discrete optimization via simulation. L J Hong, B L Nelson, J Xu, Handbook of Simulation Optimization. SpringerL. J. Hong, B. L. Nelson, and J. Xu. Discrete optimization via simulation. In Handbook of Simulation Optimization, pages 9-44. Springer, 2015.</p>
<p>G Imbens, D Rubin, Causal Inference for Statistics, Social, and Biomedical Sciences. Cambridge University PressG. Imbens and D. Rubin. Causal Inference for Statistics, Social, and Biomedical Sciences. Cambridge University Press, 2015.</p>
<p>Best-arm identification algorithms for multi-armed bandits in the fixed confidence setting. K Jamieson, R Nowak, 48th Annual Conference on Information Sciences and Systems (CISS). IEEEK. Jamieson and R. Nowak. Best-arm identification algorithms for multi-armed bandits in the fixed confidence setting. In 2014 48th Annual Conference on Information Sciences and Systems (CISS), pages 1-6. IEEE, 2014.</p>
<p>lil ucb : An optimal exploration algorithm for multi-armed bandits. K Jamieson, M Malloy, R Nowak, S Bubeck, Proceedings of the Twenty Seventh Annual Conference on Computational Learning Theory. the Twenty Seventh Annual Conference on Computational Learning TheoryPMLRK. Jamieson, M. Malloy, R. Nowak, and S. Bubeck. lil ucb : An optimal exploration algo- rithm for multi-armed bandits. In Proceedings of the Twenty Seventh Annual Conference on Computational Learning Theory, pages 423-439. PMLR, 2014.</p>
<p>Binoculars for efficient, nonmyopic sequential experimental design. S Jiang, H Chai, J González, R Garnett, Proceedings of the 37th International Conference on Machine Learning. the 37th International Conference on Machine LearningS. Jiang, H. Chai, J. González, and R. Garnett. Binoculars for efficient, nonmyopic sequen- tial experimental design. In Proceedings of the 37th International Conference on Machine Learning, 2020.</p>
<p>Efficient nonmyopic bayesian optimization via one-shot multi-step trees. S Jiang, D Jiang, M Balandat, B Karrer, J Gardner, R Garnett, Advances in Neural Information Processing Systems. 20S. Jiang, D. Jiang, M. Balandat, B. Karrer, J. Gardner, and R. Garnett. Efficient nonmy- opic bayesian optimization via one-shot multi-step trees. In Advances in Neural Information Processing Systems 20, 2020.</p>
<p>Improving the statistical power of economic experiments using adaptive designs. S Jobjörnsson, H Schaak, O Musshoff, T Friede, Experimental Economics. 2022S. Jobjörnsson, H. Schaak, O. Musshoff, and T. Friede. Improving the statistical power of economic experiments using adaptive designs. Experimental Economics, 2022.</p>
<p>Simple regret minimization for contextual bandits using bayesian optimal experimental design. M Jörke, J Lee, E Brunskill, ICML2022 Workshop on Adaptive Experimental Design and Active Learning in the Real World. M. Jörke, J. Lee, and E. Brunskill. Simple regret minimization for contextual bandits using bayesian optimal experimental design. In ICML2022 Workshop on Adaptive Experimental Design and Active Learning in the Real World, 2022.</p>
<p>Online learning under delayed feedback. P Joulani, A Gyorgy, C Szepesvári, Proceedings of the 30th International Conference on Machine Learning. the 30th International Conference on Machine LearningPMLRP. Joulani, A. Gyorgy, and C. Szepesvári. Online learning under delayed feedback. In Pro- ceedings of the 30th International Conference on Machine Learning, pages 1453-1461. PMLR, 2013.</p>
<p>Top arm identification in multi-armed bandits with batch arm pulls. K.-S Jun, K Jamieson, R Nowak, X Zhu, Proceedings of the 19th International Conference on Artificial Intelligence and Statistics. the 19th International Conference on Artificial Intelligence and StatisticsPMLRK.-S. Jun, K. Jamieson, R. Nowak, and X. Zhu. Top arm identification in multi-armed bandits with batch arm pulls. In Proceedings of the 19th International Conference on Artificial Intelligence and Statistics, pages 139-148. PMLR, 2016.</p>
<p>Batched thompson sampling. C Kalkanli, A Ozgur, Advances in Neural Information Processing Systems. 34C. Kalkanli and A. Ozgur. Batched thompson sampling. In Advances in Neural Information Processing Systems 34, 2021.</p>
<p>A closer look at the worst-case behavior of multi-armed bandit algorithms. A Kalvit, A Zeevi, Advances in Neural Information Processing Systems. 34A. Kalvit and A. Zeevi. A closer look at the worst-case behavior of multi-armed bandit algorithms. In Advances in Neural Information Processing Systems 34, 2021.</p>
<p>Parallelised bayesian optimisation via thompson sampling. K Kandasamy, A Krishnamurthy, J Schneider, B Poczos, Proceedings of the 21st International Conference on Artificial Intelligence and Statistics. the 21st International Conference on Artificial Intelligence and Statistics84K. Kandasamy, A. Krishnamurthy, J. Schneider, and B. Poczos. Parallelised bayesian op- timisation via thompson sampling. In Proceedings of the 21st International Conference on Artificial Intelligence and Statistics, volume 84, pages 133-142, 2018.</p>
<p>Parallelizing thompson sampling. A Karbasi, V Mirrokni, M Shadravan, Advances in Neural Information Processing Systems. 21A. Karbasi, V. Mirrokni, and M. Shadravan. Parallelizing thompson sampling. In Advances in Neural Information Processing Systems 21, volume 34, 2021.</p>
<p>Almost optimal exploration in multi-armed bandits. Z Karnin, T Koren, O Somekh, Proceedings of the 30th International Conference on Machine Learning. the 30th International Conference on Machine LearningZ. Karnin, T. Koren, and O. Somekh. Almost optimal exploration in multi-armed bandits. In Proceedings of the 30th International Conference on Machine Learning, pages 1238-1246, 2013.</p>
<p>Adaptive treatment assignment in experiments for policy choice. M Kasy, A Sautmann, Econometrica. 891M. Kasy and A. Sautmann. Adaptive treatment assignment in experiments for policy choice. Econometrica, 89(1):113-132, 2021.</p>
<p>Information complexity in bandit subset selection. E Kaufmann, S Kalyanakrishnan, Proceedings of the Twenty Sixth Annual Conference on Computational Learning Theory. the Twenty Sixth Annual Conference on Computational Learning TheoryPMLRE. Kaufmann and S. Kalyanakrishnan. Information complexity in bandit subset selection. In Proceedings of the Twenty Sixth Annual Conference on Computational Learning Theory, pages 228-251. PMLR, 2013.</p>
<p>On the complexity of best-arm identification in multi-armed bandit models. E Kaufmann, O Cappé, A Garivier, Journal of Machine Learning Research. 171E. Kaufmann, O. Cappé, and A. Garivier. On the complexity of best-arm identification in multi-armed bandit models. Journal of Machine Learning Research, 17(1):1-42, 2016.</p>
<p>Recent advances in ranking and selection. S.-H Kim, B L Nelson, Proceedings of the 2007 Winter Simulation Conference. the 2007 Winter Simulation ConferenceS.-H. Kim and B. L. Nelson. Recent advances in ranking and selection. In Proceedings of the 2007 Winter Simulation Conference, 2007.</p>
<p>Adam: A method for stochastic optimization. D P Kingma, J Ba, Proceedings of the Third International Conference on Learning Representations. the Third International Conference on Learning RepresentationsD. P. Kingma and J. Ba. Adam: A method for stochastic optimization. In Proceedings of the Third International Conference on Learning Representations, 2015.</p>
<p>Auto-encoding variational bayes. D P Kingma, M Welling, D. P. Kingma and M. Welling. Auto-encoding variational bayes, 2014.</p>
<p>Controlled experiments on the web: survey and practical guide. R Kohavi, R Longbotham, D Sommerfield, R M Henne, Data Mining and Knowledge Discovery. 181R. Kohavi, R. Longbotham, D. Sommerfield, and R. M. Henne. Controlled experiments on the web: survey and practical guide. Data Mining and Knowledge Discovery, 18(1):140-181, 2009.</p>
<p>Trustworthy online controlled experiments: Five puzzling outcomes explained. R Kohavi, A Deng, B Frasca, R Longbotham, T Walker, Y Xu, Proceedings of the 18th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. the 18th ACM SIGKDD International Conference on Knowledge Discovery and Data MiningR. Kohavi, A. Deng, B. Frasca, R. Longbotham, T. Walker, and Y. Xu. Trustworthy online controlled experiments: Five puzzling outcomes explained. In Proceedings of the 18th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 786-794, 2012.</p>
<p>Online controlled experiments at large scale. R Kohavi, A Deng, B Frasca, T Walker, Y Xu, N Pohlmann, Proceedings of the 19th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. the 19th ACM SIGKDD International Conference on Knowledge Discovery and Data MiningR. Kohavi, A. Deng, B. Frasca, T. Walker, Y. Xu, and N. Pohlmann. Online controlled experiments at large scale. In Proceedings of the 19th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 1168-1176, 2013.</p>
<p>J Komiyama, K Ariu, M Kato, C Qin, arXiv:2111.09885Optimal simple regret in bayesian best arm identification. 2021cs.LGJ. Komiyama, K. Ariu, M. Kato, and C. Qin. Optimal simple regret in bayesian best arm identification. arXiv:2111.09885 [cs.LG], 2021.</p>
<p>Asymptotically efficient adaptive allocation rules. T L Lai, H Robbins, Advances in Applied Mathematics. 6T. L. Lai and H. Robbins. Asymptotically efficient adaptive allocation rules. Advances in Applied Mathematics, 6:4-22, 1985.</p>
<p>Bayesian optimization with a finite budget: an approximate dynamic programming approach. R R Lam, K E Willcox, D H Wolpert, Advances in Neural Information Processing Systems 16. R. R. Lam, K. E. Willcox, and D. H. Wolpert. Bayesian optimization with a finite budget: an approximate dynamic programming approach. In Advances in Neural Information Processing Systems 16, 2016.</p>
<p>Bandit algorithms. Cambridge. T Lattimore, C Szepesvári, T. Lattimore and C. Szepesvári. Bandit algorithms. Cambridge, 2019.</p>
<p>Asymptotics in Statistics: Some Basic Concepts. L , Le Cam, G L Yang, SpringerL. Le Cam and G. L. Yang. Asymptotics in Statistics: Some Basic Concepts. Springer, 2000.</p>
<p>Dynamically aggregating diverse information. Econometrica. A Liang, X Mu, V Syrgkanis, 90A. Liang, X. Mu, and V. Syrgkanis. Dynamically aggregating diverse information. Econo- metrica, 90(1):47-80, 2022.</p>
<p>Continuous control with deep reinforcement learning. T P Lillicrap, J J Hunt, A Pritzel, N Heess, T Erez, Y Tassa, D Silver, D Wierstra, Proceedings of the Fourth International Conference on Learning Representations. the Fourth International Conference on Learning RepresentationsT. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa, D. Silver, and D. Wier- stra. Continuous control with deep reinforcement learning. In Proceedings of the Fourth International Conference on Learning Representations, 2016.</p>
<p>On the limited memory BFGS method for large scale optimization. D Liu, J , Mathematical Programming. 451D. Liu and J. Nocedal. On the limited memory BFGS method for large scale optimization. Mathematical Programming, 45(1):503-528, 1989.</p>
<p>Y Liu, A M Devraj, B Van Roy, K Xu, arXiv:2201.01902Gaussian imagination in bandit learning. 2022cs.LGY. Liu, A. M. Devraj, B. Van Roy, and K. Xu. Gaussian imagination in bandit learning. arXiv:2201.01902 [cs.LG], 2022.</p>
<p>Parametric-rate inference for one-sided differentiable parameters. A R Luedtke, M J V D Laan, Journal of the American Statistical Association. 113522A. R. Luedtke and M. J. v. d. Laan. Parametric-rate inference for one-sided differentiable parameters. Journal of the American Statistical Association, 113(522):780-788, 2018.</p>
<p>Super-learning of an optimal dynamic treatment rule. A R Luedtke, M J Van Der Laan, International Journal of Biostatistics. 121A. R. Luedtke and M. J. van der Laan. Super-learning of an optimal dynamic treatment rule. International Journal of Biostatistics, 12(1):305-332, 2016.</p>
<p>The sample complexity of exploration in the multi-armed bandit problem. S Mannor, J N Tsitsiklis, Journal of Machine Learning Research. 5S. Mannor and J. N. Tsitsiklis. The sample complexity of exploration in the multi-armed bandit problem. Journal of Machine Learning Research, 5(Jun):623-648, 2004.</p>
<p>On stein's method for multivariate normal approximation. E Meckes, 5High dimensional probability V: the Luminy volumeE. Meckes. On stein's method for multivariate normal approximation. High dimensional probability V: the Luminy volume, 5:153-178, 2009.</p>
<p>Gaussian marginals of convex bodies with symmetries. Beitrage zur Algebra und Geometrie. M W Meckes, 50M. W. Meckes. Gaussian marginals of convex bodies with symmetries. Beitrage zur Algebra und Geometrie, 50(1):101-118, 2009.</p>
<p>Policy gradient optimization of thompson sampling policies. S Min, C C Moallemi, D Russo, arXiv:2006.165072020cs.LGS. Min, C. C. Moallemi, and D. Russo. Policy gradient optimization of thompson sampling policies. arXiv:2006.16507 [cs.LG], 2020.</p>
<p>A stochastic multi-armed bandit approach to nonparametric h∞-norm estimation. M I Müller, P E Valenzuela, A Proutiere, C R Rojas, 56th IEEE Conference on Decisions and Control. IEEEM. I. Müller, P. E. Valenzuela, A. Proutiere, and C. R. Rojas. A stochastic multi-armed bandit approach to nonparametric h∞-norm estimation. In 56th IEEE Conference on Decisions and Control, pages 4632-4637. IEEE, 2017.</p>
<p>Distilled thompson sampling: Practical and efficient thompson sampling via imitation learning. H Namkoong, S Daulton, E Bakshy, arXiv:2011.14266[cs.LG2020H. Namkoong, S. Daulton, and E. Bakshy. Distilled thompson sampling: Practical and efficient thompson sampling via imitation learning. arXiv:2011.14266 [cs.LG], 2020.</p>
<p>Adaptive experimental design: Prospects and applications in political science. M Offer-Westort, A Coppock, D P Green, 10.2139/ssrn.3364402SSRN. 3364402M. Offer-Westort, A. Coppock, and D. P. Green. Adaptive experimental design: Prospects and applications in political science. SSRN 3364402, 2020. URL http://dx.doi.org/10. 2139/ssrn.3364402.</p>
<p>Pytorch: An imperative style, high-performance deep learning library. A Paszke, S Gross, F Massa, A Lerer, J Bradbury, G Chanan, T Killeen, Z Lin, N Gimelshein, L Antiga, A Desmaison, A Kopf, E Yang, Z Devito, M Raison, A Tejani, S Chilamkurthy, B Steiner, L Fang, J Bai, S Chintala, Advances in Neural Information Processing Systems. 32A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein, L. Antiga, A. Desmaison, A. Kopf, E. Yang, Z. DeVito, M. Raison, A. Te- jani, S. Chilamkurthy, B. Steiner, L. Fang, J. Bai, and S. Chintala. Pytorch: An imperative style, high-performance deep learning library. In Advances in Neural Information Processing Systems 32, 2019.</p>
<p>Batched bandit problems. V Perchet, P Rigollet, S Chassang, E Snowberg, Annals of Statistics. 442V. Perchet, P. Rigollet, S. Chassang, and E. Snowberg. Batched bandit problems. Annals of Statistics, 44(2):660-681, 2016.</p>
<p>Bandits with delayed, aggregated anonymous feedback. C Pike-Burke, S Agrawal, C Szepesvari, S Grunewalder, Proceedings of the 35th International Conference on Machine Learning. the 35th International Conference on Machine LearningPMLRC. Pike-Burke, S. Agrawal, C. Szepesvari, and S. Grunewalder. Bandits with delayed, aggre- gated anonymous feedback. In Proceedings of the 35th International Conference on Machine Learning, pages 4105-4113. PMLR, 2018.</p>
<p>Improving the expected improvement algorithm. C Qin, D Klabjan, D Russo, Advances in Neural Information Processing Systems 17. C. Qin, D. Klabjan, and D. Russo. Improving the expected improvement algorithm. In Advances in Neural Information Processing Systems 17, 2017.</p>
<p>Some aspects of the sequential design of experiments. H Robbins, Bulletin American Mathematical Society. 55H. Robbins. Some aspects of the sequential design of experiments. Bulletin American Math- ematical Society, 55:527-535, 1952.</p>
<p>R T Rockafellar, R J B Wets, Variational Analysis. New YorkSpringerR. T. Rockafellar and R. J. B. Wets. Variational Analysis. Springer, New York, 1998.</p>
<p>Simple bayesian algorithms for best-arm identification. D Russo, Operations Research. 686D. Russo. Simple bayesian algorithms for best-arm identification. Operations Research, 68 (6):1625-1647, 2020.</p>
<p>A tutorial on thompson sampling. Foundations and Trends® in Machine Learning. D J Russo, B Van Roy, A Kazerouni, I Osband, Z Wen, 11D. J. Russo, B. Van Roy, A. Kazerouni, I. Osband, and Z. Wen. A tutorial on thompson sampling. Foundations and Trends® in Machine Learning, 11(1):1-96, 2018.</p>
<p>A review of modern computational algorithms for bayesian optimal design. E G Ryan, C C Drovandi, J M Mcgree, A N Pettitt, International Statistics Review. 841E. G. Ryan, C. C. Drovandi, J. M. McGree, and A. N. Pettitt. A review of modern computa- tional algorithms for bayesian optimal design. International Statistics Review, 84(1):128-154, 2016.</p>
<p>Hidden technical debt in machine learning systems. D Sculley, G Holt, D Golovin, E Davydov, T Phillips, D Ebner, V Chaudhary, M Young, J.-F Crespo, D Dennison, Advances in Neural Information Processing Systems. 28D. Sculley, G. Holt, D. Golovin, E. Davydov, T. Phillips, D. Ebner, V. Chaudhary, M. Young, J.-F. Crespo, and D. Dennison. Hidden technical debt in machine learning systems. In Advances in Neural Information Processing Systems 28, pages 2503-2511, 2015.</p>
<p>Parallel predictive entropy search for batch global optimization of expensive objective functions. A Shah, Z Ghahramani, Advances in Neural Information Processing Systems 15. A. Shah and Z. Ghahramani. Parallel predictive entropy search for batch global optimization of expensive objective functions. In Advances in Neural Information Processing Systems 15, 2015.</p>
<p>Fixed-confidence guarantees for bayesian best-arm identification. X Shang, R De Heide, P Menard, E Kaufmann, M Valko, PMLRProceedings of the 23rd International Conference on Artificial Intelligence and Statistics. S. Chiappa and R. Calandrathe 23rd International Conference on Artificial Intelligence and Statistics108X. Shang, R. de Heide, P. Menard, E. Kaufmann, and M. Valko. Fixed-confidence guarantees for bayesian best-arm identification. In S. Chiappa and R. Calandra, editors, Proceedings of the 23rd International Conference on Artificial Intelligence and Statistics, volume 108 of Proceedings of Machine Learning Research, pages 1823-1832. PMLR, 26-28 Aug 2020.</p>
<p>Introduction to multi-armed bandits. A Slivkins, arXiv:1904.07272cs.LGA. Slivkins. Introduction to multi-armed bandits. arXiv:1904.07272 [cs.LG], 2019.</p>
<p>Reinforcement learning: An introduction. R S Sutton, A G Barto, MIT pressR. S. Sutton and A. G. Barto. Reinforcement learning: An introduction. MIT press, 2018.</p>
<p>On the likelihood that one unknown probability exceeds another in view of the evidence of two samples. W R Thompson, Biometrika. 253-4W. R. Thompson. On the likelihood that one unknown probability exceeds another in view of the evidence of two samples. Biometrika, 25(3-4):285-294, 1933.</p>
<p>A W Van Der, Vaart, Asymptotic Statistics. Cambridge Series in Statistical and Probabilistic Mathematics. Cambridge University PressA. W. van der Vaart. Asymptotic Statistics. Cambridge Series in Statistical and Probabilistic Mathematics. Cambridge University Press, 1998.</p>
<p>Linear bandits with stochastic delayed feedback. C Vernade, A Carpentier, T Lattimore, G Zappella, B Ermis, M Brueckner, Proceedings of the 37th International Conference on Machine Learning. the 37th International Conference on Machine LearningPMLRC. Vernade, A. Carpentier, T. Lattimore, G. Zappella, B. Ermis, and M. Brueckner. Linear bandits with stochastic delayed feedback. In Proceedings of the 37th International Conference on Machine Learning, pages 9712-9721. PMLR, 2020.</p>
<p>Diffusion asymptotics for sequential experiments. S Wager, K Xu, arXiv:2101.098552021math.STS. Wager and K. Xu. Diffusion asymptotics for sequential experiments. arXiv:2101.09855 [math.ST], 2021.</p>
<p>Batched large-scale bayesian optimization in high-dimensional spaces. Z Wang, C Gehring, P Kohli, S Jegelka, Proceedings of the 21st International Conference on Artificial Intelligence and Statistics. the 21st International Conference on Artificial Intelligence and StatisticsPMLRZ. Wang, C. Gehring, P. Kohli, and S. Jegelka. Batched large-scale bayesian optimization in high-dimensional spaces. In Proceedings of the 21st International Conference on Artificial Intelligence and Statistics, pages 745-754. PMLR, 2018.</p>
<p>Simple statistical gradient-following algorithms for connectionist reinforcement learning. R J Williams, Machine Learning. 8R. J. Williams. Simple statistical gradient-following algorithms for connectionist reinforce- ment learning. Machine Learning, 8:229-256, 1992.</p>
<p>Maximizing acquisition functions for bayesian optimization. J T Wilson, F Hutter, M P Deisenroth, Advances in Neural Information Processing Systems 18. J. T. Wilson, F. Hutter, and M. P. Deisenroth. Maximizing acquisition functions for bayesian optimization. In Advances in Neural Information Processing Systems 18, 2018.</p>
<p>Practical two-step look-ahead bayesian optimization. J Wu, P I Frazier, Advances in Neural Information Processing Systems 19. J. Wu and P. I. Frazier. Practical two-step look-ahead bayesian optimization. In Advances in Neural Information Processing Systems 19, 2019.</p>
<p>Optimal experimental design for staggered rollouts. R Xiong, S Athey, M Bayati, G W Imbens, 10.2139/ssrn.3483934SSRN. 3483934R. Xiong, S. Athey, M. Bayati, and G. W. Imbens. Optimal experimental design for staggered rollouts. SSRN 3483934, 2023. URL https://dx.doi.org/10.2139/ssrn.3483934.</p>
<p>Inference for batched bandits. K Zhang, L Janson, S Murphy, Advances in Neural Information Processing Systems. 20K. Zhang, L. Janson, and S. Murphy. Inference for batched bandits. Advances in Neural Information Processing Systems 20, 33:9818-9829, 2020.</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9268 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9268</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9268</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-162.html">extraction-schema-162</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using language models for detecting anomalies in lists, sequences, or tabular data, including details of the models, data types, methods, results, comparisons, and limitations.</div>
                <p><strong>Paper ID:</strong> paper-81e4cd1855c9f068f5de593d878dad95cb214546</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/81e4cd1855c9f068f5de593d878dad95cb214546" target="_blank">Self-Attentive Classification-Based Anomaly Detection in Unstructured Logs</a></p>
                <p><strong>Paper Venue:</strong> Industrial Conference on Data Mining</p>
                <p><strong>Paper TL;DR:</strong> Logsy is proposed, a classification-based method to learn log representations that allow to distinguish between normal system log data and anomaly samples from auxiliary log datasets, easily accessible via the internet.</p>
                <p><strong>Paper Abstract:</strong> The detection of anomalies is an essential data mining task for achieving security and reliability in computer systems. Logs are a common and major data source for anomaly detection methods in almost every computer system. Recent studies have focused predominantly on one-class deep learning methods on manually specified log representations. The main limitation is that these models are not able to learn log representations describing the semantic differences between normal and anomaly logs, leading to a poor generalization on unseen logs. We propose Logsy, a classification-based method to learn log representations that allow to distinguish between normal system log data and anomaly samples from auxiliary log datasets, easily accessible via the internet. The idea behind such an approach to anomaly detection is that the auxiliary dataset is sufficiently informative to enhance the representation of the normal data, yet diverse to regularize against overfitting and improve generalization. We perform several experiments on publicly available datasets to evaluate the performance and properties, where we show improvement of 0.25 in F1 compared to previous methods.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9268.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9268.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using language models for detecting anomalies in lists, sequences, or tabular data, including details of the models, data types, methods, results, comparisons, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Logsy</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Logsy (Self-Attentive Classification-Based Anomaly Detection in Unstructured Logs)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A classification-based anomaly detection method for system logs that trains a Transformer encoder to produce compact log embeddings by separating target-system (normal) logs from auxiliary (anomalous) logs using a hyperspherical loss; anomaly score is the squared norm of the embedding.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Logsy (custom Transformer encoder)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Transformer encoder with multi-head self-attention</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Embedding dim d=16; 2 encoder layers; feed-forward size 16; max tokens per sample 50</td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>Unstructured text log messages (token sequences); per-message embedding via special [EMBEDDING] token; sequences of log entries used for dataset splitting</td>
                        </tr>
                        <tr>
                            <td><strong>data_domain</strong></td>
                            <td>High-performance computing system logs (Blue Gene/L, Thunderbird, Spirit) and auxiliary HPC/RAS logs</td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_type</strong></td>
                            <td>Anomalous log messages / error events / outlier log entries</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Tokenize raw logs to tokens, learn token embeddings + positional encodings, pass through Transformer encoder, extract [EMBEDDING] token vector z = φ(x;θ); train as binary classifier between target (y=0) and auxiliary logs (y=1) with a Gaussian radial basis (hyperspherical) loss that minimizes ||z||^2 for normal and pushes anomalies away; anomaly score A(x)=||z||^2; thresholding determines anomaly decisions. Auxiliary (internet-accessible) logs act as a proxy anomalous class; optional fine-tuning with small amounts of labeled anomalies.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_methods</strong></td>
                            <td>DeepLog (LSTM next-template predictor using template indices), PCA (TF-IDF + PCA session-based approach)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>F1, precision, recall, accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Logsy averaged F1 across splits: Blue Gene/L 0.448, Thunderbird 0.99, Spirit 0.77. Reported average F1 improvement of ~0.25 over previous methods; replacing PCA's TF-IDF with Logsy embeddings improved PCA F1 by 0.09 (Blue Gene/L), 0.11 (Thunderbird), 0.01 (Spirit) with average relative improvement 28.2%. High precision and preserved high recall compared to baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baseline</strong></td>
                            <td>Better — Logsy outperforms baselines (DeepLog and PCA) in F1 and precision, while maintaining high recall. On Blue Gene/L Logsy had 2–4× higher precision than DeepLog in some splits. Outperforms baselines even with only 10% of target training data.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Requires auxiliary data to bias the anomalous distribution (though even a single auxiliary sample prevents trivial collapse); performance saturates with auxiliary size (~100k samples); not evaluated on non-text/tabular list data; performance can degrade in splits with extremely few anomalies (e.g., some Spirit splits); relies on retraining/fine-tuning to incorporate new labeled anomalies; the study does not explore very large transformer sizes or zero-shot usage of large pre-trained LMs.</td>
                        </tr>
                        <tr>
                            <td><strong>unique_insights</strong></td>
                            <td>Using auxiliary logs as an explicit anomalous class is an effective regularizer that improves generalization to unseen log messages; hyperspherical loss enforces compact 'normal' region making distance-based scoring meaningful; learned token-level contextual embeddings reduce false positives versus template-index methods; embeddings produced by the model can improve other anomaly detectors (e.g., PCA).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Self-Attentive Classification-Based Anomaly Detection in Unstructured Logs', 'publication_date_yy_mm': '2020-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9268.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9268.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using language models for detecting anomalies in lists, sequences, or tabular data, including details of the models, data types, methods, results, comparisons, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DeepLog</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Deeplog: Anomaly detection and diagnosis from system logs through deep learning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LSTM-based method that models sequences of log template indices to predict the next log template; deviations between predicted and actual template indicate anomalies.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Deeplog: Anomaly detection and diagnosis from system logs through deep learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DeepLog (LSTM)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>LSTM (recurrent neural network) sequence model</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>Sequences of log template indices (categorical sequences represented as one-hot vectors)</td>
                        </tr>
                        <tr>
                            <td><strong>data_domain</strong></td>
                            <td>System logs (HPC and other system logs)</td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_type</strong></td>
                            <td>Anomalous log events / unexpected next-template predictions (outliers)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Train LSTM over sequences of parsed log-template indices to forecast next template index t_{m+1} given history H. If the actual next template is not among predicted/high-probability candidates, flag anomaly. Uses parser to map free-form logs to discrete templates.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_methods</strong></td>
                            <td>Compared against Logsy (Transformer + hyperspherical loss) and PCA in this paper</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>F1, precision, recall</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Reported to have high recall but low precision compared to Logsy; on Blue Gene/L DeepLog and PCA show 2–4× lower precision than Logsy (exact numeric F1 values for DeepLog not provided in paper's text excerpt).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baseline</strong></td>
                            <td>Worse: DeepLog attains high recall but produces many false positives (low precision), leading to lower F1 than Logsy in the experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Input depends on parsed log-template indices (one-hot); cannot correctly handle newly appearing/unseen log messages (no semantic token representation), leading to false positives; ignores semantic similarity between different templates that may be normal.</td>
                        </tr>
                        <tr>
                            <td><strong>unique_insights</strong></td>
                            <td>Sequence-modeling on template indices is vulnerable to unseen templates and benefits significantly from token-level semantic representations instead of index-based inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Self-Attentive Classification-Based Anomaly Detection in Unstructured Logs', 'publication_date_yy_mm': '2020-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9268.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9268.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using language models for detecting anomalies in lists, sequences, or tabular data, including details of the models, data types, methods, results, comparisons, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Transformer encoder (multi-head self-attention)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Transformer encoder with multi-head self-attention (as used in Logsy)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An attention-based encoder that produces contextualized token and message-level embeddings via multi-head self-attention and positional encodings; in Logsy the [EMBEDDING] token's output is used as the log message embedding.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Attention is all you need</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Transformer encoder</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Transformer (self-attention) architecture</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>In Logsy: 2 encoder layers, embedding dim d=16, w=d/L where L=number of heads; positional encodings with sine/cosine</td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>Tokenized text sequences (log messages), positional information encoded</td>
                        </tr>
                        <tr>
                            <td><strong>data_domain</strong></td>
                            <td>System logs (HPC) in this work; generally applicable to text sequences</td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_type</strong></td>
                            <td>Anomalous log messages / outlier text sequences</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Token embeddings + positional encodings → multi-head self-attention layers → feed-forward layers; extract pooled [EMBEDDING] token representation summarizing the message; embeddings used with hyperspherical loss for anomaly detection.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_methods</strong></td>
                            <td>Compared empirically to LSTM-based DeepLog and PCA; transformer-based approach replaced template-index input with contextual token embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>F1, precision, recall, accuracy (applied to downstream anomaly detection task)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>As part of Logsy, the transformer-based encoder yielded significantly better precision and F1 when combined with hyperspherical classification and auxiliary data (see Logsy results).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baseline</strong></td>
                            <td>Transformer encoder (token-level contextual embeddings) + hyperspherical objective outperformed LSTM-on-template-index (DeepLog) and TF-IDF+PCA baselines in the experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Small transformer used in experiments; not evaluated at large pre-trained-scale or in zero-shot setups; architecture alone does not guarantee compact normal representations without the hyperspherical loss and auxiliary anomalous data.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Self-Attentive Classification-Based Anomaly Detection in Unstructured Logs', 'publication_date_yy_mm': '2020-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9268.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e9268.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using language models for detecting anomalies in lists, sequences, or tabular data, including details of the models, data types, methods, results, comparisons, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Pretrained embeddings (word2vec, BERT)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Pretrained word embeddings and language models (word2vec, BERT) as mentioned</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Pretrained word embeddings (word2vec) and pretrained transformer language models (BERT) are discussed as prior-art ways to represent log text; prior log-anomaly work used such embeddings to improve representation, but the domain mismatch and small gains are noted.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>word2vec; BERT</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>word embeddings (word2vec) / pretrained transformer (BERT)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>Text tokens (word-level or subword-level embeddings); used to represent log templates or tokens</td>
                        </tr>
                        <tr>
                            <td><strong>data_domain</strong></td>
                            <td>General text corpora used for pretraining (e.g., Wikipedia) vs. log-specific language</td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_type</strong></td>
                            <td>Anomalous log messages / outliers (in prior work embeddings were used as inputs to anomaly detectors)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Prior studies used pretrained embeddings to numerically represent log templates or message tokens (either fixed embeddings or fine-tuned) and then trained sequence models (e.g., LSTMs) or classifiers on those vectors to detect anomalies; BERT-style pretraining on large corpora followed by task adaptation is referenced as general NLP practice.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_methods</strong></td>
                            <td>Referenced as augmentations to LSTM/Bi-LSTM models in prior log-anomaly papers (e.g., used with LogAnomaly, LogRobust)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>In prior reports: next-template prediction improvement; in general downstream metrics like F1 when used with sequence models</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Paper notes only small improvements when using pretrained word embeddings (e.g., word2vec pretrained on Wikipedia) for log sequence prediction in previous studies; exact numeric improvements not provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baseline</strong></td>
                            <td>Mentioned as helpful but limited: pretrained embeddings trained on general text (Wikipedia) have domain mismatch with log language and yield only small improvements; Logsy trains embeddings end-to-end on tokenized logs and achieves larger gains.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Domain mismatch: general-text pretraining (Wikipedia) differs from developer-authored log language; leads to imperfect vector representations and limited generalization for unseen logs; previous approaches using them still struggle with unseen template events.</td>
                        </tr>
                        <tr>
                            <td><strong>unique_insights</strong></td>
                            <td>Pretraining on broad corpora helps representation but is insufficient alone for good generalization on log anomalies; task-specific training and auxiliary anomaly data are effective complementary strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Self-Attentive Classification-Based Anomaly Detection in Unstructured Logs', 'publication_date_yy_mm': '2020-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9268.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e9268.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using language models for detecting anomalies in lists, sequences, or tabular data, including details of the models, data types, methods, results, comparisons, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LogRobust / LogAnomaly (attention Bi-LSTM + pretrained embeddings)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LogRobust and LogAnomaly: prior attention-based Bi-LSTM approaches that incorporate pretrained word vectors</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prior works that model log sequences with attention-based (Bi-)LSTM architectures using pretrained word embeddings to represent log messages and detect anomalies in log sequences.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Robust log-based anomaly detection on unstable log data</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LogRobust; LogAnomaly</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Attention-based Bi-LSTM sequence models (RNNs) with pretrained word embeddings</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>Tokenized log messages, sequences of log events (text sequences), sometimes using token embeddings instead of template indices</td>
                        </tr>
                        <tr>
                            <td><strong>data_domain</strong></td>
                            <td>System logs (various)</td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_type</strong></td>
                            <td>Sequential and quantitative anomalies in unstructured logs</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Use pretrained word vectors to represent log messages and train attention-enabled Bi-LSTM sequence models to capture sequential/contextual patterns; anomalies detected by deviations in predicted sequences or learned patterns.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_methods</strong></td>
                            <td>Mentioned as related work and baselines in the general literature; not reimplemented in this paper but compared conceptually.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>F1, precision, recall as commonly reported in prior papers</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Not provided in this paper; LogAnomaly reportedly gives only marginal improvements over DeepLog in prior literature according to the authors.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baseline</strong></td>
                            <td>Described as comparable to DeepLog in prior work; not directly compared numerically in this paper beyond qualitative statements.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Prior implementations may have marginal gains over index-based LSTM methods; still limited by domain mismatches of pretraining and by the need for robust representation of unseen logs.</td>
                        </tr>
                        <tr>
                            <td><strong>unique_insights</strong></td>
                            <td>Incorporating pretrained word vectors into sequence models helps but, per this paper, is not sufficient to close the generalization gap that Logsy addresses via auxiliary anomaly data and hyperspherical objective.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Self-Attentive Classification-Based Anomaly Detection in Unstructured Logs', 'publication_date_yy_mm': '2020-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Deeplog: Anomaly detection and diagnosis from system logs through deep learning <em>(Rating: 2)</em></li>
                <li>Robust log-based anomaly detection on unstable log data <em>(Rating: 2)</em></li>
                <li>Loganomaly: Unsupervised detection of sequential and quantitative anomalies in unstructured logs <em>(Rating: 2)</em></li>
                <li>Attention is all you need <em>(Rating: 1)</em></li>
                <li>Bert: Pre-training of deep bidirectional transformers for language understanding <em>(Rating: 1)</em></li>
                <li>Distributed representations of words and phrases and their compositionality <em>(Rating: 1)</em></li>
                <li>Detecting large-scale system problems by mining console logs <em>(Rating: 2)</em></li>
                <li>Experience report: Log mining using natural language processing and application to anomaly detection <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9268",
    "paper_id": "paper-81e4cd1855c9f068f5de593d878dad95cb214546",
    "extraction_schema_id": "extraction-schema-162",
    "extracted_data": [
        {
            "name_short": "Logsy",
            "name_full": "Logsy (Self-Attentive Classification-Based Anomaly Detection in Unstructured Logs)",
            "brief_description": "A classification-based anomaly detection method for system logs that trains a Transformer encoder to produce compact log embeddings by separating target-system (normal) logs from auxiliary (anomalous) logs using a hyperspherical loss; anomaly score is the squared norm of the embedding.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Logsy (custom Transformer encoder)",
            "model_type": "Transformer encoder with multi-head self-attention",
            "model_size": "Embedding dim d=16; 2 encoder layers; feed-forward size 16; max tokens per sample 50",
            "data_type": "Unstructured text log messages (token sequences); per-message embedding via special [EMBEDDING] token; sequences of log entries used for dataset splitting",
            "data_domain": "High-performance computing system logs (Blue Gene/L, Thunderbird, Spirit) and auxiliary HPC/RAS logs",
            "anomaly_type": "Anomalous log messages / error events / outlier log entries",
            "method_description": "Tokenize raw logs to tokens, learn token embeddings + positional encodings, pass through Transformer encoder, extract [EMBEDDING] token vector z = φ(x;θ); train as binary classifier between target (y=0) and auxiliary logs (y=1) with a Gaussian radial basis (hyperspherical) loss that minimizes ||z||^2 for normal and pushes anomalies away; anomaly score A(x)=||z||^2; thresholding determines anomaly decisions. Auxiliary (internet-accessible) logs act as a proxy anomalous class; optional fine-tuning with small amounts of labeled anomalies.",
            "baseline_methods": "DeepLog (LSTM next-template predictor using template indices), PCA (TF-IDF + PCA session-based approach)",
            "performance_metrics": "F1, precision, recall, accuracy",
            "performance_results": "Logsy averaged F1 across splits: Blue Gene/L 0.448, Thunderbird 0.99, Spirit 0.77. Reported average F1 improvement of ~0.25 over previous methods; replacing PCA's TF-IDF with Logsy embeddings improved PCA F1 by 0.09 (Blue Gene/L), 0.11 (Thunderbird), 0.01 (Spirit) with average relative improvement 28.2%. High precision and preserved high recall compared to baselines.",
            "comparison_to_baseline": "Better — Logsy outperforms baselines (DeepLog and PCA) in F1 and precision, while maintaining high recall. On Blue Gene/L Logsy had 2–4× higher precision than DeepLog in some splits. Outperforms baselines even with only 10% of target training data.",
            "limitations_or_failure_cases": "Requires auxiliary data to bias the anomalous distribution (though even a single auxiliary sample prevents trivial collapse); performance saturates with auxiliary size (~100k samples); not evaluated on non-text/tabular list data; performance can degrade in splits with extremely few anomalies (e.g., some Spirit splits); relies on retraining/fine-tuning to incorporate new labeled anomalies; the study does not explore very large transformer sizes or zero-shot usage of large pre-trained LMs.",
            "unique_insights": "Using auxiliary logs as an explicit anomalous class is an effective regularizer that improves generalization to unseen log messages; hyperspherical loss enforces compact 'normal' region making distance-based scoring meaningful; learned token-level contextual embeddings reduce false positives versus template-index methods; embeddings produced by the model can improve other anomaly detectors (e.g., PCA).",
            "uuid": "e9268.0",
            "source_info": {
                "paper_title": "Self-Attentive Classification-Based Anomaly Detection in Unstructured Logs",
                "publication_date_yy_mm": "2020-08"
            }
        },
        {
            "name_short": "DeepLog",
            "name_full": "Deeplog: Anomaly detection and diagnosis from system logs through deep learning",
            "brief_description": "An LSTM-based method that models sequences of log template indices to predict the next log template; deviations between predicted and actual template indicate anomalies.",
            "citation_title": "Deeplog: Anomaly detection and diagnosis from system logs through deep learning",
            "mention_or_use": "use",
            "model_name": "DeepLog (LSTM)",
            "model_type": "LSTM (recurrent neural network) sequence model",
            "model_size": null,
            "data_type": "Sequences of log template indices (categorical sequences represented as one-hot vectors)",
            "data_domain": "System logs (HPC and other system logs)",
            "anomaly_type": "Anomalous log events / unexpected next-template predictions (outliers)",
            "method_description": "Train LSTM over sequences of parsed log-template indices to forecast next template index t_{m+1} given history H. If the actual next template is not among predicted/high-probability candidates, flag anomaly. Uses parser to map free-form logs to discrete templates.",
            "baseline_methods": "Compared against Logsy (Transformer + hyperspherical loss) and PCA in this paper",
            "performance_metrics": "F1, precision, recall",
            "performance_results": "Reported to have high recall but low precision compared to Logsy; on Blue Gene/L DeepLog and PCA show 2–4× lower precision than Logsy (exact numeric F1 values for DeepLog not provided in paper's text excerpt).",
            "comparison_to_baseline": "Worse: DeepLog attains high recall but produces many false positives (low precision), leading to lower F1 than Logsy in the experiments.",
            "limitations_or_failure_cases": "Input depends on parsed log-template indices (one-hot); cannot correctly handle newly appearing/unseen log messages (no semantic token representation), leading to false positives; ignores semantic similarity between different templates that may be normal.",
            "unique_insights": "Sequence-modeling on template indices is vulnerable to unseen templates and benefits significantly from token-level semantic representations instead of index-based inputs.",
            "uuid": "e9268.1",
            "source_info": {
                "paper_title": "Self-Attentive Classification-Based Anomaly Detection in Unstructured Logs",
                "publication_date_yy_mm": "2020-08"
            }
        },
        {
            "name_short": "Transformer encoder (multi-head self-attention)",
            "name_full": "Transformer encoder with multi-head self-attention (as used in Logsy)",
            "brief_description": "An attention-based encoder that produces contextualized token and message-level embeddings via multi-head self-attention and positional encodings; in Logsy the [EMBEDDING] token's output is used as the log message embedding.",
            "citation_title": "Attention is all you need",
            "mention_or_use": "use",
            "model_name": "Transformer encoder",
            "model_type": "Transformer (self-attention) architecture",
            "model_size": "In Logsy: 2 encoder layers, embedding dim d=16, w=d/L where L=number of heads; positional encodings with sine/cosine",
            "data_type": "Tokenized text sequences (log messages), positional information encoded",
            "data_domain": "System logs (HPC) in this work; generally applicable to text sequences",
            "anomaly_type": "Anomalous log messages / outlier text sequences",
            "method_description": "Token embeddings + positional encodings → multi-head self-attention layers → feed-forward layers; extract pooled [EMBEDDING] token representation summarizing the message; embeddings used with hyperspherical loss for anomaly detection.",
            "baseline_methods": "Compared empirically to LSTM-based DeepLog and PCA; transformer-based approach replaced template-index input with contextual token embeddings.",
            "performance_metrics": "F1, precision, recall, accuracy (applied to downstream anomaly detection task)",
            "performance_results": "As part of Logsy, the transformer-based encoder yielded significantly better precision and F1 when combined with hyperspherical classification and auxiliary data (see Logsy results).",
            "comparison_to_baseline": "Transformer encoder (token-level contextual embeddings) + hyperspherical objective outperformed LSTM-on-template-index (DeepLog) and TF-IDF+PCA baselines in the experiments.",
            "limitations_or_failure_cases": "Small transformer used in experiments; not evaluated at large pre-trained-scale or in zero-shot setups; architecture alone does not guarantee compact normal representations without the hyperspherical loss and auxiliary anomalous data.",
            "uuid": "e9268.2",
            "source_info": {
                "paper_title": "Self-Attentive Classification-Based Anomaly Detection in Unstructured Logs",
                "publication_date_yy_mm": "2020-08"
            }
        },
        {
            "name_short": "Pretrained embeddings (word2vec, BERT)",
            "name_full": "Pretrained word embeddings and language models (word2vec, BERT) as mentioned",
            "brief_description": "Pretrained word embeddings (word2vec) and pretrained transformer language models (BERT) are discussed as prior-art ways to represent log text; prior log-anomaly work used such embeddings to improve representation, but the domain mismatch and small gains are noted.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "word2vec; BERT",
            "model_type": "word embeddings (word2vec) / pretrained transformer (BERT)",
            "model_size": null,
            "data_type": "Text tokens (word-level or subword-level embeddings); used to represent log templates or tokens",
            "data_domain": "General text corpora used for pretraining (e.g., Wikipedia) vs. log-specific language",
            "anomaly_type": "Anomalous log messages / outliers (in prior work embeddings were used as inputs to anomaly detectors)",
            "method_description": "Prior studies used pretrained embeddings to numerically represent log templates or message tokens (either fixed embeddings or fine-tuned) and then trained sequence models (e.g., LSTMs) or classifiers on those vectors to detect anomalies; BERT-style pretraining on large corpora followed by task adaptation is referenced as general NLP practice.",
            "baseline_methods": "Referenced as augmentations to LSTM/Bi-LSTM models in prior log-anomaly papers (e.g., used with LogAnomaly, LogRobust)",
            "performance_metrics": "In prior reports: next-template prediction improvement; in general downstream metrics like F1 when used with sequence models",
            "performance_results": "Paper notes only small improvements when using pretrained word embeddings (e.g., word2vec pretrained on Wikipedia) for log sequence prediction in previous studies; exact numeric improvements not provided in this paper.",
            "comparison_to_baseline": "Mentioned as helpful but limited: pretrained embeddings trained on general text (Wikipedia) have domain mismatch with log language and yield only small improvements; Logsy trains embeddings end-to-end on tokenized logs and achieves larger gains.",
            "limitations_or_failure_cases": "Domain mismatch: general-text pretraining (Wikipedia) differs from developer-authored log language; leads to imperfect vector representations and limited generalization for unseen logs; previous approaches using them still struggle with unseen template events.",
            "unique_insights": "Pretraining on broad corpora helps representation but is insufficient alone for good generalization on log anomalies; task-specific training and auxiliary anomaly data are effective complementary strategies.",
            "uuid": "e9268.3",
            "source_info": {
                "paper_title": "Self-Attentive Classification-Based Anomaly Detection in Unstructured Logs",
                "publication_date_yy_mm": "2020-08"
            }
        },
        {
            "name_short": "LogRobust / LogAnomaly (attention Bi-LSTM + pretrained embeddings)",
            "name_full": "LogRobust and LogAnomaly: prior attention-based Bi-LSTM approaches that incorporate pretrained word vectors",
            "brief_description": "Prior works that model log sequences with attention-based (Bi-)LSTM architectures using pretrained word embeddings to represent log messages and detect anomalies in log sequences.",
            "citation_title": "Robust log-based anomaly detection on unstable log data",
            "mention_or_use": "mention",
            "model_name": "LogRobust; LogAnomaly",
            "model_type": "Attention-based Bi-LSTM sequence models (RNNs) with pretrained word embeddings",
            "model_size": null,
            "data_type": "Tokenized log messages, sequences of log events (text sequences), sometimes using token embeddings instead of template indices",
            "data_domain": "System logs (various)",
            "anomaly_type": "Sequential and quantitative anomalies in unstructured logs",
            "method_description": "Use pretrained word vectors to represent log messages and train attention-enabled Bi-LSTM sequence models to capture sequential/contextual patterns; anomalies detected by deviations in predicted sequences or learned patterns.",
            "baseline_methods": "Mentioned as related work and baselines in the general literature; not reimplemented in this paper but compared conceptually.",
            "performance_metrics": "F1, precision, recall as commonly reported in prior papers",
            "performance_results": "Not provided in this paper; LogAnomaly reportedly gives only marginal improvements over DeepLog in prior literature according to the authors.",
            "comparison_to_baseline": "Described as comparable to DeepLog in prior work; not directly compared numerically in this paper beyond qualitative statements.",
            "limitations_or_failure_cases": "Prior implementations may have marginal gains over index-based LSTM methods; still limited by domain mismatches of pretraining and by the need for robust representation of unseen logs.",
            "unique_insights": "Incorporating pretrained word vectors into sequence models helps but, per this paper, is not sufficient to close the generalization gap that Logsy addresses via auxiliary anomaly data and hyperspherical objective.",
            "uuid": "e9268.4",
            "source_info": {
                "paper_title": "Self-Attentive Classification-Based Anomaly Detection in Unstructured Logs",
                "publication_date_yy_mm": "2020-08"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Deeplog: Anomaly detection and diagnosis from system logs through deep learning",
            "rating": 2,
            "sanitized_title": "deeplog_anomaly_detection_and_diagnosis_from_system_logs_through_deep_learning"
        },
        {
            "paper_title": "Robust log-based anomaly detection on unstable log data",
            "rating": 2,
            "sanitized_title": "robust_logbased_anomaly_detection_on_unstable_log_data"
        },
        {
            "paper_title": "Loganomaly: Unsupervised detection of sequential and quantitative anomalies in unstructured logs",
            "rating": 2,
            "sanitized_title": "loganomaly_unsupervised_detection_of_sequential_and_quantitative_anomalies_in_unstructured_logs"
        },
        {
            "paper_title": "Attention is all you need",
            "rating": 1,
            "sanitized_title": "attention_is_all_you_need"
        },
        {
            "paper_title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
            "rating": 1,
            "sanitized_title": "bert_pretraining_of_deep_bidirectional_transformers_for_language_understanding"
        },
        {
            "paper_title": "Distributed representations of words and phrases and their compositionality",
            "rating": 1,
            "sanitized_title": "distributed_representations_of_words_and_phrases_and_their_compositionality"
        },
        {
            "paper_title": "Detecting large-scale system problems by mining console logs",
            "rating": 2,
            "sanitized_title": "detecting_largescale_system_problems_by_mining_console_logs"
        },
        {
            "paper_title": "Experience report: Log mining using natural language processing and application to anomaly detection",
            "rating": 1,
            "sanitized_title": "experience_report_log_mining_using_natural_language_processing_and_application_to_anomaly_detection"
        }
    ],
    "cost": 0.016762,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Self-Attentive Classification-Based Anomaly Detection in Unstructured Logs</h1>
<p>Sasho Nedelkoski<em>, Jasmin Bogatinovski</em>, Alexander Acker<em>, Jorge Cardoso ${ }^{\dagger}$, Odej Kao</em><br>*Distributed and Operating Systems, TU Berlin, Berlin, Germany<br>{nedelkoski, jasmin.bogatinovski, alexander.acker, odej.kao}@tu-berlin.de<br>${ }^{\dagger}$ Huawei Munich Research Center, Huawei Technologies, Munich, Germany<br>jorge.cardoso@huawei.com</p>
<h4>Abstract</h4>
<p>The detection of anomalies is essential mining task for the security and reliability in computer systems. Logs are a common and major data source for anomaly detection methods in almost every computer system. They collect a range of significant events describing the runtime system status. Recent studies have focused predominantly on one-class deep learning methods on predefined non-learnable numerical log representations. The main limitation is that these models are not able to learn log representations describing the semantic differences between normal and anomaly logs, leading to a poor generalization of unseen logs. We propose Logsy, a classification-based method to learn log representations in a way to distinguish between normal data from the system of interest and anomaly samples from auxiliary log datasets, easily accessible via the internet. The idea behind such an approach to anomaly detection is that the auxiliary dataset is sufficiently informative to enhance the representation of the normal data, yet diverse to regularize against overfitting and improve generalization. We propose an attention-based encoder model with a new hyperspherical loss function. This enables learning compact log representations capturing the intrinsic differences between normal and anomaly logs. Empirically, we show an average improvement of 0.25 in the F1 score, compared to the previous methods. To investigate the properties of Logsy, we perform additional experiments including evaluation of the effect of the auxiliary data size, the influence of expert knowledge, and the quality of the learned log representations. The results show that the learned representation boost the performance of the previous methods such as PCA with a relative improvement of $\mathbf{2 8 . 2 \%}$.</p>
<p>Index Terms-anomaly detection, log data, transformers, systems reliability</p>
<h2>I. INTRODUCTION</h2>
<p>Anomaly detection [1]-[3] is a data mining task of finding observations in a corpus of data that differ from the expected behaviour. Anomalies in large systems such as cloud and highperformance computing (HPC) platforms can impact critical applications and a large number of users [4]. Owing to the inevitable weaknesses in software and hardware, systems are prone to failures, which can potentially harm them to a large extent [5], [6]. Timely and accurate detection of such threats is necessary for reliability, stable operation, and mitigation of losses in a complex computer system.</p>
<p>Logs are an important data source for anomaly detection in computer systems [7]-[9]. They represent interactions between data, files, services, or applications, and are typically utilized by developers, and data-driven methods to understand system behaviours and to detect, localize, and resolve problems that
may arise. Log messages have free-form text structure written by the developers, which record a specific system event describing the runtime system status. Specifically, a log message is a composition of constant string template and variable values originating from logging instruction (e.g., print("total of $\%$ i errors detected", 5)) within the source code.</p>
<p>A common approach for log anomaly detection is one-class classification [10], where the objective is to learn a model that describes the normal system behaviour, usually assuming that most of the unlabeled training data is non-anomalous and that anomalies are samples that lie outside of the learned decision boundary. The massive log data volumes in large systems have renewed the interest in the development of oneclass deep learning methods to extract general patterns from non-anomalous samples. Previous studies have been focused mostly on the application of long short-term memory (LSTM)based models [8], [9], [11]. They leverage log parsing [12], [13] on the normal log messages and transform them into log templates, which are then utilized to train the models. The formulated task is to predict the next index of the log template in the sequence $t_{m+1}$ by utilizing the history of template indices $H=t_{0}, \ldots, t_{m}$. In other disciplines, numerous deep learning methods increase their performances by incorporating large amounts of data available through the internet. A common approach to use these data is unsupervised learning. In natural language processing (NLP), word2vec [14] and more recent language models BERT [15] are standard and responsible for significant improvements in various NLP tasks. These models are pretrained on large corpora of text such as Wikipedia and later fine-tuned on the particular task or dataset. Recent studies in log anomaly detection [9], [11] utilize a pre-trained word embeddings to numerically represent the log templates instead of the integer log sequences [8], where they observe small improvements in the prediction of unseen logs.</p>
<p>However, the learning of the sequence of template indices and the enhanced log message embedding approaches still have large limitations in terms of generalization for previously unseen log messages. They tend to produce false predictions owing to the imperfect log vector representations. For example, learning sequence of indices fails to correctly classify a newly appearing log messages, and, the domain where the word vectors are pre-trained (e.g., Wikipedia) has essential differences from the language used in computer system de-</p>
<p>velopment. To partly mitigate some of these limitations, a possibility is to incorporate labeled data from operators and perform life-long learning [16]. Yet, it still requires frequent periodical retraining, updates, and costly expert knowledge to label the data, without addressing the problem of generalization on unseen logs that appear between retraining epochs.</p>
<p>Often, the assumption for the normal data in anomaly detection methods is that it should be compact [17]. This means the normal log messages should have vector representations with close distances between each other, e.g., concentrated within a tight sphere, and the anomalies should be spread far from the distribution of the normal samples. We propose a new anomaly detection method that directly addresses the challenge of obtaining representative and compact numerical log embeddings. We train a neural network to learn log vector representations in a manner to separate the normal log data from the system of interest and log messages from auxiliary log datasets from other systems, easily accessible via the internet. The concept of such a classification approach to anomaly detection is that the auxiliary dataset helps learn a better representation of the normal data while regularizing against overfitting. This ultimately leads to a better generalization in unseen logs. For example, for a target system logs of interest $T$ where anomaly detection needs to be performed, as auxiliary data could be employed one or more datasets from an open-source log repository (e.g., [18]). As a neural network architecture, we adopt the Transformer encoder with multi-head self-attention mechanism [19], which learns context information from the log message in the form of log vector representations (embeddings). We propose a hyperspherical learning objective that enforces the model to learn compact log vector representations of the normal log messages. This enforces for the normal samples to have concentrated (compact) vector representations around the centre of a hypersphere. It enables better separation between the normal and the anomaly data, where a distance from the centre of such a sphere is used to represent an anomaly score. Small distances correspond to normal samples, while large distances correspond to anomalies. The method also enables a direct log-to-vector transformation, which can be used to improve the performances of previous related methods. Additionally, it allows the operator to intervene and correct misclassified samples, which could be used for the next retraining of the model.</p>
<p>The contributions of this study can be summarized in the following points.</p>
<ol>
<li>A new classification-based method for log anomaly detection utilizing self-attention and auxiliary easyaccessible data to improve log vector representation.</li>
<li>Modified objective function using hyperspherical decision boundary, which enables compact data representations and distance-based anomaly score.</li>
<li>The proposed approach is evaluated against three real anomaly detection datasets from HPC systems, Blue Gene/L, Thunderbird, and Spirit. The method significantly improves the evaluation scores compared to those in the previous studies.</li>
<li>In another set of experiments, an investigation of the effects of variations in the amount of auxiliary data for anomaly detection and inclusion of labelled data is performed.</li>
<li>We provide an open-source implementation of the method.</li>
</ol>
<h2>II. Related Work</h2>
<p>A significant amount of research and development of methods for log anomaly detection has been published in both industry and academia [8], [9], [11], [12], [20], [21].Supervised methods were applied in the past to address the log anomaly detection problem. For example, [20] applied a support vector machine (SVM) to detect failures, where both normal and anomalous samples are assumed to be available. For an overview of supervised approaches to log anomaly detection we refer to Brier et al. [22]. However, obtaining system-specific labelled samples is costly and often practically infeasible.</p>
<p>Several unsupervised learning methods have been proposed as well. Xu et al. [21] proposed using the Principal Component Analysis (PCA) method, where they assume that there are different sessions in a log file that can be easily identified by a session-id attached to each log entry. It first groups log keys by session and then counts the number of appearances of each log key value inside each session. A session vector is of size $n$, representing the number of appearances for each log key in $K$ in that session. A matrix is formed where each column is a log key, and each row is one session vector. PCA detects an abnormal vector (a session) by measuring the projection length on the residual subspace of a transformed coordinate system. The publicly available implementation allows for the term frequency-inverse document frequency (TF-IDF) representation of the log messages, utilized in our experiments as a baseline. Lou et al. [23] proposed Invariant Mining (IM) to mine the linear relationships among log events from log event count vectors.</p>
<p>The wide adoption of deep learning methods resulted in various new solutions for log-based anomaly detection. Zhang et al. [24] used LSTM to predict the anomaly of log sequence based on log keys. Similar to that, DeepLog [8] also use LSTM to forecast the next log event and then compare it with the current ground truth to detect anomalies. Vinayakumar et al. [25] trained a stacked-LSTM to model the operation log samples of normal and anomalous events. However, the input to the unsupervised methods is a one-hot vector of logs representing the indices of the log templates. Therefore, it cannot cope with newly appearing log events.</p>
<p>Some studies have leveraged NLP techniques to analyze log data based on the idea that log is a natural language sequence. Zhang et al. [24] proposed to use the LSTM model and TF-IDF weight to predict the anomalous log messages. Bertero et al. [26] used word2vec and traditional classifiers, like SVM and Random Forest, to check whether a log event is an anomaly or not. Similarly, LogRobust [9] and LogAnomaly [11] incorporate pre-trained word vectors for</p>
<p>learning of a sequence of logs where they train an attentionbased Bi-LSTM model.</p>
<p>Different from all the above methods, we add domain bias on the anomalous distribution to improve detection [27]. We provide such bias by employing easily accessible log datasets as an auxiliary data source. We evaluate Logsy against unsupervised approaches, as even it is a classification based approach, it does not use labels from the target system, which as mentioned are often infeasible to obtain. From the perspective of using labels of the target system it is an unsupervised approach.</p>
<h2>III. Towards Classification-Based Log Anomaly DETECTION</h2>
<p>Anomaly detection can be also viewed as density level set estimation [28]. Steinwart et al. [27] state that this can be interpreted as binary classification between the normal and the anomalous distribution and point out that the bias on the anomalous distribution is essential for improved detection. Meaning that if we provide some information to the model of how anomalous data looks like, it will boost its performance. For instance, we may interpret the class assumption that semi-supervised anomaly detection approaches require on the anomalies, as such prior knowledge [17]. Moreover, specific types of data can have an inherent properties that allows us to make more informed prior assumptions such as the word representations in texts [29]. Here the assumption is that each word meaning depends on its context.</p>
<p>We assume that drawing realistic samples from some auxiliary easy-access corpus of log data, can be much more informative for an added description of normal and anomalies compared to sampling noise, or no data used. The use of auxiliary data adds extra value to the method, while preserving the information from the normal data.</p>
<p>Problem Definition. Let $\mathcal{D}=\left{\left(\mathbf{x}<em 1="1">{\mathbf{1}},y</em>}\right), \ldots,\left(\mathbf{x<em n="n">{\mathbf{n}}, y</em>}\right)\right}$ be the training logs from the system of interest where $\mathbf{x<em i="i">{\mathbf{i}} \in$ $\mathbb{R}^{d}$ is a log message where it words are represented in $d$-dimensional space (the log message is represented by $d \times|r|$ matrix, where $|r|$ is number of words) and $y</em>}=0 ; 1&lt;i \leq n$, assuming that the data in the system of interest is mostly composed of normal samples. Let $\mathcal{A}=\left{\left(\mathbf{x<em n="n">{\mathbf{n}}, y</em>}\right), \ldots,\left(\mathbf{x<em n_m="n+m">{\mathbf{n}+\mathbf{m}}, y</em>}\right)\right}$, where $m$ is the size of the auxiliary data and $y_{i}=1 ; n&lt;i \leq n+m$. Let $\phi\left(\mathbf{x<em i="i">{\mathbf{i}}, y</em>}, \theta\right): \mathbb{R}^{d} \rightarrow \mathbb{R}^{p}$ be a function represented by a neural network, which maps the input log message embeddings to vector representations in $\mathbb{R}^{p}$, and $l: \mathbb{R}^{p} \rightarrow[0, a], a \in \mathbb{R}$ be a function, which maps the output to an anomaly score. The task is to learn the parameters $\theta$ from the training data, and then for each incoming instance in the prediction phase $\mathcal{D<em _mathbf_1="\mathbf{1">{t}=\left{\left(\mathbf{x}</em>}}^{\mathbf{t}}\right),\left(\mathbf{x<em _mathbf_t="\mathbf{t">{\mathbf{2}}^{\mathbf{t}}\right), \ldots,\left(\mathbf{x}</em>}}^{\mathbf{t}}\right), \ldots\right}$, $t$ indicates test sample, predict whether it is anomaly or normal based on the anomaly scores obtained by $l\left(\phi\left(\mathbf{x<em i="i">{\mathbf{i}}, y</em>, \theta\right)\right)$.</p>
<h2>IV. Self-ATtentive Anomaly Detection With CLASSIFICATION-BASED OBJECTIVE</h2>
<p>In this section, we explain the proposed method in detail. We provide formal definitions needed for explaining the method. We describe the data preprocessing, the neural network, the log vector representations, and how they are utilized in the modified objective function for anomaly detection.</p>
<h2>A. Preliminaries</h2>
<p>We define a log as a sequence of temporally ordered unstructured text messages $L=\left(x_{i}: i=1,2, \ldots\right)$, where each message $x_{i}$ is generated by a logging instruction (e.g. printf(), log.info()) within the software source code, and $i$ is its positional index within the sequence. The log messages consist of a constant and an optional varying part, respectively referred to as log template and variables.</p>
<p>The smallest inseparable singleton object within a log message is a token. Each log message consists of a finite sequence of tokens, $\mathbf{r}<em j="j">{\mathbf{i}}=\left(w</em>}: w_{j} \in \mathbb{V}, j=1,2, \ldots, s_{i}\right)$, where $\mathbb{V}$ is a set (vocabulary) of all tokens, $j$ is the positional index of a token within the log message $x_{i}$, and $s_{i}$ is the total number of tokens in $x_{i}$. We use $\left|r_{i}\right|$ instead of $s_{i}$ in following. For different $x_{i},\left|\mathbf{r<em j="j">{\mathbf{i}}\right|$ can vary. Depending on the concrete tokenization method, $w</em>$.}$ can be a word, word piece, or character. Therefore, tokenization is defined as a transformation function $\mathcal{T}: x \rightarrow \mathbf{r</p>
<p>With respect to our proposed method, the notions of context and numerical vector representation (embedding vector) are additionally introduced. Given a token $w_{j}$, its context is defined by a preceding and subsequent sequence of tokens, i.e. a tuple of sequences: $C\left(w_{j}\right)=\left(\left(w_{1}, w_{2}, \ldots, w_{j-1}\right)\right.$, $\left.\left(w_{j+1}, w_{j+2}, \ldots, w_{\left|\mathbf{r}<em _mathbf_i="\mathbf{i">{\mathbf{i}}\right|}\right)\right)$, where $0 \leq j \leq\left|\mathbf{r}</em>$ of either a token or a log message.}}\right|$. An embedding vector is a $d$-dimensional real valued vector representation $\mathbf{s} \in \mathbb{R}^{d</p>
<p>In the learned vector space, similar log messages should be represented by closer embedding vectors while largely different log messages should be distant. For example, the embedding vectors for "Took 10 seconds to create a VM" and "Took 9 seconds to create a VM" should have a small distance in $d$-dimensional space, while vectors for "Took 9 seconds to create a VM" and "Failed to create VM 3" should be distant.</p>
<p>We refer to the data from the system of interest as target dataset, i.e., the system where we want to detect anomalies. Important to note is that we are not using any anomaly data from the target system for learning purposes in our experiments. The term auxiliary data refers to other nonrelated systems, which serve only for training the model. All the results during test time are performed on a test set extracted from the target dataset.</p>
<h2>B. Logsy</h2>
<p>The method is composed of two main parts, the tokenization of the log messages and the neural network model. In the following section, we discuss the inner workings of the proposed method, which is depicted in Fig. 1.</p>
<p>Tokenization. Tokenization transforms the raw log messages into a sequence of tokens, as shown in Fig. 1. For this purpose, we utilize the standard text preprocessing library NLTK [30]. The message is first filtered for HTTP and system path endpoints (e.g., /p/gb2/stella/RAPTOR/). Every capital letter is converted to a lower letter, and all of the ASCII special characters are removed. The log message is split into word tokens. We remove every token that contains numerical characters, as they often represent variables in the log message and are not informative. Additionally, we remove the most commonly used English words that are in the stop words dictionary of NLTK (e.g., the and is). To the front of the tokenized log message, a special '[EMBEDDING]' token is added. In the model, the '[EMBEDDING]' token attends overall original tokens from the sample, which enables the model to summarize the context of the log message in the vector representation. All tokens from every log message form vocabulary $\mathbb{V}$ of size $|\mathbb{V}|$, where each token is represented with integer label $i \in 0,1, \ldots,|\mathbb{V}|-1$. An important advantage of Logsy compared to previous approaches is that it does not depend on log parsers as a pre-processing step. We consider the tokenized log message as direct input to the model. The advantage is that there is no loss of information from the log message, due to the imperfections that exist in the log parsing methods.
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Fig. 1. Overview of the architecture and component details of Logsy.</p>
<p>Model. Logsy has two operation modes - offline and online. During the offline phase, log messages are used to tune all model parameters via backpropagation and optimal hyperparameters are selected. During the online phase, every log message is passed forward through the saved model. This generates the respective log vector representation $\mathbf{z}$ and an anomaly score for each message.</p>
<p>As depicted in Fig. 2, the model applies two operations on the input tokens: token vectorization (word embeddings) and positional encoding. The subsequent structure is the encoder of the Transformer [19] module with multi-head self-attention, which takes the result of these operations as input. At the output of the encoder, there are $\left|r_{i}\right|$ transformed vector representation from the initial tokens. Recall that the '[EMBEDDING]' token has its transformed representation, which is used as a final log vector representation. We denote the size of this vector as $d$. This also represents the size of all the layers of the model and the word embeddings. The last two parts are the objective (loss) function during training and the computation of the anomaly score for test-time samples. Based on the loss, gradients are back-propagated to tune the parameters of the model, while based on the anomaly score we decide if the sample is anomalous or normal. In the following, we provide a detailed explanation of each element of the method. Fig. 2 depicts the inner working of the transformer encoder.</p>
<p>Since all subsequent elements of the model expect numerical inputs, we initially transform the tokens into randomly initialized numerical vectors $\mathbf{x} \in \mathbb{R}^{d}$. These vectors are referred to as token embeddings and are part of the training process, which means they are adjusted during training to represent the semantic meaning of tokens depending on their context. These numerical token embeddings are passed to the positional encoding block. In contrast to e.g., recurrent architectures, attention-based models do not contain any notion of input order. Therefore, this information needs to be explicitly encoded and merged with the input vectors to take their position within the log message into account. This block calculates a vector $\mathbf{n} \in \mathbb{R}^{d}$ representing the relative position of a token based on a sine and cosine function.</p>
<p>$$
n_{2 k}=\sin \left(\frac{j}{10000^{\frac{2 k}{d}}}\right), n_{2 k+1}=\cos \left(\frac{j}{10000^{\frac{2 k+1}{d}}}\right)
$$</p>
<p>Here, $k=0,1, \ldots, d-1$ is the index of each element in $\mathbf{n}$ and $j=1,2, \ldots,\left|r_{i}\right|$ is the positional index of each token. Within the equations, the parameter $k$ describes an exponential relationship between each value of vector $\mathbf{n}$. The applied sine and cosine functions allow for better discrimination of the respective values within a specific vector of $\mathbf{n}$. They have an approximately linear dependence on the position parameter $j$, which is hypothesized to make it easy for the model to attend to the respective positions. Finally, both vectors can be combined as $\mathbf{x}^{\prime}=\mathbf{x}+\mathbf{n}$. We summarize all token embedding vectors of a log message as matrix rows $\mathbf{x}^{\prime T} \in X^{\prime}$ on which the following formula is applied:</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Fig. 2. Transformer encoder architecture with multi-head self-attention.</p>
<p>$$X_l'' = \text{softmax}\left(\frac{Q_l \times K_l^T}{\sqrt{w}}\right) \times V_l, \text{ for } l = 1, 2, \dots, L. \tag{2}$$</p>
<p>Thereby, $L$ denotes the number of attention heads, $w = \frac{d}{L}$ and $d \mod L = 0$. The parameters $Q$, $K$ and $V$ are matrices, that correspond to the query, key, and value elements in Fig. 2. They are obtained by applying matrix multiplications between the input $X'$ and respective learnable weight matrices $W_l^Q$, $W_l^K$, $W_l^V$:</p>
<p>$$Q_l = X' \times W_l^Q, \quad K_l = X' \times W_l^K, \quad V_l = X' \times W_l^V, \qquad (3)$$</p>
<p>where $W_l^Q$, $W_l^K$, $W_l^V$ ∈ℝ$^{M \times w}$. The division by $\sqrt{w}$ stabilizes the gradients during training. After that, the softmax function is applied and the result is used to scale each token embedding vector $V_l$. The scaled matrices $X_l''$ are concatenated to a single matrix $X''$ of size $M \times d$.</p>
<p>As depicted in Fig. 2 there is a residual connection between the input token matrix $X'$ and its respective attention transformation $X''$, followed by a normalization layer norm. These are used for improving the performance of the model by tackling different potential problems encountered during the learning such as small gradients and the covariate shift phenomena. Based on this, the original input is updated by the attention-transformed equivalent as $X' = \text{norm}(X' + X'')$.</p>
<p>The last element of the encoder consists of two feed-forward linear layers with a ReLU activation in between. It is applied individually on each row of $X'$. Thereby, identical weights for every row are used, which can be described as a convolution over each attention-transformed matrix row with kernel size one. This step serves as additional information enrichment for the embeddings. Again, a residual connection followed by a normalization layer between the input matrix and the output of both layers is employed. This model element preserves the dimensionality $X'$.</p>
<p>The final element of the model consists of a single linear layer. It receives the encoder result $X'$ and extracts the token embedding vector of the [EMBEDDING']. Since every log message token sequence is pre-padded by this special token, it is the first row of the matrix, i.e. $\mathbf{x}_{1,0}' \in X'$, $\forall i$. This vectors are the log vector representations and are used in the objective function and as well as log message embeddings.</p>
<h3><em>C. Objective function</em></h3>
<p>To ensure learning of the intrinsic differences of normal and anomaly log samples, we propose a spherical loss function. It is designed to integrate the previously mentioned assumption that normal data is often concentrated having close distances between the normal samples, while also learning properties to distinct from anomalous samples. This is done by employing a radial classification loss which enforces a compact hyperspherical decision region for the normal samples.</p>
<p>To derive the loss, we start with the standard binary cross entropy. Let $\mathcal{D} = { (\mathbf{x}<em n_m="n+m">{1}, y_1), \dots, (\mathbf{x}</em>$, where $|r_i|$ is the number of tokens in the log message and each token is a vector represented in $d$ – }, y_{n+m}) }$ be the concatenation of the training logs from the system of interest and the auxiliary data with $\mathbf{x}_i \in \mathbb{R}^{d \times |r_i|<em>dimensional</em> space. $y_i \in {0, 1}$, and $y_i = 0$ denotes normal samples (target system), while $y_i = 1$ denotes an anomaly (auxiliary data). Let $\phi(\mathbf{x}_i, \theta) : \mathbb{R}^d \to \mathbb{R}^p$ be our encoder architecture that maps the $|x_i|$ word embeddings from the log message to $p$ – <em>dimensional</em> vector. Let $l : \mathbb{R}^p \to [0, 1]$ be a function which maps the output to an anomaly score. Using $\phi(\mathbf{x}_i, \theta)$ and $l(\cdot)$, the standard binary cross-entropy loss can be written as:</p>
<p>$$-\frac{1}{n} \sum_{i=1}^{n} (1 - y_i) \log l(\phi(\mathbf{x}_i; \theta)) + y_i \log(1 - l(\phi(\mathbf{x}_i; \theta))) \tag{4}$$</p>
<p>For standard classifier function the $p$ – <em>dimensional</em> representation is transformed via linear layer followed by sigmoid activation function:</p>
<p>$$-\frac{1}{n} \sum_{i=1}^{n} (1 - y_i) \log((1 + \exp(-\mathbf{w}^T \phi(\mathbf{x}_i, \theta)))^{-1}) + y_i \log(1 - (1 + \exp(-\mathbf{w}^T \phi(\mathbf{x}_i, \theta)))^{-1})$$</p>
<p>In the standard binary classifier with sigmoid function, the decision boundary is half-space. The representation of the log messages is not guaranteed to be compact in this case. It could be very possible that the normal samples are scattered through the space with varying, potentially very large distances between them. To enforce compactness of the representations</p>
<p>of the log messages we utilize the Gaussian radial basis function as $l(\cdot)$:</p>
<p>$l(\mathbf{z})=\exp(-|\mathbf{z}|^{2})$ (6)</p>
<p>Replacing the function into the loss function we get the hyper-spherical classifier:</p>
<p>$\frac{1}{n}\sum_{i=1}^{n}(1-y_{i})|\phi(\mathbf{x_{i}};{\theta})|^{2}$
$-y_{i}\log(1-\exp(-|\phi(\mathbf{x_{i}};{\theta})|^{2}))$</p>
<p>This ensures compactness of the normal samples, which will be enforced to be around the center of a sphere $\mathbf{c}=\mathbf{0}$. For normal samples, i.e., $y_{i}=0$, the loss function will minimize the distance to $\mathbf{c}$. This results in low values for the left term in Equation 7. In contrast, the right term of the loss function favors large distances for the anomalous samples. The center of a sphere $c$ could be any constant value, which is not relevant during the optimization.</p>
<p>A possible problem that usually arises in such spherical classifiers [17] is that the model is prone to learn trivial solutions by mapping the inputs to output a constant vector, i.e. $c$. However, the proposed loss function will not find the trivial solution because of the second term in the equation, representing the auxiliary data or the anomalies. To formally show that, let $\phi(\cdot)$ be the encoder network, which maps every log message to $\mathbf{c}$. It follows that that $\phi(\cdot)=\mathbf{0}$. In this case, the second term in Equation 7 for $y_{i}=1$ will be infinity in the limit, which acts as a regularizer and prevents learning $\mathbf{c}$ as a trivial vector representation.</p>
<h2>D. Anomaly score and detecting anomalies</h2>
<p>Considering that the assumption of the objective function enforces compact, close to the center of the sphere $\mathbf{c}=\mathbf{0}$, representations, we define our anomaly score as the distance of the log vectors (obtained from the ’EMBEDDING’ token) to the center $\mathbf{c}$ of the hypersphere.</p>
<p>$A(\mathbf{x_{i}})=|\phi(\mathbf{x_{i}};{\theta})|^{2}$ (8)</p>
<p>We define low anomaly scores $A(\mathbf{x_{i}})$ to be normal log messages, while large scores stand for the anomalies. To decide if the sample is anomalous or normal, we use a threshold $\mathcal{E}$. If the anomaly scores $A(\mathbf{x_{i}})&gt;\mathcal{E}$, then the sample is an anomaly, otherwise, we consider it as normal. This concludes the explanation of the inner workings of the method. In the following, we describe two properties of the model.</p>
<h2>E. Including expert knowledge</h2>
<p>Most computer systems, are to some extend, supervised and operated by an administrator. Over time, the administrator can manually inspect a small portion of the log events and provide labels. As additional option, Logsy allows incorporation of such labels from the target system. The second term in Equation 7, used for the auxiliary data, could be also utilized for the inclusion of operator-labeled samples. This enables the addition of even more realistic, however, costly anomaly samples that help to learn the anomaly distribution, and further improve the performance. The labeled samples either need to be added together with the auxiliary data and retrain, or pre-training the model with the normal and auxiliary data followed by fine-tuning with the labeled data. With such a training procedure, the model extracts the relevant information from the auxiliary data and already learns good log representations for anomaly detection, as later shown in the experiments. The replacement of the auxiliary data with the labeled samples allows the model to only fine-tune its parameters in a few epochs. This preserves the already learned information from the larger auxiliary dataset as a bias to the fine-tuning procedure. In the experiments, we show that the inclusion of a small portion of labeled samples improves the performance of the model.</p>
<h2>F. Vector representations of the logs</h2>
<p>Learning numerical vector representations from the logs is fundamental for the performance of any machine learning method for log anomaly detection. Logsy can be utilized for obtaining such numerical log representations. These representations are used by the objective function of the method, to perform anomaly detection, but could be as well used to replace other, less powerful representations (e.g., TF-IDF in previous log-based anomaly detection methods such as the PCA [21]), aiming to enhance their anomaly detection.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Fig. 3. Ideal distribution of the log vector representations in space.</p>
<p>The transformed vector of the [’EMBEDDING’] token is used for representing the context of the log message, which is the only output of the model to the loss function. Thus, it is forced to summarize the log message. By using the spherical classification decision boundary, we enforce the normal samples to be close to each one and compactly represented around the center of the sphere. This leaves the anomalies to disperse around the spherical decision boundary in the highdimensional space. In Fig. 3, we illustrate a lower-dimensional plot of how the ideal log representations should look like. A decision boundary (dashed line) can be drawn to optimally separate the classes. We demonstrate such behavior in the evaluation section on real data with Logsy, where we show</p>
<p>TABLE I: DATASET DETAILS.</p>
<table>
<thead>
<tr>
<th>System</th>
<th>#Messages</th>
<th>#Anomalies</th>
<th>#Anomalies5m</th>
<th>#Unique Log messages</th>
<th></th>
<th></th>
<th></th>
<th></th>
<th>total unique messages</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td>in test and not in train for every split</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Blue Gene/L</td>
<td>4747963</td>
<td>348460</td>
<td>348460</td>
<td>2679</td>
<td>2621</td>
<td>2256</td>
<td>2231</td>
<td>465</td>
<td>4486</td>
</tr>
<tr>
<td>Thunderbird</td>
<td>211212192</td>
<td>3248239</td>
<td>226287</td>
<td>334</td>
<td>127</td>
<td>71</td>
<td>27</td>
<td>12</td>
<td>3279</td>
</tr>
<tr>
<td>Spirit</td>
<td>272298969</td>
<td>172816564</td>
<td>764890</td>
<td>1091</td>
<td>1028</td>
<td>297</td>
<td>129</td>
<td>73</td>
<td>3441</td>
</tr>
</tbody>
</table>
<p>how the normal and abnormal samples are distributed in low dimensional space.</p>
<h2>V. Evaluation</h2>
<p>To quantify the performance of Logsy, we perform a variety of experiments. We compare the method against two publicly available baselines DeepLog and PCA on three realworld HPC log datasets. We describe the main properties of the datasets, discuss the experimental setup, and present the results. We empirically and qualitatively evaluate the log vector representations from Logsy, where we utilize them in the PCA method and observed improved performance. Logsy is evaluated against unsupervised approaches, as from the perspective of using labels of the target system, it is an unsupervised approach.</p>
<h2>A. Experimental setup</h2>
<p>We select three open real-world datasets from HPC systems for evaluation as target systems, namely Blue Gene/L, Spirit, and Thunderbird [18]. They share an important characteristic associated with the appearance of many new log messages in the timeline of the data, i.e., the systems change over time. Furthermore, as an additional dataset for enriching the auxiliary data in all experiments we use the HPC RAS log dataset [31]. Due to the absence of labels this dataset cannot be used for evaluation purposes-can not be a target dataset.</p>
<p>For each target dataset as an auxiliary data to represent the anomaly class we use logs from the remaining datasets. It is important to note that the target vs auxiliary splits, ensure that there is no leak of information from the target system into the auxiliary data. Meaning, there are no labeled samples from the target system into the auxiliary data. These logs consist only of easily accessible logs from other systems via the internet. The non-anomalous samples from the target system are the target dataset. For example, when Blue Gene/L is our system of interest (i.e., the target system) proportion of the negative samples of Thunderbird, Spirit, and RAS are used as an auxiliary dataset to represent the anomaly class. These auxiliary samples could be also error messages obtained from online code repositories (e.g., GitHub). We perform anomaly detection on the test samples from the target dataset for determining the scores.</p>
<p>The datasets are collected between 2004 and 2006 on three different supercomputing systems: Blue Gene/L, Thunderbird, and Spirit. The logs contain anomaly and normal messages identified by anomaly category tags and are therefore amenable to anomaly detection and prediction research. All
systems were ranked on the Top500 Supercomputers List at the time (as of June 2006). The various machines are produced by IBM, Dell, Cray, and HP. All systems were installed at Sandia National Labs (SNL), except Blue Gene/L, which is at Lawrence Livermore National Labs (LLNL). In Table I we summarize the main characteristics of the datasets.</p>
<p>Table I shows that Thunderbird and Spirit are quite large datasets of more than 200 million log messages. For computation-time purposes we restrict the data size on the first 5 million, when sorted by timestamp, log messages. We ensure that the 5 million log lines preserve the properties of the dataset, as shown in Table I, which is that new unseen logs appear in the test data split. The Blue Gene/L dataset has less than 5 million messages, thus we keep it in total. #Anomalies5m shows the number of anomalous log messages in those 5 million messages.</p>
<p>To evaluate the robustness and generalization of Logsy in detail, we conduct several experiments with different traintest splits on the target dataset. To ensure that the test data contains new log messages previously unseen in the training we always split the data when sorted by the timestamp of the log messages. We perform 5 different data splits to cover as many possible scenarios, i.e., the first $10 \%$ training; $90 \%$ test data, $20 \%$ training $-80 \%$ test, $40 \%$ training $-60 \%$ test, $60 \%$ training $-40 \%$ test, and $80 \%$ training $-20 \%$ test.</p>
<p>The number of unique log messages after tokenization is presented in Table I. We observe that in every split there are new previously unseen log messages that appear in the test data, which is the main point for empirically proving generalization. Decreasing the size of the training data increases the number of novel log messages in the test split.</p>
<p>1) Evaluation methods: To enable comparability between our method to the previous work, we adopt the standard evaluation scores. We evaluate our method in F1-score, precision, recall, accuracy, which depends on the true negatives (TN), true positives (TP), false negatives (FN), and false positives (FP) predictions. The positive class of 1 , is assumed to be an anomalous log.
2) Baselines: We compare Logsy against two publicly available baseline methods, i.e., PCA [21] and Deeplog [8]. The current claimed state-of-the-art method LogAnomaly [11] to best of our knowledge has no publicly available implementation, as it is industry-related research. Moreover, LogAnomaly reports only marginal improvement over DeepLog of 0.03 F 1 score, and thus both approaches are relatively comparable. The parameters of these methods are all tuned to produce their best F1 score.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Fig. 4. Comparison of the evaluation scores against the two baselines DeepLog and PCA on three different datasets.</p>
<p>3) <strong>Logsy: Implementation details</strong>: Every log message during tokenization is truncated to a maximum of max(|r_i|) = 50 tokens. Logsy has two layers of the transformer encoder, i.e., N=2 in Fig. 1. The words are embedded with 16 neural units, and the higher level vector representations obtained with the transformer encoding are all of the same sizes. The size of the feed-forward network that takes the output of the multi-head self-attention mechanism is also 16, which makes the '[EMBEDDING]' vector the same size. For the optimization procedure for every experiment, we use a dropout of 0.05, Adam optimizer with a learning rate of 0.0001, weight decay of 0.001. We address the imbalanced number of normal versus anomaly samples with adding weights to the loss function for the two classes, 0.5 for the normal and 1.0 for the anomaly class. The models are trained until convergence and later evaluated on the respective test split.</p>
<h3>B. Results and discussion</h3>
<p>We show the overall performance of Logsy compared to the baselines in Fig. 4. Generally, Logsy achieves the best scores, having an averaged F1 score in all the splits of 0.448 on the Blue Gene/L dataset, 0.99 on the Thunderbird dataset, and 0.77 on the Spirit data. Both DeepLog and PCA, have lower F1 scores in all experiments performed. It is shown that the baselines have a very high recall, but also low precision. This means they can find the anomalies, however, producing large amounts of false-positive predictions. Logsy, on the other hand, preserves the high recall across the datasets and evaluation scenarios but shows a large improvement in the precision scores. This is due to the correct classification of new unseen log messages and the reduction of the false positive rate. For instance, on the Blue Gene/L dataset, DeepLog and PCA respectively show 2-4 times lower precision compared to Logsy. Overall, Logsy is the most accurate method having an average of 0.9. If a log anomaly detection method generates too many false alarms, it will add too much overhead to the operators and a large amount of unnecessary work. Therefore, high precision methods are favourable. DeepLog leverage the indexes of log templates, which ignore the meaning of the words in the log messages, to learn the anomalous and normal patterns. However, different templates having different indexes can share common semantic information and both could be normal. Ignoring this information results in the generation of false positives for DeepLog compared to Logsy.</p>
<p>We notice that increasing the training size also increases the F1 score in almost all methods, except for the last two splits in Spirit. These splits are unfortunate as they have a very small number of anomalies. Important to note is that Logsy outperforms the baselines even when only 10% of the data is training data. For example, in Blue Gene/L we have 0.32 F1-score on 10% training data, while the largest F1-score of the baselines is 0.24. In Thunderbird, this difference is even more noticeable, where an F1-score of 0.99 is already achieved in with the first 10%. This shows that even with a small amount of training data from the target system, Logsy extracts the needed information of what causes a log message to be normal or anomaly, and produces accurate predictions even in unseen samples.</p>
<p>1) <strong>The effect of the auxiliary data on the evaluation scores</strong>: In this experiment, we perform an analysis of how Logsy</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Fig. 5. Increasing the size auxiliary dataset, where the target system are Blue Gene/L, Thunderbird, and Spirit (left, middle, right) on 20% train - 80% test split</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Fig. 6. Increasing the size of the labeled anomaly data in the Blue Gene/L dataset (20% train - 80% test).</p>
<p>Performs when with various sizes of the auxiliary data. We evaluate the same target vs auxiliary data split for all datasets. We evaluate the approach on the 20%-80% train/test split. The results are shown in Fig. 5 for all datasets. When the auxiliary data increases from 1 to 250000 we observe an increase in all evaluation scores. We observe that increasing the size of the auxiliary data from 100000 to 250000 the scores do not change in both cases. This shows that the amount of information present in the auxiliary data is similar and all cases are already present in 100000 random samples. We note that having just one auxiliary sample, which might even be generated artificially, sufficiently acts as a regularizer to the hypersphere loss function, preventing it from learning trivial solutions. Of course, increasing the variety of data (e.g., including more diverse log datasets) could further improve the performance, due to the increased number of samples representing abnormality.</p>
<p>2) Including expert labeling: Often systems are operated by a human operator which is an expert and has system-specific knowledge. Sometimes they could provide or manually label samples to improve the performance of the model. Here we experiment with the incremental inclusion of anomaly labels of the target dataset to test the model behaviour. We experiment on the 20%-80% split of the Blue Gene/L dataset. Fig. 6 shows the results. Increasing the number of labelled anomaly samples improves performance. For as less as 2% labelled data we already have the best performance of 0.8 F1-score. This shows that adding a few percentages of anomalies as labelled samples to Logsy, the performance dramatically improves. This only strengthens the hypothesis where the log anomaly detection must be addressed with an understanding of what causes a log message to be normal or anomaly. The labelled anomalies from the target system present information Logsy exploits to learn the differences between the normal and anomalous logs on the target dataset.</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Fig. 7. Visualisations of the log vector representations of Blue Gene/L with T-SNE [32].</p>
<p>3) Utilization of the learned log embeddings in related approaches: In this experiment, we perform the extraction of the learned log message vector representations from the already trained Logsy. To illustrate the vector representations of the logs, in Fig. 7, we show their lower-dimensional representation of the test split via the T-SNE dimensionality reduction method [32] on the Blue Gene/L dataset. We show that the log vector representations are somehow structured in a way following the definition of our spherical loss function (see Section IV-F). We can observe that the normal samples are concentrated around the centre of a hypersphere, which is a circle in two dimensions. Most of the anomalies are dispersed among the space outside of the sphere. Assigning a threshold on the anomaly score A(x<sub>i</sub>), i.e., the distance from the centre of the sphere (circle), we could obtain good performance.</p>
<p>Furthermore, to evaluate the general importance of the log embeddings, we perform experiments where we replace the original TF-IDF log representations in PCA [21], as the lowest-performing method, with the extracted embeddings from Logsy. We depict the results in the bar plot in Fig. 8. We observe that this replacement of the log representation improves the performance of PCA. We show improvement of 0.09, 0.11, and 0.01 F1-score for Blue Gene/L, Thunderbird, and Spirit respectively. This demonstrates that log represen-</p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Fig. 8. Comparison in F1 score between the standard PCA [21] and PCA using the embeddings extracted from our method (80%-20% split).</p>
<p>tation learning has an impact, not only in Logsy, but also in previous approaches that could be adapted to use the new log embeddings. The relative improvement of the scores in average is 28.2% in the F1-score.</p>
<h2>VI. CONCLUSION</h2>
<p>Log anomaly detection is important to enhance the security and reliability of computer systems. Existing approaches lack generalization on new, unseen log samples, which comes from the evolution of logging statements as a consequence of system updates and the processing noise. To overcome this problem, we proposed a new anomaly detection approach, called Logsy. It is based on a self-attention encoder network with a hyperspherical classification objective. We formulated the log anomaly detection problem in a manner to discriminate between normal training data from the system of interest and samples from auxiliary easy-access log datasets from other systems, which represent an abnormality. We have presented experimental evidence that our classification-based method performs well for anomaly detection. The results of our method outperformed the baselines by a large margin of 0.25 F1 score. Furthermore, we demonstrated that the produced log vector representations could be utilized generally in other methods. We demonstrated that by adopting PCA to use the log vectors from Logsy, where we observed improvement of 0.07 (28.2%) in the F1 score.</p>
<p>We believe that future research on log anomaly detection should focus on finding alternative ways to incorporate richer domain bias emphasising the diversity of normal and anomaly data.</p>
<h2>REFERENCES</h2>
<ul>
<li>[1] F. E. Grubbs, "Procedures for detecting outlying observations in samples," <em>Technometrics</em>, vol. 11, no. 1, pp. 1–21, 1969.</li>
<li>[2] V. Hodge and J. Austin, "A survey of outlier detection methodologies," <em>Artificial intelligence review</em>, vol. 22, no. 2, pp. 85–126, 2004.</li>
<li>[3] M. A. Pimentel, D. A. Clifton, L. Clifton, and L. Tarassenko, "A review of novelty detection," <em>Signal Processing</em>, vol. 99, pp. 215–249, 2014.</li>
<li>[4] S. Zhang, Y. Liu, D. Pei, Y. Chen, X. Qu, S. Tao, and Z. Zang, "Rapid and robust impact assessment of software changes in large internet-based services," in <em>Proceedings of the 11th ACM Conference on Emerging Networking Experiments and Technologies</em>, 2015, pp. 1–13.</li>
<li>[5] V. Chandola, A. Banerjee, and V. Kumar, "Anomaly detection: A survey," <em>ACM computing surveys (CSUR)</em>, vol. 41, no. 3, pp. 1–58, 2009.</li>
<li>[6] N. Sultana, N. Chilamkurti, W. Peng, and R. Alhadad, "Survey on sdn based network intrusion detection system using machine learning approaches," <em>Peer-to-Peer Networking and Applications</em>, pp. 1–9, 01, 2018.</li>
<li>[7] J. Zhu, S. He, J. Liu, P. He, Q. Xie, Z. Zheng, and M. R. Lyu, "Tools and benchmarks for automated log parsing," in <em>2019 IEEE/ACM 41st International Conference on Software Engineering: Software Engineering in Practice (ICSE-SEIP)</em>. IEEE, 2019, pp. 121–130.</li>
<li>[8] M. Du, F. Li, G. Zheng, and V. Srikumar, "Deeplog: Anomaly detection and diagnosis from system logs through deep learning," in <em>Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security</em>. ACM, 2017, pp. 1285–1298.</li>
<li>[9] X. Zhang, Y. Xu, Q. Lin, B. Qiao, H. Zhang, Y. Dang, C. Xie, X. Yang, Q. Cheng, Z. Li et al., "Robust log-based anomaly detection on unstable log data," in <em>Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering</em>, 2019, pp. 807–817.</li>
<li>[10] M. M. Moya, M. W. Koch, and L. D. Hostetler, "One-class classifier networks for target recognition applications," <em>NASA STI/Recon Technical Report N</em>, vol. 93, 1993.</li>
<li>[11] W. Meng, Y. Liu, Y. Zhu, S. Zhang, D. Pei, Y. Liu, Y. Chen, R. Zhang, S. Tao, P. Sun et al., "Loganomaly: Unsupervised detection of sequential and quantitative anomalies in unstructured logs."</li>
<li>[12] S. Nedelkoski, J. Bogatinovski, A. Acker, J. Cardoso, and O. Kao, "Self-supervised log parsing," 2020.</li>
<li>[13] P. He, J. Zhu, Z. Zheng, and M. R. Lyu, "Drain: An online log parsing approach with fixed depth tree," in <em>2017 IEEE International Conference on Web Services (ICWS)</em>. IEEE, 2017, pp. 33–40.</li>
<li>[14] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean, "Distributed representations of words and phrases and their compositionality," in <em>Advances in neural information processing systems</em>, 2013, pp. 3111–3119.</li>
<li>[15] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, "Bert: Pre-training of deep bidirectional transformers for language understanding," <em>arXiv preprint arXiv:1810.04805</em>, 2018.</li>
<li>[16] M. Du, Z. Chen, C. Liu, R. Oak, and D. Song, "Lifelong anomaly detection through unlearning," in <em>Proceedings of the 2019 ACM SIGSAC Conference on Computer and Communications Security</em>, 2019, pp. 1283–1297.</li>
<li>[17] L. Ruff, R. A. Vandermeulen, N. Görnitz, A. Binder, E. Müller, K.-R. Müller, and M. Kloft, "Deep semi-supervised anomaly detection," <em>arXiv preprint arXiv:1906.02694</em>, 2019.</li>
<li>[18] A. Oliner and J. Stearley, "What supercomputers say: A study of five system logs," in <em>37th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN'07)</em>. IEEE, 2007, pp. 575–584.</li>
<li>[19] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin, "Attention is all you need," in <em>Advances in neural information processing systems</em>, 2017, pp. 5998–6008.</li>
<li>[20] Y. Liang, Y. Zhang, H. Xiong, and R. Sahoo, "Failure prediction in ibm bluegene/l event logs," in <em>Seventh IEEE International Conference on Data Mining (ICDM 2007)</em>. IEEE, 2007, pp. 583–588.</li>
<li>[21] W. Xu, L. Huang, A. Fox, D. Patterson, and M. I. Jordan, "Detecting large-scale system problems by mining console logs," in <em>Proceedings of the ACM SIGOPS 22nd symposium on Operating systems principles</em>, 2009, pp. 117–132.</li>
<li>[22] J. Breier and J. Branišová, "Anomaly detection from log files using data mining techniques," in <em>Information Science and Applications</em>. Springer, 2015, pp. 449–457.</li>
<li>[23] J.-G. Lou, Q. Fu, S. Yang, Y. Xu, and J. Li, "Mining invariants from console logs for system problem detection."</li>
<li>[24] K. Zhang, J. Xu, M. R. Min, G. Jiang, K. Pelechrinis, and H. Zhang, "Automated it system failure prediction: A deep learning approach," <em>2016 IEEE International Conference on Big Data (Big Data)</em>, pp. 1291–1300, 2016.</li>
<li>[25] R. Vinayakumar, K. P. Soman, and P. Poornachandran, "Long short-term memory based operation log anomaly detection," <em>2017 International Conference on Advances in Computing, Communications and Informatics (ICACCI)</em>, pp. 236–242, 2017.</li>
<li>[26] C. Bertero, M. Roy, C. Sauvanaud, and G. Trédan, "Experience report: Log mining using natural language processing and application to anomaly detection," in <em>2017 IEEE 28th International Symposium on Software Reliability Engineering (ISSRE)</em>. IEEE, 2017, pp. 351–360.</li>
</ul>
<p>[27] I. Steinwart, D. Hush, and C. Scovel, "A classification framework for anomaly detection," Journal of Machine Learning Research, vol. 6, no. Feb, pp. 211-232, 2005.
[28] A. B. Tsybakov et al., "On nonparametric estimation of density level sets," The Annals of Statistics, vol. 25, no. 3, pp. 948-969, 1997.
[29] Y. Bengio, A. Courville, and P. Vincent, "Representation learning: A review and new perspectives," IEEE transactions on pattern analysis and machine intelligence, vol. 35, no. 8, pp. 1798-1828, 2013.
[30] E. Loper and S. Bird, "Nltk: the natural language toolkit," arXiv preprint cs/0205028, 2002.
[31] Z. Zheng, L. Yu, W. Tang, Z. Lan, R. Gupta, N. Desai, S. Coghlan, and D. Buettner, "Co-analysis of ras log and job log on blue gene/p," in 2011 IEEE International Parallel \&amp; Distributed Processing Symposium. IEEE, 2011, pp. 840-851.
[32] L. v. d. Maaten and G. Hinton, "Visualizing data using t-sne," Journal of machine learning research, vol. 9, no. Nov, pp. 2579-2605, 2008.</p>            </div>
        </div>

    </div>
</body>
</html>
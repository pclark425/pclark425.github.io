<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8653 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8653</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8653</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-155.html">extraction-schema-155</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <p><strong>Paper ID:</strong> paper-277622199</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2504.03713v1.pdf" target="_blank">RLDBF: Enhancing LLMs Via Reinforcement Learning With DataBase FeedBack</a></p>
                <p><strong>Paper Abstract:</strong> While current large language models (LLMs) demonstrate remarkable linguistic capabilities through training on massive unstructured text corpora, they remain inadequate in leveraging structured scientific data (e.g., chemical molecular properties in databases) that encapsulate centuries of accumulated scientific expertise. These structured datasets hold strategic significance for advancing AI for Science yet current approaches merely treat them as auxiliary supplements to unstructured text. This study pioneers a systematic investigation into enhancing LLMs with structured scientific data, using chemical molecular science as a testbed. We investigate the impact of incorporating molecular property data on LLM across distinct training phases, including continual pre-training, supervised fine-tuning, and reinforcement learning. Notably, to address the inherent limitation of numerical insensitivity in large models, we propose an innovative methodology termed"Reinforcement Learning with Database Feedback"(RLDBF). Experimental evaluations demonstrate the efficacy of the proposed approach, with the model exhibiting remarkable generalization capabilities on previously unseen data and other chemical tasks. The results substantiate the potential of our method in advancing the field of structured scientific data processing within LLMs.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8653.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8653.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RLDBF</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reinforcement Learning with Database Feedback</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A training framework introduced in this paper that integrates structured scientific databases (PubChem molecular property values) into reinforcement learning for LLMs by constructing automatic preference pairs from verified numerical ground truths and applying Direct Preference Optimization to improve numerical fidelity and property-aware reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama3-8B-Instruct (base for RLDBF)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Transformer (autoregressive LLM); reinforcement learning via Direct Preference Optimization (DPO)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>8B parameters (Llama3-8B-Instruct base)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Structured molecular property records from PubChem (SMILES + five computed/recorded properties: H-bond acceptor count, H-bond donor count, rotatable bond count, octanol-water partition coefficient (logP), molecular weight); training set derived from first 10,000 PubChem molecules; Q&A templates generated (219 templates, 184 training templates) and template-filled entries used for CPT/SFT/RL data construction.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Chemical molecular property prediction, chemistry reasoning tasks (e.g., Mol2Caption, Retrosynthesis, product prediction), and regression for Coulombic efficiency (electrolyte evaluation).</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Not used to directly generate novel molecules; instead used to improve LLM outputs via reinforcement learning where 'reject' examples are constructed automatically from database values (preference pairs) to reduce hallucination and improve numerical sensitivity and property discrimination.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>Not applicable — RLDBF in this work is applied to property prediction and reasoning, not explicit molecule generation; no novelty metrics (e.g., fraction of novel molecules) are reported.</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Models are trained/evaluated on specific molecular properties drawn from PubChem; RLDBF constructs preference pairs by contrasting a target molecule's verified property values with those of its K nearest structural analogs to teach precise numeric discrimination for specific properties.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Accuracy on structured property-prediction tasks across four difficulty levels (Level 1–4) for in-domain and out-domain molecules; weighted score (W.S.) combining levels; ChemBench task scores (Mol2Caption, Retrosynthesis, etc.); RMSE for Coulombic Efficiency regression when paired with a small MLP.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>RLDBF substantially improved property-prediction robustness and generalization compared to the base Llama3-8B-Instruct and other baselines: e.g., in the PubChem benchmark RLDBF achieved a weighted score of 109.4 vs. 82.6 for the base Llama3-8B-Instruct and outperformed general LLMs and domain models on Level-4 (distractors from similar molecules). In downstream tasks, RLDBF achieved state-competitive results (e.g., Mol2Caption = 93.3) and when combined with a small MLP for Coulombic efficiency prediction produced RMSE = 0.160 vs base Llama3+MLP RMSE = 0.291.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Compared to general LLMs (GPT-4o-mini, GPT-3.5-Turbo) and several domain models (KALE-LM-Chem, ChemDFM, ChemLLM), RLDBF showed superior robustness in high-difficulty distractor settings and better generalization from property-only training to complex chemistry tasks; for LCE regression RLDBF+MLP outperformed MoLFormer and MultiModal-MoLFormer but did not beat the best specialized COEFF-KAN.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>RLDBF does not itself generate novel chemical structures — it focuses on leveraging structured database values to reduce hallucination and improve numeric sensitivity. It depends on available high-quality structured property data (here limited training set due to compute constraints: first 10,000 PubChem molecules). The authors note LLMs' intrinsic numerical insensitivity and multi-token molecule representations that can confuse structurally similar molecules; RLDBF mitigates but does not eliminate these issues. Also, RLDBF performance may be task-dependent (some SFT/RL variants overfit narrow distributions), and specialized architectures still outperform RLDBF on some regression benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'RLDBF: Enhancing LLMs Via Reinforcement Learning With DataBase FeedBack', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8653.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8653.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Llama3-8B-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Llama3-8B-Instruct (base model used in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An 8-billion-parameter instruction-tuned Llama3 model used as the base LLM for continual pretraining, supervised fine-tuning, and RLDBF experiments in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama3-8B-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Transformer, autoregressive LLM (instruction-tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>8B parameters</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Base pretraining not described in detail in this paper; then further trained/finetuned in experiments on PubChem-derived template-filled Q&A pairs, CPT/SFT/RL datasets constructed from PubChem properties (first 10,000 molecules).</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Property prediction, chemical QA, regression tasks (Coulombic efficiency) and generative reasoning tasks (Mol2Caption, Retrosynthesis) as evaluated in ChemBench.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Used as an LLM that can generate textual outputs (answers, captions, retrosynthesis steps); not used in this work to directly synthesize molecular structures but evaluated on tasks that map between text and molecular representations (SMILES <-> captions).</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>Not used to propose novel molecules in this work; no novelty metrics reported.</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Evaluation on specific property-prediction tasks constructed from PubChem; also evaluated on ChemBench tasks to assess transfer/generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Same suite as RLDBF: property-prediction accuracy across difficulty levels, weighted score, ChemBench per-task accuracies, and regression RMSE when a small MLP head is trained for LCE.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Serves as the baseline; RLDBF and some SFT/RL variants improve or degrade performance relative to this base depending on training type. Base model Llama3-8B-Instruct achieved weighted score 82.6 on the PubChem benchmark; RLDBF improved this to 109.4.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Compared in-paper to GPT-4o-mini, GPT-3.5-Turbo, Qwen2-7B-Instruct and several domain models; Llama3-8B-Instruct was the architecture augmented by RLDBF and other training variants.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>Intrinsic LLM numeric insensitivity and confusion of multi-token molecular SMILES leading to difficulty discriminating values for structurally similar molecules; naive SFT or CPT can lead to overfitting and loss of instruction-following ability depending on synthesis method.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'RLDBF: Enhancing LLMs Via Reinforcement Learning With DataBase FeedBack', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8653.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8653.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RLDBF+MLP (LCE)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RLDBF-enhanced Llama3-8B with small MLP head for Coulombic Efficiency prediction (LCE)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Application of the RLDBF-trained LLM encoder (hidden output as molecular embedding) followed by a two-layer MLP to predict log-transformed Coulombic Efficiency (LCE) for electrolytes; demonstrates RLDBF benefits on a regression task relevant to electrolyte design.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama3-8B-Instruct + 2-layer MLP head</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Transformer encoder (LLM last-layer embedding) + feedforward MLP for regression</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>8B (LLM) + small MLP</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Li/Cu half-cell dataset from literature (multi-component electrolyte systems with SMILES for components); 500 distilled data points from Deepseek R1 were also used for reasoning experiments; RLDBF property pretraining used PubChem property data as described.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Electrolyte performance prediction (Coulombic efficiency) — materials/battery electrolyte design domain.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Not a generative design method; a predictive regression model used to evaluate electrolyte performance (LCE) to inform material design decisions.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>Not applicable — model predicts performance of given electrolyte compositions rather than proposing novel chemistries; no novelty metrics provided.</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Trained and evaluated specifically on LCE regression derived from experimental literature; uses log transformation LCE = -log(1-CE) to enhance sensitivity.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Root-mean-square error (RMSE) on LCE prediction; comparisons to prior methods (MoLFormer, MultiModal-MoLFormer, COEFF variants).</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>RLDBF+MLP achieved RMSE = 0.160, substantially better than the base Llama3+MLP (0.291) and competitive with specialized methods (worse than best COEFF-KAN at 0.110 but better than MoLFormer and MultiModal-MoLFormer).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Outperformed MoLFormer (0.213) and MultiModal-MoLFormer (0.195) on LCE RMSE, but underperformed the top COEFF-KAN specialized architecture (0.110).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>Does not replace specialized architectures for absolute best predictive performance; success depends on quality of labeled experimental datasets and the ability of LLM-derived embeddings to capture compositionally relevant information; RLDBF's improvements stem from property-aware pretraining rather than architecture changes.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'RLDBF: Enhancing LLMs Via Reinforcement Learning With DataBase FeedBack', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8653.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8653.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>3D-structure LLM generation (related work)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Language models that generate molecules, materials and protein binding sites in 3D (referenced work)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Referenced prior work (cited in Related Works) claiming LLMs can directly generate 3D molecular/material/protein structural representations (xyz, cif, pdb), demonstrating generative molecular and materials design capabilities in other studies.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>LLM-based 3D generative models (autoregressive text-to-3D file generation implied)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Not specified in this paper (reference cited in related work; original paper would contain details).</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Molecule/material/protein structure generation — design of molecules, materials, and protein binding sites.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Direct textual/sequence generation of 3D coordinates or structural files (xyz, cif, pdb) via LLM autoregression (as reported in the referenced literature).</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>Not described in this paper; referenced work implies direct structure generation which would create novel structures, but no metrics are provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Referenced generally; specifics (targeting, property constraints) not reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Not provided in this paper; original references likely report structural validity, stability, or downstream property metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>This paper only cites the existence of such generative capabilities in the literature as motivation; no experimental details or outcomes from those works are reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Mentioned among other domain-specific generative efforts; no direct comparisons provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>Paper notes general challenges integrating structured numerical data and LLM numerical insensitivity; specific limitations of 3D-generative LLMs are not discussed here.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'RLDBF: Enhancing LLMs Via Reinforcement Learning With DataBase FeedBack', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8653.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8653.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Crystal structure generation (related work)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Crystal structure generation with autoregressive large language modeling</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Referenced study (in Related Works) that applied autoregressive LLMs to generate crystal structures, demonstrating generative capabilities for materials structures.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Crystal structure generation with autoregressive large language modeling</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Autoregressive LLM for structure generation (transformer-based)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Not specified in this paper; referenced work likely trained on crystal structure datasets and textual descriptions.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Materials science — crystal structure generation.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Autoregressive generation of crystal structures (text-encoded or structured outputs).</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>Not reported in this paper; original work likely reports novelty/diversity/validity metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Materials-specific; aims at preserving crystallographic symmetries and structural fidelity per referenced summary.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Not available here; original work presumably evaluated structural fidelity and preservation of symmetries.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Cited as an example that LLMs have been adapted for domain-specific generative tasks in materials science; no experimental numbers repeated here.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Mentioned to support that specialized tokenizers/architectures have been developed for structure generation; no direct comparisons in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>Paper notes that domain-specific tokenizers and architectures were developed in such works because natural-language tokenizers can degrade downstream performance; computational cost of large-scale pretraining is also highlighted as a constraint.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'RLDBF: Enhancing LLMs Via Reinforcement Learning With DataBase FeedBack', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8653.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8653.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Atom-by-atom protein generation (related work)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Atom-by-atom protein generation and beyond with language models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Referenced work listed in the bibliography that demonstrates atom-level protein generation using language models, indicating LLM applicability to molecular and biomolecular design.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Atom-by-atom protein generation and beyond with language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Language models adapted for atom-level structural generation</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Protein design and biomolecular structure generation.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Autoregressive or sequence-based generation of atom-level coordinates or representations (as reported in the referenced literature).</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>Not specified here; referenced work implies generation of novel protein structures.</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Protein/biomolecular design; potential to generate novel binding sites and scaffolds.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Not provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Mentioned as part of the broader landscape where LLMs have been adapted to structural generation tasks across chemistry and biology.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>No comparisons presented in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>Not discussed in detail here.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'RLDBF: Enhancing LLMs Via Reinforcement Learning With DataBase FeedBack', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Crystal structure generation with autoregressive large language modeling <em>(Rating: 2)</em></li>
                <li>Atom-by-atom protein generation and beyond with language models <em>(Rating: 2)</em></li>
                <li>Language models can generate molecules, materials, and protein binding sites directly in three dimensions as xyz, cif, and pdb files <em>(Rating: 1)</em></li>
                <li>Large-scale chemical language representations capture molecular structure and properties <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8653",
    "paper_id": "paper-277622199",
    "extraction_schema_id": "extraction-schema-155",
    "extracted_data": [
        {
            "name_short": "RLDBF",
            "name_full": "Reinforcement Learning with Database Feedback",
            "brief_description": "A training framework introduced in this paper that integrates structured scientific databases (PubChem molecular property values) into reinforcement learning for LLMs by constructing automatic preference pairs from verified numerical ground truths and applying Direct Preference Optimization to improve numerical fidelity and property-aware reasoning.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Llama3-8B-Instruct (base for RLDBF)",
            "model_type": "Transformer (autoregressive LLM); reinforcement learning via Direct Preference Optimization (DPO)",
            "model_size": "8B parameters (Llama3-8B-Instruct base)",
            "training_data": "Structured molecular property records from PubChem (SMILES + five computed/recorded properties: H-bond acceptor count, H-bond donor count, rotatable bond count, octanol-water partition coefficient (logP), molecular weight); training set derived from first 10,000 PubChem molecules; Q&A templates generated (219 templates, 184 training templates) and template-filled entries used for CPT/SFT/RL data construction.",
            "application_domain": "Chemical molecular property prediction, chemistry reasoning tasks (e.g., Mol2Caption, Retrosynthesis, product prediction), and regression for Coulombic efficiency (electrolyte evaluation).",
            "generation_method": "Not used to directly generate novel molecules; instead used to improve LLM outputs via reinforcement learning where 'reject' examples are constructed automatically from database values (preference pairs) to reduce hallucination and improve numerical sensitivity and property discrimination.",
            "novelty_of_chemicals": "Not applicable — RLDBF in this work is applied to property prediction and reasoning, not explicit molecule generation; no novelty metrics (e.g., fraction of novel molecules) are reported.",
            "application_specificity": "Models are trained/evaluated on specific molecular properties drawn from PubChem; RLDBF constructs preference pairs by contrasting a target molecule's verified property values with those of its K nearest structural analogs to teach precise numeric discrimination for specific properties.",
            "evaluation_metrics": "Accuracy on structured property-prediction tasks across four difficulty levels (Level 1–4) for in-domain and out-domain molecules; weighted score (W.S.) combining levels; ChemBench task scores (Mol2Caption, Retrosynthesis, etc.); RMSE for Coulombic Efficiency regression when paired with a small MLP.",
            "results_summary": "RLDBF substantially improved property-prediction robustness and generalization compared to the base Llama3-8B-Instruct and other baselines: e.g., in the PubChem benchmark RLDBF achieved a weighted score of 109.4 vs. 82.6 for the base Llama3-8B-Instruct and outperformed general LLMs and domain models on Level-4 (distractors from similar molecules). In downstream tasks, RLDBF achieved state-competitive results (e.g., Mol2Caption = 93.3) and when combined with a small MLP for Coulombic efficiency prediction produced RMSE = 0.160 vs base Llama3+MLP RMSE = 0.291.",
            "comparison_to_other_methods": "Compared to general LLMs (GPT-4o-mini, GPT-3.5-Turbo) and several domain models (KALE-LM-Chem, ChemDFM, ChemLLM), RLDBF showed superior robustness in high-difficulty distractor settings and better generalization from property-only training to complex chemistry tasks; for LCE regression RLDBF+MLP outperformed MoLFormer and MultiModal-MoLFormer but did not beat the best specialized COEFF-KAN.",
            "limitations_and_challenges": "RLDBF does not itself generate novel chemical structures — it focuses on leveraging structured database values to reduce hallucination and improve numeric sensitivity. It depends on available high-quality structured property data (here limited training set due to compute constraints: first 10,000 PubChem molecules). The authors note LLMs' intrinsic numerical insensitivity and multi-token molecule representations that can confuse structurally similar molecules; RLDBF mitigates but does not eliminate these issues. Also, RLDBF performance may be task-dependent (some SFT/RL variants overfit narrow distributions), and specialized architectures still outperform RLDBF on some regression benchmarks.",
            "uuid": "e8653.0",
            "source_info": {
                "paper_title": "RLDBF: Enhancing LLMs Via Reinforcement Learning With DataBase FeedBack",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "Llama3-8B-Instruct",
            "name_full": "Llama3-8B-Instruct (base model used in experiments)",
            "brief_description": "An 8-billion-parameter instruction-tuned Llama3 model used as the base LLM for continual pretraining, supervised fine-tuning, and RLDBF experiments in the paper.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Llama3-8B-Instruct",
            "model_type": "Transformer, autoregressive LLM (instruction-tuned)",
            "model_size": "8B parameters",
            "training_data": "Base pretraining not described in detail in this paper; then further trained/finetuned in experiments on PubChem-derived template-filled Q&A pairs, CPT/SFT/RL datasets constructed from PubChem properties (first 10,000 molecules).",
            "application_domain": "Property prediction, chemical QA, regression tasks (Coulombic efficiency) and generative reasoning tasks (Mol2Caption, Retrosynthesis) as evaluated in ChemBench.",
            "generation_method": "Used as an LLM that can generate textual outputs (answers, captions, retrosynthesis steps); not used in this work to directly synthesize molecular structures but evaluated on tasks that map between text and molecular representations (SMILES &lt;-&gt; captions).",
            "novelty_of_chemicals": "Not used to propose novel molecules in this work; no novelty metrics reported.",
            "application_specificity": "Evaluation on specific property-prediction tasks constructed from PubChem; also evaluated on ChemBench tasks to assess transfer/generalization.",
            "evaluation_metrics": "Same suite as RLDBF: property-prediction accuracy across difficulty levels, weighted score, ChemBench per-task accuracies, and regression RMSE when a small MLP head is trained for LCE.",
            "results_summary": "Serves as the baseline; RLDBF and some SFT/RL variants improve or degrade performance relative to this base depending on training type. Base model Llama3-8B-Instruct achieved weighted score 82.6 on the PubChem benchmark; RLDBF improved this to 109.4.",
            "comparison_to_other_methods": "Compared in-paper to GPT-4o-mini, GPT-3.5-Turbo, Qwen2-7B-Instruct and several domain models; Llama3-8B-Instruct was the architecture augmented by RLDBF and other training variants.",
            "limitations_and_challenges": "Intrinsic LLM numeric insensitivity and confusion of multi-token molecular SMILES leading to difficulty discriminating values for structurally similar molecules; naive SFT or CPT can lead to overfitting and loss of instruction-following ability depending on synthesis method.",
            "uuid": "e8653.1",
            "source_info": {
                "paper_title": "RLDBF: Enhancing LLMs Via Reinforcement Learning With DataBase FeedBack",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "RLDBF+MLP (LCE)",
            "name_full": "RLDBF-enhanced Llama3-8B with small MLP head for Coulombic Efficiency prediction (LCE)",
            "brief_description": "Application of the RLDBF-trained LLM encoder (hidden output as molecular embedding) followed by a two-layer MLP to predict log-transformed Coulombic Efficiency (LCE) for electrolytes; demonstrates RLDBF benefits on a regression task relevant to electrolyte design.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Llama3-8B-Instruct + 2-layer MLP head",
            "model_type": "Transformer encoder (LLM last-layer embedding) + feedforward MLP for regression",
            "model_size": "8B (LLM) + small MLP",
            "training_data": "Li/Cu half-cell dataset from literature (multi-component electrolyte systems with SMILES for components); 500 distilled data points from Deepseek R1 were also used for reasoning experiments; RLDBF property pretraining used PubChem property data as described.",
            "application_domain": "Electrolyte performance prediction (Coulombic efficiency) — materials/battery electrolyte design domain.",
            "generation_method": "Not a generative design method; a predictive regression model used to evaluate electrolyte performance (LCE) to inform material design decisions.",
            "novelty_of_chemicals": "Not applicable — model predicts performance of given electrolyte compositions rather than proposing novel chemistries; no novelty metrics provided.",
            "application_specificity": "Trained and evaluated specifically on LCE regression derived from experimental literature; uses log transformation LCE = -log(1-CE) to enhance sensitivity.",
            "evaluation_metrics": "Root-mean-square error (RMSE) on LCE prediction; comparisons to prior methods (MoLFormer, MultiModal-MoLFormer, COEFF variants).",
            "results_summary": "RLDBF+MLP achieved RMSE = 0.160, substantially better than the base Llama3+MLP (0.291) and competitive with specialized methods (worse than best COEFF-KAN at 0.110 but better than MoLFormer and MultiModal-MoLFormer).",
            "comparison_to_other_methods": "Outperformed MoLFormer (0.213) and MultiModal-MoLFormer (0.195) on LCE RMSE, but underperformed the top COEFF-KAN specialized architecture (0.110).",
            "limitations_and_challenges": "Does not replace specialized architectures for absolute best predictive performance; success depends on quality of labeled experimental datasets and the ability of LLM-derived embeddings to capture compositionally relevant information; RLDBF's improvements stem from property-aware pretraining rather than architecture changes.",
            "uuid": "e8653.2",
            "source_info": {
                "paper_title": "RLDBF: Enhancing LLMs Via Reinforcement Learning With DataBase FeedBack",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "3D-structure LLM generation (related work)",
            "name_full": "Language models that generate molecules, materials and protein binding sites in 3D (referenced work)",
            "brief_description": "Referenced prior work (cited in Related Works) claiming LLMs can directly generate 3D molecular/material/protein structural representations (xyz, cif, pdb), demonstrating generative molecular and materials design capabilities in other studies.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "",
            "model_type": "LLM-based 3D generative models (autoregressive text-to-3D file generation implied)",
            "model_size": "",
            "training_data": "Not specified in this paper (reference cited in related work; original paper would contain details).",
            "application_domain": "Molecule/material/protein structure generation — design of molecules, materials, and protein binding sites.",
            "generation_method": "Direct textual/sequence generation of 3D coordinates or structural files (xyz, cif, pdb) via LLM autoregression (as reported in the referenced literature).",
            "novelty_of_chemicals": "Not described in this paper; referenced work implies direct structure generation which would create novel structures, but no metrics are provided here.",
            "application_specificity": "Referenced generally; specifics (targeting, property constraints) not reported in this paper.",
            "evaluation_metrics": "Not provided in this paper; original references likely report structural validity, stability, or downstream property metrics.",
            "results_summary": "This paper only cites the existence of such generative capabilities in the literature as motivation; no experimental details or outcomes from those works are reported here.",
            "comparison_to_other_methods": "Mentioned among other domain-specific generative efforts; no direct comparisons provided in this paper.",
            "limitations_and_challenges": "Paper notes general challenges integrating structured numerical data and LLM numerical insensitivity; specific limitations of 3D-generative LLMs are not discussed here.",
            "uuid": "e8653.3",
            "source_info": {
                "paper_title": "RLDBF: Enhancing LLMs Via Reinforcement Learning With DataBase FeedBack",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "Crystal structure generation (related work)",
            "name_full": "Crystal structure generation with autoregressive large language modeling",
            "brief_description": "Referenced study (in Related Works) that applied autoregressive LLMs to generate crystal structures, demonstrating generative capabilities for materials structures.",
            "citation_title": "Crystal structure generation with autoregressive large language modeling",
            "mention_or_use": "mention",
            "model_name": "",
            "model_type": "Autoregressive LLM for structure generation (transformer-based)",
            "model_size": "",
            "training_data": "Not specified in this paper; referenced work likely trained on crystal structure datasets and textual descriptions.",
            "application_domain": "Materials science — crystal structure generation.",
            "generation_method": "Autoregressive generation of crystal structures (text-encoded or structured outputs).",
            "novelty_of_chemicals": "Not reported in this paper; original work likely reports novelty/diversity/validity metrics.",
            "application_specificity": "Materials-specific; aims at preserving crystallographic symmetries and structural fidelity per referenced summary.",
            "evaluation_metrics": "Not available here; original work presumably evaluated structural fidelity and preservation of symmetries.",
            "results_summary": "Cited as an example that LLMs have been adapted for domain-specific generative tasks in materials science; no experimental numbers repeated here.",
            "comparison_to_other_methods": "Mentioned to support that specialized tokenizers/architectures have been developed for structure generation; no direct comparisons in this paper.",
            "limitations_and_challenges": "Paper notes that domain-specific tokenizers and architectures were developed in such works because natural-language tokenizers can degrade downstream performance; computational cost of large-scale pretraining is also highlighted as a constraint.",
            "uuid": "e8653.4",
            "source_info": {
                "paper_title": "RLDBF: Enhancing LLMs Via Reinforcement Learning With DataBase FeedBack",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "Atom-by-atom protein generation (related work)",
            "name_full": "Atom-by-atom protein generation and beyond with language models",
            "brief_description": "Referenced work listed in the bibliography that demonstrates atom-level protein generation using language models, indicating LLM applicability to molecular and biomolecular design.",
            "citation_title": "Atom-by-atom protein generation and beyond with language models",
            "mention_or_use": "mention",
            "model_name": "",
            "model_type": "Language models adapted for atom-level structural generation",
            "model_size": "",
            "training_data": "Not specified in this paper.",
            "application_domain": "Protein design and biomolecular structure generation.",
            "generation_method": "Autoregressive or sequence-based generation of atom-level coordinates or representations (as reported in the referenced literature).",
            "novelty_of_chemicals": "Not specified here; referenced work implies generation of novel protein structures.",
            "application_specificity": "Protein/biomolecular design; potential to generate novel binding sites and scaffolds.",
            "evaluation_metrics": "Not provided in this paper.",
            "results_summary": "Mentioned as part of the broader landscape where LLMs have been adapted to structural generation tasks across chemistry and biology.",
            "comparison_to_other_methods": "No comparisons presented in this paper.",
            "limitations_and_challenges": "Not discussed in detail here.",
            "uuid": "e8653.5",
            "source_info": {
                "paper_title": "RLDBF: Enhancing LLMs Via Reinforcement Learning With DataBase FeedBack",
                "publication_date_yy_mm": "2025-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Crystal structure generation with autoregressive large language modeling",
            "rating": 2,
            "sanitized_title": "crystal_structure_generation_with_autoregressive_large_language_modeling"
        },
        {
            "paper_title": "Atom-by-atom protein generation and beyond with language models",
            "rating": 2,
            "sanitized_title": "atombyatom_protein_generation_and_beyond_with_language_models"
        },
        {
            "paper_title": "Language models can generate molecules, materials, and protein binding sites directly in three dimensions as xyz, cif, and pdb files",
            "rating": 1,
            "sanitized_title": "language_models_can_generate_molecules_materials_and_protein_binding_sites_directly_in_three_dimensions_as_xyz_cif_and_pdb_files"
        },
        {
            "paper_title": "Large-scale chemical language representations capture molecular structure and properties",
            "rating": 1,
            "sanitized_title": "largescale_chemical_language_representations_capture_molecular_structure_and_properties"
        }
    ],
    "cost": 0.01475425,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>RLDBF: ENHANCING LLMS VIA REINFORCEMENT LEARNING WITH DATABASE FEEDBACK
28 Mar 2025</p>
<p>Weichen Dai 
Zijie Dai 
Xi Li 
Yi Zhou yi_zhou@ustc.edu.cn 
Ji Qi 
Wu Jiang </p>
<p>Zhijie Huang
Yixuan Pan</p>
<p>Xinhe Li</p>
<p>RLDBF: ENHANCING LLMS VIA REINFORCEMENT LEARNING WITH DATABASE FEEDBACK
28 Mar 20257FA04E55D8240BEC913FF8BBB3AE6493arXiv:2504.03713v1[cs.LG]
While current large language models (LLMs) demonstrate remarkable linguistic capabilities through training on massive unstructured text corpora, they remain inadequate in leveraging structured scientific data (e.g., chemical molecular properties in databases) that encapsulate centuries of accumulated scientific expertise.These structured datasets hold strategic significance for advancing AI for Science yet current approaches merely treat them as auxiliary supplements to unstructured text.This study pioneers a systematic investigation into enhancing LLMs with structured scientific data, using chemical molecular science as a testbed.We investigate the impact of incorporating molecular property data on LLM across distinct training phases, including continual pre-training, supervised fine-tuning, and reinforcement learning.Notably, to address the inherent limitation of numerical insensitivity in large models, we propose an innovative methodology termed "Reinforcement Learning with Database Feedback" (RLDBF).Experimental evaluations demonstrate the efficacy of the proposed approach, with the model exhibiting remarkable generalization capabilities on previously unseen data and other chemical tasks.The results substantiate the potential of our method in advancing the field of structured scientific data processing within LLMs.</p>
<p>Introduction</p>
<p>In recent years, artificial intelligence has demonstrated formidable capabilities across various tasks, achieving performance levels comparable to or even surpassing top human expertise -whether in recognition/generation tasks for speech [1] and images [2], or knowledge-intensive tasks such as translation [3], Go strategy [4], and mathematical olympiad problem solving [5].The emergence of large language models (LLMs) has particularly propelled AI capabilities to unprecedented heights, catalyzing a new wave of artificial general intelligence (AGI) development.</p>
<p>The training paradigm for LLMs typically comprises three critical phases: pretraining, supervised fine-tuning (SFT), and reinforcement learning.Pretraining primarily enhances the model's knowledge, SFT empowers superior instructionfollowing capabilities, while reinforcement learning aligns model responses with human values through preference optimization.Although LLMs exhibit remarkable knowledge capacity through pretraining on massive unstructured data, the inherent limitations of unstructured text -including low information density and ambiguous knowledge representation -hinder precise acquisition of domain-specific knowledge.Structured databases offer a promising solution to this challenge.Taking PubChem [6], a chemical database as an example, it contains over 100 million molecular entities, each characterized by standardized fields (e.g., SMILES [7], physicochemical properties, reaction pathways) that form an intricate knowledge network.This structured representation inherently achieves significantly higher information density compared to unstructured text.</p>
<p>Structured databases often encapsulate meticulously curated knowledge and experimental data of immense potential value -painstakingly accumulated by generations of top scientists.These resources contain substantial under-explored information.However, existing methodologies fail to fully exploit this wealth of data.Representative works like MoLFormer [8] primarily utilize molecular representations for pretraining while neglecting more critical knowledge • We propose the first reinforcement learning framework integrated with structured databases for LLM enhancement.Our meticulously designed architecture enables efficient knowledge absorption from databases while substantially reducing annotation overhead.</p>
<p>• Multi-dimensional experiments demonstrate our method's effectiveness.Notably, models trained with RLDBF not only exhibit superior generalization on chemical tasks but also reveal emergent capabilities for assisting in-depth chemical reasoning.</p>
<p>2 Related Works</p>
<p>AI for Chemistry</p>
<p>The chemistry and materials science communities have witnessed the growing adoption of Transformer-based language models for various downstream tasks, including crystal structure prediction and coulombic efficiency prediction.These approaches can be broadly categorized by their sequence processing strategies: 1) post-training on foundation large language models (LLMs) with data encoded via pre-trained tokenizer, or 2) developing task-specific tokenizer and encoding schemes coupled with custom-defined models.</p>
<p>As demonstrated by (author?) [9], the distributional mismatch between natural language-trained tokenizer and domain-specific tasks can significantly degrade downstream performance.This has driven researchers to design specialized sub-word vocabularies and train compact Transformer-based architectures for specific applications.For instance, (author?) [10] developed a 25M-parameter GPT-like model trained on crystallographic information for structure generation.(author?) [11] achieved superior materials property prediction through a 110M-parameter BERT model trained on crystal structures and natural language descriptions.Three-dimensional generative capabilities were demonstrated by (author?) [12], while (author?) [13] pioneered protein-drug conjugates generation using specialized architectures.</p>
<p>However, the prohibitive computational resource requirements for large-scale pre-training have constrained model development.Post-training paradigms offer more accessible alternatives, benefiting from scaling laws where larger models capture more information.(author?) [14] leverages post-trained LLMs to comprehend and validate threedimensional atomic configurations, demonstrating the scaling law governing model capacity and structural fidelity in preserving critical crystal symmetries.</p>
<p>Recent efforts have focused on domain adaptation through curated datasets (e.g., textbooks, research papers, and QA pairs) to create chemistry-specific LLMs.(author?) [15] fine-tuned (author?) [16] with 7M proprietary chemical instructions for chemical QA tasks.(author?) [17] conducted incremental pretraining and instruction tuning on Llama2 13B using extensive chemical corpora.Specialized applications include GPT-3 fine-tuning for electrochemical property prediction by (author?) [18], and text-encoded atomistic data adaptation of Llama2 7B-70B for stable crystal generation by (author?) [14], demonstrating that model scales positively correlated with symmetry preservation capabilities.</p>
<p>Structured Data Integration</p>
<p>Integrating structured/sub-structured data into natural language pre-trained LLMs presents unique challenges.Conventional approaches often naively serialize structured data into text sequences, as exemplified by (author?) [19].However, (author?) [20] revealed through the ARC benchmark [21] that LLMs exhibit varying sensitivity to structural semantics, complicating direct alignment between structured data representations and textual embeddings.</p>
<p>To bridge this semantic gap, (author?) [22] incorporated knowledge graph triples via autoencoder networks, though scalability remains constrained by structured data requirements.Alternative strategies leverage LLMs to generate synthetic text from structured input, as demonstrated in the synthesis of clinical data by (author?) [23].Recent advances address hallucination in synthetic data through structured data verification, as proposed by (author?) [24], enabling higher-fidelity QA pair generation.</p>
<p>Property Extraction</p>
<p>{"Canonicalized Compound": 1, "Hydrogen Bond Acceptor Count": 4, "Hydrogen Bond Donor Count": 0, "Rotatable Bond Count": 5, "XLogP3-AA Log P": 0.4, "Exact Mass": "203.11575802","Molecular Formula": "C9H17NO4", "Molecular Weight": "203.24","Canonical SMILES": "CC(=O)OC(CC(=O
)[O- ])C<a href="C">N+</a>(C)C"}</p>
<p>Numerical Domain Knowledge</p>
<p>The integration of experimentally derived or computationally generated structured data, often requiring precision to multiple decimal places, exposes critical limitations in LLM numerical reasoning.(author?) [25] systematically identified deficiencies in chemical numerical tasks.This highlights the fundamental challenge of achieving highprecision numerical memorization and reasoning in AI4Science applications.</p>
<p>Emerging research quantifies these limitations: (author?) [26] documented systematic underperformance in numerical operations, while (author?) [27] revealed deficiencies in numerical common sense reasoning.Domain-specific challenges are addressed by (author?) [28], identifying chemically grounded numerical tasks (e.g., stoichiometric balancing, molecular weight calculation) where current models struggle without explicit integration of chemical knowledge.</p>
<p>Methods</p>
<p>We demonstrate our pipeline for CPT, SFT and RL in Figure 1, including template genration, data construction and model training.</p>
<p>Q&amp;A Template Construction</p>
<p>The data in databases is highly structured, while LLMs are trained on unstructured text.A natural approach is to transform structured data into unstructured training data via template filling.We used GPT-4o to generate multiple Q&amp;A templates.To boost model performance on diverse instructions, GPT-4o was instructed to create templates beneficial for subsequent fine -tuning, ensuring diversity in style, structure, and expression.Next, we applied a three -step filter: 1) For templates with identical questions, only one was kept; 2) Templates with general yes -no questions were removed to prevent the model from only learning yes/no answers; 3)Templates were manually checked, and those with unreasonable content or wording errors were deleted.Finally, 219 templates were generated, examples are shown in appendix Table 3.To increase the difficulty of subsequent tests, we split these templates into training and test sets.Specifically, we first used sentence-transformers2 [29] to embed each question template, then clustered them using
ChatGPT PubChem CC(=O)OC(CC(=O)O)C<a href="C">N+</a>(C)C CCCC(=O)OC(CC(=O)O)C<a href="C">N+</a>(C)C … … K most</p>
<p>CC(=O)OC(CC(=O)[O-])C<a href="C">N+</a>(C)C</p>
<p>Compound</p>
<p>Is there an exact {PROPERTY} for {COMPOUND} ?</p>
<p>Generate templates</p>
<p>Final LM the DBSCAN algorithm.We adjusted the algorithm's parameters to get a reasonable number of outliers, using the corresponding templates to construct the test set.Ultimately, we obtained 184 training templates and 35 test templates.</p>
<p>CPT &amp; SFT Data Synthesis</p>
<p>Continual Pre-Training (CPT) and SFT are two common methods for training LLMs.To explore the impact of different training data synthesis methods and establish a comprehensive benchmark for subsequent tests, we designed various ways to synthesize training data based on the templates from the previous subsection.</p>
<p>CPT Data</p>
<p>The main purpose of CPT is to equip LLMs with more knowledge.We tried five methods to help LLMs better establish connections among compounds, properties, and values: 1) Directly concatenating Q&amp;A pairs; 2) Directly concatenating molecule, property, and value; 3) &amp; 4) Using question templates with numerical answers only.</p>
<p>To help LLMs better connect questions with real answers and avoid them only outputting answers, we added "Answer the question only with the corresponding value" after each question.This was also applied in synthesizing SFT and RL data.We then decorated Q&amp;A pairs with "User-Assistant" or "**"; 5) Directly building data with "The [property] of
[compound] is [value]" template.
SFT Data We tried three methods: 1) Directly using Q&amp;A pairs; 2) Using Q&amp;A pairs with answer templates added to questions as prompts to reduce the impact on the model's instruction -following ability; 3) Using question templates with only numerical answers.The synthesis methods above and examples are summarized in appendix Figure 3.</p>
<p>RLDBF Motivation</p>
<p>The feedback in Supervised Fine-Tuning (SFT) is one-way, only indicating correct answers, not wrong ones.Though effective, it has limitations.Reinforcement Learning from Human Feedback (RLHF) [30] enables models to reject incorrect outputs, enhancing the safety and stability of their outputs.Implementation RLAIF [31] proposes to directly use the feedback from a LLM as the reward signal in RL.This approach not only reduces the overhead associated with AI preference labeling and reward model training (which is essentially a form of distillation) but also demonstrates superior performance compared to learning reward models through distillation.This finding aligns with our perspective, as training a reward model on database data may introduce additional biases.To address this, we adopt a more streamlined approach through Direct Preference Optimization (DPO) [32], enabling direct discrimination between correct and erroneous data without intermediary models.We enhance output fidelity by exclusively constructing preference pairs using verified numerical ground truths-even for "reject" samples-thereby minimizing noise from stochastic perturbations.Empirical analysis reveals that LLMs exhibit limited discriminative capacity for molecular structures due to multi-token compositional representations, frequently conflating properties of structurally analogous molecules.To mitigate this, we strategically generate preference pairs by contrasting target molecules' verified property values with those of their nearest structural analogs.Specifically, for each specific molecule C, we first identify K most similar molecule {S i }.Then, for a given property P of molecule C, we construct K preference pairs, where the "chosen" value in each pair is the ground truth value, and the "reject" value is the value of property P from {S i }.Finally, to minimize the influence of textual descriptors, the preference pairs contain only the specific numerical values without additional words.These constructed pairs finally serve as the DPO data for model learning.Through the above steps, we have implemented the entire process of RLDBF.</p>
<p>Comparable Alternatives We also proposed several alternative strategies for constructing "reject" data, including: PubChem [6] is an open-access chemical database, systematically integrates structural, physicochemical, and bioactivity data for small molecules from 750+ authoritative sources.Its triad architecture (Compound, Substance, BioAssay) enables cross-domain exploration of structure-biological activity relationships through curated experimental results and predictive models.PubChem has become fundamental infrastructure for computational chemistry and translational studies, ensuring reproducibility while bridging experimental and computational research paradigms.</p>
<p>In this study, we constructed datasets using downloadable PubChem data3 due to its substantial volume and accessibility.The processed data was partitioned into multiple files, each containing 500,000 molecules.We first examined the key-value pairs of each entry, selecting non-empty keys to ensure data integrity for subsequent experiments.Five molecular properties were ultimately chosen for dataset construction: Hydrogen Bond Acceptor Count, Hydrogen Bond Donor Count, Rotatable Bond Count, Octanol-water Partition Coefficient, and Molecular Weight.These properties can also be computationally derived.Additionally, SMILES [7] was adopted as the molecular textual representation.</p>
<p>Training Set Under computational resource constraints, we maximized the training dataset size within feasible limits, ultimately constructing it based on the first 10,000 molecules from PubChem.</p>
<p>RLDBF</p>
<p>Test Set</p>
<p>The test set was systematically designed along two dimensions with four difficulty levels.The two dimensions distinguish between in-domain molecules (from the training set) and out-domain molecules (from PubChem molecules beyond the 500,000th entry), enabling evaluation of both model learning capacity and generalization ability.Each question requires the model to predict specific molecular properties, with four progressively challenging levels:</p>
<p>• Level 1: Distractors involve basic numerical perturbations of the correct answer: ±0.1, ×10, ÷10</p>
<p>• Level 2: Sophisticated perturbations include nine variants: ±0.1, ±1, ×(−1), and combined operations (×(−1) followed by ±0.1/ ± 1) • Level 3: Distractors comprise values from different properties of the same molecule, same properties of different molecules, or different properties of different molecules; • Level 4: Distractors are selected from the most similar molecules' corresponding properties To mitigate random bias, we implemented three independent sampling repetitions for each dimension-level combination, generating 200 questions per sampling instance.We also calculate the weighted sum for comparison:
W.S. = 0.4 * S 4 + 0.3 * S 3 + 0.2 * S 2 + 0.1 * S 1 ,(1)
where S i is the sum of the average scores of Level i.</p>
<p>ChemBench</p>
<p>ChemBench [15] is a comprehensive benchmark designed to evaluate the performance of AI models in chemistryrelated tasks.It encompasses a diverse set of problems, including Name Conversion(NC), Property Prediction(PP1), Mol2caption(M2C), Caption2mol(C2M), Product Prediction(PP2), Retrosynthesis(RS), Yield Prediction(YP), Temperature Prediction(TP) and Solvent Prediction(SP).This benchmark provides a rigorous assessment of model capabilities in the chemical domain, and facilitates standardized comparisons across different approaches, promoting advancements in AI-driven chemistry research.Results are rounded to the nearest tenth.</p>
<p>Training Setup</p>
<p>Our experiments were conducted on a computing cluster equipped with 8×NVIDIA A100 80GB GPUs, utilizing the Llama3-8B-Instruct [33] model as the base model.To optimize experimental efficiency, we initially performed preliminary investigations to determine appropriate learning rate ranges.The final learning rates for both the CPT and RL were selected from [1e-7, 5e-7, 1e-6], while SFT employed learning rates from [1e-7, 1e-6].For each candidate learning rate configuration, we trained corresponding model instances for 1 epoch and subsequently selected the optimal performing model for comparative analysis.Throughout the training process, all model parameters remained activated and trainable, the DeepSpeed ZeRO-1 [34] are employed.</p>
<p>Baselines</p>
<p>In addition to the trained models discussed in previous sections, we conducted comprehensive comparative analyses with other prominent LLMs within our benchmarking framework.These models include: GPT-4o-mini [35], GPT-3.5-Turbo,Qwen2-7B-Instruct [36], Llama3-8B-Instruct; and domain-specific chemical models comprising KALE-LM-Chem [37], ChemDFM [17], ChemLLM [15], and LlaSMol-Mistral-7B [38].</p>
<p>Results</p>
<p>PubChem Results</p>
<p>As shown in Table 1, general models like GPT-4o-mini and GPT-3.5-Turboexhibit performance degradation as task complexity increases, particularly in Level 3 and Level 4 scenarios.For instance, GPT-4o-mini achieves 67.8% accuracy in Level 1 (In-domain) but drops to 27.0% in Level 4 (In-domain), indicating limited robustness against sophisticated perturbations.Domain-specific models such as KALE-LM-Chem and ChemDFM show moderate improvements in Level 4 (Out-domain) compared to GPT, yet their performance remains suboptimal compared to our trained models.These observations highlight two key limitations: 1) general models lack domain-specific robustness to handle perturbations, and 2) existing domain models fail to fully exploit structural and numerical relationships in molecular property prediction.</p>
<p>Our proposed RLDBF method demonstrates significant advantages over the general and domain-specific baselines.balanced proficiency, exceeding the best domain model (KALE-LM-Chem: 79.7) and general models by substantial margins.This advancement stems from our method's simple but efficient design, which systematically mitigates the hallucination issues observed in baseline models.</p>
<p>A critical ablation is observed when comparing RLDBF to its base model Llama3-8B-Instruct and other types of trained models (except for SFT Type 1 and RL Type 1, further analysis is conducted later), demonstrating that our training framework effectively amplifies the base architecture's latent capabilities.The consistent outperformance across all perturbation types-particularly in Level 4 where distractors derive from structurally similar molecules-underscores the efficacy of our method.</p>
<p>This analysis substantiates that RLDBF establishes a new paradigm for molecular property prediction, balancing generalization and precision through its innovative training strategy.</p>
<p>Chembench Results</p>
<p>Though SFT Type 1 and RL Type 1 show impressive performance in previous experiment, as shown in Table 2, SFT Type 1 and RL Type 1-both derived from the Llama3-8B-Instruct base model-demonstrate significant performance degradation compared to their strong results in the PubChem benchmark.SFT Type 1 underperforms the base model in 5 out of 9 tasks, while RL Type 1 also shows declines in 5 tasks.Remarkably, RLDBF's success stems solely from training on molecular property data, yet it achieves competitive performance in complex tasks like Retrosynthesis, compared to specialized models like KALE-LM.This demonstrates that learning fundamental molecular properties-such as hydrogen bonding patterns and partition coefficients-provides a foundational understanding that generalizes to higher-order chemical reasoning.For example, RLDBF's 93.3% accuracy in Mol2Caption suggests that property-aware training enhances the model's ability to map structural features to semantic descriptions.These results highlight the untapped potential of leveraging large-scale chemical databases to augment LLMs, enabling them to transcend narrow task boundaries and achieve holistic chemical intelligence.</p>
<p>More Results</p>
<p>The method's ability to outperform cutting-edge models across challenging scenarios positions it as a potential solution for real-world cheminformatics applications.We show some applications in appendix B(Regression) and C(Reasoning).</p>
<p>Conclusion</p>
<p>We present RLDBF, an approach that achieves two critical objectives: 1) enhanced utilization of high-density knowledge embedded in structured databases, and 2) significant reduction in human annotation costs for reinforcement learning.</p>
<p>Experimental evaluations demonstrate the efficacy of our proposed method.Looking forward, we will continue to investigate database-enhanced methodologies for LLMs, extending their applications across diverse scientific domains.for electrolytes.We employ a unified network architecture: the hidden output of the last LlamaDecoderLayer is first extracted as the molecular embedding, which is then processed through a simple MLP comprising two fully connected layers to generate the final prediction.</p>
<p>MODEL LCE</p>
<p>CE Prediction</p>
<p>We systematically evaluated our methodology using a Li/Cu half-cell dataset [39] sourced from peer-reviewed chemical literature.This comprehensive dataset encompasses multi-component electrolyte systems, each comprising 2 to 7 distinct chemical components rigorously characterized by their SMILES representations.To enhance the quantitative sensitivity of the model to electrolyte composition variations, we implemented a logarithmic transformation of Coulombic Efficiency (CE) according to the relationship:
LCE = −log(1 − CE).(2)
This mathematical treatment amplifies differential responses in electrochemical performance while maintaining proportionality to the original CE metric, following established conventions for analyzing nonlinear relationships in electrochemical systems.We compare our results with the state-of-the-art methods, including Multimodal-MoLFormer [40] and COEFF [41].</p>
<p>The evaluation further validates the effectiveness of our RLDBF.As shown in</p>
<p>C Application on reasoning tasks</p>
<p>With the emergence of OpenAI-o1 and Deepseek-R1 [42] models, the research community has moved beyond simply requiring LLMs to provide correct answers, and now expects these models to demonstrate valid reasoning processes.This paradigm shift has prompted us to thoroughly investigate how to enhance model reasoning capabilities through RLDBF.</p>
<p>Inspired by the "Aha Moment" proposed in R1 research, we posit that enabling models to systematically compare reference answers with their current reasoning trajectory at critical cognitive checkpoints during deep thinking could effectively guide reasoning pathways and mitigate excessive rumination.To actualize this concept, we first distilled 500 data points on molecular physicochemical property calculations from Deepseek R1, then fine-tuned the Llama3-8B-Instruct model to cultivate both deep reasoning abilities and capacity for reference-guided reasoning (Llama3-8B Reasoner).Subsequently, we applied our novel RLDBF method to further reinforce the model's capability in generating accurate numerical predictions for molecular properties (RLDBF Reasoner).</p>
<p>The models are required to address problems through a 2-step reasoning process:</p>
<p>• Step 1: Directly generate the final answer (system 1, fast thinking);</p>
<p>• Step 2: Utilize the initial answer as a reference to perform stepwise reasoning (system 2, slow thinking).</p>
<p>As shown below, reasoning models examine their thoughts through recalling the reference answer, where only the correct reference leads to the correct result.</p>
<p>We believe this System1+System2 reasoning paradigm holds significant potential for advancing LLM applications in scientific domains.Our future work will focus on extending this framework to complex scenarios requiring multi-step quantitative reasoning and cross-domain knowledge integration, while further optimizing the RLDBF mechanism for improved training efficiency and reasoning robustness.Secondly, using the value as the reference answer, please think step by step and answer the quesiton again.</p>
<p>You should think about the reasoning process in the mind and then provide the user with the answer.The reasoning process and answer are enclosed within ⟨think⟩⟨/think⟩ and ⟨answer⟩⟨/answer⟩ tags, respectively, i.e., ⟨think⟩ reasoning process here ⟨/think⟩⟨answer⟩ answer here ⟨/answer⟩.</p>
<p>Answer:</p>
<p>The Hydrogen Bond Donor Count is 3.</p>
<p>⟨think⟩ I start by looking at the molecule and identifying the atoms that can act as hydrogen bond donors.Hydrogen bond donors are typically atoms that have a hydrogen atom bonded to a highly electronegative atom, such as oxygen, nitrogen, or fluorine.In this molecule, I see that there are several atoms that fit this criteria.First, I notice the oxygen atom bonded to the carbon atom with a double bond (O=C).This oxygen atom is highly electronegative, and the hydrogen atom bonded to it is a potential hydrogen bond donor.</p>
<p>Next, I look at the oxygen atom bonded to the carbon atom with a single bond (O-C).This oxygen atom is also highly electronegative, and the hydrogen atom bonded to it is another potential hydrogen bond donor.Finally, I notice the nitrogen atom bonded to the carbon atom with a single bond (N-C).This nitrogen atom is also highly electronegative, and the two hydrogen atoms bonded to it are potential hydrogen bond donors.So, I count a total of 3 hydrogen bond donors in this molecule: the hydrogen atom bonded to the oxygen atom with a double bond, the hydrogen atom bonded to the oxygen atom with a single bond, and the two hydrogen atoms bonded to the nitrogen atom.⟨/think⟩ ⟨answer⟩ 3 ⟨/answer⟩</p>
<p>Figure 1 :
1
Figure 1: Overview of the working pipeline.</p>
<p>there an exact Hydrogen Bond Acceptor Count for CC(=O)OC(CC(=O)[O-])C<a href="C">N+</a>(C)C ?</p>
<p>Figure 2 :
2
Figure 2: Overview of RLDBF.</p>
<p>Using values of different properties for the same molecule, values of the same property for different molecules, and values of different properties for different molecules (SMILES remains unchanged); 2) Using the same molecule with different properties, different molecules with the same property, and different molecules with different properties; 3) Only using the same molecule with different properties and different molecules with the same property; 4) Only using the same molecule with different properties and the most similar different molecule with the same property; 5) Adding answer templates to the instruction part based on 4); 6) Refining 5) by only using the most similar different molecule with the same property.Examples are shown in appendix Figure 4.These construction methods are also used as benchmarks for subsequent tests.</p>
<p>Figure 4 :
4
Figure 4: Examples for RL data.</p>
<p>RLDBF Case 1 :
1
Llama-3 8B Instruct Instruction: What is the Hydrogen Bond Donor Count for CC(=O)OC(CC(=O)O)C<a href="C">N+</a>(C)C?Firstly, answer the question only with the corresponding value.</p>
<p>from different database instances and establish their priority by disturbing attributes and values.Take these two data entries as an example: "[Ins.A]-[Attr.1]-[Attr.2]; [Ins.B]-[Attr.1]-[Attr.2]".Based on the severity of knowledge-based errors, the preference data can be constructed as follows:
• Score 3 (result match): [Attr. 1] of [Ins. A] is [Attr. 1 Value of A]• Score 2 (instance match, attribute mismatch): [Attr. 1] of [Ins A] is Value(Attr. 2, A)• Score 1 (instance mismatch, attribute match): [Attr. 1] of [Ins A] is Value(Attr. 1, B)• Score 0 (all mismatch): [Attr. 1] of [Ins A] is Value(Attr. 2, B)
It allows models to learn both correct answers and how to avoid wrong ones.Inspired by this, we decided to employ reinforcement learning to better leverage database data for model training.However, RLHF relies heavily on high quality human preference labels, which increases data acquisition costs and exposes model performance to annotator bias due to the lack of factual basis in data comparison.Considering this, we proposed Reinforcement Learning from DataBase Feedback (RLDBF), using databases to automatically build preference data.Databases are natural data labelers.We can create many preference RLDBF data pairs</p>
<p>Table 1 :
1
PubChem results, rounded to the nearest tenth.We mark the best results with double underlines, the second-best with single underlines.
MODELLevel 1 In OutLevel 2 In OutLevel 3 In OutLevel 4 In OutW.S.GPT-4o-mini67.8 70.0 55.7 53.0 49.8 39.3 27.0 24.8 83.0GPT-3.5-Turbo54.5 54.5 54.5 49.0 33.8 32.5 23.0 24.3 70.4Qwen2-7B-Instruct71.0 72.7 58.0 56.2 41.5 36.5 24.5 21.0 78.8KALE-LM-Chem57.3 61.2 54.7 56.5 42.2 37.3 27.0 27.3 79.7ChemDFM42.8 46.2 46.7 48.2 33.0 32.7 25.3 29.8 69.6ChemLLM55.3 55.3 51.2 58.7 41.8 37.0 27.2 25.2 77.6LlaSMol-Mistral-7B 51.7 52.0 41.3 42.3 27.5 28.3 25.0 26.7 64.5CPT Type 167.7 70.7 58.0 51.7 68.5 53.7 47.0 42.2 108.1CPT Type 273.3 74.5 57.2 61.3 55.3 41.0 33.3 34.3 94.5CPT Type 361.3 58.7 51.7 56.5 40.7 31.3 27.8 22.7 75.4CPT Type 463.3 63.5 54.5 57.0 51.0 40.5 32.7 29.3 87.2CPT Type 569.2 73.5 55.2 57.3 57.8 47.5 37.0 35.2 97.2SFT Type 177.8 77.5 64.5 72.7 69.3 61.0 52.2 50.0 122.9SFT Type 258.2 60.7 46.3 51.5 53.7 48.3 43.5 41.2 95.9SFT Type 365.8 67.5 55.2 59.2 55.2 48.7 34.5 34.0 94.8RL Type 163.0 75.0 56.5 67.3 68.7 60.3 42.5 38.0 109.5RL Type 263.2 60.5 53.3 57.7 41.2 32.8 28.7 23.3 77.6RL Type 362.8 60.2 53.2 57.0 40.5 33.2 29.3 22.3 77.1RL Type 462.2 60.2 54.3 58.7 40.3 31.8 28.7 22.2 76.8RL Type 568.7 74.2 52.8 59.2 44.8 36.5 30.5 27.2 84.2RL Type 665.0 60.3 55.5 58.2 41.8 32.2 26.5 22.5 77.1Llama3-8B-Instruct64.2 61.3 53.8 57.3 41.5 32.2 28.3 24.0 82.6RLDBF64.0 68.5 58.2 62.5 64.2 53.8 47.5 44.0 109.4
It outperforms GPT-4o-mini by +20.5% in Level 4 (In-domain) and surpasses ChemLLM by +20.3% in the same category.Notably, RLDBF maintains superior consistency between In-domain and Out-domain evaluations (e.g., 47.5% vs. 44.0% in Level 4), indicating enhanced generalization capabilities.The weighted score of 109.4 further validates its</p>
<p>Table 2 :
2
ChemBench results, rounded to the nearest tenth.We mark the best results with double underlines, the second-best with single underlines.thesetwo strategies may induce overfitting to narrow task distributions (e.g., PubChem's property prediction), thereby compromising generalization to broader, heterogeneous tasks in ChemBench.In contrast, our proposed RLDBF method achieves consistent improvements over the base model Llama3-8B-Instruct across multiple metrics.RLDBF outperforms the base model by +5.8% in Caption2Mol, +5.3% in Product Prediction and Retrosynthesis, and +2.2% in overall average score.Furthermore, RLDBF surpasses all baseline models in Mol2Caption with a score of 93.3, demonstrating superior interpretability in molecular description generation.
This suggests that</p>
<p>Table 4 :
4
We report the RMSE of LCE, baseline results are directly taken from COEFF.
TL F-GCN0.390MoLFormer0.213MultiModal-MoLFormer 0.195COEFF-MLP0.132COEFF-KAN0.110Llama3+MLP0.291RLDBF+MLP0.160</p>
<p>Table 4 ,
4
RLDBF+MLP achieves 0.160 RMSE, significantly outperforming the base model Llama3+MLP (0.291).Compared to SOTA methods, RLDBF demonstrates strong competitiveness.While COEFF-KAN achieves the lowest RMSE (0.110), our method outperforms specialized architectures like MoLFormer (0.213) and MultiModal-MoLFormer (0.195).Notably, RLDBF achieves this without relying on task-specific architectural modifications or extensive domain pretraining, highlighting its generalizability.</p>
<p>https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2
https://pubchem.ncbi.nlm.nih.gov/docs/downloads
A Template ExamplesWe provide examples for the ChatGPT-generated templates, CPT &amp; SFT data and RL data.EXAMPLEShuman: What is the {PROPERTY} of {COMPOUND}?assistant: The {PROPERTY} of {COMPOUND} is {VALUE}.human: I need to know the {PROPERTY} of {COMPOUND}.assistant: The {PROPERTY} for {COMPOUND} is {VALUE}.human: Could you provide the {PROPERTY} for {COMPOUND}?assistant: Certainly, the {PROPERTY} of {COMPOUND} is {VALUE}.B Application on regression tasksThe experimental results on ChemBench demonstrate that our RLDBF method enhances LLMs' ability to uncover structural and numerical relationships in molecular prediction takss, suggesting that RLDBF may implicitly refine molecular representations in the latent embedding space, thereby unlocking the potential of large models for regression tasks.To validate this hypothesis, we conducted experiments on regression task: Coulombic efficiency prediction[39]TYPE EXAMPLE Secondly, using the value as the reference answer, please think step by step and answer the quesiton again.RLYou should think about the reasoning process in the mind and then provide the user with the answer.The reasoning process and answer are enclosed within ⟨think⟩⟨/think⟩ and ⟨answer⟩⟨/answer⟩ tags, respectively, i.e., ⟨think⟩ reasoning process here ⟨/think⟩⟨answer⟩ answer here ⟨/answer⟩.But the reference answer says 1.So where is the mistake here?Wait, perhaps the structure is a zwitterion.Let's consider that.... Therefore, no donor.Hence, the count is 0. But the reference answer is 1.This contradicts.Alternatively, maybe the structure is such that the carboxyl group is actually a carboxylic acid.... Therefore, the reference answer is 1, but according to the given SMILES, it should be 0.This suggests that the SMILES might be incorrect or that there's a mistake in the reference answer.Wait, perhaps the reference answer is correct, and there's a mistake in my reasoning.
Qwen-audio: Advancing universal audio understanding via unified large-scale audio-language models. Yunfei Chu, Jin Xu, Xiaohuan Zhou, Qian Yang, Shiliang Zhang, Zhijie Yan, Chang Zhou, Jingren Zhou, arXiv:2311.079192023arXiv preprint</p>
<p>Sharpness-aware minimization for efficiently improving generalization. Pierre Foret, Ariel Kleiner, Hossein Mobahi, Behnam Neyshabur, ArXiv, abs/2010.014122020</p>
<p>George van den Driessche, Thore Graepel, and Demis Hassabis. Mastering the game of go without human knowledge. David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez, Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, Yutian Chen, Timothy P Lillicrap, Fan Hui, L Sifre, Nature. 5502017</p>
<p>Lessons on parameter sharing across layers in transformers. Sho Takase, Shun Kiyono, ArXiv, abs/2104.060222021</p>
<p>Solving olympiad geometry without human demonstrations. H Trieu, Yuhuai Trinh, Quoc V Wu, He Le, Thang He, Luong, Nature. 6252024</p>
<p>Pubchem 2019 update: improved access to chemical data. Sunghwan Kim, Jie Chen, Tiejun Cheng, Asta Gindulyte, Jia He, Siqian He, Qingliang Li, Benjamin A Shoemaker, Paul A Thiessen, Bo Yu, Nucleic acids research. 47D12019</p>
<p>Smiles, a chemical language and information system. 1. introduction to methodology and encoding rules. David Weininger, Journal of chemical information and computer sciences. 2811988</p>
<p>Large-scale chemical language representations capture molecular structure and properties. Jerret Ross, Brian Belgodere, Inkit Vijil Chenthamarakshan, Youssef Padhi, Payel Mroueh, Das, Nature Machine Intelligence. 4122022</p>
<p>Getting the most out of your tokenizer for pre-training and domain adaptation. Gabriel Gautier Dagan, Baptiste Synnaeve, Roziere, arXiv:2402.010352024arXiv preprint</p>
<p>Crystal structure generation with autoregressive large language modeling. Keith T Luis M Antunes, Ricardo Butler, Grau-Crespo, Nature Communications. 1512024</p>
<p>Alchembert: Exploring lightweight language models for materials informatics. Xiaotong Liu, Yuhang Wang, Tao Yang, Xingchen Liu, Xiaodong Wen, 2025</p>
<p>Language models can generate molecules, materials, and protein binding sites directly in three dimensions as xyz, cif, and pdb files. Daniel Flam, - Shepherd, Alán Aspuru-Guzik, arXiv:2305.057082023arXiv preprint</p>
<p>Atom-by-atom protein generation and beyond with language models. Daniel Flam-Shepherd, Kevin Zhu, Alán Aspuru-Guzik, arXiv:2308.094822023arXiv preprint</p>
<p>Fine-tuned language models generate stable inorganic materials as text. Nate Gruver, Anuroop Sriram, Andrea Madotto, Andrew Gordon Wilson, Lawrence Zitnick, Zachary Ulissi, arXiv:2402.043792024arXiv preprint</p>
<p>Di Zhang, Wei Liu, Qian Tan, Jingdan Chen, Hang Yan, Yuliang Yan, Jiatong Li, Weiran Huang, Xiangyu Yue, Wanli Ouyang, arXiv:2402.06852A chemical large language model. 2024arXiv preprint</p>
<p>. Zheng Cai, Maosong Cao, Haojiong Chen, Kai Chen, Keyu Chen, Xin Chen, Xun Chen, Zehui Chen, Zhi Chen, Pei Chu, arXiv:2403.172972024et al. Internlm2 technical report. arXiv preprint</p>
<p>Zihan Zhao, Da Ma, Lu Chen, Liangtai Sun, Zihao Li, Hongshen Xu, Zichen Zhu, Su Zhu, Shuai Fan, Guodong Shen, Dialogue foundation model for chemistry. 20242401arXiv e-prints</p>
<p>Large language models as master key: unlocking the secrets of materials science with gpt. Tong Xie, Yuwei Wan, Wei Huang, Yufei Zhou, Yixuan Liu, Qingyuan Linghu, Shaozhou Wang, Chunyu Kit, Clara Grazian, Wenjie Zhang, arXiv:2304.022132023arXiv preprint</p>
<p>Watergpt: Training a large language model to become a hydrology expert. Yi Ren, Tianyi Zhang, Xurong Dong, Weibin Li, Zhiyang Wang, Jie He, Hanzhi Zhang, Licheng Jiao, Water. 162130752024</p>
<p>Large language models as general pattern machines. Suvir Mirchandani, Fei Xia, Pete Florence, Brian Ichter, Danny Driess, Montserrat Gonzalez Arenas, Kanishka Rao, Dorsa Sadigh, Andy Zeng, arXiv:2307.047212023arXiv preprint</p>
<p>François Chollet, arXiv:1911.01547On the measure of intelligence. 2019arXiv preprint</p>
<p>Ernie 3.0: Large-scale knowledge enhanced pre-training for language understanding and generation. Yu Sun, Shuohuan Wang, Shikun Feng, Siyu Ding, Chao Pang, Junyuan Shang, Jiaxiang Liu, Xuyi Chen, Yanbin Zhao, Yuxiang Lu, arXiv:2107.021372021arXiv preprint</p>
<p>Sunjun Kweon, Junu Kim, Jiyoun Kim, Sujeong Im, Eunbyeol Cho, Seongsu Bae, Jungwoo Oh, Gyubok Lee, Jong Hak Moon, Seng Chan You, arXiv:2309.00237Publicly shareable clinical large language model built on synthetic clinical notes. 2023arXiv preprint</p>
<p>Knowledge graph finetuning enhances knowledge manipulation in large language models. Hanzhu Chen, Xu Shen, Jie Wang, Zehao Wang, Qitan Lv, Junjie He, Rong Wu, Jieping Ye, Feng Wu, The Thirteenth International Conference on Learning Representations. </p>
<p>Large language models for reticular chemistry. Zhiling Zheng, Nakul Rampal, Theo Jaffrelot Inizan, Christian Borgs, Jennifer T Chayes, Omar M Yaghi, Nature Reviews Materials. 2025</p>
<p>How well do large language models perform in arithmetic tasks?. Zheng Yuan, Hongyi Yuan, Chuanqi Tan, Wei Wang, Songfang Huang, arXiv:2304.020152023arXiv preprint</p>
<p>Development of numerical error detection tasks to analyze the numerical capabilities of language models. Taku Sakamoto, Saku Sugawara, Akiko Aizawa, Proceedings of the 31st International Conference on Computational Linguistics. the 31st International Conference on Computational Linguistics2025</p>
<p>Towards question format independent numerical reasoning: A set of prerequisite tasks. Swaroop Mishra, Arindam Mitra, Neeraj Varshney, Bhavdeep Sachdeva, Chitta Baral, arXiv:2005.085162020arXiv preprint</p>
<p>Sentence-bert: Sentence embeddings using siamese bert-networks. Nils Reimers, Iryna Gurevych, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing. the 2019 Conference on Empirical Methods in Natural Language ProcessingAssociation for Computational Linguistics112019</p>
<p>Training language models to follow instructions with human feedback. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, Advances in neural information processing systems. 202235</p>
<p>Rlaif vs. rlhf: Scaling reinforcement learning from human feedback with ai feedback. Harrison Lee, Samrat Phatale, Hassan Mansoor, Thomas Mesnard, Johan Ferret, Kellie Lu, Colton Bishop, Ethan Hall, Victor Carbune, Abhinav Rastogi, arXiv:2309.002672023arXiv preprint</p>
<p>Direct preference optimization: Your language model is secretly a reward model. Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, Chelsea Finn, Advances in Neural Information Processing Systems. 362023</p>
<p>The llama 3 herd of models. Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, arXiv:2407.217832024arXiv preprint</p>
<p>Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters. Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, Yuxiong He, Proceedings of the 26th ACM SIGKDD international conference on knowledge discovery &amp; data mining. the 26th ACM SIGKDD international conference on knowledge discovery &amp; data mining2020</p>
<p>Aaron Hurst, Adam Lerer, Adam P Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, Akila Ostrow, Alan Welihinda, Alec Hayes, Radford, arXiv:2410.21276Gpt-4o system card. 2024arXiv preprint</p>
<p>. An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, Guanting Dong, Haoran Wei, Huan Lin, Jialong Tang, Jialin Wang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Ma, Jianxin Yang, Jin Xu, Jingren Zhou, Jinze Bai, Jinzheng He, Junyang Lin, Kai Dang, Keming Lu, Keqin Chen, Kexin Yang, Mei Li, Mingfeng Xue, Na Ni, Pei Zhang, Peng Wang, Ru Peng, Rui Men, Ruize Gao, Runji Lin, Shijie Wang, Shuai Bai, Sinan Tan, Tianhang Zhu, Tianhao Li, Tianyu Liu, Wenbin Ge, Xiaodong Deng, Xiaohuan Zhou, Xingzhang Ren, Xinyu Zhang, Xipin Wei, 2024Xuancheng Ren, Xuejing Liu, Yang Fan, Yang Yao, Yichang Zhang, Yu Wan, Yunfei ChuYuqiong Liu, Zeyu Cui, Zhenru Zhang, Zhifang Guo, and Zhihao Fan. Qwen2 technical report</p>
<p>Kale-lm: Unleash the power of ai for science via knowledge and logic enhanced large model. Weichen Dai, Yezeng Chen, Zijie Dai, Zhijie Huang, Yubo Liu, Yixuan Pan, Baiyang Song, Chengli Zhong, Xinhe Li, Zeyu Wang, arXiv:2409.186952024arXiv preprint</p>
<p>Advancing large language models for chemistry with a large-scale, comprehensive, high-quality instruction tuning dataset. Botao Yu, Frazier N Baker, Ziqi Chen, Xia Ning, Huan Sun, Llasmol, arXiv:2402.093912024arXiv preprint</p>
<p>Data-driven electrolyte design for lithium metal anodes. Sang Cheol, Kim , Solomon T Oyakhire, Constantine Athanitis, Jingyang Wang, Zewen Zhang, Wenbo Zhang, David T Boyle, Mun Sek Kim, Zhiao Yu, Xin Gao, Proceedings of the National Academy of Sciences. 12010e22143571202023</p>
<p>Beyond chemical language: A multimodal approach to enhance molecular property prediction. Eduardo Soares, Emilio Vital Brazil, Karen Fiorela Aquino, Renato Gutierrez, Dan Cerqueira, Kristin Sanders, Dmitry Schmidt, Zubarev, arXiv:2306.149192023arXiv preprint</p>
<p>Xinhe Li, Zhuoying Feng, Yezeng Chen, Weichen Dai, Zixu He, Yi Zhou, Shuhong Jiao, arXiv:2407.20265Coeff-kans: A paradigm to address the electrolyte field with kans. 2024arXiv preprint</p>
<p>Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, arXiv:2501.129482025arXiv preprint</p>
<p>C)C? Firstly, answer the question only with the corresponding value. Secondly, using the value as the reference answer, please think step by step and answer the quesiton again. You should think about the reasoning process in the mind and then provide the user with the answer. The reasoning process and answer are enclosed within ⟨think⟩⟨/think⟩ and ⟨answer⟩⟨/answer⟩ tags, respectively, i.e., ⟨think⟩ reasoning process here ⟨/think⟩⟨answer⟩ answer here ⟨/answer⟩. Cc(=o)oc(cc( =o)o)c[n+, Llama-3 8B Reasoner Instruction: What is the Hydrogen Bond Donor Count for. Breaking it down: ... Secondly, ... ... 2But according to the reference answer, there are two H donors. Wait, but this is getting too confusing. Let's approach this differently. Let's count the hydrogen bond donors: ... However, since the reference answer is 2. Wait, maybe I made a mistake in parsing. Let me try again. ... However, the reference answer is 2. Alternatively, perhaps the structure is different. Let me try drawing it step by step</p>            </div>
        </div>

    </div>
</body>
</html>
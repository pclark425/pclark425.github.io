<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7689 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7689</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7689</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-143.html">extraction-schema-143</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to distill quantitative laws, equations, or functional relationships from collections of scholarly papers, including details of the models, prompting or fine‑tuning approaches, input corpora, extraction methods, types of laws, representation formats, evaluation datasets, metrics, baseline comparisons, validation procedures, and reported performance or limitations.</div>
                <p><strong>Paper ID:</strong> paper-273501987</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2410.14716v3.pdf" target="_blank">A Systematic Survey on Large Language Models for Algorithm Design</a></p>
                <p><strong>Paper Abstract:</strong> Algorithm Design (AD) is crucial for effective problem-solving across various domains. The advent of Large Language Models (LLMs) has notably enhanced the automation and innovation within this field, offering new perspectives and promising solutions. Over the past three years, the integration of LLMs into AD (LLM4AD) has seen substantial progress, with applications spanning optimization, machine learning, mathematical reasoning, and scientific discovery. Given the rapid advancements and expanding scope of this field, a systematic review is both timely and necessary. This paper provides a systematic review of LLM4AD. First, we offer an overview and summary of existing studies. Then, we introduce a taxonomy and review the literature across four dimensions: the roles of LLMs, search methods, prompt methods, and application domains with a discussion of potential and achievements of LLMs in AD. Finally, we identify current challenges and highlight several promising directions for future research.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7689.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7689.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to distill quantitative laws, equations, or functional relationships from collections of scholarly papers, including details of the models, prompting or fine‑tuning approaches, input corpora, extraction methods, types of laws, representation formats, evaluation datasets, metrics, baseline comparisons, validation procedures, and reported performance or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM4ED</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM4ED: Large Language Models for Automatic Equation Discovery</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A framework that uses LLMs in an iterative loop (with black‑box optimization and evolutionary operators) to generate and refine candidate equations from data for uncovering governing laws of nonlinear dynamical systems.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>LLM4ED: Large Language Models for Automatic Equation Discovery</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>A Systematic Survey on Large Language Models for Algorithm Design</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_type</strong></td>
                            <td>Pretrained LLM used as a generator/reasoner in an iterative optimization loop (black-box + evolutionary operators); prompting/interaction details not specified in survey.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Problem‑specific time series / simulation data from nonlinear dynamical systems (survey description: general nonlinear dynamics; no token counts or paper‑corpus metadata provided).</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_method</strong></td>
                            <td>Iterative generation and optimization of candidate equations via LLM sampling combined with evolutionary/black‑box optimization operators (LLM proposes equation structures which are optimized/refined by evolutionary operators).</td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td>Physical governing laws for nonlinear dynamical systems (physics / PDE/ODE-type laws).</td>
                        </tr>
                        <tr>
                            <td><strong>law_representation</strong></td>
                            <td>Symbolic equation forms (candidate algebraic/differential equations / governing equations).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_dataset</strong></td>
                            <td>Described generally as nonlinear dynamical system testbeds (no specific benchmark names given in survey for LLM4ED entry).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Not specified in survey for this entry (survey summary uses qualitative terms: stability, usability).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Iterative refinement with optimization loop; survey states improvements in stability/usability for equation discovery but does not detail held‑out test protocols.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Survey notes limited detail in the literature: lack of explicit metrics reported, black‑box nature of LLM proposals, and general concerns about interpretability and scalability when using LLMs for equation discovery.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Systematic Survey on Large Language Models for Algorithm Design', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7689.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7689.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to distill quantitative laws, equations, or functional relationships from collections of scholarly papers, including details of the models, prompting or fine‑tuning approaches, input corpora, extraction methods, types of laws, representation formats, evaluation datasets, metrics, baseline comparisons, validation procedures, and reported performance or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-SR</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-SR: Scientific Equation Discovery via Programming with Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method combining LLMs' scientific knowledge and code generation capabilities with evolutionary search to propose and refine initial equation structures; reported to outperform traditional symbolic regression methods on diverse scientific problems (as summarized in the survey).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>LLM-SR: Scientific Equation Discovery via Programming with Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>A Systematic Survey on Large Language Models for Algorithm Design</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_type</strong></td>
                            <td>Pretrained LLM used for program/structure proposal and code generation that is embedded within an evolutionary search/refinement pipeline; specific prompting/fine‑tuning strategies not detailed in the survey.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Scientific datasets across multiple domains used for equation discovery (survey summary: 'multiple scientific domains'); the survey does not provide counts or preprocessing details.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_method</strong></td>
                            <td>LLM generates programmatic representations or symbolic skeletons of candidate equations; evolutionary search refines structure and parameters (LLM + evolutionary symbolic regression hybrid).</td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td>Scientific equations / functional relationships across physics and other scientific domains (physical laws, empirical functional forms).</td>
                        </tr>
                        <tr>
                            <td><strong>law_representation</strong></td>
                            <td>Symbolic equations and programmatic equation candidates (algebraic forms and differential relations depending on domain).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_dataset</strong></td>
                            <td>Multiple scientific-domain datasets (survey does not enumerate specific datasets for LLM-SR), evaluation compares discovered equations to ground truth or to results from symbolic regression baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Not enumerated in the survey; description is qualitative (claims of outperforming traditional symbolic regression). Typical metrics implied would be correctness of recovered symbolic form and generalization to unseen data, but survey gives no numeric metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Comparison to traditional symbolic regression methods and testing on held‑out / unseen instances (survey states LLM‑SR outperforms symbolic regression across multiple domains but provides no numeric detail).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Survey notes general issues: limited transparency on quantitative metrics, reliance on search frameworks for refinement, and broader LLM limitations (interpretability, context size, and domain specificity).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Systematic Survey on Large Language Models for Algorithm Design', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7689.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7689.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to distill quantitative laws, equations, or functional relationships from collections of scholarly papers, including details of the models, prompting or fine‑tuning approaches, input corpora, extraction methods, types of laws, representation formats, evaluation datasets, metrics, baseline comparisons, validation procedures, and reported performance or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Du et al. (governing equations)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Large language models for automatic equation discovery of nonlinear dynamics</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A study that adopts LLMs to iteratively generate and refine candidate governing equations from data for nonlinear dynamical systems (tested on Burgers, Chafee‑Infante, Navier‑Stokes in the survey summary), showing superior ability to recover correct laws and improved generalization versus other models according to the survey.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Large language models for automatic equation discovery of nonlinear dynamics</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>A Systematic Survey on Large Language Models for Algorithm Design</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_type</strong></td>
                            <td>LLM used as a generator and reasoner to propose symbolic governing equations; incorporated in an iterative refinement workflow (specific prompting, fine‑tuning not described in the survey).</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Simulation / measurement data from canonical PDE/nonlinear systems (survey explicitly cites Burgers, Chafee‑Infante, and Navier‑Stokes as test problems).</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_method</strong></td>
                            <td>Iterative LLM generation of candidate equations followed by refinement and selection; LLMs used to propose symbolic terms/structures that are evaluated against data and refined (LLM‑guided symbolic regression style).</td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td>Physical governing laws / partial differential equations and other nonlinear dynamical system equations.</td>
                        </tr>
                        <tr>
                            <td><strong>law_representation</strong></td>
                            <td>Symbolic forms including differential equations (PDEs/ODEs) and algebraic relations representing governing dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_dataset</strong></td>
                            <td>Canonical PDE/dynamics test cases: Burgers equation, Chafee‑Infante, Navier‑Stokes (as reported in the survey).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Survey reports qualitative claims (correctness of recovered physical laws and generalization on unseen data); no numeric metrics reported in the survey summary.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Evaluation on canonical PDE benchmark systems and tests of generalization to unseen data; compared against 'other models' (survey does not specify which baselines or numeric validation procedures).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Survey notes improved generalization but does not report quantitative error metrics; general limitations include LLM black‑box behavior, lack of precise evaluation detail, and broader concerns about interpretability and sample/context limits.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Systematic Survey on Large Language Models for Algorithm Design', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7689.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7689.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to distill quantitative laws, equations, or functional relationships from collections of scholarly papers, including details of the models, prompting or fine‑tuning approaches, input corpora, extraction methods, types of laws, representation formats, evaluation datasets, metrics, baseline comparisons, validation procedures, and reported performance or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SGA / Bilevel (Ma et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM and Simulation as Bilevel Optimizers (SGA-style bilevel framework)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A bilevel optimization framework merging LLM abstract reasoning with simulation feedback: the LLM generates hypotheses and discrete proposals while simulations provide continuous feedback for optimizing continuous variables, enabling hypothesis generation and refinement.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>LLM and Simulation as Bilevel Optimizers: A New Paradigm to Advance Physical Scientific Discovery</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>A Systematic Survey on Large Language Models for Algorithm Design</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_type</strong></td>
                            <td>LLM used as the upper‑level (abstract/hypothesis) proposer in a bilevel optimization loop, with simulations as the lower level providing numerical feedback; prompting/fine‑tuning specifics not provided in the survey.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Simulation outputs and domain problem specifications; survey describes use of experimental/simulation feedback rather than a textual scholarly corpus for extraction.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_method</strong></td>
                            <td>Bilevel loop: LLM proposes hypotheses/equation forms; simulations evaluate continuous variables and provide feedback used to optimize proposals (integration of symbolic proposal + numerical simulation).</td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td>Physical hypotheses and quantitative functional relationships tying simulation variables (physical discovery / experimental laws).</td>
                        </tr>
                        <tr>
                            <td><strong>law_representation</strong></td>
                            <td>Symbolic/hybrid representations (equation forms proposed by LLM + numerical parameter tuning via simulation).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_dataset</strong></td>
                            <td>Simulation cases and experimental feedback loops; survey does not list standard dataset names for this framework.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Not specified in survey; described qualitatively as improved performance due to combined reasoning + simulation feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Validation via simulation‑informed optimization and reported improved performance in experiments described by the original work (survey does not provide numeric protocol details).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Survey highlights general limitations: need for computationally expensive simulation loops, interpretability of LLM proposals, and the broader challenges of scaling LLM‑driven scientific discovery workflows.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Systematic Survey on Large Language Models for Algorithm Design', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7689.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e7689.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to distill quantitative laws, equations, or functional relationships from collections of scholarly papers, including details of the models, prompting or fine‑tuning approaches, input corpora, extraction methods, types of laws, representation formats, evaluation datasets, metrics, baseline comparisons, validation procedures, and reported performance or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Mathematical program search (Romera‑Paredes et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mathematical discoveries from program search with large language models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Work that uses program search guided by LLMs to discover new mathematical identities/results; included in the survey's list of scientific discovery/equation discovery references as an example of LLMs used for formal/quantitative discovery.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Mathematical discoveries from program search with large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>A Systematic Survey on Large Language Models for Algorithm Design</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_type</strong></td>
                            <td>LLM used to guide programmatic search for mathematical identities and constructions; survey lists it as a related example without protocol detail.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Mathematical problems / program search spaces (survey does not give corpus size or preprocessing details).</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_method</strong></td>
                            <td>Program search guided by LLM proposals to produce symbolic/mathematical constructs and discover novel results; more programmatic than pure symbolic regression.</td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td>Mathematical identities and provable relationships (formal mathematical discoveries rather than empirical physical laws).</td>
                        </tr>
                        <tr>
                            <td><strong>law_representation</strong></td>
                            <td>Programmatic symbolic expressions / formal mathematical statements.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_dataset</strong></td>
                            <td>Mathematical benchmark problems and verification tasks (survey does not list the concrete benchmarks in detail).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Not specified in the survey summary; original work evaluates novelty and correctness of discovered mathematical statements, but survey does not extract numeric metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Programmatic verification / correctness checks for discovered mathematical identities (survey notes the paper as an instance of program search yielding mathematical discoveries).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Survey only lists this as an example; broader limitations apply (LLM transparency, ability to extrapolate beyond training data, and need for search/verification frameworks).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Systematic Survey on Large Language Models for Algorithm Design', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>LLM4ED: Large Language Models for Automatic Equation Discovery <em>(Rating: 2)</em></li>
                <li>LLM-SR: Scientific Equation Discovery via Programming with Large Language Models <em>(Rating: 2)</em></li>
                <li>Large language models for automatic equation discovery of nonlinear dynamics <em>(Rating: 2)</em></li>
                <li>LLM and Simulation as Bilevel Optimizers: A New Paradigm to Advance Physical Scientific Discovery <em>(Rating: 2)</em></li>
                <li>Mathematical discoveries from program search with large language models <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7689",
    "paper_id": "paper-273501987",
    "extraction_schema_id": "extraction-schema-143",
    "extracted_data": [
        {
            "name_short": "LLM4ED",
            "name_full": "LLM4ED: Large Language Models for Automatic Equation Discovery",
            "brief_description": "A framework that uses LLMs in an iterative loop (with black‑box optimization and evolutionary operators) to generate and refine candidate equations from data for uncovering governing laws of nonlinear dynamical systems.",
            "citation_title": "LLM4ED: Large Language Models for Automatic Equation Discovery",
            "mention_or_use": "mention",
            "paper_title": "A Systematic Survey on Large Language Models for Algorithm Design",
            "llm_name": null,
            "llm_type": "Pretrained LLM used as a generator/reasoner in an iterative optimization loop (black-box + evolutionary operators); prompting/interaction details not specified in survey.",
            "input_corpus_description": "Problem‑specific time series / simulation data from nonlinear dynamical systems (survey description: general nonlinear dynamics; no token counts or paper‑corpus metadata provided).",
            "extraction_method": "Iterative generation and optimization of candidate equations via LLM sampling combined with evolutionary/black‑box optimization operators (LLM proposes equation structures which are optimized/refined by evolutionary operators).",
            "law_type": "Physical governing laws for nonlinear dynamical systems (physics / PDE/ODE-type laws).",
            "law_representation": "Symbolic equation forms (candidate algebraic/differential equations / governing equations).",
            "evaluation_dataset": "Described generally as nonlinear dynamical system testbeds (no specific benchmark names given in survey for LLM4ED entry).",
            "evaluation_metrics": "Not specified in survey for this entry (survey summary uses qualitative terms: stability, usability).",
            "performance_results": null,
            "baseline_comparison": null,
            "baseline_performance": null,
            "validation_method": "Iterative refinement with optimization loop; survey states improvements in stability/usability for equation discovery but does not detail held‑out test protocols.",
            "limitations": "Survey notes limited detail in the literature: lack of explicit metrics reported, black‑box nature of LLM proposals, and general concerns about interpretability and scalability when using LLMs for equation discovery.",
            "uuid": "e7689.0",
            "source_info": {
                "paper_title": "A Systematic Survey on Large Language Models for Algorithm Design",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "LLM-SR",
            "name_full": "LLM-SR: Scientific Equation Discovery via Programming with Large Language Models",
            "brief_description": "A method combining LLMs' scientific knowledge and code generation capabilities with evolutionary search to propose and refine initial equation structures; reported to outperform traditional symbolic regression methods on diverse scientific problems (as summarized in the survey).",
            "citation_title": "LLM-SR: Scientific Equation Discovery via Programming with Large Language Models",
            "mention_or_use": "mention",
            "paper_title": "A Systematic Survey on Large Language Models for Algorithm Design",
            "llm_name": null,
            "llm_type": "Pretrained LLM used for program/structure proposal and code generation that is embedded within an evolutionary search/refinement pipeline; specific prompting/fine‑tuning strategies not detailed in the survey.",
            "input_corpus_description": "Scientific datasets across multiple domains used for equation discovery (survey summary: 'multiple scientific domains'); the survey does not provide counts or preprocessing details.",
            "extraction_method": "LLM generates programmatic representations or symbolic skeletons of candidate equations; evolutionary search refines structure and parameters (LLM + evolutionary symbolic regression hybrid).",
            "law_type": "Scientific equations / functional relationships across physics and other scientific domains (physical laws, empirical functional forms).",
            "law_representation": "Symbolic equations and programmatic equation candidates (algebraic forms and differential relations depending on domain).",
            "evaluation_dataset": "Multiple scientific-domain datasets (survey does not enumerate specific datasets for LLM-SR), evaluation compares discovered equations to ground truth or to results from symbolic regression baselines.",
            "evaluation_metrics": "Not enumerated in the survey; description is qualitative (claims of outperforming traditional symbolic regression). Typical metrics implied would be correctness of recovered symbolic form and generalization to unseen data, but survey gives no numeric metrics.",
            "performance_results": null,
            "baseline_comparison": true,
            "baseline_performance": null,
            "validation_method": "Comparison to traditional symbolic regression methods and testing on held‑out / unseen instances (survey states LLM‑SR outperforms symbolic regression across multiple domains but provides no numeric detail).",
            "limitations": "Survey notes general issues: limited transparency on quantitative metrics, reliance on search frameworks for refinement, and broader LLM limitations (interpretability, context size, and domain specificity).",
            "uuid": "e7689.1",
            "source_info": {
                "paper_title": "A Systematic Survey on Large Language Models for Algorithm Design",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Du et al. (governing equations)",
            "name_full": "Large language models for automatic equation discovery of nonlinear dynamics",
            "brief_description": "A study that adopts LLMs to iteratively generate and refine candidate governing equations from data for nonlinear dynamical systems (tested on Burgers, Chafee‑Infante, Navier‑Stokes in the survey summary), showing superior ability to recover correct laws and improved generalization versus other models according to the survey.",
            "citation_title": "Large language models for automatic equation discovery of nonlinear dynamics",
            "mention_or_use": "mention",
            "paper_title": "A Systematic Survey on Large Language Models for Algorithm Design",
            "llm_name": null,
            "llm_type": "LLM used as a generator and reasoner to propose symbolic governing equations; incorporated in an iterative refinement workflow (specific prompting, fine‑tuning not described in the survey).",
            "input_corpus_description": "Simulation / measurement data from canonical PDE/nonlinear systems (survey explicitly cites Burgers, Chafee‑Infante, and Navier‑Stokes as test problems).",
            "extraction_method": "Iterative LLM generation of candidate equations followed by refinement and selection; LLMs used to propose symbolic terms/structures that are evaluated against data and refined (LLM‑guided symbolic regression style).",
            "law_type": "Physical governing laws / partial differential equations and other nonlinear dynamical system equations.",
            "law_representation": "Symbolic forms including differential equations (PDEs/ODEs) and algebraic relations representing governing dynamics.",
            "evaluation_dataset": "Canonical PDE/dynamics test cases: Burgers equation, Chafee‑Infante, Navier‑Stokes (as reported in the survey).",
            "evaluation_metrics": "Survey reports qualitative claims (correctness of recovered physical laws and generalization on unseen data); no numeric metrics reported in the survey summary.",
            "performance_results": null,
            "baseline_comparison": true,
            "baseline_performance": null,
            "validation_method": "Evaluation on canonical PDE benchmark systems and tests of generalization to unseen data; compared against 'other models' (survey does not specify which baselines or numeric validation procedures).",
            "limitations": "Survey notes improved generalization but does not report quantitative error metrics; general limitations include LLM black‑box behavior, lack of precise evaluation detail, and broader concerns about interpretability and sample/context limits.",
            "uuid": "e7689.2",
            "source_info": {
                "paper_title": "A Systematic Survey on Large Language Models for Algorithm Design",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "SGA / Bilevel (Ma et al.)",
            "name_full": "LLM and Simulation as Bilevel Optimizers (SGA-style bilevel framework)",
            "brief_description": "A bilevel optimization framework merging LLM abstract reasoning with simulation feedback: the LLM generates hypotheses and discrete proposals while simulations provide continuous feedback for optimizing continuous variables, enabling hypothesis generation and refinement.",
            "citation_title": "LLM and Simulation as Bilevel Optimizers: A New Paradigm to Advance Physical Scientific Discovery",
            "mention_or_use": "mention",
            "paper_title": "A Systematic Survey on Large Language Models for Algorithm Design",
            "llm_name": null,
            "llm_type": "LLM used as the upper‑level (abstract/hypothesis) proposer in a bilevel optimization loop, with simulations as the lower level providing numerical feedback; prompting/fine‑tuning specifics not provided in the survey.",
            "input_corpus_description": "Simulation outputs and domain problem specifications; survey describes use of experimental/simulation feedback rather than a textual scholarly corpus for extraction.",
            "extraction_method": "Bilevel loop: LLM proposes hypotheses/equation forms; simulations evaluate continuous variables and provide feedback used to optimize proposals (integration of symbolic proposal + numerical simulation).",
            "law_type": "Physical hypotheses and quantitative functional relationships tying simulation variables (physical discovery / experimental laws).",
            "law_representation": "Symbolic/hybrid representations (equation forms proposed by LLM + numerical parameter tuning via simulation).",
            "evaluation_dataset": "Simulation cases and experimental feedback loops; survey does not list standard dataset names for this framework.",
            "evaluation_metrics": "Not specified in survey; described qualitatively as improved performance due to combined reasoning + simulation feedback.",
            "performance_results": null,
            "baseline_comparison": true,
            "baseline_performance": null,
            "validation_method": "Validation via simulation‑informed optimization and reported improved performance in experiments described by the original work (survey does not provide numeric protocol details).",
            "limitations": "Survey highlights general limitations: need for computationally expensive simulation loops, interpretability of LLM proposals, and the broader challenges of scaling LLM‑driven scientific discovery workflows.",
            "uuid": "e7689.3",
            "source_info": {
                "paper_title": "A Systematic Survey on Large Language Models for Algorithm Design",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Mathematical program search (Romera‑Paredes et al.)",
            "name_full": "Mathematical discoveries from program search with large language models",
            "brief_description": "Work that uses program search guided by LLMs to discover new mathematical identities/results; included in the survey's list of scientific discovery/equation discovery references as an example of LLMs used for formal/quantitative discovery.",
            "citation_title": "Mathematical discoveries from program search with large language models",
            "mention_or_use": "mention",
            "paper_title": "A Systematic Survey on Large Language Models for Algorithm Design",
            "llm_name": null,
            "llm_type": "LLM used to guide programmatic search for mathematical identities and constructions; survey lists it as a related example without protocol detail.",
            "input_corpus_description": "Mathematical problems / program search spaces (survey does not give corpus size or preprocessing details).",
            "extraction_method": "Program search guided by LLM proposals to produce symbolic/mathematical constructs and discover novel results; more programmatic than pure symbolic regression.",
            "law_type": "Mathematical identities and provable relationships (formal mathematical discoveries rather than empirical physical laws).",
            "law_representation": "Programmatic symbolic expressions / formal mathematical statements.",
            "evaluation_dataset": "Mathematical benchmark problems and verification tasks (survey does not list the concrete benchmarks in detail).",
            "evaluation_metrics": "Not specified in the survey summary; original work evaluates novelty and correctness of discovered mathematical statements, but survey does not extract numeric metrics.",
            "performance_results": null,
            "baseline_comparison": null,
            "baseline_performance": null,
            "validation_method": "Programmatic verification / correctness checks for discovered mathematical identities (survey notes the paper as an instance of program search yielding mathematical discoveries).",
            "limitations": "Survey only lists this as an example; broader limitations apply (LLM transparency, ability to extrapolate beyond training data, and need for search/verification frameworks).",
            "uuid": "e7689.4",
            "source_info": {
                "paper_title": "A Systematic Survey on Large Language Models for Algorithm Design",
                "publication_date_yy_mm": "2024-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "LLM4ED: Large Language Models for Automatic Equation Discovery",
            "rating": 2,
            "sanitized_title": "llm4ed_large_language_models_for_automatic_equation_discovery"
        },
        {
            "paper_title": "LLM-SR: Scientific Equation Discovery via Programming with Large Language Models",
            "rating": 2,
            "sanitized_title": "llmsr_scientific_equation_discovery_via_programming_with_large_language_models"
        },
        {
            "paper_title": "Large language models for automatic equation discovery of nonlinear dynamics",
            "rating": 2,
            "sanitized_title": "large_language_models_for_automatic_equation_discovery_of_nonlinear_dynamics"
        },
        {
            "paper_title": "LLM and Simulation as Bilevel Optimizers: A New Paradigm to Advance Physical Scientific Discovery",
            "rating": 2,
            "sanitized_title": "llm_and_simulation_as_bilevel_optimizers_a_new_paradigm_to_advance_physical_scientific_discovery"
        },
        {
            "paper_title": "Mathematical discoveries from program search with large language models",
            "rating": 2,
            "sanitized_title": "mathematical_discoveries_from_program_search_with_large_language_models"
        }
    ],
    "cost": 0.01928075,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>A Systematic Survey on Large Language Models for Algorithm Design
1 Nov 2024</p>
<p>Fei Liu 
Yiming Yao yimingyao3-c@my.cityu.edu.hk 
Ping Guo pingguo5-c@my.cityu.edu 
Zhiyuan Yang zhiyuan.yang@my.cityu.edu.hk 
Zhe Zhao 
Zhichao Lu luzhichaocn@gmail.com 
Zhenkun Wang 
Qingfu Zhang qingfu.zhang@cityu.edu.hk </p>
<p>City University of Hong Kong
China</p>
<p>City University of Hong Kong
China</p>
<p>City University of Hong Kong
China</p>
<p>and Huawei Noah's Ark Lab
City University of Hong Kong
China, China</p>
<p>XI LIN
City University of Hong Kong
China</p>
<p>City University of Hong Kong
China</p>
<p>University of Science and Technology of China
China</p>
<p>City University of Hong Kong
China</p>
<p>Southern University of Science and Technology
China</p>
<p>City University of Hong Kong
China</p>
<p>City University of Hong Kong
Hong Kong, YaoChina; Yiming</p>
<p>City University of Hong Kong
Hong KongChina</p>
<p>City University of Hong Kong
Hong Kong, Zhiyuan YangChina</p>
<p>and Huawei Noah's Ark Lab
City University of Hong Kong
Hong Kong, Hong Kong, Xi LinChina, China</p>
<p>City University of Hong Kong
Hong Kong, Zhe ZhaoChina</p>
<p>City University of Hong Kong
Hong KongChina</p>
<p>University of Science and Technology of China
HefeiChina</p>
<p>Xialiang Tong</p>
<p>Huawei Noah's Ark Lab
ShenzhenMingxuan YuanChina</p>
<p>Huawei Noah's Ark Lab
Hong Kong, Zhichao LuChina</p>
<p>City University of Hong Kong
Hong Kong, Zhenkun WangChina</p>
<p>Southern University of Science and Technology
ShenzhenQingfu ZhangChina</p>
<p>City University of Hong Kong
Hong KongChina</p>
<p>A Systematic Survey on Large Language Models for Algorithm Design
1 Nov 2024E14BD50C57533F3FC9FEF82AFB8D4A6FarXiv:2410.14716v3[cs.LG]Large language modelAutomated algorithm designOptimizationHeuristicHyperheuristicEvolutionary algorithm Google scholarWeb of ScienceScopus Results (remove duplication): 850 papers not Englishnot algorithm designnot using large language model Remaining Results: 260 papers
Algorithm Design (AD) is crucial for effective problem-solving across various domains.The advent of Large Language Models (LLMs) has notably enhanced the automation and innovation within this field, offering new perspectives and promising solutions.Over the past three years, the integration of LLMs into AD (LLM4AD) has seen substantial progress, with applications spanning optimization, machine learning, mathematical reasoning, and scientific discovery.Given the rapid advancements and expanding scope of this field, a systematic review is both timely and necessary.This paper provides a systematic review of LLM4AD.First, we offer an overview and summary of existing studies.Then, we introduce a taxonomy and review the literature across four dimensions: the roles of LLMs, search methods, prompt methods, and application domains with a discussion of potential and achievements of LLMs in AD.Finally, we identify current challenges and highlight several promising directions for future research.</p>
<p>Introduction</p>
<p>Algorithms play a crucial role in addressing various problems across various domains such as industry, economics, healthcare, and technology [26,75].Traditionally, designing algorithm has been a labor-intensive process that demands deep expertise.Recently, there has been a surge in interest towards employing learning and computational intelligence methods techniques to enhance and automate the algorithm development process [10,142].</p>
<p>In the realm of artificial intelligence, Large Language Models (LLMs) have marked a significant advancement.Characterized by their vast scale, extensive training, and superior performance, LLMs have made notable impacts in the fields such as mathematical reasoning [4], code generation [72], and scientific discovery [152].</p>
<p>Over the past three years, the application of Large Language Models for Algorithm Design (LLM4AD) has merged as a promising research area with the potential to fundamentally transform the ways in which algorithms are designed, optimized and implemented.The remarkable capability and flexibility of LLMs have demonstrated potential in enhancing the algorithm design process, including performance prediction [56],</p>
<p>heuristic generation [88], code optimization [59], and even the invention of new algorithmic ideas [46] specifically tailored to target tasks.This approach not only reduces the human effort required in the design phase but also enhances the creativity and efficiency of the produced solutions [88,128].</p>
<p>While LLM4AD is gaining traction, there is a notable absence of a systematic review in this emerging field.The existing literature primarily focuses on the applications of LLMs within specific algorithmic contexts.For instance, several studies have been conducted to survey the use of LLMs for optimization topics [58,64,163], while others review general LLM applications [53] or their use in particular domains such as electronic design automation [189], planning [118], recommendation systems [162], and agents [154].</p>
<p>This paper aims to address this gap by providing a systematic review with a multi-dimensional taxonomy of the current state of LLMs in algorithm design.We will also explore various applications, discuss key challenges, and propose directions for future research.By synthesizing these insights, this paper contributes to a deeper understanding of the potential of LLMs to enhance and automate algorithm design and lays the groundwork for further innovations in this exciting field.We expect this paper to be a helpful resource for both newcomers to the field and experienced experts seeking a consolidated and systematic update on current developments.The contributions of this paper are outlined as follows:</p>
<p>• Systematic Review of LLM4AD: We present the first systematic review of the developments in using LLMs for algorithm design, covering a significant corpus of 180+ highly related research papers published in the last three years.</p>
<p>• Development of a Multi-dimensional Taxonomy: We introduce a multi-dimensional taxonomy that categorizes the works and functionalities of LLM4AD into four distinct dimensions: 1) Roles of LLMs in algorithm design, which delineates how these models contribute to or enhance algorithm design; 2) Search methods, which explores the various approaches used by LLMs to navigate and optimize search spaces in algorithm design; 3) Prompt methods, which examines how diverse prompting strategies are used; and 4) Application domains, which identifies the key fields and industries where LLMs are being applied to solve complex algorithmic challenges.This taxonomy not only clarifies the landscape but also aids in identifying gaps and opportunities for future research.</p>
<p>• Discussion on Challenges and Future Directions: We go beyond mere summarization of existing literature to critically analyze the limitations present in current research on LLMs for algorithm design.</p>
<p>Furthermore, we highlight potential future research directions, including developing domain-specific LLMs, exploring multi-modal LLMs, facilitating human-LLM interaction, using LLMs for algorithm assessment and understanding LLM behavior, advancing fully automated algorithm design, and benchmarking for systematic evaluation of LLMs in algorithm design.This discussion is intended to spur novel approaches and foster further advancements in the field.</p>
<p>Methodology and Taxonomy</p>
<p>Scope of Survey</p>
<p>This paper aims to conduct a systematic survey and classification of existing research works in the emerging field of Large Language Model for Algorithm Design (LLM4AD).We do not intend to cover all the literature on both LLMs and algorithms.We delineate the scope of our survey as follows:</p>
<p>• The term Large Language Models refers to language models of sufficient scale.These models typically utilize a transformer architecture and operate in an autoregressive manner [188].Studies employing smaller models for algorithm design, such as conventional model-based and machine learning-assisted algorithms [10], are excluded.Research utilizing other large models that lack language processing capabilities, such as purely vision-based models, are not considered.However, multi-modal LLMs that include language processing are within our scope.</p>
<p>• The term Algorithm in this context refers to a set of mathematical instructions or rules designed to solve a problem, particularly when executed by a computer [26].This broad definition encompasses traditional mathematical algorithms [4], most heuristic approaches [108], and certain agents or policies that can be interpreted as algorithms [165].</p>
<p>We introduce the detailed pipeline for paper collection and scanning, which consists of four stages:</p>
<p>• Stage I Data Extraction and Collection: We collect the related papers through Google Scholar, Web of Science, and Scopus.The logic of our search is the title must include any combinations of at least one of the following two groups of words "LLM", "LLMs", "Large Language Model", "Large Language Models" and "Algorithm", "Heuristic", "Search", "Optimization", "Optimizer", "Design", "Function" (e.g., LLM and optimization, LLMs and algorithm).After removing duplicated papers, we ended up with 850 papers as of July 1, 2024.</p>
<p>• Stage II Abstract Scanning: We check the title and abstract of each paper to efficiently exclude irrelevant papers.The criteria used for exclusion include these papers that are not in English, not for algorithm design and not using large language models.After scanning, 260 papers are remaining.</p>
<p>• Stage III Full Scanning: We thoroughly review each manuscript to exclude papers lacking relevant content.After scanning, there are 160 papers left.</p>
<p>• Stage IV Supplementation: We append some related works manually according to our past knowledge in this field to avoid missing any important contributions.After integrating the additional papers, we got 180+ papers in the end.</p>
<p>We will first present an overview of the LLM4AD paper list and then present a taxonomy to systematically review the progress.In addition to the organized list of papers, we also incorporate some important publications released after July 1, 2024.</p>
<p>Overview</p>
<p>Fig. 2a illustrates the trend in the number of papers published over time, with the timeline expressed in months.The graph shows a marked rise in research activity related to LLM4AD, particularly noting that most of the studies have been conducted in the last year.This suggests that LLM4AD is an emerging field, and we expect a significant increase in research output in the near future as scholars from diverse fields become aware of its considerable potential.Fig. 2c and Fig. 2b display the leading institutions and their respective countries contributing to publications on LLM4AD.The United States leads, closely followed by China, with these two countries alone accounting for 50% of the publications.The next eight countries, including Singapore, Canada, and Japan, collectively contribute one-third of the total publications.Prominent institutions involved in this research include esteemed universities such as Tsinghua University, Nanyang Technological University, and the University of Toronto, alongside major corporations like Huawei, Microsoft, and Google.This distribution underscores the widespread interest in the research topics and their substantial relevance to practical applications in the real world.</p>
<p>In Fig. 3, the word cloud is generated from the titles and abstracts of all reviewed papers, with each word appearing at least five times.It showcases the top 80 keywords, organized into four color-coded clusters on "language", "GPT", "search and optimization", and "scientific discovery".Several keywords such as "evolution", "strategy", "optimizer", and "agent" are also highlighted.</p>
<p>Taxonomy</p>
<p>This paper presents a taxonomy organized into four dimensions as shown in Fig. 4: 1) LLM Roles, 2) Search Methods, 3) Prompt Techniques, and 4) Applications.• Search Methods: This category involves the search methods used including the basic sampling methods and more complex approaches like evolutionary algorithms, and uncertainty-guided methods.</p>
<p>• Prompt Methods: The effectiveness of pre-trained LLMs is heavily influenced by prompt engineering.</p>
<p>We investigate the typical prompt strategies used in LLM4AD including zero-shot, few-shot, chain-ofthought, self-consistency, and reflection prompts.</p>
<p>• Applications: The broad application of LLM4AD covers diverse fields.We identify the main domains including optimization, machine learning, industry, and scientific discovery.</p>
<p>LLM Roles</p>
<p>According to the roles of LLM in algorithm design, existing works can be categorized into four classes:</p>
<p>LLMs as Optimizers (LLMaO), LLMs as Predictors (LLMaP), LLMs as Extractors (LLMaE), and LLMs as Designers (LLMaD).This section presents the progress and explore the advantages and limitations associated with each category.</p>
<p>LLMs as Optimizers (LLMaO)</p>
<p>In LLMaO, LLMs are utilized as a black-box optimizer within an algorithm framework to generate and refine solutions (Fig. 5).The integration of LLMs into optimization tasks leverages their ability to understand and generate complex patterns and solutions and good flexibility [86,92,167].These capabilities are often challenging for traditional optimizers and models to match.However, the black-box nature of LLMs typically results in a lack of interpretability and presents difficulties in large-scale generation [86].</p>
<p>Problem Algorithm Solution</p>
<p>LLM</p>
<p>Solution Solution</p>
<p>Fig. 5. Large Language Models as Optimizers (LLMaO).LLMs serve as optimizers within the algorithm to generate new solutions.This typically involves using the LLM in an iterative search process to enhance solution quality.Here, the algorithm and its parameters are usually crafted by humans.</p>
<p>One of the initial efforts to utilize LLMs as optimizers in algorithm design is by Yang et al. [167].They leverage the in-context learning capabilities of LLMs to generate novel solutions for specific problems based on previously evaluated solutions.This method is applied iteratively to refine solutions further.Yang et al. [167] have successfully demonstrated this technique across various domains, including continuous and combinatorial optimization, as well as machine learning tasks.</p>
<p>From an evolutionary algorithm (EA) perspective, using LLMs to generate solutions from existing data can be seen as analogous to search operators in EA.For instance, Liu et al. [86] introduce the use of LLMs as evolutionary operators to tackle multi-objective problems.This method involves breaking down a multi-objective problem into simpler single-objective tasks, with LLMs acting as black-box search operators for each sub-problem to suggest new solutions.In a related study, Liu et al. [92] explore the integration of LLMs within EAs, not just for generating solutions but also for guiding selection, crossover, and mutation processes.Meanwhile, Brahmachary et al. [14] propose a new population-based evolutionary framework that includes both exploration and exploitation pools, with solutions being exchanged during the optimization process and LLMs generating solutions for both pools.</p>
<p>Differing from direct solution generation, Lange et al. [77] investigate the use of LLMs in designing evolution strategies, introducing a new prompting strategy to enhance the mean statistic in their EvoLLM method, which shows superior performance over baseline algorithms in synthetic black-box optimization functions and neuroevolution tasks.They also demonstrate that fine-tuning LLMs with data from teacher algorithms can further improve the performance of EvoLLM.Moreover, Custode et al. [28] present a preliminary study that uses LLMs to automate hyperparameter selection by analyzing optimization logs and providing real-time recommendations.</p>
<p>Beyond traditional optimization tasks, LLMaO has been widely adopted in prompt engineering for LLMs, a process often referred to as "automatic prompt optimization" [192].These methods primarily involve iterative refinement of prompts by LLMs to improve their effectiveness for specific models (typically LLMs).</p>
<p>Techniques include resampling-based strategies, where LLMs generate variations of original prompts while maintaining semantic similarity [156], and reflection-based strategies, where LLMs optimize by analyzing and learning from previous prompt iterations or errors [50], have been explored.Ma et al. [99] note that LLM optimizers often struggle to accurately identify the root causes of errors during the reflection process, influenced by their pre-existing knowledge rather than an objective analysis of mistakes.To address these issues, they propose a new approach termed "automatic behavior optimization", aimed at directly and more effectively controlling the behavior of target models.</p>
<p>LLMs as Predictors (LLMaP)</p>
<p>LLMaP employs LLMs as surrogate models (Fig. 6) to predict the outcomes or responses of solutions, functioning either in a classification or regression context [56].Compared to other model-based predictors, such as the Gaussian process and conventional neural networks, 1) LLMs are capable of processing and generating human-like responses.This capability allows them to understand and interpret complex patterns in data, making them suitable for tasks where traditional modeling techniques might struggle due to the intricacies in the data and complicated representations [36,161].2) Pre-trained LLMs can significantly reduce the computational load and time required compared to training high-fidelity models [56,70].</p>
<p>Problem</p>
<p>Algorithm Solution</p>
<p>LLM</p>
<p>Fitness / Category Solution Fig. 6.Large Language Models as Predictors (LLMaP).LLMs are utilized iteratively in algorithms to predict a solution's outcomes or responses, typically functioning in classification or regression tasks.</p>
<p>The majority of LLMaP works use LLMs as pre-trained models to predict solution scores.For instance, LLMs have been used as performance predictors for deep neural network architectures by Jawahar et al. [70].It offers a cost-effective alternative for performance estimation in neural architecture search.Zhang et al. [185] introduce LINVIT, an algorithm that incorporates guidance from LLMs as a regularization factor in value-based RL to improve sample efficiency.Science discovery is another domain that LLMaP has commonly investigated.For example, Li et al. [82] introduce CodonBERT for sequence optimization of mRNA-based vaccines and therapeutics.CodonBERT uses codons as inputs and is trained on over 10 million mRNA sequences from various organisms.Soares et al. [138] demonstrate the use of LLMs in predicting the performance of battery electrolytes.Other applications include employing LLMs to determine the fame score of celebrities to predict the box office performance of projects in the motion pictures industry [6] and adopting LLMs to score the video question answering by using detailed video captions as content proxies [182].</p>
<p>For classification, Hao et al. [56] introduce LAEA, which employs LLMs as surrogate models within evolutionary algorithms for both regression and classification, eliminating the need for costly model training.</p>
<p>In another study, Chen et al. [23] develope a label-free node classification method that leverages LLMs to annotate nodes.These annotations are subsequently used to train graph neural networks, resulting in enhanced performance.Moving beyond binary classification, Bhambri et al. [12] utilize LLMs to predict discrete actions for constructing reward shaping functions in Reinforcement Learning (RL).Their method demonstrate effectiveness within the BabyAI environment, showcasing the versatility of LLMs in various settings.Wang et al. [155] explore the use of LLMs in federated search, applying them in a zero-shot setting to effectively select resources.This approach highlights the potential of LLMs in improving resource selection without prior explicit training on specific tasks.Lastly, Mehrdad et al. [106] focus on automating the relevance judgment of query-item pairs in product searches.By finetuning LLMs, they have achieved significant improvements, underscoring the adaptability and effectiveness of LLMs in complex search tasks.Despite these advantages, using LLMs as predictors for algorithm design also suffers from a lack of interpretability, and the results can be influenced by the quality of prompt engineering [23].Moreover, when the target landscape is easily understood, conventional machine learning techniques and surrogate models are often preferred.In such scenarios, LLMs can enhance the utilization of existing modeling methods, for example, by aiding in the selection of the most effective model [127].</p>
<p>LLMs as Extractors (LLMaE)</p>
<p>LLMaE employs LLMs to mine and extract embedded features or specific knowledge from target problems and/or algorithms, which are then utilized in the enhancement of algorithm-based problem solving (Fig 7 ).</p>
<p>For example, Kristiadi et al. [76] use LLMs as pre-trained feature extractors and the embeddings are used to enhance standard Bayesian optimization surrogate models in the molecular space.</p>
<p>Problem</p>
<p>Algorithm Solution</p>
<p>LLM</p>
<p>Embedding Traditional techniques, which have previously struggled with semantic ambiguities, are outperformed by LLMs due to their superior ability to capture broader language contexts.</p>
<p>The multi-modal capabilities of LLMs also distinguish them from traditional methods.Park et al. [120] demonstrate that incorporating lexical information extracted from LLMs into an acoustic-based speaker diarization system through a probabilistic multi-modal decoding process and beam searches can significantly improve speech-processing performance.</p>
<p>LLMs as Designers (LLMaD)</p>
<p>LLMaD directly creates algorithms or specific components (Fig. 8).This utilization of LLMs extends their application beyond traditional boundaries, enabling them to actively participate in algorithm development by generating heuristics [88], writing code snippets [59], or formulating functions [128] that can be integrated into algorithmic systems.By doing so, LLMs can significantly accelerate the algorithm design process, reduce human effort, and bring creativity and optimization to algorithm development [88], which is difficult to achieve through traditional methods.</p>
<p>Problem Algorithm Solution</p>
<p>LLM</p>
<p>Algorithm Fig. 8. Large Language Models as Designers (LLMaD).LLMs are used to directly create algorithms or specific components, which are commonly incorporated iteratively to continuously search for better designs.</p>
<p>Function design is among the early applications of LLMaD.Eureka [100] leverages the capabilities of LLMs in code-writing, and in-context learning to evolve and optimize reward functions for RL.It can generate reward functions without specific prompts or predefined templates, achieving better performance than rewards designed by human experts.Similarly, Auto MC-Reward [80] utilizes LLMs to automatically design dense reward functions for RL agents in environments with sparse rewards.The three key components of Auto MC-Reward work together to iteratively refine the reward function based on feedback from the agent's interactions with the environment.Through this iterative process, the agent is able to learn complex tasks more efficiently, as demonstrated in experiments in Minecraft.Moreover, FunSearch [128] adopts LLMs for function generation in an evolutionary framework with a multi-island population management.It demonstrates promising results on both mathematical problems and combinatorial optimization problems.</p>
<p>EoH [88], originates from AEL [87], presents an early attempt to adopt the LLM as a designer for automated heuristic design.It uses both heuristic ideas and code implementations to represent heuristics and adopts LLM in an evolutionary framework to create, combine, and revise the heuristics.EoH is applied on well-studied combinatorial optimization problems, where it surpasses both traditional human-designed metaheuristics and deep-learning-based neural solvers.The application of EoH has expanded to Bayesian optimization [171],</p>
<p>image adversary attack [49], and edge server task scheduling [172], among others.Moreover, LLaMEA [150] develops an iterative framework to generate, mutate, and select algorithms based on performance metrics and runtime evaluations.The automatically designed algorithms outperform state-of-the-art optimization algorithms on some benchmark instances.ReEvo [176] introduces an evolutionary framework with both short and long-term reflections, which provides a search direction to explore the heuristic space.With the reflection, better results on black-box cases (no problem-specific information is provided in the prompts) have been observed.Unlike previous studies that focus on optimizing a single performance criterion, MEoH [170] considers multiple performance metrics, including optimality and efficiency, and seeks a set of trade-off algorithms in a single run in a multi-objective evolutionary framework.A dominance-dissimilarity score is designed for effectively searching the complex algorithm space.</p>
<p>LLM-based agent design has also gained much attention.For example, ADAS [63] proposes an automated design of agentic systems, which aims to automatically generate powerful agentic system designs by using meta agents that program new agents.They present a novel algorithm, meta agent search, which iteratively creates new agents from an archive of previous designs, demonstrating through experiments that these agents can outperform state-of-the-art hand-designed agents across various domains.Further studies on LLM-based agent systems are discussed in [139] and [94].</p>
<p>There has been a considerable effort on using LLMs for optimization modeling.Ahmed and Choudhury [3] assess various LLMs for transforming linguistic descriptions into mathematical optimization problems, introducing a fine-tuning framework, LM4OPT, to enhance the performance of smaller models.Tang et al.</p>
<p>[145] outline four key training dataset requirements for operational research LLMs and propose OR-Instruct, a semi-automated method for generating tailored synthetic data.Additionally, Mostajabdaveh et al. [111] present a novel multi-agent framework that translates natural language into optimization models, utilizing relational identifier agents and a verification mechanism for improved assessment.</p>
<p>We identify two primary challenges in existing works on LLMaD: 1) LLMs struggle with complex algorithm design tasks.Most existing studies focus on developing specific algorithmic components, such as key heuristics and code snippets, rather than complete algorithms [88].2) Algorithm design is domain-specific, and LLMs typically possess limited knowledge about specific algorithm design tasks.Consequently, standalone LLMs often fall short in terms of performance [183].The effectiveness of LLMaD depends on search frameworks that can iteratively interact with both LLMs and their environments.Details on the search methods are provided in the next section.</p>
<p>Search Methods</p>
<p>Incorporating LLMs within a search framework significantly enhances LLMs' utility in algorithm design, making it the preferred option [88,100,128].This section categorizes existing research according to the search methods employed, presents recent advancements, and discusses some of the limitations.</p>
<p>Sampling</p>
<p>The most straightforward search manner is by repeatedly instructing LLM to sample new designs [192].The best sample is selected as the final design.However, recent studies have shown that simple sampling from LLMs can be costly [183].</p>
<p>Beam Search.</p>
<p>Beam search explores multiple promising paths within a search space.For example, in the context of LLM-based prompt engineering [99,122,192], increasing the beam size has been shown to significantly improve performance.Beyond prompt engineering.Beam search is also employed in a variety of other applications.For instance, Text2Motion [84] uses beam search for robotic task planning.Similarly, SayCanPay [57] applies it to plan action sequences in robotics.</p>
<p>MCTS.</p>
<p>Monte Carlo Tree Search (MCTS) is a tree search algorithm that uses random sampling to build a search tree and make decisions based on the results of these samples.It is particularly effective in problems with large and complex search spaces.For example, Dainese et al. [29] propose GIF-MCTS, a code generation strategy using MCTS, in which nodes in the tree are programs and edges are actions.Each action taken from a parent node produces a new complete program.The upper confidence bound for trees is used to select which action to take.Similarly, VerMCTS [15] constructs a search tree with progressive widening to effectively manage large action spaces defined by lines of code.In this search tree, expansion and evaluation are guided by the verifier, serving as a computationally inexpensive upper bound on the value function.Moreover, Wang et al. [156] regard prompt optimization as a strategic planning challenge, utilizing a principled planning algorithm based on MCTS to effectively explore the expert-level prompt space.The proposed PromptAgent generates precise insights and detailed instructions by analyzing model errors and providing constructive feedback.</p>
<p>Single-point-based Search</p>
<p>Single-point-based search methods iteratively refine a solution by leveraging neighborhood structures [90] or specific search directions [122].While these methods can produce satisfactory results within a reasonable number of iterations, they may struggle with maintaining diversity and robustness in their search processes.</p>
<p>Hillclimb. Hillclimb search iteratively searches for better results</p>
<p>, where the new one is generated based on the last one and the better one is survived.A basic version of Hillclimb is investigated by Zhang et al. [183] for algorithm design, where a heuristic is iteratively refined.The reasoning over the existing status is usually adopted to enhance each search step.For example, Li et al. [80] leverage LLMs to automatically design dense reward functions for RL agents in environments with sparse rewards, which iteratively refines the reward function based on feedback from the agent's interactions with the environment.Yang et al.</p>
<p>[169] develop a multi-module framework with innovative feedback mechanisms, demonstrating through both LLM-based and expert evaluations that LLMs can effectively produce scientific hypotheses that are both novel and reflective of reality.</p>
<p>Neighborhood Search.</p>
<p>Neighborhood search methods explore the solution space by iteratively modifying a current solution to find an improved one in a structured neighborhood.Notably, the LLM-GS framework [90] has introduced a scheduled neighborhood search method that leverages two distinct strategies:</p>
<p>1) Programmatic Neighborhood Generation, which involves generating a neighborhood of programs by selecting a node from the abstract syntax tree of a given program and substituting it with a subtree that is randomly generated following the production rules and sampling strategies of a domain-specific language;</p>
<p>and 2) Latent Space Representation, where the neighborhood is defined in a latent space by training a variational autoencoder on a corpus of randomly generated DSL programs, thereby creating a more abstract and potentially more informative neighborhood structure.Additionally, Jin et al. [73] contribute to this line of research by defining neighborhood algorithms based on a minimum cosine similarity threshold of 60% between the embeddings of a query vector and code snippets, enhancing the search process by ensuring that algorithmic variations are semantically coherent and aligned with the underlying problem structure.</p>
<p>Gradient-based Search.</p>
<p>Gradient-based search utilizes a (pseudo) gradient direction to generate new designs in each iteration.Unlike searches in a continuous space with a differentiable objective, calculating the gradient in the algorithm design space poses significant challenges.Consequently, a pseudo descent direction is often employed.Pryzant et al. [122] introduce automatic prompt optimization, which improves prompts automatically by adjusting them based on natural language "gradients" derived from training data.</p>
<p>Similarly, Tang et al. [143] introduce GPO, which draws parallels with gradient-based model optimizers for prompt optimization.Moreover, Nie et al. [115] explore the use of LLMs as interactive optimizers for solving maximization problems in a text space through natural language and numerical feedback.For an accurate gradient, Guo et al. [51] perform the gradient-based search in a mapped continuous space and propose a collaborative optimization strategy that combines a gradient-based optimizer with an LLM for tackling complex non-convex optimization challenges.These works employ either pseudo gradients or gradients over mapped text space, however, a systematic study of gradients over general algorithm space is anticipated.</p>
<p>Reinforcement Learning.</p>
<p>Reinforcement learning learns to make decisions by interacting with an environment to maximize rewards.Zhuge et al. [194] and Duan et al. [35] leverage LLMs and RL techniques to optimize code, reducing complexity and enhancing efficiency.Similarly, Liu et al. [90] present a novel LLM-guided search framework called LLM-GS, which aims at addressing the sample inefficiency in stateof-the-art programmatic RL methods by utilizing the programming expertise of LLMs to boost search efficiency.Additionally, LLMs have been utilized in the design of RL systems, particularly in the creation of reward-shaping functions.Moreover, Bhambri et al. [12] use LLMs to generate a guide policy for constructing reward-shaping functions to tackle the issue of sample inefficiency.</p>
<p>Population-based Search</p>
<p>Population-based evolutionary search is the main tool investigated in LLM4AD papers due to its effectiveness and robustness in complex search space [117,163].The majority of these works use a simple genetic algorithm with greedy population management [88].Some of them investigate advanced population management including multi-island [128], quality-diversity [63], and dominance-dissimilarity measurement [170].</p>
<p>Single-objective Evolutionary</p>
<p>Search.Liu et al. [92] explore the use of LLMs as evolutionary operators in conventional EA, guiding them in solution selection, crossover, and mutation processes.Moreover, Brahmachary et al. [14] propose a population-based evolutionary framework comprising exploration and exploitation pools, with LLMs facilitating solution generation for both pools and enabling communication between them during optimization.Lange et al. [77] investigate the application of LLMs in designing evolution strategies, introducing a prompting strategy to enhance mean statistic performance.</p>
<p>On algorithm design tasks involving text and code search, the complicated search space poses challenges for diversity control and population management.The majority of works adopt a greedy way [20,49,59,80,88,112,150,171].In these works, a population of individuals is maintained and only the ones with better fitness will survive.Romera-Paredes et al. [128] adopt a multi-island evolutionary algorithm to explore a diverse population with a dynamically adjusted population size.Moreover, Wong et al. [160] use LLMs to create digital artifacts for creative and engineering applications, utilizing a multi-factorial evolutionary algorithm to drive an LLM.Quality diversity methods [123] have also been investigated for this purpose, Lehman et al. [78] and Hu et al. [63] utilize the MAP-Elites algorithm with niches to generate diverse and high-quality solutions.</p>
<p>Uncertainty-guided Search</p>
<p>Uncertainty-guided search combines Bayesian Optimal Experiment Design (BOED) or Bayesian Optimization (BO) with Large Language Models (LLMs).It starts with a prior based on initial parameter beliefs, then uses uncertainty-driven strategies to iteratively refine the beliefs, enhancing decision-making efficiency in a sample-efficient way.This method has shown effectiveness in various applications.For example, Handa et al. [54] develop OPEN, which uses LLMs to extract environmental features and translate feature queries into conversational natural language questions, employing BOED to select the most informative queries for learning user preferences efficiently.Austin et al. [8] introduce the PEBOL algorithm within a BO framework to improve multi-turn and decision-theoretic reasoning, using Beta distributions to model user preferences probabilistically.In LLM decoding, Grosse et al. [47] propose ULTS, which treats decoding as a tree-search problem and optimizes path selection through sequential decision-making under uncertainty.</p>
<p>Prompt Strategies</p>
<p>Prompt strategies are vital for effectively utilizing LLMs, particularly in algorithm design tasks that require instructions on reasoning and reflection on the target tasks.We will begin by providing an overview of LLMs and the prompt strategies employed in existing works, followed by a review to works using each strategy.papers involve many prompt engineering methods (as illustrated in Fig. 9b) including Zero-Shot (ZS), Few-Shot (FS), Chain-of-Thought (CoT), Self-Consistency (SC), and Reflection (RF) [130].</p>
<p>(a) LLMs
) H Z 6 K R W ) 6 = H U R 6 K R W = 6 &amp; K D L Q R I 7 K R X J K W &amp; R 7 5 H I O H F W L R Q 5 ) 6 H O I F R Q V L V W H Q F \ 6 &amp; 1XPEHURI3XEOLFDWLRQV(</p>
<p>Zero-Shot</p>
<p>Zero-Shot (ZS) prompting enables a model to comprehend and perform tasks without prior specific training on those tasks.In algorithm design, zero-shot prompting allows direct requests to the LLM for solutions or responses, examples including conversational replies [132], assertion design [109], RL guidance [185],</p>
<p>and molecule generation [173].The model leverages its pre-trained knowledge to generate these responses.</p>
<p>Although this method is advantageous for swiftly producing solutions based on general knowledge, it may not consistently deliver tailored responses to the nuanced demands of complex algorithmic challenges [183].</p>
<p>Few-Shot</p>
<p>Few-Shot (FS) prompting involves providing the model with a handful of examples to illustrate the task before it attempts to solve a similar problem.This approach enhances the model's understanding of the context and its ability to tailor responses to specific tasks.In algorithm design, this method proves particularly effective by presenting the model with examples of algorithms [87], solutions [167], prompts [143,156], and codes [101,110].The examples used in the prompt may be manually-designed seeds [63,128] or derived from LLM-generated designs [28,88,98,110,143,156,160].These samples are typically sorted to suggest a preference to enhance performance [128,167].</p>
<p>Chain-of-Thought</p>
<p>Chain-of-Thought (CoT) prompting encourages the model to articulate intermediate steps or reasoning paths that lead to the final answer.This technique is particularly valuable in algorithm design, where understanding the step-by-step process or engaging in instructed reasoning over existing designs helps prevent outlier results and fosters effective algorithm development.For instance, Liu et al. [88] introduce several prompting strategies, one of which directs the LLM to reason over existing heuristics, summarize the general pipeline, and design a new one based on this reasoning.Custode et al. [28] prompt the LLM to evaluate whether the current step size is sufficient for convergence and to suggest adjustments based on its reasoning.In addition, multiple LLM-based agents are employed within a CoT framework by Sun et al. [140] to enhance heuristic design for a boolean satisfiability problem solver.</p>
<p>Self-consistency</p>
<p>Self-Consistency (SC) involves generating multiple answers or solutions to the same prompt and then synthesizing them to improve accuracy and reliability.In algorithm design, this could mean prompting the model multiple times for different approaches to solve a problem and then comparing these solutions to identify the most efficient or robust algorithm.This approach leverages the model's ability to generate diverse solutions and builds a more comprehensive understanding of possible strategies.For example, Guan et al. [48] set the number of responses as 10 and uses the majority vote to get the predicted label.As each response may provide different keywords or regular expressions, it takes the union of the keywords or regular expressions to create a candidate set.Moreover, Lehman et al. [79] sample multiple revised codes from the same selected code and updates the population accordingly.</p>
<p>Reflection</p>
<p>Reflection (RF) in prompt engineering involves asking the model to critique or evaluate its own responses or solutions.After generating an algorithm/solution, the model can be prompted to reflect on the efficiency, potential flaws, or improvements.Ma et al. [99] investigate both implicit and explicit reflection in LLM-driven prompt optimization, which lets LLM analyze the errors and generate a reflection or feedback regarding the current prompt.An additional study by Ye et al. [175] incorporates short-term and long-term reflections in heuristic design, while Zhang et al. [179] focus on self-evaluation and reasoning over results from network detection.Furthermore, both Ma et al. [100] and Narin [112] adopt LLM for RL reward function design with a reward reflection that monitors the scalar values of all reward components and the task fitness function at various policy checkpoints during training.</p>
<p>Applications</p>
<p>Optimization</p>
<p>In this subsection, we delve into the practical applications of LLMs in algorithm design for optimization.</p>
<p>We categorize the existing literature into combinatorial optimization, continuous optimization, Bayesian optimization, prompt optimization, and optimization modeling based on the specific domains of optimization applications.Then we proceed to compare the various roles played by LLMs, the prompt strategies employed, and the specific problems or tasks to which they are applied.The comparative analysis is summarized in Table 1, where we list the names of the frameworks or methods proposed by the authors.For studies that do not explicitly name their methods, we assign appropriate designations in our article and denote them with asterisks for easy reference (e.g., MH-LLM* for [131]).</p>
<p>Combinatorial Optimization.</p>
<p>In the domain of Combinatorial Optimization (CO), automated algorithm heuristics design has been a significant area of interest for a long time.The Traveling Salesman Problem (TSP) stands out as one of the most renowned CO problems, involving the quest for the shortest route to visit all specified locations exactly once and return to the starting point.Some recent work leverages LLMs to evolve algorithms within Evolutionary Computation (EC) framework, such as AEL [87], ReEvo [176] and EoH [88].Differently, OPRO [167] employs LLMs as optimizers with a proposed meta-prompt, in which the solution-score pairs with task descriptions are added in each optimization step.Additionally, LMEA [91] investigates the utilization of LLMs as evolutionary combinatorial optimizers for generating offspring solutions, wherein a self-adaptation mechanism is introduced to balance exploration and exploitation.The Capacitated Vehicle Routing Problem (CVRP) extends the TSP by introducing constraints related to vehicle capacity.To address this challenge, MLLM [68] devises a multi-modal LLM-based framework with textual and visual inputs to enhance optimization performance.In addition to routing problems, other combinatorial optimization problems that have also been investigated include cap set [128], bin packing [88], flow job shop scheduling [88] and social networks problems [131].</p>
<p>Continuous Optimization.</p>
<p>In the realm of single-objective continuous optimization, LLaMEA [150] utilizes LLMs to automate the evolution of algorithm design.It demonstrates the effectiveness in generating new metaphor-based optimization algorithms on BBOB benchmark [55] within IOHexperimenter benchmarking tool [30], which supports evaluating the quality of the generated algorithms and also provides feedback to the LLM during evolution.Instead of creating new algorithms, EvolLLM [77] introduces a prompt strategy that enables LLM-based optimization to act as an Evolution Strategy (ES) and showcases robust performance on synthetic BBOB functions and neuroevolution tasks.OPRO [167] illustrates that LLMs can effectively capture optimization directions for linear regression problems by leveraging the past optimization trajectory from the meta-prompt.Additionally, LEO [14] devises an explore-exploit policy using LLMs for solution generation, the method has been tested in both benchmark functions as well as industrial engineering problems.Different with directly employing LLMs for generating solutions, LAEA [56] introduces LLM-based surrogate models for both regression and classification tasks and has been validated on 2D test functions using nine mainstream LLMs.</p>
<p>Multi-objective optimization problems (MOPs) seek to identify a set of optimal solutions, referred to as Pareto optimal solution set.An initial exploration of utilizing LLMs to tackle MOPs is introduced in MOEA/D-LMO [86].Benefiting from the decomposition-based framework, the in-context learning process of LLMs is easily incorporated to generate candidate solutions for each subproblem derived from the original MOP.In the realm of large-scale MOPs, LLM-MOEA* [136] showcases the inferential capabilities of LLMs in multi-objective sustainable infrastructure planning problem.The study highlights the LLM's proficiency in filtering crucial decision variables, automatically analyzing the Pareto front, and providing customized inferences based on varying levels of expertise.Additionally, CMOEA-LLM [158] leverages LLMs with evolutionary search operators to address the challenges of constrained MOPs and exhibits robust competitiveness in DAS-CMOP test suite [39].</p>
<p>Bayesian Optimization. Bayesian optimization (BO) is a model-based optimization paradigm for</p>
<p>solving expensive optimization problems and has found wide applications in various real-world scenarios [44].It typically employs a surrogate model to approximate the expensive function and well-designed Acquisition Functions (AFs) to select potential solutions in a sample-efficient manner.To facilitate the direct generation of solutions using LLMs, HPO-LLM<em> [181] provides LLMs with an initial set of instructions that outlines the specific dataset, model, and hyperparameters to propose recommended hyperparameters for evaluation in Hyperparameter Optimization (HPO) tasks.Furthermore, LLAMBO [93] incorporates LLM capabilities to enhance BO efficiency, in which three specific enhancements throughout the BO pipeline have been systematically investigated on tasks selected from Bayesmark [148] and HPOBench [37].Instead of utilizing LLMs for direct solution generation, BO-LIFT</em> [125] utilizes predictions with uncertainties provided by a Language-Interfaced Fine-Tuning (LIFT) framework [31] with LLMs to perform BO for catalyst optimization using natural language.EvolCAF [171] introduces a novel paradigm integrating LLMs within EC framework to design AFs automatically for cost-aware BO, the approach showcases remarkable efficiency and generalization for synthetic functions and HPO tasks with heterogeneous costs.Similarly, FunBO [1] discoveres novel and well-performing AFs for BO by extending FunSearch [128], the discovered AFs are evaluated on various global optimization benchmarks in and out of the training distribution and HPO tasks for RBF-based SVM and AdaBoost algorithms.</p>
<p>Prompt Optimization.</p>
<p>Prompt optimization aims to identify the most effective task prompt that maximizes the performance of the LLM on a specific task dataset.Despite of requiring specialized training for each specific task, traditional discrete or continuous approaches [83,121] typically necessitate access to the logits or internal states of LLMs, which may not be applicable when the LLM can only be accessed through an API.To address these issues, recent works propose to model the optimization problem in natural language with LLMs as prompts.APE [192] utilizes the LLM as an inference model to generate instruction candidates directly based on a small set of demonstrations in the form of input-output pairs.This approach has demonstrated human-level performance on various tasks, including Instruction Induction [60] and Big-Bench Hard (BBH) [141].OPRO [167] enables the LLM as an optimizer to gradually generate new prompts based on the full optimization trajectory, the optimizer prompt showcases significant improvement compared with human-designed prompts on BBH and GSM8K [25].Inspired by the numerical gradient descent method, APO [122] conducts textual "gradient descent" by identifying the current prompts' flaws and adjusting the prompt in the opposite semantic direction of the gradient.Similar practices are also found in the gradient-inspired LLM-based prompt optimizer named GPO [143], as well as the collaborative optimization approach [52] integrating a gradient-based optimizer and an LLM-based optimizer.Differently, Guo et al. [50] introduce a discrete prompt tuning framework named EvoPrompt that prompts LLM to act like evolutionary operators in generating new candidate prompts, harnessing the benefits of evolutionary algorithms that strike a good balance between exploration and exploitation.StrategyLLM [45] integrates four LLM-based agents-strategy generator, executor, optimizer, and evaluator-to collaboratively induce and deduce reasoning.This method generates more generalizable and consistent few-shot prompts than CoT prompting techniques.</p>
<p>Optimization Modeling.</p>
<p>Optimization modeling [11] aims to construct a mathematical model of a real-world optimization problem in a standard form, represented with an objective function and a set of constraints [2,145].Recently, LLMs have opened up promising avenues for automating optimization modeling, facilitating the formulation of problems and the implementation of solutions, thereby democratizing expertise in optimization skills and domain knowledge in various applications.For example, OptiMUS [2] is developed as an LLM-based agent to formulate and solve optimization problems interpreted from natural language.Despite its outstanding performance on the proposed NLP4LP dataset [2], the heavy reliance on sensitive data submission to proprietary LLMs can pose data privacy concerns in industrial applications.To solve this issue, Tang et al. [145] propose training open-source LLMs with OR-Instruct, a semi-automated process for generating synthetic data customized to meet specific requirements.The best-performing resulting model named ORLM demonstrates state-of-the-art performance on NL4Opt [124],</p>
<p>MAMO [67], and the proposed IndustryOR benchmark.A related study is found in LM4OPT [3], which is a progressive fine-tuning framework to enhance LLMs' specificity for optimization problem formulation task.To make it accessible to decision makers lacking problem-related modeling skills, DOCP [159] aids in decision-making by interacting with the user in natural language, and utilizes human's knowledge and feedback to create and solve a problem-specific optimization model.In addition to the previously mentioned optimization applications, we have identified further uses of LLMs in related fields.For example, AS-LLM [164] utilizes LLMs to extract and select key algorithm features for algorithm selection, enhancing the match between algorithms and problems.</p>
<p>LLaMoCo [101] introduces the first instruction-tuning framework for LLMs, optimizing code generation with a structured instruction set and a two-phase training approach, thereby minimizing the need for extensive domain expertise and prompt design effort.</p>
<p>Machine Learning</p>
<p>In this subsection, we investigate the applications of LLMs in the machine learning domain, focusing on their contribution to algorithmic design.These applications are summarized in Table 2.</p>
<p>6.2.1</p>
<p>Task Planning with LLMs.The advent of LLMs has garnered significant interest in their application to planning tasks, owing to their remarkable generative capabilities of instructions in the form of natural language.Pioneering work by Huang et al. [65] demonstrates the effectiveness of LLMs in leveraging world knowledge to produce actionable plans for VirtualHome environments.This method was further enhanced by Ahn et al. [5], who employed a value function to ground the world knowledge, enabling the derivation of an optimal policy for robotic task execution in lifelike kitchen settings.In subsequent research, Huang et al.</p>
<p>[66] broaden the scope grounded information in a more adaptive way.Moreover, Lin et al. [84] utilize greedy search in combination with heuristic algorithms to refine the planning system.Distinctly, Singh et al. [137] tackle planning challenges by integrating programming languages, thereby generating code to streamline planning processes.A more comprehensive approach, named SayCanPay [57], is developed for planning by considering both the affordance function and reward within the task framework.</p>
<p>Reinforcement</p>
<p>Learning.Reinforcement Learning (RL) has been the de facto standard for sequential decision-making tasks, and recently, the synergy between RL and LLMs has emerged as a novel trend in the domain.This convergence mirrors the dynamics of task planning, yet places RL at the core of its methodology.</p>
<p>Many of the LLM4AD papers on RL is for automatically designing the reward functions [12,100,112].In addition, Shah et al. [133] investigate the employment of LLMs for heuristic planning to steer the search process within RL frameworks.Zhang et al. [185] integrate LLMs into RL by introducing a Kullback-Leibler divergence regularization term that aligns LLM-driven policies with RL-derived policies.LLMs have also extended their reach to multi-agent RL scenarios, as shown by Du et al. [32], who illustrates their application within a Mixture-of-Experts system to direct RL models in the realm of intelligent network solutions.</p>
<p>Neural Architecture Search.</p>
<p>Neural Architecture Search (NAS), which is a significant focus within the AutoML community, has been investigated in many LLM4AD papers.For example, Chen et al. [20] have integrated LLMs with evolutionary search to successfully generate NAS code for diverse tasks.Nasir et al.</p>
<p>[113] introduce a quality-diversity algorithm tailored to NAS, producing architectures for CIFAR-10 and NAS-bench-201 benchmarks.Moreover, Morris et al. [110] introduce guided evolution for the development of neural architectures and suggest the concept of the evolution of thought in algorithm design.Except for using LLM for design, Jawahar et al. [70] employ LLMs in predicting NAS performance, combining this approach with evolutionary search to effectively create novel network architectures.In contrast to the LLM-based architecture design and performance prediction, Zhou et al. [191] explore the adoption of LLMs for transferring design principles to narrow and guide the search space.</p>
<p>Graph Learning.</p>
<p>Graph learning is another application with the advancing capabilities of LLMs in symbolic reasoning and graph processing.For example, Chen et al. [23] apply LLMs to the task of labeling in Text-Attributed Graphs (TAGs), capitalizing on the language task proficiency of LLMs.Both Mao et al.</p>
<p>[104] and Chen et al. [21] adopt LLMs in an evolutionary framework for designing functions.The former evolves heuristic code functions to identify critical nodes in a graph while the latter identifies meta-structures within heterogeneous information networks to enhance their interpretability.Moreover, knowledge graphs have also seen substantial benefits from the application of LLMs.Zhang et al. [184] introduce AutoAlign, a method that employs LLMs to semantically align entities across different knowledge graphs, and Feng et al.</p>
<p>[41] develop the knowledge search language to effectively conduct searches within knowledge graphs.</p>
<p>6.2.5 Dataset Labeling.LLMs have been used for mining semantic and multi-modal information from datasets.LLMs are employed to train interpretable classifiers to extract attributes from images [24] and to generate label functions for weakly supervised learning [48].</p>
<p>Science Discovery</p>
<p>This subsection is dedicated to exploring LLM-based scientific discoveries related to algorithm designs.In the field of chemistry, LLMs can be applied not only to conventional molecular generation and design [13,69], but also to specialized areas such as drug molecule design [173],</p>
<p>chemical reaction prediction and optimization [126], and catalyst design [153], providing customized solutions.</p>
<p>Furthermore, LLMs have also shown promising application prospects in materials discovery [71,180], synthesis route planning [89], green chemistry [129], and other areas.These studies demonstrate the advantages of LLMs in molecular representation and optimization.</p>
<p>In biology, LLMs are increasingly being used for tasks such as protein engineering [97,134], enzyme design [146], and biological sequence analysis [42].By combining LLMs with vast amounts of biological data, they can more accurately predict interactions between biological molecules and significantly improve the efficiency of bioinformatics workflows.This has important implications for drug discovery and therapeutic protein design.The unique sequence generation and optimization capabilities of LLMs offer new possibilities for tackling combinatorial optimization challenges in biomacromolecular design.</p>
<p>Although applications in physics are relatively fewer, some emerging work has demonstrated the broad prospects of LLMs.Pan et al. [119] use multi-step prompt templates to prove that LLMs can perform complex analytical calculations in theoretical physics.Quantum computing algorithm design, physics simulation optimization, and computational methods in condensed matter.</p>
<p>Mechanics.</p>
<p>MechAgents [114] introduces a class of physics-inspired generative machine learning platforms that utilize multiple LLMs to solve mechanics problems, such as elasticity, by autonomously collaborating to write, execute, and self-correct code using finite element methods.Moreover, Du et al. [33] adopt LLMs in automatically discovering governing equations from data, utilizing the models' generation and reasoning capabilities to iteratively refine candidate equations.This approach, tested on various nonlinear systems including the Burgers, Chafee-Infante, and Navier-Stokes equations, demonstrates a superior ability to uncover correct physical laws and shows better generalization on unseen data compared to other models.</p>
<p>Another example in fluid dynamics has been investigated by Zhu et al. [193], which introduces FLUID-LLM, a novel framework that integrates pre-trained LLMs to enhance the prediction of unsteady fluid dynamics.</p>
<p>AutoTurb [186], on the other hand, adopts LLMs in an evolutionary framework to automatically design and search for turbulence model in computational fluid dynamics.Furthermore, Buehler [17] study LLM-based methods for forward and inverse mechanics problems including bio-inspired hierarchical honeycomb design, carbon nanotube mechanics, and protein unfolding.</p>
<p>Industry</p>
<p>This subsection explores the transformative impact of LLMs on algorithm design across various industries.</p>
<p>As we envision the future of manufacturing, LLMs are poised to play a pivotal role.Existing developments focus on enhancing research and investment to refine these models while assessing their performance in terms of safety, bias, and utility.LLMs can be seen as a gateway between humans and machines, facilitating informed decision-making.</p>
<p>For instance, LLMs are instrumental in building a comprehensive intelligent 6G network system [95],</p>
<p>addressing challenges such as low latency, high-frequency bands, high bandwidth, high transmission rates, and overall intelligence [190].Moreover, LLMs have the potential to revolutionize the telecom industry by streamlining tasks and reducing the need for extensive manpower and engineering expertise.However, it is crucial to address existing challenges to fully realize their potential [102].</p>
<p>Recently, the application of LLMs in Electronic Design Automation (EDA) has emerged as a promising area of exploration.LLM-based solutions have been developed to enhance interactions with EDA tools [40].</p>
<p>In particular, within the VLSI design flow, it is essential to verify that the implementation conforms to its specifications.To achieve this, system verilog assertions need to be generated automatically.Generative AI techniques, such as LLMs, can be effectively utilized to facilitate this process [109].</p>
<p>To enhance the reliability and availability of cloud services, conducting root cause analysis (RCA) for cloud incidents is essential.RCACopilot [22] effectively matches incoming incidents with the appropriate incident handlers based on their alert types.It aggregates critical runtime diagnostic information, predicts the incident's root cause category, and provides explanatory results without the need for manual investigations.</p>
<p>More applicable scenarios include various industrial design challenges, such as chip design [18], car shape design [161], aircraft concept design [96] UI design [43], and robot design [177].LLMs primarily leverage billions of web resources.However, practical applications require fine-tuning with thousands of specific representations of design intent to recognize the distinct patterns associated with each design task.Other traditional industrial tasks, such as itinerary planning, can also be enhanced by leveraging the power of LLMs [166].LLM-based open-domain urban itinerary planning [144] combines spatial optimization with LLMs to create detailed urban itineraries tailored to users' requests.</p>
<p>Challenges of LLMs in Algorithm Design</p>
<p>Scalability</p>
<p>One of the primary limitations of LLMs in algorithm design is their scalability.LLMs are limited by a fixed context size, restricting the amount of information they can process at once, which is a challenge for complex algorithmic tasks requiring detailed specifications and extensive content.Additionally, even when input and output lengths are sufficient, LLMs struggle with comprehension and reasoning over long inputs, impacting their effectiveness in designing complex algorithms.For instance, LLMs tend to solve only low-dimensional problems when used as optimizers [167], and they are employed to design a function or heuristic component of an algorithm rather than the entire algorithm when used as designers [88].</p>
<p>Interpretability</p>
<p>Interpretability is another challenge when using pre-trained LLMs for algorithm design.It is difficult to trace how specific inputs lead to particular outputs in LLMs due to their black-box nature.This opacity can be problematic, especially in critical applications, e.g., some industry applications, where understanding the decision-making process is essential for trust and reliability.Efforts to enhance the interpretability involve techniques such as feature visualization, model simplification [86], and the study of the contribution of different parts of the data to the model's behavior [7].Despite these efforts, achieving principled interpretability without compromising the performance of LLMs remains a key area of ongoing research.</p>
<p>Security</p>
<p>Integrating LLMs into algorithm design introduces significant security risks, including the mishandling of personal data and the potential creation of harmful code.LLMs are trained on extensive datasets that often contain sensitive information, posing a risk of privacy breaches if these models inadvertently generate outputs influenced by this data.Additionally, the autonomous nature of LLMs can lead to the unintentional development of flawed or malicious algorithms, which is a critical concern in sectors like financial services and personal data management [187].To address these issues, it is vital to establish stringent data governance and model training protocols that prevent the inclusion of sensitive data, rigorously test algorithms under various conditions to prevent unintended behaviors, and continuously update security measures to counteract emerging vulnerabilities.</p>
<p>Cost</p>
<p>The utilization of LLMs in algorithm design also faces challenges related to high cost in model training and usage.Training domain LLMs requires significant computational resources, often involving extensive GPU or TPU usage over several weeks or even months, which incurs high financial and time costs.Once trained, the inference cost-the computational expense involved in applying the model to algorithm design-also becomes a critical factor, especially when the model is used frequently, such as the evolutionary search used in many LLM4AD works [128].For practical applications in algorithm design, where multiple iterations and tests might be necessary, these costs can accumulate, making the use of LLMs potentially less viable for researchers without substantial computational budgets.</p>
<p>Idea Innovation</p>
<p>Despite extensive research on LLM4AD, their capacity for generating novel ideas remains questionable.</p>
<p>LLMs are adept at interpolating within their training data but often falter in extrapolating to new scenarios.</p>
<p>In algorithm design, this means LLMs can suggest or slightly enhance existing algorithms but struggle to develop radically new methods.Although there is evidence of LLMs producing new ideas in certain tasks [171], consistently generating practical, innovative concepts continues to be a challenge [63].</p>
<p>8 Future Directions</p>
<p>Domain LLM for Algorithm Design</p>
<p>Instead of using a general pre-trained general-purpose LLM, it is worthwhile studying how to train an LLM specifically for automatic algorithm design tasks.The following aspects can be explored in developing domain LLMs: 1) Training domain LLMs are costly and resource-consuming.On the one hand, advanced light fine-tuning techniques (e.g., LoRA [61]) could be used for efficient domain adaptation.On the other hand, the size of algorithm LLMs for a specific application can be reduced with the help of domain data and knowledge.2) Generating and collecting domain data for algorithm design poses challenges.Unlike general code generation or linguistic process tasks, there is no large and formatted data specific for algorithm design.</p>
<p>Some Github repositories on algorithms might be helpful, but the resources should be scanned and cleaned for algorithm design.Lehman et al. [79] provide an example of generation domain data for LLMs, where the samples generated during searching are used as the training dataset to fine-tune a pre-trained model resulting in better performance on the target problem.3) Instead of learning a text and code generation model, how to learn the algorithm development ideas and the algorithmic reasoning ability remain unexplored [116,157].</p>
<p>Multi-modal LLM for Algorithm Design</p>
<p>The primary focus of existing LLM4AD works is on utilizing the text comprehension and generation capabilities of LLMs, whether in language, code, or statistics.One advantage of LLMs compared to traditional model-based optimization is their ability to process multi-modal information like humans, which is rarely studied.Some attempts have been conducted to showcase the advantages of incorporating multimodal information in algorithm design, as evidenced by Huang et al. [68] and Narin [112].It is anticipated that more methods and applications utilizing multi-modal LLMs will be developed in the future.</p>
<p>Interaction with Human Experts</p>
<p>Further research is needed to explore the interaction between LLMs and human experts in algorithm design [81].For example, in LLMaD works, LLMs can be seen as intelligent agents, making it feasible for human experts to step in and take over tasks such as generating, modifying, and evaluating algorithms.</p>
<p>Investigating how to facilitate efficient and productive collaboration between LLMs and human experts would be valuable.Ideas and techniques in collective intelligence [103] can be used for this purpose.</p>
<p>Multi-objective Algorithm Design</p>
<p>There are usually multiple criteria (e.g., optimal solution generated by algorithm, optimization efficiency, and generalization performance) in real-world algorithm design.The majority of existing LLM4AD papers consider one objective [88,128] focusing only the optimal solutions.A preliminary attempt has been conducted by Yao et al. [170], where a dominance-dissimilarity measurement, considering both the dominance relation in objective space and the distance in code space, is designed for efficient multi-objective search in the heuristics space.More future works are anticipated to develop new methods for effective multi-objective algorithm design.</p>
<p>LLM-based Algorithm Assessment</p>
<p>LLMs can be benefitial in algorithm assessment.Some attempts have been carried out on test instance generation and algorithm evaluation.For example, Jorgensen et al. [74] use LLMs to generate test cases of increasing difficulties to enhance genetic programming and OMNI-EPIC [38] utilizes LLMs to automatically produce code that defines the next engaging and learnable tasks for algorithm evaluation.Further research in this area is anticipated.</p>
<p>Understand the Behavior of LLM</p>
<p>In the majority of works, LLM works as a black-box model.The interpretation of LLM's behavior not only enriches our understanding of LLM's behavior but also benefits cases where it is challenging or costly to directly request LLMs.Some attempts have been made to approximate and understand the in-context learning behavior of LLM in solution generation.For example, Liu et al. [86] has designed a white-box linear operator to approximate the results of LLM in multi-objective evolutionary optimization.In spite of these preliminary attempts, how to interpret LLM's behavior is still an open question in many algorithm design cases including heuristic generation and idea exploration.</p>
<p>Fully Automatic Algorithm Design</p>
<p>Fully automatic algorithm design faces two primary challenges: 1) the generation of novel algorithmic ideas and 2) the creation of complex, lengthy code.While the generation of new ideas has been investigated in some works [88], the complete design of algorithms (instead of a heuristic component), remains a challenge.</p>
<p>Existing applications typically focus on automating components within predefined algorithm frameworks rather than creating new algorithms from scratch.Future research needs to tackle these complexities to advance the field of fully automatic algorithm design.</p>
<p>Benchmarking LLM4AD</p>
<p>Benchmarking allows for a fair, standardized, and convenient comparison, which can identify best practices and enable generating new insights for innovation.While we are glad to witness the emergence of diverse research works and applications, LLM4AD still lacks a benchmarking on either algorithm design test sets for systematic and scientific assessment or a strict pipeline and evaluation criteria on large language models for algorithm design.Several attempts have been made on related topics such as mathematical reasoning [85] and planning [149].[151] presents an algorithm test set built on some basic algorithms and [105] demonstrates the superior performance of LLM when compared to neural algorithm reasoners.In the future, more benchmarks are expected and they are going to play a crucial role in advancing LLM4AD.</p>
<p>Conclusion</p>
<p>This paper has provided a systematic and up-to-date survey on large language models for algorithm design (LLM4AD).By systematically reviewing a significant corpus of main contributions in this emerging research field, this paper not only highlights the current state and evolution of LLM applications in algorithm design but also introduces a multi-dimensional taxonomy with comprehensive lists of papers that categorize the roles and functionalities of LLMs in this domain.This taxonomy serves as a framework for both academic and industrial researchers to understand and navigate the complex landscape of algorithm design using LLMs.We have also identified the limitations and challenges currently faced by the field, such as issues of scalability, interpretability, and security.Moreover, we have highlighted and discussed some promising research directions to inspire and guide future studies.</p>
<p>As we look forward, the intersection of LLMs and algorithm design holds promising potential for revolutionizing how algorithms are developed and implemented.We hope this paper can contribute to a deeper understanding of this potential and set the stage for future innovations and collaborations in this emerging avenue of research.</p>
<p>Fig. 1 .
1
Fig. 1.Four stages for paper collection.</p>
<p>Fig. 3 .Fig. 4 .
34
Fig. 3.The word cloud is generated from the titles and abstracts of all reviewed papers, with each word appearing at least five times.It features the top 80 keywords, organized into four color-coded clusters.</p>
<p>Fig. 7 .
7
Fig. 7. Large Language Models as Extractors (LLMaE).LLMs are employed to extract features or specific knowledge from target problem and/or algorithms to enhance problem-solving.</p>
<ol>
<li>3 . 2
32
Multi-objective Evolutionary Search.Liu et al.[86] propose MOEA/D-LMO to use LLM to solve continuous multi-objective optimization problems.Benefiting from the decomposition-based framework, the in-context learning of LLM is used to generate new solutions for each subproblem.For multi-objective LLM instruction generation, Yang and Li[168] develop InstOptima, which utilizes an LLM to simulate instruction operators and improve instruction quality to simultaneously optimize performance, length, and perplexity of the instruction.Moreover, Morris et al.[110] introduce guided evolution for neural architecture design.It adopts NSGA-II and optimizes both accuracy and model size.In order to enhance the diversity control in multiobjective heuristic design, MEoH[170] introduce a dominance-dissimilarity score for multi-objective heuristic design, achieving a good balance of exploration and exploitation in the heuristic search space.</li>
</ol>
<p>Fig. 9a depicts</p>
<p>Fig.9adepicts the percentage of domain or pre-trained LLMs used in the literature.Among them, over 80% choose to use the pre-trained model without any specific finetuning, and about 10% fine-tuned the pre-trained model on domain datasets[27] in which 4.4% are trained from scratch.LLM4AD papers show a strong preference for GPT models.GPT-4 and GPT-3.5 are the most commonly utilized LLMs, collectively accounting for approximately 50%.Llama-2 is the most used open-source LLM.Once we have the pre-trained LLMs, prompt engineering is significant for effectively integrating LLMs into algorithm design.LLM4AD</p>
<p>Fig. 9. LLM types and prompt engineering methods</p>
<p>Table 1 .
1
An Overview of Optimization Applications Utilizing Language Models Across Various Domains and Tasks.
ApplicationMethodRole of LLM Prompt StrategySpecific Problems or TasksAEL [87]LLMaDFS, CoTTSPReEvo [176]LLMaDCoT, RFTSPCombinatorial OptimizationOPRO [167] LMEA [91]LLMaO MixedFS FSTSP TSPMLLM [68]MixedZS, FSCVRPFunSearch [128]LLMaDFSCap Set Problem, Online BPPEoH [88]LLMaDFS, OS, CoTTSP, Online BPP, FSSPMH-LLM<em> [131]LLMaDZS, FSSocial Networks ProblemLLaMEA [150]LLMaDCoTBBOBEvoLLM [77]LLMaOFSBBOB, NeuroevolutionContinuous OptimizationOPRO [167] LEO [14]LLMaO LLMaOFS FSLinear Regression Numerical Benchmarks, Industrial Engineering ProblemsLAEA [56]LLMaPFS, CoTEllipsoid, Rosenbrock, Ackley, GriewankMOEA/D-LMO [86]LLMaOFSZDT, UFLLM-MOEA</em> [136]LLMaEZSMulti-objective Sustainable Infrastructure Planning ProblemCMOEA-LLM [158]LLMaOFSDAS-CMOPHPO-LLM<em> [181]LLMaOFSHPOBenchBayesian OptimizationLLAMBO [93]MixedFS, ZS, CoTBayesmark, HPOBenchBO-LIFT</em> [125]LLMaDFSCatalyst OptimizationEvolCAF [171]LLMaDFS, OS, CoTSynthetic Functions, HPOFunBO [1]LLMaDFSSynthetic Functions, HPOAPE [192]LLMaOFSInstruction Induction, BBHOPRO [167]LLMaOFSGSM8K, BBHPrompt OptimizationAPO [122]LLMaOZS, FSNLP Benchmark Classification TasksGPO [143]LLMaOFSReasoning, Knowledge-intensive, NLP TasksMaaO [52]LLMaOFSNLU Tasks, Image Classification TasksEvoPrompt [50]LLMaOCoT, FSLanguage Understanding and Generation Tasks, BBHStrategyLLM [45]MixedZS, FS, CoTReasoning TasksDOCP [159]LLMaOFS, CoTFacility Location ProblemsOptimization ModelingOptiMUS [2]MixedZS, FSNL4LPORLMs [145]LLMaDZSNL4Opt, MAMO, IndustryORLM4OPT [3]LLMaDFS, ZSNL4OptAS-LLM [164]LLMaEZSAlgorithm SelectionOther ApplicationsLLaMoCo [101]LLMaDFSOptimization Code Generation6.1.6 Other Applications.</p>
<p>Table 2 .
2
[49]]erview of Machine Learning Applications Utilizing Language Models Across Various Domains and Tasks.Other applications of LLMs extend to a myriad of machine learning tasks.Notably, Wong et al.[160]capitalize on multi-task learning and the iterative refinement of prompts to foster innovative design approaches.Zeng et al.[178]integrate LLMs with evolutionary search to develop a heuristic function aimed at efficiently sifting through candidate tensor sets representing the primal tensor network.Furthermore, Guo et al.[49]have blazed a trail in employing LLMs to generate novel decision-based adversarial attack algorithms, thus opening up a new diagram for the automatic assessment of model robustness.
ApplicationMethodRole of LLM Prompt StrategySpecific Problems or TasksZero-Planner [65]LLMaPZS, FSVirtualHome TasksSayCan [5]LLMaPFSReal-world Kitchen Tasks with RobotsTask PlanningLLM-GM [66]LLMaPFSReal-world Kitchen Tasks with RobotsText2Motion [84]LLMaPFSLong Sequence Tasks for RobotsProgPrompt [137]LLMaDFS, COTVirtualHome TasksSayCanPay [57]LLMaPFSVirtualHome Tasks, Ravens Tasks, BabyAI TasksLFG [133]LLMaPFS, CoTObjectNav Tasks, Real-world TasksSLINVIT [185]LLMaPZS, FSALFWorld Tasks, InterCode Tasks, BlocksWorld TasksReinforcement LearningMEDIC [12]LLMaPZSBabyAI TasksEureka [100]LLMaDFS, CoTIsaacGym Tasks, Bidexterous Manipulation TasksEROM [112]LLMaDZS, CoTIsaacGym TasksLLM-MOE [32]LLMaPFS, CoTIntelligent NetworksEvoPrompting [19]LLMaDFS, ZS, CoTMNIST Dataset, CLRS Algorithmic ReasoningNeural Architecture SearchHS-NAS [70]LLMaPFS, ZS, CoTMachine Translation TasksLLMatic [113]LLMaDFS, ZS, CoTCIFAR-10 Dataset, NAS-bench-201 BenchmarksLAPT [191]LLMaDZS, FSNAS201, Trans101, DARTsLLM-GE [110]LLMaDFS, ZS, CoTCIFAR-10 DatasetLLM-Critical [104]LLMaDFS, ZS, CoTCritical Node IdentificationGraph LearningLLM-GNN [23]LLMaPFS, CoTLabel-free Node ClassificationReStruct [21]LLMaEFS, ZSMeta-structure DiscoveryAutoAlign [184]LLMaEFS, ZSEntity Type InferenceKSL [41]LLMaPFSKnowledge SearchDataset LabelingInter-Classier [24]LLMaOFS, ZSiNaturalist Datasets, KikiBouba datasetsDataSculpt [48]LLMaPFSLabel Function DesignLLM2FEA [160]LLMaPFS, CoTObjective-oriented GenerationOther ApplicationstnGPS [178]LLMaDFS, OS, CoTTensor Network Structure SearchL-AutoDA [49]LLMaDFS, OS, CoTAdversarial Attack6.2.6 Other Applications.</p>
<p>Table . 3
.
[34]s the related works in this domain.6.3.1 General Science Discovery.In the realm of scientific discovery, LLMs are usually adopted for equation or functioin search.Notably, Du et al.[34]introduce LLM4ED, a framework that employs iterative strategies, including a black-box optimizer and evolutionary operators, to generate and optimize equations.This
approach has shown significant advancements in stability and usability for uncovering physical laws fromnonlinear dynamic systems. Similarly, Shojaee et al. [135] present LLM-SR, which combines LLMs' extensivescientific knowledge and code generation capabilities with evolutionary search. This framework excels inproposing and refining initial equation structures based on physical understanding, outperforming traditionalsymbolic regression methods in discovering physically accurate equations across multiple scientific domains. Abilevel optimization framework, named SGA, is by Ma et al. [98]. It merges the abstract reasoningcapabilities of LLMs with the computational power of simulations. This integration facilitates hypothesisgeneration and discrete reasoning with simulations for experimental feedback and optimization of continuousvariables, leading to improved performance.6.3.2 Chemistry, Biology &amp; Physics.</p>
<p>Table 3 .
3
An Overview of Science Discovery Applications Utilizing Language Models Across Various Domains and Tasks.
ApplicationMethodRole of LLMSpecific Problems or TasksBilevel [98]LLMaDPhysical Scientific DiscoveryGeneral Scientific Equation DiscoveryLLM-SR [135]LLMaDScientific Equation DiscoveryLLM4ED [34]LLMaDEquation DiscoveryChatChemTS [69]LLMaDMolecule DesignDebjyoti Bhattacharya, et al. [13]LLMaOMolecule DesignChemicalAgustinus Kristiadi, et al. [76] BoChemian [126]LLMaE LLMaEMolecule Design Chemical ReactionMulti-modal MoLFormer [138]LLMaPBattery Electrolytes Formulation DesignCataLM [153]LLMaPCatalyst DesignGavin Ye [173]LLMaDDrug DesignDrugAssist [174]LLMaODrug DesignMLDE [147]LLMaPProtein DesignBiologyProllama [97]MixedProtein DesignX-LoRA [16]LLMaPProtein DesignCodonBERT [82]LLMaPmRNA Design and OptimizationRevisiting-PLMs [62]LLMaPProtein Function PredictionFLUID-LLM [193]LLMaPComputational Fluid DynamicsMechanicsMechAgents [114]LLMaDMechanics DesignMeLM [17]LLMaP, LLMaD Carbon Nanotubes and Proteins DesignMengge Du, et al. [33]LLMaDNonlinear Dynamics Equation DiscoveryAutoTurb [186]LLMaDComputational Fliud Dynamics</p>
<p>Virginia Aglietti, Ira Ktena, Jessica Schrouff, Eleni Sgouritsa, J R Francisco, Alexis Ruiz, Silvia Bellot, Chiappa, arXiv:2406.04824FunBO: Discovering Acquisition Functions for Bayesian Optimization with FunSearch. 2024. 2024arXiv preprint</p>
<p>Optimus: Optimization modeling using mip solvers and large language models. Ali Ahmaditeshnizi, Wenzhi Gao, Madeleine Udell, arXiv:2310.061162023. 2023arXiv preprint</p>
<p>LM4OPT: Unveiling the potential of Large Language Models in formulating mathematical optimization problems. INFOR: Information Systems and Operational Research. Tasnim Ahmed, Salimur Choudhury, 2024. 2024</p>
<p>Janice Ahn, Rishu Verma, Renze Lou, Di Liu, Rui Zhang, Wenpeng Yin, arXiv:2402.00157Large language models for mathematical reasoning: Progresses and challenges. 2024. 2024arXiv preprint</p>
<p>Do As I Can. Anthony Michael Ahn, Noah Brohan, Yevgen Brown, Omar Chebotar, Byron Cortes, Chelsea David, Keerthana Finn, Karol Gopalakrishnan, Alexander Hausman, Daniel Herzog, Jasmine Ho, Julian Hsu, Brian Ibarz, Alex Ichter, Eric Irpan, Rosario Jang, Kyle Jauregui Ruano, Sally Jeffrey, Jesmonth, J Nikhil, Ryan Joshi, Dmitry Julian, Yuheng Kalashnikov, Kuang-Huei Kuang, Sergey Lee, Yao Levine, Linda Lu, Carolina Luu, Peter Parada, Jornell Pastor, Kanishka Quiambao, Jarek Rao, Diego Rettinghouse, Pierre Reyes, Sermanet, Grounding Language in Robotic Affordances. Nicolas Sievers, Clayton Tan, Alexander Toshev, Vincent Vanhoucke, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu, Mengyuan Yan, Not As I SayCoRR2022. 2022</p>
<p>Mohammad Alipour, - Vaezi, Kwok-Leung Tsui, arXiv:2404.07434Data-Driven Portfolio Management for Motion Pictures Industry: A New Data-Driven Optimization Methodology Using a Large Language Model as the Expert. 2024. 2024arXiv preprint</p>
<p>Zeyuan Allen, -Zhu , Yuanzhi Li, arXiv:2309.14316Physics of language models: Part 3.1, knowledge storage and extraction. 2023. 2023arXiv preprint</p>
<p>Bayesian Optimization with LLM-Based Acquisition Functions for Natural Language Preference Elicitation. David Eric Austin, Anton Korikov, Armin Toroghi, Scott Sanner, arXiv:2405.009812024. 2024arXiv preprint</p>
<p>From Data to Design: LLM-enabled information extraction across industries. Robert Becker, Laura Steffny, Thomas Bleistein, Dirk Werth, atp magazin. 662024. 2024</p>
<p>Machine learning for combinatorial optimization: a methodological tour d'horizon. Yoshua Bengio, Andrea Lodi, Antoine Prouvost, European Journal of Operational Research. 29022021. 2021</p>
<p>Mathematical modelling. John Berry, Ken Houston, 1995Gulf Professional Publishing</p>
<p>Siddhant Bhambri, Amrita Bhattacharjee, Huan Liu, Subbarao Kambhampati, arXiv:2405.15194Efficient Reinforcement Learning via Large Language Model-based Search. 2024. 2024arXiv preprint</p>
<p>Large Language Models as Molecular Design Engines. Debjyoti Bhattacharya, Harrison J Cassady, Michael A Hickner, Wesley F Reinhart, Journal of Chemical Information and Modeling. 2024. 2024</p>
<p>Shuvayan Brahmachary, M Subodh, Aniruddha Joshi, Kaushik Panda, Arun Koneripalli, Harshil Kumar Sagotra, Ankush Patel, Ameya D Sharma, Kaushic Jagtap, Kalyanaraman, arXiv:2403.02054Large Language Model-Based Evolutionary Optimizer: Reasoning with elitism. 2024. 2024arXiv preprint</p>
<p>David Brandfonbrener, Simon Henniger, Sibi Raja, Tarun Prasad, Chloe Loughridge, Federico Cassano, Sabrina Ruixin Hu, Jianang Yang, William E Byrd, Robert Zinkov, Nada Amin, arXiv:2402.08147[cs.SE]VerMCTS: Synthesizing Multi-Step Programs using a Verifier, a Large Language Model, and Tree Search. 2024</p>
<p>X-LoRA: Mixture of low-rank adapter experts, a flexible framework for large language models with applications in protein mechanics and molecular design. L Eric, Markus J Buehler, Buehler, APL Machine Learning. 222024. 2024</p>
<p>MeLM, a generative pretrained language modeling framework that solves forward and inverse mechanics problems. Markus J Buehler, Journal of the Mechanics and Physics of Solids. 1811054542023. 2023</p>
<p>Data is all you need: Finetuning LLMs for Chip Design via an Automated design-data augmentation framework. Kaiyan Chang, Kun Wang, Nan Yang, Ying Wang, Dantong Jin, Wenlong Zhu, Zhirong Chen, Cangyuan Li, Hao Yan, Yunhao Zhou, arXiv:2403.112022024. 2024arXiv preprint</p>
<p>EvoPrompting: Language Models for Code-Level Neural Architecture Search. Angelica Chen, David Dohan, David So, Advances in Neural Information Processing Systems. 2023</p>
<p>EvoPrompting: language models for code-level neural architecture search. Angelica Chen, David Dohan, David So, Advances in Neural Information Processing Systems. 362024. 2024</p>
<p>Large Language Model-driven Meta-structure Discovery in Heterogeneous Information Network. Lin Chen, Fengli Xu, Nian Li, Zhenyu Han, Meng Wang, Yong Li, Pan Hui, Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, KDD. the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, KDDACM2024</p>
<p>Automatic root cause analysis via large language models for cloud incidents. Yinfang Chen, Huaibing Xie, Minghua Ma, Yu Kang, Xin Gao, Liu Shi, Yunjie Cao, Xuedong Gao, Ming Hao Fan, Wen, Proceedings of the Nineteenth European Conference on Computer Systems. the Nineteenth European Conference on Computer Systems2024</p>
<p>Labelfree Node Classification on Graphs with Large Language Models (LLMs). Zhikai Chen, Haitao Mao, Hongzhi Wen, Haoyu Han, Wei Jin, Haiyang Zhang, Hui Liu, Jiliang Tang, The Twelfth International Conference on Learning Representations. 2024</p>
<p>Evolving Interpretable Visual Classifiers with Large Language Models. Mia Chiquier, Utkarsh Mall, Carl Vondrick, 2024. 2024CoRR</p>
<p>Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, arXiv:2110.14168Training verifiers to solve math word problems. 2021. 2021arXiv preprint</p>
<p>Introduction to algorithms. Charles E Thomas H Cormen, Ronald L Leiserson, Clifford Rivest, Stein, 2022MIT press</p>
<p>Chris Cummins, Volker Seeker, Dejan Grubisic, Jonas Baptiste Roziere, Gabriel Gehring, Hugh Synnaeve, Leather, arXiv:2407.02524Meta Large Language Model Compiler: Foundation Models of Compiler Optimization. 2024. 2024arXiv preprint</p>
<p>An investigation on the use of Large Language Models for hyperparameter tuning in Evolutionary Algorithms. Leonardo Lucio Custode, Fabio Caraffini, Anil Yaman, Giovanni Iacca, Proceedings of the Genetic and Evolutionary Computation Conference Companion. the Genetic and Evolutionary Computation Conference Companion2024</p>
<p>Generating Code World Models with Large Language Models Guided by Monte Carlo Tree Search. Nicola Dainese, Matteo Merler, Minttu Alakuijala, Pekka Marttinen, arXiv:2405.153832024. 2024arXiv preprint</p>
<p>Iohexperimenter: Benchmarking platform for iterative optimization heuristics. Furong Jacob De Nobel, Diederick Ye, Hao Vermetten, Carola Wang, Thomas Doerr, Bäck, Evolutionary Computation. 2024. 2024</p>
<p>Lift: Language-interfaced fine-tuning for non-language machine learning tasks. Tuan Dinh, Yuchen Zeng, Ruisu Zhang, Ziqian Lin, Michael Gira, Shashank Rajput, Advances in Neural Information Processing Systems. 352022. 2022Jy-yong Sohn, Dimitris Papailiopoulos, and Kangwook Lee</p>
<p>Hongyang Du, Guangyuan Liu, Yijing Lin, Dusit Niyato, Jiawen Kang, Zehui Xiong, Dong In Kim, arXiv:2402.09756Mixture of Experts for Network Optimization: A Large Language Model-enabled Approach. 2024. 2024arXiv preprint</p>
<p>Large language models for automatic equation discovery of nonlinear dynamics. Mengge Du, Yuntian Chen, Zhongzheng Wang, Longfeng Nie, Dongxiao Zhang, Physics of Fluids. 3692024. 2024</p>
<p>LLM4ED: Large Language Models for Automatic Equation Discovery. Mengge Du, Yuntian Chen, Zhongzheng Wang, Longfeng Nie, Dongxiao Zhang, arXiv:2405.077612024. 2024arXiv preprint</p>
<p>Leveraging Reinforcement Learning and Large Language Models for Code Optimization. Nikos Shukai Duan, Xiongye Kanakaris, Heng Xiao, Chenyu Ping, Nesreen K Zhou, Guixiang Ahmed, Mihai Ma, Capota, Shahin Theodore L Willke, Nazarian, arXiv:2312.056572023. 2023arXiv preprint</p>
<p>Using imperfect surrogates for downstream inference: Design-based supervised learning for social science applications of large language models. Naoki Egami, Musashi Hinck, Brandon Stewart, Hanying Wei, Advances in Neural Information Processing Systems. 362024. 2024</p>
<p>Katharina Eggensperger, Philipp Müller, Neeratyoy Mallik, Matthias Feurer, René Sass, Aaron Klein, Noor Awad, Marius Lindauer, Frank Hutter, arXiv:2109.06716HPOBench: A collection of reproducible multi-fidelity benchmark problems for HPO. 2021. 2021arXiv preprint</p>
<p>OMNI-EPIC: Open-endedness via Models of human Notions of Interestingness with Environments Programmed in Code. Maxence Faldor, Jenny Zhang, Antoine Cully, Jeff Clune, arXiv:2405.155682024. 2024arXiv preprint</p>
<p>Difficulty adjustable and scalable constrained multiobjective test problem toolkit. Zhun Fan, Wenji Li, Xinye Cai, Hui Li, Caimin Wei, Qingfu Zhang, Kalyanmoy Deb, Erik Goodman, Evolutionary computation. 282020. 2020</p>
<p>Assertllm: Generating and evaluating hardware verification assertions from design specifications via multi-llms. Wenji Fang, Mengming Li, Min Li, Zhiyuan Yan, Shang Liu, Hongce Zhang, Zhiyao Xie, arXiv:2402.003862024. 2024arXiv preprint</p>
<p>Knowledge Solver: Teaching LLMs to Search for Domain Knowledge from Knowledge Graphs. Chao Feng, Xinyu Zhang, Zichu Fei, 2023. 2023</p>
<p>Large language models for biomolecular analysis: From methods to applications. Ruijun Feng, Chi Zhang, Yang Zhang, TrAC Trends in Analytical Chemistry. 1175402024. 2024</p>
<p>Designing with Language: Wireframing UI Design Intent with Generative Large Language Models. Sidong Feng, Mingyue Yuan, Jieshan Chen, Zhenchang Xing, Chunyang Chen, arXiv:2312.077552023. 2023arXiv preprint</p>
<p>Bayesian optimization for materials design. Information science for materials discovery and design. I Peter, Jialei Frazier, Wang, 2016. 2016</p>
<p>Strategyllm: Large language models as strategy generators, executors, optimizers, and evaluators for problem solving. Chang Gao, Haiyun Jiang, Deng Cai, Shuming Shi, Wai Lam, arXiv:2311.088032023. 2023arXiv preprint</p>
<p>Ideas are dimes a dozen: Large language models for idea generation in innovation. Karan Girotra, Lennart Meincke, Christian Terwiesch, Karl T Ulrich, Available at SSRN. 45260712023. 2023</p>
<p>Julia Grosse, Ruotian Wu, Ahmad Rashid, Philipp Hennig, Pascal Poupart, Agustinus Kristiadi, arXiv:2407.03951Uncertainty-Guided Optimization on Large Language Model Search Trees. 2024. 2024arXiv preprint</p>
<p>Can Large Language Models Design Accurate Label Functions?. Naiqing Guan, Kaiwen Chen, Nick Koudas, arXiv:2311.007392023. 2023CoRR</p>
<p>L-autoda: Leveraging large language models for automated decision-based adversarial attacks. Ping Guo, Fei Liu, Xi Lin, Qingchuan Zhao, Qingfu Zhang, arXiv:2401.153352024. 2024arXiv preprint</p>
<p>Qingyan Guo, Rui Wang, Junliang Guo, Bei Li, Kaitao Song, Xu Tan, Guoqing Liu, Jiang Bian, Yujiu Yang, arXiv:2309.08532Connecting large language models with evolutionary algorithms yields powerful prompt optimizers. 2023. 2023arXiv preprint</p>
<p>. Zixian Guo, Ming Liu, Zhilong Ji, Jinfeng Bai, Yiwen Guo, Wangmeng Zuo, n. d.</p>
<p>Two Optimizers Are Better Than One: LLM Catalyst Empowers Gradient-Based Optimization for Prompt Tuning. arXiv:2405.19732[cs.CV]</p>
<p>Two Optimizers Are Better Than One: LLM Catalyst for Enhancing Gradient-Based Optimization. Zixian Guo, Ming Liu, Zhilong Ji, Jinfeng Bai, Yiwen Guo, Wangmeng Zuo, arXiv:2405.197322024. 2024arXiv preprint</p>
<p>Large language models: a comprehensive survey of its applications, challenges, limitations, and future prospects. Qasem Muhammad Usman Hadi, Abbas Al Tashi, Rizwan Shah, Amgad Qureshi, Muhammad Muneer, Anas Irfan, Muhammad Zafar, Naveed Bilal Shaikh, Jia Akhtar, Wu, Authorea Preprints. 2024. 2024</p>
<p>Kunal Handa, Yarin Gal, Ellie Pavlick, Noah Goodman, Jacob Andreas, Alex Tamkin, Belinda Z Li, arXiv:2403.05534Bayesian preference elicitation with language models. 2024. 2024arXiv preprint</p>
<p>Black-box optimization benchmarking of NEWUOA compared to BIPOP-CMA-ES: on the BBOB noiseless testbed. Nikolaus Hansen, Raymond Ros, Proceedings of the 12th annual conference companion on Genetic and evolutionary computation. the 12th annual conference companion on Genetic and evolutionary computation2010</p>
<p>Large Language Models as Surrogate Models in Evolutionary Algorithms: A Preliminary Study. Hao Hao, Xiaoqun Zhang, Aimin Zhou, arXiv:2406.106752024. 2024arXiv preprint</p>
<p>Saycanpay: Heuristic planning with large language models using learnable domain knowledge. Rishi Hazra, Pedro Zuidberg Dos Martires, Luc De, Raedt , Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence2024. 20123-2013338</p>
<p>. Cheng He, Ye Tian, Zhichao Lu, n. d.</p>
<p>Evolutionary Computation Evolves with Large Language Models. Artificial Evolutionary Intelligence. </p>
<p>Evolving code with a large language model. Erik Hemberg, Stephen Moskal, Una-May O' Reilly, Genetic Programming and Evolvable Machines. 25212024. 2024</p>
<p>Instruction induction: From few examples to natural language task descriptions. Or Honovich, Uri Shaham, Omer Samuel R Bowman, Levy, arXiv:2205.107822022. 2022arXiv preprint</p>
<p>LoRA: Low-Rank Adaptation of Large Language Models. J Edward, Phillip Hu, Zeyuan Wallis, Yuanzhi Allen-Zhu, Shean Li, Lu Wang, Weizhu Wang, Chen, International Conference on Learning Representations. 2022</p>
<p>Exploring evolution-aware &amp;-free protein language models as protein function predictors. Mingyang Hu, Fajie Yuan, Kevin Yang, Fusong Ju, Jin Su, Hui Wang, Fei Yang, Qiuyang Ding, Advances in Neural Information Processing Systems. 352022. 2022</p>
<p>Automated design of agentic systems. Shengran Hu, Cong Lu, Jeff Clune, arXiv:2408.084352024. 2024arXiv preprint</p>
<p>Sen Huang, Kaixiang Yang, Sheng Qi, Rui Wang, arXiv:2405.10098When Large Language Model Meets Optimization. 2024. 2024arXiv preprint</p>
<p>Language Models as Zero-Shot Planners: Extracting Actionable Knowledge for Embodied Agents. Wenlong Huang, Pieter Abbeel, Deepak Pathak, Igor Mordatch, International Conference on Machine Learning, ICML (Proceedings of Machine Learning Research). PMLR2022</p>
<p>Grounded Decoding: Guiding Text Generation with Grounded Models for Embodied Agents. Wenlong Huang, Fei Xia, Dhruv Shah, Danny Driess, Andy Zeng, Yao Lu, Pete Florence, Igor Mordatch, Sergey Levine, Karol Hausman, Brian Ichter, Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems. NeurIPS2023. 2023</p>
<p>Xuhan Huang, Qingning Shen, Yan Hu, Anningzhe Gao, Benyou Wang, arXiv:2405.13144Mamo: a Mathematical Modeling Benchmark with Solvers. 2024. 2024arXiv preprint</p>
<p>How multimodal integration boost the performance of llm for optimization: Case study on capacitated vehicle routing problems. Yuxiao Huang, Wenjie Zhang, Liang Feng, Xingyu Wu, Kay Chen, Tan , arXiv:2403.017572024. 2024arXiv preprint</p>
<p>Large Language Models Open New Way of AI-Assisted Molecule Design for Chemists. Shoichi Ishida, Tomohiro Sato, Teruki Honma, Kei Terayama, 2024. 2024</p>
<p>Ganesh Jawahar, Muhammad Abdul-Mageed, Laks Vs Lakshmanan, Dujian Ding, arXiv:2310.16712LLM Performance Predictors are good initializers for Architecture Search. 2023. 2023arXiv preprint</p>
<p>LLMatDesign: Autonomous Materials Discovery with Large Language Models. Shuyi Jia, Chao Zhang, Victor Fung, arXiv:2406.131632024. 2024arXiv preprint</p>
<p>A Survey on Large Language Models for Code Generation. Juyong Jiang, Fan Wang, Jiasi Shen, Sungju Kim, Sunghun Kim, arXiv:2406.005152024. 2024arXiv preprint</p>
<p>Inferfix: End-to-end program repair with llms. Matthew Jin, Syed Shahriar, Michele Tufano, Xin Shi, Shuai Lu, Neel Sundaresan, Alexey Svyatkovskiy, Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering. the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering2023</p>
<p>Large Language Model-based Test Case Generation for GP Agents. Steven Jorgensen, Giorgia Nadizar, Gloria Pietropolli, Luca Manzoni, Eric Medvet, Una-May O' Reilly, Erik Hemberg, Proceedings of the Genetic and Evolutionary Computation Conference. the Genetic and Evolutionary Computation Conference2024</p>
<p>. Kleinberg, Algorithm Design. 922006Pearson Education</p>
<p>A Sober Look at LLMs for Material Discovery: Are They Actually Good for Bayesian Optimization Over Molecules?. Agustinus Kristiadi, Felix Strieth-Kalthoff, Marta Skreta, Pascal Poupart, Forty-first International Conference on Machine Learning. 2024Alan Aspuru-Guzik, and Geoff Pleiss</p>
<p>Large language models as evolution strategies. Robert Lange, Yingtao Tian, Yujin Tang, Proceedings of the Genetic and Evolutionary Computation Conference Companion. the Genetic and Evolutionary Computation Conference Companion2024</p>
<p>Evolution through large models. Joel Lehman, Jonathan Gordon, Shawn Jain, Kamal Ndousse, Cathy Yeh, Kenneth O Stanley, Handbook of Evolutionary Machine Learning. Springer2023</p>
<p>Evolution Through Large Models. Joel Lehman, Jonathan Gordon, Shawn Jain, Kamal Ndousse, Cathy Yeh, Kenneth O Stanley, 2024Springer NatureSingapore; Singapore</p>
<p>Auto mc-reward: Automated dense reward design with large language models for minecraft. Hao Li, Xue Yang, Zhaokai Wang, Xizhou Zhu, Jie Zhou, Yu Qiao, Xiaogang Wang, Hongsheng Li, Lewei Lu, Jifeng Dai, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2024</p>
<p>A Map of Exploring Human Interaction Patterns with LLM: Insights into Collaboration and Creativity. Jiayang Li, Jiale Li, Yunsheng Su, International Conference on Human-Computer Interaction. Springer2024</p>
<p>Codonbert: Large language models for mrna design and optimization. Sizhen Li, Saeed Moayedpour, Ruijiang Li, Michael Bailey, Saleh Riahi, Lorenzo Kogler-Anele, Milad Miladi, Jacob Miner, Dinghai Zheng, bioRxiv. Jun Wang, et al. 2023. 2023</p>
<p>Prefix-tuning: Optimizing continuous prompts for generation. Lisa Xiang, Percy Li, Liang, arXiv:2101.001902021. 2021arXiv preprint</p>
<p>Text2motion: From natural language instructions to feasible plans. Kevin Lin, Christopher Agia, Toki Migimatsu, Marco Pavone, Jeannette Bohg, Autonomous Robots. 472023. 2023</p>
<p>Xiaohan Lin, Qingxing Cao, Yinya Huang, Zhicheng Yang, Zhengying Liu, Zhenguo Li, Xiaodan Liang, arXiv:2405.06677ATG: Benchmarking Automated Theorem Generation for Generative Language Models. 2024. 2024arXiv preprint</p>
<p>Large Language Model for Multi-objective Evolutionary Optimization. Fei Liu, Xi Lin, Zhenkun Wang, Shunyu Yao, Xialiang Tong, Mingxuan Yuan, Qingfu Zhang, arXiv:2310.125412023. 2023arXiv preprint</p>
<p>Fei Liu, Xialiang Tong, Mingxuan Yuan, Qingfu Zhang, arXiv:2311.15249Algorithm evolution using large language model. 2023. 2023arXiv preprint</p>
<p>Evolution of Heuristics: Towards Efficient Automatic Algorithm Design Using Large Language Model. Fei Liu, Tong Xialiang, Mingxuan Yuan, Xi Lin, Fu Luo, Zhenkun Wang, Zhichao Lu, Qingfu Zhang, Forty-first International Conference on Machine Learning. 2024</p>
<p>Multimodal Large Language Models for Inverse Molecular Design with Retrosynthetic Planning. Gang Liu, Michael Sun, Wojciech Matusik, Meng Jiang, Jie Chen, arXiv:2410.042232024. 2024arXiv preprint</p>
<p>Max Liu, Chan-Hung Yu, Wei-Hsu Lee, Cheng-Wei Hung, Yen-Chun Chen, Shao-Hua Sun, arXiv:2405.16450Synthesizing Programmatic Reinforcement Learning Policies with Large Language Model Guided Search. 2024. 2024arXiv preprint</p>
<p>Large language models as evolutionary optimizers. Shengcai Liu, Caishun Chen, Xinghua Qu, IEEE Congress on Evolutionary Computation (CEC). IEEE. 2024. 2024Ke Tang, and Yew-Soon Ong</p>
<p>Large Language Model Agent for Hyper-Parameter Optimization. Siyi Liu, Chen Gao, Yong Li, arXiv:2402.018812024. 2024arXiv preprint</p>
<p>Large language models to enhance bayesian optimization. Tennison Liu, Nicolás Astorga, arXiv:2402.039212024. 2024arXiv preprintNabeel Seedat, and Mihaela van der Schaar</p>
<p>Zhiwei Liu, Weiran Yao, Jianguo Zhang, Liangwei Yang, Zuxin Liu, Juntao Tan, Tian Prafulla K Choubey, Jason Lan, Huan Wu, Wang, arXiv:2402.15538AgentLite: A Lightweight Library for Building and Advancing Task-Oriented LLM Agent System. 2024. 2024arXiv preprint</p>
<p>Sifan Long, Fengxiao Tang, Yangfan Li, Tiao Tan, Zhengjie Jin, Ming Zhao, Nei Kato, arXiv:2404.183736G comprehensive intelligence: network operations and optimization based on Large Language Models. 2024. 2024arXiv preprint</p>
<p>LARGE LANGUAGE MODEL-DRIVEN SIMULATIONS FOR SYSTEM OF SYSTEMS ANALYSIS IN FIREFIGHTING AIRCRAFT CONCEPTUAL DESIGN. Jorge Lovaco, Raghu Chaitanya Munjulury, Ingo Staack, Petter Krus, 2024. 2024In 34th Congress of the International Council of the Aeronautical Sciences</p>
<p>Liuzhenghao Lv, Zongying Lin, Hao Li, Yuyang Liu, Jiaxi Cui, Calvin Yu-Chian Chen, Li Yuan, Yonghong Tian, arXiv:2402.16445Prollama: A protein large language model for multi-task protein language processing. 2024. 2024arXiv preprint</p>
<p>Pingchuan Ma, Tsun-Hsuan Wang, Minghao Guo, Zhiqing Sun, Joshua B Tenenbaum, Daniela Rus, Chuang Gan, Wojciech Matusik, arXiv:2405.09783LLM and Simulation as Bilevel Optimizers: A New Paradigm to Advance Physical Scientific Discovery. 2024. 2024arXiv preprint</p>
<p>Ruotian Ma, Xiaolei Wang, Xin Zhou, Jian Li, Nan Du, Tao Gui, Qi Zhang, Xuanjing Huang, arXiv:2402.02101Are Large Language Models Good Prompt Optimizers?. 2024. 2024arXiv preprint</p>
<p>Eureka: Human-Level Reward Design via Coding Large Language Models. Jason Yecheng, William Ma, Guanzhi Liang, De-An Wang, Osbert Huang, Dinesh Bastani, Yuke Jayaraman, Linxi Zhu, Anima Fan, Anandkumar, The Twelfth International Conference on Learning Representations. 2024</p>
<p>LLaMoCo: Instruction Tuning of Large Language Models for Optimization Code Generation. Zeyuan Ma, Hongshu Guo, Jiacheng Chen, Guojun Peng, Zhiguang Cao, Yining Ma, Yue-Jiao Gong, arXiv:2403.011312024. 2024arXiv preprint</p>
<p>Large language models for telecom: Forthcoming impact on the industry. Ali Maatouk, Nicola Piovesan, Fadhel Ayed, Antonio De, Domenico , Merouane Debbah, IEEE Communications Magazine. 2024. 2024</p>
<p>Handbook of collective intelligence. W Thomas, Michael S Malone, Bernstein, 2022MIT press</p>
<p>Identify Critical Nodes in Complex Network with Large Language Models. Jinzhu Mao, Dongyun Zou, Li Sheng, Siyi Liu, Chen Gao, Yue Wang, Yong Li, 2024. 2024</p>
<p>Sean Mcleish, Avi Schwarzschild, Tom Goldstein, arXiv:2404.03441Benchmarking ChatGPT on Algorithmic Reasoning. 2024. 2024arXiv preprint</p>
<p>Large Language Models for Relevance Judgment in Product Search. Navid Mehrdad, Hrushikesh Mohapatra, Mossaab Bagdouri, Prijith Chandran, Alessandro Magnani, Xunfan Cai, Ajit Puthenputhussery, Sachin Yadav, Tony Lee, Chengxiang Zhai, arXiv:2406.002472024. 2024arXiv preprint</p>
<p>Enriching building function classification using Large Language Model embeddings of OpenStreetMap Tags. Abdulkadir Memduhoğlu, Nir Fulman, Alexander Zipf, Earth Science Informatics. 2024. 2024</p>
<p>Jean-Yves Potvin, Michel Gendreau, Handbook of metaheuristics. Springer2019</p>
<p>Assert-O: Context-based Assertion Optimization using LLMs. Samit Shahnawaz Miftah, Amisha Srivastava, Hyunmin Kim, Kanad Basu, Proceedings of the Great Lakes Symposium on VLSI 2024. the Great Lakes Symposium on VLSI 20242024</p>
<p>Clint Morris, Michael Jurado, Jason Zutty, arXiv:2403.11446LLM Guided Evolution-The Automation of Models Advancing Models. 2024. 2024arXiv preprint</p>
<p>Optimization modeling and verification from problem specifications using a multi-agent multi-stage LLM framework. Mahdi Mostajabdaveh, Timothy T Yu, Rindranirina Ramamonjison, Giuseppe Carenini, Zirui Zhou, Yong Zhang, INFOR: Information Systems and Operational Research. 2024. 2024</p>
<p>Evolutionary Reward Design and Optimization with Multimodal Large Language Models. Ali Narin, Proceedings of the 3rd Workshop on Advances in Language and Vision Research (ALVR). the 3rd Workshop on Advances in Language and Vision Research (ALVR)2024</p>
<p>Sam Muhammad U Nasir, Julian Earle, Steven Togelius, Christopher James, Cleghorn, arXiv:2306.01102LLMatic: Neural Architecture Search via Large Language Models and Quality-Diversity Optimization. 2023. 2023arXiv preprint</p>
<p>MechAgents: Large language model multi-agent collaborations can solve mechanics problems, generate new data, and integrate knowledge. Bo Ni, Markus J Buehler, Extreme Mechanics Letters. 671021312024. 2024</p>
<p>Allen Nie, Ching-An Cheng, Andrey Kolobov, Adith Swaminathan, arXiv:2405.16434The Importance of Directional Feedback for LLM-based Optimizers. 2024. 2024arXiv preprint</p>
<p>Introducing OpenAI o1-preview. 2024OpenAI</p>
<p>Using Large Language Models for Evolutionary Search. Una-May O' Reilly, Erik Hemberg, Proceedings of the Genetic and Evolutionary Computation Conference Companion. the Genetic and Evolutionary Computation Conference Companion2024</p>
<p>On the prospects of incorporating large language models (llms) in automated planning and scheduling (aps). Vishal Pallagani, Chandra Bharath, Kaushik Muppasani, Francesco Roy, Andrea Fabiano, Keerthiram Loreggia, Biplav Murugesan, Francesca Srivastava, Lior Rossi, Amit Horesh, Sheth, Proceedings of the International Conference on Automated Planning and Scheduling. the International Conference on Automated Planning and Scheduling202434</p>
<p>Quantum Many-Body Physics Calculations with Large Language Models. Haining Pan, Nayantara Mudur, Will Taranto, Maria Tikhanovskaya, Subhashini Venugopalan, Yasaman Bahri, Michael P Brenner, Eun-Ah Kim, arXiv:2403.031542024. 2024arXiv preprint</p>
<p>Enhancing speaker diarization with large language models: A contextual beam search approach. Jin Tae, Kunal Park, Nithin Dhawan, Jagadeesh Koluguri, Balam, ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE2024</p>
<p>Grips: Gradient-free, edit-based instruction search for prompting large language models. Archiki Prasad, Peter Hase, Xiang Zhou, Mohit Bansal, arXiv:2203.072812022. 2022arXiv preprint</p>
<p>Automatic prompt optimization with" gradient descent" and beam search. Reid Pryzant, Dan Iter, Jerry Li, Yin Tat Lee, Chenguang Zhu, Michael Zeng, arXiv:2305.034952023. 2023arXiv preprint</p>
<p>Quality diversity: A new frontier for evolutionary computation. Justin K Pugh, Lisa B Soros, Kenneth O Stanley, Frontiers in Robotics and AI. 32028452016. 2016</p>
<p>Nl4opt competition: Formulating optimization problems based on their natural language descriptions. Rindranirina Ramamonjison, Timothy Yu, Raymond Li, Haley Li, Giuseppe Carenini, Bissan Ghaddar, Shiqi He, Mahdi Mostajabdaveh, Amin Banitalebi-Dehkordi, Zirui Zhou, NeurIPS 2022 Competition Track. 2023</p>
<p>Bayesian optimization of catalysts with in-context learning. Shane S Mayk Caldas Ramos, Marc D Michtavy, Andrew D Porosoff, White, arXiv:2304.053412023. 2023arXiv preprint</p>
<p>BoChemian: Large language model embeddings for Bayesian optimization of chemical reactions. Bojana Ranković, Philippe Schwaller, NeurIPS 2023 Workshop on Adaptive Experimental Design and Active Learning in the Real World. 2023</p>
<p>Large language model-assisted surrogate modelling for engineering optimization. Thiago Rios, Felix Lanfermann, Stefan Menzel, 2024 IEEE Conference on Artificial Intelligence (CAI). IEEE2024</p>
<p>Mathematical discoveries from program search with large language models. Bernardino Romera-Paredes, Mohammadamin Barekatain, Alexander Novikov, Matej Balog, Pawan Kumar, Emilien Dupont, Francisco Jr Ruiz, Jordan S Ellenberg, Pengming Wang, Omar Fawzi, Nature. 6252024. 2024</p>
<p>Using ChatGPT for Method Development and Green Chemistry Education in Upper-Level Laboratory Courses. Emily F Ruff, Jeanne L Franz, Joseph K West, Journal of Chemical Education. 1012024. 2024</p>
<p>A systematic survey of prompt engineering in large language models. Pranab Sahoo, Ayush Kumar Singh, Sriparna Saha, Vinija Jain, Samrat Mondal, Aman Chadha, arXiv:2402.079272024. 2024Techniques and applications. arXiv preprint</p>
<p>Chacón Camilo, Christian Sartori, Blum, arXiv:2405.18272Filippo Bistaffa, and Guillem Rodríguez Corominas. 2024. Metaheuristics and Large Language Models Join Forces: Towards an Integrated Optimization Approach. 2024arXiv preprint</p>
<p>Analysing utterances in llm-based user simulation for conversational search. Ivan Sekulić, Mohammad Alinannejadi, Fabio Crestani, ACM Transactions on Intelligent Systems and Technology. 152024. 2024</p>
<p>Navigation with large language models: Semantic guesswork as a heuristic for planning. Dhruv Shah, Michael Robert Equi, Błażej Osiński, Fei Xia, Brian Ichter, Sergey Levine, Conference on Robot Learning. PMLR2023</p>
<p>Yiqing Shen, Zan Chen, Michail Mamalakis, Yungeng Liu, Tianbin Li, Yanzhou Su, Junjun He, Pietro Liò, Yu Guang, Wang , arXiv:2408.15299TourSynbio: A Multi-Modal Large Model and Agent Framework to Bridge Text and Protein Sequences for Protein Engineering. 2024. 2024arXiv preprint</p>
<p>Parshin Shojaee, Kazem Meidani, Shashank Gupta, Amir Barati Farimani, Chandan K Reddy, arXiv:2404.18400LLM-SR: Scientific Equation Discovery via Programming with Large Language Models. 2024. 2024arXiv preprint</p>
<p>Enhancing Decision-Making in. Gaurav Singh, Kavitesh Kumar, Bali , arXiv:2405.07212Optimization through LLM-Assisted Inference: A Neural Networks Perspective. 2024. 2024arXiv preprint</p>
<p>ProgPrompt: Generating Situated Robot Task Plans using Large Language Models. Ishika Singh, Valts Blukis, Arsalan Mousavian, Ankit Goyal, Danfei Xu, Jonathan Tremblay, Dieter Fox, Jesse Thomason, Animesh Garg, ICRA. IEEE2023</p>
<p>Capturing Formulation Design of Battery Electrolytes with Chemical Large Language Model. Eduardo Soares, Vidushi Sharma, Emilio Vital Brazil, Young-Hye Na, Renato Cerqueira, 2024. 2024</p>
<p>Chuanneng Sun, Songjun Huang, Dario Pompili, arXiv:2405.11106LLM-based Multi-Agent Reinforcement Learning: Current and Future Directions. 2024. 2024arXiv preprint</p>
<p>Yiwen Sun, Xianyin Zhang, Shiyu Huang, Shaowei Cai, Bing-Zhen Zhang, Ke Wei, arXiv:2402.10705AutoSAT: Automatically Optimize SAT Solvers via Large Language Models. 2024. 2024arXiv preprint</p>
<p>Challenging big-bench tasks and whether chain-of-thought can solve them. Mirac Suzgun, Nathan Scales, Nathanael Schärli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Ed H Quoc V Le, Denny Chi, Zhou, arXiv:2210.092612022. 2022arXiv preprint</p>
<p>Learn to Optimize-A Brief Overview. Ke Tang, Xin Yao, National Science Review. e1322024. 2024</p>
<p>Unleashing the Potential of Large Language Models as Prompt Optimizers: An Analogical Analysis with Gradient-based Model Optimizers. Xinyu Tang, Xiaolei Wang, Wayne Xin Zhao, Siyuan Lu, Yaliang Li, Ji-Rong Wen, arXiv:2402.175642024. 2024arXiv preprint</p>
<p>Synergizing Spatial Optimization with Large Language Models for Open-Domain Urban Itinerary Planning. Yihong Tang, Zhaokai Wang, Ao Qu, Yihao Yan, Kebing Hou, Dingyi Zhuang, Xiaotong Guo, Jinhua Zhao, Zhan Zhao, Wei Ma, arXiv:2402.072042024. 2024arXiv preprint</p>
<p>Zhengyang Tang, Chenyu Huang, Xin Zheng, Shixi Hu, Zizhuo Wang, Dongdong Ge, Benyou Wang, arXiv:2405.17743ORLM: Training Large Language Models for Optimization Modeling. 2024. 2024arXiv preprint</p>
<p>Integrating Genetic Algorithms and Language Models for Enhanced Enzyme Design. Yves Gaetan, Nana Teukam, Federico Zipoli, Teodoro Laino, Emanuele Criscuolo, Francesca Grisoni, Matteo Manica, 2024. 2024</p>
<p>Protein design by directed evolution guided by large language models. Thanh Vt, Tran , Truong Son, Hy , IEEE Transactions on Evolutionary Computation. 2024. 2024</p>
<p>Uber/bayesmark: Benchmark framework to easily compare bayesian optimization methods on real machine learning tasks. Uber, 2020</p>
<p>Planbench: An extensible benchmark for evaluating large language models on planning and reasoning about change. Karthik Valmeekam, Matthew Marquez, Alberto Olmo, Sarath Sreedharan, Subbarao Kambhampati, Advances in Neural Information Processing Systems. 362024. 2024</p>
<p>LLaMEA: A Large Language Model Evolutionary Algorithm for Automatically Generating Metaheuristics. Niki Van Stein, Thomas Bäck, arXiv:2405.201322024. 2024arXiv preprint</p>
<p>The CLRS algorithmic reasoning benchmark. Petar Veličković, Adrià Puigdomènech Badia, David Budden, Razvan Pascanu, Andrea Banino, Misha Dashevskiy, Raia Hadsell, Charles Blundell, International Conference on Machine Learning. PMLR2022</p>
<p>Scientific discovery in the age of artificial intelligence. Hanchen Wang, Tianfan Fu, Yuanqi Du, Wenhao Gao, Kexin Huang, Ziming Liu, Payal Chandak, Shengchao Liu, Peter Van Katwyk, Andreea Deac, Nature. 6202023. 2023</p>
<p>CataLM: Empowering Catalyst Design Through Large Language Models. Ludi Wang, Xueqing Chen, Yi Du, Yuanchun Zhou, Yang Gao, Wenjuan Cui, arXiv:2405.174402024. 2024arXiv preprint</p>
<p>A survey on large language model based autonomous agents. Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, Frontiers of Computer Science. 181863452024. 2024</p>
<p>Shuai Wang, Shengyao Zhuang, Bevan Koopman, Guido Zuccon, arXiv:2401.17645ReSLLM: Large Language Models are Strong Resource Selectors for Federated Search. 2024. 2024arXiv preprint</p>
<p>Xinyuan Wang, Chenxi Li, Zhen Wang, Fan Bai, Haotian Luo, Jiayou Zhang, Nebojsa Jojic, Eric P Xing, Zhiting Hu, arXiv:2310.16427[cs.CL]PromptAgent: Strategic Planning with Language Models Enables Expert-level Prompt Optimization. 2023</p>
<p>Chain-of-thought reasoning without prompting. Xuezhi Wang, Denny Zhou, arXiv:2402.102002024. 2024arXiv preprint</p>
<p>Large Language Model-Aided Evolutionary Search for Constrained Multiobjective Optimization. Zeyi Wang, Songbai Liu, Jianyong Chen, Kay Chen, Tan , International Conference on Intelligent Computing. Springer2024</p>
<p>From Large Language Models and Optimization to Decision Optimization CoPilot: A Research Manifesto. Segev Wasserkrug, Leonard Boussioux, Farzaneh Dick Den Hertog, Ilker Mirzazadeh, Jannis Birbil, Donato Kurtz, Maragno, arXiv:2402.162692024. 2024arXiv preprint</p>
<p>Melvin Wong, Jiao Liu, Thiago Rios, Stefan Menzel, Yew Soon Ong, arXiv:2406.14917LLM2FEA: Discover Novel Designs with Generative Evolutionary Multitasking. 2024. 2024arXiv preprint</p>
<p>Generative AI-based Prompt Evolution Engineering Design Optimization With Vision-Language Model. Melvin Wong, Thiago Rios, Stefan Menzel, Yew Soon Ong, arXiv:2406.091432024. 2024arXiv preprint</p>
<p>A survey on large language models for recommendation. Likang Wu, Zhi Zheng, Zhaopeng Qiu, Hao Wang, Hongchao Gu, Tingjia Shen, Chuan Qin, Chen Zhu, Hengshu Zhu, Qi Liu, World Wide Web. 27602024. 2024</p>
<p>Xingyu Wu, Sheng-Hao Wu, Jibin Wu, Liang Feng, Kay Chen, Tan , arXiv:2401.10034Evolutionary Computation in the Era of Large Language Model: Survey and Roadmap. 2024. 2024arXiv preprint</p>
<p>Large language model-enhanced algorithm selection: towards comprehensive algorithm representation. Xingyu Wu, Yan Zhong, Jibin Wu, Bingbing Jiang, Kay Chen Tan, International Joint Conference on Artificial Intelligence. 2024</p>
<p>Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang Hong, Ming Zhang, Junzhe Wang, Senjie Jin, Enyu Zhou, arXiv:2309.07864The rise and potential of large language model based agents: A survey. 2023. 2023arXiv preprint</p>
<p>Jian Xie, Kai Zhang, Jiangjie Chen, Tinghui Zhu, Renze Lou, Yuandong Tian, Yanghua Xiao, Yu Su, arXiv:2402.01622Travelplanner: A benchmark for real-world planning with language agents. 2024. 2024arXiv preprint</p>
<p>Large Language Models as Optimizers. Chengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao Liu, Denny Quoc V Le, Xinyun Zhou, Chen, The Twelfth International Conference on Learning Representations. 2024</p>
<p>Heng Yang, Ke Li, arXiv:2310.17630Instoptima: Evolutionary multi-objective instruction optimization via large language model-based instruction operators. 2023. 2023arXiv preprint</p>
<p>Large language models for automated open-domain scientific hypotheses discovery. Zonglin Yang, Xinya Du, Junxian Li, Jie Zheng, Soujanya Poria, Erik Cambria, arXiv:2309.027262023. 2023arXiv preprint</p>
<p>Shunyu Yao, Fei Liu, Xi Lin, Zhichao Lu, Zhenkun Wang, Qingfu Zhang, arXiv:2409.16867Multi-objective Evolution of Heuristic Using Large Language Model. 2024. 2024arXiv preprint</p>
<p>Yiming Yao, Fei Liu, Ji Cheng, Qingfu Zhang, arXiv:2404.16906Evolve Cost-aware Acquisition Functions Using Large Language Models. 2024. 2024arXiv preprint</p>
<p>Wang Yatong, Pei Yuchen, Zhao Yuqi, arXiv:2409.09063TS-EoH: An Edge Server Task Scheduling Algorithm Based on Evolution of Heuristic. 2024. 2024arXiv preprint</p>
<p>De novo drug design as GPT language modeling: large chemistry models with supervised and reinforcement learning. Gavin Ye, Journal of Computer-Aided Molecular Design. 38202024. 2024</p>
<p>Geyan Ye, Xibao Cai, Houtim Lai, Xing Wang, Junhong Huang, Longyue Wang, Wei Liu, Xiangxiang Zeng, arXiv:2401.10334Drugassist: A large language model for molecule optimization. 2023. 2023arXiv preprint</p>
<p>Large language models as hyper-heuristics for combinatorial optimization. Haoran Ye, Jiarui Wang, Zhiguang Cao, Federico Berto, Chuanbo Hua, Haeyeon Kim, Jinkyoo Park, Guojie Song, arXiv:2402.011452024. 2024arXiv preprint</p>
<p>Haoran Ye, Jiarui Wang, Zhiguang Cao, Guojie Song, arXiv:2402.01145ReEvo: Large Language Models as Hyper-Heuristics with Reflective Evolution. 2024. 2024arXiv preprint</p>
<p>Language to rewards for robotic skill synthesis. Wenhao Yu, Nimrod Gileadi, Chuyuan Fu, Sean Kirmani, Kuang-Huei Lee, Montse Gonzalez Arenas, Lewis Hao-Tien, Tom Chiang, Leonard Erez, Hasenclever, arXiv:2306.08647Jan Humplik, et al. 2023. 2023arXiv preprint</p>
<p>tnGPS: Discovering Unknown Tensor Network Structure Search Algorithms via Large Language Models (LLMs). Junhua Zeng, Chao Li, Zhun Sun, Qibin Zhao, Guoxu Zhou, Forty-first International Conference on Machine Learning, ICML. 2024</p>
<p>Large Language Models in Wireless Application Design: In-Context Learning-enhanced Automatic Network Intrusion Detection. Han Zhang, Akram Bin Sediq, Ali Afana, Melike Erol-Kantarci, arXiv:2405.110022024. 2024arXiv preprint</p>
<p>Huan Zhang, Yu Song, Ziyu Hou, Santiago Miret, Bang Liu, arXiv:2409.00135HoneyComb: A Flexible LLM-Based Agent System for Materials Science. 2024. 2024arXiv preprint</p>
<p>Using large language models for hyperparameter optimization. Nishkrit Michael R Zhang, Juhan Desai, Jonathan Bae, Jimmy Lorraine, Ba, NeurIPS 2023 Foundation Models for Decision Making Workshop. 2023</p>
<p>Ruohong Zhang, Liangke Gui, Zhiqing Sun, Yihao Feng, Keyang Xu, Yuanhan Zhang, Di Fu, Chunyuan Li, Alexander Hauptmann, Yonatan Bisk, arXiv:2404.01258Direct Preference Optimization of Video Large Multimodal Models from Language Model Reward. 2024. 2024arXiv preprint</p>
<p>Understanding the Importance of Evolutionary Search in Automated Heuristic Design with Large Language Models. Rui Zhang, Fei Liu, Xi Lin, Zhenkun Wang, Zhichao Lu, Qingfu Zhang, International Conference on Parallel Problem Solving from Nature. Springer2024</p>
<p>AutoAlign: Fully Automatic and Effective Knowledge Graph Alignment Enabled by Large Language Models. Rui Zhang, Yixin Su, Xiaoyan Bayu Distiawan Trisedya, Min Zhao, Hong Yang, Jianzhong Cheng, Qi, IEEE Trans. Knowl. Data Eng. 2024. 2024</p>
<p>Shenao Zhang, Sirui Zheng, Shuqi Ke, Zhihan Liu, Wanxin Jin, Jianbo Yuan, Yingxiang Yang, Hongxia Yang, Zhaoran Wang, arXiv:2402.16181How Can LLM Guide RL? A Value-Based Approach. 2024. 2024arXiv preprint</p>
<p>AutoTurb: Using Large Language Models for Automatic Algebraic Model Discovery of Turbulence Closure. Yu Zhang, Kefeng Zheng, Fei Liu, Qingfu Zhang, Zhenkun Wang, arXiv:2410.106572024. 2024arXiv preprint</p>
<p>Revolutionizing finance with llms: An overview of applications and insights. Huaqin Zhao, Zhengliang Liu, Zihao Wu, Yiwei Li, Tianze Yang, Peng Shu, Shaochen Xu, Haixing Dai, Lin Zhao, Gengchen Mai, arXiv:2401.116412024. 2024arXiv preprint</p>
<p>Kun Wayne Xin Zhao, Junyi Zhou, Tianyi Li, Xiaolei Tang, Yupeng Wang, Yingqian Hou, Beichen Min, Junjie Zhang, Zican Zhang, Dong, arXiv:2303.18223A survey of large language models. 2023. 2023arXiv preprint</p>
<p>Ruizhe Zhong, Xingbo Du, Shixiong Kai, Zhentao Tang, Siyuan Xu, Hui-Ling Zhen, Jianye Hao, Qiang Xu, Mingxuan Yuan, Junchi Yan, arXiv:2401.12224Llm4eda: Emerging progress in large language models for electronic design automation. 2023. 2023arXiv preprint</p>
<p>Hao Zhou, Chengming Hu, Xue Liu, arXiv:2405.17439An Overview of Machine Learning-Enabled Optimization for Reconfigurable Intelligent Surfaces-Aided 6G Networks: From Reinforcement Learning to Large Language Models. 2024. 2024arXiv preprint</p>
<p>Design Principle Transfer in Neural Architecture Search via Large Language Models. Xun Zhou, Liang Feng, Xingyu Wu, Zhichao Lu, Kay Chen, Tan , arXiv:2408.113302024. 2024arXiv preprint</p>
<p>Large language models are human-level prompt engineers. Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han, Keiran Paster, Silviu Pitis, Harris Chan, Jimmy Ba, arXiv:2211.019102022. 2022arXiv preprint</p>
<p>Max Zhu, Adrián Bazaga, Pietro Liò, arXiv:2406.04501FLUID-LLM: Learning Computational Fluid Dynamics with Spatiotemporal-aware Large Language Models. 2024. 2024arXiv preprint</p>
<p>GPTSwarm: Language Agents as Optimizable Graphs. Mingchen Zhuge, Wenyi Wang, Louis Kirsch, Francesco Faccio, Dmitrii Khizbullin, Jürgen Schmidhuber, Forty-first International Conference on Machine Learning. 2024</p>            </div>
        </div>

    </div>
</body>
</html>
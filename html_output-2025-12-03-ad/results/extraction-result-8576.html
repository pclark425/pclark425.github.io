<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8576 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8576</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8576</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-154.html">extraction-schema-154</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-270219176</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2406.00284v2.pdf" target="_blank">A Closer Look at Tool-based Logical Reasoning with LLMs: The Choice of Tool Matters</a></p>
                <p><strong>Paper Abstract:</strong> The emergence of Large Language Models (LLMs) has demonstrated promising progress in solving logical reasoning tasks effectively. Several recent approaches have proposed to change the role of the LLM from the reasoner into a translator between natural language statements and symbolic representations which are then sent to external symbolic solvers to resolve. This paradigm has established the current state-of-the-art result in logical reasoning (i.e., deductive reasoning). However, it remains unclear whether the variance in performance of these approaches stems from the methodologies employed or the specific symbolic solvers utilized. There is a lack of consistent comparison between symbolic solvers and how they influence the overall reported performance. This is important, as each symbolic solver also has its own input symbolic language, presenting varying degrees of challenge in the translation process. To address this gap, we perform experiments on 3 deductive reasoning benchmarks with LLMs augmented with widely used symbolic solvers: Z3, Pyke, and Prover9. The tool-executable rates of symbolic translation generated by different LLMs exhibit a near 50% performance variation. This highlights a significant difference in performance rooted in very basic choices of tools. The almost linear correlation between the executable rate of translations and the accuracy of the outcomes from Prover9 highlight a strong alignment between LLMs ability to translate into Prover9 symbolic language, and the correctness of those translations.</p>
                <p><strong>Cost:</strong> 0.013</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8576.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8576.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A state-of-the-art large language model (OpenAI family) used in the paper as a representative high-capability LLM for translation-to-symbolic formats and tool-augmented logical reasoning experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>A Closer Look at Tool-based Logical Reasoning with LLMs: The Choice of Tool Matters</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>High-capability commercial LLM from the GPT family; used in experiments as a translator from natural language premises/questions into symbolic input for external theorem provers/SMT engines.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>ProofWriter, PrOntoQA (hard fictional 5-hop), FOLIO</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Deductive logical reasoning benchmarks requiring first-order logic style reasoning: ProofWriter (synthetic deductive proofs under OWA/CWA across varying hop depths), PrOntoQA (synthetic multi-hop deductive reasoning, closed-world T/F), and FOLIO (human-authored challenging FOL problems with real-world language).</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Tool-based method: one-shot prompting of LLM to translate natural language premises/questions into solver-specific symbolic formats (Z3, Prover9, Pyke), then execute with the external solver. No self-refinement or random guessing was used; experiments included varying demonstration shots (1, 2, 4) for FOLIO.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Varies strongly by solver and dataset. Representative numbers reported in the paper: on ProofWriter (avg. OWA) GPT-4o achieved Z3 ExecR=75.00% / Acc=74.17%, Prover9 ExecR=97.33% / Acc=95.67%, Pyke ExecR=99.83% / Acc=79.17%. On PrOntoQA GPT-4o achieved Z3 96.00% / 96.00%, Prover9 100.00% / 100.00% (high). On FOLIO (1-shot) best result reported: GPT-4o + Prover9 achieved 66.5% accuracy (paper notes best FOLIO = 66.5% with GPT-4o & Prover9). Combined across datasets the paper reports GPT-4o combined: Z3 ExecR ≈ 74.3% / Acc ≈ 73.5%; Prover9 ExecR ≈ 94.1% / Acc ≈ 91.7%; Pyke ExecR ≈ 99.9% / Acc ≈ 85.5%.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared across tools (same prompt and shots): Prover9 and Z3 substantially outperform Pyke on harder FOL tasks; Prover9 often has the highest executable rate and accuracy for GPT-4o. The paper excluded self-refinement baselines to isolate tool effects. The authors report up-to ~50% performance variation for the same LLM under different tools.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Failures stem from translation (parse) errors, runtime/solver execution errors, and solver-specific limitations: ambiguous natural-language leading to wrong connective selection (e.g., XOR vs Not(And(...))), missing reasoning chain in premises (tool-based approach fails when chain incomplete), syntax/bracketing mistakes produced by LLMs (e.g., Forall() missing bracket), Pyke lacks expressive constructs like XOR so cannot handle some FOLIO cases, and small token-level errors cause complete solver failure. The paper documents parse errors, execution errors, and a Z3-specific 'execution exception' case (both a formula and its negation reported true due to inconsistent facts).</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Main conclusion: the choice of external symbolic tool strongly determines downstream performance — tool choice accounts for large performance variance. Prover9 tends to be easier for LLMs to produce executable translations for and shows strong alignment between executable translation rate and correctness; Z3 provides useful error feedback and flexibility for defining additional rules but has lower exec rates in some settings; Pyke's restricted syntax makes it brittle on complex FOL benchmarks. Tool-based approaches guarantee logical grounding when translation is correct but are unforgiving to small translation errors and to tasks missing explicit premises.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Closer Look at Tool-based Logical Reasoning with LLMs: The Choice of Tool Matters', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8576.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8576.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5-Turbo</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3.5-Turbo</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A widely used OpenAI LLM evaluated as a translator in tool-augmented logical reasoning experiments against several symbolic solvers and deductive benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>A Closer Look at Tool-based Logical Reasoning with LLMs: The Choice of Tool Matters</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5-Turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A strong commercial LLM (earlier GPT family generation) used as a comparative translator into solver formats; behaves differently from GPT-4o in error patterns and solver preferences.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>ProofWriter, PrOntoQA (5-hop hardest), FOLIO</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Same deductive/FOL benchmarks requiring multi-hop reasoning and precise FOL translation.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>One-shot translation to solver-specific formats (Z3, Prover9, Pyke) and execution by those solvers; experiments avoid self-refinement; shot count varied for FOLIO.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Reported numbers show variability by tool: Example ProofWriter (avg. OWA) GPT-3.5-Turbo achieved Z3 ExecR=84.83% / Acc=82.88%, Prover9 ExecR=90.67% / Acc=87.00%, Pyke ExecR=62.83% / Acc=53.33%. Combined across datasets the paper reports GPT-3.5 combined performance roughly: Z3 ExecR ≈ 80.5% with Acc ≈78.8%; Prover9 combined ExecR ≈ 78.8%/Acc ≈ 86.1% (table shows variations); on FOLIO GPT-3.5 had low exec/acc (e.g., FOLIO 1-shot Z3 ≈29.0% exec / 31.0% acc; Prover9 ≈24.49%/25.50% in one table slice).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared to GPT-4o, GPT-3.5-Turbo typically attains lower accuracy and demonstrates different tool preferences (paper notes GPT-3.5 shifts preference from Prover9 to Z3 depending on dataset/setting). No self-refinement baseline was used; comparisons are across identical prompts and tools.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Similar failure modes: parse errors and bracket/syntax mistakes (notably consistent bracket issues in Forall() during some runs), mis-interpretation of exclusive-or language, and reduced performance on human-authored FOLIO. Paper notes GPT-3.5 sometimes fails to add a closing bracket for Forall(), causing drastic drop in exec rate.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>LLM model version affects which external solver it pairs best with; older/smaller models may prefer Z3 over Prover9 in some settings. Correctness depends both on translation quality and solver expressivity; simple syntactic slip-ups by the LLM induce solver failures.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Closer Look at Tool-based Logical Reasoning with LLMs: The Choice of Tool Matters', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8576.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8576.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Gemini-1.0-Pro</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Gemini-1.0-Pro</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A high-capability multimodal LLM (Gemini family) used as a translator in the paper's tool-based logical reasoning experiments to compare translation/execution performance across solvers.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>A Closer Look at Tool-based Logical Reasoning with LLMs: The Choice of Tool Matters</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Gemini-1.0-Pro</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Representative high-capability model from the Gemini family; evaluated as a translator into solver input and compared with other LLMs across tasks and solvers.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>ProofWriter, PrOntoQA (5-hop), FOLIO</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Same deductive and FOL reasoning benchmarks: multi-hop, first-order logic reasoning with natural language premises.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Same tool-based translation approach: one-shot demonstrations to produce solver-specific input for Z3, Prover9, Pyke; execution measured by exec rate and accuracy; FOLIO experiments included multiple shot counts.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Reported as generally weaker than GPT models on accuracy despite sometimes high exec rates: e.g., ProofWriter (avg. OWA) Gemini ExecR/Acc: Z3 93.00% / 91.00%, Prover9 86.83% / 62.50%, Pyke 49.33% / 36.67% (table excerpt). Combined results indicate Gemini often attains high exec rates but lower correct-output rates for some solvers (suggesting executable-but-incorrect translations).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared to GPT-4o and GPT-3.5, Gemini sometimes shows high executable translation rates but lower end-to-end accuracy, highlighting that an executable translation need not be correct. The paper compared identical prompting across models to isolate tool effects.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Gemini exhibits more cases where translations are executable but lead to incorrect solver outputs (the paper emphasizes inability to get corrective feedback from executables). It also suffers from parse and predicate extraction errors on FOLIO (complex predicates).</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>High executable rate alone is not sufficient; the paper finds that for Prover9 a high executable rate correlates strongly with correctness, but for Gemini this alignment is weaker. This underscores the importance of both translation fidelity and solver semantics.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Closer Look at Tool-based Logical Reasoning with LLMs: The Choice of Tool Matters', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8576.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8576.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Command R+</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Cohere Command R Plus (command-r-plus)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A commercial command-style language model (Cohere's Command R Plus) used in the paper as another representative LLM to evaluate translation-to-solver performance on deductive benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>A Closer Look at Tool-based Logical Reasoning with LLMs: The Choice of Tool Matters</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>command-r-plus</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Cohere's high-capability instruction-following model used to produce solver-specific translations in one-shot experiments; required small prompt adjustments because it tends to produce full executable code.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>ProofWriter, PrOntoQA, FOLIO</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Same set of deductive/FOL benchmarks; tasks require translation to solver input and solver execution to obtain true/false/unknown conclusions.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>One-shot translation into solver formats (Z3, Prover9, Pyke) with minimal prompt adjustments to discourage producing full executable scaffolding; no self-refinement.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Mixed performance: on ProofWriter (avg. OWA) reported numbers include Z3 ExecR=88.67% / Acc=87.00%, Prover9 ExecR=61.33% / Acc=56.66%, Pyke ExecR=61.83% / Acc=51.50% for one table slice. On PrOntoQA and FOLIO performance drops in some configurations; FOLIO 1-shot for command-r-plus in table excerpt shows low exec/acc (e.g., 25.50% exec / 19.00% acc for Z3 in one slice).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared to other LLMs, command-r-plus sometimes shifts its solver preference (paper notes Command R+ changes preference to Pyke in fictional setting). Overall it underperforms GPT-4o on many tasks in this study.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Tends to produce full executable code rather than the expected translation format (requiring prompt adjustments). Exhibits bracket and punctuation inconsistencies in some runs (e.g., opening bracket instead of closing), and suffers the same parse vs execution error failure modes as other LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Different LLM families have distinct error profiles and solver preferences; prompt engineering (to avoid full-code output) is sometimes needed. Model-tool pairing must be chosen carefully; command-r-plus was less robust on FOLIO and synthetic hard benchmarks compared to GPT-4o.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Closer Look at Tool-based Logical Reasoning with LLMs: The Choice of Tool Matters', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Logic-lm: Empowering large language models with symbolic solvers for faithful logical reasoning <em>(Rating: 2)</em></li>
                <li>PAL: program-aided language models <em>(Rating: 2)</em></li>
                <li>FOLIO: natural language reasoning with first-order logic <em>(Rating: 2)</em></li>
                <li>ProofWriter: Generating and Explaining Proofs in Natural Language for Deductive Reasoning <em>(Rating: 2)</em></li>
                <li>PrOntoQA: Testing the general deductive reasoning capacity of large language models <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8576",
    "paper_id": "paper-270219176",
    "extraction_schema_id": "extraction-schema-154",
    "extracted_data": [
        {
            "name_short": "GPT-4o",
            "name_full": "GPT-4o",
            "brief_description": "A state-of-the-art large language model (OpenAI family) used in the paper as a representative high-capability LLM for translation-to-symbolic formats and tool-augmented logical reasoning experiments.",
            "citation_title": "A Closer Look at Tool-based Logical Reasoning with LLMs: The Choice of Tool Matters",
            "mention_or_use": "use",
            "model_name": "GPT-4o",
            "model_description": "High-capability commercial LLM from the GPT family; used in experiments as a translator from natural language premises/questions into symbolic input for external theorem provers/SMT engines.",
            "model_size": null,
            "reasoning_task_name": "ProofWriter, PrOntoQA (hard fictional 5-hop), FOLIO",
            "reasoning_task_description": "Deductive logical reasoning benchmarks requiring first-order logic style reasoning: ProofWriter (synthetic deductive proofs under OWA/CWA across varying hop depths), PrOntoQA (synthetic multi-hop deductive reasoning, closed-world T/F), and FOLIO (human-authored challenging FOL problems with real-world language).",
            "method_or_approach": "Tool-based method: one-shot prompting of LLM to translate natural language premises/questions into solver-specific symbolic formats (Z3, Prover9, Pyke), then execute with the external solver. No self-refinement or random guessing was used; experiments included varying demonstration shots (1, 2, 4) for FOLIO.",
            "performance": "Varies strongly by solver and dataset. Representative numbers reported in the paper: on ProofWriter (avg. OWA) GPT-4o achieved Z3 ExecR=75.00% / Acc=74.17%, Prover9 ExecR=97.33% / Acc=95.67%, Pyke ExecR=99.83% / Acc=79.17%. On PrOntoQA GPT-4o achieved Z3 96.00% / 96.00%, Prover9 100.00% / 100.00% (high). On FOLIO (1-shot) best result reported: GPT-4o + Prover9 achieved 66.5% accuracy (paper notes best FOLIO = 66.5% with GPT-4o & Prover9). Combined across datasets the paper reports GPT-4o combined: Z3 ExecR ≈ 74.3% / Acc ≈ 73.5%; Prover9 ExecR ≈ 94.1% / Acc ≈ 91.7%; Pyke ExecR ≈ 99.9% / Acc ≈ 85.5%.",
            "baseline_comparison": "Compared across tools (same prompt and shots): Prover9 and Z3 substantially outperform Pyke on harder FOL tasks; Prover9 often has the highest executable rate and accuracy for GPT-4o. The paper excluded self-refinement baselines to isolate tool effects. The authors report up-to ~50% performance variation for the same LLM under different tools.",
            "limitations_or_failures": "Failures stem from translation (parse) errors, runtime/solver execution errors, and solver-specific limitations: ambiguous natural-language leading to wrong connective selection (e.g., XOR vs Not(And(...))), missing reasoning chain in premises (tool-based approach fails when chain incomplete), syntax/bracketing mistakes produced by LLMs (e.g., Forall() missing bracket), Pyke lacks expressive constructs like XOR so cannot handle some FOLIO cases, and small token-level errors cause complete solver failure. The paper documents parse errors, execution errors, and a Z3-specific 'execution exception' case (both a formula and its negation reported true due to inconsistent facts).",
            "insights_or_conclusions": "Main conclusion: the choice of external symbolic tool strongly determines downstream performance — tool choice accounts for large performance variance. Prover9 tends to be easier for LLMs to produce executable translations for and shows strong alignment between executable translation rate and correctness; Z3 provides useful error feedback and flexibility for defining additional rules but has lower exec rates in some settings; Pyke's restricted syntax makes it brittle on complex FOL benchmarks. Tool-based approaches guarantee logical grounding when translation is correct but are unforgiving to small translation errors and to tasks missing explicit premises.",
            "uuid": "e8576.0",
            "source_info": {
                "paper_title": "A Closer Look at Tool-based Logical Reasoning with LLMs: The Choice of Tool Matters",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "GPT-3.5-Turbo",
            "name_full": "GPT-3.5-Turbo",
            "brief_description": "A widely used OpenAI LLM evaluated as a translator in tool-augmented logical reasoning experiments against several symbolic solvers and deductive benchmarks.",
            "citation_title": "A Closer Look at Tool-based Logical Reasoning with LLMs: The Choice of Tool Matters",
            "mention_or_use": "use",
            "model_name": "GPT-3.5-Turbo",
            "model_description": "A strong commercial LLM (earlier GPT family generation) used as a comparative translator into solver formats; behaves differently from GPT-4o in error patterns and solver preferences.",
            "model_size": null,
            "reasoning_task_name": "ProofWriter, PrOntoQA (5-hop hardest), FOLIO",
            "reasoning_task_description": "Same deductive/FOL benchmarks requiring multi-hop reasoning and precise FOL translation.",
            "method_or_approach": "One-shot translation to solver-specific formats (Z3, Prover9, Pyke) and execution by those solvers; experiments avoid self-refinement; shot count varied for FOLIO.",
            "performance": "Reported numbers show variability by tool: Example ProofWriter (avg. OWA) GPT-3.5-Turbo achieved Z3 ExecR=84.83% / Acc=82.88%, Prover9 ExecR=90.67% / Acc=87.00%, Pyke ExecR=62.83% / Acc=53.33%. Combined across datasets the paper reports GPT-3.5 combined performance roughly: Z3 ExecR ≈ 80.5% with Acc ≈78.8%; Prover9 combined ExecR ≈ 78.8%/Acc ≈ 86.1% (table shows variations); on FOLIO GPT-3.5 had low exec/acc (e.g., FOLIO 1-shot Z3 ≈29.0% exec / 31.0% acc; Prover9 ≈24.49%/25.50% in one table slice).",
            "baseline_comparison": "Compared to GPT-4o, GPT-3.5-Turbo typically attains lower accuracy and demonstrates different tool preferences (paper notes GPT-3.5 shifts preference from Prover9 to Z3 depending on dataset/setting). No self-refinement baseline was used; comparisons are across identical prompts and tools.",
            "limitations_or_failures": "Similar failure modes: parse errors and bracket/syntax mistakes (notably consistent bracket issues in Forall() during some runs), mis-interpretation of exclusive-or language, and reduced performance on human-authored FOLIO. Paper notes GPT-3.5 sometimes fails to add a closing bracket for Forall(), causing drastic drop in exec rate.",
            "insights_or_conclusions": "LLM model version affects which external solver it pairs best with; older/smaller models may prefer Z3 over Prover9 in some settings. Correctness depends both on translation quality and solver expressivity; simple syntactic slip-ups by the LLM induce solver failures.",
            "uuid": "e8576.1",
            "source_info": {
                "paper_title": "A Closer Look at Tool-based Logical Reasoning with LLMs: The Choice of Tool Matters",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Gemini-1.0-Pro",
            "name_full": "Gemini-1.0-Pro",
            "brief_description": "A high-capability multimodal LLM (Gemini family) used as a translator in the paper's tool-based logical reasoning experiments to compare translation/execution performance across solvers.",
            "citation_title": "A Closer Look at Tool-based Logical Reasoning with LLMs: The Choice of Tool Matters",
            "mention_or_use": "use",
            "model_name": "Gemini-1.0-Pro",
            "model_description": "Representative high-capability model from the Gemini family; evaluated as a translator into solver input and compared with other LLMs across tasks and solvers.",
            "model_size": null,
            "reasoning_task_name": "ProofWriter, PrOntoQA (5-hop), FOLIO",
            "reasoning_task_description": "Same deductive and FOL reasoning benchmarks: multi-hop, first-order logic reasoning with natural language premises.",
            "method_or_approach": "Same tool-based translation approach: one-shot demonstrations to produce solver-specific input for Z3, Prover9, Pyke; execution measured by exec rate and accuracy; FOLIO experiments included multiple shot counts.",
            "performance": "Reported as generally weaker than GPT models on accuracy despite sometimes high exec rates: e.g., ProofWriter (avg. OWA) Gemini ExecR/Acc: Z3 93.00% / 91.00%, Prover9 86.83% / 62.50%, Pyke 49.33% / 36.67% (table excerpt). Combined results indicate Gemini often attains high exec rates but lower correct-output rates for some solvers (suggesting executable-but-incorrect translations).",
            "baseline_comparison": "Compared to GPT-4o and GPT-3.5, Gemini sometimes shows high executable translation rates but lower end-to-end accuracy, highlighting that an executable translation need not be correct. The paper compared identical prompting across models to isolate tool effects.",
            "limitations_or_failures": "Gemini exhibits more cases where translations are executable but lead to incorrect solver outputs (the paper emphasizes inability to get corrective feedback from executables). It also suffers from parse and predicate extraction errors on FOLIO (complex predicates).",
            "insights_or_conclusions": "High executable rate alone is not sufficient; the paper finds that for Prover9 a high executable rate correlates strongly with correctness, but for Gemini this alignment is weaker. This underscores the importance of both translation fidelity and solver semantics.",
            "uuid": "e8576.2",
            "source_info": {
                "paper_title": "A Closer Look at Tool-based Logical Reasoning with LLMs: The Choice of Tool Matters",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Command R+",
            "name_full": "Cohere Command R Plus (command-r-plus)",
            "brief_description": "A commercial command-style language model (Cohere's Command R Plus) used in the paper as another representative LLM to evaluate translation-to-solver performance on deductive benchmarks.",
            "citation_title": "A Closer Look at Tool-based Logical Reasoning with LLMs: The Choice of Tool Matters",
            "mention_or_use": "use",
            "model_name": "command-r-plus",
            "model_description": "Cohere's high-capability instruction-following model used to produce solver-specific translations in one-shot experiments; required small prompt adjustments because it tends to produce full executable code.",
            "model_size": null,
            "reasoning_task_name": "ProofWriter, PrOntoQA, FOLIO",
            "reasoning_task_description": "Same set of deductive/FOL benchmarks; tasks require translation to solver input and solver execution to obtain true/false/unknown conclusions.",
            "method_or_approach": "One-shot translation into solver formats (Z3, Prover9, Pyke) with minimal prompt adjustments to discourage producing full executable scaffolding; no self-refinement.",
            "performance": "Mixed performance: on ProofWriter (avg. OWA) reported numbers include Z3 ExecR=88.67% / Acc=87.00%, Prover9 ExecR=61.33% / Acc=56.66%, Pyke ExecR=61.83% / Acc=51.50% for one table slice. On PrOntoQA and FOLIO performance drops in some configurations; FOLIO 1-shot for command-r-plus in table excerpt shows low exec/acc (e.g., 25.50% exec / 19.00% acc for Z3 in one slice).",
            "baseline_comparison": "Compared to other LLMs, command-r-plus sometimes shifts its solver preference (paper notes Command R+ changes preference to Pyke in fictional setting). Overall it underperforms GPT-4o on many tasks in this study.",
            "limitations_or_failures": "Tends to produce full executable code rather than the expected translation format (requiring prompt adjustments). Exhibits bracket and punctuation inconsistencies in some runs (e.g., opening bracket instead of closing), and suffers the same parse vs execution error failure modes as other LLMs.",
            "insights_or_conclusions": "Different LLM families have distinct error profiles and solver preferences; prompt engineering (to avoid full-code output) is sometimes needed. Model-tool pairing must be chosen carefully; command-r-plus was less robust on FOLIO and synthetic hard benchmarks compared to GPT-4o.",
            "uuid": "e8576.3",
            "source_info": {
                "paper_title": "A Closer Look at Tool-based Logical Reasoning with LLMs: The Choice of Tool Matters",
                "publication_date_yy_mm": "2024-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Logic-lm: Empowering large language models with symbolic solvers for faithful logical reasoning",
            "rating": 2,
            "sanitized_title": "logiclm_empowering_large_language_models_with_symbolic_solvers_for_faithful_logical_reasoning"
        },
        {
            "paper_title": "PAL: program-aided language models",
            "rating": 2,
            "sanitized_title": "pal_programaided_language_models"
        },
        {
            "paper_title": "FOLIO: natural language reasoning with first-order logic",
            "rating": 2,
            "sanitized_title": "folio_natural_language_reasoning_with_firstorder_logic"
        },
        {
            "paper_title": "ProofWriter: Generating and Explaining Proofs in Natural Language for Deductive Reasoning",
            "rating": 2,
            "sanitized_title": "proofwriter_generating_and_explaining_proofs_in_natural_language_for_deductive_reasoning"
        },
        {
            "paper_title": "PrOntoQA: Testing the general deductive reasoning capacity of large language models",
            "rating": 2,
            "sanitized_title": "prontoqa_testing_the_general_deductive_reasoning_capacity_of_large_language_models"
        }
    ],
    "cost": 0.013354499999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>A Closer Look at Tool-based Logical Reasoning with LLMs: The Choice of Tool Matters
11 Jul 2024</p>
<p>Long Hei 
DSAI
Monash University</p>
<p>Matthew Lam 
DSAI
Monash University</p>
<p>Ramya Keerthy 
DSAI
Monash University</p>
<p>Thatikonda Ehsan ramya.thatikonda1@monash.edu 
DSAI
Monash University</p>
<p>A Closer Look at Tool-based Logical Reasoning with LLMs: The Choice of Tool Matters
11 Jul 2024E5F7884B328C4600167E2852182EBE8DarXiv:2406.00284v2[cs.CL]
The emergence of Large Language Models (LLMs) has demonstrated promising progress in solving logical reasoning tasks effectively.Several recent approaches have proposed to change the role of the LLM from the reasoner into a translator between natural language statements and symbolic representations which are then sent to external symbolic solvers to resolve.This paradigm has established the current state-of-the-art result in logical reasoning (i.e., deductive reasoning).However, it remains unclear whether the variance in performance of these approaches stems from the methodologies employed or the specific symbolic solvers utilized.There is a lack of consistent comparison between symbolic solvers and how they influence the overall reported performance.This is important, as each symbolic solver also has its own input symbolic language, presenting varying degrees of challenge in the translation process.To address this gap, we perform experiments on 3 deductive reasoning benchmarks with LLMs augmented with widely used symbolic solvers: Z3, Pyke, and Prover9.The tool-executable rates of symbolic translation generated by different LLMs exhibit a near 50% performance variation.This highlights a significant difference in performance rooted in very basic choices of tools.The almost linear correlation between the executable rate of translations and the accuracy of the outcomes from Prover9 highlight a strong alignment between LLMs ability to translate into Prover9 symbolic language, and the correctness of those translations. 1</p>
<p>Introduction</p>
<p>The recent state-of-the-art approaches to logical reasoning have combined Large Language Models (LLMs) with external symbolic mechanisms (Nye et al., 2021;Pan et al., 2023;Ye et al., 2023;Gao et al., 2023;Lyu et al., 2023).This approach leverages LLMs' remarkable proficiency in translating natural language into symbolic representation such as First Order Logic (FOL) or symbolic solvers' specified language (e.g., Pyke, Z3) (Yang et al., 2023), and the symbolic solver's ability to execute these translations through a fully deterministic proof process (Metaxiotis et al., 2002).These existing published methods try a variety of tools and tool-specific formalism.Table 1 summarises various tools used in recent state-of-the-art studies.This variability of tools makes it impossible to have a fair understanding of each approach.There is currently a lack of consistent comparison that will allow others to understand better where this performance gain stems from.</p>
<p>In this paper, we take 3 widely used tools: Z3 (de Moura and Bjørner, 2008), Pyke (Frederiksen, 2008), and Prover9 (McCune, 2005) and analyse the difficulty LLMs face for translating natural language into their desired input format, and the internal capability of these tools at solving certain satisfiability tasks.We select GPT4o, GPT-3.5-Turbo (OpenAI, 2023), Gemini-1.0-Pro(Team et al., 2023) and Cohere Command R Plus, as representatives of the most capable family of LLMs, along with 3 widely used deductive reasoning benchmarks ProofWriter (Tafjord et al., 2021), FO-LIO (Han et al., 2022), and ProntoQA (Saparov and He, 2023).We conduct a fair side-by-side comparison of tools by trying various number of identical prompts, demonstration shots, and minimal adjustment for each solver.</p>
<p>Our findings indicate that LLMs find it easier to translate for Prover9, followed by Z3, and lastly Pyke.Although Prover9 can solve more questions accurately, Prover9 demonstrates a lower discrepancy between execution rate and overall accuracy.This means that Prover9 is more likely to solve a question given the right syntax and format produced by LLMs.Overall, Z3 and Prover9 are all</p>
<p>Solver</p>
<p>Dataset</p>
<p>Papers Problem Z3 AR-LSAT (Zhong et al., 2022), ProntoQA (Saparov and He, 2023), ProofWriter (Tafjord et al., 2021), BoardgameQA (Kazemi et al., 2023) LogicLM, SatLM Analytical, Deductive, FOL Pyke ProntoQA (Saparov and He, 2023), ProofWriter (Tafjord et al., 2021) LogicLM, Logical Solver</p>
<p>Deductive, FOL</p>
<p>Prover9 FOLIO (Han et al., 2022) LogicLM, LINC</p>
<p>Deductive, FOL</p>
<p>Table 1: A summary of the symbolic solvers and the datasets it has solved in different studies: LogicLM (Pan et al., 2023), LINC (Olausson et al., 2023), Logical Solver (Feng et al., 2023), andSatLM (Ye et al., 2023).</p>
<p>competitive options, Pyke's performance is significantly inferior and only comparable to the other tools in solving PrOntoQA.Our experiments across 3 benchmarks (based on the accuracy of outputs) highlight an up-to 50% of performance variation for each LLM under different tools, and well as the performance change for each tool under different LLMs.</p>
<p>Tools &amp; Logical Reasoning with LLMs</p>
<p>The tool-based approaches to logical reasoning combine LLMs with external symbolic solvers.This synergy harnesses the capability of LLMs to convert diverse natural language statements into logical symbolic formalism.While being less flexible compared with free-form reasoning methods, such as Chain-of-Thought (Wei et al., 2022), the tool-based approach, given a correct formal translation, has important advantages: logical coherence during the reasoning (i.e., unlike LLMs, theorem provers cannot make reasoning shortcuts or hallucinate) is guaranteed, while the internal proof trace of the theorem provers offers a transparent and verifiable reasoning chain.</p>
<p>Logical Solvers</p>
<p>Automated theorem provers (ATPs) and Satisfiability Modulo Theories (SMT) solvers are tools equipped with built-in functions designed to assist in logical reasoning tasks.These solvers can vary in syntax, proof search strategies, theorem automation, and complexity.ATPs efficiently resolve first order logic problems without external interaction.SMT solvers closely resemble ATPs in solving first-order formulae but add complexity by handling theories such as equality, arrays, and bitvectors.Logical solvers, specifically Z3, Prover9, and Pyke, are used for logical reasoning tasks with LLMs due to their ease of use in a Python environment (Pan et al., 2023;Ye et al., 2023).We study the logical solvers based on their ability to handle first-order logic and explore the crucial differences in external syntax and internal theories of these tools.In this context, we define the task as follows: given a set of premises P ∈ {P 1 , P 2 , . . ., P n }, the objective is to determine whether the conclusion C logically follows from these premises.The translation syntax for each tool is presented in Figure1.In Python, Prover9 is accessible through the NLTK logic library.</p>
<p>Pyke short for Python Knowledge Engine, is a solver used for building and executing rulebased expert systems (Frederiksen, 2008).Although pyke is used for optimizing software development, Pan et al. (2023) demonstrated its application in solving a first-order logic problem.Given a logical inference task, Pyke establishes a knowledge base and incorporates known facts (fact.kfb)and rules (rule.krb)from the input, i.e., P − → (P facts , P rules ).The conclusion is parsed as a rule that is propagated through the knowledge base until it reaches a resolution.The predicates in the first order logic are treated as facts and are connected to form rules.Given its limited syntax, Pyke supports simple connectives such as 'and', 'or', and 'implies'.The free variables (e.g., $x) are generally considered to be universal quantifiers, thus restricting the use of existential quantifiers.Due to these limitations, Pyke may not adequately handle complex tasks involving first-order logic, such as FOLIO.However, it remains well-suited for rule-based tasks like ProofWriter and ProntoQA.</p>
<p>Free-form Logical Reasoning with LLMs</p>
<p>The free-form approaches to reasoning rely on LLMs' internal capabilities via various mechanisms to help improve LLM's performance in logical reasoning.For example, prompts that encourage LLMs to solve tasks in a Chain-of-Thought approach is a general technique that enhances LLM's performance (Wei et al., 2022;Kojima et al., 2022).</p>
<p>Despite the promising outcomes, this approach falls short when dealing with complex logical reasoning tasks.This limitation stems from the lack of explicit logical grounding and the inherent ambiguous and nuanced nature of natural language.Recent studies have revisited Formal Logic to address this challenge.Han et al. (2022) shows that incorporating first-order logic (FOL) translations into the context can notably enhance LLM's performance.Feng et al. (2023) emulates the reasoning processes of an automated theorem solver (Pyke) through solving Logical tasks using the tool-based approach and training LLMs on Pyke's reasoning steps.The free-form approach capitalises on the inherent capabilities of LLM to learn complex logical rules.However, this approach solely relies upon LLM's logical reasoning prowess and is susceptible to issues such as hallucinations and taking shortcuts (Dasgupta et al., 2022;Ji et al., 2023).To address this issue, recent approaches aim to augment LLMs with external symbolic solvers (Ye et al., 2023;Gao et al., 2023).</p>
<p>Tool-based Logical Reasoning with LLMs</p>
<p>Ye et al. ( 2023) and Gao et al. (2023) integrated Z3 and Python interpreters with LLMs to tackle various reasoning datasets.Pan et al. (2023) expanded upon this by incorporating a broader range of symbolic solvers and employing error-solving self-refinement techniques.However, the rationale behind the adoption of symbolic solvers primarily relied on theoretical definitions rather than empirical performance evaluations.Consequently, there exists a gap in the literature regarding the exploration of the interplay between LLMs, symbolic solvers, and their respective performance characteristics.</p>
<p>The primary advantages of the tool-based ap-proach are: (1) The tasks are now processed with clear logical grounding and unambiguous language.This approach guarantees that the answer is not a product of hallucination or shortcuts, because the symbolic tools will exhaustively process all logical rules in the premise and only execute clear and correct commands.(2) As LLM's translation capability continues to improve, the tool-based approach will be able to solve more complex logical problems, provided they fall within the logical reasoning capacity of symbolic solvers.</p>
<p>(3) The tool errors are clearly labeled and displayed (i.e., run-time error messages).This allows the introduction of various error-solving mechanisms like selfrefinement (Pan et al., 2023).In contrast, it is difficult for the free-form approach to improve upon its current results in the absence of any reliable feedback, specially in the light of recent debates on LLMs self-correction capability (Huang et al., 2024;Li et al., 2024).In this study, errors are isolated into solver-specific errors (e.g., LLM's translation misses a bracket, which causes the solver to throw an error) and parse errors (i.e., Predicate extraction mistakes or LLMs interpreting the logical statements incorrectly, examples of these are shown in Appendix A.3).</p>
<p>The main disadvantages of the tool-based approach are: (1) This approach does not apply to tasks that do not have a complete reasoning chain.All symbolic solvers require a full chain of logic to reach the correct conclusion.For instance, consider the followint example: Premise: People like Mark love bbq.Question: Mark is not Human?Both humans and LLMs can answer this question correctly, but a tool-based approach will fail.This is due to the break in the chain of logic.The term "Mark is human" is missing from the premise.Although this term is obvious for humans and LLMs, symbolic solvers require the exact match in predicates to process the task.A detailed discussion of this issue is included in section 3.2.(2) Changes in LLMs can cause solver-specific errors.2(3) This approach is unforgiving to simple translation errors.While processing logical tasks, Human and LLMs can often bypass errors to some extent and still reach the correct conclusion.However, a tool-based approach requires the LLM to translate tasks flawlessly, even minor mistakes like misusing suffixes (e.g., "Jompuses(x)" instead of "Jompus(x)") will cause the symbolic solver to throw an error.One of the main focuses of this study is the analysis of how different symbolic tools handle errors caused by LLMs.</p>
<p>Experiments</p>
<p>Experimental Setup</p>
<p>In our experiments we assess the performance variations of LLM when paired with various symbolic solvers.We evaluate GPT-4o, GPT-3.5-Turbo,Gemini-Pro-1.0,and Command-r-plus integrated with Z3, Pyke, Prover9 on three common logical reasoning benchmarks (introduced shortly).Unlike Pan et al. (2023) and other studies, we exclude self-refinement methods and random guessing procedures.In cases where LLM's translation is infeasible, it will not yield an answer, and any specific errors encountered are documented.The only exception is the missing bracket issue for the translation of Z3, as this was not an issue in experiments done in Ye et al. ( 2023) and Pan et al. (2023).We use a one-shot demonstration for all experiments.If different solvers are employed to tackle the same dataset, the given prompt problem remains consistent, with the sole variance lying in the solver-specific translations of the prompts.Examples of the prompt are shown in Appendix A.2.We also expand the one-shot experiment for FOLIO to two-shot and four-shot to highlight the impact of additional shots.The primary metrics for evaluation consist of two key factors: the percentage of executable logical formulations (ExecR.),and the overall accuracy (Acc).</p>
<p>Data The 3 benchmarks are introduced shortly and examples are included in Appendix A.1.We limit the test set size to 200 for cost reason.PrOn-toQA (Saparov and He, 2023) is a synthetic dataset created to analyze the capacity of LLMs for deductive reasoning.We use the hardest fictional characters version and the hardest 5-hop subset for evaluation.PrOntoQA only has questions in the close world setting (i.e., True/False only).We include this dataset in the experiment to compare natural and fictional settings, as it has a similar level of logical difficulty to ProofWriter.ProofWriter (Tafjord et al., 2021) and 5).We present the percentage of executable logical formulations (ExecR.)together with the overall accuracy (Acc.).✗: the tool was unable to solve this dataset.The numbers highlighted in red color represent the highest accuracy between the 3 chosen tools.</p>
<p>(OWA) and close-world assumptions (CWA), including depth-2, depth-3, and depth-5 (i.e., each part requiring 2, 3, and 5 hops of reasoning).To ensure a fair evaluation, we control all datasets to have a uniform distribution of True, False, and Unknown (if applicable) answers.FOLIO (Han et al., 2022) is a difficult expert-written dataset for firstorder logical reasoning.The problems are mostly aligned with real-world knowledge and expressed in natural flowing language.Tackling its questions demands adeptness in complex first-order logic reasoning.Pyke is unable to solve FOLIO, this is due to the lack of a built-in function for the exclusive disjunction (i.e., either-or).In contrast, Prover9 and Z3 offer a built-in function to handle this logic seamlessly.</p>
<p>Main Results</p>
<p>We report the results of the tool-based reasoning approach experiments in   The executable rate on average decreases for all LLMs, and average accuracy drops by 1.38% in a fictional setting.Both Z3 and Pyke's overall accuracy increased by 6.62% and 30.87%.This shows that while using Z3 and Prover9, fictional wording helps LLMs in generating consistent and correct translations.Overall, in a fictional setting, Pyke's performance is significantly boosted.Meanwhile, GPT-3.5-Turboshifts its preference from Prover9 to Z3, and Command R+ changes its preference to Pyke.We speculate the nuance in results to be reflective of potential interference between commonsense knowledge and fictional statements.</p>
<p>Depth The relaiton between depth and executable rate is somewhat mixed, specially between depth 2 and 3.While for command-r-plus we observe a general decay in performance (i.e., between depth 2 and 5) across all tools, both GPT models and Gemini exhibit resilence to depth, with performance even improving across most tools (except for Prover9).This observation highlights the robustness of translation-based approaches (i.e., using LLMs for translation and tools for solving) in handling various complexities, while prior findings reported the reasoning ability of LLMs (alone) generally diminish as the number of reasoning hops increases (Han et al., 2022).</p>
<p>Demonstration Shots We present the statistics of the FOLIO dataset in varying number of shots in Table 3. Prover9 achieves the best performance, while Z3 struggles with execution rate.The best result for FOLIO was 66.5%, which is achieved with 1 shot prompting using GPT-4o and Prover9.The primary factors that limit the execution rate performance on FOLIO are: (1) some natural wordings in FOLIO make it difficult for predicate extraction.For example, GPT4o interpreted the term "Eastern wild turkey" as two separate terms "Eastern(x)" and "WildTurkey(x)", but "Eastern(x)" has no meaning and the predicate should be extracted as EasternWildTurkey(x). ( 2) FOLIO is annotated by humans and thus assumes a degree of commonsense, this presents incomplete reasoning chains and ambiguous sentences.As shown in A.3, GPT-3.5-Turboincorrectly translated the statement "Marvin cannot be from Earth and from Mars." into "Not(And(FromEarth(marvin), From-Mars(marvin)))", which entails Marvin is not from Earth and not from Mars.The simple fix is just to change Not() into Xor().This problem was caused by the inherently ambiguous nature of the natural language.</p>
<p>(3) there is a limitation to learning by increasing the number of shots.Specifically, GPT-4o and Prover9's parse errors increased with a higher number of shots, as shown in  The effect of varying number of shots (k = 1, 2, 4) on accuracy and executable rates under GPT-4o, GPT-3.5-turbo,Gemini-1.0-proand command-r-plus on FOLIO.We present the percentage of executable logical formulations (ExecR.)together with the overall accuracy (Acc.).</p>
<p>Analysis</p>
<p>As indicated by the executable rate in Table 2, LLMs generally find it easier to produce executable logical formulations for Prover9.This is attributed to its foundation in FOL-based programming language, which most large language models (LLMs) are familiar with as a form of logical formulation.</p>
<p>While GPT models are more successful at converting these logical formulations into accurate results, Gemini-1.0-proand Command R+ face challenges in achieving similar accuracy.This is an issue because an executable formulation cannot provide feedback when an incorrect result is given.This hinders further improvement and self-refinement.Z3 does not have this issue.Its executable rate is a reflection of its accuracy.Moreover, Z3's programming language closely aligns with Python, offering a unique advantage in error displaying and further improvement.Z3 is also a flexible tool that allows the inclusion of self-defined complex logical rules like "XorAnd()" (i.e., a combination of the rule "Either or" and "And".).This capability is par-ticularly useful for addressing complex reasoning datasets like FOLIO.We did not define such a rule during our experiment but this capability should be considered in further studies.Non-executable logical formulations can be categorized into parse errors and execution errors.Additionally, for Z3, there is a separate category known as execution exceptions.</p>
<p>• parse error refers to the mistakes identified by the parser.Through the prompt, we have predefined a set of instructions and logical rules that LLMs can use.However, when LLMs hallucinate and generate logical rules or code that do not exist in the solver, the parser will detect these discrepancies and throw an parse error.This error indicates the LLM's inability to adhere to the oneshot prompt, resulting in methods or code that the parser cannot process.For instance, using Exist() instead of Exists() for Z3 is an example of such an error.</p>
<p>• execution error occurs when the solver encounters given facts that are inconsistent, predicates that are defined wrong, or when there are solverspecific syntax errors.This type of error can be resolved through self-refinement, as the errors are explicitly displayed.We call this run-time error.</p>
<p>• execution exception is a special case for Z3, where the solver runs both the original conclusion and the negation of the same conclusion but receives true as the answer in both cases.This indicates that the facts are inconsistent.We combined these errors into run-time errors for Figure 3 Z3 visualisation.</p>
<p>As shown in Figure 3, for GPT4o, while Pyke produced 3 execution errors on easier logical reasoning datasets in total, its high execution rate did not translate to high accuracy.Predominately Prover9 and Z3's error is a parse error, with execution error controlled at around 8 questions.In addition, all non-executable questions are different, there are no common questions that all 3 solvers find difficult to solve.For FOLIO, the execution error increases, and the parse error drops significantly.</p>
<p>Challenging datasets, such as FOLIO, encompass a larger number of unseen, complex logical rules and more intricate predicates, which result in higher error rates during translation by LLMs.Additionally, there is an increasing number of questions that both solvers are unable to process.This suggests that both solvers find around 25-30% of questions hard to solve.</p>
<p>Conclusion</p>
<p>In this study, we investigated and compared the performance of LLMs combined with three widely used symbolic solvers to closely examine how each solver influences the performance of toolaugmented LLMs in logical reasoning.Our experiments demonstrated that the choice of tools (i.e., Z3, Pyke, Prover9) has a significant impact on the downstream performance across various benchmarks and LLMs.</p>
<p>Limitations</p>
<p>The tool-based approach to logical reasoning is limited to deductive reasoning datasets with a complete reasoning chain.This constraint arises from the inherent nature of symbolic solvers.A potential solution is for LLMs to generate the missing segments of the reasoning chain.Additionally, black-box LLMs can exhibit inconsistencies, producing results that change in the course of time.For instance, during our experiment, GPT-3.5-Turboconsistently failed to add a closing bracket to the method "Forall()", while Command R+ failed to include an opening bracket.This was not an issue for Pan et al. (2023) and Ye et al. ( 2023) (or at least was not reported in their papers).We limited our use of solvers to their built-in functions.To enhance the performance of each tool, more unique logical combinations can be integrated and implemented.For example, Z3 is a flexible tool that allows the inclusion of rules such as "Male(x) == Not(Female(x))".There is further potential to include more defined complex logical rules that can make LLM translation easier.both attends and is very engaged with school events and is a student who attends the school, or she neither attends and is very engaged with school events nor is a student who attends the school.</p>
<p>Question: Based on the above information, is the following statement true, false, or uncertain?If Bonnie is either both a young child or teenager who wishes to further her academic career and educational opportunities and chaperones high school dances or neither is a young child nor teenager who wishes to further her academic career and educational opportunities, then Bonnie is either a student who attends the school or is an inactive and disinterested member of the community.</p>
<p>Answer  ForAll([x], Implies(And(young(x), green(x)), rough(x))) # If someone is green then they are white.</p>
<p>ForAll ([x], Implies(green(x), white(x))) # If someone is furry and quiet then they are white.</p>
<p>ForAll ([x], Implies(And(furry(x), quiet(x)), white(x))) # If someone is young and white then they are rough.</p>
<p>ForAll ([x], Implies(And(young(x), white(x)), rough(x))) # All red people are young.</p>
<p>ForAll ([x], Implies(red(x), young(x))) # Question: the following statement true, false, or unknown?Anne is white.return white(Anne)</p>
<p>ProofWriter Prompts for Prover9 One shot demonstration for LLM Given a problem description and a question, the task is to parse the problem and the question into first-order logic formulas.The grammar of the first-order logic formula is defined as follows:</p>
<ol>
<li>Logical conjunction of expr1 and expr2: expr1 ∧ expr2 2. Logical disjunction of expr1 and expr2: expr1 ∨ expr2 A.5 GPT4o and Cohere command-r-plus Prompts</li>
</ol>
<p>The prompts require some adjustments for GPT-4O and Cohere, as both models tend to produce complete executable code rather than adhering to the provided example.For instance, GPT-4O will define "s.solver()" and create the decision rule for Z3, instead of generating translations as specified in the prompt.Here we provide an overview of what is changed in the prompt.</p>
<p>ProofWriter GPT4O Prompts for Z3 Solver One-shot demonstration</p>
<p>The grammar of the first-order logic formula is defined as follows: 1) logical conjunction of expr1 and expr2: And(expr1, expr2) 2) logical disjunction of expr1 and expr2: Or(expr1, expr2) 3) logical exclusive disjunction of expr1 and expr2: Xor(expr1, expr2) 4) logical negation of expr1: Not(expr1) 5) expr1 implies expr2 ProofWriter Cohere Prompts for Z3 Solver One-shot demonstration For the Z3 solver, the Cohere prompt was slightly adjusted because produces translation not aligned with the given example.</p>
<p>The grammar of the first-order logic formula is defined as follows: 1) logical conjunction of expr1 and expr2: And(expr1, expr2) 2) logical disjunction of expr1 and expr2: Or(expr1, expr2) 3) logical exclusive disjunction of expr1 and expr2: Xor(expr1, expr2) 4) logical negation of expr1: Not(expr1) 5) expr1 implies expr2: Implies(expr1, expr2) 6) expr1 if and only if expr2: expr1 == expr2 7) logical universal quantification: ForAll() 8) logical existential quantification: Exists() Given a problem description and a question.The task is to parse the [Problem] and the [Question] into Python Z3 solver.You are meant to follow the example format and do not provide any further explanations.Follow the format given and do not define "s" and "s.solver" for the Z3 solver.Keep all the # signs as symbols and do not interpret them as markdown marker.The grammar of the first-order logic formula is defined as follows: 1) logical conjunction of expr1 and expr2: expr1 &amp;&amp; expr2 2) logical negation of expr1: expr1($x, False), as example if "Anne is not quiet", the term would be "Quiet(Anne, False)" 3) expr1 implies expr2: expr1 »&gt; expr2 Given a problem description and a question.The task is to parse the [Problem] and the [Question] into Pyke solver.You are meant to follow the example format and do not provide any further explanations.Keep all the ::: signs as symbols and do not interpret them as markdown marker.</p>
<p>[Problem]: Anne is quiet.Erin is furry.Erin is green.Fiona is furry.Fiona is quiet.Fiona is red.Fiona is rough.Fiona is white.4: Average accuracy of Experiment done with GPT-4o, GPT-3.5-turbo,Gemini-1.0-proand command-r-plus on all datasets.We present the percentage of the overall average accuracy of tools (Avg_Acc).The shots represent the number of shots used in the prompt.✗:the tool was unable to solve this dataset.The numbers highlighted in red color represent the highest accuracy between the 3 chosen tools.</p>
<p>Figure 1 :
1
Figure 1: Overview of syntax used for different Theorem Provers: Z3 and Prover9 adhere to the traditional first-order logic (FOL) format, while Pyke adopts a simplified formula approach, distinguishing premises into rules and facts</p>
<p>Figure 2 :
2
Figure 2: Executable Rate for different LLM-Tool combinations, for depth 2, 3, 5 of the ProofWriter Open World Assumption (OWA).Similar trend exists for the Close World Assumption (CWA).</p>
<p>white(Fiona) ::: Fiona is white.furry(Harry) ::: Harry is furry.quiet(Harry) ::: Harry is quiet.white(Harry) ::: Harry is white.∀x(young(x) → furry(x)) ::: Young people are furry.(quiet(Anne) → red(Anne)) ::: If Anne is quiet then Anne is red.∀x(young(x) ∧ green(x) → rough(x)) ::: Young, green people are rough.∀x(green(x) → white(x)) ::: If someone is green then they are white.∀x((furry(x) ∧ quiet(x)) → white(x)) ::: If someone is furry and quiet then they are white.∀x((young(x) ∧ white(x)) → rough(x)) ::: If someone is young and white then they are rough.∀x(red(x) → young(x)) ::: All red people are young.[Question Parse Output]: Conclusion: white(Anne) ProofWriter GPT4o and Cohere Prompts for Pyke Solver One-shot demonstration</p>
<p>is a commonly used dataset for deductive logical reasoning.Compared with PrOntoQA, the problems are expressed in a more naturalistic language form.We evaluate 6 different variations of ProofWriter.We use both open-world
Z3Prover9PykeDatasetLLMsExecR.Acc.ExecR.Acc.ExecR.Acc.gpt-4o75.00%74.17%97.33%95.67%99.83%79.17%ProofWritergpt-3.5-turbo84.83%82.88%90.67%87.00%62.83%53.33%(Avg. OWA) gemini-1.0-pro93.00%91.00%86.83%62.50%49.33%36.67%command-r-plus88.67%87.00%61.33%56.66%61.83%51.50%gpt-4o77.83%77.83%98.00%98.00%99.83%87.00%ProofWritergpt-3.5-turbo88.33%88.00%94.00%93.83%58.17%51.67%(Avg. CWA) gemini-1.0-pro96.83%96.83%84.83%58.50%42.83%34.17%command-r-plus92.50%92.50%58.67%58.33%45.33%41.33%gpt-4o96.00%96.00% 100.00% 100.00% 100.00% 100.00%PrOntoQAgpt-3.5-turbo gemini-1.0-pro95.50% 100.00% 100.00% 100.00% 97.50% 100.00% 100.00% 93.49% 85.50% 63.50% 99.50% 72.50%command-r-plus93.00%87.00%64.50%46.50%96.50%92.00%gpt-4o40.00%36.00%84.00%66.50%✗✗FOLIOgpt-3.5-turbo gemini-1.0-pro29.00% 31.00%24.49% 25.50%61.00% 67.50%39.99% 50.00%✗ ✗✗ ✗command-r-plus25.50%19.00%50.50%32.50%✗✗gpt-4o74.31%73.50%94.06%91.71%99.86%85.50%Combinedgpt-3.5-turbo gemini-1.0-pro80.50% 87.56%78.83% 86.12%87.56% 85.31%80.75% 63.81%66.07% 53.79%55.36% 44.64%command-r-plus82.75%80.56%59.38%53.00%60.64%52.93%</p>
<p>Table 2 :
2
Accuracy and execution rate of 1-shot experiments done with gpt-4o, gpt-3.5-turbo,gemini-pro-1.0andcommand-r-plus on 3 Datasets.Results for Proofwriter Open and Closed World Assumptions (OWA and CWA) are averaged over depths(Depth 2, 3,</p>
<p>Table 2
2. Different LLMsexhibit varying preferences for tools. For datasets</p>
<p>Table 3
3.</p>
<p>Table 3 :
3</p>
<p>Figure3: The proportion of various executable and non-executable instances per each tool for GPT4o.Note, Pyke does not include FOLIO (hence 1400 instances compared to Z3 and Prover 9).The Exec w/ CorrectO, and Exec w/ IncorrectO denote Executable translations that lead to correct, and incorrect outputs once executed by the tool.The Non-exec (Parse) or (Runtime) denote the non-executable translations which are either due to parsing error or other potential runtime issues.
Z3 (1600)Prover9 (1600)Pyke (1400)73.50%8.06%93.44%2.81% 3.00% 0.75%85.50%14.36% 0.00% 0.14%0.81% 17.62%Exec w/ CorrectOExec w/ IncorrectONon-Exec (Parse)Non-Exec (Runtime)</p>
<p>ProblemAnne is quiet.Erin is furry.Erin is green.Fiona is furry.Fiona is quiet.Fiona is red.Fiona is rough.Fiona is white.Harry is furry.Harry is quiet.Harry is white.Young people are furry.If Anne is quiet then Anne is red.Young, green people are rough.If someone is green then they are white.If someone is furry and quiet then they are white.If someone is young and white then they are rough.All red people are young.Erin is furry.Erin is green.Fiona is furry.Fiona is quiet.Fiona is red.Fiona is rough.Fiona is white.Harry is furry.Harry is quiet.Harry is white.Young people are furry.If Anne is quiet then Anne is red.Young, green people are rough.If someone is green then they are white.If someone is furry and quiet then they are white.If someone is young and white then they are rough.All red people are young.&gt;&gt;&gt;red(Anne,True)) ::: If Anne is quiet then Anne is red.young($x,True)&amp;&amp; green($x, True) &gt;&gt;&gt; rough($x, True) ::: Young, green people are rough.green($x,True)&gt;&gt;&gt; white($x, True) ::: If someone is green then they are white.furry($x,True)&amp;&amp; quiet($x, True) &gt;&gt;&gt; white($x, True) ::: If someone is furry and quiet then they are white.young($x,True)&amp;&amp; white($x, True) &gt;&gt;&gt; rough($x, True) ::: If someone is young and white then they are rough.red($x,True)&gt;&gt;&gt; young($x, True) ::: All red people are young.All aliens are extraterrestrial.If someone is from Mars, then they are aliens.No extraterrestrial is human.Everyone from Earth is a human.Marvin cannot be from Earth and from Mars.If Marvin is not from Earth, then Marvin is an extraterrestrial.A.4 How Z3 tackles BoardgameQAA few players are playing a boardgame.The current state of the game is as follows.The cat is named Meadow.The cat published a high-quality paper.The gecko winks at the eagle.The puffin is named Peddi.And the rules of the game are as follows.Rule1: Regarding the cat, if it has a high-quality paper, then we can conclude that it prepares armor for the canary.Rule2: If you are positive that you saw one of the animals rolls the dice for the eagle, you can be certain that it will also proceed to the spot right after the sun bear.Rule3: If the cat prepares armor for the canary and the cricket gives a magnifier to the canary, then the canary will not remove one of the pieces of the aardvark.Rule4: If the cat has a name whose first letter is the same as the first letter of the puffin's name, then the cat does not prepare armor for the canary.Rule5: The canary removes one of the pieces of the aardvark whenever at least one animal proceeds to the spot right after the sun bear.Rule6: Regarding the cat, if it has something to sit on, then we can conclude that it does not prepare armor for the canary.Rule3 is preferred over Rule5.Rule4 is preferred over Rule1.Rule6 is preferred over Rule1.Based on the game state and the rules and preferences, does the canary remove from the board one of the pieces of the aardvark?If you are positive that you saw one of the animals rolls the dice for the eagle, you can be certain that it will also proceed to the spot right after the sun bear.The current state of the game is as follows.The cat is named Meadow.The cat published a high-quality paper.The gecko winks at the eagle.The puffin is named Peddi.# The cat is named Meadow.The puffin is named Peddi.The first letter of Meadow is M. The first letter of the Peddi is P.So the cat does not have the same first letter name as the puffin.
green(x) ::: x is green. young($x, bool) ::: x is young over Rule1.red(x) ::: x is red. Facts: soft_rules = [Rule5, Rule1, Rule1]rough(x) ::: x is rough. quiet(Anne, True) ::: Anne is quiet. # Rule3 is preferred over Rule5. So Rule5 is suppressed by the precondition of Rule3.white(x) ::: x is white. furry(Erin, True) ::: Erin is furry. Rule5 = Or(And(prepare_armor(cat, canary), give_magnifier(cricket, canary)), Rule5)young(x) ::: x is young green(Erin, True) ::: Erin is green. # Rule4 is preferred over Rule1. So Rule1 is suppressed by the precondition of Rule4.Premises furry(Fiona, True) ::: Fiona is furry. Rule1 = Or(has_same_first_letter_name(cat, puffin), Rule1)quiet(Anne) ::: Anne is quiet. quiet(Fiona, True) ::: Fiona is quiet. # Rule6 is preferred over Rule1. So Rule1 is suppressed by the precondition of Rule6.furry(Erin) ::: Erin is furry. red(Fiona, True) ::: Fiona is red. Rule1 = Or(has_something_to_sit_on(cat), Rule1)green(Erin) ::: Erin is green. rough(Fiona, True) ::: Fiona is rough. # question: does the canary remove from the board one of the pieces of the aardvark?furry(Fiona) ::: Fiona is furry. white(Fiona, True) ::: Fiona is white. return remove_piece(canary, aardvark)quiet(Fiona) ::: Fiona is quiet. furry(Harry, True) ::: Harry is furry.red(Fiona) ::: Fiona is red. quiet(Harry, True) ::: Harry is quiet.rough(Fiona) ::: Fiona is rough. white(Harry, True) ::: Harry is white.white(Fiona) ::: Fiona is white. young($x, True) &gt;&gt;&gt; furry($x, True)) ::: Young people are furry.3. Logical exclusive disjunction of expr1 and expr2: expr1 ⊕ expr2 4. Logical negation of expr1: ¬expr1 5. expr1 implies expr2: expr1 → expr2 6. expr1 if and only if expr2: expr1 ↔ expr2 7. Logical universal quantification: ∀x 8. Logical existential quantification: ∃x Question: Based on the above information, is the following statement true, false, or unknown? Anne is white. ### Predicates furry(Harry) ::: Harry is furry. quiet(Harry) ::: Harry is quiet. Conclusion: white(Anne) ProofWriter Prompts for Pyke Solver One-shot demonstration Task Description: You are given a problem description and a question. The task is to: 1) define all the predicates in the problem 2) parse the problem into logic rules based on the defined predicates 3) write all the facts mentioned in the problem 4) parse the question into the logic form Problem: Anne is quiet. Question: Based on the above information, is the following statement true, false, or unknown? Anne is white. ### Predicates: quiet($x, bool) ::: x is quiet. furry($x, bool) ::: x is furry. quiet(Anne, True) Query: white(Anne) A.3 Incorrect Example Generation The following section includes classic Incorrect translations, more incorrect translations can be found in Processed_Datasets in https://github.com/Mattylam/Logic_Symbolic_Solvers_Experiment Example 1: Prover9 PrOntoQA_2 Parse error .......... ∀x(Dumpus(x) → Happy(x)) ::: Every dumpus is happy. ∀x(Dumpus(x) → Vumpus(x)) ::: Each dumpus is a vumpus. ∀x(Vumpus(x) → Bright(x)) ::: Vumpuses are bright. ∀x(Vumpus(x) → Jompus(x)) ::: Every vumpus is a jompus. ∀x(Jompus(x) → Large(x)) ::: Jompuses are large. Stella is a yumpus. Conclusion: Bright(Stella) ::: Stella is bright. Analysis: LLM failed to perform all translations, "Stella is yumpus" was not translated, thus system throws a parse error Example 2: Z3 FOLIO_dev_27 Incorrect Interpretation def solution(): # rules # Rule1: Regarding the cat, if it has a high-quality paper, then we can conclude that it prepares armor for the canary. Rule1 = Implies(has_high_quality_paper(cat), prepare_armor(cat, canary)) # Rule2: Rule2 = ForAll([x], Implies(roll_dice(x, eagle), proceed_to_spot_right_after(x, sun_bear))) # Rule3: If the cat prepares armor for the canary and the cricket gives a magnifier to the canary, then the canary will not remove one of the pieces of the aardvark. Rule3 = Implies(And(prepare_armor(cat, canary), give_magnifier(cricket, canary)), Not(remove_piece(canary, aardvark))) # Rule4: If the cat has a name whose first letter is the same as the first letter of the puffin's name, then the cat does not prepare armor for the canary. Rule4 = Implies(has_same_first_letter_name(cat, puffin), Not(prepare_armor(cat, canary))) # Rule5: The canary removes one of the pieces of the aardvark whenever at least one animal proceeds to the spot right after the sun bear. Rule5 = Implies(Exists([x], proceed_to_spot_right_after(x, sun_bear)), remove_piece(canary, aardvark)) # Rule6: Regarding the cat, if it has something to sit on, then we can conclude that it does not prepare armor for the canary. Rule6 = Implies(has_something_to_sit_on(cat), Not(prepare_armor(cat, canary))) # has_same_first_letter_name(cat, puffin) == False # The cat published a high-quality paper. has_high_quality_paper(cat) == True Problem: # solution in Python: # The gecko winks at the eagle.quiet(x) ::: x is quiet. winks_at(gecko, eagle) == Truefurry(x) ::: x is furry. # preferences. Rule3 is preferred over Rule5. Rule4 is preferred over Rule1. Rule6 is preferred
white(Harry) ::: Harry is white.∀x(young(x) → furry(x)) ::: Young people are furry.(quiet(Anne) → red(Anne)) ::: If Anne is quiet then Anne is red.∀x(young(x) ∧ green(x) → rough(x)) ::: Young, green people are rough.∀x(green(x) → white(x)) ::: If someone is green then they are white.∀x((furry(x) ∧ quiet(x)) → white(x)) ::: If someone is furry and quiet then they are white.∀x((young(x) ∧ white(x)) → rough(x)) ::: If someone is young and white then they are rough.∀x(red(x) → young(x)) ::: All red people are young.green($x, bool) ::: x is green.red($x, bool) ::: x is red.rough($x, bool) ::: x is rough.white($x, bool) ::: x is white.</p>
<p>Erin is furry.Erin is green.Fiona is furry.Fiona is quiet.Fiona is red.Fiona is rough.Fiona is white.Harry is furry.Harry is quiet.Harry is white.Young people are furry.If Anne is quiet then Anne is red.Young, green people are rough.If someone is green then they are white.If someone is furry and quiet then they are white.If someone is young and white then they are rough.All red people are young.
furry(Fiona)# Fiona is quiet.quiet(Fiona)# Fiona is red.red(Fiona)# Fiona is rough.rough(Fiona)# Fiona is white.white(Fiona)# Harry is furry.furry(Harry)# Harry is quiet.quiet(Harry)# Harry is white.white(Harry)# Young people are furry.ForAll([x], Implies(young(x), furry(x)))# If Anne is quiet then Anne is red.Implies(quiet(Anne), red(Anne))# Young, green people are rough.ForAll([x], Implies(And(young(x), green(x)), rough(x)))# If someone is green then they are white.ForAll([x], Implies(green(x), white(x)))# If someone is furry and quiet then they are white.ForAll([x], Implies(And(furry(x), quiet(x)), white(x)))# If someone is young and white then they are rough. : Implies(expr1, expr2) ForAll([x], Implies(And(young(x), white(x)), rough(x))) 6) expr1 if and only if expr2: expr1 == expr2 # All red people are young. 7) logical universal quantification: ForAll() ForAll([x], Implies(red(x), young(x))) 8) logical existential quantification: Exists() [Question Parse Output]: Given a problem description and a question. The task is to parse the [Problem] and the [Question] # Question: the following statement true, false, or unknown? Anne is white. into Python Z3 solver. You are meant to follow the example format and do not provide any further explanations. Keep all the # signs as symbols and do not interpret them as markdown marker. return white(Anne)--[Problem]:Anne is quiet. [Question]:Based on the above information, is the following statement true, false, or unknown? Anne is white.####[Problem Parse Output]:# Anne is quiet.quiet(Anne)# Erin is furry.furry(Erin)# Erin is green.green(Erin)# Fiona is furry.</p>
<p>Table</p>
<p>Harry is furry.Harry is quiet.Harry is white.Young people are furry.If Anne is quiet then Anne is red.Young, green people are rough.If someone is green then they are white.If someone is furry and quiet then they are white.If someone is young and white then they are rough.All red people are young.[Question]:Based on the above information, is the following statement true, false, or unknown?Anne is white.
DatasetZ3Prover9 PykeAvg_Acc Avg_Acc Avg_AccProofWriter D5 OWA 85.75%75.00%56.63%ProofWriter D3 OWA 83.04%75.37%52.37%ProofWriter D2 OWA 82.50%76.00%56.50%ProofWriter D5 CWA 87.50%78.25%60.25%ProofWriter D3 CWA 89.25%76.13%45.63%ProofWriter D2 CWA 89.63%77.12%54.75%PrOntoQA94.12%76.87%91.12%FOLIO (1 Shot)26.25%43.78%✗FOLIO (2 Shot)34.36%41.60%✗FOLIO (4 Shot)36.87%49.13%✗####[Problem Parse Output]:Predicates:quiet($x, bool) ::: x is quiet.furry($x, bool) ::: x is furry.green($x, bool) ::: x is green.red($x, bool) ::: x is red.rough($x, bool) ::: x is rough.white($x, bool) ::: x is white.young($x, bool) ::: x is youngFacts:quiet(Anne, True) ::: Anne is quiet.furry(Erin, True) ::: Erin is furry.
green(Erin, True) ::: Erin is green.furry(Fiona, True) ::: Fiona is furry.quiet(Fiona, True) ::: Fiona is quiet.red(Fiona, True) ::: Fiona is red.</p>
<p>Code and data are publicly available at https://github. com/Mattylam/Logic_Symbolic_Solvers_Experiment.
For instance, during the experiment stage, we tried to rerun the SatLM experiment on ProofWriter, but the execution rate dropped from 99% to 20%. This is caused by GPT3.5 not being able to add a complete bracket to the method Forall()). It is a surprising mistake that continues to happen.
p-values: 1.02 × 10 −18 , 5.37 × 10 −7 , 6.1 × 10 −12
A AppendixThe bald eagle is blue.The bald eagle is kind.The bald eagle likes the cat.The bald eagle does not visit the tiger.The cat chases the mouse.The cat is green.The cat likes the bald eagle.The cat likes the mouse.The cat does not like the tiger.The mouse likes the cat.The tiger chases the cat.The tiger chases the mouse.The tiger is red.The tiger likes the cat.The tiger visits the cat.The tiger visits the mouse.If something likes the bald eagle then it is blue.If something visits the bald eagle and it visits the cat then the bald eagle is red.If something chases the mouse then it visits the cat.If something is blue then it chases the tiger.If something visits the cat and the cat chases the tiger then the tiger likes the bald eagle.If something likes the tiger then the tiger likes the bald eagle.If something chases the mouse then it visits the mouse.Question:Based on the above information, is the following statement true, false, or unknown?The cat does not like the mouse.Answer: FalsePrOntoQA Example: ProntoQA Q3 Problem: Vumpuses are floral.Vumpuses are tumpuses.Tumpuses are brown.Each tumpus is a wumpus.Wumpuses are small.Each wumpus is a rompus.Each zumpus is metallic.Every rompus is happy.Rompuses are impuses.Each impus is amenable.Each impus is a dumpus.Every dumpus is not metallic.Dumpuses are numpuses.Each numpus is bitter.Each numpus is a jompus.Every jompus is cold.Each jompus is a yumpus.Wren is a tumpus.Question: Is the following statement true or false?Wren is not metallic.Answer: TrueFOLIOExample: FOLIO dev Q1Problem: If people perform in school talent shows often, then they attend and are very engaged with school events.People either perform in school talent shows often or are inactive and disinterested members of their community.If people chaperone high school dances, then they are not students who attend the school.All people who are inactive and disinterested members of their community chaperone high school dances.All young children and teenagers who wish to further their academic careers and educational opportunities are students who attend the school.Bonnie either Question: Based on the above information, is the following statement true, false, or uncertain?Marvin is an alien.Answer: False Predicted Answer: Unknown Translation from LLM: ........ # All aliens are extraterrestrial.ForAll([x], Implies(Alien(x), Extraterrestrial(x))) # If someone is from Mars, then they are aliens.ForAll([x], Implies(FromMars(x), Alien(x))) # No extraterrestrial is human.ForAll([x], Implies(Extraterrestrial(x), Not(Human(x)))) # Everyone from Earth is a human.ForAll([x], Implies(FromEarth(x), Human(x))) # Marvin cannot be from Earth and from Mars.Not(And(FromEarth(marvin), FromMars(marvin))) # If Marvin is not from Earth, then Marvin is an extraterrestrial.Implies(Not(FromEarth(marvin)), Extraterrestrial(marvin)) # Question: Marvin is an alien.return Alien(marvin) Analysis: LLM incorrectly interpreted the logic of the statement.The correct translation should be Xor(And(FromEarth(marvin), FromMars(marvin))), the wording of the statement makes it ambiguous for LLMs to interpret.If the statement was, Marvin is either from Earth or from Mars, LLMs would have been able to translate it.Example 3: Z3 FOLIO_dev_16 Difficult predicate extractionProblem: All rabbits are cute.Some turtles exist.An animal is either a rabbit or a squirrel.If something is skittish, then it is not still.All squirrels are skittish.Rock is still.Question: Based on the above information, is the following statement true, false, or uncertain?Rock is a turtle.Answer: Unknown Error: ExecutionError NameError: name 'Exist' is not defined.Did you mean: 'exit'?"Translation from LLM: ........All rabbits are cute.ForAll([x], Implies(Rabbit(x), Cute(x))) # Some turtles exist.Exist([x], Turtle(x)) # An animal is either a rabbit or a squirrel.ForAll([x], Or(Rabbit(x), Squirrel(x))) # If something is skittish, then it is not still.ForAll([x], Implies(Skittish(x), Not(Still(x)))) # All squirrels are skittish.[Problem]: Anne is quiet.Erin is furry.Erin is green.Fiona is furry.Fiona is quiet.Fiona is red.Fiona is rough.Fiona is white.Harry is furry.Harry is quiet.Harry is white.Young people are furry.If Anne is quiet then Anne is red.Young, green people are rough.If someone is green then they are white.If someone is furry and quiet then they are white.If someone is young and white then they are rough.All red people are young.[Question]: Based on the above information, is the following statement true, false, or unknown?Anne is white.#### [Problem Parse Output]: Predicates quiet(x) ::: x is quiet.furry(x) ::: x is furry.green(x) ::: x is green.red(x) ::: x is red.rough(x) ::: x is rough.white(x) ::: x is white.young(x) ::: x is young Premises quiet(Anne) ::: Anne is quiet.furry(Erin) ::: Erin is furry.green(Erin) ::: Erin is green.furry(Fiona) ::: Fiona is furry.quiet(Fiona) ::: Fiona is quiet.red(Fiona) ::: Fiona is red.rough(Fiona) ::: Fiona is rough.
Language models show human-like content effects on reasoning. Ishita Dasgupta, Andrew K Lampinen, C Y Stephanie, Antonia Chan, Dharshan Creswell, James L Kumaran, Felix Mcclelland, Hill, 10.48550/ARXIV.2207.07051CoRR, abs/2207.070512022</p>
<p>Z3: an efficient SMT solver. Leonardo Mendonça, De Moura, Nikolaj S Bjørner, 10.1007/978-3-540-78800-3_24Tools and Algorithms for the Construction and Analysis of Systems, 14th International Conference, TACAS 2008, Held as Part of the Joint European Conferences on Theory and Practice of Software, ETAPS 2008. Lecture Notes in Computer Science. HungarySpringer2008. March 29-April 6, 20084963Proceedings</p>
<p>Jiazhan Feng, Ruochen Xu, Junheng Hao, Hiteshi Sharma, Yelong Shen, Dongyan Zhao, Weizhu Chen, arXiv:2311.06158Language models can be logical solvers. 2023arXiv preprint</p>
<p>Applying expert system technology to code reuse with pyke. Bruce Frederiksen, 2008PyCon: Chicago</p>
<p>PAL: program-aided language models. Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, Graham Neubig, International Conference on Machine Learning, ICML 2023. Honolulu, Hawaii, USAPMLR2023. July 2023202of Proceedings of Machine Learning Research</p>
<p>FOLIO: natural language reasoning with first-order logic. Simeng Han, Hailey Schoelkopf, Yilun Zhao, Zhenting Qi, Martin Riddell, Luke Benson, Lucy Sun, Ekaterina Zubova, Yujie Qiao, Matthew Burtell, David Peng, Jonathan Fan, Yixin Liu, Brian Wong, Malcolm Sailor, Ansong Ni, Linyong Nan, Jungo Kasai, Tao Yu, Rui Zhang, Shafiq R Joty, Alexander R Fabbri, Wojciech Kryscinski, 10.48550/ARXIV.2209.00840CoRR, abs/2209.008402022Xi Victoria Lin, Caiming Xiong, and Dragomir Radev</p>
<p>Large language models cannot self-correct reasoning yet. Jie Huang, Xinyun Chen, Swaroop Mishra, Steven Huaixiu, Adams Wei Zheng, Xinying Yu, Denny Song, Zhou, The Twelfth International Conference on Learning Representations. 2024</p>
<p>Survey of hallucination in natural language generation. Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Yejin Bang, Andrea Madotto, Pascale Fung, 10.1145/3571730ACM Comput. Surv. 5512382023</p>
<p>Boardgameqa: A dataset for natural language reasoning with contradictory information. Mehran Kazemi, Quan Yuan, Deepti Bhatia, Najoung Kim, Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems. NeurIPS; New Orleans, LA, USA2023. 2023. 2023. December 10 -16, 2023Xin Xu, Vaiva Imbrasaite, and Deepak Ramachandran</p>
<p>Large language models are zero-shot reasoners. Takeshi Kojima, Shane Shixiang, Machel Gu, Yutaka Reid, Yusuke Matsuo, Iwasawa, Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems. NeurIPS; New Orleans, LA, USA2022. 2022. 2022. November 28 -December 9, 2022</p>
<p>Confidence matters: Revisiting intrinsic selfcorrection capabilities of large language models. Loka Li, Guangyi Chen, Yusheng Su, Zhenhao Chen, Yixuan Zhang, Eric P Xing, Kun Zhang, 10.48550/ARXIV.2402.12563CoRR, abs/2402.125632024</p>
<p>Faithful chain-ofthought reasoning. Qing Lyu, Shreya Havaldar, Adam Stein, Li Zhang, Delip Rao, Eric Wong, Marianna Apidianaki, Chris Callison-Burch, 10.48550/ARXIV.2301.13379CoRR, abs/2301.133792023</p>
<p>William Mccune, arXiv preprint cs/0310056Otter 3.3 reference manual. 2003</p>
<p>Release of prover9. William Mccune, Mile high conference on quasigroups, loops and nonassociative systems. Denver, Colorado2005</p>
<p>Expert systems in production planning and scheduling: A state-of-the-art survey. Kostas S Metaxiotis, Dimitris Askounis, John E Psarras, 10.1023/A%3A1016064126976J. Intell. Manuf. 1342002</p>
<p>Improving coherence and consistency in neural sequence models with dual-system, neuro-symbolic reasoning. I Maxwell, Michael Henry Nye, Joshua B Tessler, Brenden M Tenenbaum, Lake, Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems. NeurIPS2021. 2021. 2021. December 6-14, 2021</p>
<p>LINC: A neurosymbolic approach for logical reasoning by combining language models with first-order logic provers. Theo Olausson, Alex Gu, Ben Lipkin, Cedegao Zhang, Armando Solar-Lezama, Joshua Tenenbaum, Roger Levy, 10.18653/v1/2023.emnlp-main.313Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language ProcessingSingaporeAssociation for Computational Linguistics2023</p>
<p>10.48550/ARXIV.2303.08774CoRR, abs/2303.08774GPT-4 technical report. 2023OpenAI</p>
<p>Logic-lm: Empowering large language models with symbolic solvers for faithful logical reasoning. Liangming Pan, Alon Albalak, Xinyi Wang, William Wang, Findings of the Association for Computational Linguistics: EMNLP 2023. SingaporeAssociation for Computational Linguistics2023. December 6-10, 2023</p>
<p>Language models are greedy reasoners: A systematic formal analysis of chain-of-thought. Abulhair Saparov, He He, The Eleventh International Conference on Learning Representations, ICLR 2023. Kigali, Rwanda2023. May 1-5, 2023OpenReview.net</p>
<p>Testing the general deductive reasoning capacity of large language models using OOD examples. Abulhair Saparov, Richard Yuanzhe Pang, Vishakh Padmakumar, Nitish Joshi, Mehran Kazemi, Najoung Kim, He He, 10.18653/V1/2021.FINDINGS-ACL.317Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023. Oyvind Tafjord, Bhavana Dalvi, Peter Clark, New Orleans, LA, USAAssociation for Computational Linguistics2023. 2023. December 10 -16, 2023. 2021. August 1-6, 2021ACL/IJCNLP 2021 of Findings of ACL</p>
<p>Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, arXiv:2312.11805Gemini: a family of highly capable multimodal models. 2023arXiv preprint</p>
<p>furry(Harry, True) ::: Harry is furry. quiet(Harry, True) ::: Harry is quiet. white(Harry, True) ::: Harry is white. young($x, True) &gt;&gt;&gt; furry($x, True)) ::: Young people are furry. quiet(Anne, True) &gt;&gt;&gt; red(Anne, True)) ::: If Anne is quiet then Anne is red. young($x, True) &amp;&amp; green($x, True) &gt;&gt;&gt; rough($x, True) ::: Young, green people are rough. green($x, True) &gt;&gt;&gt; white($x, True) ::: If someone is green then they are white. furry($x, True) &amp;&amp; quiet($x, True) &gt;&gt;&gt; white($x, True) ::: If someone is furry and quiet then they are white. young($x, True) &amp;&amp; white($x, True) &gt;&gt;&gt; rough($x, True) ::: If someone is young and white then they are rough. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H Chi, V Quoc, Denny Le, Zhou, Advances in Neural Information Processing Systems 35: rough(Fiona, True) ::: Fiona is rough. white(Fiona, True) ::: Fiona is white. 2022Chain-of-thought prompting elicits reasoning in large language models. red($x, True) &gt;&gt;&gt; young($x, True) ::: All red people are young. Question Parse Output]: Query: white(Anne</p>            </div>
        </div>

    </div>
</body>
</html>
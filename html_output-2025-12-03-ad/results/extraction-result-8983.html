<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8983 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8983</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8983</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-158.html">extraction-schema-158</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-23e41de8b36c9bdf20d9de931f05510bb1885c25</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/23e41de8b36c9bdf20d9de931f05510bb1885c25" target="_blank">Controllable Meaning Representation to Text Generation: Linearization and Data Augmentation Strategies</a></p>
                <p><strong>Paper Venue:</strong> Conference on Empirical Methods in Natural Language Processing</p>
                <p><strong>Paper TL;DR:</strong> It is found that properly aligning input sequences during training leads to highly controllable generation, both when training from scratch or when fine-tuning a larger pre-trained model.</p>
                <p><strong>Paper Abstract:</strong> We study the degree to which neural sequence-to-sequence models exhibit fine-grained controllability when performing natural language generation from a meaning representation. Using two task-oriented dialogue generation benchmarks, we systematically compare the effect of four input linearization strategies on controllability and faithfulness. Additionally, we evaluate how a phrase-based data augmentation method can improve performance. We find that properly aligning input sequences during training leads to highly controllable generation, both when training from scratch or when fine-tuning a larger pre-trained model. Data augmentation further improves control on difficult, randomly generated utterance plans.</p>
                <p><strong>Cost:</strong> 0.023</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8983.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8983.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RND</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Random linearization</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Linearize the meaning representation (MR) by randomly ordering the attribute-value pairs for each example (resampled each training epoch); used as a baseline to test sensitivity to input order.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>random linearization</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Represent the input MR as a flat token sequence [dialogue_act, x1, x2, ..., xn] where the attribute-value pairs x_i are placed in a random permutation; during training the ordering is re-sampled each epoch (validation kept fixed). Start/stop tokens are prepended/appended.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>task-oriented MR (unordered set of attribute-value pairs); analogous to a flat graph of attribute-value nodes</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>random permutation of attribute-value tokens to form a linear token sequence; repeated sampling each epoch during training</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>MR-to-text natural language generation for task-oriented dialogue (E2E, ViGGO datasets); sequence-to-sequence training (biGRU, Transformer, BART fine-tuning)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>On E2E and ViGGO test evaluations, RND models achieve moderate BLEU/ROUGE-L but higher semantic error rates than alignment-based methods; e.g., RND (various architectures) reported SERs ranging from ~0.14% up to >12% depending on dataset/architecture (E2E SER examples: 0.14%--2.64% in various settings; ViGGO SER examples: 1.06%--12.56%). Exact numbers vary by architecture and dataset (see Tables 2-3).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Compared to Alignment Training (AT), RND yields substantially worse order controllability (no OA reported) and higher semantic error rates in many cases; RND sometimes outperforms IF (increasing-frequency) or FP (fixed-position) due to denoising-like benefits but is generally inferior to AT+planner combinations.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Simple to implement; may provide denoising benefits by exposing model to varied input orders and reduce overfitting to a single ordering.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Does not provide any mapping between input order and realization order; poor for fine-grained controllability; often higher semantic errors than aligned linearizations.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Fails to permit controllable realization of a provided utterance plan; higher semantic error rates, particularly in small-data regimes (e.g., ViGGO) and with recurrent encoders (biGRU).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Controllable Meaning Representation to Text Generation: Linearization and Data Augmentation Strategies', 'publication_date_yy_mm': '2020-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8983.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8983.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>IF</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Increasing-frequency linearization</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Order attribute-value pairs by increasing frequency in the training data, so rare items appear earlier/later consistently.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>increasing-frequency linearization</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Map MR to a linear sequence [dialogue_act, x1, x2, ..., xn] where x_i are sorted by their frequency in the training set (non-decreasing).</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>task-oriented MR (unordered attribute-value set)</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>deterministic sort of attribute-value tokens by global frequency counts computed on training data; missing attributes are simply absent (unlike FP which adds explicit N/A tokens).</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>MR-to-text generation (E2E and ViGGO) using S2S models (biGRU, Transformer, BART)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>IF models show varied BLEU/ROUGE-L; often higher SER than AT: examples include E2E SERs up to 12.64% (some biGRU settings) and lower SER after phrase augmentation (IF+P) in some cases (e.g., E2E IF+P SER 0.24% in one setting). ViGGO IF SERs were higher (e.g., 19.20% for some Transformer biGRU settings) but improved with phrase augmentation.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>IF can outperform completely fixed or random orderings for frequently occurring items due to consistent placement, but generally underperforms Alignment Training (AT) in controllability and faithfulness. Phrase augmentation (+P) reduces SER for IF in several cases.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Provides a consistent, deterministic ordering which may help the model learn reliable realization positions for frequent attributes.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Places rare items in less-consistent positions which can hurt realization of rarer attribute-values; still no direct correspondence to utterance realization order.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>High semantic error rates on some architectures/datasets (notably biGRU on ViGGO); inconsistent realization of rare attributes.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Controllable Meaning Representation to Text Generation: Linearization and Data Augmentation Strategies', 'publication_date_yy_mm': '2020-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8983.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8983.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>FP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Fixed-position linearization</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Use a fixed global ordering of attributes (not attribute-values) determined by training frequency; absent attributes are filled with explicit N/A tokens and list slots are expanded to fixed length.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>fixed-position linearization</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Create a fixed ordering of attribute slots across all MRs (ordered by increasing attribute frequency); represent absent attributes explicitly as 'N/A' tokens; for list-valued attributes allocate a maximum observed list length as repeated slots.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>task-oriented MR with a fixed attribute schema (suitable when attribute set is modest in size)</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>deterministic serialization of MR into a slot sequence where each attribute has a fixed slot position and a concrete token (value or N/A) is placed in that slot; list-valued attributes use repeated slots up to a preset max.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>MR-to-text generation (E2E and ViGGO) with S2S models</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>FP has mixed BLEU/ROUGE-L and generally higher SERs than AT; examples include E2E FP SERs up to 6.54% in some settings and ViGGO SERs up to 17.12% in some settings. FP sometimes shows lower variance but nonetheless underperforms AT for controllability.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Better than IF in some metrics due to consistent slot positions and explicit N/A, but worse than AT for generating utterances that follow a supplied realization order. FP can approach AT in some low-variance situations but overall AT+planner is superior.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Consistency across examples; explicit representation of absent attributes reduces ambiguity; feasible for datasets with modest attribute sets.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Scales poorly to large attribute vocabularies (10s-100s attributes); forces a fixed structure that may be unnatural for variable-length/list-valued slots; still not aligned to realization order.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Higher semantic error rates on several settings; not suitable when attribute vocabulary is large or highly variable.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Controllable Meaning Representation to Text Generation: Linearization and Data Augmentation Strategies', 'publication_date_yy_mm': '2020-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8983.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8983.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Alignment Training (aligned linearization)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Linearize the MR during training to match the order in which attribute-values are realized in the reference utterance, enabling fine-grained controllability of phrase order at test time.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>alignment training / aligned linearization</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>During training, order attribute-value pairs x1..xn in the input MR according to the order their corresponding attribute-values are realized (aligned) in the reference utterance, obtained via pattern-matching rules linking MR attributes to utterance subspans. At test time the trained model can accept arbitrary input orderings (utterance plans) and attempt to realize phrases in that order.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>task-oriented MR where attributes map one-to-one to utterance subspans; effectively a mapping from a flat attribute graph to a linear plan</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Use manually constructed heuristic matching rules to align attribute-values to spans in reference utterances, then serialize MR tokens in that realized order to form the encoder input sequence. At test time, external utterance planners (bigram or neural) or humans can supply arbitrary orderings as input plans.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Controllable MR-to-text generation (produce utterances that follow a given utterance plan) on E2E and ViGGO; experiments with biGRU, Transformer, and BART</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>AT substantially improves controllability and faithfulness. Reported examples: on E2E test AT+NUP+P achieved SER = 0.00% and OA = 100.0% (near-zero semantic errors and perfect order accuracy) in top configurations; AT models frequently reduced SER to near 0 and OA to >98% across architectures. On ViGGO AT+NUP reduced SER relative to baselines but non-zero (e.g., Transformer AT+NUP reported SERs ~2.28%--2.70% and OA ~88%--94% depending on architecture; BART AT+NUP+p achieved SER 0.02% and OA ~99.8% in some reported settings). Random-permutation stress tests showed AT models with phrase training and a neural planner could achieve low SER on E2E (<0.6%) but higher SER/OA degradation on ViGGO (e.g., biGRU+NUP+P SER 8.98% and OA 64.50% on random permutations).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Directly compared to RND, IF, FP: AT yields far superior order accuracy (OA) and much lower semantic error rates (SER) for controllability tasks. Prior work (Nayak et al., Reed et al., Balakrishnan) observed similar benefits; the paper extends comparisons by stress-testing on random permutations and evaluating augmentation effects.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Enables explicit, fine-grained control of phrase realization order; greatly reduces semantic errors when paired with an utterance planner; robust across architectures and with pre-trained models (BART). Also reduces variance across random initializations.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Requires accurate MR-utterance alignments during training, which in this paper demanded extensive manual heuristic rule development and dataset cleaning; may not generalize well to attribute-values that do not map cleanly to disjoint subspans.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Performance degrades on arbitrary/random utterance plans not seen in training distribution, especially in small-data settings (ViGGO) and with weaker encoders (biGRU, Transformer trained from scratch). Requires manual alignment rules; alignment errors or missing alignments limit applicability.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Controllable Meaning Representation to Text Generation: Linearization and Data Augmentation Strategies', 'publication_date_yy_mm': '2020-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8983.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8983.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PhraseAug</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Phrase-based data augmentation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Augment training data with MR/utterance pairs derived from constituent phrases of original utterances to improve realization of fragments and robustness to unusual orderings.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>phrase-based data augmentation</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Parse training utterances for constituent phrases (NP, VP, ADJP, ADVP, PP, S, Sbar), apply the same matching rules to each phrase to produce a corresponding MR fragment, and add these fragment MR/utterance pairs to the training set; phrase examples use phrase-specific start/stop tokens (e.g., start-NP/stop-NP) to prevent generating incomplete sentences for full MRs.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>subgraph fragments of task-oriented MR (attribute-value subsets corresponding to phrase realizations)</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Syntactic parsing (Stanford CoreNLP) to extract constituent phrases, apply alignment/matching rules to map phrase textual spans to attribute-values, reclassify/dialogue-act preserved, filter out phrases with no realized attributes, and insert phrase examples (with special boundary tokens) into training.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>MR-to-text generation; improves robustness of S2S models (biGRU, Transformer, BART) to arbitrary utterance plans and reduces semantic error rate</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Phrase augmentation (+P) reduced SER in 8/12 reported cases; large improvements on E2E (e.g., many models reached near-zero SER after +P). Random permutation stress test: phrase training helped E2E models achieve <0.6% SER and improved ViGGO results in several architectures; in one top BART setting AT+NUP+P achieved ViGGO SER = 0.76% and OA = 95.32% (Table 4). Human eval: phrase-augmented BART+NUP preferred for naturalness.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>When combined with AT and a neural planner (NUP), phrase augmentation produced the strongest control and lowest SER compared to models trained without phrase data; for some small-data ViGGO settings phrase training slightly hurt fluency (human eval) but still improved SER overall.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Improves model robustness to follow arbitrary or difficult utterance plans; reduces semantic errors especially where base SER is higher; captures fragment-level realizations and inversions (e.g., handling negation flips).</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>In small-data settings (ViGGO) phrase augmentation can slightly hurt fluency when models must follow very unnatural random orders; increases training data size considerably and may require careful filtering.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>May reduce naturalness for difficult random orderings in small-data regimes; does not fully eliminate SER for complex, out-of-distribution permutations (ViGGO random-permutation OA and SER still degraded).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Controllable Meaning Representation to Text Generation: Linearization and Data Augmentation Strategies', 'publication_date_yy_mm': '2020-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8983.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8983.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BGUP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Attribute-value bigram utterance planner</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A simple bigram language model over attribute-value sequences trained on AT-orderings to propose likely utterance plans.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>bigram utterance planner (BGUP)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Estimate bigram probabilities of attribute-value token sequences from training data (using Lidstone smoothing alpha=1e-6) according to AT ordering; at test time generate candidate attribute-value orderings via beam search constrained to the input MR.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>ordering/planner over flat MR attribute-value nodes (sequence of nodes)</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Statistical bigram model over serialized attribute-value tokens; generation via constrained beam search (beam=32) producing permutations that cover all MR items without repeats.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Provide utterance plans (input orderings) for AT-trained S2S models to follow in MR-to-text generation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>BGUP-produced plans when used with AT-trained generators achieved substantial OA and low SER on E2E (e.g., AT+BgUP E2E OA ~98.2%, SER as low as 0.26% in some settings); BGUP Kendall's tau on validation/test lower than NUP (Table 10: E2E BGUP tau 0.417/0.347; NUP 0.739/0.651).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>NUP (neural planner) typically generates plans closer to reference orderings (higher Kendall's tau) and often yields slightly higher BLEU/ROUGE-L when paired with AT models. BGUP is simpler but effective at reducing SER compared to no-planner baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Simple, fast to train; provides plausible, low-entropy orderings that are easier for AT models to follow and reduce semantic errors.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Limited modeling capacity (only local bigram dependencies); produces less human-like orderings than NUP according to Kendall's tau and BLEU/ROUGE comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Less effective at matching complex, dialogue-act-specific ordering preferences that NUP can learn; lower Kendall's tau than neural planner.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Controllable Meaning Representation to Text Generation: Linearization and Data Augmentation Strategies', 'publication_date_yy_mm': '2020-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8983.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e8983.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>NUP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Neural utterance planner</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A sequence-to-sequence RNN planner that maps a default ordering (IF) of attribute-values to AT ordering, producing more natural utterance plans for AT-trained generators.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>neural utterance planner (NUP)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>BiGRU/LSTM-based seq2seq model with attention trained to map IF-ordered attribute-value sequences to AT (aligned) orderings; conditions on dialogue act so planner can learn act-specific ordering preferences; at test time constrained beam search generates orderings that include all MR items.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>planner over sequence representations of flat MR attribute-value nodes</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Train RNN encoder-decoder (512-dim embeddings/hidden etc.) to predict permutation (sequence) of given MR, using beam search constrained to MR items to produce candidate utterance plans.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Generate test-time utterance plans that AT-trained S2S models should follow, improving semantic correctness and naturalness of generated text</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>NUP achieved higher Kendall's tau than BGUP (Table 10: E2E validation/test tau 0.739/0.651 vs BGUP 0.417/0.347; ViGGO 0.502/0.447 vs BGUP 0.433/0.432). Pairing AT+NUP often yielded lowest SER and highest OA (e.g., many configurations showed AT+NUP SER near 0% on E2E and reduced SER on ViGGO relative to baselines).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Outperforms BGUP in producing orderings closer to reference orders (higher Kendall's tau) and generally yields slightly higher BLEU/ROUGE-L and lower SER when paired with AT-trained generators. AT+NUP typically best overall.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Learns global ordering preferences including dialogue-act specific patterns; produces more natural/closer-to-reference plans than BGUP, improving BLEU/ROUGE-L and reducing SER.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Needs training data and hyperparameter tuning; may still produce plans that are easier for the generator than true human oracle orders (models tend to prefer low-entropy orders seen in training).</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>When given arbitrary random permutations as input (stress test), NUP reordering helps but cannot fully eliminate SER increases for hard permutations, especially in small-data settings (ViGGO).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Controllable Meaning Representation to Text Generation: Linearization and Data Augmentation Strategies', 'publication_date_yy_mm': '2020-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8983.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e8983.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BART-format</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Formatting MR as string for BART fine-tuning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Instead of extending the encoder vocabulary, represent the linearized MR as a plain string (e.g., 'inform rating=good name=NAME platforms=PC') so BART's subword encoder can encode MR tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>string-serialization for pretrained S2S encoder (BART)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Serialize the linearized MR into a plain textual string respecting the chosen linearization order and feed it to BART's subword tokenizer/encoder without adding new tokens to the encoder vocabulary.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>flat MR (attribute-value sequence) serialized as text</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Concatenate dialogue-act and attribute-value pairs into a single textual string in the target linearization order; rely on subword encoding to represent attribute tokens; delexicalize rare values where needed and post-process outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Fine-tune BART for MR-to-text generation using same downstream evaluation (E2E, ViGGO)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Fine-tuned BART models using AT+NUP+P achieved among the best tradeoffs: examples include E2E SER as low as 0.02% and OA ~99.8% in top runs; ViGGO BART AT+NUP+P reported SER 0.76% and OA 95.32% in stress tests (Table 4); BART generally outperforms from-scratch Transformer/biGRU on small-data ViGGO.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>BART fine-tuning with serialized MR competes with or exceeds from-scratch Transformer/biGRU models, especially in small-data settings where pretraining helps generalization and following arbitrary plans.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Leverages large pretrained language model robustness to generalize ordering-to-text mapping; avoids modifying BART vocab by serializing MR as plain text; strong faithfulness and fluency in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Relies on subword segmentation of attribute tokens which may be inconsistent; still benefits from alignment training and planner components.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Although strong, BART still shows non-zero SER and less-than-perfect OA on ViGGO under random permutations; formatting does not eliminate the need for aligned training and augmentation to achieve controllability.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Controllable Meaning Representation to Text Generation: Linearization and Data Augmentation Strategies', 'publication_date_yy_mm': '2020-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8983.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e8983.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Delex</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Delexicalization of rare attribute values</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Replace infrequent attribute values (e.g., names, developer) with attribute-specific placeholders during training, and re-lexicalize at inference to reduce data sparsity.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>delexicalization / placeholder serialization</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Map rare open-class or low-frequency attribute values to placeholder tokens (e.g., NAME, SPECIFIER_V_S) in both MR and target utterance during training; at decode time post-process generated placeholder tokens by substituting the original attribute values.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>task-oriented MR containing open-class attribute-values (names, specifiers)</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Replace attribute values in both MR input and reference outputs with attribute-specific placeholder tokens; represent grammatical features (e.g., consonant/vowel initial, regular/superlative) for specifiers as part of placeholder encoding.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>MR-to-text generation; reduces sparsity and improves generalization for rare values in E2E/ViGGO</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Used as pre/post-processing; improves robustness/generalization but no isolated numeric ablation reported; included as part of overall system configuration contributing to the low SERs reported for AT+NUP+P and BART runs.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Standard practical technique; combined with AT and phrase augmentation it supports low SERs on small datasets where lexical sparsity would otherwise hurt.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Reduces learning difficulty for rare tokens; handles infrequent open-class specifiers and ensures correct insertion of values at re-lexicalization.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Requires correct post-processing re-lexicalization; mask may hide subtle context-dependent realization decisions during training.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>If re-lexicalization or placeholder selection is incorrect, can produce wrong surface forms; not a replacement for alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Controllable Meaning Representation to Text Generation: Linearization and Data Augmentation Strategies', 'publication_date_yy_mm': '2020-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8983.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e8983.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TreeMR (cited)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Tree-structured MR and linearized trees (Balakrishnan et al., 2019)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prior work experimented with explicit tree-structured MRs and compared tree encoders vs linearized-tree S2S encoders, finding aligned linearization can enable controllable generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Constrained Decoding for Neural NLG from Compositional Representations in Task-Oriented Dialogue</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>tree-structured MR / linearized tree serialization</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Represent MR as a tree (compositional representation) and either encode it with a tree-structured encoder or linearize the tree (e.g., depth-first traversal) for standard S2S encoders.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>compositional/task-oriented MR that can be structured as a tree</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Either use specialized tree encoders or serialize the tree into a linear sequence (linearized tree) for S2S encoders; traversal order choices matter for downstream realization.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Data-to-text generation in task-oriented dialogue; comparison of encoder types/serializations</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Referenced as comparable work; this paper notes Balakrishnan et al. also found aligned linearization leads to controllable generation but does not provide detailed numeric comparisons here.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Paper notes Balakrishnan compared tree encoders to linearized trees and found aligned linearization effective; our work extends by systematically comparing alternative linearizations and stress-testing random permutations.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Tree encodings can capture compositional structure explicitly; linearized trees allow use of standard S2S models while preserving structural hints.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Tree encoders require specialized architectures; linearizations introduce order choices that strongly affect behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Not explored in detail in this paper; cited as related work with similar findings about alignment importance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Controllable Meaning Representation to Text Generation: Linearization and Data Augmentation Strategies', 'publication_date_yy_mm': '2020-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8983.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e8983.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AMR-linear</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AMR linearizations (cited)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prior comparisons of different AMR graph linearizations for AMR-to-text generation showed alignment-like strategies can be effective but earlier evaluations lacked explicit semantic correctness measures.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Linguistic realisation as machine translation: Comparing different MT models for AMR-to-text generation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>AMR graph linearization / serialization</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Convert AMR abstract meaning representation graphs into linear token sequences (various orders) for use with MT-like or S2S generators; alignment-style serializations correspond to realization order when available.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Abstract Meaning Representation (AMR) graphs</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Graph-to-sequence linearization schemes (various traversals and alignments) to produce input sequences for seq2seq models.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>AMR-to-text generation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Cited Castro Ferreira et al. (2017) compared several linearizations for AMR but evaluated mostly with automatic quality metrics (BLEU) and did not measure semantic correctness or order-following explicitly.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Paper references this work as related but notes the lack of semantic correctness/order-following evaluation; our paper provides such analyses in the MR-to-text context.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Allows use of standard S2S models for graph-to-text tasks; alignment-based linearizations may improve controllability.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>May not capture graph structure fully; choice of linearization affects model behavior and is under-studied in terms of semantic fidelity.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Not directly evaluated in this paper; Castro Ferreira et al. focused on automatic metrics only.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Controllable Meaning Representation to Text Generation: Linearization and Data Augmentation Strategies', 'publication_date_yy_mm': '2020-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>To Plan or not to Plan? Discourse Planning in Slot-Value Informed Sequence to Sequence Models for Language Generation <em>(Rating: 2)</em></li>
                <li>Linguistic realisation as machine translation: Comparing different MT models for AMR-to-text generation <em>(Rating: 2)</em></li>
                <li>Constrained Decoding for Neural NLG from Compositional Representations in Task-Oriented Dialogue <em>(Rating: 2)</em></li>
                <li>Can Neural Generators for Dialogue Learn Sentence Planning and Discourse Structuring? <em>(Rating: 2)</em></li>
                <li>Neural data-to-text generation: A comparison between pipeline and end-to-end architectures <em>(Rating: 2)</em></li>
                <li>A Deep Ensemble Model with Slot Alignment for Sequence-toSequence Natural Language Generation <em>(Rating: 1)</em></li>
                <li>Improving Quality and Efficiency in Planbased Neural Data-to-text Generation <em>(Rating: 1)</em></li>
                <li>Step-by-Step: Separating Planning from Realization in Neural Data-to-Text Generation <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8983",
    "paper_id": "paper-23e41de8b36c9bdf20d9de931f05510bb1885c25",
    "extraction_schema_id": "extraction-schema-158",
    "extracted_data": [
        {
            "name_short": "RND",
            "name_full": "Random linearization",
            "brief_description": "Linearize the meaning representation (MR) by randomly ordering the attribute-value pairs for each example (resampled each training epoch); used as a baseline to test sensitivity to input order.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "random linearization",
            "representation_description": "Represent the input MR as a flat token sequence [dialogue_act, x1, x2, ..., xn] where the attribute-value pairs x_i are placed in a random permutation; during training the ordering is re-sampled each epoch (validation kept fixed). Start/stop tokens are prepended/appended.",
            "graph_type": "task-oriented MR (unordered set of attribute-value pairs); analogous to a flat graph of attribute-value nodes",
            "conversion_method": "random permutation of attribute-value tokens to form a linear token sequence; repeated sampling each epoch during training",
            "downstream_task": "MR-to-text natural language generation for task-oriented dialogue (E2E, ViGGO datasets); sequence-to-sequence training (biGRU, Transformer, BART fine-tuning)",
            "performance_metrics": "On E2E and ViGGO test evaluations, RND models achieve moderate BLEU/ROUGE-L but higher semantic error rates than alignment-based methods; e.g., RND (various architectures) reported SERs ranging from ~0.14% up to &gt;12% depending on dataset/architecture (E2E SER examples: 0.14%--2.64% in various settings; ViGGO SER examples: 1.06%--12.56%). Exact numbers vary by architecture and dataset (see Tables 2-3).",
            "comparison_to_others": "Compared to Alignment Training (AT), RND yields substantially worse order controllability (no OA reported) and higher semantic error rates in many cases; RND sometimes outperforms IF (increasing-frequency) or FP (fixed-position) due to denoising-like benefits but is generally inferior to AT+planner combinations.",
            "advantages": "Simple to implement; may provide denoising benefits by exposing model to varied input orders and reduce overfitting to a single ordering.",
            "disadvantages": "Does not provide any mapping between input order and realization order; poor for fine-grained controllability; often higher semantic errors than aligned linearizations.",
            "failure_cases": "Fails to permit controllable realization of a provided utterance plan; higher semantic error rates, particularly in small-data regimes (e.g., ViGGO) and with recurrent encoders (biGRU).",
            "uuid": "e8983.0",
            "source_info": {
                "paper_title": "Controllable Meaning Representation to Text Generation: Linearization and Data Augmentation Strategies",
                "publication_date_yy_mm": "2020-11"
            }
        },
        {
            "name_short": "IF",
            "name_full": "Increasing-frequency linearization",
            "brief_description": "Order attribute-value pairs by increasing frequency in the training data, so rare items appear earlier/later consistently.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "increasing-frequency linearization",
            "representation_description": "Map MR to a linear sequence [dialogue_act, x1, x2, ..., xn] where x_i are sorted by their frequency in the training set (non-decreasing).",
            "graph_type": "task-oriented MR (unordered attribute-value set)",
            "conversion_method": "deterministic sort of attribute-value tokens by global frequency counts computed on training data; missing attributes are simply absent (unlike FP which adds explicit N/A tokens).",
            "downstream_task": "MR-to-text generation (E2E and ViGGO) using S2S models (biGRU, Transformer, BART)",
            "performance_metrics": "IF models show varied BLEU/ROUGE-L; often higher SER than AT: examples include E2E SERs up to 12.64% (some biGRU settings) and lower SER after phrase augmentation (IF+P) in some cases (e.g., E2E IF+P SER 0.24% in one setting). ViGGO IF SERs were higher (e.g., 19.20% for some Transformer biGRU settings) but improved with phrase augmentation.",
            "comparison_to_others": "IF can outperform completely fixed or random orderings for frequently occurring items due to consistent placement, but generally underperforms Alignment Training (AT) in controllability and faithfulness. Phrase augmentation (+P) reduces SER for IF in several cases.",
            "advantages": "Provides a consistent, deterministic ordering which may help the model learn reliable realization positions for frequent attributes.",
            "disadvantages": "Places rare items in less-consistent positions which can hurt realization of rarer attribute-values; still no direct correspondence to utterance realization order.",
            "failure_cases": "High semantic error rates on some architectures/datasets (notably biGRU on ViGGO); inconsistent realization of rare attributes.",
            "uuid": "e8983.1",
            "source_info": {
                "paper_title": "Controllable Meaning Representation to Text Generation: Linearization and Data Augmentation Strategies",
                "publication_date_yy_mm": "2020-11"
            }
        },
        {
            "name_short": "FP",
            "name_full": "Fixed-position linearization",
            "brief_description": "Use a fixed global ordering of attributes (not attribute-values) determined by training frequency; absent attributes are filled with explicit N/A tokens and list slots are expanded to fixed length.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "fixed-position linearization",
            "representation_description": "Create a fixed ordering of attribute slots across all MRs (ordered by increasing attribute frequency); represent absent attributes explicitly as 'N/A' tokens; for list-valued attributes allocate a maximum observed list length as repeated slots.",
            "graph_type": "task-oriented MR with a fixed attribute schema (suitable when attribute set is modest in size)",
            "conversion_method": "deterministic serialization of MR into a slot sequence where each attribute has a fixed slot position and a concrete token (value or N/A) is placed in that slot; list-valued attributes use repeated slots up to a preset max.",
            "downstream_task": "MR-to-text generation (E2E and ViGGO) with S2S models",
            "performance_metrics": "FP has mixed BLEU/ROUGE-L and generally higher SERs than AT; examples include E2E FP SERs up to 6.54% in some settings and ViGGO SERs up to 17.12% in some settings. FP sometimes shows lower variance but nonetheless underperforms AT for controllability.",
            "comparison_to_others": "Better than IF in some metrics due to consistent slot positions and explicit N/A, but worse than AT for generating utterances that follow a supplied realization order. FP can approach AT in some low-variance situations but overall AT+planner is superior.",
            "advantages": "Consistency across examples; explicit representation of absent attributes reduces ambiguity; feasible for datasets with modest attribute sets.",
            "disadvantages": "Scales poorly to large attribute vocabularies (10s-100s attributes); forces a fixed structure that may be unnatural for variable-length/list-valued slots; still not aligned to realization order.",
            "failure_cases": "Higher semantic error rates on several settings; not suitable when attribute vocabulary is large or highly variable.",
            "uuid": "e8983.2",
            "source_info": {
                "paper_title": "Controllable Meaning Representation to Text Generation: Linearization and Data Augmentation Strategies",
                "publication_date_yy_mm": "2020-11"
            }
        },
        {
            "name_short": "AT",
            "name_full": "Alignment Training (aligned linearization)",
            "brief_description": "Linearize the MR during training to match the order in which attribute-values are realized in the reference utterance, enabling fine-grained controllability of phrase order at test time.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "alignment training / aligned linearization",
            "representation_description": "During training, order attribute-value pairs x1..xn in the input MR according to the order their corresponding attribute-values are realized (aligned) in the reference utterance, obtained via pattern-matching rules linking MR attributes to utterance subspans. At test time the trained model can accept arbitrary input orderings (utterance plans) and attempt to realize phrases in that order.",
            "graph_type": "task-oriented MR where attributes map one-to-one to utterance subspans; effectively a mapping from a flat attribute graph to a linear plan",
            "conversion_method": "Use manually constructed heuristic matching rules to align attribute-values to spans in reference utterances, then serialize MR tokens in that realized order to form the encoder input sequence. At test time, external utterance planners (bigram or neural) or humans can supply arbitrary orderings as input plans.",
            "downstream_task": "Controllable MR-to-text generation (produce utterances that follow a given utterance plan) on E2E and ViGGO; experiments with biGRU, Transformer, and BART",
            "performance_metrics": "AT substantially improves controllability and faithfulness. Reported examples: on E2E test AT+NUP+P achieved SER = 0.00% and OA = 100.0% (near-zero semantic errors and perfect order accuracy) in top configurations; AT models frequently reduced SER to near 0 and OA to &gt;98% across architectures. On ViGGO AT+NUP reduced SER relative to baselines but non-zero (e.g., Transformer AT+NUP reported SERs ~2.28%--2.70% and OA ~88%--94% depending on architecture; BART AT+NUP+p achieved SER 0.02% and OA ~99.8% in some reported settings). Random-permutation stress tests showed AT models with phrase training and a neural planner could achieve low SER on E2E (&lt;0.6%) but higher SER/OA degradation on ViGGO (e.g., biGRU+NUP+P SER 8.98% and OA 64.50% on random permutations).",
            "comparison_to_others": "Directly compared to RND, IF, FP: AT yields far superior order accuracy (OA) and much lower semantic error rates (SER) for controllability tasks. Prior work (Nayak et al., Reed et al., Balakrishnan) observed similar benefits; the paper extends comparisons by stress-testing on random permutations and evaluating augmentation effects.",
            "advantages": "Enables explicit, fine-grained control of phrase realization order; greatly reduces semantic errors when paired with an utterance planner; robust across architectures and with pre-trained models (BART). Also reduces variance across random initializations.",
            "disadvantages": "Requires accurate MR-utterance alignments during training, which in this paper demanded extensive manual heuristic rule development and dataset cleaning; may not generalize well to attribute-values that do not map cleanly to disjoint subspans.",
            "failure_cases": "Performance degrades on arbitrary/random utterance plans not seen in training distribution, especially in small-data settings (ViGGO) and with weaker encoders (biGRU, Transformer trained from scratch). Requires manual alignment rules; alignment errors or missing alignments limit applicability.",
            "uuid": "e8983.3",
            "source_info": {
                "paper_title": "Controllable Meaning Representation to Text Generation: Linearization and Data Augmentation Strategies",
                "publication_date_yy_mm": "2020-11"
            }
        },
        {
            "name_short": "PhraseAug",
            "name_full": "Phrase-based data augmentation",
            "brief_description": "Augment training data with MR/utterance pairs derived from constituent phrases of original utterances to improve realization of fragments and robustness to unusual orderings.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "phrase-based data augmentation",
            "representation_description": "Parse training utterances for constituent phrases (NP, VP, ADJP, ADVP, PP, S, Sbar), apply the same matching rules to each phrase to produce a corresponding MR fragment, and add these fragment MR/utterance pairs to the training set; phrase examples use phrase-specific start/stop tokens (e.g., start-NP/stop-NP) to prevent generating incomplete sentences for full MRs.",
            "graph_type": "subgraph fragments of task-oriented MR (attribute-value subsets corresponding to phrase realizations)",
            "conversion_method": "Syntactic parsing (Stanford CoreNLP) to extract constituent phrases, apply alignment/matching rules to map phrase textual spans to attribute-values, reclassify/dialogue-act preserved, filter out phrases with no realized attributes, and insert phrase examples (with special boundary tokens) into training.",
            "downstream_task": "MR-to-text generation; improves robustness of S2S models (biGRU, Transformer, BART) to arbitrary utterance plans and reduces semantic error rate",
            "performance_metrics": "Phrase augmentation (+P) reduced SER in 8/12 reported cases; large improvements on E2E (e.g., many models reached near-zero SER after +P). Random permutation stress test: phrase training helped E2E models achieve &lt;0.6% SER and improved ViGGO results in several architectures; in one top BART setting AT+NUP+P achieved ViGGO SER = 0.76% and OA = 95.32% (Table 4). Human eval: phrase-augmented BART+NUP preferred for naturalness.",
            "comparison_to_others": "When combined with AT and a neural planner (NUP), phrase augmentation produced the strongest control and lowest SER compared to models trained without phrase data; for some small-data ViGGO settings phrase training slightly hurt fluency (human eval) but still improved SER overall.",
            "advantages": "Improves model robustness to follow arbitrary or difficult utterance plans; reduces semantic errors especially where base SER is higher; captures fragment-level realizations and inversions (e.g., handling negation flips).",
            "disadvantages": "In small-data settings (ViGGO) phrase augmentation can slightly hurt fluency when models must follow very unnatural random orders; increases training data size considerably and may require careful filtering.",
            "failure_cases": "May reduce naturalness for difficult random orderings in small-data regimes; does not fully eliminate SER for complex, out-of-distribution permutations (ViGGO random-permutation OA and SER still degraded).",
            "uuid": "e8983.4",
            "source_info": {
                "paper_title": "Controllable Meaning Representation to Text Generation: Linearization and Data Augmentation Strategies",
                "publication_date_yy_mm": "2020-11"
            }
        },
        {
            "name_short": "BGUP",
            "name_full": "Attribute-value bigram utterance planner",
            "brief_description": "A simple bigram language model over attribute-value sequences trained on AT-orderings to propose likely utterance plans.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "bigram utterance planner (BGUP)",
            "representation_description": "Estimate bigram probabilities of attribute-value token sequences from training data (using Lidstone smoothing alpha=1e-6) according to AT ordering; at test time generate candidate attribute-value orderings via beam search constrained to the input MR.",
            "graph_type": "ordering/planner over flat MR attribute-value nodes (sequence of nodes)",
            "conversion_method": "Statistical bigram model over serialized attribute-value tokens; generation via constrained beam search (beam=32) producing permutations that cover all MR items without repeats.",
            "downstream_task": "Provide utterance plans (input orderings) for AT-trained S2S models to follow in MR-to-text generation",
            "performance_metrics": "BGUP-produced plans when used with AT-trained generators achieved substantial OA and low SER on E2E (e.g., AT+BgUP E2E OA ~98.2%, SER as low as 0.26% in some settings); BGUP Kendall's tau on validation/test lower than NUP (Table 10: E2E BGUP tau 0.417/0.347; NUP 0.739/0.651).",
            "comparison_to_others": "NUP (neural planner) typically generates plans closer to reference orderings (higher Kendall's tau) and often yields slightly higher BLEU/ROUGE-L when paired with AT models. BGUP is simpler but effective at reducing SER compared to no-planner baselines.",
            "advantages": "Simple, fast to train; provides plausible, low-entropy orderings that are easier for AT models to follow and reduce semantic errors.",
            "disadvantages": "Limited modeling capacity (only local bigram dependencies); produces less human-like orderings than NUP according to Kendall's tau and BLEU/ROUGE comparisons.",
            "failure_cases": "Less effective at matching complex, dialogue-act-specific ordering preferences that NUP can learn; lower Kendall's tau than neural planner.",
            "uuid": "e8983.5",
            "source_info": {
                "paper_title": "Controllable Meaning Representation to Text Generation: Linearization and Data Augmentation Strategies",
                "publication_date_yy_mm": "2020-11"
            }
        },
        {
            "name_short": "NUP",
            "name_full": "Neural utterance planner",
            "brief_description": "A sequence-to-sequence RNN planner that maps a default ordering (IF) of attribute-values to AT ordering, producing more natural utterance plans for AT-trained generators.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "neural utterance planner (NUP)",
            "representation_description": "BiGRU/LSTM-based seq2seq model with attention trained to map IF-ordered attribute-value sequences to AT (aligned) orderings; conditions on dialogue act so planner can learn act-specific ordering preferences; at test time constrained beam search generates orderings that include all MR items.",
            "graph_type": "planner over sequence representations of flat MR attribute-value nodes",
            "conversion_method": "Train RNN encoder-decoder (512-dim embeddings/hidden etc.) to predict permutation (sequence) of given MR, using beam search constrained to MR items to produce candidate utterance plans.",
            "downstream_task": "Generate test-time utterance plans that AT-trained S2S models should follow, improving semantic correctness and naturalness of generated text",
            "performance_metrics": "NUP achieved higher Kendall's tau than BGUP (Table 10: E2E validation/test tau 0.739/0.651 vs BGUP 0.417/0.347; ViGGO 0.502/0.447 vs BGUP 0.433/0.432). Pairing AT+NUP often yielded lowest SER and highest OA (e.g., many configurations showed AT+NUP SER near 0% on E2E and reduced SER on ViGGO relative to baselines).",
            "comparison_to_others": "Outperforms BGUP in producing orderings closer to reference orders (higher Kendall's tau) and generally yields slightly higher BLEU/ROUGE-L and lower SER when paired with AT-trained generators. AT+NUP typically best overall.",
            "advantages": "Learns global ordering preferences including dialogue-act specific patterns; produces more natural/closer-to-reference plans than BGUP, improving BLEU/ROUGE-L and reducing SER.",
            "disadvantages": "Needs training data and hyperparameter tuning; may still produce plans that are easier for the generator than true human oracle orders (models tend to prefer low-entropy orders seen in training).",
            "failure_cases": "When given arbitrary random permutations as input (stress test), NUP reordering helps but cannot fully eliminate SER increases for hard permutations, especially in small-data settings (ViGGO).",
            "uuid": "e8983.6",
            "source_info": {
                "paper_title": "Controllable Meaning Representation to Text Generation: Linearization and Data Augmentation Strategies",
                "publication_date_yy_mm": "2020-11"
            }
        },
        {
            "name_short": "BART-format",
            "name_full": "Formatting MR as string for BART fine-tuning",
            "brief_description": "Instead of extending the encoder vocabulary, represent the linearized MR as a plain string (e.g., 'inform rating=good name=NAME platforms=PC') so BART's subword encoder can encode MR tokens.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "string-serialization for pretrained S2S encoder (BART)",
            "representation_description": "Serialize the linearized MR into a plain textual string respecting the chosen linearization order and feed it to BART's subword tokenizer/encoder without adding new tokens to the encoder vocabulary.",
            "graph_type": "flat MR (attribute-value sequence) serialized as text",
            "conversion_method": "Concatenate dialogue-act and attribute-value pairs into a single textual string in the target linearization order; rely on subword encoding to represent attribute tokens; delexicalize rare values where needed and post-process outputs.",
            "downstream_task": "Fine-tune BART for MR-to-text generation using same downstream evaluation (E2E, ViGGO)",
            "performance_metrics": "Fine-tuned BART models using AT+NUP+P achieved among the best tradeoffs: examples include E2E SER as low as 0.02% and OA ~99.8% in top runs; ViGGO BART AT+NUP+P reported SER 0.76% and OA 95.32% in stress tests (Table 4); BART generally outperforms from-scratch Transformer/biGRU on small-data ViGGO.",
            "comparison_to_others": "BART fine-tuning with serialized MR competes with or exceeds from-scratch Transformer/biGRU models, especially in small-data settings where pretraining helps generalization and following arbitrary plans.",
            "advantages": "Leverages large pretrained language model robustness to generalize ordering-to-text mapping; avoids modifying BART vocab by serializing MR as plain text; strong faithfulness and fluency in experiments.",
            "disadvantages": "Relies on subword segmentation of attribute tokens which may be inconsistent; still benefits from alignment training and planner components.",
            "failure_cases": "Although strong, BART still shows non-zero SER and less-than-perfect OA on ViGGO under random permutations; formatting does not eliminate the need for aligned training and augmentation to achieve controllability.",
            "uuid": "e8983.7",
            "source_info": {
                "paper_title": "Controllable Meaning Representation to Text Generation: Linearization and Data Augmentation Strategies",
                "publication_date_yy_mm": "2020-11"
            }
        },
        {
            "name_short": "Delex",
            "name_full": "Delexicalization of rare attribute values",
            "brief_description": "Replace infrequent attribute values (e.g., names, developer) with attribute-specific placeholders during training, and re-lexicalize at inference to reduce data sparsity.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "delexicalization / placeholder serialization",
            "representation_description": "Map rare open-class or low-frequency attribute values to placeholder tokens (e.g., NAME, SPECIFIER_V_S) in both MR and target utterance during training; at decode time post-process generated placeholder tokens by substituting the original attribute values.",
            "graph_type": "task-oriented MR containing open-class attribute-values (names, specifiers)",
            "conversion_method": "Replace attribute values in both MR input and reference outputs with attribute-specific placeholder tokens; represent grammatical features (e.g., consonant/vowel initial, regular/superlative) for specifiers as part of placeholder encoding.",
            "downstream_task": "MR-to-text generation; reduces sparsity and improves generalization for rare values in E2E/ViGGO",
            "performance_metrics": "Used as pre/post-processing; improves robustness/generalization but no isolated numeric ablation reported; included as part of overall system configuration contributing to the low SERs reported for AT+NUP+P and BART runs.",
            "comparison_to_others": "Standard practical technique; combined with AT and phrase augmentation it supports low SERs on small datasets where lexical sparsity would otherwise hurt.",
            "advantages": "Reduces learning difficulty for rare tokens; handles infrequent open-class specifiers and ensures correct insertion of values at re-lexicalization.",
            "disadvantages": "Requires correct post-processing re-lexicalization; mask may hide subtle context-dependent realization decisions during training.",
            "failure_cases": "If re-lexicalization or placeholder selection is incorrect, can produce wrong surface forms; not a replacement for alignment.",
            "uuid": "e8983.8",
            "source_info": {
                "paper_title": "Controllable Meaning Representation to Text Generation: Linearization and Data Augmentation Strategies",
                "publication_date_yy_mm": "2020-11"
            }
        },
        {
            "name_short": "TreeMR (cited)",
            "name_full": "Tree-structured MR and linearized trees (Balakrishnan et al., 2019)",
            "brief_description": "Prior work experimented with explicit tree-structured MRs and compared tree encoders vs linearized-tree S2S encoders, finding aligned linearization can enable controllable generation.",
            "citation_title": "Constrained Decoding for Neural NLG from Compositional Representations in Task-Oriented Dialogue",
            "mention_or_use": "mention",
            "representation_name": "tree-structured MR / linearized tree serialization",
            "representation_description": "Represent MR as a tree (compositional representation) and either encode it with a tree-structured encoder or linearize the tree (e.g., depth-first traversal) for standard S2S encoders.",
            "graph_type": "compositional/task-oriented MR that can be structured as a tree",
            "conversion_method": "Either use specialized tree encoders or serialize the tree into a linear sequence (linearized tree) for S2S encoders; traversal order choices matter for downstream realization.",
            "downstream_task": "Data-to-text generation in task-oriented dialogue; comparison of encoder types/serializations",
            "performance_metrics": "Referenced as comparable work; this paper notes Balakrishnan et al. also found aligned linearization leads to controllable generation but does not provide detailed numeric comparisons here.",
            "comparison_to_others": "Paper notes Balakrishnan compared tree encoders to linearized trees and found aligned linearization effective; our work extends by systematically comparing alternative linearizations and stress-testing random permutations.",
            "advantages": "Tree encodings can capture compositional structure explicitly; linearized trees allow use of standard S2S models while preserving structural hints.",
            "disadvantages": "Tree encoders require specialized architectures; linearizations introduce order choices that strongly affect behavior.",
            "failure_cases": "Not explored in detail in this paper; cited as related work with similar findings about alignment importance.",
            "uuid": "e8983.9",
            "source_info": {
                "paper_title": "Controllable Meaning Representation to Text Generation: Linearization and Data Augmentation Strategies",
                "publication_date_yy_mm": "2020-11"
            }
        },
        {
            "name_short": "AMR-linear",
            "name_full": "AMR linearizations (cited)",
            "brief_description": "Prior comparisons of different AMR graph linearizations for AMR-to-text generation showed alignment-like strategies can be effective but earlier evaluations lacked explicit semantic correctness measures.",
            "citation_title": "Linguistic realisation as machine translation: Comparing different MT models for AMR-to-text generation",
            "mention_or_use": "mention",
            "representation_name": "AMR graph linearization / serialization",
            "representation_description": "Convert AMR abstract meaning representation graphs into linear token sequences (various orders) for use with MT-like or S2S generators; alignment-style serializations correspond to realization order when available.",
            "graph_type": "Abstract Meaning Representation (AMR) graphs",
            "conversion_method": "Graph-to-sequence linearization schemes (various traversals and alignments) to produce input sequences for seq2seq models.",
            "downstream_task": "AMR-to-text generation",
            "performance_metrics": "Cited Castro Ferreira et al. (2017) compared several linearizations for AMR but evaluated mostly with automatic quality metrics (BLEU) and did not measure semantic correctness or order-following explicitly.",
            "comparison_to_others": "Paper references this work as related but notes the lack of semantic correctness/order-following evaluation; our paper provides such analyses in the MR-to-text context.",
            "advantages": "Allows use of standard S2S models for graph-to-text tasks; alignment-based linearizations may improve controllability.",
            "disadvantages": "May not capture graph structure fully; choice of linearization affects model behavior and is under-studied in terms of semantic fidelity.",
            "failure_cases": "Not directly evaluated in this paper; Castro Ferreira et al. focused on automatic metrics only.",
            "uuid": "e8983.10",
            "source_info": {
                "paper_title": "Controllable Meaning Representation to Text Generation: Linearization and Data Augmentation Strategies",
                "publication_date_yy_mm": "2020-11"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "To Plan or not to Plan? Discourse Planning in Slot-Value Informed Sequence to Sequence Models for Language Generation",
            "rating": 2
        },
        {
            "paper_title": "Linguistic realisation as machine translation: Comparing different MT models for AMR-to-text generation",
            "rating": 2
        },
        {
            "paper_title": "Constrained Decoding for Neural NLG from Compositional Representations in Task-Oriented Dialogue",
            "rating": 2
        },
        {
            "paper_title": "Can Neural Generators for Dialogue Learn Sentence Planning and Discourse Structuring?",
            "rating": 2
        },
        {
            "paper_title": "Neural data-to-text generation: A comparison between pipeline and end-to-end architectures",
            "rating": 2
        },
        {
            "paper_title": "A Deep Ensemble Model with Slot Alignment for Sequence-toSequence Natural Language Generation",
            "rating": 1
        },
        {
            "paper_title": "Improving Quality and Efficiency in Planbased Neural Data-to-text Generation",
            "rating": 1
        },
        {
            "paper_title": "Step-by-Step: Separating Planning from Realization in Neural Data-to-Text Generation",
            "rating": 1
        }
    ],
    "cost": 0.0225325,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Controllable Meaning Representation to Text Generation: Linearization and Data Augmentation Strategies</h1>
<p>Chris Kedzie<br>Columbia University<br>Department of Computer Science<br>kedzie@cs.columbia.edu</p>
<h2>Kathleen McKeown</h2>
<p>Columbia University
Department of Computer Science
kathy@cs.columbia.edu</p>
<h2>Abstract</h2>
<p>We study the degree to which neural sequence-to-sequence models exhibit fine-grained controllability when performing natural language generation from a meaning representation. Using two task-oriented dialogue generation benchmarks, we systematically compare the effect of four input linearization strategies on controllability and faithfulness. Additionally, we evaluate how a phrase-based data augmentation method can improve performance. We find that properly aligning input sequences during training leads to highly controllable generation, both when training from scratch or when fine-tuning a larger pre-trained model. Data augmentation further improves control on difficult, randomly generated utterance plans.</p>
<h2>1 Introduction</h2>
<p>In this work, we study the degree to which neural sequence-to-sequence (S2S) models exhibit finegrained controllability when performing natural language generation (NLG) from a meaning representation (MR). In particular, we focus on an S2S approach that respects the realization ordering constraints of a given utterance plan; such a model can generate utterances whose phrases follow the order of the provided plan.</p>
<p>In non-neural NLG, fine-grained control for planning sentence structure has received extensive study under the names sentence or micro-planning (Reiter and Dale, 2000; Walker et al., 2001; Stone et al., 2003). Contemporary practice, however, eschews modeling at this granularity, instead preferring to train an S2S model to directly map an input MR to a natural language utterance, with the utterance plan determined implicitly by the model which is learned from the training data (Duek et al., 2020).</p>
<p>We argue that robust and fine grained control in an S2S model is desirable because it enables neural</p>
<h2>MR/Utterance Pair</h2>
<p>$\left[\begin{array}{l}\text { REQUEST } \ \text { EXPLANATION } \ \text { genres }={ \ \text { "role-playing", }{ }^{(1)} \ \text { "hack-and-slash", }{ }^{(2)}\end{array}\right]$ ESRB $=$ "M (Mature) ${ }^{\prime \prime(3)}$ rating $=$ "good" ${ }^{\prime(4)}$</p>
<p>What is it about
$\frac{M \text { rated }^{T} \text { hack-and- }}{\text { slash }^{\overline{2}} \text { RPGs }^{\overline{1}} \text { that }}$ $\frac{\text { makes you enjoy }}{\text { them? }^{\text {4 }}}$</p>
<p>Figure 1: Example MR for Request Explanation dialogue act (left) and utterance (right) pair from the ViGGO dataset. Superscripts indicate which attributevalues correspond to which utterance subspans.
implementations of various psycho-linguistic theories of discourse (e.g., Centering Theory (Grosz et al., 1995), or Accessibility Theory (Ariel, 2001)). This could, in turn, encourage the validation and/or refinement of additional psychologically plausible models of language production.</p>
<p>In this paper, we study controllability in the context of task-oriented dialogue generation (Mairesse et al., 2010; Wen et al., 2015), where the input to the NLG model is an MR consisting of a dialogue act (i.e. a communicative goal) such as to REQUEST EXPLANATION, and an unordered set of attribute-value pairs defining the semantics of the intended utterance (see Figure 1 for an example).</p>
<p>The NLG model is expected to produce an utterance that adequately and faithfully communicates the MR. In the S2S paradigm, the MR must be "linearized" (i.e. represented as a linear sequence of tokens corresponding to the dialogue act and attribute-value pairs) before being presented to the S2S encoder. We explore several linearization strategies and measure their effectiveness for controlling phrase order, as well as their effect on model faithfulness (i.e., the semantic correctness of generated utterances).</p>
<p>Of particular note, alignment training (i.e. at training time, linearizing the attribute-value pairs according to the order in which they are realized by their corresponding reference utterance) produces highly controllable S2S models. While we are not the first to observe this (c.f., Nayak et al. (2017)), we study this behavior extensively. We refer to an ordered sequence of attribute-value pairs $x_{1}, x_{2}, \ldots, x_{n}$ as an utterance plan, and evaluate models on their ability to follow such plans given by either another model, a human, or, most difficultly, from random permutation.</p>
<p>Additionally, we experiment with a data augmentation method, where we create fragmentary MR/utterance pairs obtained from the constituent phrases of the original training data. We find that this data augmentation results in reduced semantic error rates and increases the ability of a model to follow an arbitrary utterance plan.</p>
<p>We summarize our contributions as follows. (1) We show that alignment training produces highly controllable language generation models, especially when following a model created utterance plan. (2) We demonstrate that phrasebased data augmentation improves the robustness of the control even on arbitrary and difficult to follow utterance plans. (3) We conclude with a human evaluation that shows that phrase-based data augmentation training can increase the robustness of control without hurting fluency. ${ }^{1}$</p>
<h2>2 Methods</h2>
<p>In an MR-to-text task, we are given as input an MR $\mu \in \mathcal{M}$ from which to generate an appropriate natural language utterance $\mathbf{y} \in \mathcal{Y}$, where $\mu$ consists of a dialogue act that characterizes the communicative goal of the utterance and an unordered and variably sized set of attribute-value pairs. Attributes are either binary or categorical variables (e.g., family-friendly: ["yes", "no"] or food: ["Chinese", "English", "French", ...]). ${ }^{2}$</p>
<p>Let each attribute-value pair $x$ and dialogue act $a$ be tokens from a vocabulary $\mathcal{V}$, and define the size of an MR, denoted $|\mu|$, to be the number of attribute-value pairs $x \in \mu$. A linearization strategy $\pi: \mathcal{M} \rightarrow \mathcal{V}^{*}$ is a mapping of the dialogue act and</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>attribute-value pairs in $\mu$ to an ordered sequence, i.e. $\pi(\mu)=\left[a, x_{1}, x_{2}, \ldots, x_{|\mu|}\right]$. Regardless of the choice of $\pi$, the first token in $\pi(\mu)$ is always the dialogue act $a$.</p>
<p>We experiment with both gated recurrent unit (GRU) (Cho et al., 2014) and Transformer (Vaswani et al., 2017) based S2S model variants to implement a conditional probability model $p(\cdot \mid \pi(\mu) ; \theta): \mathcal{Y} \rightarrow(0,1)$ over utterances. The model parameters, $\theta$, are learned by approximately maximizing the log-likelihood $\mathcal{L}(\theta)=$ $\sum_{(p, \mathbf{y}) \in \mathcal{D}} \log p(\mathbf{y} \mid \pi(\mu) ; \theta)$ on the training set $\mathcal{D}$. Additionally, we experiment with a pretrained S2S Transformer, BART (Lewis et al., 2020), with parameters $\theta_{0}$ fine-tuned on $\mathcal{L}\left(\theta_{0}\right)$.</p>
<h3>2.1 Linearization Strategies</h3>
<p>Because of the recurrence in the GRU and position embeddings in the Transformer, it is usually the case that different linearization strategies, i.e. $\pi(\mu) \neq \pi^{\prime}(\mu)$, will result in different model internal representations and therefore different conditional probability distributions. These differences can be non-trivial, yielding changes in model behavior with respect to faithfulness and control.</p>
<p>We study four linearization strategies, (i) random, (ii) increasing-frequency, (iii) fixed-position, and (iv) alignment training, which we describe below. For visual examples of each strategy, see Figure 2. Note that linearization determines the order of the attribute-value pairs presented to the S2S encoder, and only in the case of alignment training does it correspond to the order in which the attribute-value pairs are realized in the utterance. When presenting a linearized MR to the model encoder, we always prepend and append distinguished start and stop tokens respectively.</p>
<p>Random (RND) In the random linearization (RND), we randomly order the attribute-value pairs for a given MR. This strategy serves as a baseline for determining if linearization matters at all for faithfulness. RND is similar to token level noise used in denoising auto-encoders (Wang et al., 2019) and might even improve faithfulness. During training, we resample the ordering for each example at every epoch. We do not resample the validation set in order to obtain stable results for model selection.</p>
<p>Increasing Frequency (IF) In the increasing frequency linearization (IF), we order the attributevalue pairs by increasing frequency of occurrence in the training data (i.e., $\operatorname{count}\left(x_{i}\right) \leq$</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 2: Example MR linearization strategies for the utterance above from the ViGGO training set.
$\operatorname{count}\left(x_{i+1}\right)$ ). We hypothesize that placing frequently occurring items in a consistent location may make it easier for $p$ to realize those items correctly, possibly at the expense of rarer items.</p>
<p>Fixed Position (FP) We take consistency one step further and create a fixed ordering of all attributes, n.b. not attribute-values, ordering them in increasing frequency of occurrence on the training set (i.e. every instance has the same order of attributes in the encoder input). In this fixed position linearization (FP), attributes that are not present in an MR are explicitly represented with an "N/A" value. For list-valued slots, we determine the maximum length list in the training data and create that many repeated slots in the input sequence. This linearization is feasible for datasets with a modest number of unique attributes but may not easily scale to $10 \mathrm{~s}, 100 \mathrm{~s}$, or larger attribute vocabularies.</p>
<p>Alignment Training (AT) In the alignment training linearization (AT), during training, the order of attribute-value pairs $x_{1}, x_{2}, \ldots, x_{|\mu|}$ matches the order in which they are realized in the corresponding training utterance. This is feasible because in the majority of cases, there is a one-toone mapping of attribute-values and utterance subspans.</p>
<p>We obtain this ordering using a manually constructed set of matching rules to identify which utterance subspans correspond to each attribute-</p>
<div class="codehilite"><pre><span></span><code><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">pi</span><span class="p">(</span><span class="err">\</span><span class="nx">mu</span><span class="p">)=</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="p">[</span><span class="nx">inform</span><span class="p">,</span><span class="w"> </span><span class="nx">name</span><span class="p">=</span><span class="nx">Aromi</span><span class="p">,</span><span class="w"> </span><span class="nx">eat_type</span><span class="p">=</span><span class="nx">coffee</span><span class="w"> </span><span class="nx">shop</span><span class="p">,</span><span class="w"> </span><span class="nx">area</span><span class="p">=</span><span class="nx">city</span><span class="w"> </span><span class="nx">centre</span><span class="p">]</span>
<span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">hat</span><span class="p">{</span><span class="nx">y</span><span class="p">}=</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">Aromi</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="nx">a</span><span class="w"> </span><span class="nx">coffee</span><span class="w"> </span><span class="nx">shop</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">city</span><span class="w"> </span><span class="nx">centre</span><span class="p">.</span>
<span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">pi</span><span class="p">(</span><span class="err">\</span><span class="nx">mu</span><span class="p">)=</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="p">[</span><span class="nx">inform</span><span class="p">,</span><span class="w"> </span><span class="nx">eat_type</span><span class="p">=</span><span class="nx">coffee</span><span class="w"> </span><span class="nx">shop</span><span class="p">,</span><span class="w"> </span><span class="nx">name</span><span class="p">=</span><span class="nx">Aromi</span><span class="p">,</span><span class="w"> </span><span class="nx">area</span><span class="p">=</span><span class="nx">city</span><span class="w"> </span><span class="nx">centre</span><span class="p">]</span>
<span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">hat</span><span class="p">{</span><span class="nx">y</span><span class="p">}=</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">There</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="nx">a</span><span class="w"> </span><span class="nx">coffee</span><span class="w"> </span><span class="nx">shop</span><span class="w"> </span><span class="nx">called</span><span class="w"> </span><span class="nx">Aromi</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">city</span><span class="w"> </span><span class="nx">centre</span><span class="p">.</span>
<span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">pi</span><span class="p">(</span><span class="err">\</span><span class="nx">mu</span><span class="p">)=</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="p">[</span><span class="nx">inform</span><span class="p">,</span><span class="w"> </span><span class="nx">eat_type</span><span class="p">=</span><span class="nx">coffee</span><span class="w"> </span><span class="nx">shop</span><span class="p">,</span><span class="w"> </span><span class="nx">area</span><span class="p">=</span><span class="nx">city</span><span class="w"> </span><span class="nx">centre</span><span class="p">,</span><span class="w"> </span><span class="nx">name</span><span class="p">=</span><span class="nx">Aromi</span><span class="p">]</span>
<span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">hat</span><span class="p">{</span><span class="nx">y</span><span class="p">}=</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">For</span><span class="w"> </span><span class="nx">coffee</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">centre</span><span class="w"> </span><span class="nx">of</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">city</span><span class="p">,</span><span class="w"> </span><span class="nx">try</span><span class="w"> </span><span class="nx">Aromi</span><span class="p">.</span>
</code></pre></div>

<p>Figure 3: Example outputs ( $\hat{\mathbf{y}}$ ) from a controllable model, i.e. a S2S model trained with AT linearization, under different input utterance plans $(\pi(\mu))$.
value pair (see $\S 3.1$ ).
Crucially, AT stands in contrast to the first three strategies (RND, IF, and FP) which do not have any correspondence between the the order of attributevalue pairs in $\pi(\mu)$ and the order in which they are realized in the corresponding utterance $\mathbf{y}$.</p>
<p>At test time, when there is no reference utterance AT cannot specify a linearization. However, models trained with AT can generate an utterance from an arbitrary utterance plan $x_{1}, x_{2}, \ldots, x_{|\mu|}$ provided by an external source, such as an utterance planner model or human reference. See Figure 3 for an example of how an AT-trained model might follow three different plans for the same MR.</p>
<h3>2.2 Phrase-based Data Augmentation</h3>
<p>We augment the training data with MR/utterance pairs taken from constituent phrases in the original training data. We parse all training utterances and enumerate all constituent phrases governed by NP,</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">E2E</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Viggo</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Orig.</td>
<td style="text-align: center;">Phr.</td>
<td style="text-align: center;">Orig.</td>
<td style="text-align: center;">Phr.</td>
</tr>
<tr>
<td style="text-align: center;"># ex.</td>
<td style="text-align: center;">33,523</td>
<td style="text-align: center;">443,192</td>
<td style="text-align: center;">5,103</td>
<td style="text-align: center;">67,445</td>
</tr>
<tr>
<td style="text-align: center;">Avg. $</td>
<td style="text-align: center;">\mu</td>
<td style="text-align: center;">$</td>
<td style="text-align: center;">5.3</td>
<td style="text-align: center;">1.8</td>
</tr>
<tr>
<td style="text-align: center;">Avg. $</td>
<td style="text-align: center;">\mathbf{y}</td>
<td style="text-align: center;">$</td>
<td style="text-align: center;">23.6</td>
<td style="text-align: center;">7.0</td>
</tr>
</tbody>
</table>
<p>Table 1: Statistics of original training and phrase data.</p>
<p>VP, ADJP, ADVP, PP, S, or Sbar non-terminals. ${ }^{3}$ We then apply the attribute-value matching rules used for AT (see $\S 3.1$ ) to obtain a corresponding MR, keeping the dialogue act of the original utterance. We discard phrases with no realized attributes. See Table 1 for augmented data statistics.</p>
<p>Because we reclassify the MR of phrases using the matching rules, the augmented data includes examples of how to invert binary attributes, e.g. from the phrase "is not on Mac," which denotes has_mac_release $=$ "no," we obtain the phrase "on Mac" which denotes has_mac_release $=$ "yes." When presenting the linearized MR of phrase examples to the model encoder we prepend and append phrase specific start and stop tokens respectively (e.g., start-NP and stop-NP) to discourage the model from ever producing an incomplete sentence when generating for a complete MR.</p>
<h2>3 Datasets</h2>
<p>We run our experiments on two English language, task-oriented dialogue datasets, the E2E Challenge corpus (Novikova et al., 2017) and the ViGGO corpus (Juraska et al., 2019). These datasets provide MR/utterance pairs from the restaurant and video game domains, respectively. Examples from the E2E corpus (33,523 train/1,426 dev/630 test) can have up to eight unique attributes. There is only one dialogue act for the corpus, INFORM. Attributevalues are either binary or categorical valued.</p>
<p>The ViGGO corpus ( 5,103 train/246 dev/359 test) contains 14 attribute types and nine dialogue acts. In addition to binary and categorical valued attributes, the corpus also features list-valued attributes (see the genres attribute in Figure 1) which can have a variable number of values, and an openclass specifier attribute (see $\S$ A. 1 for details).</p>
<h3>3.1 MR/Utterance Alignments</h3>
<p>The original datasets do not have alignments between individual attribute-value pairs and the sub-</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>spans of the utterances they occur in, which we need for the AT linearization strategy. We manually developed a list of heuristic pattern matching rules (e.g. not kid-friendly $\rightarrow$ family_friendly $=$ "no"). For ViGGO, we started from scratch, but for E2E we greatly expanded the rule-set created by Duek et al. (2019). To ensure the correctness of the rules, we iteratively added new matching rules, ran them on the training and validation sets, and verified that they produced the same MR as was provided in the dataset. This process took one author roughly two weeks to produce approximately 25,000 and 1,500 rules for the E2E and ViGGO datasets respectively. Note that the large number of rules is obtained programmatically, i.e. creating template rules and inserting matching keywords or phrases (e.g., enumerating variants such as not kid-friendly, non kid-friendly, not family-friendly, etc.).</p>
<p>In cases where the matching rules produced different MRs than provided in the original dataset, we manually checked them. In many cases on the E2E dataset and several times on ViGGO, we found the rule to be correct and the MR to be incorrect for the given utterance. In those cases, we used the corrected MRs for training and validation. We do not modify the test sets in any way. Using the matching rules, we can determine alignments between the provided MR and the realized utterances.</p>
<p>For most cases, the attribute-values uniquely correspond to a non-overlapping subspan of the utterance. The rating attribute in the ViGGO dataset, however, could have multiple reasonable mappings to the utterance, so we treat it in practice like an addendum to the dialogue act, occurring directly after the dialogue act as part of a "header" section in any MR linearization strategy (see Figure 2 where rating $=$ "N/A" occurs after the dialogue act regardless of choice of linearization strategy).</p>
<h2>4 Models</h2>
<h3>4.1 Generation Models</h3>
<p>We examine the effects of linearization strategy and data augmentation on a bidirectional GRU with attention (biGRU) and Transformer-based S2S models. Hyperparameters were found using grid-search, selecting the model with best validation Bleu (Papineni et al., 2002) score. We performed a separate grid-search for each architecture-linearization strategy pairing in case there was no one best hyperparameter setting.</p>
<p>Additionally, we fine-tune BART (Lewis et al., 2020), a large pretrained Transformer-based S2S model. We stop fine-tuning after validation set cross-entropy stops decreasing.</p>
<p>Complete architecture specification, hyperparameter search space, and validation results for all three models can be found in Appendix A.</p>
<p>Decoding When decoding at test time, we use beam search with a beam size of eight. Beam candidates are ranked by length normalized log likelihood. Similar to Duek et al. (2019) and Juraska et al. (2019) we rerank the beam output to maximize the $F$-measure of correctly generated attribute-values using the matching-rules described in $\S 3.1$.</p>
<p>For models using the RND linearization, at test time, we sample five random MR orderings and generate beam candidates for each. Reranking is then performed on the union of beam candidates.</p>
<h3>4.2 Utterance Planner Model</h3>
<p>We experiment with three approaches to creating a test-time utterance plan for the AT trained models. The first is a bigram language model (BGUP) over attribute-value sequences. Attribute-value bigram counts are estimated from the training data (using Lidstone smoothing (Chen and Goodman, 1996) with $\alpha=10^{-6}$ ) according to the ordering determined by the matching rules (i.e. the AT ordering).</p>
<p>The second model is a biGRU based S2S model, which we refer to as the neural utterance planner (NUP). We train the NUP to map IF ordered attribute-values to the AT ordering. We grid-search model hyperparameters, selecting the model with highest average Kendall's $\tau$ (Kendall, 1938) on the validation set AT orderings. See Appendix B for hyperparameter/model specification details. Unlike the BGUP model, the NUP model also conditions on the dialogue act, so it can learn ordering preferences that differ across dialogue acts.</p>
<p>For both BGUP and NUP, we use beam search (with beam size 32) to generate candidate utterance plans. The beam search is constrained to only generate attribute-value pairs that are given in the supplied MR, and to avoid generating repeated attributes. The search is not allowed to terminate until all attribute-values in the MR are generated. Beam candidates are ranked by log likelihood.</p>
<p>The final ordering we propose is the Oracle ordering, i.e. the utterance plan implied by the human-authored test-set reference utterances. This
plan represents the model performance if it had $a$ priori knowledge of the reference utterance plan. When a test example has multiple references, we select the most frequent ordering in the references, breaking ties according to BGUP log-likelihood.</p>
<h2>5 Experiments</h2>
<h3>5.1 Test-Set Evaluation</h3>
<p>In our first experiment, we compare performance of the proposed models and linearization strategies on the E2E and ViGGO test sets. For the IF and AT+NUP models we also include variants trained on the union of original training data and phraseaugmented data (see $\S 2.2$ ), which we denote +P .</p>
<p>Evaluation Measures For automatic quality measures, we report Bleu and Rouge-L (Lin, 2004) scores. ${ }^{4}$ Additionally, we use the matching rules to automatically annotate the attribute-value spans of the model generated utterances, and then manually verify/correct them. With the attributevalue annotations in hand we compute the number of missing, wrong, or added attribute-values for each model. From these counts, we compute the semantic error rate (SER) (Duek et al., 2020) where</p>
<p>$$
\mathrm{SER}=\frac{# \text { missing }+# \text { wrong }+# \text { added }}{# \text { attributes }}
$$</p>
<p>On ViGGO, we do not include the rating attribute in this evaluation since we consider it part of the dialogue act. Additionally, for AT variants, we report the order accuracy (OA) as the percentage of generated utterances that correctly follow the provided utterance plan. Utterances with wrong or added attribute values are counted as not following the utterance plan. Additional metrics and SER error break downs can be found in Appendix C.</p>
<p>All models are trained five times with different random seeds; we report the mean of all five runs. We report statistical significance using Welch's $t$ test (Welch, 1947), comparing the score distribution of the five runs from the best linearization strategy against all other strategies at the 0.05 level.</p>
<p>Baselines On the ViGGO dataset we compare to the Transformer baseline of Juraska et al. (2019), which used a beam search of size 10 and heuristic slot reranker (similar to our matching rules).</p>
<p>On the E2E dataset, we report the results of TGen+ (Duek et al., 2019), an LSTM-based S2S</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>model, which also uses beam search with a matching rule based reranker to select the most semantically correct utterance and is trained on a cleaned version of the corpus (similar to our approach).</p>
<h3>5.2 Random Permutation Stress Test</h3>
<p>Differences between an AT model following a utterance planner model and the human oracle are often small so we do not learn much about the limits of controllability of such models, or how they behave in extreme conditions (i.e. on an arbitrary, random utterance plan, not drawn from the training data distribution). In order to perform such an experiment we generate random utterance plans (i.e. permutations of attribute-values) and have the AT models generate utterances for them, which we evaluate with respect to SER and OA (we lack ground truth references with which to evaluate Bleu or ROUGE-L). We generate random permutations of size $3,4, \ldots, 8$ on the E2E dataset, since there are 8 unique attributes on the E2E dataset. For ViGGO we generate permutations of size $3,4, \ldots, 10$ ( $96 \%$ of the ViGGO training examples fall within this range). For each size we generated 100 random permutations and all generated plans were given the INFORM dialogue act. In addition to running the AT models on these random permutations, we also compare them to the same model after using the NUP to reorder them into an easier ${ }^{5}$ ordering. Example outputs can be found in Appendix D.</p>
<h3>5.3 Human Evaluation</h3>
<p>In our final experiment, we had human evaluators rank the 100 outputs of the size 5 random permutations for three BART models on both datasets: (i) AT+P model with NUP, (ii) AT+P model, and (iii) AT model. The first model, which uses an utterance planner, is likely to be more natural since it doesn't have to follow the random order, so it serves as a ceiling. The second and third models will try to follow the random permutation ordering, and are more likely to produce unnatural transitions between awkward sequences of attribute-values. Differences between these models will allow us to understand how the phrase-augmented data affects the fluency of the models. The annotators were asked to rank outputs by their naturalness/fluency. Each set was annotated twice by different annotators so we can compute agreement. More details can be found in Appendix E.</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h2>6 Results</h2>
<p>AT models accurately follow utterance plans. See Table 2 and Table 3 for results on E2E and ViGGO test sets respectively. The best nonORACLE results are bolded for each model and results that are not different with statistical significance to the best results are underlined. We see that the AT+NUP strategy consistently receives the lowest semantic error rate and highest order accuracy, regardless of architecture or dataset, suggesting that alleviating the model's decoder of content planning is highly beneficial to avoiding errors. The Transformer AT model is able to consistently achieve virtually zero semantic error on E2E using either the bigram or neural planner model.</p>
<p>We also see that fine-tuned BART is able to learn to follow an utterance plan as well. When following the neural utterance planner, BART is highly competitive with the trained from scratch Transformer on E2E and surpassing it on ViGGO in terms of semantic error rate.</p>
<p>Generally, the AT models had a smaller variance in test-set evaluation measures over the five random initializations as compared to the other strategies. This is reflected in some unusual equivalency classes by statistical significance. For example, on the E2E dataset biGRU models, the AT+NUP+P strategy acheives $0 \%$ semantic error and is significantly different than all other linearization strategies except the FP strategy even though the absolute difference in score is $6.54 \%$. This is unusual because the AT+NUP+P strategy is significantly different from AT+NUP but the absolute difference is only $0.26 \%$. This happens because the variance in test-set results is higher for FP making it harder to show signficance with only five samples.</p>
<p>Transformer-based models are more faithful than biGRU on RND, FP, and IF linearizations. On the ViGGO dataset, BART and Transformer IF achieve $1.86 \%$ and $7.50 \%$ semantic error rate respectively, while the biGRU IF model has $19.20 \%$ semantic error rate. These trends hold for FP and RND, and on the E2E dataset as well. Because there is no sequential correspondence in the input, it is possible that the recurrence in the biGRU makes it difficult to ignore spurious input ordering effects. Additionally, we see that RND does offer some</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">$\mathrm{B} \uparrow$</th>
<th style="text-align: center;">$\mathrm{R} \uparrow$</th>
<th style="text-align: center;">SER $\downarrow$</th>
<th style="text-align: center;">OA $\uparrow$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">TGen+ <br> (Dulek et al., 2019)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">66.0</td>
<td style="text-align: center;">67.6</td>
<td style="text-align: center;">0.03</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">RND</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">66.8</td>
<td style="text-align: center;">68.3</td>
<td style="text-align: center;">2.64</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">FP</td>
<td style="text-align: center;">63.4</td>
<td style="text-align: center;">65.6</td>
<td style="text-align: center;">6.54</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">IF</td>
<td style="text-align: center;">59.2</td>
<td style="text-align: center;">62.7</td>
<td style="text-align: center;">12.64</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">IF+P</td>
<td style="text-align: center;">65.8</td>
<td style="text-align: center;">68.1</td>
<td style="text-align: center;">0.24</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">AT+BgUP</td>
<td style="text-align: center;">66.4</td>
<td style="text-align: center;">68.3</td>
<td style="text-align: center;">0.26</td>
<td style="text-align: center;">98.2</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">AT+NUP</td>
<td style="text-align: center;">66.3</td>
<td style="text-align: center;">68.9</td>
<td style="text-align: center;">0.26</td>
<td style="text-align: center;">98.3</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">AT+NUP+p</td>
<td style="text-align: center;">66.5</td>
<td style="text-align: center;">69.1</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">100.0</td>
</tr>
<tr>
<td style="text-align: center;">At Oracle</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">69.8</td>
<td style="text-align: center;">77.3</td>
<td style="text-align: center;">0.84</td>
<td style="text-align: center;">94.3</td>
</tr>
<tr>
<td style="text-align: center;">RND</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">67.4</td>
<td style="text-align: center;">68.2</td>
<td style="text-align: center;">1.06</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">FP</td>
<td style="text-align: center;">67.4</td>
<td style="text-align: center;">68.7</td>
<td style="text-align: center;">3.10</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">IF</td>
<td style="text-align: center;">67.1</td>
<td style="text-align: center;">68.1</td>
<td style="text-align: center;">0.66</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">IF+p</td>
<td style="text-align: center;">66.8</td>
<td style="text-align: center;">68.3</td>
<td style="text-align: center;">0.28</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">AT+BgUP</td>
<td style="text-align: center;">66.8</td>
<td style="text-align: center;">68.4</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">99.9</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">AT+NUP</td>
<td style="text-align: center;">67.0</td>
<td style="text-align: center;">69.0</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">100.0</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">AT+NUP+p</td>
<td style="text-align: center;">66.7</td>
<td style="text-align: center;">69.1</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">100.0</td>
</tr>
<tr>
<td style="text-align: center;">At Oracle</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">69.3</td>
<td style="text-align: center;">77.0</td>
<td style="text-align: center;">0.76</td>
<td style="text-align: center;">95.0</td>
</tr>
<tr>
<td style="text-align: center;">RND</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">66.5</td>
<td style="text-align: center;">68.3</td>
<td style="text-align: center;">0.14</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">FP</td>
<td style="text-align: center;">65.5</td>
<td style="text-align: center;">67.2</td>
<td style="text-align: center;">0.16</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">IF</td>
<td style="text-align: center;">65.6</td>
<td style="text-align: center;">67.4</td>
<td style="text-align: center;">0.18</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">IF+p</td>
<td style="text-align: center;">65.9</td>
<td style="text-align: center;">68.2</td>
<td style="text-align: center;">0.30</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">AT+BgUP</td>
<td style="text-align: center;">66.2</td>
<td style="text-align: center;">68.7</td>
<td style="text-align: center;">0.20</td>
<td style="text-align: center;">98.6</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">AT+NUP</td>
<td style="text-align: center;">66.6</td>
<td style="text-align: center;">69.2</td>
<td style="text-align: center;">0.20</td>
<td style="text-align: center;">98.6</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">AT+NUP+p</td>
<td style="text-align: center;">66.3</td>
<td style="text-align: center;">69.3</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">100.0</td>
</tr>
<tr>
<td style="text-align: center;">At Oracle</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">68.3</td>
<td style="text-align: center;">77.1</td>
<td style="text-align: center;">0.70</td>
<td style="text-align: center;">95.3</td>
</tr>
</tbody>
</table>
<p>Table 2: E2E test set (B) Bleu, (R) Rouge-L, SER, and OA. All numbers are percents.
benefits of denoising; RND models have lower semantic error rate than IF models in 3 of 6 cases and FP models in 5 out of 6 cases.</p>
<p>Model based plans are easier to follow than human reference plans. On E2E, there is very little difference in semantic error rate when following either the bigram-based utterance planner, BGUP, or neural utterance planner, NUP. This is also true of the ViGGO BART models as well. In the small data (i.e. ViGGO) setting, biGRU and Transformer models achieve better semantic error rate when following the neural utterance planner. In most cases, neural utterance planner models have slightly higher Bleu and Rouge-L than the bigram utterance planner, suggesting the neural planner produces utterance plans closer to the reference orderings. The neural and bigram planner models have slightly lower semantic error rate than when following the</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">$\mathrm{B} \uparrow$</th>
<th style="text-align: center;">$\mathrm{R} \uparrow$</th>
<th style="text-align: center;">SER $\downarrow$</th>
<th style="text-align: center;">OA $\uparrow$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Transformer <br> (Juraska et al., 2019)</td>
<td style="text-align: center;">52.1</td>
<td style="text-align: center;">63.8</td>
<td style="text-align: center;">$1.60^{6}$</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">RND</td>
<td style="text-align: center;">50.2</td>
<td style="text-align: center;">61.6</td>
<td style="text-align: center;">12.56</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">FP</td>
<td style="text-align: center;">50.2</td>
<td style="text-align: center;">61.0</td>
<td style="text-align: center;">17.12</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">IF</td>
<td style="text-align: center;">50.2</td>
<td style="text-align: center;">61.3</td>
<td style="text-align: center;">19.20</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">IF+p</td>
<td style="text-align: center;">49.5</td>
<td style="text-align: center;">61.6</td>
<td style="text-align: center;">12.46</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">AT+BgUP</td>
<td style="text-align: center;">48.5</td>
<td style="text-align: center;">58.5</td>
<td style="text-align: center;">3.40</td>
<td style="text-align: center;">89.8</td>
</tr>
<tr>
<td style="text-align: center;">AT+NUP</td>
<td style="text-align: center;">51.8</td>
<td style="text-align: center;">62.6</td>
<td style="text-align: center;">1.58</td>
<td style="text-align: center;">93.7</td>
</tr>
<tr>
<td style="text-align: center;">AT+NUP+p</td>
<td style="text-align: center;">52.4</td>
<td style="text-align: center;">62.7</td>
<td style="text-align: center;">1.62</td>
<td style="text-align: center;">94.3</td>
</tr>
<tr>
<td style="text-align: center;">At Oracle</td>
<td style="text-align: center;">54.1</td>
<td style="text-align: center;">65.5</td>
<td style="text-align: center;">2.42</td>
<td style="text-align: center;">92.2</td>
</tr>
<tr>
<td style="text-align: center;">RND</td>
<td style="text-align: center;">52.0</td>
<td style="text-align: center;">62.9</td>
<td style="text-align: center;">9.62</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">FP</td>
<td style="text-align: center;">52.6</td>
<td style="text-align: center;">63.0</td>
<td style="text-align: center;">8.70</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">IF</td>
<td style="text-align: center;">52.3</td>
<td style="text-align: center;">62.6</td>
<td style="text-align: center;">7.50</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">IF+p</td>
<td style="text-align: center;">52.3</td>
<td style="text-align: center;">63.1</td>
<td style="text-align: center;">4.24</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">AT+BgUP</td>
<td style="text-align: center;">48.7</td>
<td style="text-align: center;">59.2</td>
<td style="text-align: center;">4.68</td>
<td style="text-align: center;">79.1</td>
</tr>
<tr>
<td style="text-align: center;">AT+NUP</td>
<td style="text-align: center;">51.6</td>
<td style="text-align: center;">62.4</td>
<td style="text-align: center;">2.70</td>
<td style="text-align: center;">88.3</td>
</tr>
<tr>
<td style="text-align: center;">AT+NUP+p</td>
<td style="text-align: center;">51.1</td>
<td style="text-align: center;">62.0</td>
<td style="text-align: center;">2.28</td>
<td style="text-align: center;">89.8</td>
</tr>
<tr>
<td style="text-align: center;">At Oracle</td>
<td style="text-align: center;">53.2</td>
<td style="text-align: center;">65.0</td>
<td style="text-align: center;">4.08</td>
<td style="text-align: center;">83.0</td>
</tr>
<tr>
<td style="text-align: center;">RND</td>
<td style="text-align: center;">43.7</td>
<td style="text-align: center;">55.1</td>
<td style="text-align: center;">1.50</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">FP</td>
<td style="text-align: center;">47.0</td>
<td style="text-align: center;">58.9</td>
<td style="text-align: center;">1.68</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">IF</td>
<td style="text-align: center;">43.1</td>
<td style="text-align: center;">54.4</td>
<td style="text-align: center;">1.86</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">IF+p</td>
<td style="text-align: center;">49.1</td>
<td style="text-align: center;">59.7</td>
<td style="text-align: center;">1.78</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">AT+BgUP</td>
<td style="text-align: center;">43.8</td>
<td style="text-align: center;">54.0</td>
<td style="text-align: center;">0.52</td>
<td style="text-align: center;">98.3</td>
</tr>
<tr>
<td style="text-align: center;">AT+NUP</td>
<td style="text-align: center;">45.5</td>
<td style="text-align: center;">57.6</td>
<td style="text-align: center;">0.54</td>
<td style="text-align: center;">98.2</td>
</tr>
<tr>
<td style="text-align: center;">AT+NUP+p</td>
<td style="text-align: center;">48.5</td>
<td style="text-align: center;">59.2</td>
<td style="text-align: center;">0.46</td>
<td style="text-align: center;">98.1</td>
</tr>
<tr>
<td style="text-align: center;">At Oracle</td>
<td style="text-align: center;">47.1</td>
<td style="text-align: center;">60.4</td>
<td style="text-align: center;">0.82</td>
<td style="text-align: center;">97.2</td>
</tr>
</tbody>
</table>
<p>Table 3: ViGGO test set (B) Bleu, (R) Rouge-L, SER, and OA. All numbers are percents.</p>
<p>ORACLE utterance plans. This suggests that the models are producing orders more commonly seen in the training data, similar to how neural language generators frequently learn the least interesting, lowest entropy responses (Serban et al., 2016). On the other hand, when given the Oracle orderings, models achieve much higher word overlap with the reference, e.g. achieving an E2E Rouge-L $\geq 77$.</p>
<p>Phrase-training reduces SER. We see that phrase data improves semantic error rate in 8 out of 12 cases, with the largest gains coming from the biGRU IF model. Where the base semantic error rate was higher, phrase training has a more noticeable effect. After phrase training, all E2E models are operating at near zero semantic error rate and almost perfectly following the neural utterance planner. Model performance on ViGGO is more varied, with phrase training slighting hurting</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">E2E</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">ViGGo</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Model</td>
<td style="text-align: center;">SER $\downarrow$</td>
<td style="text-align: center;">OA $\uparrow$</td>
<td style="text-align: center;">SER $\downarrow$</td>
<td style="text-align: center;">OA $\uparrow$</td>
</tr>
<tr>
<td style="text-align: center;">biGRU</td>
<td style="text-align: center;">1.14</td>
<td style="text-align: center;">94.44</td>
<td style="text-align: center;">13.58</td>
<td style="text-align: center;">46.72</td>
</tr>
<tr>
<td style="text-align: center;">+P</td>
<td style="text-align: center;">0.54</td>
<td style="text-align: center;">97.34</td>
<td style="text-align: center;">14.46</td>
<td style="text-align: center;">49.26</td>
</tr>
<tr>
<td style="text-align: center;">+NUP</td>
<td style="text-align: center;">0.22</td>
<td style="text-align: center;">98.72</td>
<td style="text-align: center;">9.62</td>
<td style="text-align: center;">62.04</td>
</tr>
<tr>
<td style="text-align: center;">+NUP+P</td>
<td style="text-align: center;">$\mathbf{0 . 0 2}$</td>
<td style="text-align: center;">$\mathbf{9 9 . 8 6}$</td>
<td style="text-align: center;">$\mathbf{8 . 9 8}$</td>
<td style="text-align: center;">$\mathbf{6 4 . 5 0}$</td>
</tr>
<tr>
<td style="text-align: center;">Transformer</td>
<td style="text-align: center;">0.78</td>
<td style="text-align: center;">95.20</td>
<td style="text-align: center;">28.34</td>
<td style="text-align: center;">18.70</td>
</tr>
<tr>
<td style="text-align: center;">+P</td>
<td style="text-align: center;">0.40</td>
<td style="text-align: center;">98.10</td>
<td style="text-align: center;">25.72</td>
<td style="text-align: center;">18.10</td>
</tr>
<tr>
<td style="text-align: center;">+NUP</td>
<td style="text-align: center;">0.08</td>
<td style="text-align: center;">99.64</td>
<td style="text-align: center;">24.18</td>
<td style="text-align: center;">31.34</td>
</tr>
<tr>
<td style="text-align: center;">+NUP+P</td>
<td style="text-align: center;">$\mathbf{0 . 0 2}$</td>
<td style="text-align: center;">$\mathbf{9 9 . 8 6}$</td>
<td style="text-align: center;">$\mathbf{2 1 . 6 4}$</td>
<td style="text-align: center;">$\mathbf{3 1 . 8 6}$</td>
</tr>
<tr>
<td style="text-align: center;">BART</td>
<td style="text-align: center;">0.42</td>
<td style="text-align: center;">97.78</td>
<td style="text-align: center;">2.30</td>
<td style="text-align: center;">82.00</td>
</tr>
<tr>
<td style="text-align: center;">+P</td>
<td style="text-align: center;">0.22</td>
<td style="text-align: center;">98.78</td>
<td style="text-align: center;">1.82</td>
<td style="text-align: center;">87.98</td>
</tr>
<tr>
<td style="text-align: center;">+NUP</td>
<td style="text-align: center;">0.64</td>
<td style="text-align: center;">96.52</td>
<td style="text-align: center;">1.34</td>
<td style="text-align: center;">91.40</td>
</tr>
<tr>
<td style="text-align: center;">+NUP+P</td>
<td style="text-align: center;">$\mathbf{0 . 2 0}$</td>
<td style="text-align: center;">$\mathbf{9 9 . 0 2}$</td>
<td style="text-align: center;">$\mathbf{0 . 7 6}$</td>
<td style="text-align: center;">$\mathbf{9 5 . 3 2}$</td>
</tr>
</tbody>
</table>
<p>Table 4: Random permutation stress test of AT models.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">1</th>
<th style="text-align: center;">2</th>
<th style="text-align: center;">3</th>
<th style="text-align: center;">Avg.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">AT+NUP+P</td>
<td style="text-align: center;">61.5</td>
<td style="text-align: center;">16.5</td>
<td style="text-align: center;">22.0</td>
<td style="text-align: center;">$\mathbf{1 . 6 1}$</td>
</tr>
<tr>
<td style="text-align: center;">AT+P</td>
<td style="text-align: center;">30.0</td>
<td style="text-align: center;">44.0</td>
<td style="text-align: center;">26.0</td>
<td style="text-align: center;">1.96</td>
</tr>
<tr>
<td style="text-align: center;">AT</td>
<td style="text-align: center;">25.0</td>
<td style="text-align: center;">49.5</td>
<td style="text-align: center;">25.5</td>
<td style="text-align: center;">2.01</td>
</tr>
<tr>
<td style="text-align: center;">AT+NUP+P</td>
<td style="text-align: center;">57.5</td>
<td style="text-align: center;">27.5</td>
<td style="text-align: center;">15.0</td>
<td style="text-align: center;">$\mathbf{1 . 5 8}$</td>
</tr>
<tr>
<td style="text-align: center;">AT+P</td>
<td style="text-align: center;">10.0</td>
<td style="text-align: center;">29.5</td>
<td style="text-align: center;">60.5</td>
<td style="text-align: center;">2.51</td>
</tr>
<tr>
<td style="text-align: center;">AT</td>
<td style="text-align: center;">43.0</td>
<td style="text-align: center;">46.0</td>
<td style="text-align: center;">11.0</td>
<td style="text-align: center;">1.68</td>
</tr>
</tbody>
</table>
<p>Table 5: Human Evaluation results. Table shows the percent of times each model was ranked 1 (best), 2, 3 (worst) in terms of naturalness and average rank.
the biGRU AT+NUP model, but otherwise helping performance.</p>
<p>Random Permutation Stress Test Results of the random permutation experiment are shown in Table 4. Overall, all models have an easier time following the neural utterance planner's reordering of the random permutations. Phrase training also generally improved semantic error rate. All models perform quite well on the E2E permutations. With phrase-training, all E2E models achieve less than $0.6 \%$ semantic error rate following random utterance plans. Starker differences emerge on the ViGGO dataset. The biGRU+NUP+P model achieves a $8.98 \%$ semantic error rate and only correctly follows the given order $64.5 \%$ of the time, which is a large decrease in performance compared to the ViGGO test set.</p>
<p>Human Evaluation Results of the human evaluation are shown in Table 5. We show the number
of times each system was ranked 1 (most natural), 2 , or 3 (least natural) and the average rank overall. Overall, we see that BART with the neural utterance planner and phrase-augmentation training is preferred on both datasets, suggesting that the utterance planner is producing natural orderings of the attribute-values, and the model can generate reasonable output for it. On the E2E dataset, we also see small differences in between the AT+P and AT models suggesting that when following an arbitrary ordering, the phrase-augmented model is about as natural as the non-phrase trained model. This is encouraging as the phrase trained model has lower semantic error rates. On the ViGGO dataset we do find that the phrase trained model is less natural, suggesting that in the small data setting, phrasetraining may hurt fluency when trying to follow a difficult utterance plan.</p>
<p>For agreement we compute average Kendall's $\tau$ between each pair of annotators for each dataset. On E2E, we have $\tau=.853$ and ViGGO we have $\tau=.932$ suggesting very strong agreement.</p>
<h2>7 Discussion</h2>
<p>One consistently worrying sign throughout the first two experiments is that the automatic metrics are not good indicators of semantic correctness. For example the Rouge-L score of the E2E At Oracle models is about 8 points higher than the AT+NUP models, but the AT+NUP models make fewer semantic errors. Other similar examples can be found where the automatic metric would suggest picking the more error prone model over another. As generating fluent text becomes less of a difficult a problem, these shallow ngram overlap methods will cease to suffice as distinguishing criteria.</p>
<p>The second experiments also reveal limitations in the controllable model's ability to follow arbitrary orderings. The biGRU and Transformer models in the small-data ViGGO setting are not able to generalize effectively on non-training distribution utterance plans. BART performance is much better here, but is still hovering around $2 \%$ semantic error rate and only roughly $88 \%$ of outputs conform to the intended utterance plan. Thankfully, if an exact ordering is not required, using the neural utterance planner to propose an order leads to more semantically correct outputs.</p>
<h1>8 Limitations</h1>
<p>While we are able to acheive very low test-set SER for both corpora, we should caution that this required extensive manual development of matching rules to produce MR/utterance alignments, which in turn resulted in significant cleaning of the training datasets. We chose to do this over pursuing a model based strategy of aligning utterance subspans to attribute-values because we wanted to better understand how systematically S2S models can represent arbitray order permutations independent of alignment model error.</p>
<p>Also we should note that data cleaning can yield more substantial decreases in semantic errors (Duek et al., 2019; Wang, 2019) and is an important consideration in any practical neural NLG.</p>
<h2>9 Related Work</h2>
<p>MR linearizations for S2S models have been studied in a variety of prior works. Nayak et al. (2017) explore several ways of incorporating sentence planning into an MR linearization for S2S models, comparing a flat alignment order (equivalent to the alignment order used in this paper) against various sentence level groupings. Reed et al. (2018) add additional sentence and discourse structuring variables to indicate contrasts or sentential groupings. Balakrishnan et al. (2019) experiment both with tree structured MRs and encoders and compare them to linearized trees with standard S2S models. They also find that properly aligned linearization can lead to a controllable generator. These papers do not, however, explore how other linearization strategies compare in terms of faithfulness, and they do not evaluate the degree to which a S2S model can follow realization orders not drawn from the training distribution.</p>
<p>Castro Ferreira et al. (2017) compare a S2S NLG model using various linearizations of abstract meaning representation (AMR) graphs, including a model-based alignment very similar to the AT linearization presented in this work. However, they evaluate only on automatic quality measures and do not explicitly measure the semantic correctness of the generated text or the degree to which the model realizes the text in the order implied by the linearized input.</p>
<p>Works like Moryossef et al. (2019a,b) and Castro Ferreira et al. (2019) show that treating various planning tasks as separate components in a pipeline, where the components themselves are implemented
with neural models, improves the overall quality and semantic correctness of generated utterances relative to a completely end-to-end neural NLG model. However, they do not test the systematicty of the neural generation components, i.e. the ability to perform correctly when given an arbitrary or random input from the preceding component, as we do here with the random permutation stress test.</p>
<p>Other papers mention linearization order anecdottally but do quantify its impact. For example, Juraska et al. (2018) experiment with random linearization orderings during development, but do not use them in the final model or report results using them, and Gehrmann et al. (2018) report that using a consistent linearization strategy worked best for their models but do not specify the exact order. Juraska et al. (2018) also used sentence level data augmentation, i.e. splitting a multi-sentence example in multiple single sentence examples, similar in spirit to our proposed phrase based method, but they do not evaluate its effect independently.</p>
<h2>10 Conclusion</h2>
<p>We present an empirical study on the effects of linearization order and phrase based data augmentation on controllable MR-to-text generation. Our findings support the importance of aligned linearization and phrase training for improving model control. Additionally, we identify limitations to this ability, specifically in the small data, random permutation setting, and will focus on this going forward.</p>
<h2>Acknowledgments</h2>
<p>We would like to thank the anonymous reviewers, as well as Katy Gero and Faisal Ladhak for their comments and discussion when drafting this paper. We would also like to thank Juraj Juraska for making the outputs of their ViGGO models available.</p>
<p>This research is based upon work supported in part by the Office of the Director of National Intelligence (ODNI), Intelligence Advanced Research Projects Activity (IARPA), via contract #FA8650-17-C-9117. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies, either expressed or implied, of ODNI, IARPA, or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for governmental purposes notwithstanding any copyright annotation therein.</p>
<h2>References</h2>
<p>Mira Ariel. 2001. Accessibility theory: An overview. In Text Representation: Linguistic and psycholinguistic aspects, volume 8, pages 29-87. John Benjamins.</p>
<p>Lei Jimmy Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. 2016. Layer Normalization. CoRR, abs/1607.06450.</p>
<p>Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2015. Neural Machine Translation by Jointly Learning to Align and Translate. In 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings.</p>
<p>Anusha Balakrishnan, Jinfeng Rao, Kartikeya Upasani, Michael White, and Rajen Subba. 2019. Constrained Decoding for Neural NLG from Compositional Representations in Task-Oriented Dialogue. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 831844, Florence, Italy. Association for Computational Linguistics.</p>
<p>Thiago Castro Ferreira, Iacer Calixto, Sander Wubben, and Emiel Krahmer. 2017. Linguistic realisation as machine translation: Comparing different MT models for AMR-to-text generation. In Proceedings of the 10th International Conference on Natural Language Generation, pages 1-10, Santiago de Compostela, Spain. Association for Computational Linguistics.</p>
<p>Thiago Castro Ferreira, Chris van der Lee, Emiel van Miltenburg, and Emiel Krahmer. 2019. Neural data-to-text generation: A comparison between pipeline and end-to-end architectures. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 552-562, Hong Kong, China. Association for Computational Linguistics.</p>
<p>Stanley F. Chen and Joshua Goodman. 1996. An Empirical Study of Smoothing Techniques for Language Modeling. In 34th Annual Meeting of the Association for Computational Linguistics, pages 310318, Santa Cruz, California, USA. Association for Computational Linguistics.</p>
<p>Kyunghyun Cho, Bart van Merrinboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. 2014. Learning Phrase Representations using RNN EncoderDecoder for Statistical Machine Translation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1724-1734, Doha, Qatar. Association for Computational Linguistics.</p>
<p>Ondej Duek, David M. Howcroft, and Verena Rieser. 2019. Semantic Noise Matters for Neural Natural</p>
<p>Language Generation. In Proceedings of the 12th International Conference on Natural Language Generation, pages 421-426, Tokyo, Japan. Association for Computational Linguistics.</p>
<p>Ondej Duek, Jekaterina Novikova, and Verena Rieser. 2020. Evaluating the state-of-the-art of End-to-End Natural Language Generation: The E2E NLG challenge. Computer Speech \&amp; Language, 59:123-156.</p>
<p>Sebastian Gehrmann, Falcon Dai, Henry Elder, and Alexander Rush. 2018. End-to-End Content and Plan Selection for Data-to-Text Generation. In Proceedings of the 11th International Conference on Natural Language Generation, pages 46-56, Tilburg University, The Netherlands. Association for Computational Linguistics.</p>
<p>Barbara J. Grosz, Aravind K. Joshi, and Scott Weinstein. 1995. Centering: A Framework for Modeling the Local Coherence of Discourse. Computational Linguistics, 21(2):203-225.</p>
<p>Juraj Juraska, Kevin Bowden, and Marilyn Walker. 2019. ViGGO: A Video Game Corpus for Data-To-Text Generation in Open-Domain Conversation. In Proceedings of the 12th International Conference on Natural Language Generation, pages 164-172, Tokyo, Japan. Association for Computational Linguistics.</p>
<p>Juraj Juraska, Panagiotis Karagiannis, Kevin Bowden, and Marilyn Walker. 2018. A Deep Ensemble Model with Slot Alignment for Sequence-toSequence Natural Language Generation. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 152-162, New Orleans, Louisiana. Association for Computational Linguistics.
M. G. Kendall. 1938. A NEW MEASURE OF RANK CORRELATION. Biometrika, 30(1-2):81-93.</p>
<p>Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2020. BART: Denoising Sequence-to-Sequence Pretraining for Natural Language Generation, Translation, and Comprehension. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7871-7880, Online. Association for Computational Linguistics.</p>
<p>Chin-Yew Lin. 2004. ROUGE: A Package for Automatic Evaluation of Summaries. In Text Summarization Branches Out, pages 74-81, Barcelona, Spain. Association for Computational Linguistics.</p>
<p>Thang Luong, Hieu Pham, and Christopher D. Manning. 2015. Effective Approaches to Attentionbased Neural Machine Translation. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 14121421, Lisbon, Portugal. Association for Computational Linguistics.</p>
<p>Franois Mairesse, Milica Gai, Filip Jurek, Simon Keizer, Blaise Thomson, Kai Yu, and Steve Young. 2010. Phrase-Based Statistical Language Generation Using Graphical Models and Active Learning. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1552-1561, Uppsala, Sweden. Association for Computational Linguistics.</p>
<p>Amit Moryossef, Yoav Goldberg, and Ido Dagan. 2019a. Improving Quality and Efficiency in Planbased Neural Data-to-text Generation. In Proceedings of the 12th International Conference on Natural Language Generation, pages 377-382, Tokyo, Japan. Association for Computational Linguistics.</p>
<p>Amit Moryossef, Yoav Goldberg, and Ido Dagan. 2019b. Step-by-Step: Separating Planning from Realization in Neural Data-to-Text Generation. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 2267-2277, Minneapolis, Minnesota. Association for Computational Linguistics.</p>
<p>Vinod Nair and Geoffrey E. Hinton. 2010. Rectified Linear Units Improve Restricted Boltzmann Machines. In Proceedings of the 27th International Conference on International Conference on Machine Learning, ICML'10, page 807-814, Madison, WI, USA. Omnipress.</p>
<p>Neha Nayak, Dilek Hakkani-Tr, Marilyn Walker, and Larry Heck. 2017. To Plan or not to Plan? Discourse Planning in Slot-Value Informed Sequence to Sequence Models for Language Generation. In Proceedings of Interspeech 2017, pages 3339-3343.</p>
<p>Jekaterina Novikova, Ondej Duek, and Verena Rieser. 2017. The E2E Dataset: New Challenges For End-to-End Generation. In Proceedings of the 18th Annual SIGdial Meeting on Discourse and Dialogue, pages 201-206, Saarbrcken, Germany. Association for Computational Linguistics.</p>
<p>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: a Method for Automatic Evaluation of Machine Translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 311-318, Philadelphia, Pennsylvania, USA. Association for Computational Linguistics.</p>
<p>Lena Reed, Shereen Oraby, and Marilyn Walker. 2018. Can Neural Generators for Dialogue Learn Sentence Planning and Discourse Structuring? In Proceedings of the 11th International Conference on Natural Language Generation, pages 284-295, Tilburg University, The Netherlands. Association for Computational Linguistics.</p>
<p>Ehud Reiter and Robert Dale. 2000. Building Natural Language Generation Systems. Studies in Natural Language Processing. Cambridge University Press.</p>
<p>Alexander Rush. 2018. The Annotated Transformer. In Proceedings of Workshop for NLP Open Source Software (NLP-OSS), pages 52-60, Melbourne, Australia. Association for Computational Linguistics.</p>
<p>Iulian V. Serban, Alessandro Sordoni, Yoshua Bengio, Aaron Courville, and Joelle Pineau. 2016. Building End-to-End Dialogue Systems Using Generative Hierarchical Neural Network Models. In Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence, AAAI'16, page 3776-3783. AAAI Press.</p>
<p>Matthew Stone, Christine Doran, Bonnie Webber, Ton ia Bleam, and Martha Palmer. 2003. Microplanning with Communicative Intentions: The SPUD System. Computational Intelligence, 19(4):311381.</p>
<p>Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, undefinedukasz Kaiser, and Illia Polosukhin. 2017. Attention is All You Need. In Proceedings of the 31st International Conference on Neural Information Processing Systems, NIPS'17, page 6000-6010, Red Hook, NY, USA. Curran Associates Inc.</p>
<p>Marilyn A. Walker, Owen Rambow, and Monica Rogati. 2001. SPoT: A Trainable Sentence Planner. In Second Meeting of the North American Chapter of the Association for Computational Linguistics.</p>
<p>Hongmin Wang. 2019. Revisiting Challenges in Data-to-Text Generation with Fact Grounding. In Proceedings of the 12th International Conference on Natural Language Generation, pages 311-322, Tokyo, Japan. Association for Computational Linguistics.</p>
<p>Liang Wang, Wei Zhao, Ruoyu Jia, Sujian Li, and Jingming Liu. 2019. Denoising based Sequence-to-Sequence Pre-training for Text Generation. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 40034015, Hong Kong, China. Association for Computational Linguistics.
B. L. Welch. 1947. THE GENERALIZATION OF 'STUDENT'S' PROBLEM WHEN SEVERAL DIFFERENT POPULATION VARLANCES ARE INVOLVED. Biometrika, 34(1-2):28-35.</p>
<p>Tsung-Hsien Wen, Milica Gai, Nikola Mrki, PeiHao Su, David Vandyke, and Steve Young. 2015. Semantically Conditioned LSTM-based Natural Language Generation for Spoken Dialogue Systems. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1711-1721, Lisbon, Portugal. Association for Computational Linguistics.</p>
<h2>A Models and Hyper-paramter Search Details</h2>
<h2>A. 1 General Details</h2>
<p>Utterance text was sentence and word tokenized, and all tokens were lower-cased. A special sentence-boundary token was inserted between sentences. All words occurring fewer than 3 times on the training set were replaced with a special unknown token. We used a batch size of 128 for all biGRU and Transformer models. All models were trained on a single Nvidia Tesla v100 for at most 700 epochs.</p>
<p>Delexicalization The ViGGO corpus is relatively small and the attributes name, developer, release_year, expected_release_date, and specifier can have values that are only seen several times during training. Neural models often struggle to learn good representations for infrequent inputs, which can, in turn, lead to poor test-set generalization. To alleviate this, we delexicalize these values in the utterance. That is, we replace them with an attribute specific placeholder token.</p>
<p>Additionally, for specifier whose values come from the open class of adjectives, we represent the specified adjective with a placeholder which marks two features, whether it is consonant (C) or vowel initial (V) (e.g. "dull" vs. "old") and whether it is in regular (R) or superlative (S) form (e.g. "dull" vs. "dullest") since these features can effect the surrounding context in which the adjective is realized. See the following lexicalized/delexicalized examples:</p>
<ul>
<li>specifier $=$ "oldest" - vowel initial, superlative</li>
<li>What is the oldest game you've played?</li>
<li>What is the SPECIFIER_V_S game you've played?</li>
<li>specifier $=$ "old" - vowel initial, regular</li>
<li>What is an old game you've played?</li>
<li>What is an SPECIFIER_V_R game you've played?</li>
<li>specifier $=$ "new" - consonant initial, regular</li>
<li>What is a new game you've played?</li>
<li>What is a SPECIFIER_C_R game you've played?</li>
</ul>
<p>All generated delexicalized utterances are postprocessed with the corresponding attribute-values
before computing evaluation metrics (i.e., they are re-lexicalized with the appropriate value strings from the input MR).</p>
<h2>A. 2 biGRU Model Definition</h2>
<p>Let $\mathcal{V}$ be the encoder input vocabulary, and $\mathbf{E} \in$ $\mathbb{R}^{|\mathcal{V}| \times D_{w}}$ an associated word embedding matrix where $\mathbf{E}<em w="w">{x} \in \mathbb{R}^{D</em>}}$ denotes the $D_{w}$-dimensional embedding for each $x \in \mathcal{V}$. Given a linearized MR $\pi(\mu)=\mathbf{x}=\left[a, x_{1}, x_{2}, \ldots, x_{|\mu|}\right] \in \mathcal{V}^{m}$ where the length of the sequence is $m=|\mu|+1$, let $\mathbf{v<em _mathbf_x="\mathbf{x">{i}=\mathbf{E}</em>$ for $i \in{1, \ldots m}$.}_{i}</p>
<p>The hidden states of the first GRU encoder layer are computed as</p>
<p>$$
\begin{aligned}
&amp; \tilde{\boldsymbol{h}}<em m_1="m+1">{0}^{(1)}=\tilde{\boldsymbol{h}}</em> \
&amp; \tilde{\boldsymbol{h}}}^{(1)}=\mathbf{0<em i="i">{i}^{(1)}=\operatorname{GRU}\left(\mathbf{v}</em>}, \tilde{\boldsymbol{h}<em i="i">{i-1}^{(1)} ; \hat{\eta}^{(1)}\right) \quad \text { for } i \in 1, \ldots, m \
&amp; \tilde{\boldsymbol{h}}</em>}^{(1)}=\operatorname{GRU}\left(\mathbf{v<em i_1="i+1">{i}, \tilde{\boldsymbol{h}}</em> i \in m, \ldots, 1 \
&amp; \boldsymbol{h}}^{(1)} ; \hat{\eta}^{(1)}\right) \quad \text { for <em i="i">{i}^{(1)}=\left[\tilde{\boldsymbol{h}}</em>\right]
\end{aligned}
$$}, \tilde{\boldsymbol{h}}_{i</p>
<p>where $[\cdot]$ is the concatenation operator, $\tilde{\boldsymbol{h}}<em i="i">{i}^{(1)}, \tilde{\boldsymbol{h}}</em>}^{(1)} \in$ $\mathbb{R}^{D_{h}}, \boldsymbol{h<em h="h">{i}^{(1)} \in \mathbb{R}^{2 D</em>$ are the forward and backward encoder GRU parameters.}}$, and $\hat{\eta}^{(1)}$ and $\hat{\eta}^{(1)</p>
<p>When using a two layer GRU, we similarly compute</p>
<p>$$
\begin{aligned}
&amp; \tilde{\boldsymbol{h}}<em m_1="m+1">{0}^{(2)}=\tilde{\boldsymbol{h}}</em> \
&amp; \tilde{\boldsymbol{h}}}^{(2)}=\mathbf{0<em i="i">{i}^{(2)}=\operatorname{GRU}\left(\boldsymbol{h}</em>}^{(1)}, \tilde{\boldsymbol{h}<em i="i">{i-1}^{(2)} ; \hat{\eta}^{(2)}\right) \quad \text { for } i \in 1, \ldots, m \
&amp; \tilde{\boldsymbol{h}}</em>}^{(2)}=\operatorname{GRU}\left(\boldsymbol{h<em i_1="i+1">{i}^{(1)}, \tilde{\boldsymbol{h}}</em> i \in m, \ldots, 1 \
&amp; \boldsymbol{h}}^{(2)} ; \hat{\eta}^{(2)}\right) \quad \text { for <em i="i">{i}^{(2)}=\left[\tilde{\boldsymbol{h}}</em>\right]
\end{aligned}
$$}, \tilde{\boldsymbol{h}}_{i</p>
<p>where $\tilde{\boldsymbol{h}}<em i="i">{i}^{(2)}, \tilde{\boldsymbol{h}}</em>}^{(2)} \in \mathbb{R}^{D_{h}}, \boldsymbol{h<em h="h">{i}^{(2)} \in \mathbb{R}^{2 D</em>$ are the forward and backward encoder GRU parameters for the second layer.}}$, and $\hat{\eta}^{(2)}$ and $\hat{\eta}^{(2)</p>
<p>Going forward, let $\boldsymbol{h}<em i="i">{i}$ correspond to the final encoder output, i.e. $\boldsymbol{h}</em>}=\boldsymbol{h<em i="i">{i}^{(1)}$ in the one-layer biGRU case, and $\boldsymbol{h}</em>$ in the two layer case.}=\boldsymbol{h}_{i}^{(2)</p>
<p>Let $\mathcal{W}$ be the vocabulary of utterance tokens, and $\mathbf{D} \in \mathbb{R}^{|\mathcal{W}| \times D_{w}}$ an associated embedding matrix, where $\mathbf{D}<em w="w">{y} \in \mathbb{R}^{D</em>$.}}$ denotes a $D_{w}$-dimensional embedding for each $y \in \mathcal{W</p>
<p>Given the decoder input sequence $\mathbf{y}=$ $y_{1}, y_{2}, \ldots, y_{|\mathbf{y}|}$, let $\mathbf{w}<em y__i="y_{i">{i}=\mathbf{D}</em>|-1$}}$ for $i \in{1, \ldots n}$, where $n=|\mathbf{y</p>
<p>We compute the hidden states of the $i$-th layer</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">Layers</th>
<th style="text-align: center;">LS</th>
<th style="text-align: center;">WD</th>
<th style="text-align: center;">Optim.</th>
<th style="text-align: center;">LR</th>
<th style="text-align: center;">Emb.</th>
<th style="text-align: center;">Attention</th>
<th style="text-align: center;">Params</th>
<th style="text-align: center;">Train Time</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">$\begin{aligned} &amp; \text {  } \ &amp; \text {  } \end{aligned}$</td>
<td style="text-align: center;">RND</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">0.1</td>
<td style="text-align: center;">$10^{-5}$</td>
<td style="text-align: center;">Adam</td>
<td style="text-align: center;">$10^{-5}$</td>
<td style="text-align: center;">untied</td>
<td style="text-align: center;">Bahdanau</td>
<td style="text-align: center;">14,820,419</td>
<td style="text-align: center;">31.16</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">FP</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">0.1</td>
<td style="text-align: center;">$10^{-5}$</td>
<td style="text-align: center;">SGD</td>
<td style="text-align: center;">0.1</td>
<td style="text-align: center;">untied</td>
<td style="text-align: center;">Bahdanau</td>
<td style="text-align: center;">14,820,003</td>
<td style="text-align: center;">24.78</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">IF</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">0.1</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">SGD</td>
<td style="text-align: center;">0.5</td>
<td style="text-align: center;">untied</td>
<td style="text-align: center;">General</td>
<td style="text-align: center;">14,557,763</td>
<td style="text-align: center;">26.44</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">IF+P</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">0.1</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">SGD</td>
<td style="text-align: center;">0.5</td>
<td style="text-align: center;">untied</td>
<td style="text-align: center;">General</td>
<td style="text-align: center;">14,557,763</td>
<td style="text-align: center;">15.23</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">AT</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">0.1</td>
<td style="text-align: center;">$10^{-5}$</td>
<td style="text-align: center;">Adam</td>
<td style="text-align: center;">$10^{-5}$</td>
<td style="text-align: center;">untied</td>
<td style="text-align: center;">Bahdanau</td>
<td style="text-align: center;">14,820,419</td>
<td style="text-align: center;">26.07</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">AT+P</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">0.1</td>
<td style="text-align: center;">$10^{-5}$</td>
<td style="text-align: center;">Adam</td>
<td style="text-align: center;">$10^{-5}$</td>
<td style="text-align: center;">untied</td>
<td style="text-align: center;">Bahdanau</td>
<td style="text-align: center;">14,820,419</td>
<td style="text-align: center;">36.49</td>
</tr>
<tr>
<td style="text-align: center;">$\begin{aligned} &amp; \text {  } \ &amp; \text {  } \ &amp; \text {  } \ &amp; \text {  } \ &amp; \text {  } \ &amp; \text { If } \ &amp; \text { If }+ \ &amp; \text { AT } \ &amp; \text { AT } \ &amp; \text { AT } \end{aligned}$</td>
<td style="text-align: center;">RND</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">0.1</td>
<td style="text-align: center;">$10^{-5}$</td>
<td style="text-align: center;">SGD</td>
<td style="text-align: center;">0.25</td>
<td style="text-align: center;">untied</td>
<td style="text-align: center;">General</td>
<td style="text-align: center;">14,274,865</td>
<td style="text-align: center;">20.69</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">FP</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0.1</td>
<td style="text-align: center;">$10^{-5}$</td>
<td style="text-align: center;">Adam</td>
<td style="text-align: center;">$10^{-5}$</td>
<td style="text-align: center;">untied</td>
<td style="text-align: center;">Bahdanau</td>
<td style="text-align: center;">7,718,193</td>
<td style="text-align: center;">30.07</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">IF</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">SGD</td>
<td style="text-align: center;">0.5</td>
<td style="text-align: center;">untied</td>
<td style="text-align: center;">Bahdanau</td>
<td style="text-align: center;">7,712,049</td>
<td style="text-align: center;">11.62</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">IF+</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">SGD</td>
<td style="text-align: center;">0.5</td>
<td style="text-align: center;">untied</td>
<td style="text-align: center;">Bahdanau</td>
<td style="text-align: center;">7,712,049</td>
<td style="text-align: center;">5.08</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">AT</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">0.1</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">Adam</td>
<td style="text-align: center;">$10^{-5}$</td>
<td style="text-align: center;">untied</td>
<td style="text-align: center;">Bahdanau</td>
<td style="text-align: center;">14,537,521</td>
<td style="text-align: center;">21.01</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">AT+P</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">0.1</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">Adam</td>
<td style="text-align: center;">$10^{-5}$</td>
<td style="text-align: center;">untied</td>
<td style="text-align: center;">Bahdanau</td>
<td style="text-align: center;">14,537,521</td>
<td style="text-align: center;">14.95</td>
</tr>
</tbody>
</table>
<p>Table 6: Winning hyperparameter settings for biGRU models. LS and WD indicate label smoothing and weight decay respectively. Train time is in hours.
of the decoder as,</p>
<p>$$
\boldsymbol{g}<em m="m">{0}^{(i)}=\tanh \left(\mathbf{W}^{(i)} \boldsymbol{h}</em>\right)
$$}^{(i)}+\mathbf{b}^{(i)</p>
<p>for $j \in 1, \ldots, n$</p>
<p>$$
\boldsymbol{g}<em j="j">{j}^{(i)}=\operatorname{GRU}\left(\boldsymbol{g}</em>\right)
$$}^{(i-1)}, \boldsymbol{g}_{j-1}^{(i)} ; \zeta^{(i)</p>
<p>where $\boldsymbol{g}<em j="j">{j}^{(0)}=\mathbf{w}</em>}, \boldsymbol{g<em h="h">{j}^{(i)} \in \mathbb{R}^{D</em>$ are the decoder GRU parameters.}}, \mathbf{W}^{(i)} \in \mathbb{R}^{D_{h} \times 2 D_{h}}$, $\mathbf{b}^{(i)} \in \mathbb{R}^{D_{h}}$ and $\zeta^{(i)</p>
<p>Going forward, let $\boldsymbol{g}<em i="i">{i}$ correspond to the final decoder output, i.e. $\boldsymbol{g}</em>}=\boldsymbol{g<em i="i">{i}^{(1)}$ in the one-layer biGRU case, and $\boldsymbol{g}</em>$ in the two layer case.}=\boldsymbol{g}_{i}^{(2)</p>
<p>Then the decoder states attend to the encoder states,</p>
<p>$$
\boldsymbol{\hbar}<em j="1">{i}=\sum</em> i \in 1, \ldots, n
$$}^{m} \alpha_{i, j} \boldsymbol{h}_{j} \quad \text { for </p>
<p>where $\alpha_{i, j} \in(0,1)$ is the attention weight of decoder state $i$ on encoder state $j$ and $\sum_{j=1}^{m} \alpha_{i, j}=1$. We compute attention in one of two ways (the attention method is a hyperparemeter option):</p>
<ol>
<li>Feed-forward "Bahdanau" style attention (Bahdanau et al., 2015), also known as "concat" (Luong et al., 2015):</li>
</ol>
<p>$$
\alpha_{i, j}=\mathbf{k} \tanh \left(\mathbf{K}\left[\begin{array}{c}
\boldsymbol{g}<em j="j">{i} \
\boldsymbol{h}</em>
\end{array}\right]\right)
$$</p>
<p>with $\mathbf{K} \in \mathbb{R}^{D_{h} \times 3 D_{h}}$ and $\mathbf{k} \in \mathbb{R}^{D_{h}}$.
2. "general" (Luong et al., 2015) :</p>
<p>$$
\alpha_{i, j}=\boldsymbol{g}<em j="j">{i} \mathbf{K} \boldsymbol{h}</em>
$$</p>
<p>with $\mathbf{K} \in \mathbb{R}^{D_{h} \times 2 D_{h}}$.
Finally, for $i \in 1, \ldots, n$ we compute</p>
<p>$$
\mathbf{z}<em i="i">{i}=\tanh \left(\mathbf{W}^{(z)}\left[\begin{array}{c}
\boldsymbol{g}</em> \
\boldsymbol{h}_{i}
\end{array}\right]+\mathbf{b}^{(z)}\right)
$$</p>
<p>and</p>
<p>$$
\begin{aligned}
&amp; p\left(y_{i+1} \mid y_{\leq i}, \pi(\mu)\right)= \
&amp; \quad \operatorname{softmax}\left(\mathbf{W}^{(o)} \mathbf{z}<em y__i_1="y_{i+1">{i}+\mathbf{b}^{(o)}\right)</em>
\end{aligned}
$$}</p>
<p>where $\mathbf{W}^{(z)} \in \mathbb{R}^{D_{h} \times 3 D_{h}}, \mathbf{b}^{(z)} \in \mathbb{R}^{D_{h}}, \mathbf{b}^{(o)} \in$ $\mathbb{R}^{|\mathcal{W}|}$, and $\mathbf{W}^{(o)} \in \mathbb{R}^{|\mathcal{W}| \times D_{h}}$ is the output embedding matrix. As a hyperparamter setting, we consider tieing the decoder input and output embedding matrices, i.e. $\mathbf{D}=\mathbf{W}^{(o)}$. Dropout of 0.1 is applied to all embedding, GRU outputs, and linear layer outputs. We set $D_{w}=D_{h}=512$.</p>
<h2>A. 3 biGRU Hyperparameter Search</h2>
<p>We grid-search over the following hyperparameter values:</p>
<ul>
<li>Layers: 1, 2</li>
<li>Label Smoothing: $0,0.1$</li>
<li>
<p>Weight Decay: $0,10^{-5}$</p>
</li>
<li>
<p>Optimizer/Learning Rate: Adam/10 ${ }^{-3}$, Adam/10 ${ }^{-4}, \quad$ Adam/10 ${ }^{-5}, \quad$ SGD/0.5, SGD/0.25, SGD/0.1</p>
</li>
<li>Tie Decoder Embeddings: tied, untied</li>
<li>Attention: Bahdanau, General</li>
</ul>
<p>During hyperparameter search, we train for at most 500 epochs, evaluating BLEU every 25 epochs to select the best model. We decay the learning if validation log-likelihood stops increasing for five epochs. We decay the learning rate by $l r^{i+1}=$ $.99 \times l r^{i}$.</p>
<p>Winning hyperparameter settings are presented Table 6.</p>
<h2>A. 4 Transformer Model Definition</h2>
<p>Each Transformer layer is divided into blocks which each have three parts, (i) layer norm, (ii) feed-forward/attention, and (iii) skip-connection. We first define the components used in the transformer blocks before describing the overall S2S transformer. Starting with layer norm (Ba et al., 2016), let $\mathbf{H} \in \mathbb{R}^{m \times n}$, then we have LN : $\mathbb{R}^{m \times n} \rightarrow \mathbb{R}^{m \times n}$,</p>
<p>$$
\operatorname{LN}(\mathbf{H} ; \mathbf{a}, \mathbf{b})=\mathbf{A} \odot(\mathbf{H}-\boldsymbol{\mu}) \odot \Lambda+\mathbf{b}
$$</p>
<p>where $\mathbf{a}, \mathbf{b} \in \mathbb{R}^{n}$ are learned parameters, $\odot$ is the elementwise product, $\mathbf{A}=[\mathbf{a}, \ldots, \mathbf{a}] \in \mathbb{R}^{m \times n}$ is a tiling of the parameter vector, $\mathbf{a}, m$ times, and $\boldsymbol{\mu}, \boldsymbol{\Lambda} \in \mathbb{R}^{m \times n}$ are defined elementwise as</p>
<p>$$
\boldsymbol{\mu}<em k="1">{i, j}=\frac{1}{n} \sum</em>
$$}^{n} \mathbf{H}_{i, k</p>
<p>and</p>
<p>$$
\boldsymbol{\Lambda}<em k="1">{i, j}=\left(\sqrt{\frac{1}{n-1} \sum</em>}^{n}\left(\mathbf{H<em i_="i," j="j">{i, k}-\boldsymbol{\mu}</em>
$$}\right)^{2}+\epsilon}\right)^{-1</p>
<p>respectively. The $\epsilon$ term is a small constant for numerical stability, set to $10^{-5}$.</p>
<p>The inplace feed-forward layer, FF, is a simple single-layer perceptron with ReLU activation $(\operatorname{ReLU}(\mathbf{H})=\max (\mathbf{0}, \mathbf{H}))$ (Nair and Hinton, 2010), applied to each row of an $m \times n$ input matrix, i.e. a sequence of $m$ objects with $n$ features,</p>
<p>$$
\begin{aligned}
&amp; \operatorname{FF}\left(\mathbf{H} ; \mathbf{W}^{(i)}, \mathbf{W}^{(j)}, \mathbf{b}^{(i)}, \mathbf{b}^{(j)}\right)= \
&amp; \operatorname{ReLU}\left(\mathbf{H} \mathbf{W}^{(i)}+\mathbf{b}^{(i)}\right) \mathbf{W}^{(j)}+\mathbf{b}^{(j)}
\end{aligned}
$$</p>
<p>where $\mathbf{W}^{(i)} \in \mathbb{R}^{D_{w} \times D_{h}}, \mathbf{b}^{(i)} \in \mathbb{R}^{D_{h}}, \mathbf{W}^{(j)} \in$ $\mathbb{R}^{D_{h} \times D_{w}}, \mathbf{b}^{(j)} \in \mathbb{R}^{D_{w}}$ are learned parameters and matrix-vector additions (i.e. $\mathbf{X}+\mathbf{b}$ ) are broadcast across the matrix rows.</p>
<p>The final component to be defined is the multi-head attention, MultiAttn which is defined as</p>
<p>MultiAttn $\left(\mathbf{Q}, \mathbf{K} ; \mathbf{W}^{\left(a_{1}\right)}, \mathbf{W}^{\left(a_{2}\right)}\right)=$</p>
<p>$$
\left[\begin{array}{c}
\operatorname{Attn}\left(\mathbf{Q} \mathbf{W}<em 1="1">{1,1}^{\left(a</em>}\right)}, \mathbf{K} \mathbf{W<em 1="1">{2,1}^{\left(a</em>}\right)}, \mathbf{K} \mathbf{W<em 1="1">{3,1}^{\left(a</em>\right) \
\vdots \
\operatorname{Attn}\left(\mathbf{Q} \mathbf{W}}\right)<em 1="1">{1, H}^{\left(a</em>}\right)}, \mathbf{K} \mathbf{W<em 1="1">{2, H}^{\left(a</em>}\right)}, \mathbf{K} \mathbf{W<em 1="1">{3, H}^{\left(a</em>\right)
\end{array}\right] \mathbf{W}^{\left(a_{2}\right)}
$$}\right)</p>
<p>where $[\cdot]$ indicates column-wise concatenation, $\mathbf{W}<em 1="1">{1, s}^{\left(a</em>$ are learned parameters, $H$ is the number of attention heads, and Attn is defined,}\right)} \in \mathbb{R}^{D_{w} \times D_{w} / H}$ and $\mathbf{W}^{\left(a_{2}\right)} \in \mathbb{R}^{D_{w} \times D_{w}</p>
<p>$$
\operatorname{Attn}(\mathbf{Q}, \mathbf{K}, \mathbf{V})=\operatorname{softmax}\left(\frac{\mathbf{Q} \mathbf{K}^{T}}{\sqrt{D_{w}}}\right) \mathbf{V}
$$</p>
<p>Additionally, there is a masked variant of attention, MultiAttn $_{M}$ where the attention is computed</p>
<p>$$
\operatorname{Attn}(\mathbf{Q}, \mathbf{K}, \mathbf{V})=\operatorname{softmax}\left(\frac{\mathbf{Q} \mathbf{K}^{T} \odot \mathbf{M}}{\sqrt{D_{w}}}\right) \mathbf{V}
$$</p>
<p>where $\mathbf{M} \in \mathbb{R}^{n \times m}$ is a lower triangular matrix, i.e. values on or below the diagonal are 1 and all other values are $-\infty$.</p>
<p>Given these definitions, we now define the S2S transformer. Let $\mathcal{V}$ be the encoder input vocabulary, and $\mathbf{E} \in \mathbb{R}^{|\mathcal{V}| \times D_{w}}$ an associated word embedding matrix where $\mathbf{E}<em w="w">{x} \in \mathbb{R}^{D</em>}}$ denotes the $D_{w^{-}}$ dimensional embedding for each $x \in \mathcal{V}$. Given a linearized $\operatorname{MR} \pi(\mu)=\mathbf{x}=\left[a, x_{1}, x_{2}, \ldots, x_{|\mu|}\right] \in$ $\mathcal{V}^{m}$ where the length of the sequence is $m=$ $|\mu|+1$, let $\mathbf{v<em _mathbf_x="\mathbf{x">{i}=\mathbf{E}</em>$ for $i \in{1, \ldots m}$.}_{i}</p>
<p>Additionally let $\mathbf{P} \in \mathbb{R}^{m_{\max } \times D_{w}}$ be a sinusoidal position embedding matrix defined elementwise with</p>
<p>$$
\begin{aligned}
\mathbf{P}<em w="w">{i, 2 j} &amp; =\sin \left(\frac{i}{10,000^{\frac{2 j}{D</em>\right) \
\mathbf{P}}}}<em w="w">{i, 2 j+1} &amp; =\cos \left(\frac{i}{10,000^{\frac{2 i}{D</em>\right)
\end{aligned}
$$}}}</p>
<p>The encoder input sequence $\mathbf{H}^{(0)} \in \mathbb{R}^{m \times D_{w}}$ is then defined by</p>
<p>$$
\mathbf{H}^{(0)}=\left[\begin{array}{c}
\mathbf{v}<em 1="1">{1}+\mathbf{P}</em> \
\mathbf{v}<em 2="2">{2}+\mathbf{P}</em> \
\vdots \
\mathbf{v}<em m="m">{m}+\mathbf{P}</em>
\end{array}\right]
$$</p>
<p>A sequence of $l$ transformer encoder layers are then applied to the encoder input, i.e. $\mathbf{H}^{(i+1)}=\operatorname{TF}_{c m i}^{(i)}\left(\mathbf{H}^{(i)}\right)$. Each encoder transformer layer computes the following,
(Self-Attention Block)</p>
<p>$$
\begin{gathered}
\hat{\mathbf{H}}^{(i)}=\operatorname{LN}\left(\mathbf{H}^{(i)} ; \mathbf{a}^{(i, 1)}, \mathbf{b}^{(i, 1)}\right) \
\hat{\mathbf{H}}^{(i)}=\operatorname{MultiAttn}\left(\hat{\mathbf{H}}^{(i)}, \hat{\mathbf{H}}^{(i)} ; \mathbf{W}^{\left(i, a_{1}\right)}, \mathbf{W}^{\left(i, a_{2}\right)}\right) \
\hat{\mathbf{H}}^{(i)}=\mathbf{H}^{(i)}+\overline{\mathbf{H}}^{(i)}
\end{gathered}
$$</p>
<p>(Feed-Forward Block)</p>
<p>$$
\begin{gathered}
\hat{\mathbf{H}}^{(i)}=\operatorname{LN}\left(\hat{\mathbf{H}}^{(i)} ; \mathbf{a}^{(i, 2)}, \mathbf{b}^{(i, 2)}\right) \
\overline{\mathbf{H}}^{(i)}=\operatorname{FF}\left(\hat{\mathbf{H}}^{(i)} ; \mathbf{W}^{(i, 1)}, \mathbf{W}^{(i, 2)}, \mathbf{b}^{(i, 1)}, \mathbf{b}^{(i, 2)}\right) \
\mathbf{H}^{(i+1)}=\hat{\mathbf{H}}^{(i)}+\overline{\mathbf{H}}^{(i)}
\end{gathered}
$$</p>
<p>We denote the final encoder output for $l$ layers as $\mathbf{H}=\mathbf{H}^{(l)}$.</p>
<p>Let $\mathcal{W}$ be the vocabulary of utterance tokens, and $\mathbf{D} \in \mathbb{R}^{|\mathcal{W}| \times D_{w}}$ an associated embedding matrix, where $\mathbf{D}<em w="w">{y} \in \mathbb{R}^{D</em>$.}}$ denotes a $D_{w}$-dimensional embedding for each $y \in \mathcal{W</p>
<p>Given the decoder input sequence $\mathbf{y}=$ $y_{1}, y_{2}, \ldots, y_{|\mathbf{y}|}$, let $\mathbf{w}<em y__i="y_{i">{i}=\mathbf{D}</em>|-1$}}$ for $i \in{1, \ldots n}$. where $n=|\mathbf{y</p>
<p>$$
\mathbf{G}^{(0)}=\left[\begin{array}{c}
\mathbf{w}<em 1="1">{1}+\mathbf{P}</em> \
\mathbf{w}<em 2="2">{2}+\mathbf{P}</em> \
\vdots \
\mathbf{w}<em n="n">{n}+\mathbf{P}</em>
\end{array}\right]
$$</p>
<p>A sequence of $l$ transformer decoder layers are then applied to the decoder input, i.e. $\mathbf{G}^{(i+1)}=\operatorname{TF}_{d e e}^{(i)}\left(\mathbf{G}^{(i)}\right)$. Each decoder transformer layer computes the following,
(Masked Self-Attention Block)</p>
<p>$$
\begin{gathered}
\overline{\mathbf{G}}^{(i)}=\operatorname{LN}\left(\mathbf{G}^{(i)} ; \mathbf{a}^{(i, 1)}, \mathbf{b}^{(i, 1)}\right) \
\overline{\mathbf{G}}^{(i)}=\operatorname{MultiAttn}<em 1="1">{M}\left(\overline{\mathbf{G}}^{(i)}, \overline{\mathbf{G}}^{(i)} ; \mathbf{W}^{\left(i, a</em>\right) \
\overline{\mathbf{G}}^{(i)}=\mathbf{G}^{(i)}+\overline{\mathbf{G}}^{(i)}
\end{gathered}
$$}\right)}, \mathbf{W}^{\left(i, a_{2}\right)</p>
<p>(Encoder-Attention Block)</p>
<p>$$
\begin{gathered}
\overline{\mathbf{G}}^{(i)}=\operatorname{LN}\left(\overline{\mathbf{G}}^{(i)} ; \mathbf{a}^{(i, 2)}, \mathbf{b}^{(i, 2)}\right) \
\overline{\mathbf{G}}^{(i)}=\operatorname{MultiAttn}\left(\overline{\mathbf{G}}^{(i)}, \mathbf{H} ; \mathbf{W}^{\left(i, a_{3}\right)}, \mathbf{W}^{\left(i, a_{4}\right)}\right) \
\overline{\mathbf{G}}^{(i)}=\overline{\mathbf{G}}^{(i)}+\overline{\mathbf{G}}^{(i)}
\end{gathered}
$$</p>
<p>(Feed-Forward Block)</p>
<p>$$
\begin{gathered}
\overline{\mathbf{G}}^{(i)}=\operatorname{LN}\left(\overline{\mathbf{G}}^{(i)} ; \mathbf{a}^{(i, 3)}, \mathbf{b}^{(i, 3)}\right) \
\overline{\mathbf{G}}^{(i)}=\operatorname{FF}\left(\overline{\mathbf{G}}^{(i)} ; \mathbf{W}^{(i, 1)}, \mathbf{W}^{(i, 2)}, \mathbf{b}^{(i, 1)}, \mathbf{b}^{(i, 2)}\right) \
\mathbf{G}^{(i+1)}=\overline{\mathbf{G}}^{(i)}+\overline{\mathbf{G}}^{(i)}
\end{gathered}
$$</p>
<p>Let the $\mathbf{G}=\mathbf{G}^{(l)}$ denote the final decoder output, and let $\mathbf{g}_{i}$ be the $i$-th row of $\mathbf{G}$ corresponding to the decoder representation of the $i$-th decoder state. The probability of the next word is</p>
<p>$$
\begin{aligned}
&amp; p\left(y_{i+1} \mid y_{\leq i}, \pi(\mu)\right) \
&amp; \quad=\operatorname{softmax}\left(\mathbf{W}^{(o)} \mathbf{g}<em y__i_1="y_{i+1">{i}+\mathbf{b}^{(o)}\right)</em>
\end{aligned}
$$}</p>
<p>where $\mathbf{W}^{(o)} \in \mathbb{R}^{|\mathcal{W}| \times D_{w}}$ and $\mathbf{b}^{(o)} \in \mathbb{R}^{D_{w}}$ are learned parameters.</p>
<p>The input embedding dimension is $D_{w}=512$ and inner hidden layer size is $D_{h}=2048$. The encoder and decoder have separate parameters. We used $H=8$ heads in all multi-head attention layers. We used Adam with the learning rate schedule provided in Rush (2018) (factor=1, warmup=8000). Dropout was set to 0.1 was applied to input embeddings and each skip connection (i.e. the third line in each block definition). As a hyperparameter, we optionally tie the decoder input and output embeddings, i.e. $\mathbf{D}=\mathbf{W}^{(o)}$.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">Layers</th>
<th style="text-align: center;">LS</th>
<th style="text-align: center;">Emb.</th>
<th style="text-align: center;">Params</th>
<th style="text-align: center;">Train Time</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">$\begin{aligned} &amp; \text {  } \ &amp; \text {  } \end{aligned}$</td>
<td style="text-align: center;">RND</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0.1</td>
<td style="text-align: center;">tied</td>
<td style="text-align: center;">7,966,787</td>
<td style="text-align: center;">18.09</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">FP</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0.1</td>
<td style="text-align: center;">tied</td>
<td style="text-align: center;">7,970,371</td>
<td style="text-align: center;">17.30</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">IF</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0.1</td>
<td style="text-align: center;">untied</td>
<td style="text-align: center;">8,525,379</td>
<td style="text-align: center;">17.52</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">IF+P</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0.1</td>
<td style="text-align: center;">untied</td>
<td style="text-align: center;">8,525,379</td>
<td style="text-align: center;">28.11</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">AT</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">0.1</td>
<td style="text-align: center;">untied</td>
<td style="text-align: center;">15,881,795</td>
<td style="text-align: center;">23.73</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">AT+P</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">0.1</td>
<td style="text-align: center;">untied</td>
<td style="text-align: center;">15,881,795</td>
<td style="text-align: center;">29.39</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">RND</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">untied</td>
<td style="text-align: center;">15,598,897</td>
<td style="text-align: center;">11.22</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">FP</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">0.1</td>
<td style="text-align: center;">untied</td>
<td style="text-align: center;">15,605,041</td>
<td style="text-align: center;">9.68</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">IF</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">0.1</td>
<td style="text-align: center;">untied</td>
<td style="text-align: center;">15,598,897</td>
<td style="text-align: center;">11.35</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">IF+P</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">0.1</td>
<td style="text-align: center;">untied</td>
<td style="text-align: center;">15,598,897</td>
<td style="text-align: center;">9.09</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">AT</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">0.1</td>
<td style="text-align: center;">untied</td>
<td style="text-align: center;">15,598,897</td>
<td style="text-align: center;">7.26</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">AT+P</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">0.1</td>
<td style="text-align: center;">untied</td>
<td style="text-align: center;">15,598,897</td>
<td style="text-align: center;">5.87</td>
</tr>
</tbody>
</table>
<p>Table 7: Winning hyperparameter settings for Transformer models (trained from scratch). LS indicates label smoothing. Train time is in hours.</p>
<h1>A. 5 Transformer Hyperparameter Search</h1>
<p>We grid searched over the following Transformer hyper-parameters:</p>
<ul>
<li>Tied Decoder Embeddings: tied, untied</li>
<li>Layers: 1, 2</li>
<li>Label Smoothing: $0.0,0.1$</li>
</ul>
<p>During hyperparameter search, we train for at most 500 epochs, evaluating Bleu every 25 epochs to select the best model.</p>
<p>Winning hyperparameter settings are presented Table 7.</p>
<h2>A. 6 BART Model Hyperparameters</h2>
<p>We use the same settings as the fine-tuning for the CNN-DailyMail summarization task, although we modify the maximum number of updates to be roughly to be equivalent to 10 epochs on the training set when using a 500 token batch size, since the number of updates effects the learning rate scheduler. We selected the model iterate with lowest validation set cross-entropy.</p>
<p>While BART is unlikely to have seen any linearized MR in its pretraining data, its use of subword encoding allows it to encode arbitrary strings. Rather than extending it's encoder input vocabulary to add the MR tokens, we simply format the input MR as a string (in the correpsonding linearization order), e.g. "inform rating=good name=NAME platforms=PC platforms=Xbox".</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">$\mathrm{B} \uparrow$</th>
<th style="text-align: center;">$\mathrm{R} \uparrow$</th>
<th style="text-align: center;">SER $\downarrow$</th>
<th style="text-align: center;">$\mathrm{OA} \uparrow$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">RND</td>
<td style="text-align: center;">47.8</td>
<td style="text-align: center;">59.5</td>
<td style="text-align: center;">1.84</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">FP</td>
<td style="text-align: center;">49.1</td>
<td style="text-align: center;">60.3</td>
<td style="text-align: center;">1.90</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">IF</td>
<td style="text-align: center;">48.4</td>
<td style="text-align: center;">59.9</td>
<td style="text-align: center;">3.86</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">IF+P</td>
<td style="text-align: center;">48.1</td>
<td style="text-align: center;">60.0</td>
<td style="text-align: center;">1.12</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">AT+BgUP</td>
<td style="text-align: center;">44.2</td>
<td style="text-align: center;">57.4</td>
<td style="text-align: center;">0.14</td>
<td style="text-align: center;">99.0</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">AT+NUP</td>
<td style="text-align: center;">48.8</td>
<td style="text-align: center;">61.2</td>
<td style="text-align: center;">0.02</td>
<td style="text-align: center;">99.8</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">AT+NUP+P</td>
<td style="text-align: center;">48.7</td>
<td style="text-align: center;">61.1</td>
<td style="text-align: center;">0.02</td>
<td style="text-align: center;">99.8</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">At Oracle</td>
<td style="text-align: center;">56.9</td>
<td style="text-align: center;">73.2</td>
<td style="text-align: center;">0.18</td>
<td style="text-align: center;">99.2</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">RND</td>
<td style="text-align: center;">47.8</td>
<td style="text-align: center;">59.4</td>
<td style="text-align: center;">2.26</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">FP</td>
<td style="text-align: center;">49.2</td>
<td style="text-align: center;">60.6</td>
<td style="text-align: center;">2.84</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">IF</td>
<td style="text-align: center;">49.7</td>
<td style="text-align: center;">60.9</td>
<td style="text-align: center;">1.46</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">IF+P</td>
<td style="text-align: center;">48.9</td>
<td style="text-align: center;">60.7</td>
<td style="text-align: center;">1.00</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">AT+BgUP</td>
<td style="text-align: center;">44.9</td>
<td style="text-align: center;">57.4</td>
<td style="text-align: center;">0.28</td>
<td style="text-align: center;">98.3</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">AT+NUP</td>
<td style="text-align: center;">49.3</td>
<td style="text-align: center;">61.3</td>
<td style="text-align: center;">0.10</td>
<td style="text-align: center;">99.4</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">AT+NUP+P</td>
<td style="text-align: center;">48.8</td>
<td style="text-align: center;">61.1</td>
<td style="text-align: center;">0.08</td>
<td style="text-align: center;">99.6</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">At Oracle</td>
<td style="text-align: center;">56.9</td>
<td style="text-align: center;">73.1</td>
<td style="text-align: center;">0.48</td>
<td style="text-align: center;">97.5</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">RND</td>
<td style="text-align: center;">46.9</td>
<td style="text-align: center;">59.5</td>
<td style="text-align: center;">0.48</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">FP</td>
<td style="text-align: center;">48.6</td>
<td style="text-align: center;">60.3</td>
<td style="text-align: center;">0.04</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">IF</td>
<td style="text-align: center;">48.2</td>
<td style="text-align: center;">60.3</td>
<td style="text-align: center;">0.18</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">IF+P</td>
<td style="text-align: center;">47.8</td>
<td style="text-align: center;">60.1</td>
<td style="text-align: center;">0.46</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">AT+BgUP</td>
<td style="text-align: center;">45.9</td>
<td style="text-align: center;">57.3</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">99.9</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">AT+NUP</td>
<td style="text-align: center;">49.0</td>
<td style="text-align: center;">61.2</td>
<td style="text-align: center;">0.02</td>
<td style="text-align: center;">99.8</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">AT+NUP+P</td>
<td style="text-align: center;">48.8</td>
<td style="text-align: center;">61.1</td>
<td style="text-align: center;">0.02</td>
<td style="text-align: center;">99.8</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">At Oracle</td>
<td style="text-align: center;">55.7</td>
<td style="text-align: center;">72.8</td>
<td style="text-align: center;">0.20</td>
<td style="text-align: center;">99.0</td>
</tr>
</tbody>
</table>
<p>Table 8: E2E validation set (B) Bleu, (R) Rouge-L, SER, and OA.</p>
<h2>A. 7 Validation Results</h2>
<p>Validation set results are shown in Table 8 and Table 9 for the E2E and ViGGO datasets respectively. Unlike the test results, reported in the main paper and appendix, validation SER and OA are computed automatically and not manually validated. All results are the average of five random initializations. Also we use the corrected MR produced by our attribute-value matching rules as input, rather than the original validation set MR.</p>
<h2>B Neural Utterance Planner Model and Hyper-Parameter Search</h2>
<p>We use the same general recurrent neural network model as defined in $\S$ A. 2 with Bahdanau style attention (Bahdanau et al., 2015) to implement the neural utterance planner model. We trained for at most 50 epochs with batch size 128. We used the Adam optimizer with 0.0 weight decay. Decoder input/output embeddings were not tied. Models</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">$\mathrm{B} \uparrow$</th>
<th style="text-align: center;">$\mathrm{R} \uparrow$</th>
<th style="text-align: center;">SER $\downarrow$</th>
<th style="text-align: center;">$\mathrm{OA} \uparrow$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">RND</td>
<td style="text-align: center;">50.2</td>
<td style="text-align: center;">62.0</td>
<td style="text-align: center;">14.26</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">FP</td>
<td style="text-align: center;">50.5</td>
<td style="text-align: center;">62.3</td>
<td style="text-align: center;">16.68</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">IF</td>
<td style="text-align: center;">51.5</td>
<td style="text-align: center;">62.7</td>
<td style="text-align: center;">19.28</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">IF+P</td>
<td style="text-align: center;">49.6</td>
<td style="text-align: center;">61.7</td>
<td style="text-align: center;">12.88</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">AT+BgUP</td>
<td style="text-align: center;">49.8</td>
<td style="text-align: center;">60.5</td>
<td style="text-align: center;">2.76</td>
<td style="text-align: center;">91.1</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">AT+NUP</td>
<td style="text-align: center;">52.9</td>
<td style="text-align: center;">64.4</td>
<td style="text-align: center;">1.52</td>
<td style="text-align: center;">93.8</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">AT+NUP+P</td>
<td style="text-align: center;">52.7</td>
<td style="text-align: center;">63.9</td>
<td style="text-align: center;">1.40</td>
<td style="text-align: center;">94.3</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">At Oracle</td>
<td style="text-align: center;">55.7</td>
<td style="text-align: center;">67.4</td>
<td style="text-align: center;">1.82</td>
<td style="text-align: center;">92.7</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">RND</td>
<td style="text-align: center;">52.6</td>
<td style="text-align: center;">63.4</td>
<td style="text-align: center;">8.96</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">FP</td>
<td style="text-align: center;">52.8</td>
<td style="text-align: center;">63.5</td>
<td style="text-align: center;">8.48</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">IF</td>
<td style="text-align: center;">53.3</td>
<td style="text-align: center;">63.5</td>
<td style="text-align: center;">7.00</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">IF+P</td>
<td style="text-align: center;">52.5</td>
<td style="text-align: center;">63.3</td>
<td style="text-align: center;">3.88</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">AT+BgUP</td>
<td style="text-align: center;">48.8</td>
<td style="text-align: center;">60.3</td>
<td style="text-align: center;">3.72</td>
<td style="text-align: center;">80.3</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">AT+NUP</td>
<td style="text-align: center;">51.5</td>
<td style="text-align: center;">63.5</td>
<td style="text-align: center;">2.84</td>
<td style="text-align: center;">88.1</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">AT+NUP+P</td>
<td style="text-align: center;">51.6</td>
<td style="text-align: center;">63.4</td>
<td style="text-align: center;">2.60</td>
<td style="text-align: center;">88.5</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">At Oracle</td>
<td style="text-align: center;">53.4</td>
<td style="text-align: center;">66.2</td>
<td style="text-align: center;">3.78</td>
<td style="text-align: center;">82.7</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">RND</td>
<td style="text-align: center;">45.2</td>
<td style="text-align: center;">57.4</td>
<td style="text-align: center;">2.08</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">FP</td>
<td style="text-align: center;">46.2</td>
<td style="text-align: center;">58.8</td>
<td style="text-align: center;">1.86</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">IF</td>
<td style="text-align: center;">44.9</td>
<td style="text-align: center;">57.0</td>
<td style="text-align: center;">2.12</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">IF+P</td>
<td style="text-align: center;">48.4</td>
<td style="text-align: center;">60.0</td>
<td style="text-align: center;">2.26</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">AT+BgUP</td>
<td style="text-align: center;">44.8</td>
<td style="text-align: center;">56.1</td>
<td style="text-align: center;">1.00</td>
<td style="text-align: center;">95.5</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">AT+NUP</td>
<td style="text-align: center;">47.5</td>
<td style="text-align: center;">60.2</td>
<td style="text-align: center;">0.90</td>
<td style="text-align: center;">96.1</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">AT+NUP+P</td>
<td style="text-align: center;">49.0</td>
<td style="text-align: center;">61.1</td>
<td style="text-align: center;">1.02</td>
<td style="text-align: center;">95.7</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">At Oracle</td>
<td style="text-align: center;">48.7</td>
<td style="text-align: center;">62.8</td>
<td style="text-align: center;">1.44</td>
<td style="text-align: center;">94.1</td>
</tr>
</tbody>
</table>
<p>Table 9: ViGGO validation set (B) Bleu, (R) RougeL, SER, and OA.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Dataset</th>
<th style="text-align: left;">Model</th>
<th style="text-align: left;">Valid</th>
<th style="text-align: left;">Test</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">ViGGO</td>
<td style="text-align: left;">BGUP</td>
<td style="text-align: left;">0.417</td>
<td style="text-align: left;">0.347</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">NUP</td>
<td style="text-align: left;">0.739</td>
<td style="text-align: left;">0.651</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">BGUP</td>
<td style="text-align: left;">0.433</td>
<td style="text-align: left;">0.432</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">NUP</td>
<td style="text-align: left;">0.502</td>
<td style="text-align: left;">0.447</td>
</tr>
</tbody>
</table>
<p>Table 10: Validation and test set Kendall's $\tau$ for BGUP and NUP models.
used embeddings and hidden layers of 512 dimensions. Models were trained to map IF inputs to AT outputs. We grid-searched over the following hyper-parameters:</p>
<ul>
<li>Layers: 1, 2</li>
<li>Learning Rate: $10^{-3}, 10^{-4}, 10^{-5}$</li>
<li>RNN cell: GRU, LSTM</li>
<li>
<p>Bidirectional Encoder: uni, bi</p>
</li>
<li>
<p>Label Smoothing: $0.0,0.1$
with the following winning settings determined by Kendall's $\tau$ on the validation set:</p>
</li>
<li>E2E - 1 layers, biLSTM, $\operatorname{lr}=10^{-5}, 0.1$ label smoothing</li>
<li>ViGGO - 1 layer, uniLSTM, $\operatorname{lr}=10^{-4}, 0.1$ label smoothing</li>
</ul>
<p>Validation and test set Kendall's $\tau$ are shown in Table 10.</p>
<h1>C Expanded Test Set Results</h1>
<p>We show full automatic evaluation metrics from the E2E official evaluation script. E2E and ViGGO results are shown in Table 11 and Table 12 respectively. We also show full manual semantic evaluation results in Table 13 and Table 14 for E2E and ViGGO respectively. We break out the counts of missing, wrong, and added attributes used for SER calculation. Wrong attributes occur when an attribute is realized with the wrong value. Added attribute indicate the model realized an attributevalue that was not given in the input MR. Repeated attributes, even when specified in the input MR are included in added counts. We also include the percentage of utterances with correct semantics regardless of order (Perf.).</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Automatic Quality Metrics</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Bleu</td>
<td style="text-align: center;">Nist</td>
<td style="text-align: center;">Meteor</td>
<td style="text-align: center;">Rouge-L</td>
<td style="text-align: center;">Cider</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">RND</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">66.82</td>
<td style="text-align: center;">8.696</td>
<td style="text-align: center;">44.46</td>
<td style="text-align: center;">68.26</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">FP</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">63.40</td>
<td style="text-align: center;">8.414</td>
<td style="text-align: center;">42.32</td>
<td style="text-align: center;">65.64</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">IF</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">59.24</td>
<td style="text-align: center;">7.996</td>
<td style="text-align: center;">38.74</td>
<td style="text-align: center;">62.66</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$+P$</td>
<td style="text-align: center;">65.82</td>
<td style="text-align: center;">8.604</td>
<td style="text-align: center;">45.10</td>
<td style="text-align: center;">68.14</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">AT+BGUP</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">66.38</td>
<td style="text-align: center;">8.682</td>
<td style="text-align: center;">45.04</td>
<td style="text-align: center;">68.28</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">AT+NUP</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">66.30</td>
<td style="text-align: center;">8.744</td>
<td style="text-align: center;">44.92</td>
<td style="text-align: center;">68.92</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$+P$</td>
<td style="text-align: center;">66.48</td>
<td style="text-align: center;">8.758</td>
<td style="text-align: center;">44.98</td>
<td style="text-align: center;">69.12</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">At Oracle</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">69.84</td>
<td style="text-align: center;">9.244</td>
<td style="text-align: center;">47.92</td>
<td style="text-align: center;">77.28</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">RND</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">67.36</td>
<td style="text-align: center;">8.722</td>
<td style="text-align: center;">44.86</td>
<td style="text-align: center;">68.20</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">FP</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">67.44</td>
<td style="text-align: center;">8.722</td>
<td style="text-align: center;">44.26</td>
<td style="text-align: center;">68.70</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">IF</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">67.12</td>
<td style="text-align: center;">8.706</td>
<td style="text-align: center;">44.96</td>
<td style="text-align: center;">68.10</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$+P$</td>
<td style="text-align: center;">66.80</td>
<td style="text-align: center;">8.674</td>
<td style="text-align: center;">45.04</td>
<td style="text-align: center;">68.30</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">AT+BGUP</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">66.82</td>
<td style="text-align: center;">8.722</td>
<td style="text-align: center;">45.20</td>
<td style="text-align: center;">68.44</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">AT+NUP</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">67.00</td>
<td style="text-align: center;">8.792</td>
<td style="text-align: center;">45.08</td>
<td style="text-align: center;">68.98</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$+P$</td>
<td style="text-align: center;">66.74</td>
<td style="text-align: center;">8.760</td>
<td style="text-align: center;">45.08</td>
<td style="text-align: center;">69.14</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">At Oracle</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">69.30</td>
<td style="text-align: center;">9.198</td>
<td style="text-align: center;">47.88</td>
<td style="text-align: center;">77.02</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">RND</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">66.46</td>
<td style="text-align: center;">8.652</td>
<td style="text-align: center;">45.54</td>
<td style="text-align: center;">68.34</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">FP</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">65.54</td>
<td style="text-align: center;">8.594</td>
<td style="text-align: center;">45.18</td>
<td style="text-align: center;">67.18</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">IF</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">65.62</td>
<td style="text-align: center;">8.608</td>
<td style="text-align: center;">45.26</td>
<td style="text-align: center;">67.38</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$+P$</td>
<td style="text-align: center;">65.92</td>
<td style="text-align: center;">8.660</td>
<td style="text-align: center;">45.24</td>
<td style="text-align: center;">68.18</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">AT+BGUP</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">66.24</td>
<td style="text-align: center;">8.620</td>
<td style="text-align: center;">45.66</td>
<td style="text-align: center;">68.68</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">AT+NUP</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">66.56</td>
<td style="text-align: center;">8.682</td>
<td style="text-align: center;">45.52</td>
<td style="text-align: center;">69.22</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$+P$</td>
<td style="text-align: center;">66.26</td>
<td style="text-align: center;">8.678</td>
<td style="text-align: center;">45.30</td>
<td style="text-align: center;">69.34</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">At Oracle</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">68.34</td>
<td style="text-align: center;">9.084</td>
<td style="text-align: center;">48.28</td>
<td style="text-align: center;">77.08</td>
</tr>
</tbody>
</table>
<p>Table 11: E2E test set automatic quality measures from the official E2E evaluation script.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Automatic Quality Metrics</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Bleu</td>
<td style="text-align: center;">Nist</td>
<td style="text-align: center;">Meteor</td>
<td style="text-align: center;">Rouge-L</td>
<td style="text-align: center;">Cider</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">RND</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">50.18</td>
<td style="text-align: center;">8.300</td>
<td style="text-align: center;">37.78</td>
<td style="text-align: center;">61.56</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">FP</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">50.18</td>
<td style="text-align: center;">8.132</td>
<td style="text-align: center;">37.18</td>
<td style="text-align: center;">61.04</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">IF</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">50.24</td>
<td style="text-align: center;">8.160</td>
<td style="text-align: center;">37.40</td>
<td style="text-align: center;">61.32</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$+P$</td>
<td style="text-align: center;">49.48</td>
<td style="text-align: center;">8.010</td>
<td style="text-align: center;">37.26</td>
<td style="text-align: center;">61.58</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">AT+BGUP</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">48.52</td>
<td style="text-align: center;">7.946</td>
<td style="text-align: center;">37.32</td>
<td style="text-align: center;">58.48</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">AT+NUP</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">51.84</td>
<td style="text-align: center;">8.252</td>
<td style="text-align: center;">38.48</td>
<td style="text-align: center;">62.56</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$+P$</td>
<td style="text-align: center;">52.40</td>
<td style="text-align: center;">8.084</td>
<td style="text-align: center;">38.34</td>
<td style="text-align: center;">62.66</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">At Oracle</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">54.08</td>
<td style="text-align: center;">8.504</td>
<td style="text-align: center;">39.38</td>
<td style="text-align: center;">65.48</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">RND</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">52.04</td>
<td style="text-align: center;">8.166</td>
<td style="text-align: center;">38.10</td>
<td style="text-align: center;">62.86</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">FP</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">52.58</td>
<td style="text-align: center;">8.246</td>
<td style="text-align: center;">38.32</td>
<td style="text-align: center;">63.02</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">IF</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">52.28</td>
<td style="text-align: center;">8.184</td>
<td style="text-align: center;">38.14</td>
<td style="text-align: center;">62.58</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$+P$</td>
<td style="text-align: center;">52.34</td>
<td style="text-align: center;">8.106</td>
<td style="text-align: center;">38.44</td>
<td style="text-align: center;">63.12</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">AT+BGUP</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">48.70</td>
<td style="text-align: center;">8.174</td>
<td style="text-align: center;">37.50</td>
<td style="text-align: center;">59.22</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">AT+NUP</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">51.60</td>
<td style="text-align: center;">8.352</td>
<td style="text-align: center;">38.52</td>
<td style="text-align: center;">62.42</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$+P$</td>
<td style="text-align: center;">51.06</td>
<td style="text-align: center;">8.138</td>
<td style="text-align: center;">38.12</td>
<td style="text-align: center;">62.00</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">At Oracle</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">53.18</td>
<td style="text-align: center;">8.508</td>
<td style="text-align: center;">39.12</td>
<td style="text-align: center;">64.96</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">RND</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">43.72</td>
<td style="text-align: center;">7.814</td>
<td style="text-align: center;">37.70</td>
<td style="text-align: center;">55.12</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">FP</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">47.04</td>
<td style="text-align: center;">8.184</td>
<td style="text-align: center;">38.48</td>
<td style="text-align: center;">58.88</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">IF</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">43.06</td>
<td style="text-align: center;">7.744</td>
<td style="text-align: center;">37.62</td>
<td style="text-align: center;">54.36</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$+P$</td>
<td style="text-align: center;">49.06</td>
<td style="text-align: center;">8.284</td>
<td style="text-align: center;">38.36</td>
<td style="text-align: center;">59.66</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">AT+BGUP</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">43.76</td>
<td style="text-align: center;">7.888</td>
<td style="text-align: center;">37.38</td>
<td style="text-align: center;">53.98</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">AT+NUP</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">45.46</td>
<td style="text-align: center;">8.034</td>
<td style="text-align: center;">37.84</td>
<td style="text-align: center;">57.62</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$+P$</td>
<td style="text-align: center;">48.50</td>
<td style="text-align: center;">8.248</td>
<td style="text-align: center;">38.04</td>
<td style="text-align: center;">59.24</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">At Oracle</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">47.10</td>
<td style="text-align: center;">8.194</td>
<td style="text-align: center;">38.50</td>
<td style="text-align: center;">60.40</td>
</tr>
</tbody>
</table>
<p>Table 12: ViGGO test set automatic quality measures from the official E2E evaluation script.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Manual Semantic Metrics</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Model</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Missing</td>
<td style="text-align: center;">Wrong</td>
<td style="text-align: center;">Added</td>
<td style="text-align: center;">Total</td>
<td style="text-align: center;">SER $\downarrow$</td>
<td style="text-align: center;">OA $\uparrow$</td>
<td style="text-align: center;">Perf. $\uparrow$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">RND</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">112.8</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">2.6</td>
<td style="text-align: center;">115.4</td>
<td style="text-align: center;">2.64</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">81.70</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">FP</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">157.8</td>
<td style="text-align: center;">79.8</td>
<td style="text-align: center;">47.6</td>
<td style="text-align: center;">285.2</td>
<td style="text-align: center;">6.54</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">68.84</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">IF</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">215.0</td>
<td style="text-align: center;">320.8</td>
<td style="text-align: center;">14.6</td>
<td style="text-align: center;">550.4</td>
<td style="text-align: center;">12.64</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">26.96</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$+P$</td>
<td style="text-align: center;">7.4</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">2.4</td>
<td style="text-align: center;">9.8</td>
<td style="text-align: center;">0.24</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">98.44</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">AT+BGUP</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">11.4</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">11.4</td>
<td style="text-align: center;">0.26</td>
<td style="text-align: center;">98.18</td>
<td style="text-align: center;">98.18</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">AT+NUP</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">10.8</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">10.8</td>
<td style="text-align: center;">0.26</td>
<td style="text-align: center;">98.30</td>
<td style="text-align: center;">98.30</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$+P$</td>
<td style="text-align: center;">0.2</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.2</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">99.96</td>
<td style="text-align: center;">99.96</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">At Oracle</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">36.6</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.8</td>
<td style="text-align: center;">37.4</td>
<td style="text-align: center;">0.84</td>
<td style="text-align: center;">94.34</td>
<td style="text-align: center;">94.34</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">RND</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">44.8</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">1.0</td>
<td style="text-align: center;">45.8</td>
<td style="text-align: center;">1.06</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">92.80</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">FP</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">128.0</td>
<td style="text-align: center;">1.6</td>
<td style="text-align: center;">5.6</td>
<td style="text-align: center;">135.2</td>
<td style="text-align: center;">3.10</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">79.32</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">IF</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">25.2</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">3.6</td>
<td style="text-align: center;">28.8</td>
<td style="text-align: center;">0.66</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">95.64</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$+P$</td>
<td style="text-align: center;">12.4</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">12.4</td>
<td style="text-align: center;">0.28</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">98.04</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">AT+BGUP</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.2</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.2</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">99.94</td>
<td style="text-align: center;">99.96</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">AT+NUP</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">100.00</td>
<td style="text-align: center;">100.00</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$+P$</td>
<td style="text-align: center;">0.2</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.2</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">99.96</td>
<td style="text-align: center;">99.96</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">At Oracle</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">32.4</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">2.6</td>
<td style="text-align: center;">35.0</td>
<td style="text-align: center;">0.76</td>
<td style="text-align: center;">94.96</td>
<td style="text-align: center;">95.06</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">RND</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">5.8</td>
<td style="text-align: center;">5.8</td>
<td style="text-align: center;">0.14</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">99.14</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">FP</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">7.0</td>
<td style="text-align: center;">7.0</td>
<td style="text-align: center;">0.16</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">98.90</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">IF</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">8.6</td>
<td style="text-align: center;">8.6</td>
<td style="text-align: center;">0.18</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">98.62</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$+P$</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">13.0</td>
<td style="text-align: center;">13.0</td>
<td style="text-align: center;">0.30</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">97.94</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">AT+BGUP</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">2.2</td>
<td style="text-align: center;">6.6</td>
<td style="text-align: center;">8.8</td>
<td style="text-align: center;">0.20</td>
<td style="text-align: center;">98.60</td>
<td style="text-align: center;">98.60</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">AT+NUP</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">2.0</td>
<td style="text-align: center;">6.6</td>
<td style="text-align: center;">8.6</td>
<td style="text-align: center;">0.20</td>
<td style="text-align: center;">98.64</td>
<td style="text-align: center;">98.64</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$+P$</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">100.00</td>
<td style="text-align: center;">100.00</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">At Oracle</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">17.0</td>
<td style="text-align: center;">2.2</td>
<td style="text-align: center;">12.2</td>
<td style="text-align: center;">31.4</td>
<td style="text-align: center;">0.70</td>
<td style="text-align: center;">95.30</td>
<td style="text-align: center;">95.42</td>
</tr>
</tbody>
</table>
<p>Table 13: E2E test set semantic errors.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{5}$ Easier in the sense that the NUP re-ordering is closer to the training set distribution of AT utterance plans.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{6}$ Since their model does not realize specifier attributes, we do not include them in SER calculation. When including them, their model achieves $2.6 \%$ SER.&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
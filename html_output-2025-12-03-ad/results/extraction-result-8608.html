<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8608 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8608</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8608</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-154.html">extraction-schema-154</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-260899983</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2308.07902v1.pdf" target="_blank">Through the Lens of Core Competency: Survey on Evaluation of Large Language Models</a></p>
                <p><strong>Paper Abstract:</strong> “From pre-trained language model (PLM) to large language model (LLM), the field of naturallanguage processing (NLP) has witnessed steep performance gains and wide practical uses. Theevaluation of a research field guides its direction of improvement. However, LLMs are extremelyhard to thoroughly evaluate for two reasons. First of all, traditional NLP tasks become inade-quate due to the excellent performance of LLM. Secondly, existing evaluation tasks are difficultto keep up with the wide range of applications in real-world scenarios. To tackle these problems,existing works proposed various benchmarks to better evaluate LLMs. To clarify the numerousevaluation tasks in both academia and industry, we investigate multiple papers concerning LLMevaluations. We summarize 4 core competencies of LLM, including reasoning, knowledge, relia-bility, and safety. For every competency, we introduce its definition, corresponding benchmarks,and metrics. Under this competency architecture, similar tasks are combined to reflect corre-sponding ability, while new tasks can also be easily added into the system. Finally, we give oursuggestions on the future direction of LLM’s evaluation.”</p>
                <p><strong>Cost:</strong> 0.023</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8608.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8608.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Zero-shot CoT ("Let's think step by step")</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Zero-shot Chain-of-Thought prompting ("Let's think step by step")</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting technique that elicits step-by-step intermediate reasoning from large language models by appending a short instruction (e.g., "Let's think step by step") to a problem, enabling improved deductive-style reasoning in zero-shot settings.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Large language models are zero-shot reasoners</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Large language models (various; unspecified in survey)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Transformer-based large language models prompted to generate chain-of-thought style intermediate steps instead of a direct short answer; survey references this technique as applied to contemporary LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Deductive Reasoning via Zero-shot Chain-of-Thought</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Deduction-style problems where deriving a correct conclusion requires applying rules/logic step-by-step; CoT turns generation into an explicit multi-step chain of intermediate inferences.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Zero-shot chain-of-thought prompting using short natural-language instruction (e.g., "Let's think step by step").</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Survey reports that this technique better enables evaluation of deduction reasoning and elicits reasoning behavior; no quantitative results are reported in the survey itself.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Reported (in referenced work) to outperform standard direct prompting/no-CoT in many reasoning tasks and to exhibit stronger effects in larger models (survey: emergence with scale), but the survey does not provide numeric comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Survey notes deductive reasoning tests often intertwine with other skills and lack independent evaluation; CoT explanations can be unreliable or unfaithful to actual decision process (see Ye & Durrett 2022 and other cited concerns).</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Chain-of-thought style prompting is a practical and widely-adopted method for probing and improving deductive reasoning capabilities in LLMs, and its effectiveness tends to increase with model scale; however, faithfulness and independent evaluation remain open issues.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Through the Lens of Core Competency: Survey on Evaluation of Large Language Models', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8608.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8608.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CoT prompting (NeurIPS)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Thought prompting elicits reasoning in large language models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting paradigm shown (in referenced work) to elicit multi-step reasoning from LLMs by providing exemplars or instructions that demonstrate intermediate reasoning steps (chain-of-thought), improving performance on many complex reasoning benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Chain-of-thought prompting elicits reasoning in large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Large language models (various; unspecified in survey)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Transformer-based LLMs evaluated by researchers using chain-of-thought exemplars or instructions; survey references CoT as a general approach across LLM evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Various multi-step reasoning tasks (deduction, math word problems, commonsense multi-hop)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Tasks requiring explicit multi-step reasoning or intermediate calculations (e.g., arithmetic MWPs, multi-hop QA) where providing intermediate steps can help reach the correct final answer.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Chain-of-Thought prompting (few-shot or zero-shot) to elicit intermediate reasoning; sometimes combined with verification/reranking.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Survey states CoT improves performance on many benchmarks per referenced work but does not report exact metrics; it highlights CoT as widely used for reasoning evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Survey indicates CoT outperforms standard prompting/no intermediate steps and benefits from larger model scale; specific ablation numbers are not given in the survey.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Survey references reliability concerns (unreliable explanations, faithfulness) and that CoT may not fully separate reasoning ability from token-prediction capabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>CoT is an effective and influential technique for eliciting reasoning in LLMs and is central to many modern evaluations, but researchers should validate the faithfulness of produced rationales and consider independent benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Through the Lens of Core Competency: Survey on Evaluation of Large Language Models', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8608.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8608.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Faithful CoT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Faithful chain-of-thought reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An approach focused on improving the faithfulness (alignment between generated rationales and model decision) of chain-of-thought explanations to produce more reliable intermediate reasoning traces.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Faithful chain-of-thought reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Large language models (various; unspecified in survey)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>LLMs producing chain-of-thought rationales; the referenced work proposes techniques to encourage those rationales to be faithful to the actual computation or answer.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Deductive/chain-of-thought reasoning evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Evaluation of whether produced step-by-step rationales truly reflect valid reasoning leading to the final answer (faithfulness to deduction).</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Techniques to improve faithfulness of CoT outputs (referenced work proposes methods and evaluation strategies).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Survey does not report quantitative performance; referenced work aims to increase faithfulness compared to standard CoT.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared to standard CoT prompting without faithfulness constraints (survey: improvement claimed in referenced work, but numeric details absent here).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Survey highlights that explanations can be unreliable and that faithfulness remains challenging to guarantee across tasks and models.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Ensuring the faithfulness of chain-of-thought rationales is crucial for trustworthy logical reasoning evaluations; the survey emphasizes the need for methods that verify that intermediate steps genuinely reflect the model's reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Through the Lens of Core Competency: Survey on Evaluation of Large Language Models', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8608.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8608.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Binding LMs in Symbolic Languages</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Binding language models in symbolic languages</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method that constrains or translates LM outputs into symbolic languages/formalisms so models can perform more exact symbolic reasoning or generate programs that solve structured problem classes.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Binding language models in symbolic languages</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Large language models (various; unspecified in survey)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Survey references work that binds LLMs to symbolic representations (e.g., program-like output) to enable precise inductive/symbolic reasoning across problem classes.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Symbolic / Induction-style reasoning (program induction, class-of-problem solutions)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Tasks requiring generation of formal programs or symbolic expressions that generalize from examples to solve a class of related problems (inductive generalization into executable form).</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Constrain LM outputs to symbolic languages or induce programmatic solutions (neuro-symbolic binding), enabling exact execution/verification.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Survey notes this paradigm as promising (citing Cheng et al. 2023) but does not provide quantitative performance figures.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Survey implies improvement in systematicity and verifiability compared to unconstrained free-form generation, but no numeric ablations are provided.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Survey states that more systematic, comprehensive benchmarks for inductive/symbolic reasoning are still needed; current evaluations are preliminary.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Binding LMs to symbolic languages offers a path toward rigorous, verifiable logical reasoning (induction/program synthesis), but standardized benchmarks and further study are required to assess strengths and limits.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Through the Lens of Core Competency: Survey on Evaluation of Large Language Models', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8608.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8608.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GSM8K / Math benchmarks</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GSM8K (grade-school math benchmark) and related mathematical reasoning datasets</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A suite of mathematical word-problem benchmarks (e.g., GSM8K, LILA, NumGLUE) used to evaluate multi-step mathematical reasoning and logic in LLMs, often leveraged with chain-of-thought and verifier techniques.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Training verifiers to solve math word problems</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Large language models (various; unspecified in survey)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Transformer LLMs evaluated on elementary and more complex math word problems; survey cites GSM8K as the most widely used elementary math benchmark for LLM evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Mathematical Reasoning (e.g., GSM8K, LILA, NumGLUE)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Math word problems requiring multi-step numeric calculation and symbolic manipulation; tasks probe logical arithmetic reasoning and problem decomposition.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Chain-of-thought prompting, training verifiers, dynamic prompting, and execution/verification pipelines are commonly used to improve performance.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Survey states these benchmarks are widely used to demonstrate LLM mathematical reasoning capability improvements using CoT and verifiers; no numeric accuracies are reported in the survey.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Referenced works show gains from CoT and verifier training versus direct prompting; survey emphasizes emergence with larger models but provides no specific ablation numbers.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Survey highlights that mathematical reasoning remains challenging, especially for complex problems and across modalities/languages; external verification and grounding are active needs.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Mathematical benchmarks are central for assessing strict logical reasoning; combining CoT with verifiers or execution improves reliability, but robust generalization to harder problems is still limited.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Through the Lens of Core Competency: Survey on Evaluation of Large Language Models', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8608.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8608.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CLUTRR (Inductive)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CLUTRR: A diagnostic benchmark for inductive reasoning from text</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A benchmark designed to test inductive relational reasoning: models must infer general relationships/rules (e.g., family relations) from specific textual examples and generalize to new instances.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>CLUTRR: A diagnostic benchmark for inductive reasoning from text</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Language models (various; unspecified in survey)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Benchmarks aimed at probing LLMs' ability to perform inductive generalization from structured textual examples.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Inductive Relational Reasoning (CLUTRR)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Given specific relational stories/examples, induce underlying relations or rules and apply them to reason about new cases; tests abstraction and pattern induction.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Evaluate via few-shot/in-context learning and induction-focused prompts; survey references CLUTRR as an established diagnostic.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Survey notes CLUTRR as a diagnostic for inductive reasoning; no quantitative results are provided in the survey text.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Used to compare models' inductive abilities; survey does not report concrete ablation numbers.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Survey suggests more systematic benchmarks and evaluations are needed to fully characterize inductive reasoning in LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Inductive reasoning remains a distinct competency; CLUTRR helps probe it, but LLMs' ability to induce robust generalizations across diverse settings needs further study.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Through the Lens of Core Competency: Survey on Evaluation of Large Language Models', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8608.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e8608.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Transformers as soft reasoners</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Transformers as soft reasoners over language</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The idea and empirical line of work that transformers can perform approximate/soft logical reasoning directly over textual representations, treating reasoning as a learned soft operation rather than symbolic manipulation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Transformers as soft reasoners over language</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Transformer LMs (general; unspecified in survey)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Treats transformer LMs as soft differentiable reasoners operating on natural-language premises and rules; survey cites this conceptual framing as part of reasoning literature.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Soft linguistic logical reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Tasks where models infer consequences from textual premises and rules in a graded (soft) manner rather than via strict symbolic proof.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Train/evaluate transformers on textual rule-following and inference tasks; evaluate ability to emulate logical deduction in language space.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Survey references this approach conceptually; no quantitative performance is provided in the survey.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared to symbolic systems, soft reasoning trades exactness for flexibility; survey notes the distinction and the need to test on symbolic/deductive datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Survey remarks that soft, token-prediction-based reasoning may not equate to genuine logical reasoning and that independent evaluation on symbolic deductive datasets is lacking.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Viewing transformers as soft reasoners is useful but researchers should carefully distinguish statistical token-prediction from strict logical inference and create benchmarks that probe symbolic rigor.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Through the Lens of Core Competency: Survey on Evaluation of Large Language Models', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8608.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e8608.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4 (induction test mention)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 (as evaluated for inductive reasoning in referenced work)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-4 is mentioned in the survey as having been tested for inductive/pattern-induction abilities in referenced studies (e.g., Bills et al. 2023), used as an example of large models evaluated on higher-level reasoning tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Language models can explain neurons in language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI's large multimodal/LLM (referenced in survey); the survey cites works that analyze GPT-4's inductive capabilities and role in reasoning studies.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Inductive reasoning / pattern induction (as evaluated in referenced work)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Tasks that require discovering patterns or general rules from examples and using them to solve new instances; referenced works probe GPT-4's limits here.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Empirical probing and interpretability-style analyses; referenced work includes using LMs to explain internal activations and to evaluate pattern induction.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Survey states referenced works (Bills et al. 2023) tested GPT-4's inductive ability but does not report quantitative results in the survey.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Survey does not provide explicit baselines; referenced analyses are exploratory and interpretability-focused rather than strict numeric benchmark comparisons in the survey text.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Survey highlights that inductive reasoning remains challenging and that systematic, comprehensive benchmarks for this capability are still needed.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>High-capacity models like GPT-4 are the subject of inductive-reasoning studies, but the survey emphasizes the preliminary nature of current evaluations and the need for better benchmarks and interpretability tools.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Through the Lens of Core Competency: Survey on Evaluation of Large Language Models', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Chain-of-thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Large language models are zero-shot reasoners <em>(Rating: 2)</em></li>
                <li>Faithful chain-of-thought reasoning <em>(Rating: 2)</em></li>
                <li>Binding language models in symbolic languages <em>(Rating: 2)</em></li>
                <li>Training verifiers to solve math word problems <em>(Rating: 2)</em></li>
                <li>LILA: A unified benchmark for mathematical reasoning <em>(Rating: 2)</em></li>
                <li>CLUTRR: A diagnostic benchmark for inductive reasoning from text <em>(Rating: 2)</em></li>
                <li>Transformers as soft reasoners over language <em>(Rating: 2)</em></li>
                <li>Language models can explain neurons in language models <em>(Rating: 1)</em></li>
                <li>The unreliability of explanations in few-shot prompting for textual reasoning <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8608",
    "paper_id": "paper-260899983",
    "extraction_schema_id": "extraction-schema-154",
    "extracted_data": [
        {
            "name_short": "Zero-shot CoT (\"Let's think step by step\")",
            "name_full": "Zero-shot Chain-of-Thought prompting (\"Let's think step by step\")",
            "brief_description": "A prompting technique that elicits step-by-step intermediate reasoning from large language models by appending a short instruction (e.g., \"Let's think step by step\") to a problem, enabling improved deductive-style reasoning in zero-shot settings.",
            "citation_title": "Large language models are zero-shot reasoners",
            "mention_or_use": "mention",
            "model_name": "Large language models (various; unspecified in survey)",
            "model_description": "Transformer-based large language models prompted to generate chain-of-thought style intermediate steps instead of a direct short answer; survey references this technique as applied to contemporary LLMs.",
            "model_size": null,
            "reasoning_task_name": "Deductive Reasoning via Zero-shot Chain-of-Thought",
            "reasoning_task_description": "Deduction-style problems where deriving a correct conclusion requires applying rules/logic step-by-step; CoT turns generation into an explicit multi-step chain of intermediate inferences.",
            "method_or_approach": "Zero-shot chain-of-thought prompting using short natural-language instruction (e.g., \"Let's think step by step\").",
            "performance": "Survey reports that this technique better enables evaluation of deduction reasoning and elicits reasoning behavior; no quantitative results are reported in the survey itself.",
            "baseline_comparison": "Reported (in referenced work) to outperform standard direct prompting/no-CoT in many reasoning tasks and to exhibit stronger effects in larger models (survey: emergence with scale), but the survey does not provide numeric comparisons.",
            "limitations_or_failures": "Survey notes deductive reasoning tests often intertwine with other skills and lack independent evaluation; CoT explanations can be unreliable or unfaithful to actual decision process (see Ye & Durrett 2022 and other cited concerns).",
            "insights_or_conclusions": "Chain-of-thought style prompting is a practical and widely-adopted method for probing and improving deductive reasoning capabilities in LLMs, and its effectiveness tends to increase with model scale; however, faithfulness and independent evaluation remain open issues.",
            "uuid": "e8608.0",
            "source_info": {
                "paper_title": "Through the Lens of Core Competency: Survey on Evaluation of Large Language Models",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "CoT prompting (NeurIPS)",
            "name_full": "Chain-of-Thought prompting elicits reasoning in large language models",
            "brief_description": "A prompting paradigm shown (in referenced work) to elicit multi-step reasoning from LLMs by providing exemplars or instructions that demonstrate intermediate reasoning steps (chain-of-thought), improving performance on many complex reasoning benchmarks.",
            "citation_title": "Chain-of-thought prompting elicits reasoning in large language models",
            "mention_or_use": "mention",
            "model_name": "Large language models (various; unspecified in survey)",
            "model_description": "Transformer-based LLMs evaluated by researchers using chain-of-thought exemplars or instructions; survey references CoT as a general approach across LLM evaluations.",
            "model_size": null,
            "reasoning_task_name": "Various multi-step reasoning tasks (deduction, math word problems, commonsense multi-hop)",
            "reasoning_task_description": "Tasks requiring explicit multi-step reasoning or intermediate calculations (e.g., arithmetic MWPs, multi-hop QA) where providing intermediate steps can help reach the correct final answer.",
            "method_or_approach": "Chain-of-Thought prompting (few-shot or zero-shot) to elicit intermediate reasoning; sometimes combined with verification/reranking.",
            "performance": "Survey states CoT improves performance on many benchmarks per referenced work but does not report exact metrics; it highlights CoT as widely used for reasoning evaluation.",
            "baseline_comparison": "Survey indicates CoT outperforms standard prompting/no intermediate steps and benefits from larger model scale; specific ablation numbers are not given in the survey.",
            "limitations_or_failures": "Survey references reliability concerns (unreliable explanations, faithfulness) and that CoT may not fully separate reasoning ability from token-prediction capabilities.",
            "insights_or_conclusions": "CoT is an effective and influential technique for eliciting reasoning in LLMs and is central to many modern evaluations, but researchers should validate the faithfulness of produced rationales and consider independent benchmarks.",
            "uuid": "e8608.1",
            "source_info": {
                "paper_title": "Through the Lens of Core Competency: Survey on Evaluation of Large Language Models",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "Faithful CoT",
            "name_full": "Faithful chain-of-thought reasoning",
            "brief_description": "An approach focused on improving the faithfulness (alignment between generated rationales and model decision) of chain-of-thought explanations to produce more reliable intermediate reasoning traces.",
            "citation_title": "Faithful chain-of-thought reasoning",
            "mention_or_use": "mention",
            "model_name": "Large language models (various; unspecified in survey)",
            "model_description": "LLMs producing chain-of-thought rationales; the referenced work proposes techniques to encourage those rationales to be faithful to the actual computation or answer.",
            "model_size": null,
            "reasoning_task_name": "Deductive/chain-of-thought reasoning evaluation",
            "reasoning_task_description": "Evaluation of whether produced step-by-step rationales truly reflect valid reasoning leading to the final answer (faithfulness to deduction).",
            "method_or_approach": "Techniques to improve faithfulness of CoT outputs (referenced work proposes methods and evaluation strategies).",
            "performance": "Survey does not report quantitative performance; referenced work aims to increase faithfulness compared to standard CoT.",
            "baseline_comparison": "Compared to standard CoT prompting without faithfulness constraints (survey: improvement claimed in referenced work, but numeric details absent here).",
            "limitations_or_failures": "Survey highlights that explanations can be unreliable and that faithfulness remains challenging to guarantee across tasks and models.",
            "insights_or_conclusions": "Ensuring the faithfulness of chain-of-thought rationales is crucial for trustworthy logical reasoning evaluations; the survey emphasizes the need for methods that verify that intermediate steps genuinely reflect the model's reasoning.",
            "uuid": "e8608.2",
            "source_info": {
                "paper_title": "Through the Lens of Core Competency: Survey on Evaluation of Large Language Models",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "Binding LMs in Symbolic Languages",
            "name_full": "Binding language models in symbolic languages",
            "brief_description": "A method that constrains or translates LM outputs into symbolic languages/formalisms so models can perform more exact symbolic reasoning or generate programs that solve structured problem classes.",
            "citation_title": "Binding language models in symbolic languages",
            "mention_or_use": "mention",
            "model_name": "Large language models (various; unspecified in survey)",
            "model_description": "Survey references work that binds LLMs to symbolic representations (e.g., program-like output) to enable precise inductive/symbolic reasoning across problem classes.",
            "model_size": null,
            "reasoning_task_name": "Symbolic / Induction-style reasoning (program induction, class-of-problem solutions)",
            "reasoning_task_description": "Tasks requiring generation of formal programs or symbolic expressions that generalize from examples to solve a class of related problems (inductive generalization into executable form).",
            "method_or_approach": "Constrain LM outputs to symbolic languages or induce programmatic solutions (neuro-symbolic binding), enabling exact execution/verification.",
            "performance": "Survey notes this paradigm as promising (citing Cheng et al. 2023) but does not provide quantitative performance figures.",
            "baseline_comparison": "Survey implies improvement in systematicity and verifiability compared to unconstrained free-form generation, but no numeric ablations are provided.",
            "limitations_or_failures": "Survey states that more systematic, comprehensive benchmarks for inductive/symbolic reasoning are still needed; current evaluations are preliminary.",
            "insights_or_conclusions": "Binding LMs to symbolic languages offers a path toward rigorous, verifiable logical reasoning (induction/program synthesis), but standardized benchmarks and further study are required to assess strengths and limits.",
            "uuid": "e8608.3",
            "source_info": {
                "paper_title": "Through the Lens of Core Competency: Survey on Evaluation of Large Language Models",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "GSM8K / Math benchmarks",
            "name_full": "GSM8K (grade-school math benchmark) and related mathematical reasoning datasets",
            "brief_description": "A suite of mathematical word-problem benchmarks (e.g., GSM8K, LILA, NumGLUE) used to evaluate multi-step mathematical reasoning and logic in LLMs, often leveraged with chain-of-thought and verifier techniques.",
            "citation_title": "Training verifiers to solve math word problems",
            "mention_or_use": "mention",
            "model_name": "Large language models (various; unspecified in survey)",
            "model_description": "Transformer LLMs evaluated on elementary and more complex math word problems; survey cites GSM8K as the most widely used elementary math benchmark for LLM evaluation.",
            "model_size": null,
            "reasoning_task_name": "Mathematical Reasoning (e.g., GSM8K, LILA, NumGLUE)",
            "reasoning_task_description": "Math word problems requiring multi-step numeric calculation and symbolic manipulation; tasks probe logical arithmetic reasoning and problem decomposition.",
            "method_or_approach": "Chain-of-thought prompting, training verifiers, dynamic prompting, and execution/verification pipelines are commonly used to improve performance.",
            "performance": "Survey states these benchmarks are widely used to demonstrate LLM mathematical reasoning capability improvements using CoT and verifiers; no numeric accuracies are reported in the survey.",
            "baseline_comparison": "Referenced works show gains from CoT and verifier training versus direct prompting; survey emphasizes emergence with larger models but provides no specific ablation numbers.",
            "limitations_or_failures": "Survey highlights that mathematical reasoning remains challenging, especially for complex problems and across modalities/languages; external verification and grounding are active needs.",
            "insights_or_conclusions": "Mathematical benchmarks are central for assessing strict logical reasoning; combining CoT with verifiers or execution improves reliability, but robust generalization to harder problems is still limited.",
            "uuid": "e8608.4",
            "source_info": {
                "paper_title": "Through the Lens of Core Competency: Survey on Evaluation of Large Language Models",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "CLUTRR (Inductive)",
            "name_full": "CLUTRR: A diagnostic benchmark for inductive reasoning from text",
            "brief_description": "A benchmark designed to test inductive relational reasoning: models must infer general relationships/rules (e.g., family relations) from specific textual examples and generalize to new instances.",
            "citation_title": "CLUTRR: A diagnostic benchmark for inductive reasoning from text",
            "mention_or_use": "mention",
            "model_name": "Language models (various; unspecified in survey)",
            "model_description": "Benchmarks aimed at probing LLMs' ability to perform inductive generalization from structured textual examples.",
            "model_size": null,
            "reasoning_task_name": "Inductive Relational Reasoning (CLUTRR)",
            "reasoning_task_description": "Given specific relational stories/examples, induce underlying relations or rules and apply them to reason about new cases; tests abstraction and pattern induction.",
            "method_or_approach": "Evaluate via few-shot/in-context learning and induction-focused prompts; survey references CLUTRR as an established diagnostic.",
            "performance": "Survey notes CLUTRR as a diagnostic for inductive reasoning; no quantitative results are provided in the survey text.",
            "baseline_comparison": "Used to compare models' inductive abilities; survey does not report concrete ablation numbers.",
            "limitations_or_failures": "Survey suggests more systematic benchmarks and evaluations are needed to fully characterize inductive reasoning in LLMs.",
            "insights_or_conclusions": "Inductive reasoning remains a distinct competency; CLUTRR helps probe it, but LLMs' ability to induce robust generalizations across diverse settings needs further study.",
            "uuid": "e8608.5",
            "source_info": {
                "paper_title": "Through the Lens of Core Competency: Survey on Evaluation of Large Language Models",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "Transformers as soft reasoners",
            "name_full": "Transformers as soft reasoners over language",
            "brief_description": "The idea and empirical line of work that transformers can perform approximate/soft logical reasoning directly over textual representations, treating reasoning as a learned soft operation rather than symbolic manipulation.",
            "citation_title": "Transformers as soft reasoners over language",
            "mention_or_use": "mention",
            "model_name": "Transformer LMs (general; unspecified in survey)",
            "model_description": "Treats transformer LMs as soft differentiable reasoners operating on natural-language premises and rules; survey cites this conceptual framing as part of reasoning literature.",
            "model_size": null,
            "reasoning_task_name": "Soft linguistic logical reasoning",
            "reasoning_task_description": "Tasks where models infer consequences from textual premises and rules in a graded (soft) manner rather than via strict symbolic proof.",
            "method_or_approach": "Train/evaluate transformers on textual rule-following and inference tasks; evaluate ability to emulate logical deduction in language space.",
            "performance": "Survey references this approach conceptually; no quantitative performance is provided in the survey.",
            "baseline_comparison": "Compared to symbolic systems, soft reasoning trades exactness for flexibility; survey notes the distinction and the need to test on symbolic/deductive datasets.",
            "limitations_or_failures": "Survey remarks that soft, token-prediction-based reasoning may not equate to genuine logical reasoning and that independent evaluation on symbolic deductive datasets is lacking.",
            "insights_or_conclusions": "Viewing transformers as soft reasoners is useful but researchers should carefully distinguish statistical token-prediction from strict logical inference and create benchmarks that probe symbolic rigor.",
            "uuid": "e8608.6",
            "source_info": {
                "paper_title": "Through the Lens of Core Competency: Survey on Evaluation of Large Language Models",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "GPT-4 (induction test mention)",
            "name_full": "GPT-4 (as evaluated for inductive reasoning in referenced work)",
            "brief_description": "GPT-4 is mentioned in the survey as having been tested for inductive/pattern-induction abilities in referenced studies (e.g., Bills et al. 2023), used as an example of large models evaluated on higher-level reasoning tasks.",
            "citation_title": "Language models can explain neurons in language models",
            "mention_or_use": "mention",
            "model_name": "GPT-4",
            "model_description": "OpenAI's large multimodal/LLM (referenced in survey); the survey cites works that analyze GPT-4's inductive capabilities and role in reasoning studies.",
            "model_size": null,
            "reasoning_task_name": "Inductive reasoning / pattern induction (as evaluated in referenced work)",
            "reasoning_task_description": "Tasks that require discovering patterns or general rules from examples and using them to solve new instances; referenced works probe GPT-4's limits here.",
            "method_or_approach": "Empirical probing and interpretability-style analyses; referenced work includes using LMs to explain internal activations and to evaluate pattern induction.",
            "performance": "Survey states referenced works (Bills et al. 2023) tested GPT-4's inductive ability but does not report quantitative results in the survey.",
            "baseline_comparison": "Survey does not provide explicit baselines; referenced analyses are exploratory and interpretability-focused rather than strict numeric benchmark comparisons in the survey text.",
            "limitations_or_failures": "Survey highlights that inductive reasoning remains challenging and that systematic, comprehensive benchmarks for this capability are still needed.",
            "insights_or_conclusions": "High-capacity models like GPT-4 are the subject of inductive-reasoning studies, but the survey emphasizes the preliminary nature of current evaluations and the need for better benchmarks and interpretability tools.",
            "uuid": "e8608.7",
            "source_info": {
                "paper_title": "Through the Lens of Core Competency: Survey on Evaluation of Large Language Models",
                "publication_date_yy_mm": "2023-08"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Chain-of-thought prompting elicits reasoning in large language models",
            "rating": 2,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Large language models are zero-shot reasoners",
            "rating": 2,
            "sanitized_title": "large_language_models_are_zeroshot_reasoners"
        },
        {
            "paper_title": "Faithful chain-of-thought reasoning",
            "rating": 2,
            "sanitized_title": "faithful_chainofthought_reasoning"
        },
        {
            "paper_title": "Binding language models in symbolic languages",
            "rating": 2,
            "sanitized_title": "binding_language_models_in_symbolic_languages"
        },
        {
            "paper_title": "Training verifiers to solve math word problems",
            "rating": 2,
            "sanitized_title": "training_verifiers_to_solve_math_word_problems"
        },
        {
            "paper_title": "LILA: A unified benchmark for mathematical reasoning",
            "rating": 2,
            "sanitized_title": "lila_a_unified_benchmark_for_mathematical_reasoning"
        },
        {
            "paper_title": "CLUTRR: A diagnostic benchmark for inductive reasoning from text",
            "rating": 2,
            "sanitized_title": "clutrr_a_diagnostic_benchmark_for_inductive_reasoning_from_text"
        },
        {
            "paper_title": "Transformers as soft reasoners over language",
            "rating": 2,
            "sanitized_title": "transformers_as_soft_reasoners_over_language"
        },
        {
            "paper_title": "Language models can explain neurons in language models",
            "rating": 1,
            "sanitized_title": "language_models_can_explain_neurons_in_language_models"
        },
        {
            "paper_title": "The unreliability of explanations in few-shot prompting for textual reasoning",
            "rating": 2,
            "sanitized_title": "the_unreliability_of_explanations_in_fewshot_prompting_for_textual_reasoning"
        }
    ],
    "cost": 0.023171749999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Through the Lens of Core Competency: Survey on Evaluation of Large Language Models
15 Aug 2023</p>
<p>Ziyu Zhuang zyzhuang@ir.hit.edu.cn 
Research Center for Social Computing and Information Retrieval
Harbin Institute of Technology</p>
<p>Qiguang Chen qgchen@ir.hit.edu.cn 
Research Center for Social Computing and Information Retrieval
Harbin Institute of Technology</p>
<p>Longxuan Ma lxma@ir.hit.edu.cn 
Research Center for Social Computing and Information Retrieval
Harbin Institute of Technology</p>
<p>Mingda Li mdli@ir.hit.edu.cn 
Research Center for Social Computing and Information Retrieval
Harbin Institute of Technology</p>
<p>Yi Han yihan@ir.hit.edu.cn 
Research Center for Social Computing and Information Retrieval
Harbin Institute of Technology</p>
<p>Yushan Qian ysqian@ir.hit.edu.cn 
Research Center for Social Computing and Information Retrieval
Harbin Institute of Technology</p>
<p>Haopeng Bai hpbai@ir.hit.edu.cn 
Research Center for Social Computing and Information Retrieval
Harbin Institute of Technology</p>
<p>Zixian Feng zxfeng@ir.hit.edu.cn 
Research Center for Social Computing and Information Retrieval
Harbin Institute of Technology</p>
<p>Weinan Zhang wnzhang@ir.hit.edu.cn 
Research Center for Social Computing and Information Retrieval
Harbin Institute of Technology</p>
<p>Ting Liu tliu@ir.hit.edu.cn 
Research Center for Social Computing and Information Retrieval
Harbin Institute of Technology</p>
<p>Through the Lens of Core Competency: Survey on Evaluation of Large Language Models
15 Aug 2023
From pre-trained language model (PLM) to large language model (LLM), the field of natural language processing (NLP) has witnessed steep performance gains and wide practical uses. The evaluation of a research field guides its direction of improvement. However, LLMs are extremely hard to thoroughly evaluate for two reasons. First of all, traditional NLP tasks become inadequate due to the excellent performance of LLM. Secondly, existing evaluation tasks are difficult to keep up with the wide range of applications in real-world scenarios. To tackle these problems, existing works proposed various benchmarks to better evaluate LLMs. To clarify the numerous evaluation tasks in both academia and industry, we investigate multiple papers concerning LLM evaluations. We summarize 4 core competencies of LLM, including reasoning, knowledge, reliability, and safety. For every competency, we introduce its definition, corresponding benchmarks, and metrics. Under this competency architecture, similar tasks are combined to reflect corresponding ability, while new tasks can also be easily added into the system. Finally, we give our suggestions on the future direction of LLM's evaluation. *</p>
<p>Introduction</p>
<p>Large language models(LLMs) have achieved great progresses in many areas. One representative, Chat-GPT 0 , which applies the ability of LLMs in the form of dialogue, has received much attention due to its incredible versatility such as creative writing, coding, planning, etc. The evaluation of such a model thus becomes necessary to benchmark and build up its ability while preventing potential harmfulness.</p>
<p>Existing works on the evaluation of LLMs can be divided into three paradigms. The first line of work is evaluating LLMs with traditional NLP tasks like dialogue, summarization, etc. Since LLMs are actually pre-trained language models(PLMs) with huge model parameter size and data size (Kaplan et al., 2020), benchmarks like GLUE (Wang et al., 2019b), SuperGLUE (Wang et al., 2019a) can be adopted to evaluate its language understanding ability. The problem is that LLMs work really well on less restrictive tasks like translation, summarization, and natural language understanding tasks. Sometimes LLMs generated outputs' third-party scores are even higher than human generations , showing the need for higher-quality tasks. Secondly, advanced ability evaluations are proposed to completely test language models. The parameter size difference between LLMs and PLMs brings an amazing phenomenon, emergence (Wei et al., 2022a;Srivastava et al., 2022), which means that scaled models exhibit abilities that are not possessed in small-scaled language models. For instance, in tasks like reasoning, and tool manipulation, the correlation curve between the number of model parameters and the task effect is non-linear. And the effect will rise sharply when the model parameter exceeds a certain parameter scale. They're called "advanced" because they're more closely related to human abilities and harder for models to complete (Zhong et al., 2023). Thirdly, test language models' intrinsic abilities independent of the specific tasks. It can be tested in parallel with almost every task above. Robustness is a classic abil-ity in this paradigm. Due to the black-box nature of neural networks (Szegedy et al., 2014), robustness problems exist for every modality of input data (vision, audio, test, etc.).</p>
<p>Current evaluation benchmarks Srivastava et al., 2022;Gao et al., 2021;Zhong et al., 2023;Li et al., 2023a) are mostly a mixture of the former three paradigms. They emphasize a complete system of evaluation tasks, in which all tasks are of equal importance. But the significance of marginal increases in model effects on tasks with excellent performance is debatable. Thus numerous evaluation tasks and benchmarks are proposed to follow and challenge the ever-evolving LLMs, while, oddly, seldom being reviewed in a systematic way. How to link numerous tasks and benchmarks, better present the evaluation results, and thus facilitate the research of LLMs is an urgent problem.</p>
<p>An ideal large language model needs to be capable, reliable, and safe (Ouyang et al., 2022). One surely needs extensive tests on multiple datasets to meet these miscellaneous standards. Moreover, to avoid the prevalent training set leakage, test sets also should be updated regularly . This is similar to the competency (Hoffmann, 1999) tests adopted in corporate recruitment. In competency tests, different task sets are combined to test the corresponding competency. And task sets also need renewal to prevent possible fraud.</p>
<p>In this survey, we draw on the concept of the core competency to integrate multiple evaluation research for LLMs. We investigated 540+ tasks widely used in various papers, aggregating tasks corresponding to a certain competency. During this process, 4 core competencies are summarized, including knowledge, reasoning, reliability, and safety. We will introduce the definition, taxonomy, and metrics for these competencies. Through this competency test, superabundant evaluation tasks and benchmarks are combed and clarified for their aiming utility. Furthermore, the evaluation results presented with this procedure will be direct, concise, and focused. Updated new tasks can also be added comprehensively. To support the community in taking this competency test further, We also create an extensible project, which will show the many-to-many relationship between competencies and tasks precisely 1 . Due to the length of the paper, we can only present part of the surveyed results in this paper. A more comprehensive study will be released in a later version.</p>
<p>Core Competencies</p>
<p>In this section, we introduce the definition and taxonomy of the core competencies we summarized.</p>
<p>Knowledge</p>
<p>Knowledge is generally defined as the cognition of humans when practicing in the subjective and objective world, which is verified and can be reused over time 2 . The large language models (LLMs) nowadays obtain human knowledge from a large scale of training corpus, so that it can use the knowledge to solve various downstream tasks. In this section, we focus on the fundamental knowledge competency of LLMs that facilitates communication and other downstream tasks (such as reasoning). Specifically, we divide the fundamental knowledge into linguistic knowledge and world knowledge (Day et al., 1998) and introduce the definitions of them and the benchmarks that can evaluate them.</p>
<p>Linguistic Knowledge Competency</p>
<p>Linguistic knowledge includes grammatical, semantic, and pragmatic knowledge (Fromkin et al., 2018). The grammar of a natural language is its set of structural constraints on speakers' or writers' composition of clauses, phrases, and words. The term can also refer to the study of such constraints, a field that includes domains such as phonology, morphology, and syntax, often complemented by phonetics, semantics, and pragmatics. Semantic (Austin, 1975) studies the meaning of words, phrases, and sentences, focusing on general meanings rather than on what an individual speaker may want them to mean. Pragmatics (Austin, 1975) studies language use and how listeners bridge the gap between sentence meaning and the speaker's meaning. It is concerned with the relationship between semantic meaning, the context of use, and the speaker's meaning.  (Warstadt et al., 2020) evaluates what language models (LMs) know about major grammatical phenomena. Linguistic mappings 3 task aims to explore the depth of linguistic knowledge in enormous language models trained on word prediction. It aims to discover whether such knowledge is structured so as to support the use of grammatical abstractions, both morphological (past tense formation and pluralization) and syntactic (question formation, negation, and pronominalization). The minute mysteries qa 4 is a reading comprehension task focusing on short crime and mystery stories where the goal is to identify the perpetrator and to explain the reasoning behind the deduction and the clues that support it. The metaphor boolean 5 task presents a model with a metaphoric sentence and asks it to identify whether a second sentence is the correct interpretation of the first. The last three are selected from BIG-Bench (Srivastava et al., 2022), containing diverse task topics including linguistics.</p>
<p>World Knowledge Competency</p>
<p>World knowledge is non-linguistic information that helps a reader or listener interpret the meanings of words and sentences (Ovchinnikova, 2012). It is also referred to as extra-linguistic knowledge. In this paper, we categorize world knowledge into general knowledge and domain knowledge. The general knowledge includes commonsense knowledge (Davis, 2014) and prevalent knowledge. The commonsense knowledge consists of world facts, such as "Lemons are sour", or "Cows say moo", that most humans are expected to know. The prevalent knowledge exists at a particular time or place. For example, "Chinese people are used to drinking boiled water." is only known by a part of human beings; "There were eight planets in the solar system" is prevalent knowledge until it is overthrown. The domain knowledge (Alexander, 1992) is of a specific, specialized discipline or field, in contrast to general or domain-independent knowledge. People who have domain knowledge, are often considered specialists or experts in the field.</p>
<p>The bottom group of Table 1 shows some task examples that are used for testing world knowledge. For example, the LexGLUE (Chalkidis et al., 2022) tests whether LLMs perform well in the legal domain; WikiFact  is a fact completion scenario that tests language models' factual knowledge based on Wikipedia. The input will be a partial sentence such as "The capital of France is ", and the output will be the continuation of the sentence such as "Paris"; TruthfulQA (Lin et al., 2022b) comprises questions spanning numerous categories including economics, science, and law. The questions are strategically chosen so humans may also incorrectly answer them based on misconceptions and biases; language models should ideally return accurate and truthful responses; HellaSwag (Zellers et al., 2019) tests commonsense inference and was created through adversarial filtering to synthesize wrong answers. The World knowledge competency, along with linguistic knowledge, serves as the foundation for solving different NLP tasks and is one of the core competencies of LLMs.</p>
<p>Reasoning</p>
<p>Reasoning competency is a crucial skill for LLMs to solve complex problems. What's more, from the perspective of intelligent agents, reasoning ability is also one of the core capabilities towards achieving  AGI (Bubeck et al., 2023;Qiao et al., 2022). However, there remains no consensus whether LLMs can really reason, or just simply produce a larger context that increases the likelihood of correctly predicting the missing tokens (Mialon et al., 2023). Although "reasoning" itself may currently be an excuse of language, we can still objectively verify the reasoning performance of LLMs through various reasoning competencies. Previous methods mainly focus on the division of reasoning tasks.  divides existing evaluation tasks into three major categories, namely knowledge reasoning, symbolic reasoning, and mathematical reasoning, based on the type of logic and evidence involved in the reasoning process. Zhao et al. (2023) divides reasoning tasks into deductive reasoning and defeasible reasoning according to the reasoning form. In this section, we decompose the reasoning competency into 6 sub-parts from the perspective of model competency, providing a comprehensive overview of existing research efforts and suggesting potential future directions. And Table 2 presents some datasets for evaluating LLM's reasoning competency using this categorization approach.</p>
<p>Causal Reasoning Competency</p>
<p>Causal reasoning competency is a highly significant cognitive ability aimed at inferring causality through the observation of cause-effect relationships (Vowels et al., 2023;Dündar-Coecke, 2022;Chan et al., 2023). It enables us to comprehend and explain the relationships between events, variables, and actions, ultimately empowering us to make informed predictions and decisions .</p>
<p>The benchmarks Causal-TimeBank (Mirza et al., 2014), StoryLine (Caselli and Vossen, 2017), and MAVEN-ERE (Wang et al., 2022c) aim to test the existence of causal relationships between two events in sentences. COPA (Gordon et al., 2012) and XCOPA (Ponti et al., 2020) are evaluation benchmarks for extracting causal relationships in sentences, consisting of a set of premises and possible causes or effects. Tested systems are required to apply commonsense knowledge to identify the correct answers. e-CARE (Du et al., 2022) and CALM-Bench (Dalal et al., 2023) introduce a set of causal querying tasks to evaluate models, which include a cause and several potential effect sentences. Additionally, an annotated and interpretable causal reasoning dataset is provided for these tasks.</p>
<p>Deduction Reasoning Competency</p>
<p>In the era of Large Language Models (LLMs), deductive reasoning abilities serve as the foundational skills for logical reasoning (Evans, 2002). Unlike traditional rule-based deductive reasoning systems, it involves deriving specific conclusions or answers from general and universally applicable premises using given rules and logic. Specifically, it manifests as a process of Zero-Shot Chain-of-Thought utilizing given rules (Lyu et al., 2023;Kojima et al., 2022). For instance, (Kojima et al., 2022) introduced the "Let's think step by step" prompt technique to better evaluate the Deduction Reasoning Competency.</p>
<p>Current testing of this ability often intertwines with other skills and still lacks an independent evaluation on typical text (Clark et al., 2020) and symbol-related (Wu et al., 2021) deductive datasets. However, in general, almost all QA tasks can be explicitly evaluated for Deduction Reasoning using the Chain-of-Thought (CoT) approach. Therefore, the effectiveness of models' Deduction Reasoning Competency can be to some extent reflected by evaluating the performance of QA tasks after applying the CoT method.</p>
<p>Induction Reasoning Competency</p>
<p>In contrast to deductive reasoning, inductive reasoning aims to derive conclusions from specific observations to general principles Olsson et al., 2022). In recent years, a new paradigm of Induction Reasoning has been proposed by (Cheng et al., 2023), which requires models to generate general-purpose program code to solve a class of problems based on given contextual questions and a specific question. For example, Cheng et al. (2023), Jiang et al. (2023) and Surís et al. (2023) induced general principle-based solutions by generalizing each question into a universal executable language.</p>
<p>Therefore, for competency evaluation, while DEER  and Mathematical Induction (BIGBench Split (Srivastava et al., 2022)) took the first step in inductive reasoning, we still hope to establish a more systematic and comprehensive benchmark for evaluating this capability. Recently, Bills et al. (2023) has tested the inductive ability of GPT-4 (OpenAI, 2023) to evaluate its effectiveness in inducing patterns that are difficult for humans to express clearly. Intriguingly, Mankowitz et al. (2023) used some techniques to evaluate the extent to which LLM can mine previously unknown patterns.</p>
<p>Abduction Reasoning Competency</p>
<p>Abduction Reasoning Competency encompasses the task of providing explanations for the output generated based on given inputs (Kakas and Michael, 2020). This form of reasoning is particularly critical in scenarios where uncertainty or incomplete information exists, enabling systems to generate hypotheses and make informed decisions based on the available evidence. Notably, the research conducted by LIREx (Zhao and Vydiswaran, 2021) and STaR (Zelikman et al., 2022) delved into the Abduction Reasoning Competency of models and demonstrated the effectiveness of rationales provided during the Abduction Reasoning process in facilitating improved learning in downstream models.</p>
<p>In terms of datasets within the LLM setting, the benchmarks HUMMINGBIRD (Mathew et al., 2021) and HateXplain (Hayati et al., 2021) require models to output word-level textual segments as explanations for sentiment classification results. On the other hand, benchmarks such as Wik-iQA (Yang et al., 2015), HotpotQA (Yang et al., 2018), and SciFact (Wadden et al., 2020) provide sentence-level coarse-grained textual segments as explanations for model classification results. ERASER (DeYoung et al., 2020) and FineIEB  provide benchmarks for evaluating Abduction Reasoning with diverse granularity explanations. Based on previous research, Synthetic Reasoning  provides a comprehensive evaluation of both Deduction Reasoning and Abduction Reasoning Competency. Moreover, Hessel et al. (2022) introduced the first comprehensive multimodal benchmark for testing Abduction Reasoning capabilities, providing a solid foundation for future advancements in this domain. Recently, Bills et al. (2023) evaluate GPT-4 by observing the activation of neurons in GPT-2 and offering explanations for the GPT-2's outputs. This research avenue also presents a novel approach for exploring the future evaluation of Abduction Reasoning Competency.</p>
<p>Analogical Reasoning Competency</p>
<p>Analogy reasoning competency encompasses the ability of reasoning by identifying and applying similarities between diverse situations or domains. It is based on the assumption that similar cases or objects tend to exhibit common attributes or behaviors. By recognizing these similarities, analogy reasoning enables systems to transfer knowledge or experience from one context to another (Sinha et al., 2019;Wei et al., 2022b). This type of reasoning plays a vital role in problem-solving, decision-making, and learning from past experiences. A typical example is In-Context-Learning (Dong et al., 2023), where the model is required to perform analogical reasoning based on given contexts, which are evaluated based on the final analogical results.</p>
<p>For a better assessment and understanding of the model's analogical reasoning ability, Brown et al. (2020) introduces SAT Analogies as a test to evaluate LLM's analogical reasoning capabilities. In recent years, Authorship Verification and ARC datasets (Srivastava et al., 2022) have also proposed evaluation benchmark that involve presenting contextual examples and requiring the model to produce induced pattern-compliant results. However, it should be noted that In-Context Learning (ICL) can be utilized for almost all tasks, enabling the evaluation of models' Analogical Reasoning Competency to some extent through the assessment of their performance after undergoing ICL.</p>
<p>Multi-hop Reasoning Competency</p>
<p>Multi-hop reasoning refers to the ability to combine and integrate information from multiple sources or contexts to arrive at logical conclusions. This competency of reasoning enables systems to retrieve coherent and comprehensive answers by traversing multiple pieces of information, thus performing complex tasks of information retrieval, comprehension, and reasoning (Wang et al., 2022a;Qiu et al., 2019).</p>
<p>Currently, HotpotQA (Yang et al., 2018) serves as a commonly used dataset for multi-hop question answering tasks. Expanding on this, Ye and Durrett (2022) introduced a new and demanding subset that aimed to achieve a balance between accurate and inaccurate predictions using their model. Similarly, StrategyQA (Geva et al., 2021) is another widely used benchmark for multi-hop question answering (Wei et al., 2022b), where the required reasoning steps are implicit in the questions and should be inferred using strategies.</p>
<p>Reasoning in Scenarios</p>
<p>Commonsense Reasoning Commonsense reasoning is crucial for machines to achieve human-like understanding and interaction with the world in the field of machine intelligence (Storks et al., 2019;Bhargava and Ng, 2022). The ability to comprehend and apply commonsense knowledge enables machines to make accurate predictions, engage in logical reasoning, and navigate complex social situations.</p>
<p>OpenBookQA (Mihaylov et al., 2018) provides a foundational test for evaluating Commonsense Reasoning abilities in the form of an open-book exam. Building upon this, Common-senseQA (Talmor et al., 2019) requires models to employ rich world knowledge for reasoning tasks. PIQA (Bisk et al., 2020) introduces a dataset for testing models' understanding of physical world commonsense reasoning. StrategyQA (Geva et al., 2021) presents a complex benchmark that requires commonsense-based multi-step/multi-hop reasoning, enabling a better exploration of the upper limits of models' Commonsense Reasoning Competency. Currently, due to early research on LLM (Wei et al., 2022b), CommonsenseQA (Talmor et al., 2019) remains the most widely used benchmark for commonsense reasoning.</p>
<p>Mathematical Reasoning Mathematical reasoning competency is crucial for general intelligent systems. It empowers intelligent systems with the capability of logical reasoning, problem-solving, and data manipulation and analysis, thereby facilitating the development and application of intelligent systems (Qiao et al., 2022;Mishra et al., 2022b;Mishra et al., 2022a).</p>
<p>Early evluation studies focused on small datasets of elementary-level mathematical word problems (MWPs) (Hosseini et al., 2014), but subsequent research aimed to increase complexity and scale (Srivastava et al., 2022;Brown et al., 2020).</p>
<p>Furthermore, recent benchmarks (Mishra et al., 2022b;Mishra et al., 2022a) have provided comprehensive evaluation platforms and benchmarks for mathematical reasoning abilities. GSM8K (Cobbe et al., 2021) aims to evaluate elementary school MWPs. Currently, due to early research efforts on LLMs (Wei et al., 2022b), it remains the most widely used benchmark for mathematical reasoning in the LLM evaluation. Moreover, There have been recent advancements in evaluation research that explore mathematical reasoning competency integrating external knowledge, leveraging language diversity for multilingual evaluation , and testing mathematical reasoning on multi-modal setting (Lindström and Abraham, 2022), aiming to judge the broader data reasoning capabilities of large language models (LLMs).</p>
<p>Structured Data Reasoning Structured data reasoning involves the ability to reason and derive insights and answers from structured data sources, such as structured tabular data (Qiao et al., 2022;Li et al., 2023b;Xie et al., 2022).</p>
<p>WikiSQL (Zhong et al., 2017) and WikiTQ (Pasupat and Liang, 2015) provide tables as input and answer questions based on the additional input of questions.</p>
<p>HybridQA (Chen et al., 2020b) and MultiModalQA (Talmor et al., 2021) propose benchmarks for hybrid Structure Reasoning by combining structured table inputs with text (and even other modalities).</p>
<p>Similarly, Multi-WoZ (Budzianowski et al., 2018), KVRET (Eric et al., 2017) and SQA (Iyyer et al., 2017) integrate table data into task-oriented dialogue systems to generate more complex structures and output dialog-related classifications. Unlike traditional QA, FeTaQA (Nan et al., 2021) requires free-form answers instead of extracting answer spans from passages. ToTTo (Parikh et al., 2020) introduces an open-domain English table-to-text dataset for Structured Data Reasoning. Additionally, benchmarks such as Tab-Fact (Chen et al., 2020a) and FEVEROUS (Aly et al., 2021) evaluate whether model statements are consistent with facts mentioned in structured data. In recent years, with a deeper focus on testing models' mathematical abilities, TabMWP  introduces a grade-level dataset of table-based mathematical word problems that require mathematical reasoning using both text and table data.</p>
<p>Reliability</p>
<p>Reliability measures to what extent a human can trust the contents generated by a LLM. It is of vital importance for the deployment and usability of the LLM, and attracts tons of concerns along with the rapid and astonishing development of recent LLMs (Weidinger et al., 2021;Wang et al., 2022d;Ji et al., 2023;Zhuo et al., 2023). Lots of concepts are closely related to reliability under the context of LLM, including but not limited to hallucination, truthfulness, factuality, honesty, calibration, robustness, interpretability (Lee et al., 2018;Belinkov et al., 2020;Evans et al., 2021;Mielke et al., 2022;Lin et al., 2022b). Reliability also overlaps with the safety and generalization of a LLM (Weidinger et al., 2021). In this section, we will give an overview of two most concerned directions: Hallucination, Uncertainty and Calibration.</p>
<p>Hallucination</p>
<p>Hallucination is a term often used to describe LLM's falsehoods, which is the opposite side of truthfulness or factuality OpenAI, 2023;Bubeck et al., 2023). Hallucination is always categorized into intrinsic (close domain) hallucination and extrinsic (open domain) hallucination OpenAI, 2023). Intrinsic hallucination refers to the unfaithfulness of the model output to a given context, while extrinsic hallucination refers to the untruthful contents about the world generated by the model without reference to a given source.</p>
<p>Early research on hallucination mainly focused on the intrinsic hallucination and lots of interesting metrics were proposed to evaluate the intrinsic hallucination level of a PTM . However, Bang et al. (2023) claimed that intrinsic hallucination was barely found after conducting a comprehensive analysis of ChatGPT's responses. Hence for LLM, the extrinsic hallucination is of the greatest concern. To evaluate the extrinsic hallucination potential of a LLM, a common practice is to leverage knowledge-intensive tasks such as Factual Question Answering (Joshi et al., 2017;Zheng et al., 2023) or Knowledge-grounded Dialogue (Dinan et al., 2019b;Das et al., 2022). TruthfulQA (Lin et al., 2022b) is the most popular dataset used to quantify hallucination level of a LLM. This dataset is adversarially constructed to exploit the weakness of LLM, which contained 817 questions that span 38 categories. OpenAI (2023) leveraged real-world data flagged as non-factual to construct an adversarial dataset to test GPT-4's hallucination potential. BIG-bench (Srivastava et al., 2022), a famous benchmark to evaluate LLM's capabilities, also contains many sub-tasks on factual correctness including TruthfulQA. Although most of these tasks are multiple choices or classification in a fact verification (Thorne et al., 2018) manner, they are closely associated with truthfulness and can be regarded as a generalized hallucination evaluation.</p>
<p>Uncertainty and Calibration</p>
<p>A reliable and trustworthy Language model must have the capability to accurately articulate its level of confidence over its response, which requires the model to be aware of its uncertainty. A model that can precisely measure its own uncertainty is sometimes called self-aware, honesty or known-unknown (Kadavath et al., 2022;. In general deep learning applications, calibration concerns about the uncertainty estimation of a classifier. Output probability from a well-calibrated classifier are supposed to be consistent with the empirical accuracy in real world (Vaicenavicius et al., 2019). HELM  treated calibration as one of general metrics and comprehensively evaluated the calibration degree of many prevailing models on multiple choice and classification tasks. (OpenAI, 2023) also showed that GPT-4 before RLHF was well-calibrated on multiple choice tasks, although the decent calibration degree was compromised significantly by post-training.  when it comes to free-form generation, it's a different story. Kuhn et al. (2023) pointed out that semantic nature of language and intractable output space guaranteed the uniqueness of free-form generation. They proposed an algorithm to cluster model outputs and then estimate the model uncertainty. Mielke et al. (2022) claimed that models always express confidence over incorrect answers and proposed the notion of linguistic calibration, which teached models to verbally express uncertainty rather than estimating a probability. Lin et al. (2022a) trained models to directly generate predicted uncertainty probability in natural language.  proposed the SelfAware dataset which contains unanswerable questions and used the accuracy of model rejection as a measure of uncertainty.</p>
<p>Safety</p>
<p>As the LLMs rapidly penetrate into the manufactural and interactive activities of human society, such as LLM-based poem-template generators and chatting robots, the safety concerns for LLMs gain much attention nowadays. The rationales of LLMs are statistics-based, and this inherent stochasticity brings limitations and underlying risks, which deeply affect the real-world deployment of LLMs. Some datasets are proposed to evaluated the safety of LLMs (Table 3), however, the corresponding validity and authority of the safety judgement are inadequate as the current evaluative dimensions are not sufficient (Waseem et al., 2017;Weidinger et al., 2021) and the perception of safety is highly subjective (Kocoń et al., 2021;Weidinger et al., 2021). To this end, based on our survey on relevant papers, we propose a comprehensive perspective on the safety competency of LLMs, ranging from harmful contents to the ethical consideration, to inspire the further developments towards the techniques and evaluations of LLMs safety.</p>
<p>Harmfulness</p>
<p>The harmful contents include the offensive language or others that have the explicit harm towards the specific object, such content that has been widely discussed. However, there is not a unified definition of the constitution of harmful contents, based on our surveys, we conclude the relevant themes into five aspects, including offensiveness, violence, crime, sexual-explicit, and unauthorized expertise. Many researches focus on the language detection for the outputs of LLMs to ensure the harmlessness (Wulczyn et al., 2017;Zampieri et al., 2019;Dinan et al., 2019a), while other techniques are proposed to stimulate LLMs to generate safe outputs directly (Krause et al., 2021;Atwell et al., 2022). For the unauthorized expertise, a general LLM should avoid any unauthorized expertise before the establishment of accountability system , which involves the psychological orientation and any medical advice. Besides, the impact of conversation context on safety gains more attention recently, as a results, detective and generative algorithms base on the context are proposed successively (Dinan et al., 2019a;Baheti et al., 2021;. RealToxicityPrompts (Gehman et al., 2020) is a dataset derived from English web texts, where prompts are automatically truncated from sentences classified as toxicity from a widely-used toxicity classifier. RealToxicityPrompts consists of 100K natural prompts, with average 11.7 tokens in length. BAD (Xu et al., 2021) is a dataset collected by the human-in-the-loop strategy, where crowdworkers are ask to prob harmful model outputs. BAD consist of 5k conversations with around 70k utterances in total, which could be used in both non-adversarially and adversarially testing the model weakness.</p>
<p>Unfairness and Social Bias</p>
<p>Unfairness and social bias present more covertly and widely for LLMs. Following the previous studies, we conclude that social bias is an inherent characteristic of a LLM, which mainly embody in the dis-tribution difference of a LLM in language selection based on different demographic groups. Compared to the social bias, unfairness is the external form, which reflected in the output performance of specific tasks, for example, the African American English (AAE) is frequently mis-classified as the offensive language by some language detector (Lwowski et al., 2022). However, issues of unfairness and social bias are inevitable as they are widely distributed in human languages, and LLMs are required to memorize language as accurately as possible in the training stage (Weidinger et al., 2021). With respect to evaluate this important aspect, CrowS-Pairs (Nangia et al., 2020) is benchmark proposed to evaluating social bias. There are 1508 examples in CrowS-Pairs that involves nine types of social bias, like gender, race, and Nationality. StereoSet (Nadeem et al., 2021) is a dataset that could be used to evaluate social bias level in both word-level and sentence level, which examples are in four domains: race, gender,religion, and profession. For the StereoSet, the bias level is computed by the difference between model generation probabilities of biased and anti-biased sentence.</p>
<p>Others</p>
<p>As current algorithms for model safety based on the human perception, there is still no golden standardized judgement for LLMs to refer to, especially when a judgement is highly various across societies. It is necessary to align LLMs with the morality, ethics, and values of human society. More and more works focus on reifying this abstract concept into textual data recently, for example,  proposal an implicit reasoning frame to explain the underlying harm of the target language. Besides, other works leverage rule-of-thumb (RoT) annotations of texts to support the judgement (Forbes et al., 2020;Ziems et al., 2022). However, current works in this area are neonatal, and we could expect more related works in the future.</p>
<p>Besides, we are also concerned about the privacy and political risks of LLMs. Since the LLMs are trained on vast corpus collected from books, conversations, web texts and so on, the privacy safety of LLMs arouses people's concern. These training texts might contain the private or sensitive information such as personal physical information, home address, etc. Many studies indicate LLMs are brittle under attacks, leaking the sensitive information unintentionally (Carlini et al., 2020;Li et al., 2022). Therefore, it is essential to test the privacy protection ability of a LLM. Moreover, the politics ignorance is also intractable for a LLM. The politics-related risk mainly stems from the composition of the training corpus. Texts in the corpus are derived from different language and social environments (usually the larger the more diversified), and different countries have different political prudence and stance, which brings additional risks to the wide deployment of a LM.</p>
<p>Future Directions</p>
<p>In this section, we outline some other competencies that are important for evaluating LLMs.</p>
<p>Sentiment</p>
<p>It is crucial to equip LLMs with the ability to understand and generate sentiments. As an indispensable factor in human life, sentiments are widely present in daily chats, social media posts, customer reviews, and news articles (Liu, 2015). Through the comprehensive research and high-level summary of the literature related to sentiments, we introduce the sentiment competency of LLMs in two aspects: sentiment understand and sentiment generation.</p>
<p>Sentiment Understanding</p>
<p>Sentiment understand mainly involves the understanding of opinions, sentiments and emotions in the text (Liu, 2015). Representative tasks that reflect this competency include sentiment classification (SC), aspect-based sentiment analysis (ABSA), and multifaceted analysis of subjective texts (MAST). SC aims at assigning pre-defined sentiment classes to given texts. The typical datasets include IMDB (Maas et al., 2011), SST (Socher et al., 2013), Twitter (Rosenthal et al., 2017), Yelp (Zhang et al., 2015). ABSA focuses on identifying the sentiments of specific aspects in a sentence , and the most widely used datasets are the SemEval series (Pontiki et al., 2014;Pontiki et al., 2015;Pontiki et al., 2016). MAST are tasks that involve the finer-grained and broader range of human subjective feelings (emotions (Sailunaz et al., 2018), stance (Küçük and Can, 2021), hate (Schmidt and Wiegand, 2017), irony (Zeng and Li, 2022), offensive (Pradhan et al., 2020), etc.) (Poria et al., 2023). Given that MAST includes a wide range of tasks, the datasets are not listed here in detail. Among them, the commonly used evaluation metrics for the above tasks are accuracy and F1 score (micro or macro). Some preliminary empirical studies  indicate that LLMs can significantly improve performance on these tasks in few-shot learning settings. LLMs have the potential to be a general solution without designing different models for various tasks. Therefore, the sentiment understand competency of different LLMs deserves comprehensive exploration and empirical evaluation. To evaluate the performance of this competency, we can utilize multiple domainspecific datasets or choose the comprehensive benchmark (Srivastava et al., 2022;.</p>
<p>Sentiment Generation</p>
<p>We categorize sentiment generation into two manifestations. One is to generate text that contains sentiments, and the other is to generate text that elicits sentiments. The former requires specifying the desired sentiment, and the latter requires a combination of commonsense knowledge (Speer et al., 2017;Hwang et al., 2021) or theory of mind (Sodian and Kristen, 2010). A classic application scenario is in open-domain dialogue, specifically, emotional dialogue (Zhou et al., 2018), empathetic dialogue (Rashkin et al., 2019), and emotional support conversation (Liu et al., 2021). To measure the quality of the generated text, it is necessary to employ both automatic metrics (such as sentiment accuracy, BLEU (Papineni et al., 2002), perplexity) and human evaluations (human ratings or preference tests). Currently, no work has comprehensively explored this aspect, but it is an essential path towards artificial general intelligence (AGI) (Bubeck et al., 2023).</p>
<p>Planning</p>
<p>Planning is the thinking before the actions take place. Given a specific goal, planning is the process to decide the means to achieve the goal. There're few works (Valmeekam et al., 2023;Valmeekam et al., 2022;Pallagani et al., 2023;) that look at the planning ability of LLMs. Some of them focus on commonsense areas  like wedding or menu planning. Others adopted automated planning problems, formal language translators, and verifiers to automatically evaluate LLMs' competency (Valmeekam et al., 2023). With PDDL 6 represented problem descriptions and the translation of such problems into text and back, LLMs can thus sequence a series of actions to reach the planning goal. Whether the planning purpose is achieved can be easily verified via automatic verifiers. Possessing web-scale knowledge, LLMs have great potential for executing planning tasks or assisting planners.</p>
<p>Code</p>
<p>Coding competency is one of the advanced abilities of LLMs. LLMs with this competency can not only perform program synthesis but also possess the potential of self-evolving. Technically, all of the tasks involved with code like code generation and code understanding need this competency. In oracle manual evaluation, prominent LLMs like ChatGPT are capable of up to 15 ubiquitous software engineering tasks and perform well in most of them (Sridhara et al., 2023). The most explored evaluation task in coding competency would be program synthesis, where program description and function signature are given for its code implementation. One of the most pioneering benchmarks in program synthesis, HUMANEVAL (Chen et al., 2021), consists of 164 pairs of human-generated docstrings and the associated unit tests to test the functional correctness of model generation. However, with the worry of insufficient testing and the imprecise problem description , existing LLM-for-code benchmarks still have lots of room for improvement.</p>
<p>Conclusion</p>
<p>This survey provides a comprehensive review of various literature for the evaluation of LLMs. We aggregate different works with their intended competencies. Some of the competencies(reasoning, knowl-edge) already have holistic evaluation benchmarks, while others(planning, coding) still face disparate challenges. The goal of this paper is to comb the numerous work concerning LLMs' evaluation through the lens of the core competencies test. Lighten the cognitive load for assimilating numerous evaluation works due to the various functions of LLMs. In doing so, we have also identified the challenge faced by each competency, looking forward to alleviating it in the future.</p>
<p>Table 1 :
1Datasets that are used to evaluate the knowledge Competency of LLMs. The Linguistic Knowledge competency is embodied in almost all NLP tasks, researchers usually design specific scenarios to test the linguistic competency of LLMs. Some examples are shown in the upper group of Table 1. BLiMPDataset 
Knowledge Category 
LLM evaluated 
Task Format 
Lang 
BLiMP 
grammatical 
MT-NLG;BLOOM 
Classification 
En 
linguistic mappings 
grammar/syntax 
Gopher;Chinchilla;FLAN-T5;GLM;etc. 
Generation 
En 
minute mysteries qa 
semantic 
Gopher;Chinchilla;FLAN-T5;GLM;etc. Generation/QA 
En 
metaphor boolean 
pragmatic/semantic 
Gopher;Chinchilla;FLAN-T5;GLM;etc. 
Classification 
En 
LexGLUE 
domain 
BLOOM 
Multiple choice 
En 
WikiFact 
world 
BLOOM 
Generation 
En 
TruthfulQA 
world 
GPT-3/InstructGPT/GPT-4 
Generation 
En 
HellaSwag 
commonsense 
GPT-3/InstructGPT/GPT-4 
Generation 
En </p>
<p>Table 2 :
2Datasets that are used to evaluate the reasoning competency of LLMs. * represents a specific reasoning scenario.</p>
<p>Table 3 :
3Datasets used to evaluate the safety competency of LLMs.
https://github.com/HITSCIR-DT-Code/Core-Competency-Test-for-the-Evaluation-of-LLMs 2 https://plato.stanford.edu/entries/epistemology/
https://github.com/google/BIG-bench/blob/main/bigbench/benchmark tasks/linguistic mappings 4 https://github.com/google/BIG-bench/blob/main/bigbench/benchmark tasks/minute mysteries qa 5 https://github.com/google/BIG-bench/tree/main/bigbench/benchmark tasks/metaphor boolean
Planning Domain Definition Language, a formal language used to describe classical planning problems.
AcknowledgementsWe want to thank Yuanxing Liu, Xuesong Wang, Mengzhou Sun, Runze Liu, Yuhang Gou, Shuhan Zhou, Yifan Chen, Ruiyu Xiao, Xinyu Li, Yuchi Zhang, Yang Wang, Jiahang Han, Wenqi Ding, and Xinpeng Liu for their priceless help with the initial dataset investigation process.
Domain knowledge: Evolving themes and emerging concerns. A Patricia, Alexander, Educational psychologist. 271Patricia A Alexander. 1992. Domain knowledge: Evolving themes and emerging concerns. Educational psychol- ogist, 27(1):33-51.</p>
<p>Andreas Vlachos, Christos Christodoulopoulos, Oana Cocarascu, and Arpit Mittal. 2021. The fact extraction and verification over unstructured and structured information (feverous) shared task. Rami Aly, Zhijiang Guo, M Schlichtkrull, James Thorne, Proceedings of the Fourth Workshop on Fact Extraction and VERification (FEVER). the Fourth Workshop on Fact Extraction and VERification (FEVER)Rami Aly, Zhijiang Guo, M. Schlichtkrull, James Thorne, Andreas Vlachos, Christos Christodoulopoulos, Oana Cocarascu, and Arpit Mittal. 2021. The fact extraction and verification over unstructured and structured in- formation (feverous) shared task. Proceedings of the Fourth Workshop on Fact Extraction and VERification (FEVER).</p>
<p>APPDIA: A discourse-aware transformer-based style transfer model for offensive social media conversations. Katherine Atwell, Sabit Hassan, Malihe Alikhani, Proceedings of the 29th International Conference on Computational Linguistics. the 29th International Conference on Computational LinguisticsGyeongju, Republic of KoreaInternational Committee on Computational LinguisticsKatherine Atwell, Sabit Hassan, and Malihe Alikhani. 2022. APPDIA: A discourse-aware transformer-based style transfer model for offensive social media conversations. In Proceedings of the 29th International Conference on Computational Linguistics, pages 6063-6074, Gyeongju, Republic of Korea, October. International Committee on Computational Linguistics.</p>
<p>How to do things with words. Austin John Langshaw, Oxford university press88John Langshaw Austin. 1975. How to do things with words, volume 88. Oxford university press.</p>
<p>Just say no: Analyzing the stance of neural dialogue generation in offensive contexts. Ashutosh Baheti, Maarten Sap, Alan Ritter, Mark Riedl, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. the 2021 Conference on Empirical Methods in Natural Language ProcessingDominican RepublicAssociation for Computational LinguisticsOnline and Punta CanaAshutosh Baheti, Maarten Sap, Alan Ritter, and Mark Riedl. 2021. Just say no: Analyzing the stance of neural dialogue generation in offensive contexts. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 4846-4862, Online and Punta Cana, Dominican Republic, November. Association for Computational Linguistics.</p>
<p>A multitask, multilingual, multimodal evaluation of chatgpt on reasoning. Yejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wenliang Dai, Dan Su, Bryan Wilie, Holy Lovenia, Ziwei Ji, Tiezheng Yu, Willy Chung, Quyet V Do, Yan Xu, Pascale Fung, abs/2302.04023Yejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wenliang Dai, Dan Su, Bryan Wilie, Holy Lovenia, Ziwei Ji, Tiezheng Yu, Willy Chung, Quyet V. Do, Yan Xu, and Pascale Fung. 2023. A multitask, multilingual, multi- modal evaluation of chatgpt on reasoning, hallucination, and interactivity. CoRR, abs/2302.04023.</p>
<p>Interpretability and analysis in neural NLP. Yonatan Belinkov, Sebastian Gehrmann, Ellie Pavlick, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: Tutorial Abstracts. the 58th Annual Meeting of the Association for Computational Linguistics: Tutorial AbstractsAssociation for Computational LinguisticsYonatan Belinkov, Sebastian Gehrmann, and Ellie Pavlick. 2020. Interpretability and analysis in neural NLP. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: Tutorial Abstracts, pages 1-5, Online, July. Association for Computational Linguistics.</p>
<p>Commonsense knowledge reasoning and generation with pre-trained language models: A survey. Prajjwal Bhargava, Vincent Ng, Thirty-Sixth AAAI Conference on Artificial Intelligence, AAAI 2022, Thirty-Fourth Conference on Innovative Applications of Artificial Intelligence, IAAI 2022, The Twelveth Symposium on Educational Advances in Artificial Intelligence, EAAI 2022 Virtual Event. AAAI PressPrajjwal Bhargava and Vincent Ng. 2022. Commonsense knowledge reasoning and generation with pre-trained language models: A survey. In Thirty-Sixth AAAI Conference on Artificial Intelligence, AAAI 2022, Thirty- Fourth Conference on Innovative Applications of Artificial Intelligence, IAAI 2022, The Twelveth Symposium on Educational Advances in Artificial Intelligence, EAAI 2022 Virtual Event, February 22 -March 1, 2022, pages 12317-12325. AAAI Press.</p>
<p>Language models can explain neurons in language models. Steven Bills, Nick Cammarata, Dan Mossing, Henk Tillman, Leo Gao, Gabriel Goh, Ilya Sutskever, Jan Leike, Jeff Wu, William Saunders, Steven Bills, Nick Cammarata, Dan Mossing, Henk Tillman, Leo Gao, Gabriel Goh, Ilya Sutskever, Jan Leike, Jeff Wu, and William Saunders. 2023. Language models can explain neurons in language models. https://openaipublic.blob.core.windows.net/neuron-explainer/paper/index.html.</p>
<p>PIQA: reasoning about physical commonsense in natural language. Yonatan Bisk, Rowan Zellers, Jianfeng Ronan Le Bras, Yejin Gao, Choi, The Thirty-Second Innovative Applications of Artificial Intelligence Conference. New York, NY, USAAAAI Press2020The Tenth AAAI Symposium on Educational Advances in Artificial IntelligenceYonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, and Yejin Choi. 2020. PIQA: reasoning about physical commonsense in natural language. In The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020, pages 7432-7439. AAAI Press.</p>
<p>Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020. Hugo Larochelle, Marc'Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien LinScott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford; NeurIPSvirtualTom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Nee- lakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christo- pher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. In Hugo Larochelle, Marc'Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin, editors, Advances in Neural Information Processing Systems 33: Annual Conference on Neu- ral Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual.</p>
<p>. Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott M Lundberg, Harsha Nori, Hamid Palangi, Marco Túlio Ribeiro, Yi Zhang, Sparks of artificial general intelligence: Early experiments with GPT-4. CoRR, abs/2303.12712Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott M. Lundberg, Harsha Nori, Hamid Palangi, Marco Túlio Ribeiro, and Yi Zhang. 2023. Sparks of artificial general intelligence: Early experiments with GPT-4. CoRR, abs/2303.12712.</p>
<p>Multiwoz -A large-scale multi-domain wizard-of-oz dataset for task-oriented dialogue modelling. Pawel Budzianowski, Tsung-Hsien Wen, Bo-Hsiang Tseng, Iñigo Casanueva, Stefan Ultes, Milica Osman Ramadan, Gasic, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. Ellen Riloff, David Chiang, Julia Hockenmaier, and Jun'ichi Tsujiithe 2018 Conference on Empirical Methods in Natural Language ProcessingBrussels, BelgiumAssociation for Computational LinguisticsPawel Budzianowski, Tsung-Hsien Wen, Bo-Hsiang Tseng, Iñigo Casanueva, Stefan Ultes, Osman Ramadan, and Milica Gasic. 2018. Multiwoz -A large-scale multi-domain wizard-of-oz dataset for task-oriented dialogue modelling. In Ellen Riloff, David Chiang, Julia Hockenmaier, and Jun'ichi Tsujii, editors, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Brussels, Belgium, October 31 - November 4, 2018, pages 5016-5026. Association for Computational Linguistics.</p>
<p>Dawn Song,Úlfar Erlingsson, Alina Oprea, and Colin Raffel. 2020. Extracting training data from large language models. Nicholas Carlini, Florian Tramèr, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss, Katherine Lee, Adam Roberts, Tom B Brown, abs/2012.07805CoRRNicholas Carlini, Florian Tramèr, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss, Katherine Lee, Adam Roberts, Tom B. Brown, Dawn Song,Úlfar Erlingsson, Alina Oprea, and Colin Raffel. 2020. Extracting training data from large language models. CoRR, abs/2012.07805.</p>
<p>The event storyline corpus: A new benchmark for causal and temporal relation extraction. Tommaso Caselli, Piek Vossen, Proceedings of the Events and Stories in the News Workshop@ACL. Tommaso Caselli, Ben Miller, Marieke van Erp, Piek Vossen, Martha Palmer, Eduard H. Hovy, Teruko Mitamura, and David Caswellthe Events and Stories in the News Workshop@ACLVancouver, CanadaAssociation for Computational LinguisticsTommaso Caselli and Piek Vossen. 2017. The event storyline corpus: A new benchmark for causal and tem- poral relation extraction. In Tommaso Caselli, Ben Miller, Marieke van Erp, Piek Vossen, Martha Palmer, Eduard H. Hovy, Teruko Mitamura, and David Caswell, editors, Proceedings of the Events and Stories in the News Workshop@ACL 2017, Vancouver, Canada, August 4, 2017, pages 77-86. Association for Computational Linguistics.</p>
<p>Lexglue: A benchmark dataset for legal language understanding in english. Ilias Chalkidis, Abhik Jana, Dirk Hartung, Michael J Bommarito, I I , Ion Androutsopoulos, Daniel Martin Katz, Nikolaos Aletras, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Smaranda Muresan, Preslav Nakov, and Aline Villavicenciothe 60th Annual Meeting of the Association for Computational LinguisticsDublin, IrelandAssociation for Computational Linguistics1ACL 2022Ilias Chalkidis, Abhik Jana, Dirk Hartung, Michael J. Bommarito II, Ion Androutsopoulos, Daniel Martin Katz, and Nikolaos Aletras. 2022. Lexglue: A benchmark dataset for legal language understanding in english. In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio, editors, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022, pages 4310-4330. Association for Computational Linguistics.</p>
<p>Tianqing Fang, Xin Liu, and Yangqiu Song. 2023. Chatgpt evaluation on sentence level relations: A focus on temporal, causal, and discourse relations. Chunkit Chan, Jiayang Cheng, Weiqi Wang, Yuxin Jiang, abs/2304.14827CoRRChunkit Chan, Jiayang Cheng, Weiqi Wang, Yuxin Jiang, Tianqing Fang, Xin Liu, and Yangqiu Song. 2023. Chatgpt evaluation on sentence level relations: A focus on temporal, causal, and discourse relations. CoRR, abs/2304.14827.</p>
<p>Tabfact: A large-scale dataset for table-based fact verification. Wenhu Chen, Hongmin Wang, Jianshu Chen, Yunkai Zhang, Hong Wang, Shiyang Li, Xiyou Zhou, William Yang Wang, 8th International Conference on Learning Representations. Addis Ababa, Ethiopia2020OpenReview.netWenhu Chen, Hongmin Wang, Jianshu Chen, Yunkai Zhang, Hong Wang, Shiyang Li, Xiyou Zhou, and William Yang Wang. 2020a. Tabfact: A large-scale dataset for table-based fact verification. In 8th Inter- national Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net.</p>
<p>Hybridqa: A dataset of multi-hop question answering over tabular and textual data. Wenhu Chen, Hanwen Zha, Zhiyu Chen, Wenhan Xiong, Hong Wang, William Yang Wang, Findings of the Association for Computational Linguistics: EMNLP 2020. Trevor Cohn, Yulan He, and Yang LiuAssociation for Computational LinguisticsEMNLP 2020 of Findings of ACLWenhu Chen, Hanwen Zha, Zhiyu Chen, Wenhan Xiong, Hong Wang, and William Yang Wang. 2020b. Hybridqa: A dataset of multi-hop question answering over tabular and textual data. In Trevor Cohn, Yulan He, and Yang Liu, editors, Findings of the Association for Computational Linguistics: EMNLP 2020, Online Event, 16-20 November 2020, volume EMNLP 2020 of Findings of ACL, pages 1026-1036. Association for Computational Linguistics.</p>
<p>. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Pondé De Oliveira Pinto, Jared Kaplan, Harrison Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such. Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Joshua Achiam, Vedant Misra, Evan Morikawa, Alec Radfordand Wojciech Zaremba. 2021. Evaluating large language models trained on code. CoRR, abs/2107.03374Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Pondé de Oliveira Pinto, Jared Kaplan, Harrison Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Joshua Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. 2021. Evaluating large language models trained on code. CoRR, abs/2107.03374.</p>
<p>Binding language models in symbolic languages. Zhoujun Cheng, Tianbao Xie, Peng Shi, Chengzu Li, Rahul Nadkarni, Yushi Hu, Caiming Xiong, Dragomir Radev, Mari Ostendorf, Luke Zettlemoyer, Noah A Smith, Tao Yu, The Eleventh International Conference on Learning Representations, ICLR 2023. Kigali, RwandaOpenReview.netZhoujun Cheng, Tianbao Xie, Peng Shi, Chengzu Li, Rahul Nadkarni, Yushi Hu, Caiming Xiong, Dragomir Radev, Mari Ostendorf, Luke Zettlemoyer, Noah A. Smith, and Tao Yu. 2023. Binding language models in symbolic languages. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net.</p>
<p>Transformers as soft reasoners over language. Peter Clark, Oyvind Tafjord, Kyle Richardson, Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence. Christian Bessierethe Twenty-Ninth International Joint Conference on Artificial Intelligence2020Peter Clark, Oyvind Tafjord, and Kyle Richardson. 2020. Transformers as soft reasoners over language. In Chris- tian Bessiere, editor, Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence, IJCAI 2020, pages 3882-3890. ijcai.org. Computational Linguistics</p>
<p>. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. 2021. Training verifiers to solve math word problems. CoRR, abs/2110.14168Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. 2021. Training verifiers to solve math word problems. CoRR, abs/2110.14168.</p>
<p>Calm-bench: A multi-task benchmark for evaluating causality-aware language models. Dhairya Dalal, Paul Buitelaar, Mihael Arcan, Findings of the Association for Computational Linguistics: EACL 2023. Andreas Vlachos and Isabelle AugensteinDubrovnik, CroatiaAssociation for Computational LinguisticsDhairya Dalal, Paul Buitelaar, and Mihael Arcan. 2023. Calm-bench: A multi-task benchmark for evaluating causality-aware language models. In Andreas Vlachos and Isabelle Augenstein, editors, Findings of the As- sociation for Computational Linguistics: EACL 2023, Dubrovnik, Croatia, May 2-6, 2023, pages 296-311. Association for Computational Linguistics.</p>
<p>Diving deep into modes of fact hallucinations in dialogue systems. Souvik Das, Sougata Saha, Rohini K Srihari, Findings of the Association for Computational Linguistics: EMNLP 2022. Yoav Goldberg, Zornitsa Kozareva, and Yue ZhangAbu Dhabi, United Arab EmiratesAssociation for Computational LinguisticsSouvik Das, Sougata Saha, and Rohini K. Srihari. 2022. Diving deep into modes of fact hallucinations in dialogue systems. In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang, editors, Findings of the Association for Compu- tational Linguistics: EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022, pages 684-699. Association for Computational Linguistics.</p>
<p>Automated hate speech detection and the problem of offensive language. Thomas Davidson, Dana Warmsley, Michael W Macy, Ingmar Weber, International Conference on Web and Social Media. Thomas Davidson, Dana Warmsley, Michael W. Macy, and Ingmar Weber. 2017. Automated hate speech detection and the problem of offensive language. In International Conference on Web and Social Media.</p>
<p>Representations of commonsense knowledge. Ernest Davis, Morgan KaufmannErnest Davis. 2014. Representations of commonsense knowledge. Morgan Kaufmann.</p>
<p>Extensive reading in the second language classroom. Julian Richard R Day, Willy A Bamford, George M Renandya, Vivienne Wai-Sze Jacobs, Yu, RELC Journal. 292Richard R Day, Julian Bamford, Willy A Renandya, George M Jacobs, and Vivienne Wai-Sze Yu. 1998. Extensive reading in the second language classroom. RELC Journal, 29(2):187-191.</p>
<p>ERASER: A benchmark to evaluate rationalized NLP models. Jay Deyoung, Sarthak Jain, Nazneen Fatema Rajani, Eric Lehman, Caiming Xiong, Richard Socher, Byron C Wallace, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel R. Tetreaultthe 58th Annual Meeting of the Association for Computational LinguisticsOnline2020Association for Computational LinguisticsJay DeYoung, Sarthak Jain, Nazneen Fatema Rajani, Eric Lehman, Caiming Xiong, Richard Socher, and Byron C. Wallace. 2020. ERASER: A benchmark to evaluate rationalized NLP models. In Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel R. Tetreault, editors, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online, July 5-10, 2020, pages 4443-4458. Association for Computa- tional Linguistics.</p>
<p>Build it break it fix it for dialogue safety: Robustness from adversarial human attack. Emily Dinan, Samuel Humeau, Bharath Chintagunta, Jason Weston, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)Hong Kong, ChinaAssociation for Computational LinguisticsEmily Dinan, Samuel Humeau, Bharath Chintagunta, and Jason Weston. 2019a. Build it break it fix it for dia- logue safety: Robustness from adversarial human attack. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Pro- cessing (EMNLP-IJCNLP), pages 4537-4546, Hong Kong, China, November. Association for Computational Linguistics.</p>
<p>Wizard of wikipedia: Knowledge-powered conversational agents. Emily Dinan, Stephen Roller, Kurt Shuster, Angela Fan, Michael Auli, Jason Weston, ICLR 20197th International Conference on Learning Representations. New Orleans, LA, USAOpenReview.netEmily Dinan, Stephen Roller, Kurt Shuster, Angela Fan, Michael Auli, and Jason Weston. 2019b. Wizard of wikipedia: Knowledge-powered conversational agents. In 7th International Conference on Learning Represen- tations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net.</p>
<p>SafetyKit: First aid for measuring safety in open-domain conversational systems. Emily Dinan, A Gavin Abercrombie, Shannon Bergman, Dirk Spruit, Y-Lan Hovy, Verena Boureau, Rieser, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. the 60th Annual Meeting of the Association for Computational LinguisticsDublin, Ireland, MayAssociation for Computational Linguistics1Emily Dinan, Gavin Abercrombie, A. Bergman, Shannon Spruit, Dirk Hovy, Y-Lan Boureau, and Verena Rieser. 2022. SafetyKit: First aid for measuring safety in open-domain conversational systems. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 4113-4133, Dublin, Ireland, May. Association for Computational Linguistics.</p>
<p>. Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, Lei Li, and Zhifang Sui. 2023. A survey for in-context learning. CoRR, abs/2301.00234Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, Lei Li, and Zhifang Sui. 2023. A survey for in-context learning. CoRR, abs/2301.00234.</p>
<ol>
<li>e-care: a new dataset for exploring explainable causal reasoning. Li Du, Xiao Ding, Kai Xiong, Ting Liu, Bing Qin, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Smaranda Muresan, Preslav Nakov, and Aline Villavicenciothe 60th Annual Meeting of the Association for Computational LinguisticsDublin, IrelandAssociation for Computational Linguistics1ACL 2022Li Du, Xiao Ding, Kai Xiong, Ting Liu, and Bing Qin. 2022. e-care: a new dataset for exploring explainable causal reasoning. In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio, editors, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022, pages 432-446. Association for Computational Linguistics.</li>
</ol>
<p>To what extent is general intelligence relevant to causal reasoning? a developmental study. Selma Dündar-Coecke, Frontiers in Psychology. 13Selma Dündar-Coecke. 2022. To what extent is general intelligence relevant to causal reasoning? a developmental study. Frontiers in Psychology, 13.</p>
<p>Key-value retrieval networks for task-oriented dialogue. Mihail Eric, Lakshmi Krishnan, François Charette, Christopher D Manning, Proceedings of the 18th Annual SIGdial Meeting on Discourse and Dialogue. Kristiina Jokinen, Manfred Stede, David DeVault, and Annie Louisthe 18th Annual SIGdial Meeting on Discourse and DialogueSaarbrücken, GermanyAssociation for Computational LinguisticsMihail Eric, Lakshmi Krishnan, François Charette, and Christopher D. Manning. 2017. Key-value retrieval net- works for task-oriented dialogue. In Kristiina Jokinen, Manfred Stede, David DeVault, and Annie Louis, editors, Proceedings of the 18th Annual SIGdial Meeting on Discourse and Dialogue, Saarbrücken, Germany, August 15-17, 2017, pages 37-49. Association for Computational Linguistics.</p>
<p>Truthful AI: developing and governing AI that does not lie. Owain Evans, Owen Cotton-Barratt, Lukas Finnveden, Adam Bales, Avital Balwit, Peter Wills, Luca Righetti, William Saunders, abs/2110.06674CoRROwain Evans, Owen Cotton-Barratt, Lukas Finnveden, Adam Bales, Avital Balwit, Peter Wills, Luca Righetti, and William Saunders. 2021. Truthful AI: developing and governing AI that does not lie. CoRR, abs/2110.06674.</p>
<p>Logic and human reasoning: an assessment of the deduction paradigm. Jonathan Evans, Psychological bulletin. 128Jonathan Evans. 2002. Logic and human reasoning: an assessment of the deduction paradigm. Psychological bulletin, 128 6:978-96.</p>
<p>Computational Linguistics. Computational Linguistics</p>
<p>Social chemistry 101: Learning to reason about social and moral norms. Maxwell Forbes, Jena D Hwang, Vered Shwartz, Maarten Sap, Yejin Choi, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)OnlineAssociation for Computational LinguisticsMaxwell Forbes, Jena D. Hwang, Vered Shwartz, Maarten Sap, and Yejin Choi. 2020. Social chemistry 101: Learning to reason about social and moral norms. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 653-670, Online, November. Association for Computational Linguistics.</p>
<p>An Introduction to Language (w/MLA9E Updates). Victoria Fromkin, Robert Rodman, Nina Hyams, Cengage LearningVictoria Fromkin, Robert Rodman, and Nina Hyams. 2018. An Introduction to Language (w/MLA9E Updates). Cengage Learning.</p>
<p>. Leo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony Dipofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Kyle Mcdonell, Niklas Muennighoff, Jason Phang, Laria Reynolds, Eric Tang, Anish Thite, Ben Wang, Kevin Wangand Andy Zou. 2021. A framework for few-shot language model evaluationLeo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Kyle McDonell, Niklas Muennighoff, Jason Phang, Laria Reynolds, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. 2021. A framework for few-shot language model evaluation, September.</p>
<p>Is chatgpt a good causal reasoner? A comprehensive evaluation. Jinglong Gao, Xiao Ding, Bing Qin, Ting Liu, abs/2305.07375CoRRJinglong Gao, Xiao Ding, Bing Qin, and Ting Liu. 2023. Is chatgpt a good causal reasoner? A comprehensive evaluation. CoRR, abs/2305.07375.</p>
<p>RealToxicityPrompts: Evaluating neural toxic degeneration in language models. Suchin Samuel Gehman, Maarten Gururangan, Yejin Sap, Noah A Choi, Smith, Findings of the Association for Computational Linguistics: EMNLP 2020. OnlineAssociation for Computational LinguisticsSamuel Gehman, Suchin Gururangan, Maarten Sap, Yejin Choi, and Noah A. Smith. 2020. RealToxicityPrompts: Evaluating neural toxic degeneration in language models. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 3356-3369, Online, November. Association for Computational Linguistics.</p>
<p>Did aristotle use a laptop? A question answering benchmark with implicit reasoning strategies. Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, Dan Roth, Jonathan Berant, Trans. Assoc. Comput. Linguistics. 9Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, Dan Roth, and Jonathan Berant. 2021. Did aristotle use a laptop? A question answering benchmark with implicit reasoning strategies. Trans. Assoc. Comput. Linguistics, 9:346-361.</p>
<p>Semeval-2012 task 7: Choice of plausible alternatives: An evaluation of commonsense causal reasoning. Andrew S Gordon, Zornitsa Kozareva, Melissa Roemmele, Proceedings of the 6th International Workshop on Semantic Evaluation, SemEval@NAACL-HLT 2012. Eneko Agirre, Johan Bos, and Mona T. Diabthe 6th International Workshop on Semantic Evaluation, SemEval@NAACL-HLT 2012Montréal, CanadaThe Association for Computer LinguisticsAndrew S. Gordon, Zornitsa Kozareva, and Melissa Roemmele. 2012. Semeval-2012 task 7: Choice of plausible alternatives: An evaluation of commonsense causal reasoning. In Eneko Agirre, Johan Bos, and Mona T. Diab, editors, Proceedings of the 6th International Workshop on Semantic Evaluation, SemEval@NAACL-HLT 2012, Montréal, Canada, June 7-8, 2012, pages 394-398. The Association for Computer Linguistics.</p>
<p>Does BERT learn as humans perceive? understanding linguistic styles through lexica. Dongyeop Shirley Anugrah Hayati, Lyle Kang, Ungar, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yihthe 2021 Conference on Empirical Methods in Natural Language ProcessingDominican RepublicAssociation for Computational Linguistics2021Virtual Event / Punta CanaShirley Anugrah Hayati, Dongyeop Kang, and Lyle Ungar. 2021. Does BERT learn as humans perceive? un- derstanding linguistic styles through lexica. In Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih, editors, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic, 7-11 November, 2021, pages 6323-6331. Association for Computational Linguistics.</p>
<p>The abduction of sherlock holmes: A dataset for visual abductive reasoning. Jack Hessel, Jena D Hwang, Jae Sung Park, Rowan Zellers, Chandra Bhagavatula, Anna Rohrbach, Kate Saenko, Yejin Choi, Computer Vision -ECCV 2022 -17th European Conference. Shai Avidan, Gabriel J. Brostow, Moustapha Cissé, Giovanni Maria Farinella, and Tal HassnerTel Aviv, IsraelSpringer13696Proceedings, Part XXXVIJack Hessel, Jena D. Hwang, Jae Sung Park, Rowan Zellers, Chandra Bhagavatula, Anna Rohrbach, Kate Saenko, and Yejin Choi. 2022. The abduction of sherlock holmes: A dataset for visual abductive reasoning. In Shai Avidan, Gabriel J. Brostow, Moustapha Cissé, Giovanni Maria Farinella, and Tal Hassner, editors, Computer Vision -ECCV 2022 -17th European Conference, Tel Aviv, Israel, October 23-27, 2022, Proceedings, Part XXXVI, volume 13696 of Lecture Notes in Computer Science, pages 558-575. Springer.</p>
<p>The meanings of competency. Terrence Hoffmann, Journal of european industrial training. 236Terrence Hoffmann. 1999. The meanings of competency. Journal of european industrial training, 23(6):275-286.</p>
<p>Learning to solve arithmetic word problems with verb categorization. Mohammad Javad Hosseini, Hannaneh Hajishirzi, Oren Etzioni, Nate Kushman, Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing. Alessandro Moschitti, Bo Pang, and Walter Daelemansthe 2014 Conference on Empirical Methods in Natural Language ProcessingDoha, QatarACLA meeting of SIGDAT, a Special Interest Group of the ACLMohammad Javad Hosseini, Hannaneh Hajishirzi, Oren Etzioni, and Nate Kushman. 2014. Learning to solve arithmetic word problems with verb categorization. In Alessandro Moschitti, Bo Pang, and Walter Daelemans, editors, Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, EMNLP 2014, October 25-29, 2014, Doha, Qatar, A meeting of SIGDAT, a Special Interest Group of the ACL, pages 523-533. ACL.</p>
<p>Language models as zero-shot planners: Extracting actionable knowledge for embodied agents. Wenlong Huang, Pieter Abbeel, Deepak Pathak, Igor Mordatch, PMLRInternational Conference on Machine Learning, ICML 2022. Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvári, Gang Niu, and Sivan SabatoBaltimore, Maryland, USA162Wenlong Huang, Pieter Abbeel, Deepak Pathak, and Igor Mordatch. 2022. Language models as zero-shot plan- ners: Extracting actionable knowledge for embodied agents. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvári, Gang Niu, and Sivan Sabato, editors, International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA, volume 162 of Proceedings of Machine Learning Research, pages 9118-9147. PMLR.</p>
<p>Maosong Sun, and Junxian He. 2023. C-eval: A multi-level multi-discipline chinese evaluation suite for foundation models. Yuzhen Huang, Yuzhuo Bai, Zhihao Zhu, Junlei Zhang, Jinghan Zhang, Tangjun Su, Junteng Liu, Chuancheng Lv, Yikai Zhang, Jiayi Lei, Yao Fu, abs/2305.08322CoRRYuzhen Huang, Yuzhuo Bai, Zhihao Zhu, Junlei Zhang, Jinghan Zhang, Tangjun Su, Junteng Liu, Chuancheng Lv, Yikai Zhang, Jiayi Lei, Yao Fu, Maosong Sun, and Junxian He. 2023. C-eval: A multi-level multi-discipline chinese evaluation suite for foundation models. CoRR, abs/2305.08322.</p>
<ol>
<li>(comet-) atomic 2020: On symbolic and neural commonsense knowledge graphs. Jena D Hwang, Chandra Bhagavatula, Jeff Ronan Le Bras, Keisuke Da, Antoine Sakaguchi, Yejin Bosselut, Choi, Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021, Thirty-Third Conference on Innovative Applications of Artificial Intelligence, IAAI 2021, The Eleventh Symposium on Educational Advances in Artificial Intelligence, EAAI 2021, Virtual Event. AAAI PressJena D. Hwang, Chandra Bhagavatula, Ronan Le Bras, Jeff Da, Keisuke Sakaguchi, Antoine Bosselut, and Yejin Choi. 2021. (comet-) atomic 2020: On symbolic and neural commonsense knowledge graphs. In Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021, Thirty-Third Conference on Innovative Applications of Artificial Intelligence, IAAI 2021, The Eleventh Symposium on Educational Advances in Artificial Intelligence, EAAI 2021, Virtual Event, February 2-9, 2021, pages 6384-6392. AAAI Press.</li>
</ol>
<p>Search-based neural structured learning for sequential question answering. Mohit Iyyer, Yih Wen-Tau, Ming-Wei Chang, Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics. Regina Barzilay and Min-Yen Kanthe 55th Annual Meeting of the Association for Computational LinguisticsVancouver, CanadaAssociation for Computational Linguistics1Computational LinguisticsMohit Iyyer, Wen-tau Yih, and Ming-Wei Chang. 2017. Search-based neural structured learning for sequential question answering. In Regina Barzilay and Min-Yen Kan, editors, Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, ACL 2017, Vancouver, Canada, July 30 -August 4, Volume 1: Long Papers, pages 1821-1831. Association for Computational Linguistics. Computational Linguistics</p>
<p>Yejin Bang, Andrea Madotto, and Pascale Fung. 2023. Survey of hallucination in natural language generation. Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, ACM Comput. Surv. 551238Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Yejin Bang, Andrea Madotto, and Pascale Fung. 2023. Survey of hallucination in natural language generation. ACM Comput. Surv., 55(12):248:1-248:38.</p>
<p>Structgpt: A general framework for large language model to reason over structured data. Jinhao Jiang, Kun Zhou, Zican Dong, Keming Ye, Wayne Xin Zhao, Ji-Rong Wen, abs/2305.09645CoRRJinhao Jiang, Kun Zhou, Zican Dong, Keming Ye, Wayne Xin Zhao, and Ji-Rong Wen. 2023. Structgpt: A general framework for large language model to reason over structured data. CoRR, abs/2305.09645.</p>
<p>Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. Mandar Joshi, Eunsol Choi, Daniel S Weld, Luke Zettlemoyer, Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics. Regina Barzilay and Min-Yen Kanthe 55th Annual Meeting of the Association for Computational LinguisticsVancouver, CanadaAssociation for Computational Linguistics1Mandar Joshi, Eunsol Choi, Daniel S. Weld, and Luke Zettlemoyer. 2017. Triviaqa: A large scale distantly super- vised challenge dataset for reading comprehension. In Regina Barzilay and Min-Yen Kan, editors, Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, ACL 2017, Vancouver, Canada, July 30 -August 4, Volume 1: Long Papers, pages 1601-1611. Association for Computational Linguistics.</p>
<p>. Saurav Kadavath, Tom Conerly, Amanda Askell, Tom Henighan, Dawn Drain, Ethan Perez, Nicholas Schiefer, Zac Hatfield-Dodds, Nova Dassarma, Eli Tran-Johnson, Scott Johnston, Sheer El Showk, Andy Jones, Nelson Elhage, Tristan Hume, Anna Chen, Yuntao Bai, Sam Bowman, Stanislav Fort, Deep Ganguli, Danny Hernandez, Josh Jacobson, Jackson Kernion, Shauna Kravec, Liane Lovitt, Kamal Ndousse, Catherine Olsson, Sam Ringer, Dario Amodei, Tom Brown, Jack Clark, Nicholas Joseph, Ben Mann, Sam McCandlish, Chris Olahand Jared Kaplan. 2022. Language models (mostly) know what they know. CoRR, abs/2207.05221Saurav Kadavath, Tom Conerly, Amanda Askell, Tom Henighan, Dawn Drain, Ethan Perez, Nicholas Schiefer, Zac Hatfield-Dodds, Nova DasSarma, Eli Tran-Johnson, Scott Johnston, Sheer El Showk, Andy Jones, Nelson Elhage, Tristan Hume, Anna Chen, Yuntao Bai, Sam Bowman, Stanislav Fort, Deep Ganguli, Danny Hernandez, Josh Jacobson, Jackson Kernion, Shauna Kravec, Liane Lovitt, Kamal Ndousse, Catherine Olsson, Sam Ringer, Dario Amodei, Tom Brown, Jack Clark, Nicholas Joseph, Ben Mann, Sam McCandlish, Chris Olah, and Jared Kaplan. 2022. Language models (mostly) know what they know. CoRR, abs/2207.05221.</p>
<p>Abduction and argumentation for explainable machine learning: A position survey. C Antonis, Loizos Kakas, Michael, abs/2010.12896Antonis C. Kakas and Loizos Michael. 2020. Abduction and argumentation for explainable machine learning: A position survey. CoRR, abs/2010.12896.</p>
<p>. Jared Kaplan, Sam Mccandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. 2020. Scaling laws for neural language models. CoRR, abs/2001.08361Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. 2020. Scaling laws for neural language models. CoRR, abs/2001.08361.</p>
<p>Tomasz Kajdanowicz, and Przemysław Kazienko. 2021. Offensive, aggressive, and hate speech analysis: From data-centric to human-centered approach. Jan Kocoń, Alicja Figas, Marcin Gruza, Daria Puchalska, Information Processing &amp; Management. 585102643Jan Kocoń, Alicja Figas, Marcin Gruza, Daria Puchalska, Tomasz Kajdanowicz, and Przemysław Kazienko. 2021. Offensive, aggressive, and hate speech analysis: From data-centric to human-centered approach. Information Processing &amp; Management, 58(5):102643.</p>
<p>Yutaka Matsuo, and Yusuke Iwasawa. 2022. Large language models are zero-shot reasoners. Takeshi Kojima, Shane Shixiang, Machel Gu, Reid, NeurIPS. Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. 2022. Large language models are zero-shot reasoners. In NeurIPS.</p>
<p>GeDi: Generative discriminator guided sequence generation. Ben Krause, Akhilesh Deepak Gotmare, Bryan Mccann, Nitish Shirish Keskar, Shafiq Joty, Richard Socher, Nazneen Fatema Rajani, Findings of the Association for Computational Linguistics: EMNLP 2021. Punta Cana, Dominican RepublicAssociation for Computational LinguisticsBen Krause, Akhilesh Deepak Gotmare, Bryan McCann, Nitish Shirish Keskar, Shafiq Joty, Richard Socher, and Nazneen Fatema Rajani. 2021. GeDi: Generative discriminator guided sequence generation. In Findings of the Association for Computational Linguistics: EMNLP 2021, pages 4929-4952, Punta Cana, Dominican Republic, November. Association for Computational Linguistics.</p>
<p>Stance detection: A survey. Dilek Küçük, Fazli Can, ACM Comput. Surv. 53137Dilek Küçük and Fazli Can. 2021. Stance detection: A survey. ACM Comput. Surv., 53(1):12:1-12:37.</p>
<p>Semantic uncertainty: Linguistic invariances for uncertainty estimation in natural language generation. Lorenz Kuhn, Yarin Gal, Sebastian Farquhar, The Eleventh International Conference on Learning Representations, ICLR 2023. Kigali, RwandaOpenReview.netLorenz Kuhn, Yarin Gal, and Sebastian Farquhar. 2023. Semantic uncertainty: Linguistic invariances for un- certainty estimation in natural language generation. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net.</p>
<p>Hallucinations in neural machine translation. Katherine Lee, Orhan Firat, Ashish Agarwal, Clara Fannjiang, David Sussillo, Katherine Lee, Orhan Firat, Ashish Agarwal, Clara Fannjiang, and David Sussillo. 2018. Hallucinations in neural machine translation.</p>
<p>You don't know my favorite color: Preventing dialogue representations from revealing speakers' private personas. Haoran Li, Yangqiu Song, Lixin Fan, Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesSeattle, United StatesAssociation for Computational LinguisticsHaoran Li, Yangqiu Song, and Lixin Fan. 2022. You don't know my favorite color: Preventing dialogue represen- tations from revealing speakers' private personas. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 5858-5870, Seattle, United States, July. Association for Computational Linguistics.</p>
<p>CMMLU: measuring massive multitask language understanding in chinese. Haonan Li, Yixuan Zhang, Fajri Koto, Yifei Yang, Hai Zhao, Yeyun Gong, Nan Duan, Timothy Baldwin, abs/2306.09212CoRRHaonan Li, Yixuan Zhang, Fajri Koto, Yifei Yang, Hai Zhao, Yeyun Gong, Nan Duan, and Timothy Baldwin. 2023a. CMMLU: measuring massive multitask language understanding in chinese. CoRR, abs/2306.09212.</p>
<p>Chain of knowledge: A framework for grounding large language models with structured knowledge bases. Xingxuan Li, Ruochen Zhao, Yew Ken Chia, Bosheng Ding, Lidong Bing, R Shafiq, Soujanya Joty, Poria, abs/2305.13269CoRRXingxuan Li, Ruochen Zhao, Yew Ken Chia, Bosheng Ding, Lidong Bing, Shafiq R. Joty, and Soujanya Poria. 2023b. Chain of knowledge: A framework for grounding large language models with structured knowledge bases. CoRR, abs/2305.13269.</p>
<p>. Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, Benjamin Newman, Binhang Yuan, Bobby Yan, Ce Zhang, Christian Cosgrove, Christopher D Manning, Christopher Ré, Diana Acosta-Navas, Drew A Hudson, Eric Zelikman, Esin Durmus, Faisal Ladhak, Frieda Rong, Hongyu Ren, Huaxiu Yao, Jue Wang, Keshav Santhanam, Laurel J Orr, Lucia Zheng, Mert Yüksekgönül, Mirac Suzgun, Nathan Kim, Neel Guha, Niladri S Chatterji, Omar Khattab, Peter Henderson, Qian Huang, Ryan Chi, Sang Michael Xie, Shibani Santurkar, Surya Ganguli, Tatsunori Hashimoto, Thomas Icard, Tianyi Zhang, Vishrav Chaudhary, William Wang, Xuechen Li, Yifan Mai, Yuhui Zhang, and Yuta Koreeda. 2022. Holistic evaluation of language models. CoRR, abs/2211.09110. Computational LinguisticsPercy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, Benjamin Newman, Binhang Yuan, Bobby Yan, Ce Zhang, Christian Cosgrove, Christopher D. Manning, Christopher Ré, Diana Acosta-Navas, Drew A. Hudson, Eric Zelikman, Esin Durmus, Faisal Ladhak, Frieda Rong, Hongyu Ren, Huaxiu Yao, Jue Wang, Keshav Santhanam, Laurel J. Orr, Lucia Zheng, Mert Yüksekgönül, Mirac Suzgun, Nathan Kim, Neel Guha, Niladri S. Chatterji, Omar Khattab, Peter Henderson, Qian Huang, Ryan Chi, Sang Michael Xie, Shibani Santurkar, Surya Ganguli, Tatsunori Hashimoto, Thomas Icard, Tianyi Zhang, Vishrav Chaudhary, William Wang, Xuechen Li, Yifan Mai, Yuhui Zhang, and Yuta Koreeda. 2022. Holistic evaluation of language models. CoRR, abs/2211.09110. Computational Linguistics</p>
<p>Teaching models to express their uncertainty in words. Stephanie Lin, Jacob Hilton, Owain Evans, Trans. Mach. Learn. Res. Stephanie Lin, Jacob Hilton, and Owain Evans. 2022a. Teaching models to express their uncertainty in words. Trans. Mach. Learn. Res., 2022.</p>
<p>Truthfulqa: Measuring how models mimic human falsehoods. Stephanie Lin, Jacob Hilton, Owain Evans, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Smaranda Muresan, Preslav Nakov, and Aline Villavicenciothe 60th Annual Meeting of the Association for Computational LinguisticsDublin, IrelandAssociation for Computational Linguistics1ACL 2022Stephanie Lin, Jacob Hilton, and Owain Evans. 2022b. Truthfulqa: Measuring how models mimic human false- hoods. In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio, editors, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022, pages 3214-3252. Association for Computational Linguistics.</p>
<p>Clevr-math: A dataset for compositional language, visual and mathematical reasoning. Proceedings of the 16th International Workshop on Neural-Symbolic Learning and Reasoning as part of the 2nd International Joint Conference on Learning &amp; Reasoning (IJCLR 2022). Artur S. d'Avila Garcez and Ernesto Jiménez-Ruizthe 16th International Workshop on Neural-Symbolic Learning and Reasoning as part of the 2nd International Joint Conference on Learning &amp; Reasoning (IJCLR 2022)Cumberland Lodge, Windsor Great Park, UK3212CEUR-WS.orgAdam Dahlgren Lindström and Savitha Sam Abraham. 2022. Clevr-math: A dataset for compositional language, visual and mathematical reasoning. In Artur S. d'Avila Garcez and Ernesto Jiménez-Ruiz, editors, Proceedings of the 16th International Workshop on Neural-Symbolic Learning and Reasoning as part of the 2nd Interna- tional Joint Conference on Learning &amp; Reasoning (IJCLR 2022), Cumberland Lodge, Windsor Great Park, UK, September 28-30, 2022, volume 3212 of CEUR Workshop Proceedings, pages 155-170. CEUR-WS.org.</p>
<p>Towards emotional support dialog systems. Siyang Liu, Chujie Zheng, Orianna Demasi, Sahand Sabour, Yu Li, Zhou Yu, Yong Jiang, Minlie Huang, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021. Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Naviglithe 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021Association for Computational Linguistics1Virtual EventSiyang Liu, Chujie Zheng, Orianna Demasi, Sahand Sabour, Yu Li, Zhou Yu, Yong Jiang, and Minlie Huang. 2021. Towards emotional support dialog systems. In Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli, editors, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021, (Volume 1: Long Papers), Virtual Event, August 1-6, 2021, pages 3469-3483. Association for Computational Linguistics.</p>
<p>Is your code generated by chatgpt really correct? rigorous evaluation of large language models for code generation. Jiawei Liu, Chunqiu Steven Xia, Yuyao Wang, Lingming Zhang, abs/2305.01210CoRRJiawei Liu, Chunqiu Steven Xia, Yuyao Wang, and Lingming Zhang. 2023. Is your code generated by chatgpt really correct? rigorous evaluation of large language models for code generation. CoRR, abs/2305.01210.</p>
<p>Bing Liu, Sentiment Analysis -Mining Opinions, Sentiments, and Emotions. Cambridge University PressBing Liu. 2015. Sentiment Analysis -Mining Opinions, Sentiments, and Emotions. Cambridge University Press.</p>
<p>Dynamic prompt learning via policy gradient for semi-structured mathematical reasoning. Pan Lu, Liang Qiu, Kai-Wei Chang, Ying Nian Wu, Song-Chun Zhu, Tanmay Rajpurohit, Peter Clark, Ashwin Kalyan, The Eleventh International Conference on Learning Representations, ICLR 2023. Kigali, RwandaOpenReview.netPan Lu, Liang Qiu, Kai-Wei Chang, Ying Nian Wu, Song-Chun Zhu, Tanmay Rajpurohit, Peter Clark, and Ashwin Kalyan. 2023. Dynamic prompt learning via policy gradient for semi-structured mathematical reasoning. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net.</p>
<p>Measuring geographic performance disparities of offensive language classifiers. Brandon Lwowski, Paul Rad, Anthony Rios, Proceedings of the 29th International Conference on Computational Linguistics. the 29th International Conference on Computational LinguisticsGyeongju, Republic of KoreaInternational Committee on Computational LinguisticsBrandon Lwowski, Paul Rad, and Anthony Rios. 2022. Measuring geographic performance disparities of offensive language classifiers. In Proceedings of the 29th International Conference on Computational Linguistics, pages 6600-6616, Gyeongju, Republic of Korea, October. International Committee on Computational Linguistics.</p>
<p>Faithful chain-of-thought reasoning. Qing Lyu, Shreya Havaldar, Adam Stein, Li Zhang, Delip Rao, Eric Wong, Marianna Apidianaki, Chris Callison-Burch, abs/2301.13379CoRRQing Lyu, Shreya Havaldar, Adam Stein, Li Zhang, Delip Rao, Eric Wong, Marianna Apidianaki, and Chris Callison-Burch. 2023. Faithful chain-of-thought reasoning. CoRR, abs/2301.13379.</p>
<p>Learning word vectors for sentiment analysis. Andrew L Maas, Raymond E Daly, Peter T Pham, Dan Huang, Andrew Y Ng, Christopher Potts, The 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, Proceedings of the Conference. Dekang Lin, Yuji Matsumoto, and Rada MihalceaPortland, Oregon, USAThe Association for Computer LinguisticsAndrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts. 2011. Learning word vectors for sentiment analysis. In Dekang Lin, Yuji Matsumoto, and Rada Mihalcea, editors, The 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, Proceedings of the Conference, 19-24 June, 2011, Portland, Oregon, USA, pages 142-150. The Association for Computer Linguistics.</p>
<p>Faster sorting algorithms discovered using deep reinforcement learning. Andrea Daniel Jaymin Mankowitz, Anton Michi, Marco Zhernov, Marco Gelmi, Cosmin Selvi, Edouard Paduraru, Shariq Leurent, Jean-Baptiste Iqbal, Alex Lespiau, Thomas Ahern, Kevin Köppe, Stephen Millikin, Sophie Gaffney, Jackson Elster, Chris Broshear, Kieran Gamble, Robert Milan, Tung, Nature. Minjae Hwang, taylan. cemgil, Mohammadamin Barekatain, Yujia Li, Amol Mandhane, Thomas Hubert, Julian Schrittwieser, Demis Hassabis, Pushmeet Kohli, Martin A. Riedmiller, Oriol Vinyals, and David Silver618Daniel Jaymin Mankowitz, Andrea Michi, Anton Zhernov, Marco Gelmi, Marco Selvi, Cosmin Paduraru, Edouard Leurent, Shariq Iqbal, Jean-Baptiste Lespiau, Alex Ahern, Thomas Köppe, Kevin Millikin, Stephen Gaffney, Sophie Elster, Jackson Broshear, Chris Gamble, Kieran Milan, Robert Tung, Minjae Hwang, taylan. cemgil, Mohammadamin Barekatain, Yujia Li, Amol Mandhane, Thomas Hubert, Julian Schrittwieser, Demis Hass- abis, Pushmeet Kohli, Martin A. Riedmiller, Oriol Vinyals, and David Silver. 2023. Faster sorting algorithms discovered using deep reinforcement learning. Nature, 618:257 -263.</p>
<p>Hatexplain: A benchmark dataset for explainable hate speech detection. Binny Mathew, Punyajoy Saha, Chris Seid Muhie Yimam, Pawan Biemann, Animesh Goyal, Mukherjee, Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021, Thirty-Third Conference on Innovative Applications of Artificial Intelligence, IAAI 2021, The Eleventh Symposium on Educational Advances in Artificial Intelligence, EAAI 2021, Virtual Event. AAAI PressBinny Mathew, Punyajoy Saha, Seid Muhie Yimam, Chris Biemann, Pawan Goyal, and Animesh Mukherjee. 2021. Hatexplain: A benchmark dataset for explainable hate speech detection. In Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021, Thirty-Third Conference on Innovative Applications of Artificial Intelligence, IAAI 2021, The Eleventh Symposium on Educational Advances in Artificial Intelligence, EAAI 2021, Virtual Event, February 2-9, 2021, pages 14867-14875. AAAI Press.</p>
<p>. Grégoire Mialon, Roberto Dessì, Maria Lomeli, Christoforos Nalmpantis, Ramakanth Pasunuru, Roberta Raileanu, Timo Baptiste Rozière, Jane Schick, Asli Dwivedi-Yu, Edouard Celikyilmaz, Yann Grave, Lecun, and Thomas Scialom. 2023. Augmented language models: a survey. CoRR, abs/2302.07842Grégoire Mialon, Roberto Dessì, Maria Lomeli, Christoforos Nalmpantis, Ramakanth Pasunuru, Roberta Raileanu, Baptiste Rozière, Timo Schick, Jane Dwivedi-Yu, Asli Celikyilmaz, Edouard Grave, Yann LeCun, and Thomas Scialom. 2023. Augmented language models: a survey. CoRR, abs/2302.07842.</p>
<p>Reducing conversational agents' overconfidence through linguistic calibration. Sabrina J Mielke, Arthur Szlam, Emily Dinan, Y-Lan Boureau, Trans. Assoc. Comput. Linguistics. 10Sabrina J. Mielke, Arthur Szlam, Emily Dinan, and Y-Lan Boureau. 2022. Reducing conversational agents' overconfidence through linguistic calibration. Trans. Assoc. Comput. Linguistics, 10:857-872.</p>
<p>Can a suit of armor conduct electricity? A new dataset for open book question answering. Todor Mihaylov, Peter Clark, Tushar Khot, Ashish Sabharwal, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. Ellen Riloff, David Chiang, Julia Hockenmaier, and Jun'ichi Tsujiithe 2018 Conference on Empirical Methods in Natural Language ProcessingBrussels, BelgiumAssociation for Computational LinguisticsComputational LinguisticsTodor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. 2018. Can a suit of armor conduct electric- ity? A new dataset for open book question answering. In Ellen Riloff, David Chiang, Julia Hockenmaier, and Jun'ichi Tsujii, editors, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Pro- cessing, Brussels, Belgium, October 31 -November 4, 2018, pages 2381-2391. Association for Computational Linguistics. Computational Linguistics</p>
<p>Annotating causality in the tempeval-3 corpus. Paramita Mirza, R Sprugnoli, Sara Tonelli, Manuela Speranza, Conference of the European Chapter. Association for Computational LinguisticsParamita Mirza, R. Sprugnoli, Sara Tonelli, and Manuela Speranza. 2014. Annotating causality in the tempeval-3 corpus. In Conference of the European Chapter of the Association for Computational Linguistics.</p>
<p>LILA: A unified benchmark for mathematical reasoning. Swaroop Mishra, Matthew Finlayson, Pan Lu, Leonard Tang, Sean Welleck, Chitta Baral, Tanmay Rajpurohit, Oyvind Tafjord, Ashish Sabharwal, Peter Clark, Ashwin Kalyan, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. Yoav Goldberg, Zornitsa Kozareva, and Yue Zhangthe 2022 Conference on Empirical Methods in Natural Language ProcessingAbu Dhabi, United Arab EmiratesAssociation for Computational Linguistics2022Swaroop Mishra, Matthew Finlayson, Pan Lu, Leonard Tang, Sean Welleck, Chitta Baral, Tanmay Rajpurohit, Oyvind Tafjord, Ashish Sabharwal, Peter Clark, and Ashwin Kalyan. 2022a. LILA: A unified benchmark for mathematical reasoning. In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang, editors, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022, pages 5807-5832. Association for Computational Linguistics.</p>
<p>Numglue: A suite of fundamental yet challenging mathematical reasoning tasks. Swaroop Mishra, Arindam Mitra, Neeraj Varshney, Bhavdeep Singh Sachdeva, Peter Clark, Chitta Baral, Ashwin Kalyan, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Smaranda Muresan, Preslav Nakov, and Aline Villavicenciothe 60th Annual Meeting of the Association for Computational LinguisticsDublin, IrelandAssociation for Computational Linguistics1ACL 2022Swaroop Mishra, Arindam Mitra, Neeraj Varshney, Bhavdeep Singh Sachdeva, Peter Clark, Chitta Baral, and Ashwin Kalyan. 2022b. Numglue: A suite of fundamental yet challenging mathematical reasoning tasks. In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio, editors, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022, pages 3505-3523. Association for Computational Linguistics.</p>
<p>StereoSet: Measuring stereotypical bias in pretrained language models. Moin Nadeem, Anna Bethke, Siva Reddy, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language ProcessingOnlineAssociation for Computational Linguistics1Moin Nadeem, Anna Bethke, and Siva Reddy. 2021. StereoSet: Measuring stereotypical bias in pretrained lan- guage models. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 5356-5371, Online, August. Association for Computational Linguistics.</p>
<p>Caiming Xiong, and Dragomir R. Radev. 2021. Fetaqa: Free-form table question answering. Linyong Nan, Chia-Hsuan Hsieh, Ziming Mao, Xi Victoria Lin, Neha Verma, Rui Zhang, Wojciech Kryscinski, Nick Schoelkopf, Riley Kong, Xiangru Tang, Murori Mutuma, Transactions of the Association for Computational Linguistics. 10Linyong Nan, Chia-Hsuan Hsieh, Ziming Mao, Xi Victoria Lin, Neha Verma, Rui Zhang, Wojciech Kryscinski, Nick Schoelkopf, Riley Kong, Xiangru Tang, Murori Mutuma, Benjamin Rosand, Isabel Trindade, Renusree Bandaru, Jacob Cunningham, Caiming Xiong, and Dragomir R. Radev. 2021. Fetaqa: Free-form table question answering. Transactions of the Association for Computational Linguistics, 10:35-49.</p>
<p>CrowS-pairs: A challenge dataset for measuring social biases in masked language models. Nikita Nangia, Clara Vania, Rasika Bhalerao, Samuel R Bowman, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)OnlineAssociation for Computational LinguisticsNikita Nangia, Clara Vania, Rasika Bhalerao, and Samuel R. Bowman. 2020. CrowS-pairs: A challenge dataset for measuring social biases in masked language models. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1953-1967, Online, November. Association for Computational Linguistics.</p>
<p>. Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova Dassarma, Tom Henighan, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Dawn Drain, Deep Ganguli, Zac Hatfield-Dodds, Danny Hernandez, Scott Johnston, Andy Jones, Jackson Kernion, Liane Lovitt, Kamal Ndousse, Dario Amodei, Tom Brown, Jack Clark, Jared Kaplan, Sam McCandlishand Chris Olah. 2022. In-context learning and induction heads. CoRR, abs/2209.11895Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom Henighan, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Dawn Drain, Deep Ganguli, Zac Hatfield-Dodds, Danny Hernandez, Scott Johnston, Andy Jones, Jackson Kernion, Liane Lovitt, Kamal Ndousse, Dario Amodei, Tom Brown, Jack Clark, Jared Kaplan, Sam McCandlish, and Chris Olah. 2022. In-context learning and induction heads. CoRR, abs/2209.11895.</p>
<p>abs/2303.08774OpenAI. 2023. GPT-4 technical report. CoRR. OpenAI. 2023. GPT-4 technical report. CoRR, abs/2303.08774.</p>
<p>Training language models to follow instructions with human feedback. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll L Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul F Christiano, Jan Leike, Ryan Lowe, NeurIPS. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul F. Christiano, Jan Leike, and Ryan Lowe. 2022. Training language models to follow instructions with human feedback. In NeurIPS.</p>
<p>Integration of World Knowledge for Natural Language Understanding. Ekaterina Ovchinnikova, Atlantis Thinking Machines. 3Atlantis PressEkaterina Ovchinnikova. 2012. Integration of World Knowledge for Natural Language Understanding, volume 3 of Atlantis Thinking Machines. Atlantis Press.</p>
<p>Understanding the capabilities of large language models for automated planning. Vishal Pallagani, Bharath Muppasani, Keerthiram Murugesan, Francesca Rossi, Biplav Srivastava, Lior Horesh, Francesco Fabiano, Andrea Loreggia, abs/2305.16151CoRRVishal Pallagani, Bharath Muppasani, Keerthiram Murugesan, Francesca Rossi, Biplav Srivastava, Lior Horesh, Francesco Fabiano, and Andrea Loreggia. 2023. Understanding the capabilities of large language models for automated planning. CoRR, abs/2305.16151.</p>
<p>Bleu: a method for automatic evaluation of machine translation. Kishore Papineni, Salim Roukos, Todd Ward, Wei-Jing Zhu, Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics. the 40th Annual Meeting of the Association for Computational LinguisticsPhiladelphia, PA, USAACLKishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu: a method for automatic evalua- tion of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, July 6-12, 2002, Philadelphia, PA, USA, pages 311-318. ACL.</p>
<p>Totto: A controlled table-to-text generation dataset. P Ankur, Xuezhi Parikh, Sebastian Wang, Manaal Gehrmann, Bhuwan Faruqui, Diyi Dhingra, Dipanjan Yang, Das, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing. Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liuthe 2020 Conference on Empirical Methods in Natural Language ProcessingOnlineAssociation for Computational Linguistics2020Ankur P. Parikh, Xuezhi Wang, Sebastian Gehrmann, Manaal Faruqui, Bhuwan Dhingra, Diyi Yang, and Dipanjan Das. 2020. Totto: A controlled table-to-text generation dataset. In Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu, editors, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020, Online, November 16-20, 2020, pages 1173-1186. Association for Computational Linguistics.</p>
<p>Compositional semantic parsing on semi-structured tables. Panupong Pasupat, Percy Liang, Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing. the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language ProcessingBeijing, ChinaLong Papers1The Association for Computer Linguistics. Computational LinguisticsPanupong Pasupat and Percy Liang. 2015. Compositional semantic parsing on semi-structured tables. In Pro- ceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing, ACL 2015, July 26-31, 2015, Beijing, China, Volume 1: Long Papers, pages 1470-1480. The Association for Computer Linguistics. Computational Linguistics</p>
<p>XCOPA: A multilingual dataset for causal commonsense reasoning. Goran Edoardo Maria Ponti, Olga Glavas, Qianchu Majewska, Ivan Liu, Anna Vulic, Korhonen, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing. Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liuthe 2020 Conference on Empirical Methods in Natural Language ProcessingOnlineAssociation for Computational Linguistics2020Edoardo Maria Ponti, Goran Glavas, Olga Majewska, Qianchu Liu, Ivan Vulic, and Anna Korhonen. 2020. XCOPA: A multilingual dataset for causal commonsense reasoning. In Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu, editors, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020, Online, November 16-20, 2020, pages 2362-2376. Association for Computational Linguistics.</p>
<p>Semeval-2014 task 4: Aspect based sentiment analysis. Maria Pontiki, Dimitris Galanis, John Pavlopoulos, Harris Papageorgiou, Proceedings of the 8th International Workshop on Semantic Evaluation. Preslav Nakov and Torsten Zeschthe 8th International Workshop on Semantic EvaluationSemEval@COLING; Dublin, IrelandThe Association for Computer LinguisticsIon Androutsopoulos, and Suresh ManandharMaria Pontiki, Dimitris Galanis, John Pavlopoulos, Harris Papageorgiou, Ion Androutsopoulos, and Suresh Man- andhar. 2014. Semeval-2014 task 4: Aspect based sentiment analysis. In Preslav Nakov and Torsten Zesch, editors, Proceedings of the 8th International Workshop on Semantic Evaluation, SemEval@COLING 2014, Dublin, Ireland, August 23-24, 2014, pages 27-35. The Association for Computer Linguistics.</p>
<p>Semeval-2015 task 12: Aspect based sentiment analysis. Maria Pontiki, Dimitris Galanis, Haris Papageorgiou, Proceedings of the 9th International Workshop on Semantic Evaluation, SemEval@NAACL-HLT 2015. Daniel M. Cer, David Jurgens, Preslav Nakov, and Torsten Zeschthe 9th International Workshop on Semantic Evaluation, SemEval@NAACL-HLT 2015Denver, Colorado, USAThe Association for Computer LinguisticsSuresh Manandhar, and Ion AndroutsopoulosMaria Pontiki, Dimitris Galanis, Haris Papageorgiou, Suresh Manandhar, and Ion Androutsopoulos. 2015. Semeval-2015 task 12: Aspect based sentiment analysis. In Daniel M. Cer, David Jurgens, Preslav Nakov, and Torsten Zesch, editors, Proceedings of the 9th International Workshop on Semantic Evaluation, SemEval@NAACL-HLT 2015, Denver, Colorado, USA, June 4-5, 2015, pages 486-495. The Association for Computer Linguistics.</p>
<p>SemEval-2016 task 5: Aspect based sentiment analysis. Maria Pontiki, Dimitris Galanis, Haris Papageorgiou, Ion Androutsopoulos, Suresh Manandhar, Al- Mohammad, Mahmoud Smadi, Yanyan Al-Ayyoub, Bing Zhao, Orphée Qin, Véronique De Clercq, Marianna Hoste, Xavier Apidianaki, Natalia Tannier, Evgeniy Loukachevitch, Kotelnikov, Proceedings of the 10th International Workshop on Semantic Evaluation (SemEval-2016). the 10th International Workshop on Semantic Evaluation (SemEval-2016)Nuria Bel, Salud María Jiménez-Zafra, and Gülşen Eryigit; San Diego, CaliforniaAssociation for Computational LinguisticsMaria Pontiki, Dimitris Galanis, Haris Papageorgiou, Ion Androutsopoulos, Suresh Manandhar, Mohammad AL- Smadi, Mahmoud Al-Ayyoub, Yanyan Zhao, Bing Qin, Orphée De Clercq, Véronique Hoste, Marianna Apidi- anaki, Xavier Tannier, Natalia Loukachevitch, Evgeniy Kotelnikov, Nuria Bel, Salud María Jiménez-Zafra, and Gülşen Eryigit. 2016. SemEval-2016 task 5: Aspect based sentiment analysis. In Proceedings of the 10th International Workshop on Semantic Evaluation (SemEval-2016), pages 19-30, San Diego, California, June. Association for Computational Linguistics.</p>
<p>Beneath the tip of the iceberg: Current challenges and new directions in sentiment analysis research. Soujanya Poria, Devamanyu Hazarika, Navonil Majumder, Rada Mihalcea, IEEE Trans. Affect. Comput. 141Soujanya Poria, Devamanyu Hazarika, Navonil Majumder, and Rada Mihalcea. 2023. Beneath the tip of the iceberg: Current challenges and new directions in sentiment analysis research. IEEE Trans. Affect. Comput., 14(1):108-132.</p>
<p>A review on offensive language detection. Rahul Pradhan, Ankur Chaturvedi, Aprna Tripathi, Dilip Kumar Sharma, Advances in Data and Information Sciences. Mohan L. Kolhe, Shailesh Tiwari, Munesh C. Trivedi, and Krishn K. MishraSingapore; SingaporeSpringerRahul Pradhan, Ankur Chaturvedi, Aprna Tripathi, and Dilip Kumar Sharma. 2020. A review on offensive language detection. In Mohan L. Kolhe, Shailesh Tiwari, Munesh C. Trivedi, and Krishn K. Mishra, editors, Advances in Data and Information Sciences, pages 433-439, Singapore. Springer Singapore.</p>
<p>Reasoning with language model prompting: A survey. Shuofei Qiao, Yixin Ou, Ningyu Zhang, Xiang Chen, Yunzhi Yao, Shumin Deng, Chuanqi Tan, Fei Huang, Huajun Chen, abs/2212.09597CoRRShuofei Qiao, Yixin Ou, Ningyu Zhang, Xiang Chen, Yunzhi Yao, Shumin Deng, Chuanqi Tan, Fei Huang, and Huajun Chen. 2022. Reasoning with language model prompting: A survey. CoRR, abs/2212.09597.</p>
<p>Dynamically fused graph network for multi-hop reasoning. Lin Qiu, Yunxuan Xiao, Yanru Qu, Hao Zhou, Lei Li, Weinan Zhang, Yong Yu, Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019. Anna Korhonen, David R. Traum, and Lluís Màrquezthe 57th Conference of the Association for Computational Linguistics, ACL 2019Florence, ItalyAssociation for Computational Linguistics1Lin Qiu, Yunxuan Xiao, Yanru Qu, Hao Zhou, Lei Li, Weinan Zhang, and Yong Yu. 2019. Dynamically fused graph network for multi-hop reasoning. In Anna Korhonen, David R. Traum, and Lluís Màrquez, editors, Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019, Florence, Italy, July 28-August 2, 2019, Volume 1: Long Papers, pages 6140-6150. Association for Computational Linguistics.</p>
<p>Towards empathetic open-domain conversation models: A new benchmark and dataset. Eric Michael Hannah Rashkin, Margaret Smith, Y-Lan Li, Boureau, Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019. Anna Korhonen, David R. Traum, and Lluís Màrquezthe 57th Conference of the Association for Computational Linguistics, ACL 2019Florence, ItalyAssociation for Computational Linguistics1Hannah Rashkin, Eric Michael Smith, Margaret Li, and Y-Lan Boureau. 2019. Towards empathetic open-domain conversation models: A new benchmark and dataset. In Anna Korhonen, David R. Traum, and Lluís Màrquez, editors, Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019, Flo- rence, Italy, July 28-August 2, 2019, Volume 1: Long Papers, pages 5370-5381. Association for Computational Linguistics.</p>
<p>Semeval-2017 task 4: Sentiment analysis in twitter. Sara Rosenthal, Noura Farra, Preslav Nakov, Proceedings of the 11th International Workshop on Semantic Evaluation. Steven Bethard, Marine Carpuat, Marianna Apidianaki, Saif M. Mohammad, Daniel M. Cer, and David Jurgensthe 11th International Workshop on Semantic EvaluationVancouver, CanadaAssociation for Computational LinguisticsSara Rosenthal, Noura Farra, and Preslav Nakov. 2017. Semeval-2017 task 4: Sentiment analysis in twitter. In Steven Bethard, Marine Carpuat, Marianna Apidianaki, Saif M. Mohammad, Daniel M. Cer, and David Jurgens, editors, Proceedings of the 11th International Workshop on Semantic Evaluation, SemEval@ACL 2017, Vancouver, Canada, August 3-4, 2017, pages 502-518. Association for Computational Linguistics.</p>
<p>Emotion detection from text and speech: a survey. Kashfia Sailunaz, Manmeet Dhaliwal, Jon G Rokne, Reda Alhajj, Soc. Netw. Anal. Min. 8126Kashfia Sailunaz, Manmeet Dhaliwal, Jon G. Rokne, and Reda Alhajj. 2018. Emotion detection from text and speech: a survey. Soc. Netw. Anal. Min., 8(1):28:1-28:26.</p>
<p>Social bias frames: Reasoning about social and power implications of language. Maarten Sap, Saadia Gabriel, Lianhui Qin, Dan Jurafsky, Noah A Smith, Yejin Choi, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsOnlineAssociation for Computational LinguisticsMaarten Sap, Saadia Gabriel, Lianhui Qin, Dan Jurafsky, Noah A. Smith, and Yejin Choi. 2020. Social bias frames: Reasoning about social and power implications of language. In Proceedings of the 58th Annual Meet- ing of the Association for Computational Linguistics, pages 5477-5490, Online, July. Association for Compu- tational Linguistics.</p>
<p>A survey on hate speech detection using natural language processing. Anna Schmidt, Michael Wiegand, Proceedings of the Fifth International Workshop on Natural Language Processing for Social Media. Lun-Wei Ku and Cheng-Te Lithe Fifth International Workshop on Natural Language Processing for Social MediaValencia, SpainAssociation for Computational LinguisticsComputational LinguisticsAnna Schmidt and Michael Wiegand. 2017. A survey on hate speech detection using natural language processing. In Lun-Wei Ku and Cheng-Te Li, editors, Proceedings of the Fifth International Workshop on Natural Language Processing for Social Media, SocialNLP@EACL 2017, Valencia, Spain, April 3, 2017, pages 1-10. Association for Computational Linguistics. Computational Linguistics</p>
<p>Language models are multilingual chain-of-thought reasoners. Freda Shi, Mirac Suzgun, Markus Freitag, Xuezhi Wang, Suraj Srivats, Soroush Vosoughi, Hyung Won, Yi Chung, Sebastian Tay, Denny Ruder, Dipanjan Zhou, Jason Das, Wei, The Eleventh International Conference on Learning Representations, ICLR 2023. Kigali, RwandaOpenReview.netFreda Shi, Mirac Suzgun, Markus Freitag, Xuezhi Wang, Suraj Srivats, Soroush Vosoughi, Hyung Won Chung, Yi Tay, Sebastian Ruder, Denny Zhou, Dipanjan Das, and Jason Wei. 2023. Language models are multilin- gual chain-of-thought reasoners. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net.</p>
<p>CLUTRR: A diagnostic benchmark for inductive reasoning from text. Koustuv Sinha, Shagun Sodhani, Jin Dong, Joelle Pineau, William L Hamilton, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing. Kentaro Inui, Jing Jiang, Vincent Ng, and Xiaojun Wanthe 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language ProcessingHong Kong, ChinaAssociation for Computational LinguisticsKoustuv Sinha, Shagun Sodhani, Jin Dong, Joelle Pineau, and William L. Hamilton. 2019. CLUTRR: A diagnostic benchmark for inductive reasoning from text. In Kentaro Inui, Jing Jiang, Vincent Ng, and Xiaojun Wan, editors, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019, Hong Kong, China, November 3-7, 2019, pages 4505-4514. Association for Computational Linguistics.</p>
<p>Recursive deep models for semantic compositionality over a sentiment treebank. Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Ng, Christopher Potts, Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing. the 2013 Conference on Empirical Methods in Natural Language ProcessingSeattle, Washington, USAAssociation for Computational LinguisticsRichard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng, and Christopher Potts. 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1631-1642, Seattle, Washington, USA, October. Association for Computational Linguistics.</p>
<p>Theory of Mind. Beate Sodian, Susanne Kristen, SpringerBerlin Heidelberg; Berlin, HeidelbergBeate Sodian and Susanne Kristen, 2010. Theory of Mind, pages 189-201. Springer Berlin Heidelberg, Berlin, Heidelberg.</p>
<p>Conceptnet 5.5: An open multilingual graph of general knowledge. Robyn Speer, Joshua Chin, Catherine Havasi, Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence. Satinder Singh and Shaul Markovitchthe Thirty-First AAAI Conference on Artificial IntelligenceSan Francisco, California, USAAAAI PressRobyn Speer, Joshua Chin, and Catherine Havasi. 2017. Conceptnet 5.5: An open multilingual graph of general knowledge. In Satinder Singh and Shaul Markovitch, editors, Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence, February 4-9, 2017, San Francisco, California, USA, pages 4444-4451. AAAI Press.</p>
<p>Chatgpt: A study on its utility for ubiquitous software engineering tasks. Giriprasad Sridhara, H G Ranjani, Sourav Mazumdar, abs/2305.16837CoRRGiriprasad Sridhara, Ranjani H. G., and Sourav Mazumdar. 2023. Chatgpt: A study on its utility for ubiquitous software engineering tasks. CoRR, abs/2305.16837.</p>
<p>. Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam R Brown, Adam Santoro, Aditya Gupta, Adrià Garriga-Alonso, Agnieszka Kluska, Aitor Lewkowycz, Akshat Agarwal, Alethea Power, Alex Ray, Alex Warstadt, Alexander W Kocurek, Ali Safaya, Ali Tazarv, Alice Xiang, Alicia Parrish, Allen Nie, Aman Hussain, Amanda Askell, Amanda Dsouza, Ameet Rahane, Ashish Sabharwal. Anantharaman S. Iyer, Anders Andreassen, Andrea Santilli, Andreas Stuhlmüller, Andrew M. Dai, Andrew La, Andrew K. Lampinen, Andy Zou, Angela Jiang, Angelica Chen, Anh Vuong, Animesh Gupta, Anna Gottardi, Antonio Norelli, Anu Venkatesh, Arash Gholamidavoodi, Arfa Tabassum, Arul Menezes, Arun Kirubarajan, Asher Mullokandovand et al. 2022. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. CoRR, abs/2206.04615Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam R. Brown, Adam Santoro, Aditya Gupta, Adrià Garriga-Alonso, Agnieszka Kluska, Aitor Lewkowycz, Akshat Agarwal, Alethea Power, Alex Ray, Alex Warstadt, Alexander W. Kocurek, Ali Safaya, Ali Tazarv, Alice Xi- ang, Alicia Parrish, Allen Nie, Aman Hussain, Amanda Askell, Amanda Dsouza, Ameet Rahane, Ananthara- man S. Iyer, Anders Andreassen, Andrea Santilli, Andreas Stuhlmüller, Andrew M. Dai, Andrew La, Andrew K. Lampinen, Andy Zou, Angela Jiang, Angelica Chen, Anh Vuong, Animesh Gupta, Anna Gottardi, Antonio Norelli, Anu Venkatesh, Arash Gholamidavoodi, Arfa Tabassum, Arul Menezes, Arun Kirubarajan, Asher Mul- lokandov, Ashish Sabharwal, Austin Herrick, Avia Efrat, Aykut Erdem, Ayla Karakas, and et al. 2022. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. CoRR, abs/2206.04615.</p>
<p>Commonsense reasoning for natural language understanding: A survey of benchmarks, resources, and approaches. Shane Storks, Qiaozi Gao, Joyce Y Chai, abs/1904.01172CoRRShane Storks, Qiaozi Gao, and Joyce Y. Chai. 2019. Commonsense reasoning for natural language understanding: A survey of benchmarks, resources, and approaches. CoRR, abs/1904.01172.</p>
<p>On the safety of conversational models: Taxonomy, dataset, and benchmark. Hao Sun, Guangxuan Xu, Jiawen Deng, Jiale Cheng, Chujie Zheng, Hao Zhou, Nanyun Peng, Xiaoyan Zhu, Minlie Huang, Findings of the Association for Computational Linguistics: ACL 2022. Dublin, Ireland, MayAssociation for Computational LinguisticsHao Sun, Guangxuan Xu, Jiawen Deng, Jiale Cheng, Chujie Zheng, Hao Zhou, Nanyun Peng, Xiaoyan Zhu, and Minlie Huang. 2022. On the safety of conversational models: Taxonomy, dataset, and benchmark. In Findings of the Association for Computational Linguistics: ACL 2022, pages 3906-3923, Dublin, Ireland, May. Association for Computational Linguistics.</p>
<p>Vipergpt: Visual inference via python execution for reasoning. Dídac Surís, Sachit Menon, Carl Vondrick, abs/2303.08128CoRRDídac Surís, Sachit Menon, and Carl Vondrick. 2023. Vipergpt: Visual inference via python execution for reason- ing. CoRR, abs/2303.08128.</p>
<p>Intriguing properties of neural networks. Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian J Goodfellow, Rob Fergus, 2nd International Conference on Learning Representations. Banff, AB, CanadaConference Track ProceedingsChristian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian J. Goodfellow, and Rob Fergus. 2014. Intriguing properties of neural networks. In Yoshua Bengio and Yann LeCun, editors, 2nd International Conference on Learning Representations, ICLR 2014, Banff, AB, Canada, April 14-16, 2014, Conference Track Proceedings.</p>
<p>Commonsenseqa: A question answering challenge targeting commonsense knowledge. Alon Talmor, Jonathan Herzig, Nicholas Lourie, Jonathan Berant, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019. Jill Burstein, Christy Doran, and Thamar Soloriothe 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019Minneapolis, MN, USAAssociation for Computational Linguistics1Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. 2019. Commonsenseqa: A question an- swering challenge targeting commonsense knowledge. In Jill Burstein, Christy Doran, and Thamar Solorio, editors, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computa- tional Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers), pages 4149-4158. Association for Computational Linguistics.</p>
<p>Multimodalqa: complex question answering over text, tables and images. Alon Talmor, Ori Yoran, Amnon Catav, Dan Lahav, Yizhong Wang, Akari Asai, Gabriel Ilharco, Hannaneh Hajishirzi, Jonathan Berant, 9th International Conference on Learning Representations, ICLR 2021, Virtual Event. AustriaOpenReview.netAlon Talmor, Ori Yoran, Amnon Catav, Dan Lahav, Yizhong Wang, Akari Asai, Gabriel Ilharco, Hannaneh Ha- jishirzi, and Jonathan Berant. 2021. Multimodalqa: complex question answering over text, tables and images. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net.</p>
<p>FEVER: a large-scale dataset for fact extraction and verification. James Thorne, Andreas Vlachos, Christos Christodoulopoulos, Arpit Mittal, Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Marilyn A. Walker, Heng Ji, and Amanda Stentthe 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesNew Orleans, Louisiana, USAAssociation for Computational Linguistics1Long PapersJames Thorne, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mittal. 2018. FEVER: a large-scale dataset for fact extraction and verification. In Marilyn A. Walker, Heng Ji, and Amanda Stent, editors, Proceed- ings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2018, New Orleans, Louisiana, USA, June 1-6, 2018, Volume 1 (Long Papers), pages 809-819. Association for Computational Linguistics.</p>
<p>Evaluating model calibration in classification. Juozas Vaicenavicius, David Widmann, Carl R Andersson, Fredrik Lindsten, Jacob Roll, Thomas B Schön, PMLRThe 22nd International Conference on Artificial Intelligence and Statistics. Kamalika Chaudhuri and Masashi SugiyamaNaha, Okinawa, Japan2019AISTATSJuozas Vaicenavicius, David Widmann, Carl R. Andersson, Fredrik Lindsten, Jacob Roll, and Thomas B. Schön. 2019. Evaluating model calibration in classification. In Kamalika Chaudhuri and Masashi Sugiyama, editors, The 22nd International Conference on Artificial Intelligence and Statistics, AISTATS 2019, 16-18 April 2019, Naha, Okinawa, Japan, volume 89 of Proceedings of Machine Learning Research, pages 3459-3467. PMLR.</p>
<p>Large language models still can't plan (A benchmark for llms on planning and reasoning about change. Karthik Valmeekam, Alberto Olmo Hernandez, Sarath Sreedharan, Subbarao Kambhampati, abs/2206.10498CoRRKarthik Valmeekam, Alberto Olmo Hernandez, Sarath Sreedharan, and Subbarao Kambhampati. 2022. Large language models still can't plan (A benchmark for llms on planning and reasoning about change). CoRR, abs/2206.10498.</p>
<p>On the planning abilities of large language models -A critical investigation. Karthik Valmeekam, Matthew Marquez, Sarath Sreedharan, Subbarao Kambhampati, abs/2305.15771CoRRKarthik Valmeekam, Matthew Marquez, Sarath Sreedharan, and Subbarao Kambhampati. 2023. On the planning abilities of large language models -A critical investigation. CoRR, abs/2305.15771.</p>
<ol>
<li>D'ya like dags? A survey on structure learning and causal discovery. Matthew J Vowels, Richard Necati Cihan Camgöz, Bowden, ACM Comput. Surv. 55436Matthew J. Vowels, Necati Cihan Camgöz, and Richard Bowden. 2023. D'ya like dags? A survey on structure learning and causal discovery. ACM Comput. Surv., 55(4):82:1-82:36.</li>
</ol>
<p>Fact or fiction: Verifying scientific claims. David Wadden, Shanchuan Lin, Kyle Lo, Lucy Lu Wang, Madeleine Van Zuylen, Arman Cohan, Hannaneh Hajishirzi, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing. Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liuthe 2020 Conference on Empirical Methods in Natural Language ProcessingOnlineAssociation for Computational Linguistics2020David Wadden, Shanchuan Lin, Kyle Lo, Lucy Lu Wang, Madeleine van Zuylen, Arman Cohan, and Hannaneh Hajishirzi. 2020. Fact or fiction: Verifying scientific claims. In Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu, editors, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020, Online, November 16-20, 2020, pages 7534-7550. Association for Computational Linguistics.</p>
<p>Superglue: A stickier benchmark for general-purpose language understanding systems. Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, Samuel R Bowman, Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems. Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d'Alché-Buc, Emily B. Fox, and Roman GarnettNeurIPS; Vancouver, BC, CanadaAlex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. 2019a. Superglue: A stickier benchmark for general-purpose language understanding systems. In Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d'Alché-Buc, Emily B. Fox, and Roman Garnett, editors, Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pages 3261-3275.</p>
<p>GLUE: A multi-task benchmark and analysis platform for natural language understanding. Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, Samuel R Bowman, 7th International Conference on Learning Representations. New Orleans, LA, USAOpenReview.netAlex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. 2019b. GLUE: A multi-task benchmark and analysis platform for natural language understanding. In 7th International Confer- ence on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net.</p>
<p>A survey on table-and-text hybridqa: Concepts, methods, challenges and future directions. Dingzirui Wang, Longxu Dou, Wanxiang Che, abs/2212.13465CoRRDingzirui Wang, Longxu Dou, and Wanxiang Che. 2022a. A survey on table-and-text hybridqa: Concepts, methods, challenges and future directions. CoRR, abs/2212.13465.</p>
<p>A fine-grained interpretability evaluation benchmark for neural NLP. Lijie Wang, Yaozong Shen, Shuyuan Peng, Shuai Zhang, Xinyan Xiao, Hao Liu, Hongxuan Tang, Ying Chen, Hua Wu, Haifeng Wang, Proceedings of the 26th Conference on Computational Natural Language Learning (CoNLL). the 26th Conference on Computational Natural Language Learning (CoNLL)Abu Dhabi, United Arab EmiratesAssociation for Computational LinguisticsLijie Wang, Yaozong Shen, Shuyuan Peng, Shuai Zhang, Xinyan Xiao, Hao Liu, Hongxuan Tang, Ying Chen, Hua Wu, and Haifeng Wang. 2022b. A fine-grained interpretability evaluation benchmark for neural NLP. In Proceedings of the 26th Conference on Computational Natural Language Learning (CoNLL), pages 70-84, Abu Dhabi, United Arab Emirates (Hybrid), December. Association for Computational Linguistics.</p>
<p>MAVEN-ERE: A unified large-scale dataset for event coreference, temporal, causal, and subevent relation extraction. Xiaozhi Wang, Yulin Chen, Ning Ding, Hao Peng, Zimu Wang, Yankai Lin, Xu Han, Lei Hou, Juanzi Li, Zhiyuan Liu, Peng Li, Jie Zhou, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. Yoav Goldberg, Zornitsa Kozareva, and Yue Zhangthe 2022 Conference on Empirical Methods in Natural Language ProcessingAbu Dhabi, United Arab Emirates2022Association for Computational LinguisticsXiaozhi Wang, Yulin Chen, Ning Ding, Hao Peng, Zimu Wang, Yankai Lin, Xu Han, Lei Hou, Juanzi Li, Zhiyuan Liu, Peng Li, and Jie Zhou. 2022c. MAVEN-ERE: A unified large-scale dataset for event coreference, tem- poral, causal, and subevent relation extraction. In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang, editors, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022, pages 926-941. Association for Computational Lin- guistics.</p>
<p>Measure and improve robustness in NLP models: A survey. Xuezhi Wang, Haohan Wang, Diyi Yang, Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL 2022. Marine Carpuat, Marie-Catherine de Marneffe, and Iván Vladimir Meza Ruízthe 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL 2022Seattle, WA, United StatesAssociation for Computational LinguisticsXuezhi Wang, Haohan Wang, and Diyi Yang. 2022d. Measure and improve robustness in NLP models: A sur- vey. In Marine Carpuat, Marie-Catherine de Marneffe, and Iván Vladimir Meza Ruíz, editors, Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Hu- man Language Technologies, NAACL 2022, Seattle, WA, United States, July 10-15, 2022, pages 4569-4586. Association for Computational Linguistics.</p>
<p>Is chatgpt a good sentiment analyzer? A preliminary study. Zengzhi Wang, Qiming Xie, Zixiang Ding, Yi Feng, Rui Xia, abs/2304.04339CoRRZengzhi Wang, Qiming Xie, Zixiang Ding, Yi Feng, and Rui Xia. 2023. Is chatgpt a good sentiment analyzer? A preliminary study. CoRR, abs/2304.04339.</p>
<p>Blimp: The benchmark of linguistic minimal pairs for english. Alex Warstadt, Alicia Parrish, Haokun Liu, Anhad Mohananey, Wei Peng, Sheng-Fu Wang, Samuel R , Trans. Assoc. Comput. Linguistics. 8BowmanAlex Warstadt, Alicia Parrish, Haokun Liu, Anhad Mohananey, Wei Peng, Sheng-Fu Wang, and Samuel R. Bow- man. 2020. Blimp: The benchmark of linguistic minimal pairs for english. Trans. Assoc. Comput. Linguistics, 8:377-392.</p>
<p>Understanding abuse: A typology of abusive language detection subtasks. Zeerak Waseem, Thomas Davidson, Dana Warmsley, Ingmar Weber, Proceedings of the First Workshop on Abusive Language Online. the First Workshop on Abusive Language OnlineVancouver, BC, CanadaAssociation for Computational LinguisticsZeerak Waseem, Thomas Davidson, Dana Warmsley, and Ingmar Weber. 2017. Understanding abuse: A typology of abusive language detection subtasks. In Proceedings of the First Workshop on Abusive Language Online, pages 78-84, Vancouver, BC, Canada, August. Association for Computational Linguistics.</p>
<p>Jeff Dean, and William Fedus. 2022a. Emergent abilities of large language models. Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Trans. Mach. Learn. Res. Computational LinguisticsJason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H. Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, and William Fedus. 2022a. Emergent abilities of large language models. Trans. Mach. Learn. Res., 2022. Computational Linguistics</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H Chi, Quoc V Le, Denny Zhou, NeurIPS. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V. Le, and Denny Zhou. 2022b. Chain-of-thought prompting elicits reasoning in large language models. In NeurIPS.</p>
<p>Ethical and social risks of harm from language models. Laura Weidinger, John Mellor, Maribeth Rauh, Conor Griffin, Jonathan Uesato, Po-Sen Huang, Myra Cheng, Mia Glaese, Borja Balle, Atoosa Kasirzadeh, Zac Kenton, Sasha Brown, Will Hawkins, abs/2112.04359Lisa Anne Hendricks, William Isaac, Sean Legassick, Geoffrey Irving, and Iason GabrielCoRRTom Stepleton, Courtney Biles, Abeba Birhane, Julia Haas, Laura RimellLaura Weidinger, John Mellor, Maribeth Rauh, Conor Griffin, Jonathan Uesato, Po-Sen Huang, Myra Cheng, Mia Glaese, Borja Balle, Atoosa Kasirzadeh, Zac Kenton, Sasha Brown, Will Hawkins, Tom Stepleton, Court- ney Biles, Abeba Birhane, Julia Haas, Laura Rimell, Lisa Anne Hendricks, William Isaac, Sean Legassick, Geoffrey Irving, and Iason Gabriel. 2021. Ethical and social risks of harm from language models. CoRR, abs/2112.04359.</p>
<p>LIME: learning inductive bias for primitives of mathematical reasoning. Yuhuai Wu, Markus N Rabe, Wenda Li, Jimmy Ba, Roger B Grosse, Christian Szegedy, PMLRProceedings of the 38th International Conference on Machine Learning, ICML 2021. Marina Meila and Tong Zhangthe 38th International Conference on Machine Learning, ICML 2021139Yuhuai Wu, Markus N. Rabe, Wenda Li, Jimmy Ba, Roger B. Grosse, and Christian Szegedy. 2021. LIME: learning inductive bias for primitives of mathematical reasoning. In Marina Meila and Tong Zhang, editors, Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, volume 139 of Proceedings of Machine Learning Research, pages 11251-11262. PMLR.</p>
<p>Ex machina: Personal attacks seen at scale. Ellery Wulczyn, Nithum Thain, Lucas Dixon, Proceedings of the 26th International Conference on World Wide Web, WWW '17. the 26th International Conference on World Wide Web, WWW '17Republic and Canton of Geneva. Wide Web Conferences Steering CommitteeEllery Wulczyn, Nithum Thain, and Lucas Dixon. 2017. Ex machina: Personal attacks seen at scale. In Pro- ceedings of the 26th International Conference on World Wide Web, WWW '17, page 1391-1399, Republic and Canton of Geneva, CHE. International World Wide Web Conferences Steering Committee.</p>
<p>Unifiedskg: Unifying and multi-tasking structured knowledge grounding with text-to-text language models. Tianbao Xie, Chen Henry Wu, Peng Shi, Ruiqi Zhong, Torsten Scholak, Michihiro Yasunaga, Chien-Sheng Wu, Ming Zhong, Pengcheng Yin, Sida I Wang, Victor Zhong, Bailin Wang, Chengzu Li, Connor Boyle, Ansong Ni, Ziyu Yao, Dragomir Radev, Caiming Xiong, Lingpeng Kong, Rui Zhang, Noah A Smith, Luke Zettlemoyer, Tao Yu, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. Yoav Goldberg, Zornitsa Kozareva, and Yue Zhangthe 2022 Conference on Empirical Methods in Natural Language ProcessingAbu Dhabi; United Arab EmiratesAssociation for Computational Linguistics2022Tianbao Xie, Chen Henry Wu, Peng Shi, Ruiqi Zhong, Torsten Scholak, Michihiro Yasunaga, Chien-Sheng Wu, Ming Zhong, Pengcheng Yin, Sida I. Wang, Victor Zhong, Bailin Wang, Chengzu Li, Connor Boyle, Ansong Ni, Ziyu Yao, Dragomir Radev, Caiming Xiong, Lingpeng Kong, Rui Zhang, Noah A. Smith, Luke Zettlemoyer, and Tao Yu. 2022. Unifiedskg: Unifying and multi-tasking structured knowledge grounding with text-to-text language models. In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang, editors, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022, pages 602-631. Association for Computational Linguistics.</p>
<p>Bot-adversarial dialogue for safe conversational agents. Jing Xu, Da Ju, Margaret Li, Y-Lan Boureau, Jason Weston, Emily Dinan, Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesOnlineAssociation for Computational LinguisticsJing Xu, Da Ju, Margaret Li, Y-Lan Boureau, Jason Weston, and Emily Dinan. 2021. Bot-adversarial dialogue for safe conversational agents. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 2950-2968, Online, June. Association for Computational Linguistics.</p>
<p>Wikiqa: A challenge dataset for open-domain question answering. Yi Yang, Wen-Tau Yih, Christopher Meek, Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. Lluís Màrquez, Chris Callison-Burch, Jian Su, Daniele Pighin, and Yuval Martonthe 2015 Conference on Empirical Methods in Natural Language ProcessingLisbon, PortugalThe Association for Computational LinguisticsYi Yang, Wen-tau Yih, and Christopher Meek. 2015. Wikiqa: A challenge dataset for open-domain question answering. In Lluís Màrquez, Chris Callison-Burch, Jian Su, Daniele Pighin, and Yuval Marton, editors, Pro- ceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, EMNLP 2015, Lisbon, Portugal, September 17-21, 2015, pages 2013-2018. The Association for Computational Linguistics.</p>
<p>Hotpotqa: A dataset for diverse, explainable multi-hop question answering. Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W Cohen, Ruslan Salakhutdinov, Christopher D Manning, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. Ellen Riloff, David Chiang, Julia Hockenmaier, and Jun'ichi Tsujiithe 2018 Conference on Empirical Methods in Natural Language ProcessingBrussels, BelgiumAssociation for Computational LinguisticsZhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W. Cohen, Ruslan Salakhutdinov, and Christo- pher D. Manning. 2018. Hotpotqa: A dataset for diverse, explainable multi-hop question answering. In Ellen Riloff, David Chiang, Julia Hockenmaier, and Jun'ichi Tsujii, editors, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Brussels, Belgium, October 31 -November 4, 2018, pages 2369-2380. Association for Computational Linguistics.</p>
<p>. Zonglin Yang, Li Dong, Xinya Du, Hao Cheng, Erik Cambria, Xiaodong Liu, Jianfeng Gao, and Furu Wei. 2022. Language models as inductive reasoners. CoRR, abs/2212.10923Zonglin Yang, Li Dong, Xinya Du, Hao Cheng, Erik Cambria, Xiaodong Liu, Jianfeng Gao, and Furu Wei. 2022. Language models as inductive reasoners. CoRR, abs/2212.10923.</p>
<p>Linkbert: Pretraining language models with document links. Michihiro Yasunaga, Jure Leskovec, Percy Liang, Proceedings of the 60th. Smaranda Muresan, Preslav Nakov, and Aline Villavicenciothe 60thMichihiro Yasunaga, Jure Leskovec, and Percy Liang. 2022. Linkbert: Pretraining language models with doc- ument links. In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio, editors, Proceedings of the 60th</p>
<p>Annual Meeting of the Association for Computational Linguistics. Dublin, IrelandAssociation for Computational Linguistics1ACL 2022Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022, pages 8003-8016. Association for Computational Linguistics.</p>
<p>The unreliability of explanations in few-shot prompting for textual reasoning. Xi Ye, Greg Durrett, In NeurIPSXi Ye and Greg Durrett. 2022. The unreliability of explanations in few-shot prompting for textual reasoning. In NeurIPS.</p>
<p>Do large language models know what they don't know? CoRR. Zhangyue Yin, Qiushi Sun, Qipeng Guo, Jiawen Wu, Xipeng Qiu, Xuanjing Huang, abs/2305.18153Zhangyue Yin, Qiushi Sun, Qipeng Guo, Jiawen Wu, Xipeng Qiu, and Xuanjing Huang. 2023. Do large language models know what they don't know? CoRR, abs/2305.18153.</p>
<p>Natural language reasoning, a survey. Fei Yu, Hongbo Zhang, Prayag Tiwari, Benyou Wang, Fei Yu, Hongbo Zhang, Prayag Tiwari, and Benyou Wang. 2023. Natural language reasoning, a survey.</p>
<p>Predicting the type and target of offensive posts in social media. Marcos Zampieri, Shervin Malmasi, Preslav Nakov, Sara Rosenthal, Noura Farra, Ritesh Kumar, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMinneapolis, MinnesotaAssociation for Computational Linguistics1Marcos Zampieri, Shervin Malmasi, Preslav Nakov, Sara Rosenthal, Noura Farra, and Ritesh Kumar. 2019. Pre- dicting the type and target of offensive posts in social media. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 1415-1420, Minneapolis, Minnesota, June. Association for Computational Linguistics.</p>
<p>E Zelikman, Yuhuai Wu, Noah D Goodman, Star: Bootstrapping reasoning with reasoning. ArXiv, abs/2203.14465. Computational Linguistics. E. Zelikman, Yuhuai Wu, and Noah D. Goodman. 2022. Star: Bootstrapping reasoning with reasoning. ArXiv, abs/2203.14465. Computational Linguistics</p>
<p>Hellaswag: Can a machine really finish your sentence?. Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, Yejin Choi, Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019. Anna Korhonen, David R. Traum, and Lluís Màrquezthe 57th Conference of the Association for Computational Linguistics, ACL 2019Florence, ItalyAssociation for Computational Linguistics1Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. 2019. Hellaswag: Can a machine really finish your sentence? In Anna Korhonen, David R. Traum, and Lluís Màrquez, editors, Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019, Florence, Italy, July 28-August 2, 2019, Volume 1: Long Papers, pages 4791-4800. Association for Computational Linguistics.</p>
<p>A survey in automatic irony processing: Linguistic, cognitive, and multi-x perspectives. Qingcheng Zeng, An-Ran Li ; Chu-Ren Huang, Hansaem Kim, James Pustejovsky, Leo Wanner, Key-Sun, Pum-Mo Choi, Hsin-Hsi Ryu, Lucia Chen, Heng Donatelli, Sadao Ji, Patrizia Kurohashi, Nianwen Paggio, Seokhwan Xue, Younggyun Kim, Zhong Hahm, He, Proceedings of the 29th International Conference on Computational Linguistics, COLING 2022. Tony Kyungil Lee, Enrico Santus, Francis Bond, and Seung-Hoon Nathe 29th International Conference on Computational Linguistics, COLING 2022Gyeongju, Republic of KoreaNicoletta Calzolari. International Committee on Computational LinguisticsQingcheng Zeng and An-Ran Li. 2022. A survey in automatic irony processing: Linguistic, cognitive, and multi-x perspectives. In Nicoletta Calzolari, Chu-Ren Huang, Hansaem Kim, James Pustejovsky, Leo Wanner, Key-Sun Choi, Pum-Mo Ryu, Hsin-Hsi Chen, Lucia Donatelli, Heng Ji, Sadao Kurohashi, Patrizia Paggio, Nianwen Xue, Seokhwan Kim, Younggyun Hahm, Zhong He, Tony Kyungil Lee, Enrico Santus, Francis Bond, and Seung-Hoon Na, editors, Proceedings of the 29th International Conference on Computational Linguistics, COLING 2022, Gyeongju, Republic of Korea, October 12-17, 2022, pages 824-836. International Committee on Computational Linguistics.</p>
<p>Character-level convolutional networks for text classification. Xiang Zhang, Junbo Jake Zhao, Yann Lecun, Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems. Corinna Cortes, Neil D. Lawrence, Daniel D. Lee, Masashi Sugiyama, and Roman GarnettMontreal, Quebec, CanadaXiang Zhang, Junbo Jake Zhao, and Yann LeCun. 2015. Character-level convolutional networks for text classifi- cation. In Corinna Cortes, Neil D. Lawrence, Daniel D. Lee, Masashi Sugiyama, and Roman Garnett, editors, Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems 2015, December 7-12, 2015, Montreal, Quebec, Canada, pages 649-657.</p>
<ol>
<li>A survey on aspect-based sentiment analysis: Tasks, methods, and challenges. Wenxuan Zhang, Xin Li, Yang Deng, Lidong Bing, Wai Lam, abs/2203.01054CoRRWenxuan Zhang, Xin Li, Yang Deng, Lidong Bing, and Wai Lam. 2022. A survey on aspect-based sentiment analysis: Tasks, methods, and challenges. CoRR, abs/2203.01054.</li>
</ol>
<p>Sinno Jialin Pan, and Lidong Bing. 2023. Sentiment analysis in the era of large language models: A reality check. Wenxuan Zhang, Yue Deng, Bing Liu, abs/2305.15005CoRRWenxuan Zhang, Yue Deng, Bing Liu, Sinno Jialin Pan, and Lidong Bing. 2023. Sentiment analysis in the era of large language models: A reality check. CoRR, abs/2305.15005.</p>
<p>Lirex: Augmenting language inference with relevant explanations. Xinyan Zhao, V G Vinod Vydiswaran, Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021, Thirty-Third Conference on Innovative Applications of Artificial Intelligence, IAAI 2021, The Eleventh Symposium on Educational Advances in Artificial Intelligence, EAAI 2021, Virtual Event. AAAI PressXinyan Zhao and V. G. Vinod Vydiswaran. 2021. Lirex: Augmenting language inference with relevant ex- planations. In Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021, Thirty-Third Conference on Innovative Applications of Artificial Intelligence, IAAI 2021, The Eleventh Symposium on Educational Advances in Artificial Intelligence, EAAI 2021, Virtual Event, February 2-9, 2021, pages 14532-14539. AAAI Press.</p>
<p>. Kun Wayne Xin Zhao, Junyi Zhou, Tianyi Li, Xiaolei Tang, Yupeng Wang, Yingqian Hou, Beichen Min, Junjie Zhang, Zican Zhang, Yifan Dong, Chen Du, Yushuo Yang, Zhipeng Chen, Jinhao Chen, Jiang, Zikang Liu, Peiyu Liu, Jian-Yun Nie, and Ji-Rong WenRuiyang Ren, Yifan Li, Xinyu Tang2023. A survey of large language models. CoRR, abs/2303.18223Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, Yifan Du, Chen Yang, Yushuo Chen, Zhipeng Chen, Jinhao Jiang, Ruiyang Ren, Yifan Li, Xinyu Tang, Zikang Liu, Peiyu Liu, Jian-Yun Nie, and Ji-Rong Wen. 2023. A survey of large language models. CoRR, abs/2303.18223.</p>
<p>Why does chatgpt fall short in providing truthful answers?. Jie Shen Zheng, Kevin Chen-Chuan Huang, Chang, Shen Zheng, Jie Huang, and Kevin Chen-Chuan Chang. 2023. Why does chatgpt fall short in providing truthful answers?</p>
<p>Seq2sql: Generating structured queries from natural language using reinforcement learning. Victor Zhong, Caiming Xiong, Richard Socher, abs/1709.00103CoRRVictor Zhong, Caiming Xiong, and Richard Socher. 2017. Seq2sql: Generating structured queries from natural language using reinforcement learning. CoRR, abs/1709.00103.</p>
<p>Agieval: A human-centric benchmark for evaluating foundation models. Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin Saied, Weizhu Chen, Nan Duan, abs/2304.06364CoRRWanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin Saied, Weizhu Chen, and Nan Duan. 2023. Agieval: A human-centric benchmark for evaluating foundation models. CoRR, abs/2304.06364.</p>
<p>Emotional chatting machine: Emotional conversation generation with internal and external memory. Hao Zhou, Minlie Huang, Tianyang Zhang, Xiaoyan Zhu, Bing Liu, Sheila A. McIlraith and Kilian QHao Zhou, Minlie Huang, Tianyang Zhang, Xiaoyan Zhu, and Bing Liu. 2018. Emotional chatting machine: Emotional conversation generation with internal and external memory. In Sheila A. McIlraith and Kilian Q.</p>
<p>Weinberger, Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18). the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18)New Orleans, Louisiana, USAAAAI PressWeinberger, editors, Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educa- tional Advances in Artificial Intelligence (EAAI-18), New Orleans, Louisiana, USA, February 2-7, 2018, pages 730-739. AAAI Press.</p>
<p>Yujin Terry Yue Zhuo, Huang, Chunyang Chen, and Zhenchang Xing. 2023. Red teaming chatgpt via jailbreaking: Bias, robustness, reliability and toxicity. Terry Yue Zhuo, Yujin Huang, Chunyang Chen, and Zhenchang Xing. 2023. Red teaming chatgpt via jailbreaking: Bias, robustness, reliability and toxicity.</p>
<p>The moral integrity corpus: A benchmark for ethical dialogue systems. Caleb Ziems, Jane Yu, Yi-Chia Wang, Alon Halevy, Diyi Yang, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. the 60th Annual Meeting of the Association for Computational LinguisticsDublin, Ireland, MayAssociation for Computational Linguistics1Caleb Ziems, Jane Yu, Yi-Chia Wang, Alon Halevy, and Diyi Yang. 2022. The moral integrity corpus: A benchmark for ethical dialogue systems. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 3755-3773, Dublin, Ireland, May. Association for Computational Linguistics.</p>            </div>
        </div>

    </div>
</body>
</html>
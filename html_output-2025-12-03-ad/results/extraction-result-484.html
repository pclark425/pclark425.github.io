<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-484 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-484</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-484</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-18.html">extraction-schema-18</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <p><strong>Paper ID:</strong> paper-257206170</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2302.12691v1.pdf" target="_blank">Reproducibility of Machine Learning: Terminology, Recommendations and Open Issues</a></p>
                <p><strong>Paper Abstract:</strong> Reproducibility is one of the core dimensions that concur to deliver Trustworthy Artificial Intelligence. Broadly speaking, reproducibility can be defined as the possibility to reproduce the same or a similar experiment or method, thereby obtaining the same or similar results as the original scientists. It is an essential ingredient of the scientific method and crucial for gaining trust in relevant claims. A reproducibility crisis has been recently acknowledged by scientists and this seems to affect even more Artificial Intelligence and Machine Learning, due to the complexity of the models at the core of their recent successes. Notwithstanding the recent debate on Artificial Intelligence reproducibility, its practical implementation is still insufficient, also because many technical issues are overlooked. In this survey, we critically review the current literature on the topic and highlight the open issues. Our contribution is three-fold. We propose a concise terminological review of the terms coming into play. We collect and systematize existing recommendations for achieving reproducibility, putting forth the means to comply with them. We identify key elements often overlooked in modern Machine Learning and provide novel recommendations for them. We further specialize these for two critical application domains, namely the biomedical and physical artificial intelligence fields.</p>
                <p><strong>Cost:</strong> 0.024</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e484.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e484.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Insufficient documentation gap</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Insufficient or incomplete natural-language documentation vs. actual implementation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Papers often omit key experimental details (software environment, hyper-parameters, preprocessing, dataset splits, evaluation procedures) so the natural-language description does not match the runnable code or the code is not provided at all, preventing faithful reproduction.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>ML research experiment / publication artifact</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A published machine-learning experiment described in a paper (methods section, appendices, README) that may or may not be accompanied by code, data, and environment specifications.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>research paper methods section / submission checklist / README</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>source code repository, experiment scripts, notebooks</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>incomplete specification / missing details</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Natural-language descriptions often list high-level model architectures, goals and results but omit precise artifact-level details (exact hyper-parameter values, seed settings, dataset splits, preprocessing scripts, software dependency versions). Sometimes code is not released; when released it may lack the scripts to reproduce training/evaluation pipelines or the README lacks steps, so code and description are misaligned.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>end-to-end experimental pipeline (preprocessing, training configs, evaluation protocol, environment specification)</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>systematic literature surveys and checklists (manual review of papers and their supplementary materials) and cross-reference with available code repositories (Gundersen et al., Raff, Pineau checklist analysis cited in paper)</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>binary / categorical variables recorded per paper (e.g., presence/absence of code, data, hyper-parameters, environment); aggregated rates (e.g., proportion of papers documenting each item); correlation analysis between documented features and successful independent reproduction</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Prevents independent reproduction and interpretation; Gundersen & Kjensmo style surveys found only ~20-30% of recommended documentation variables present in papers, meaning many experiments are not reproducible; authors report inability to re-implement or get comparable results when critical details are missing.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>High — reported across many conference/journal surveys; Gundersen et al. reports 20–30% of recommended variables documented; many checklists show only several dozen percent of papers are reproducible</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Paper space constraints, cultural norms (insufficient sharing), ambiguous natural-language descriptions, implicit assumptions, lack of metadata standards and incentives to publish full artifacts</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Adopt community checklists (NeurIPS/ICML/IJCAI), require code/data and environment specification, provide runnable one-command reproduction scripts or containers, attach metadata (datasheets, model cards), use experiment-tracking/versioning tools (MLflow, DVC, dtoolAI)</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Partially effective: surveys recommend these, and improved documentation correlates with greater chance of reproduction; however paper notes documentation alone is insufficient without addressing non-determinism and environment issues (no single quantitative effectiveness reported in survey)</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>machine learning / deep learning (general across domains)</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e484.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e484.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Hyperparameter mismatch</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Omitted or mismatched hyper-parameter specification between description and implementation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Papers frequently fail to list the full set or exact values of hyper-parameters used to generate reported results; implementations may use different defaults, causing divergence between described method and reproduced runs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>ML training pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Model training pipeline where hyper-parameters (learning rate, batch size, optimizers, weight decay, initialization) determine training dynamics and final results.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>paper hyper-parameter list / methods section / supplemental tables</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>training scripts / config files / notebooks</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>hyperparameter mismatch / missing hyper-parameters</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Authors often provide only some hyper-parameters (or just ranges) but not the full set or the final values used for reported results; code (if available) may not expose or document the config files used for the experiments, or may contain different defaults than described.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>training procedure / model configuration</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>manual comparison of paper-reported hyper-parameters and the values found in shared code/configs; reproducibility attempts where re-implementation fails unless exact hyper-parameters discovered</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>counting presence/absence of hyper-parameter documentation per paper; correlation of hyper-parameter disclosure with successful independent reproduction (as in Raff's attribute analysis)</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Substantial — hyper-parameter differences can change convergence and final performance, making reproduction fail or yield different conclusions; Raff's study showed specification of hyper-parameters is positively correlated with independent reproducibility (no single % effect size reported in survey).</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Common — hyper-parameter omission repeatedly highlighted across checklists and surveys (NeurIPS checklist, Pineau, Gundersen, Raff).</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Authors omit exhaustive lists due to space or oversight, assume defaults, or do hyper-parameter search without recording exact chosen settings.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Record and publish full configuration files, use experiment-tracking/version-control for configs (DVC, MLflow, dtoolAI), include hyper-parameter ranges and the search method, attach exact config used for best-run as metadata.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>High in practice — explicit config sharing dramatically aids reproduction; surveys and guidelines recommend it strongly though quantitative evaluation limited in paper.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>deep learning / general ML</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e484.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e484.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Preprocessing mismatch</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Undocumented or mismatched data preprocessing and dataset-splitting steps</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Preprocessing steps (normalization, augmentation, cleaning, selection/exclusion of cases) are often described at high level but the precise code path is omitted or differs, causing results to diverge from the described experiment.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>data pipeline / dataset management</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Data acquisition and preprocessing pipeline used to produce inputs for training and evaluation (raw -> cleaned -> split -> augmented).</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>methods / data section / datasheet</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>preprocessing scripts / data-cleaning notebooks</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>missing preprocessing step / excluded data not described</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Authors sometimes do not provide raw data or the exact preprocessing scripts; exclusion criteria, patient-level separation, calibration procedures, tokenization and feature extraction details are omitted or summarized, while code may contain ad-hoc transformations not mentioned in the paper (KAIST dataset example, medical imaging slice vs patient-level splitting).</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>data preprocessing / train/validation/test splitting</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>manual inspection of dataset statements and available preprocessing code; detection during attempts to re-run experiments (different results due to different preprocessing); surveys and domain-specific analyses (medical imaging and robotics examples)</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>reporting whether raw data and preprocessing scripts are provided; counting presence of 'raw data saved' and 'preprocessing described' checklist items (Pineau checklist, Datasheets); comparing performance when preprocessing steps differ</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>High — mis-specified preprocessing can lead to data leakage, inflated metrics, inability to reproduce reported performance; in biomedical imaging, incorrect split granularity (slice vs patient) can produce non-reproducible claims</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>High in practice for domain-specific datasets (biomedical, robotics); many guidelines explicitly call out this problem</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Privacy constraints on raw data; ad-hoc preprocessing not recorded; lack of standard metadata to record these steps; implicit assumptions in text</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Provide raw and processed data where possible, publish preprocessing code and exact split files, use datasheets for datasets, assign persistent identifiers and versions (DOI), annotate preprocessing with metadata</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Effective where feasible; in fields with privacy (biomedical) mitigations include data walled-gardens for reviewers or sharing synthetic/proxy data; paper notes adoption is limited and effectiveness not broadly quantified</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>biomedical imaging / robotics / ML</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e484.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e484.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Toolchain non-determinism</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Implementation indeterminism due to software/hardware (toolchain nondeterminism)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Numerical and execution non-determinism introduced by libraries, parallel preprocessing, vendor GPU libraries (cuDNN), and hardware leads to different numerical results even when code, data, and seeds are provided.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>deep learning training stack</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Software framework (TensorFlow/PyTorch), third-party libraries (cuDNN), OS and GPU hardware used during model training and inference.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>paper environment description / high-level method description</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>framework usage, lower-level library calls, runtime configurations</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>implementation indeterminism (software/hardware) / ambiguous environment specification</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Even with the same code and random seeds, implementation-level non-determinism (reordering of floating point operations, library autotune, multi-process data loaders) and hardware-level differences across GPUs/architectures can yield different outcomes; these sources are often not documented in natural-language descriptions.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>runtime execution (training runs) & lower-level library calls</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>empirical repeated-run experiments showing variance across runs; targeted analyses (Morin & Willetts on TensorFlow ResNets, Nagarajan on RL non-determinism) and tool/system studies (Zhuang et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>observed variance across independent runs (loss/accuracy traces), stepwise comparison of loss per training step, statistical analysis of run-to-run variability, attribution studies isolating GPU/library differences (e.g., logging per-step loss sequences)</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Substantial — non-determinism can dominate observed randomness and prevent exact numeric reproduction; Morin & Willetts found GPU non-determinism can dominate; RL studies show large variance and possible failure to match published quality; exact quantitative impact depends on task but can be large (variance large enough to change conclusions).</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Widespread for GPU-accelerated DL and parallelized toolchains; documented across multiple studies</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Low-level floating-point non-associativity, parallel execution ordering, library autotuning selecting different routines, lack of deterministic defaults in frameworks and hardware</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Use deterministic implementations (disable autotune), set seeds, avoid multi-process loaders that change order, use record-and-replay and profile-and-patch techniques (Chen et al.), use reproducible container abstractions such as DetTrace, or deterministic GPU architectures when available</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Effective: DetTrace shown to render TensorFlow workloads reproducible without code changes (with a runtime overhead); record-and-replay/profile-and-patch and avoiding non-deterministic ops reduce variance — effectiveness task-dependent and may incur performance/time overhead</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>deep learning / RL / GPU-accelerated ML</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e484.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e484.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Platform metadata gap</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Lack of platform-native support for metadata, artifact citation, and experiment provenance</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>ML platforms and services often do not natively provide features to capture full experiment metadata, code/data citation, or links between analysis and claims, forcing ad-hoc and non-standard artifact sharing that breaks alignment between descriptions and implementations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>ML experimentation platforms (OpenML, MLflow, Kaggle, Codalab, cloud services)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Platforms used to host code, data, models and to run/track experiments; vary in native metadata and provenance support.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>platform documentation / paper methods / metadata schemas</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>platform-integrated experiments, external integrations (GitHub + Zenodo), metadata records</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>missing metadata and provenance support / incomplete artifact citation</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Platforms surveyed do not consistently provide native support for (1) stating how analyses support claims, (2) code/data citation with persistent URIs, (3) rich metadata for hyper-parameters, dataset provenance and experiment lineage; combining third-party services is left to authors and is non-standardized, producing mismatches between what the paper claims and what a reproducer can access/run.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>artifact publication / metadata layer / platform integration</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>platform feature survey (Gundersen et al. evaluated 13 platforms against reproducibility variables)</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>checklist of variables (22 variables) scored per platform; aggregation into reproducibility-level metrics showing which features platforms support or lack</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Reduces portability/transferability of experiments between platforms and increases friction for reproducers; Gundersen et al. found none of the considered platforms supported the statement of how analysis supports claims and many lacked native code/data citation and metadata provision.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Systemic across mainstream platforms (surveyed platforms showed notable gaps)</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Heterogeneous platform design goals; lack of agreed metadata standards for AI artifacts; reliance on third-party integrations rather than native comprehensive features</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Adopt standardized metadata models (PROV-O, DCAT, DQV, ML-Schema, MEX), integrate platform tools with DOI minting (Zenodo), use experiment-tracking and data-versioning tools (MLflow, DVC, dtoolAI) and follow platform-specific best practices</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Partially effective: using third-party standards and tools improves artifact discoverability and reproducibility, but the paper highlights that adoption is uneven and integration complexity remains a barrier</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>machine learning infrastructure / reproducible research tooling</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e484.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e484.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Baseline implementation sensitivity</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sensitivity of reported improvements to small implementation details (baseline misalignment)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Small, undocumented implementation decisions (optimizer calls order, initialization, data ordering) can create large performance differences between a new method and baselines, making natural-language experimental claims misaligned with code results.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>algorithm benchmark / comparison pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Comparative evaluation setting where new methods are measured against baselines on benchmarks; relies on consistent baseline implementations and evaluation procedure.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>paper claims comparing method performance / evaluation protocol description</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>reference implementations, baseline code, benchmark scripts</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>different algorithm variant / undocumented implementation detail causing performance delta</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Authors or reviewers may compare new algorithms to baselines that differ in minor implementation details; these small differences (non-obvious defaults, preprocessing, seed treatment) can change reported metrics substantially and misattribute improvements to the algorithmic idea rather than implementations.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>model implementation and evaluation scripts / baseline code</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>experimental replication and controlled ablation showing effect of small implementation choices; benchmark re-runs with multi-seed averages (Julian et al., multi-task RL study showed large sensitivity)</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>quantitative performance deltas when toggling single implementation choices; multi-seed statistics and confidence intervals; Julian et al. report absolute performance differences up to ~35% in meta-/multi-task RL attributable to small implementation details</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Can invalidate claimed improvements; reported advantages may fall within variance produced by implementation choices leading to false-positive claims of progress</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Observed across RL, deep learning benchmarks and multi-component systems; cited as a recurrent issue in RL and robotics</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Lack of standardized reference implementations, inconsistent baseline code, implicit implementation choices, insufficient reporting of low-level decisions</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Use widely accepted reference implementations, publish baseline code, standardize evaluation protocols, report multi-seed averages and confidence intervals, include ablations isolating low-level design choices</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Effective when applied: using reference implementations and multi-seed reporting reduces risk of mistaking implementation artifacts for algorithmic gains; effectiveness demonstrated qualitatively in cited studies (e.g., reduction of spurious improvements when controlling for implementation details)</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>deep reinforcement learning / embodied agents / benchmarking</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e484.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e484.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Dataset version / provenance mismatch</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Undocumented dataset modifications and lack of versioning leading to mismatch between described and actual dataset used</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Datasets are frequently modified (sanitized subsets, cleaned annotations) without versioning or clear description, so the dataset referenced in text differs from the dataset used in code or public repos.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>dataset curation and distribution process</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Public or proprietary datasets used for training/evaluation where researchers sometimes apply fixes, subsetting, or re-annotation before experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>paper dataset description / dataset datasheet</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>dataset files, derived datasets on authors' repos, preprocessing scripts</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>missing dataset provenance / incorrect dataset citation / undocumented modifications</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Examples (KAIST dataset) show many researchers use modified or 'sanitized' versions without explicit versioning; papers sometimes fail to state which variant was used, so reproduced experiments may use a different data instance than the authors did.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>data source and dataset distribution stage</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>inspection of literature and dataset usage patterns; recognition of unversioned, modified dataset variants in follow-up papers and inability to match metrics</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>qualitative tracking of dataset variants; when possible, measuring performance variation across dataset variants; checklist items: 'dataset versioned', 'raw data preserved', 'exclusions described' (Pineau, datasheets)</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Compromises comparability of results and reproduction attempts; untracked modifications can cause significant metric shifts and hinder fair benchmarking</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Common in physical-AI and robotics datasets and some vision datasets where ad-hoc 'sanitized' variants circulate without formal versioning</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Lack of dataset versioning practices, ad-hoc local fixes, absence of standard DOI-based dataset publication practices</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Provide dataset DOIs/URIs, maintain raw and processed copies, include datasheets describing exclusions/edits, publish exact split files and preprocessing code</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Effective when adopted; paper cites datasheets and platform DOI minting (Zenodo) as concrete mitigations though adoption remains limited</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>robotics / computer vision / ML datasets</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e484.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e484.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Randomness vs robustness tradeoff (sim2real)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Randomization for robustness (domain randomization) reduces reproducibility in simulations</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Increasing randomization in simulated environments (to improve sim-to-real transfer) increases run-to-run variability and reduces the ability to precisely reproduce simulation outcomes described in a paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>simulated training environment for RL / sim2real workflows</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Physics-based simulators and randomized parameter distributions used to train agents intended for transfer to real-world robots.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>paper description of sim2real setup / simulator configuration</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>simulator config files / randomization scripts / environment seeds</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>intentional randomness causing reduced reproducibility / ambiguous simulation config</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Authors intentionally randomize simulation parameters to learn robust policies; paper text may describe randomization broadly but not the exact distributions or seeds, and heavier randomization can lead to less convergent and less reproducible training runs.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>simulator configuration / environment parameterization / training randomness</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>empirical studies that vary the degree of randomization and measure transfer and reproducibility (Josifovski et al.), observation of decreased repeatability with more randomization</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>measuring transfer performance to real robot versus reproducibility statistics in simulation (variance, success rates); tradeoff curves quantifying robustness gain vs reproducibility loss</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Tradeoff: more randomization tends to improve robustness/sim2real transfer but reduces reproducibility and reproducible convergence; authors report decreased ability to find good policies and higher variability across runs</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Important and increasing in robotics and sim2real literature; not rare for sim2real-targeted studies</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Design choice prioritizing robustness/generalization over exact reproducibility; insufficiently detailed reporting of randomization distributions and seeds</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Publish exact randomization distributions, seeds, and multiple runs statistics; provide deterministic seeds and 'profiling' configs for debugging and for deterministic reproduction when desired; document the degree of randomization used</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Mitigates confusion: publishing distributions and seeds allows reproducers to reproduce a specific experimental regime even if many randomized instances exist; effectiveness dependent on authors providing full configs</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>physical AI / robotics / deep RL / sim2real</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>State of the Art: Reproducibility in Artificial Intelligence <em>(Rating: 2)</em></li>
                <li>Step Toward Quantifying Independently Reproducible Machine Learning Research <em>(Rating: 2)</em></li>
                <li>Towards Training Reproducible Deep Learning Models <em>(Rating: 2)</em></li>
                <li>Non-Determinism in TensorFlow ResNets <em>(Rating: 2)</em></li>
                <li>The Impact of Nondeterminism on Reproducibility in Deep Reinforcement Learning <em>(Rating: 2)</em></li>
                <li>Demystifying Reproducibility in Meta-and Multi-Task Reinforcement Learning <em>(Rating: 2)</em></li>
                <li>Do machine learning platforms provide out-of-the-box reproducibility? <em>(Rating: 2)</em></li>
                <li>Analysis of Randomization Effects on Sim2Real Transfer in Reinforcement Learning for Robotic Manipulation Tasks <em>(Rating: 1)</em></li>
                <li>Deterministic containers (DetTrace) / Reproducible Containers <em>(Rating: 1)</em></li>
                <li>Datasheets for Datasets <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-484",
    "paper_id": "paper-257206170",
    "extraction_schema_id": "extraction-schema-18",
    "extracted_data": [
        {
            "name_short": "Insufficient documentation gap",
            "name_full": "Insufficient or incomplete natural-language documentation vs. actual implementation",
            "brief_description": "Papers often omit key experimental details (software environment, hyper-parameters, preprocessing, dataset splits, evaluation procedures) so the natural-language description does not match the runnable code or the code is not provided at all, preventing faithful reproduction.",
            "citation_title": "here",
            "mention_or_use": "mention",
            "system_name": "ML research experiment / publication artifact",
            "system_description": "A published machine-learning experiment described in a paper (methods section, appendices, README) that may or may not be accompanied by code, data, and environment specifications.",
            "nl_description_type": "research paper methods section / submission checklist / README",
            "code_implementation_type": "source code repository, experiment scripts, notebooks",
            "gap_type": "incomplete specification / missing details",
            "gap_description": "Natural-language descriptions often list high-level model architectures, goals and results but omit precise artifact-level details (exact hyper-parameter values, seed settings, dataset splits, preprocessing scripts, software dependency versions). Sometimes code is not released; when released it may lack the scripts to reproduce training/evaluation pipelines or the README lacks steps, so code and description are misaligned.",
            "gap_location": "end-to-end experimental pipeline (preprocessing, training configs, evaluation protocol, environment specification)",
            "detection_method": "systematic literature surveys and checklists (manual review of papers and their supplementary materials) and cross-reference with available code repositories (Gundersen et al., Raff, Pineau checklist analysis cited in paper)",
            "measurement_method": "binary / categorical variables recorded per paper (e.g., presence/absence of code, data, hyper-parameters, environment); aggregated rates (e.g., proportion of papers documenting each item); correlation analysis between documented features and successful independent reproduction",
            "impact_on_results": "Prevents independent reproduction and interpretation; Gundersen & Kjensmo style surveys found only ~20-30% of recommended documentation variables present in papers, meaning many experiments are not reproducible; authors report inability to re-implement or get comparable results when critical details are missing.",
            "frequency_or_prevalence": "High — reported across many conference/journal surveys; Gundersen et al. reports 20–30% of recommended variables documented; many checklists show only several dozen percent of papers are reproducible",
            "root_cause": "Paper space constraints, cultural norms (insufficient sharing), ambiguous natural-language descriptions, implicit assumptions, lack of metadata standards and incentives to publish full artifacts",
            "mitigation_approach": "Adopt community checklists (NeurIPS/ICML/IJCAI), require code/data and environment specification, provide runnable one-command reproduction scripts or containers, attach metadata (datasheets, model cards), use experiment-tracking/versioning tools (MLflow, DVC, dtoolAI)",
            "mitigation_effectiveness": "Partially effective: surveys recommend these, and improved documentation correlates with greater chance of reproduction; however paper notes documentation alone is insufficient without addressing non-determinism and environment issues (no single quantitative effectiveness reported in survey)",
            "domain_or_field": "machine learning / deep learning (general across domains)",
            "reproducibility_impact": true,
            "uuid": "e484.0"
        },
        {
            "name_short": "Hyperparameter mismatch",
            "name_full": "Omitted or mismatched hyper-parameter specification between description and implementation",
            "brief_description": "Papers frequently fail to list the full set or exact values of hyper-parameters used to generate reported results; implementations may use different defaults, causing divergence between described method and reproduced runs.",
            "citation_title": "here",
            "mention_or_use": "mention",
            "system_name": "ML training pipeline",
            "system_description": "Model training pipeline where hyper-parameters (learning rate, batch size, optimizers, weight decay, initialization) determine training dynamics and final results.",
            "nl_description_type": "paper hyper-parameter list / methods section / supplemental tables",
            "code_implementation_type": "training scripts / config files / notebooks",
            "gap_type": "hyperparameter mismatch / missing hyper-parameters",
            "gap_description": "Authors often provide only some hyper-parameters (or just ranges) but not the full set or the final values used for reported results; code (if available) may not expose or document the config files used for the experiments, or may contain different defaults than described.",
            "gap_location": "training procedure / model configuration",
            "detection_method": "manual comparison of paper-reported hyper-parameters and the values found in shared code/configs; reproducibility attempts where re-implementation fails unless exact hyper-parameters discovered",
            "measurement_method": "counting presence/absence of hyper-parameter documentation per paper; correlation of hyper-parameter disclosure with successful independent reproduction (as in Raff's attribute analysis)",
            "impact_on_results": "Substantial — hyper-parameter differences can change convergence and final performance, making reproduction fail or yield different conclusions; Raff's study showed specification of hyper-parameters is positively correlated with independent reproducibility (no single % effect size reported in survey).",
            "frequency_or_prevalence": "Common — hyper-parameter omission repeatedly highlighted across checklists and surveys (NeurIPS checklist, Pineau, Gundersen, Raff).",
            "root_cause": "Authors omit exhaustive lists due to space or oversight, assume defaults, or do hyper-parameter search without recording exact chosen settings.",
            "mitigation_approach": "Record and publish full configuration files, use experiment-tracking/version-control for configs (DVC, MLflow, dtoolAI), include hyper-parameter ranges and the search method, attach exact config used for best-run as metadata.",
            "mitigation_effectiveness": "High in practice — explicit config sharing dramatically aids reproduction; surveys and guidelines recommend it strongly though quantitative evaluation limited in paper.",
            "domain_or_field": "deep learning / general ML",
            "reproducibility_impact": true,
            "uuid": "e484.1"
        },
        {
            "name_short": "Preprocessing mismatch",
            "name_full": "Undocumented or mismatched data preprocessing and dataset-splitting steps",
            "brief_description": "Preprocessing steps (normalization, augmentation, cleaning, selection/exclusion of cases) are often described at high level but the precise code path is omitted or differs, causing results to diverge from the described experiment.",
            "citation_title": "here",
            "mention_or_use": "mention",
            "system_name": "data pipeline / dataset management",
            "system_description": "Data acquisition and preprocessing pipeline used to produce inputs for training and evaluation (raw -&gt; cleaned -&gt; split -&gt; augmented).",
            "nl_description_type": "methods / data section / datasheet",
            "code_implementation_type": "preprocessing scripts / data-cleaning notebooks",
            "gap_type": "missing preprocessing step / excluded data not described",
            "gap_description": "Authors sometimes do not provide raw data or the exact preprocessing scripts; exclusion criteria, patient-level separation, calibration procedures, tokenization and feature extraction details are omitted or summarized, while code may contain ad-hoc transformations not mentioned in the paper (KAIST dataset example, medical imaging slice vs patient-level splitting).",
            "gap_location": "data preprocessing / train/validation/test splitting",
            "detection_method": "manual inspection of dataset statements and available preprocessing code; detection during attempts to re-run experiments (different results due to different preprocessing); surveys and domain-specific analyses (medical imaging and robotics examples)",
            "measurement_method": "reporting whether raw data and preprocessing scripts are provided; counting presence of 'raw data saved' and 'preprocessing described' checklist items (Pineau checklist, Datasheets); comparing performance when preprocessing steps differ",
            "impact_on_results": "High — mis-specified preprocessing can lead to data leakage, inflated metrics, inability to reproduce reported performance; in biomedical imaging, incorrect split granularity (slice vs patient) can produce non-reproducible claims",
            "frequency_or_prevalence": "High in practice for domain-specific datasets (biomedical, robotics); many guidelines explicitly call out this problem",
            "root_cause": "Privacy constraints on raw data; ad-hoc preprocessing not recorded; lack of standard metadata to record these steps; implicit assumptions in text",
            "mitigation_approach": "Provide raw and processed data where possible, publish preprocessing code and exact split files, use datasheets for datasets, assign persistent identifiers and versions (DOI), annotate preprocessing with metadata",
            "mitigation_effectiveness": "Effective where feasible; in fields with privacy (biomedical) mitigations include data walled-gardens for reviewers or sharing synthetic/proxy data; paper notes adoption is limited and effectiveness not broadly quantified",
            "domain_or_field": "biomedical imaging / robotics / ML",
            "reproducibility_impact": true,
            "uuid": "e484.2"
        },
        {
            "name_short": "Toolchain non-determinism",
            "name_full": "Implementation indeterminism due to software/hardware (toolchain nondeterminism)",
            "brief_description": "Numerical and execution non-determinism introduced by libraries, parallel preprocessing, vendor GPU libraries (cuDNN), and hardware leads to different numerical results even when code, data, and seeds are provided.",
            "citation_title": "here",
            "mention_or_use": "mention",
            "system_name": "deep learning training stack",
            "system_description": "Software framework (TensorFlow/PyTorch), third-party libraries (cuDNN), OS and GPU hardware used during model training and inference.",
            "nl_description_type": "paper environment description / high-level method description",
            "code_implementation_type": "framework usage, lower-level library calls, runtime configurations",
            "gap_type": "implementation indeterminism (software/hardware) / ambiguous environment specification",
            "gap_description": "Even with the same code and random seeds, implementation-level non-determinism (reordering of floating point operations, library autotune, multi-process data loaders) and hardware-level differences across GPUs/architectures can yield different outcomes; these sources are often not documented in natural-language descriptions.",
            "gap_location": "runtime execution (training runs) & lower-level library calls",
            "detection_method": "empirical repeated-run experiments showing variance across runs; targeted analyses (Morin & Willetts on TensorFlow ResNets, Nagarajan on RL non-determinism) and tool/system studies (Zhuang et al.)",
            "measurement_method": "observed variance across independent runs (loss/accuracy traces), stepwise comparison of loss per training step, statistical analysis of run-to-run variability, attribution studies isolating GPU/library differences (e.g., logging per-step loss sequences)",
            "impact_on_results": "Substantial — non-determinism can dominate observed randomness and prevent exact numeric reproduction; Morin & Willetts found GPU non-determinism can dominate; RL studies show large variance and possible failure to match published quality; exact quantitative impact depends on task but can be large (variance large enough to change conclusions).",
            "frequency_or_prevalence": "Widespread for GPU-accelerated DL and parallelized toolchains; documented across multiple studies",
            "root_cause": "Low-level floating-point non-associativity, parallel execution ordering, library autotuning selecting different routines, lack of deterministic defaults in frameworks and hardware",
            "mitigation_approach": "Use deterministic implementations (disable autotune), set seeds, avoid multi-process loaders that change order, use record-and-replay and profile-and-patch techniques (Chen et al.), use reproducible container abstractions such as DetTrace, or deterministic GPU architectures when available",
            "mitigation_effectiveness": "Effective: DetTrace shown to render TensorFlow workloads reproducible without code changes (with a runtime overhead); record-and-replay/profile-and-patch and avoiding non-deterministic ops reduce variance — effectiveness task-dependent and may incur performance/time overhead",
            "domain_or_field": "deep learning / RL / GPU-accelerated ML",
            "reproducibility_impact": true,
            "uuid": "e484.3"
        },
        {
            "name_short": "Platform metadata gap",
            "name_full": "Lack of platform-native support for metadata, artifact citation, and experiment provenance",
            "brief_description": "ML platforms and services often do not natively provide features to capture full experiment metadata, code/data citation, or links between analysis and claims, forcing ad-hoc and non-standard artifact sharing that breaks alignment between descriptions and implementations.",
            "citation_title": "here",
            "mention_or_use": "mention",
            "system_name": "ML experimentation platforms (OpenML, MLflow, Kaggle, Codalab, cloud services)",
            "system_description": "Platforms used to host code, data, models and to run/track experiments; vary in native metadata and provenance support.",
            "nl_description_type": "platform documentation / paper methods / metadata schemas",
            "code_implementation_type": "platform-integrated experiments, external integrations (GitHub + Zenodo), metadata records",
            "gap_type": "missing metadata and provenance support / incomplete artifact citation",
            "gap_description": "Platforms surveyed do not consistently provide native support for (1) stating how analyses support claims, (2) code/data citation with persistent URIs, (3) rich metadata for hyper-parameters, dataset provenance and experiment lineage; combining third-party services is left to authors and is non-standardized, producing mismatches between what the paper claims and what a reproducer can access/run.",
            "gap_location": "artifact publication / metadata layer / platform integration",
            "detection_method": "platform feature survey (Gundersen et al. evaluated 13 platforms against reproducibility variables)",
            "measurement_method": "checklist of variables (22 variables) scored per platform; aggregation into reproducibility-level metrics showing which features platforms support or lack",
            "impact_on_results": "Reduces portability/transferability of experiments between platforms and increases friction for reproducers; Gundersen et al. found none of the considered platforms supported the statement of how analysis supports claims and many lacked native code/data citation and metadata provision.",
            "frequency_or_prevalence": "Systemic across mainstream platforms (surveyed platforms showed notable gaps)",
            "root_cause": "Heterogeneous platform design goals; lack of agreed metadata standards for AI artifacts; reliance on third-party integrations rather than native comprehensive features",
            "mitigation_approach": "Adopt standardized metadata models (PROV-O, DCAT, DQV, ML-Schema, MEX), integrate platform tools with DOI minting (Zenodo), use experiment-tracking and data-versioning tools (MLflow, DVC, dtoolAI) and follow platform-specific best practices",
            "mitigation_effectiveness": "Partially effective: using third-party standards and tools improves artifact discoverability and reproducibility, but the paper highlights that adoption is uneven and integration complexity remains a barrier",
            "domain_or_field": "machine learning infrastructure / reproducible research tooling",
            "reproducibility_impact": true,
            "uuid": "e484.4"
        },
        {
            "name_short": "Baseline implementation sensitivity",
            "name_full": "Sensitivity of reported improvements to small implementation details (baseline misalignment)",
            "brief_description": "Small, undocumented implementation decisions (optimizer calls order, initialization, data ordering) can create large performance differences between a new method and baselines, making natural-language experimental claims misaligned with code results.",
            "citation_title": "here",
            "mention_or_use": "mention",
            "system_name": "algorithm benchmark / comparison pipeline",
            "system_description": "Comparative evaluation setting where new methods are measured against baselines on benchmarks; relies on consistent baseline implementations and evaluation procedure.",
            "nl_description_type": "paper claims comparing method performance / evaluation protocol description",
            "code_implementation_type": "reference implementations, baseline code, benchmark scripts",
            "gap_type": "different algorithm variant / undocumented implementation detail causing performance delta",
            "gap_description": "Authors or reviewers may compare new algorithms to baselines that differ in minor implementation details; these small differences (non-obvious defaults, preprocessing, seed treatment) can change reported metrics substantially and misattribute improvements to the algorithmic idea rather than implementations.",
            "gap_location": "model implementation and evaluation scripts / baseline code",
            "detection_method": "experimental replication and controlled ablation showing effect of small implementation choices; benchmark re-runs with multi-seed averages (Julian et al., multi-task RL study showed large sensitivity)",
            "measurement_method": "quantitative performance deltas when toggling single implementation choices; multi-seed statistics and confidence intervals; Julian et al. report absolute performance differences up to ~35% in meta-/multi-task RL attributable to small implementation details",
            "impact_on_results": "Can invalidate claimed improvements; reported advantages may fall within variance produced by implementation choices leading to false-positive claims of progress",
            "frequency_or_prevalence": "Observed across RL, deep learning benchmarks and multi-component systems; cited as a recurrent issue in RL and robotics",
            "root_cause": "Lack of standardized reference implementations, inconsistent baseline code, implicit implementation choices, insufficient reporting of low-level decisions",
            "mitigation_approach": "Use widely accepted reference implementations, publish baseline code, standardize evaluation protocols, report multi-seed averages and confidence intervals, include ablations isolating low-level design choices",
            "mitigation_effectiveness": "Effective when applied: using reference implementations and multi-seed reporting reduces risk of mistaking implementation artifacts for algorithmic gains; effectiveness demonstrated qualitatively in cited studies (e.g., reduction of spurious improvements when controlling for implementation details)",
            "domain_or_field": "deep reinforcement learning / embodied agents / benchmarking",
            "reproducibility_impact": true,
            "uuid": "e484.5"
        },
        {
            "name_short": "Dataset version / provenance mismatch",
            "name_full": "Undocumented dataset modifications and lack of versioning leading to mismatch between described and actual dataset used",
            "brief_description": "Datasets are frequently modified (sanitized subsets, cleaned annotations) without versioning or clear description, so the dataset referenced in text differs from the dataset used in code or public repos.",
            "citation_title": "here",
            "mention_or_use": "mention",
            "system_name": "dataset curation and distribution process",
            "system_description": "Public or proprietary datasets used for training/evaluation where researchers sometimes apply fixes, subsetting, or re-annotation before experiments.",
            "nl_description_type": "paper dataset description / dataset datasheet",
            "code_implementation_type": "dataset files, derived datasets on authors' repos, preprocessing scripts",
            "gap_type": "missing dataset provenance / incorrect dataset citation / undocumented modifications",
            "gap_description": "Examples (KAIST dataset) show many researchers use modified or 'sanitized' versions without explicit versioning; papers sometimes fail to state which variant was used, so reproduced experiments may use a different data instance than the authors did.",
            "gap_location": "data source and dataset distribution stage",
            "detection_method": "inspection of literature and dataset usage patterns; recognition of unversioned, modified dataset variants in follow-up papers and inability to match metrics",
            "measurement_method": "qualitative tracking of dataset variants; when possible, measuring performance variation across dataset variants; checklist items: 'dataset versioned', 'raw data preserved', 'exclusions described' (Pineau, datasheets)",
            "impact_on_results": "Compromises comparability of results and reproduction attempts; untracked modifications can cause significant metric shifts and hinder fair benchmarking",
            "frequency_or_prevalence": "Common in physical-AI and robotics datasets and some vision datasets where ad-hoc 'sanitized' variants circulate without formal versioning",
            "root_cause": "Lack of dataset versioning practices, ad-hoc local fixes, absence of standard DOI-based dataset publication practices",
            "mitigation_approach": "Provide dataset DOIs/URIs, maintain raw and processed copies, include datasheets describing exclusions/edits, publish exact split files and preprocessing code",
            "mitigation_effectiveness": "Effective when adopted; paper cites datasheets and platform DOI minting (Zenodo) as concrete mitigations though adoption remains limited",
            "domain_or_field": "robotics / computer vision / ML datasets",
            "reproducibility_impact": true,
            "uuid": "e484.6"
        },
        {
            "name_short": "Randomness vs robustness tradeoff (sim2real)",
            "name_full": "Randomization for robustness (domain randomization) reduces reproducibility in simulations",
            "brief_description": "Increasing randomization in simulated environments (to improve sim-to-real transfer) increases run-to-run variability and reduces the ability to precisely reproduce simulation outcomes described in a paper.",
            "citation_title": "here",
            "mention_or_use": "mention",
            "system_name": "simulated training environment for RL / sim2real workflows",
            "system_description": "Physics-based simulators and randomized parameter distributions used to train agents intended for transfer to real-world robots.",
            "nl_description_type": "paper description of sim2real setup / simulator configuration",
            "code_implementation_type": "simulator config files / randomization scripts / environment seeds",
            "gap_type": "intentional randomness causing reduced reproducibility / ambiguous simulation config",
            "gap_description": "Authors intentionally randomize simulation parameters to learn robust policies; paper text may describe randomization broadly but not the exact distributions or seeds, and heavier randomization can lead to less convergent and less reproducible training runs.",
            "gap_location": "simulator configuration / environment parameterization / training randomness",
            "detection_method": "empirical studies that vary the degree of randomization and measure transfer and reproducibility (Josifovski et al.), observation of decreased repeatability with more randomization",
            "measurement_method": "measuring transfer performance to real robot versus reproducibility statistics in simulation (variance, success rates); tradeoff curves quantifying robustness gain vs reproducibility loss",
            "impact_on_results": "Tradeoff: more randomization tends to improve robustness/sim2real transfer but reduces reproducibility and reproducible convergence; authors report decreased ability to find good policies and higher variability across runs",
            "frequency_or_prevalence": "Important and increasing in robotics and sim2real literature; not rare for sim2real-targeted studies",
            "root_cause": "Design choice prioritizing robustness/generalization over exact reproducibility; insufficiently detailed reporting of randomization distributions and seeds",
            "mitigation_approach": "Publish exact randomization distributions, seeds, and multiple runs statistics; provide deterministic seeds and 'profiling' configs for debugging and for deterministic reproduction when desired; document the degree of randomization used",
            "mitigation_effectiveness": "Mitigates confusion: publishing distributions and seeds allows reproducers to reproduce a specific experimental regime even if many randomized instances exist; effectiveness dependent on authors providing full configs",
            "domain_or_field": "physical AI / robotics / deep RL / sim2real",
            "reproducibility_impact": true,
            "uuid": "e484.7"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "State of the Art: Reproducibility in Artificial Intelligence",
            "rating": 2,
            "sanitized_title": "state_of_the_art_reproducibility_in_artificial_intelligence"
        },
        {
            "paper_title": "Step Toward Quantifying Independently Reproducible Machine Learning Research",
            "rating": 2,
            "sanitized_title": "step_toward_quantifying_independently_reproducible_machine_learning_research"
        },
        {
            "paper_title": "Towards Training Reproducible Deep Learning Models",
            "rating": 2,
            "sanitized_title": "towards_training_reproducible_deep_learning_models"
        },
        {
            "paper_title": "Non-Determinism in TensorFlow ResNets",
            "rating": 2,
            "sanitized_title": "nondeterminism_in_tensorflow_resnets"
        },
        {
            "paper_title": "The Impact of Nondeterminism on Reproducibility in Deep Reinforcement Learning",
            "rating": 2,
            "sanitized_title": "the_impact_of_nondeterminism_on_reproducibility_in_deep_reinforcement_learning"
        },
        {
            "paper_title": "Demystifying Reproducibility in Meta-and Multi-Task Reinforcement Learning",
            "rating": 2,
            "sanitized_title": "demystifying_reproducibility_in_metaand_multitask_reinforcement_learning"
        },
        {
            "paper_title": "Do machine learning platforms provide out-of-the-box reproducibility?",
            "rating": 2,
            "sanitized_title": "do_machine_learning_platforms_provide_outofthebox_reproducibility"
        },
        {
            "paper_title": "Analysis of Randomization Effects on Sim2Real Transfer in Reinforcement Learning for Robotic Manipulation Tasks",
            "rating": 1,
            "sanitized_title": "analysis_of_randomization_effects_on_sim2real_transfer_in_reinforcement_learning_for_robotic_manipulation_tasks"
        },
        {
            "paper_title": "Deterministic containers (DetTrace) / Reproducible Containers",
            "rating": 1,
            "sanitized_title": "deterministic_containers_dettrace_reproducible_containers"
        },
        {
            "paper_title": "Datasheets for Datasets",
            "rating": 1,
            "sanitized_title": "datasheets_for_datasets"
        }
    ],
    "cost": 0.024451,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Reproducibility of Machine Learning: Terminology, Recommendations and Open Issues</p>
<p>Riccardo Albertoni 
Institute of Robotics and Machine Intelligence
Institute of Computing Science
SARA COLANTONIO, Institute of Information Science and Technologies "A. Faedo" of the National Research Council of Italy (ISTI-CNR), Italy PIOTR SKRZYPCZYŃSKI
Istituto di Matematica Applicata e Tecnologie Informatiche "Enrico Magenes"
Poznań University of Technol
Poland</p>
<p>Jerzy Stefanowski 
Poznań University of Technology
Poland</p>
<p>Reproducibility of Machine Learning: Terminology, Recommendations and Open Issues
111 Consiglio Nazionale delle Ricerche (IMATI-CNR), ItalyCCS Concepts: • Computing methodologies → Machine learningArtificial intelligence• General and reference → ExperimentationEvaluation Additional Key Words and Phrases: reproducibility, terminology, recommendations, deep learning, physical artificial intelligence, biomedical applications
Reproducibility is one of the core dimensions that concur to deliver Trustworthy Artificial Intelligence. Broadly speaking, reproducibility can be defined as the possibility to reproduce the same or a similar experiment or method, thereby obtaining the same or similar results as the original scientists. It is an essential ingredient of the scientific method and crucial for gaining trust in relevant claims. A reproducibility crisis has been recently acknowledged by scientists and this seems to affect even more Artificial Intelligence and Machine Learning, due to the complexity of the models at the core of their recent successes. Notwithstanding the recent debate on Artificial Intelligence reproducibility, its practical implementation is still insufficient, also because many technical issues are overlooked. In this survey, we critically review the current literature on the topic and highlight the open issues. Our contribution is three-fold. We propose a concise terminological review of the terms coming into play. We collect and systematize existing recommendations for achieving reproducibility, putting forth the means to comply with them. We identify key elements often overlooked in modern Machine Learning and provide novel recommendations for them. We further specialize these for two critical application domains, namely the biomedical and physical artificial intelligence fields. Computational Reproducibility (Silver /Bronze/Gold) [62] Method Reproducibility [47] Recomputation [45] Replicability [132] Experiment Reproducible [54] Replicability [104, 114] Independent Reproducibility [119] Robusteness [114, 144] Data Reproducible [54] CLOSELY MATCHING ORIGINAL EXPERIMENT "Direct" Replications [56] Result Reproducibility [47] Reproducibility [1, 68, 132] CORROBORATION Replicability [112] Inferential Reproducibility [47] Corroboration [52] Conceptual Replication [56] Method Reproducible [54] Generalizable [114]Replicability[2,30,88]</p>
<p>INTRODUCTION</p>
<p>In recent years, Artificial Intelligence (AI) has made rapid progress in terms of new methods, tools and application range. Despite the positive benefits of this development of AI, just in consideration of its impact on individuals' daily life and societies at large, there is also a growing awareness of AI limitations, risks and new challenges that were not so apparent in the previous period. Some concerns also cover the limited people's trust in the current AI systems applied in real life [76]. To increase the positive consequences of AI and, at the same time, mitigate its risks or dangers, several communities are promoting the ideas of Responsible AI or Trustworthy AI, see e.g. their reviews in [24,82]. Following these postulates, besides the traditionally considered performance criteria (such as accuracy), many other aspects of AI systems should be taken into account to improve their trustworthiness, see e.g. EU proposal in [137]. The reproducibility of AI methods is among their core requirements and a cornerstone of Trustworthy AI.</p>
<p>Reproducibility in science means that one can repeat or replicate the same (or sufficiently similar) experiment and obtain the same (or sufficiently similar) research results as the original scientists based on their publications and provided documentations. Diverse reproducibility settings and their definitions have been identified in the literature, see e.g. [2,11,47,52], but from a more general standpoint, reproducibility entails that studies are reproduced by independent researchers. Moreover, it is postulated to share sufficient documentation, data and original code, or ability to its proper re-implementation (in the case of a broader meaning of the term replicability).</p>
<p>Reproducibility is important for many reasons. Firstly, it is an essential ingredient of science, meant to verify the published results and claims and to enable a continuous self-correcting process. This is crucial for gaining trust in the presented research, but it also allows the community to convert the verified approaches into practice or to build foundations for conducting follow-up new research. In the case of computer science and artificial intelligence, researchers and R&amp;D engineers should understand new and often very complex methods [58]. They need to check their correctness, examine their working conditions and limitations, and verify the presented results, especially if they want to use them in the applied systems. Then, many AI projects receive either public or business funds, so they should be subject to accountability, and it is necessary to convince others that these projects can produce reliable results, see e.g. discussion in [119]. Finally, as many AI methods and algorithms are used in critical systems or applications, where their decisions can have an impact on people and society, and their improper operation may cause harm, reproducibility of these methods is one of solutions to test their quality and a signal of their credibility [38].</p>
<p>Despite the aforementioned arguments, there is still a lack of details and sufficient reporting in many published works [146]. On the one hand, the practice of sharing source code and the postulates of open science are still too slowly developing [79,132]. However, other reasons are also discussed, such as increasing pressure on scientists to produce and publish results very fast, not having enough time to test the algorithms under different conditions and hyper-parameters, still page limits of some journals or conferences and not encouraging authors to submit supplementary materials or electronic appendixes, too strong bias for presenting only positive results which leads to presenting only selected details of experiments, reluctance to report failed results, sometimes using questionable research procedures (e.g. in designing experiments, data collection and inappropriate validation), insufficient statistical analysis and testing [13,31,146].</p>
<p>Let us emphasise that concerns about reproducibility come from the growing observations that many of the modern results in science, including AI, in particular in Machine Learning (ML) and Deep Learning (DL) are very difficult to be reproduced. It is even called a reproducibility or repeatability crisis [31,60]. It also occurs in many fields, see e.g. [96,107,133] but notably it is stronger in AI and, in particular ML [65,109]. This is also supported by analyses of reproducibility elements in published papers from the major AI and ML conferences, which showed that only several dozen percent of them are reproducible; for more details see e.g. [54,119].</p>
<p>In our paper, the aspects of reproducibility of research in artificial intelligence will be considered mainly in relation to machine learning. Our focus is due to two reasons. First of all, nowadays machine learning, and deep neural networks in particular, has become one of the most important sub-fields of artificial intelligence, both due to recent research achievements and the success of many applications. Secondly, they include rather complex methods that place much greater demands on reproducibility than methods previously considered in other areas of science. Moreover, ensuring the reproducibility of machine learning is particularly important from the perspective of increasing human trust in the practical solutions of artificial intelligence, which we previously discussed.</p>
<p>Indeed, most of the modern ML approaches, in particular those based on deep neural networks, use very complex models, which are difficult for potential inspection and interpretation by humans. They exploit many hyper-parameters that need specialised and sophisticated optimization strategies.</p>
<p>Moreover their training, tuning and evaluation often require additional software libraries accompanied with advanced computational resources, e.g. GPU cards, which may also introduce some nondeterminism and randomness. In this context, some authors postulate that guidelines for the reproducibility of ML and DL complex methods should be extended and provide more technically details than earlier proposed ones or known from other fields of science, see e.g. [58,59,114,136].</p>
<p>We see an increase in the interest of reproducibiulity in the AI community, which is reflected in the growing number of papers and the introduction of recommendations for authors' submissions to well-known conferences, such as NeurIPS, ICML or IJCAI, or journals. Nevertheless, in many cases, the perspectives of these best practices are either too general touching upon other issues such as traceability (e.g., [114]) or too specific focusing on data or workflow lineage (e.g., [125]).</p>
<p>Following these motivations, we believe that it is worth writing a survey paper on the reproducibility of artificial intelligence, and machine learning in particular, and their results, addressing researchers interested in these approaches rather than the overly general science perspective.</p>
<p>Firstly, due to the ambiguities in defining the basic concepts of reproducibility, which is visible in the literature, we believe that the terminology issues should be better organized and unified, especially with regard to the type of shared documentation, resources and software environments. Moreover we claim that it is necessary to compare and harmonize existing best practices and guidelines for reproducibility to provide more specialized information and recommendations based on the above-mentioned difficulties for AI complex models and to extend the ones already identified in the surveyed papers for the context of machine learning and deep networks.</p>
<p>Our survey analyzes many papers on reproducibility issues gathered by means of a systematic literature review with additional elements coming from the narrative review approach. We further explain this methodology in the supplementary material to this paper.</p>
<p>The aims of our paper are the following:</p>
<p>(1) To present in section 2 a concise terminological review of various definitions of reproducibility, repeatability and replication of research results that we found in surveyed papers. We will attempt to clarify and harmonize terminology and definitions' confusion currently present in the literature. Later in this text we will focus on reproducibility. (2) To collect and systematically organize the guidelines and recommendations for achieving reproducibility. Unlike many of previous surveys, we want to be more specialized to the perspective of AI/ML researchers. This is why we will approach this issue in two steps.</p>
<p>• Firstly, in Section 3, we will focus on presenting existing recommendations and guidelines from a more general AI perspective by which we plan to identify more precisely the elements in each of their main categories corresponding to availability of code, data and experiment documentations. In order to show how they could be implemented we will also try to link them to some scenarios or concrete proposals found in the literature (e.g. metadata sheets or best practices in the literature). Our special contribution will cover three summary tables with them, which should be more comprehensive with respect to AI reproducibility than typically discussed recommendations. • Then, in Section 4, we will discuss the missing elements and gaps in these general AI recommendations with respect to the characteristics of machine learning reproducibility.</p>
<p>In this and the next section 4.1, we will discuss the needs for their specialization, especially when considering deep neural networks. (3) To discuss in the next two sections 4.2 and 4.3, two cases of reproducibility issues in MLoriented research, concerning namely bio-medical and physical artificial intelligence fields, which will additionally show the needs for specializing and extending reproducibility requirements. We will summarize our proposals of such recommendations in new summary tables (see section 4.4). The aforementioned elements make our survey comprehensive and different from existing surveys. We hope that these elements will contribute to the discussion on reproducibility of machine learning and improve their recommendation both for research and considered applications' perspectives. In the rest of the paper, we have adopted specific text styles to highlight keywords, the structure of long sections, remarks and open issues to ease the reading. In particular, we have used italics with bold to indicate keywords, bold to partition long sections with subsections' titles making the flow of discussion more evident, and italics for emphasizing remarks and open issues.</p>
<p>2 TERMINOLOGY ABOUT REPRODUCIBILITY Different terms come into play when discussing reproducibility: reproducibility, replicability, and repeatability but also robustness and generability. Not all these terms are interchangeable. Authors use the same terms to refer to distinct concepts. Terms result equivalent at a certain level of abstraction but differ when drilling down the steps in the experimental workflow. To a certain extent, reproducibility terms' proliferation and evolution are evidence of the rich and participated scientific debate. However, redundancy and partial overlaps of terms slow the communication among practitioners or even distract from the core focus of the reproducibility challenges.</p>
<p>This section provides a short overview of reproducibility-related terms to guide the readers in the terminological labyrinth and promote the adoption of more stable and less ambiguous terms. We have (i) analyzed an extensive corpus of literature starting and integrating our early review [4], (ii) identified a set of dimensions to compare the reproducibility-related terms, (iii) clustered the terms according to the dimensions, and (iv) elaborated some light recommendations for their use.</p>
<p>The considered corpus of literature includes works focusing on scientific reproducibility (e.g., ACMv1.1 [2], Gent and Kotthoff [45], Goodman et al. [47], Guttinger [56]), previous terminological reviews (e.g., Barba [11], Plesser [115]), and other works specifically addressing reproducibility in AI, ML and DL (e.g., Gundersen [52], Lynnerup et al. [88], Pineau et al. [114]). Based on the analysis of the literature, we have identified the following dimensions:</p>
<p>(1) Workflow components availability -availability of the components originally deployed in experimental workflows (i.e., data, code and analysis as considered by Gundersen and Kjensmo [54], Heil et al. [62], Pineau et al. [114]); (2) Teams -teams involved in the experimentation (i.e., whether or not the experiments was conducted by the same group that is running the reproducibility validation); (3) Reasons -reasons because the experiment or part of it is reconducted (i.e., validating the repeatability of the experiment or corroborating the scientific hypothesis and theory the experiment aims to support. Considering these conceptual dimensions, the reproducibility-related terms used in the literature can be clustered in the following way:</p>
<p>• Most of the literature (including Barba [11], Gundersen [52], National Academies of Sciences, Engineering, and Medicine [104], Pineau et al. [114]) refers to reproducibility as the attempt to replicate experiment as much as possible as the original one, that is by using original data, code and analysis when available. Computational reproducibility, method reproducibility, direct replication and recomputation are used in lieu of reproducibility respectively by Heil et al. [62], Goodman et al. [47], Guttinger [56], Gent and Kotthoff [45]. • The term replicability is highlighted by Claerbout and Karrenbach [30], Lynnerup et al. [88], National Academies of Sciences, Engineering, and Medicine [104], Pineau et al. [114],</p>
<p>where an independent team can obtain the same result using the data, which could be slightly different, and methods which they develop completely independently or change slightly. Raff [119] uses in a similar sense the term independently reproducible, describing a situation where the outcome of a paper can be obtained in a statistically similar form by independently implementing an algorithm from this paper. Furthermore Lynnerup et al. [88], Pineau et al. [114] use another name -robust -for carrying out the experiments with the same data and some changes in an analysis or code implementations. • Some works such as ACMv1.1 [2], Gundersen [52], Joint Committee for Guides in Metrology [71], Lynnerup et al. [88] uses repeatability to indicate a weaker level of reproducibility where the replication of the experiment is achieved by the same team that provided the original experiments. • Gundersen [52] distinguishes the notion of reproducibility from corroboration. While reproducibility is to produce the result of experiments again or make a copy, corroboration is related to gathering new evidence and make more certain a scientific theory. Some of the analyzed literature adopts terms that indicate some level of corroboration when reconducting an experiment. In particular, inferential reproducibility, method reproducible, conceptual replication, replicability, generalizability are used respectively by Goodman et al. [47], Gundersen and Kjensmo [54], Guttinger [56], Peng [112], Pineau et al. [114];</p>
<p>As the first result of our literature analysis, we have delivered the diagram in Figure 1. The diagram summarises terms and their main meaning and the above conceptual dimensions. The diagram is built as a tool to map terms according to their distinguishing characteristics. It might be helpful as a reference for mapping future terminological proposals. It does not represent all the possible combinations of values for the dimensions but highlights as separate boxes those most representative to orienteering into the terminological confusion. The diagram extends the diagram adopted by Pineau et al. [114], mainly concerned with workflow component dimension (e.g., data, code and analysis) with concepts from dimensions that were not considered previously. In particular, the diagram distinguishes two new boxes for relevant values of the previously unrepresented Teams and Reasons dimensions: the "SAME TEAM" and "CORROBORATION", respectively, to gather terms referring to replications that are conducted by the original team of experimenters and to represent replications aiming at corroborating the experimental hypothesis and theory. The diagram associates the terms with their supporting references. Similarly to the analysis conducted by Gundersen [52], the terms are not necessarily the same as used in the reviewed resources, and interpretations have been necessary to highlight the gist of the terms' meaning.</p>
<p>As another result of our literature analysis, we provide some terminological recommendations in Figure 1: we indicate the terms recommended for use in bold. We build upon the existing terms, purposely restraining from minting new terms. Minting new terms would risk adding complexity to an already quite complex picture. In particular, we built upon the concept of reproducibility introduced by Claerbout and Karrenbach [30], which is a seminal work also adopted in a number of more recent papers (e.g., [52,88,114]). According to this concept, reproducibility refers to the ability of an independent researcher to reproduce the same or reasonably similar results using the data and the experimental setup provided by the original authors. Reproducibility should not be confused with other terms such as repeatability and replicability ( [115]). The term repeatability appears in some references, e.g. ACMv1.1 [2] that uses a notion of reproducibility inconsistent with our definition but should be considered to describe an ability of a researcher to repeat his/her own experimental procedures using same experimental setup and data, while achieving reasonably repeatable results that support the same conclusions. Replicability defined in a way consistent with our understanding of reproducibility is the ability of a typically independent researcher to produce results that are consistent with the conclusions of the original work, using new data or different the experimental setup. It is worth clarifying what is reproduced as a result of the above activities and how to understand the term result. Distinguishing among outcome, analysis and interpretation, hypothesis, and theory in an experiment defines the edge between corroboration and other terms in the reproducibility realm. Reproducibility's main aim is to repeat an experiment, while corroboration is more general, it is to support with new evidence or make more certain the conclusion of experiments. Reproducibility is related to experiments, while theories and hypotheses can only be corroborated [52].</p>
<p>REVIEWS OF THE RECOMMENDATIONS ON A GENERAL PERSPECTIVE OF AI</p>
<p>This section reviews general recommendations introducing reproducibility levels and their corresponding features to document experiments. Firstly, it presents an overview of the related works, including the scientific literature and conference reproducibility guidelines. Secondly, it deepens this analysis, collecting and harmonizing specific recommendations from a representative literature. These recommendations will be further extended in the section 4, considering issues that arise when considering modern machine learning, related specific technologies and application contexts (i.e., deep learning, cyber-physical systems, and the biomedical images in medical applications).</p>
<p>3.1 Review of the related works on the reproducibility guidelines The texts collected from our systematic review differs in terms of their typology and their recommendation specificity. We divided them into the following general categories:</p>
<p>(1) Survey studies or position papers, which also contain some recommendations.</p>
<p>(2) Critical texts analyzing problems of insufficient reproducibility in publications and proposing solutions. (3) Specialized reproducibility checklists. (4) Guidelines formulated for paper submissions to major conferences or journals. and other texts, such as recommendations for applying machine learning in other, usually applied, fields of sciences. These collected papers are reviewed below following this order.</p>
<p>Most of the texts discussed so far recognise that ensuring reproducibility for someone's experiments and results requires proper documentation and sharing of the necessary resources. The documentation should include relevant information which has to be specified to a certain level of detail. However, there are clear differences of the authors as to the scope and detail of the necessary information. Note that in the case of experimental research in science, it was previously sufficient to describe the procedure, the laboratory environment and the component factors of the experiment to allow this experiment to be repeated by another team.</p>
<p>Nevertheless, in the case of computer science, and in particular now artificial intelligence, such a paper description is not enough due to the implementation of the method and particular conditions of its running [112]. Even in the 90's, authors such as [30] recommended sharing everything on additional resources (discs in that time) so that anyone could read reports and re-run the provided software and experiments. However, these were not common practices in the previous century until interest in open science and open source repositories grew. Since then, expectations for the details of documentation have increased. For instance, [47] recommends to report all relevant aspects of the experimental design, conduct, measurements, data and analysis to achieve the method reproducibility. Moreover, since the last century, more complex intelligent systems have been developed that definitely require much more experimented effort to fine-tune their parameters than before, especially it concerns modern deep neural networks. General recommendations from survey studies or position papers: In recent years, the highly influential papers of Gundersen and his co-authors have introduced the division of the available documentation of papers in the field of artificial intelligence into various categories. Following Gundersen and Kjensmo [54] these categories were related to method, data and experiments.</p>
<p>The first method category should contain a text description of the AI method, with its high level pseudo-code and Its explanation.</p>
<p>The data category requires sharing the data used in the experiment with an additional text summary. Besides the metadata it should also contain more details on its split into training, testing and validation parts.</p>
<p>The third experiment part -these authors recommended that a proper experiment text documentation should contain the purpose of the experiment and its setup, the specification of necessary hardware and software used in it, information on independent variables, hyper-parameters of the AI program and other environmental settings.</p>
<p>Given these general categories, they proposed to distinguish between three different degrees of reproducibility: Experiment Reproducible (R1), Data Reproducible (R2) and Method Reproducible (R3), depending on which documentation was provided to reproduce results.</p>
<p>The R1 degree is the most expected and demanding, where the same implementation of the AI method, data, and the full detailed description of the experiments could be used to produce the same results. It is also called fully reproducible [112] or the method reproducibility [47]. R2 degree requires the method description and data (so an alternative implementation should be done), while R3 expects only the method documentation. In terms of generality, R1, R2, and R3 stretch from reproducibility to corroboration. R1 reproducible results are less general than R2, which are less general than R3 [54]. Experiment reproducible (R1) corresponds to the reproducibility as defined by [30,52,88,104,114,144] and meant in this survey. Data Reproducible (R2) and Method Reproducible (R3) aim at testing the generality of results rather than assessing the experiment's reproducibility. In term of documentation required the order is inverted [54]. Not having original data, code might be not strictly necessary for generalising an experiment, but it makes it harder to reach Experiment Reproducibility. In general, the increased documentation efforts lead to decreasing efforts to reproduce other research. Also, it might be challenging to state anyone has corroborated a theory resulting from a given experiment if he/she cannot access data and codes, and none has ensured the original experiment's reproducibility.</p>
<p>Gundersen in [53] extended these guidelines on different categories by providing more detailed recommendations based on best practices from literature and practical experiences. These were separate lists of detailed recommendations for the following categories: data, source code, description of the method, experiments. It is further extended in the latest paper [52], where the authors extended an interpretation of previous documentations into types of text, code and data. Depending which of them are shared with independent researchers, these re-defined four types of reproducibility are following: R1 (description), R2 (code), R3 (data), and R4 (experiments which should contain the most complete documentation of the experiments including data, code in addition to the textual description of the experiments with all details and analysis outcome).</p>
<p>The similar categorization of reproducibility could be found in [62]'s proposal, where the three degrees of the reproducibility standards for ML, bronze, silver or gold, are defined basing on availability of data, model, and code, as well as other analyses or programming dependencies. Critical analyses of insufficient reproducibility in ML publications: Based on the analysis of ML papers from ICML and NIPS conferences Tatman et al [136] distinguished three levels of their reproducibility: low (paper only), medium (sharing code and data) and high (also environment used to run this code over the data) and recommended some practical steps. For instance, for the medium level they advocate of making the project modular, documenting the original environment including all software dependencies, distributing code or data as open source with appropriate licenses, reproducing own project on a new machine, if possible, before sharing it, also they promote using appropriate hosting services for providing the code or data. For high reproducibility they refer to the executable environment with linked all libraries and dependencies necessary to rune the code on a new machine or another operating system. Recall that Peng also referred to improving reproducibility by linking the environment with code and data [112].</p>
<p>In [136] recommendations its sharing is discussed in three possible technological options:</p>
<p>• using special hosting services (in particular non-profit ones),</p>
<p>• providing a container (in particular dockers, which allow researchers to combine code, data and all environmental dependencies needed to run the code in a single portable formats) • or a virtual machine image.</p>
<p>Following their recommendations all these efforts should better support reproducers and try to minimize the number of steps they need to perform. For instance using special scripts that can be called to read data, prepare the appropriate environment and run the code with ideally a single command. Note that [62]'s proposal of the gold standard also recommends the authors should also prepare this analysis reproducible with a single command -which is certainly the most demanding with respect to full automatization of the reproducibility process. Specialized reproducibility checklists: Pineau et al. [114] proposed some activities to improve reproducibility in AI and ML community. The best known refers to NeurIPS conference special reproducibility program, including also the submission paper and code procedure and defining the Machine Learning Reproducibility Checklist as its part. In this checklist they specified the necessary elements to be documented and made public with respect to the following categories: model and algorithm, theoretical claims (concerning the rules of writing the paper), datasets used in experiments, shared code including dependencies specifications, all reported experimental results (with all details for the experimental setup, splits of the dataset, hyper-parameters, training details, definitions of evaluation measures, and description of the computing infrastructure used). Guidelines formulated for paper submissions to AI conferences and journals: Similar reproducibility guidelines were also proposed for major AI/ML conferences or scientific journals. The representative examples are IJCAI 2021 Reproducibility Guidelines, AAAI conf. Reproducibility checklist (2021) or sub-parts of the reviewer's instructions for ICML, CVPR. Some of these guidelines are inspired by the above discussed Pineau et al. [114] and extend it in some points. In general they require: sharing the code and data as the basic recommendations to authors, a clear description of model/algorithms -not only its implementation code. For the data set they also indicate providing its description with respect to its characteristics and steps of its data cleaning and preprocessing to the final representation. One can notice many differences among these guidelines on how deeply experiments, its parametrization, etc., should be documented. Nearly all documents mention hyperparameters. Some of them just require to list them, while others ask for more -ranges of their values and how the best ones were tuned. Many documents ask for details of splitting data sets into training, testing and validation ones; precise lists/ definitions of evaluation metrics, number of training and evaluating runs; average training and inference time.</p>
<p>A few guidelines are even more demanding, e.g, IEEE T-IFS Guides for deep learning submissions [49] contains over one page long list with a very detailed checklist of the network topology, types of functions, internal parameters, initialization, random seeds, details of running learning algorithms. It is also more specific to the details computational experiments and the used software.</p>
<p>Note that [88] provide similar recommendations for ML in robotics, by focusing on the reproducibility of computation experiments on real robots. They also stress the role of managing properly the software dependencies, distinguishing between experimental code and library code, and documenting the measurement metrics, which is essential for reinforcement learning.</p>
<p>Recommendations for applying machine learning in other fields of sciences: In some fields of scientific research, other recommendations are also proposed. For instance, [146] contains several postulates for more detailed reporting of results of machine learning when applied in medical and health application (we will discuss them in more details in section 4.2). The position paper [96] discusses repeatability, reproducibility and replicability of empirical studies in psychology and computational neuroscience and concludes with some recommendations to improve them -one of their conclusions says that all details relevant to running the code and experiments should be shared even if they seem to be scientifically inessential. Yet another example is [8], which provide best practices in machine learning applications in the modern chemistry, where, for example, they pay more attention to the provenance of the data, ensuring their quality and their appropriate pre-processing to the representation relevant for the further analysis.</p>
<p>It should also be noted that most of these guidelines, even in the form of a checklist, are some kinds of recommendations for authors in submitting papers, or indications for editors and reviewers. Despite the inspirations of checklists from other domains, they are rather advices or indications for expected authors' routing or the organization policy. However, in some studies some of them are reformulated to more precise requirements, which are expected to be met.</p>
<p>For instance Gundersen et al defined in [54] several metrics based on the variables in categories R1, R2 and R3 to quantify reproducibility in their survey of papers coming from the top AI conferences (where each of these paper was evaluated with respect to these metrics). Another approach toward quantifying reproducibility in machine learning papers was undertaken by Raff in [119], where he defined 26 attributes for checking and evaluating each of considered papers. Recently Gundersen et al. came back to assessing how the available open source and commercial machine learning platform support the reproducibility of experiments [55]. Similarly to earlier survey [54], they defined 22 variables for their main categories, which should be verified whether the given platform satisfies them or not, and then three metrics expressing levels of reproducibility were defined on the basis of fulfilling these variables. It allowed them to quantitatively compare and rank these platforms.</p>
<p>Finally taking about more practical consequences of guidelines, we may also refer to special datasheets by [43], which specify how to document the motivation, composition, collection process, recommended uses for data deployed in the systems and experiments; model cards by [97] ease the description of model's intended use cases limiting their usage in contexts for which they are not well suited; factsheets [7] provide a template for describing the purpose, performance, safety, security, and provenance information to be completed by AI service providers for examination by consumers.</p>
<p>A reasoned summary of recommendations</p>
<p>The section presents a summary of the recommendations starting from the most representative checklists and guidelines for reproducibility we identified in the previous section. We have combined complementary perspectives including reproducibility guidelines from the most prominent AI /ML conferences (i.e., International Joint Conference On Artificial Intelligence (IJCAI 22) [140] and the official NeurIPS 2020 code submission process by Pineau et al. [114]), industry-lead documentation proposals (i.e., datasheets [43], model cards [97]), and the most up-to-date reproducibility factors adopted by Gundersen et al. [55] to evaluate the reproducibility platforms.</p>
<p>We organised these extensive set of recommendations along three categories, in the spirit of factors adopted in the levels of reproducibility by Gundersen et al. [53,55]: Experiment, Method, and Data. For each of these categories, we identified • the reference term (i.e. feature) • the matched recommendation(s) • the mean to comply with the recommendation(s), namely via metadata (M), supporting platform (P), or the scientific material, paper, report etc. (S) • the source guidelines from which the recommendation comes.</p>
<p>We have grouped the alike recommendations from distinct guidelines together. One single recommendation can have more than one supporting source. Hence, an effort was made to balance deduplication of redundant recommendations and the use of original recommendation phrasing. As the result of our work, the three tables, Experiment (Table 1), Method (Table 2) and Data (Table  3), report the recommendations for each of the considered categories. Overall, the tables collect and harmonize an extensive set of recommendations providing a more comprehensive view than those typically discussed in every separate checklist. Some aspects in the list of recommendations might deserve more attention than others, depending on the nature of the experiments and the kind of technology involved.</p>
<p>Feature</p>
<p>Recommendation Where Source guideline Results Document the results and the analysis. P,S Gundersen et al. [55] Hyper-parameters The range of hyper-parameters considered, method to select the best hyper-parameter configuration, and specification of all hyper-parameters used to generate results. For alla the (hyper)parameters used for each model/algorithm and tried development of the paper. Mathematical Setting For all models and algorithms, A clear description of the mathematical setting, algorithm, and/or model.</p>
<p>S</p>
<p>Pineau's checklist v2 [114] Algorithm Complexity For all models and algorithms, An analysis of the complexity (time, space, sample size) of any algorithm.</p>
<p>S</p>
<p>Pineau's checklist v2 [114] Pseudo code Support for pseudo code. P,S Gundersen et al. [55] Table 2. Recommendation for methods. The first and second columns summarize what to describe; the third is where the description is likely to be provided (i.e. in metadata (M), the platform(P), or the scientific material, paper, report etc. (S)); the fourth column includes the guidelines from which the recommendation comes</p>
<p>Moreover, not all the recommendations are equally straightforward to be implemented. The scientific material, paper and report (S) includes typical documentation practice in the scientific community. Specific guidance is usually provided by conferences and journal guidelines and very often are requirement for the acceptance. Researchers are at least in their general aspect rather accustomed to them. Hence, they might be easier to take up and we will not discuss them in more detail. Whereas recommendations to be implemented via a platform (P) or metadata (M) require specific supporting actions such as adopting third-party platforms and metadata models, which might deserve a more specific discussion. For this reason, we briefly discuss supporting platforms and metadata models.</p>
<p>Supporting platforms. As for to what extent existing platforms support these recommendations, Gundersen et al. [55] surveyed 13 machine learning platforms based on its subset of recommendations. Gundersen et al. [55] has chosen the platforms based on the literature on reproducibility including OpenML, MLflow, Kaggle, Codalab, StudioML, plus the most commonly used machine learning platforms provided by Amazon, Microsoft and Google. Though Gundersen et al. 's recommendations are a courser subset of those included in the tables presented in this paper, the lack of support exhibited by Gundersen's analysis might provide some insight to spot open issues. In particular, according to [55], none of the considered platforms supports the statement of how the analysis supports the claims. Among the less supported features Gundersen et al. [55] includes the code, data, experiment citation; the native provision of code and data repositories; the provision of metadata. According to [55], platforms do not necessarily need to include all the features natively. They might rely on third-parties services to integrate these functionalities. For example, source code management systems such as Git and GitHub can be adopted for sharing data and code. Permanent URI to make data sets and code citeable can be mint via services like Zenodo, Figshare, W3ID and Datacite. However, combining functionalities provided by different parties is often left to the experience of those who share the experiment. It might involve a set of different practices, which, if not standardized, might be disorienting for reproducers that are not necessarily into data stewardship and software engineering.</p>
<p>Metadata. As for metadata, Gundersen [52] suggests the provision of basic metadata only, but the tables show that many other features can be tracked via metadata. "The FAIR Principles have pointed out that metadata is a crucial aspect when making an experiment reproducible. Over the  [114] Excluded data For all datasets used, An explanation of any data that were excluded, and all preprocessing steps.</p>
<p>P,S,M Pineau's checklist v2 [114] Preprocessing cleaning and labelling Was any preprocessing/cleaning/labeling of the data done (e.g., discretization or bucketing, tokenization, part-of-speech tagging, SIFT feature extraction, removal of instances, processing of missing values)? If so, please provide a description.</p>
<p>P,S,M Datasheets [43] Raw data Was the "raw" data saved in addition to the preprocessed/cleaned/labeled data (e.g., to support unanticipated future uses)? If so, please provide a link or other access point to the "raw" data.  Table 3. Recommendation for data. The first and second columns summarize what to describe; the third is where the description is likely to be provided (i.e. in metadata (M), the platform (P), or the scientific material, paper, report etc. (S)); the fourth column includes the guidelines from which the recommendation comes past few years, awareness of this challenge has led to efforts to establish standards and procedures for better data and metadata collection and sharing practices" [15]. In particular, a quite rich set of standards have emerged in the context of open data Government and open science. For example, the World Wide Web Consortium (W3C) has developed machine-actionable metadata models for documenting provenance (PROV-O [93]), datasets (DCAT [3]), quality annotations and metrics for data and other kinds of resources (DQV [5,6]). Garijo et al. [42] have provided a holistic, Linked Data compliant, and ready-to-use solution to document workflow specifications and their executions, which exploits PROV-O [93], P-PLAN ( [41]) and the Open Provenance Model for Workflows (OPMW). Mora-Cantallops et al. [100] have discussed the importance of metadata models for traceability in AI.</p>
<p>Although not specific to AI experiments and systems, the models mentioned above offer some excellent standing and a backbone for describing data, actors, other kinds of entities, and how these might relate in experiments. Such a standing needs to be refined and extended to capture the gist of specific AI experiments. AI-related controlled terminologies are required, for example, to complement the backbones with the hyper-parameters, tasks and metrics for AI techniques. Adopting a backbone, which is defined according to linked data best practices, offers the ability to combine different models and terminologies as needed, easing the tailoring of the such backbone with the required AI-specific and community-governed refinements. Some AI-specific and community-governed metadata models have been proposed (see ML-Schema [116], Amazon ML Artefact description [126], MEX [37]). However, shared AI-related metadata standards have not yet been established. AI-specific and community-governed metadata models hardly relate to more general open science metadata standards, often reinventing patterns for dealing with trans-domain entities. The current proposals are far from being broadly adopted when documenting AI experiments. The lack of standardized specialization for describing the artefacts involved in the AI experiments and their descriptions makes moving experiments among platforms difficult. In contrast, ensuring the portability of experiments and technology might be crucial to satisfy specific constraints arising from scientists and developers and non-academic applications with particular constraints and preferences on the technology to use.</p>
<p>MACHINE LEARNING -SPECIALISING THE REQUIREMENTS</p>
<p>Nowadays, machine learning, and deep learning in particular is considered the most prominent area in artificial intelligence, due to its wide applications in solving practical problems in a number of areas, from recommendation systems, through medical diagnostics to industrial systems and autonomous agents. Therefore, ensuring reproducibility in ML research becomes particularly important and urgent task, as irreproducible papers and their research artifacts (e.g. neural network models) proliferated to the level of production systems hamper the efforts towards trustworthy AI [145], making it harder to ensure robustness, traceability and accountability in real-world applications 1 , and in particular safety-critical domains where human welfare may be harmed (e.g. medical diagnostics, autonomous driving) [84]. The following paragraphs provide the characteristics of ML and DL in the context of reproducibility, consider the features of publications that might be positively correlated with better reproducibility, and extract from the literature the issues that can render irreproducible even a well-documented ML research.</p>
<p>ML &amp; DL characteristics. Althought machine learning covers different categories of methods, the reproducibility issues largely relate to the currently most popular approaches to creating complex systems. In addition to neural networks, they also include ensembles of predictive models or hybrid solutions integrating multiple algorithms. Recall that they are often considered the so-called black boxes in not offering users information on how they operate inside, what they learned and how they came to a specific decision [50]. The lack of interpretability of models, not supporting of understanding of their more and more sophisticated operations, combined with the processing of increasingly larger and complex data, additionally reduces the reproducibility of such machine learning methods.</p>
<p>Many of the popular ML approaches, in particular those using deep neural networks, are not following the model-driven paradigm. They rather result from intensive, repeated experiments with data, in which the model architecture is constructed, along with quite complex parameter tuning. Moreover, modern ML methods often require the use of special programming languages, optimisation libraries and hardware solutions such as Graphical Processing Units (GPUs) to accelerate computations. This experiment-and-data driven approach also requires wider reproducibility recommendations than those considered in other areas of artificial intelligence. Therefore, this section is concerned with reproducibility in modern machine learning with neural networks, as this shows especially well the need to extend the requirements discussed in the previous section.</p>
<p>The literature we have collected for this review unanimously agrees that the level of reproducibility in current machine learning research is not satisfactory [51,53,84,85,114]. Tatman et al. [136] were among the first, who considered irreproducibility an issue for practitioners applying a new ML method within their own domain. They point out that non-academic ML practitioners often consider reproducing the results of a paper the first step in applying the method described there to new data or an entirely new domain. Hence, being able to reproduce these results to a reasonable degree provides an evidence that the method is worth to be considered, which is also being discussed by other researchers, see e.g. [58,114].</p>
<p>What makes a ML/DL paper reproducible? However, while the short paper of Tatman et al. gives an important motivation for considering the practical side of reproducibility in machine learning, it lacks a statistical evidence to support the claims, and sets the directions towards improved reproducibility, rather than provides detailed recommendations. Conversely, the work of Raff [119] focuses on the statistics from investigating reproducibility of 255 ML-related papers published from 1984 until 2017. Raff correlates the reproducibility of a paper with some of its features, using the notion of being "independently reproducible", which means the author was able to independently implement an algorithm from the investigated paper and reproduce the majority of claims in this paper applying reasonable and standard libraries. While 26 features are defined, some of them are highly subjective, such as paper readability or rigorous versus empirical research type. Among the more objective features (listed as unambiguous or mild subjective in [119]) the availability of pseudo code, specification of hyper-parameters and computing resources, and a higher number of tables and equations were determined as those being positively correlated with reproducibility. Interestingly, this study shows that releasing the code is not sufficient for independent reproducibility. According to Raff's comment "the inability to reproduce results without code availability may suggest problems with the paper" due to an insufficient explanation of the method or the lack of important implementation details.</p>
<p>This stands in opposition to the earlier survey analysis of reproducibility of 400 research papers in AI main conferences from 2013-2016 by Gundersen and Kjensmo [54] which identified 20 binary variables referring to their categories of documentations. However they only identified if these variables are represented in the texts of these papers, and did not re-implement methods or used codes in experiments. Their results show that only between 20%-30% of these variables are sufficiently documented. Here we may repeat [136] that [54] did not investigate if their variables actually impact the reproducibility. The more pragmatic view of machine learning, as stated in [136] is concerned with the question what artifacts should be included with a publication to allow others to understand the method and to apply it in their own work or research?</p>
<p>Also Lopresti et al. [85] focus on improving the chances to reproduce a research paper by providing better documentation and explanation of the used ML pipeline. Their investigation concerns entries in competitions associated with conferences in patter recognition and document analysis. Authors claim that while reproducibility of the paper's outcome (e.g., predictions) using the original artifacts (code, data) is often achieved during the competitions ran on pattern recognition conferences, most of these competitions do not achieve replicability by an independent team using methods which this team developed on the basis of the papers. Therefore, they focus on the documentation of the methods and experiments, acknowledging the conclusion of Raff, that making the source code available does not guarantee reproducibility, because additional meta-parameters and details of the third-party or system-level software might be needed to obtain an outcome of the experiment which is similar enough to the original results. If authors who publish code do not document their experiments well, then sharing the artifacts per se does not improve the chance of reproducing these experiments.</p>
<p>From the critical literature analysis carried out in this paragraph, we conclude that no study published so far provides a definitive answer to the question of what features of a scientific paper are those highly correlated with the reproducibility of this paper. Hence, there is a need for further investigation and initiatives encouraging AI/ML authors to follow the known reproducibility guidelines.</p>
<p>Reproducibility guidelines -beyond documentation. The main paradigm in machine learning has shifted from classic supervised methods and hand-coded algorithms to neural models with a huge number of parameters, which are both data-intensive and computation-intensive. The methods for communicating research results have to change accordingly, as the classic form of a scientific paper no longer describes all the necessary aspects of data, code, workflow and meta-parameters tuning in enough detail to reproduce them, both in terms of the proposed method and its experimental evaluation.</p>
<p>Accordingly, the reproducibility guidelines proposed in literature related to general AI define best practices pertaining to methods, data, and experiments (see Section 3), but often focus on documentation of these aspects, overlooking technical issues that may render machine learning works irreproducible, particularly if deep learning models are applied. The dependencies between reproducibility and research artifacts considered by Tatman et al. [136], Raff [119], and explained in more detail in [120] show that mathematical explanation, high-level pseudo-code and toy examples are insufficient to achieve reproducibility in machine learning. This view of [85,119,120] is corroborated by [16,26,113], that demonstrate randomness and variance in experiments with deep neural networks learning, which strongly depends on the algorithmic choices (e.g. neural network architecture), but also on the toolchain being applied, and the used hardware.</p>
<p>Indeterminism is an important problem in machine learning that can appear even when the original code and data are available. It can be caused by software, e.g. third-party libraries, which might use (directly or by invoking lower-level libraries) stochastic processes [88], but also by modern hardware, as the Graphical Processing Unit (GPU) do not guarantee to yield exactly the same results on different GPU architectures and their variants [149]. What important, indeterminism might occur not only because of using different software or hardware components, but even due to different configuration of the same software, which can be observed for the popular TensorFlow and Keras libraries for deep learning [26]. Unfortunately, the existing works on reproducibility address these problems only partially. Researchers in software engineering offer technical solutions to circumvent indeterminism [105], but usually do not investigate the improvements in machine learning reproducibility due to applying such techniques. Moreover, elements of randomness may appear in some of the methods themselves, especially when searching for solutions or optimizing parameters.</p>
<p>In the next subsection we analyse the reproducibility issues and solutions specific to deep learning, whereas the next two subsections review how reproducibility is considered in biomedical and technical applications of machine learning. Upon these analysis we attempt to extend some of the guidelines listed in Table 1 and Table 2 providing aspects specific to either DL methods or some application areas. We relate these extended guidelines to the tools and software platforms that might be of interest to DL researchers pursuing better reproducibility in their projects.</p>
<p>Reproducible deep learning</p>
<p>This subsection develops further the discussion of reproducibility issues in DL that we have identified in the literature. It summarises recent results concerning tracking of DL experiments, dealing with details of the technical stack, and managing the assets. The main aim of this part of our survey is to add understanding to the works on reproducibility problems in machine learning, providing a more detailed analysis of the methods for reproducible training of DL models and the techniques for dealing with randomness and indetermism, as we identified these issues as the major obstacles to wider adoption of reproducible deep learning. Tracking deep learning experiments. A comprehensive taxonomy of tools for tracking ML experiments is described by Quaranta et al. [118]. They address their survey to data scientists, considering that achieving reproducibility can be even more challenging in heterogeneous data science teams, when some of the members may lack software engineering expertise. Thus, authors reviewed a large and varied set of tools for reproducible ML experiments 2 , focusing on tools for data and code maintenance, but rather neglecting randomness and indeterminism. Command-line tools, e.g. DVC [35], are recommended in [118] for researchers with good software engineering experience, as they are flexible and easy to integrate with Git-type version control. If the crucial feature is the graphical comparison of experiment runs, then API-based software, e.g. MLflow [25] should be considered. The analysis of this work shows, that the issues of documenting ML, and in particular DL experiments should be discussed in a broader context, considering the technical stack details, because if such details are irreproducible, they easily render the whole research work irreproducible. How to deal with details of the technical stack. A detailed guidance for reproducing existing deep learning models/algorithms by re-implementing the results of a published paper is given by Banna et al. [10]. The purpose of this report is to define a process for reproducing a state-of-the-art machine learning model at a level of quality suitable for inclusion in the TensorFlow Model Garden 3 . This work leverages the notion of an exemplary implementation, and proposes several checklists, that serve at particular stages of engineering such an implementation:</p>
<p>• General checks: model purpose, code availability, language/framework/libraries, networks referenced. • Model and design checks: model architecture, model sub-networks, model building blocks, custom layers, loss functions, output structure. • Training and evaluation checks: dataset used, pre-processing functions, output processing functions, testing and target metrics, training steps.</p>
<p>A step-by-step explanation of the creation of a DL model compatible with the TensorFlow Model Garden is described on an example of a YOLO family network. This work leverages DevOps/MLOps practices to foster reproducibility, but again treats the technical stack issues (software frameworks and libraries, specific hardware, etc.) in deep learning as secondary.</p>
<p>How to train to make model reproducible. A very different perspective is taken by the work of Chen et al. [26], which proposes a systematic approach to training reproducible DL models. This approach consists of three parts:</p>
<p>• a set of general criteria to thoroughly evaluate the reproducibility of DL models for two different domains;</p>
<p>• a unified framework which leverages a record-and-replay technique to mitigate softwarerelated randomness and a profile-and-patch technique to control hardware-related nondeterminism; • reproducibility guidelines which explain the rationales and the mitigation strategies on conducting a reproducible training process for DL models. Chen et al. focus on the reproducibility of DL models during the training process. They follow the definitions used in [114], where a particular piece of work is considered as reproducible, if the same data, same code, and same analysis lead to the same results or conclusions. The same training process requires the same setup, which includes the same source code (including training scripts and configurations), the same training and testing data, and the same environment. Authors of [26] point to the lack of systematic guidelines in the literature, that take these issues into account. They define two factors that make DL (often) irreproducible: randomness in the software and non-determinism in the hardware, whereas there are four types of assets to manage in machine learning in order to achieve model reproducibility: resources (e.g., dataset and environment), software (e.g., source code), metadata (e.g., dependencies), and execution data (e.g., class labels). Patching 4 methods that apply a set of changes to a program or its supporting data could be necessary to fix randomness issues in the software. Prior work shows that other assets should not be managed with the same tools (e.g., Git) as used to manage source code. Hence, version management tools (e.g., MLflow [25], DVC [35]) are recommended for managing DL assets. Interestingly, Chen et al. show that the existing general guidelines [97] are insufficient in some scenarios of deep learning, due to the undocumented software dependencies. The paper provides general guidance, that should be followed in order to achieve reproducibility in deep learning, but defines also a more rigorous procedure, that consists of a number of steps and includes special tools (record-and-replay), which makes it possible to achieve reproducibility in DL tasks:</p>
<p>(1) Documentation frameworks such as Model Cards [97] should be used to document training, documenting also software dependencies. (2) Tools such as DVC 5 or MLflow 6 should be applied to manage the experimental assets, using virtualisation techniques to provide a complete runtime environment, which mitigates the risks of non-determinism. (3) Appropriate metrics, general or domain specific should be used to document the model evaluation criteria. (4) Pre-set random seeds or a record-and-replay technique has to be applied in order to mitigate sources of randomness in the software. (5) Patching methods should be considered to mitigate non-determinism from hardware. (6) Unsupported non-deterministic operations should be documented, and replaced by their deterministic alternatives.</p>
<p>Managing the assets in deep learning. The management of assets in DL experiments can be also accomplished using dedicated software, such as dtoolAI [59], which allows to automatically capture data inputs and model hyper-parameters while training a DL model, and to distribute those metadata with the model. What is more, dtoolAI 7 allows attaching unique URIs (Universal Resource Identifiers) to datasets hosted by cloud services, allowing datasets to be uniquely identifiable. This is a Python software package integrating with the Keras library and based on dtool [108], an application programming interface (API) and set of tools for management of heterogeneous data.</p>
<p>A different approach to managing DL assets is demonstrated by DeepForge [18], a platform for deep learning that enables rapid development of DL models in a cloud-based infrastructure. It leverages the concept of model integrated computing [134] and provides a hybrid text-visual programming platform. DeepForge 8 facilitates reproducibility of DL experiments using automatic versioning during development, and enabling versioning both the code and the binary artifacts (e.g. data and trained models) for the given experiment. Dealing with randomness and indeterminism. Whereas managing the provenance of a computational objects is a cornerstone of reproducibility, the issues of randomness and indeterminism from both software and hardware also need to be considered if we want to obtain a fully reproducible DL experiment, which yields quantitatively adequate results when repeated by others [26]. Thus, it is necessary to investigate closely the origins of the problem [113,149]. As shown by the experimental results in [113], identical training runs of a deep learning method, with same algorithm, network architecture, and training data still can produce different models characterised by different accuracy. This is caused by a number of issues, that may be divided into two categories, named in [113] algorithmic factors and implementation factors. The same taxonomy is used in [149].</p>
<p>The algorithmic factors are related to model design choices, and they appear when stochastic components are utilised to improve model accuracy and training efficiency. Examples are: random initialisation of weights, stochastic layers (e.g. dropout) in the network, ordering of data shuffling, and stochastic data augmentation.</p>
<p>The implementation factors are related to indeterminism introduced by either software or hardware components of the technical stack. Commonly used deep learning libraries, such as TensorFlow and PyTorch, perform data pre-processing in parallel to improve speed, but this approach can change the order of training data. Differences in training data ordering result in different variance caused by floating point operations. Moreover, different runs of a DL method using such libraries sometimes use different elementary operations on the GPU (from cuDNN lower-level library), which introduces differences between the resulting models. The GPU itself can introduce implementation noise due to different orders in the sequence of floating-point operations that result in differences in rounding error propagation [75]. This phenomenon is exemplified by Morin and Willets [101], who found that randomness in the results of image classification by ResNet models is dominated by non-determinism from GPU rather than by algorithmic factors. According to [149] these problems are observable across a wide range of parallel processing units, including TPUs. Moreover, reproducibility could be compromised if the model training is not convergent, or if performance depends on the size of testing data. Liu et al. [84] found that low convergence of the training process or testing on data of considerably different scale than those used while training can lead to DL model performance that is difficult to reproduce.</p>
<p>Because most of the algorithmic factors of irreproducibility are controlled by generators of pseudo-random numbers, one can avoid these factors in a reproducible deep learning experiment just setting the random seeds at the beginning of each run [89]. Note that the pseudo-random behaviour often needed for training efficiency will be present within a single run [113]. The nonstochastic aspects of a deep learning method that can compromise reproducibility, and thus have to be chosen carefully are hyper-parameters [86], and the choice of the activation functions [128].</p>
<p>Elimination of the implementation factors that compromise reproducibility requires to take a number of steps, which depend on the DL task at hand and the software/hardware setup being used. Pham et al. list some general recommendations for these steps:</p>
<p>• The use of multiple processes that cannot guarantee data order should be avoided. Techniques facilitating reproducibility of GPU workloads, that may be useful for DL tasks, have been proposed by computer architecture researchers [29,72]. A major role among the software engineering solutions that help to overcome the technical causes of irreproducibility is played by the containerisation techniques, such as Docker [95]. These techniques make it possible to more easily create reproducible software environments, for example by creating a Docker image based on a repository path or URL [40]. Although standard dockerfiles are helpful, they do not solve a number of problems related to the implementation factors of reproducibility, including problems specific to the operating system and hardware platform. An example of software technology that can solve these problems is described by Navarro et al. [105]. Their paper gives an extensive discussion of the sources of irreproducibility in the Linux API calls and the x86-64 ISA instructions, and describes DetTrace, a reproducible container abstraction for Linux implemented in user space. All computation that occurs inside a DetTrace container is a pure function of the initial filesystem state of the container. It is shown that DeTrace can be used to render deep learning tasks reproducible: to check the reproducibility of TensorFlow workloads, authors recorded the value of the loss function at each step during training. These values are irreproducible when running natively, even with serialized TensorFlow, due to, e.g., randomization of the training set. DetTrace [105] renders these workloads reproducible without any code changes. Reproducible containers can be used for a variety of purposes, but apparently, it may be of interest also for those, who want to conduct reproducible deep learning research. DetTrace seems to solve the technical side of DL reproducibility (implementation factors) without requiring any hardware, operating system or application changes, but with some time overhead that depends on the DL task itself.</p>
<p>Reproducibility in artificial intelligence for biomedical applications</p>
<p>The recent breakthroughs in AI and ML have demonstrate a great potential in supporting many biomedical decision problems, such as disease risk prediction [46] or diagnostic image analysis [14]. Not by chance, a new paradigm termed "High-performance medicine" has been anticipated by Eric Topol, who envisaged "the convergence of human and artificial intelligence" as a promising perspective [139]. However, despite the great expectations and promises of AI in the medical field, the actual impact of AI-powered tools on clinical routines appears anticipatory, as many issues still prevent their full uptake in real-world practice. Among these issues, reproducibility is among the most critical ones, coupled with reliability, accountability, liability, and overall trust [81]. The publication-based dispute about the reproducibility and transparency of an AI biomedical solution for the screening of breast cancer is paradigmatic of the cogency of the topic [57,94].</p>
<p>An evidence of this raised interest is the noteworthy share of papers retrieved by our queries that focused on the biomedical field (i.e., 18 out of 81, accounting for the 22% of the papers, though not all relevant for our purposes). These papers addressed the recently surging demand for transparency and trust, though it is worth noting that reproducibility is an old quest for the results coming from the statistical and ML analyses of medical data [20,61,70].</p>
<p>As it happens in the broader scientific community, the terminology behind reproducibility varies a lot within the biomedical application domain, so demonstrated the papers that we retrieved as described in more details in the supplementary material in Section B. Among them, many work referred to a general concept of reproducibility, with the goal to raise awareness on its importance [21,57]. Other works made an explicit reference to one of the definitions of reproducibility we have overviewed in the previous Section 3 [9,121]; others introduced additional notions so to take into account the peculiarities of the biomedical domain [91], and some others considered the whole spectrum of reproducibility dimensions (i.e., the four quadrants) as introduced in Section 3 [146]. The most relevant works debated the specificity of the biomedical domain and provided recommendations on how to cope with it [91,121,146]. In the following, we will first overview the criticalities that affect reproducibility in the biomedical domain, and then we discuss the guidelines or recommendations that are provided by the most relevant papers retrieved.</p>
<p>Criticalities of AI reproducibility in biomedical applications. AI reproducibility in the biomedical field is affected by several factors. Some of them are those common to all scientific domains and comprise the lack of a complete documentation of the development process, the training pipeline and the development choices done as well as the lack of a suitable management of the randomness and indeterminism characterizing the solutions based on DL [12,121]. The lack of code sharing is even more critical, as it is a less common practice in the biomedical community, where the use of open-source repositories is traditionally not common [12,91]. Although awareness on this issue is increasing, still a low share of the published papers releases the code or any supplementary materials [12]. Further to this, other challenges appear to be peculiar of the biomedical domain as it has been highlighted by many of the retrieved papers [12,91,111,121]. We summarize these challenges as follows.</p>
<p>• Difficulties in sharing data: biomedical data are privacy sensitive and their sharing policies are subject to specific authorization by individual owners and clinical institutions. Privacy preservation and suitable anonymization techniques should be applied to avoid any leakage of sensitive information. Nonetheless, these are still partially used and sometimes they may hinder the utility of the shared data [36,91]. Consequently, very few publications share the datasets used for training and testing their AI models. This highly affect the repeatability and reproducibility dimensions [12,91,121] • Lack of heterogeneous and multi-institutional data: most of the works on AI in biomedical applications make use of ad hoc datasets, often collected by a single clinical institution as retrospective data. These datasets unfortunately exhibit a low capacity to represent the large variability of real-world data. As explained for instance for the medical imaging data [22], variability may be caused by the heterogeneity and differences among clinical institutions, in terms of different patient population, clinical protocols implementation, operators' practices as well as the data acquisition procedures and equipment. Additional variability comes from the heterogeneity of individual patient's characteristics, in terms of anatomical, functional and response properties. Moreover, as the clinical knowledge evolves, also the data acquisition protocols, the clinical guidelines and standards of care, the reference ground truth, and the acquisition devices change over time, thus causing the data and the biomedical decision questions to experience variability and drift over time [124]. This variability strongly affects replicability and generalizability dimensions of reproducibility. Specific measures to cope with this condition should be adopted along the entire life-cycle of AI development [81], and particularly at the moment of data selection, by gathering heterogeneous datasets, acquired from diverse institutions and representing a large population share [91]. • Lack of standardized evaluation policies: the variability introduced in the previous point should be managed also from a statistical standpoint when presenting the outcomes of an AI model. The statistical evaluation based for instance on suitable stratified cross-validation techniques that avoid any forms of data leakage between training, test and validation (e.g., in medical imaging analyses by separating data at patient level and not at slice-level) are not often adopted. This issue mainly affects the development process, but, when coupled with the lack of documentation, it strongly affects the possibility to reproduce the claimed results.</p>
<p>As recommended by Wojtusiak [91], statistical model-quality measures, some characteristics of the learning algorithms, including learning curves, attribute selection curves, and hyperparameter tuning curves are not often reported in the published papers, while they should be included accompanied by relevant statistical measures. • Lack of reporting standards: an extensive and transparent reporting of a study, experiment or trial have been always considered as a key issue to adequately assess any risk of bias as well as the safety, efficacy and potential uptake in clinical practice of any prediction models or clinical interventions [92]. Standardized reporting strategies have been devised for this purpose, such as the TRIPOD [33], the CONSORT [127] and the SPIRIT [23] checklists. Nevertheless, the current version of these checklists does not include items specific to AI and ML methods. Most of them are currently working to fill this gap with dedicated releases, such as the TRIPOD-AI [32], the CLAIM checklist [99], and its minimum information MI-CLAIM version [106], as well the CONSORT-AI and the SPIRIT-AI [122]. Currently there are no broadly accepted guidelines and no standardized reporting used in practice [142].</p>
<p>Recommendations and guidelines. In the last few years, as testified by the results of our queries, some authors have published recommendations and guidelines to promote the reproducibility of AI solutions in the biomedical domain. Also scientific journals have recommended authors to fill ad-hoc checklists to verify the reproducibility of their work [19]. In many cases, the scope is broad and focuses on the overall transparency, scientific excellence and trust of AI development processes [81,87,99]. Indeed, for some issues, such as traceability, soundness of the approach or precise documentation, it is difficult to set a border between what pertains only reproducibility and what goes beyond. For instance, the explicit identification of the authors of the work (or part of it) is not necessarily mandatory for reproducibility, as far as the technical documentation is sound and complete to enable experiment reproduction. Instead, it may be mandatory and needed for traceability and accountability purposes. The more structured guidelines consist in either a checklist of items to be verified [81,99], or in a list of criteria to be met or questions to be answered [131,141,146]. In other cases, general recommendations are provided [91].</p>
<p>Overall, the most common recommendations touch upon the following issues:</p>
<p>• Process documentation: focusing on the need to annotate and document any data, processing steps and choices done during development as well as the hardware and software • Standardized reporting: focusing on how to standardize a detailed report of methods and results. This moves the previous issue further, by proposing structured checklists and reporting items • Resource sharing: promoting the appropriate sharing of data, code and/or AI models details (such as weights, input and output sets, training hyper-parameters) • Development strategies: recommending scientific excellence based on statistical soundness and strategies to foster replicability and generalizability of AI models (e.g., via data representativeness).</p>
<p>Some of these recommendation categories are cross-sectional to the criticalities described in the previous section (i.e., process documentation and resource sharing) and are common to almost all the work on reproducibility. Others address specific criticalities (i.e., standardized reporting) or regard the technical measures to be adopted to cope with all the mentioned criticalities.</p>
<p>Most of the recommendations provided strongly overlap with the general recommendations we have overviewed in the previous sections, namely Section 3.1 and Section 3.2. Some more specific recommendations mainly focus on the types of information to be specified when documenting the dataset (e.g., inclusion &amp; exclusion criteria, population statistics, selection criteria), and the development strategies towards statistical soundness (e.g., collection of multi-institutional datasets or model calibration). For the sake of completeness, the most relevant recommendations are overviewed in the following, grouped per issue they address. They are also further summarised in Section 6 within Tables 4 and 6.</p>
<p>As for process documentation, almost all the papers retrieved strongly pushed authors to accurately document the detailes of their work, as documentation is a key enabler of reproducibility [12,21,57,91,121,138,141,146]. More specifically, Renard et al. proposed to characterize and document any forms of uncertainty and variability in the AI development, especially in the case of DL [121]. Namely, they proposed to document the dataset used, the optimization and process, the hyper-parameter choices, the DL architecture, the middleware and the infrastructure used.</p>
<p>As far as standardized reporting is concerned, Stevens et al. [131] presented reporting criteria split into four main categories: study design, data sources and processing, and model development and validation. Vollmer et al. [141], discussed a framework consisting of 20 criteria (questions) intended to guide ML and statistical research, split into six categories: inception, study, statistical methods, reproducibility, impact evaluation and implementation. Reproducibility and quality of work was also addressed in the context of clinical trials [143]. The work by McDermott et al. [146] listed ten machine learning reporting items covering the following phases: experimental design, statistical model evaluation, model calibration, top predictors, sensitivity analysis, decision analysis, global model explanation, local prediction explanation, programming interface, and source code.</p>
<p>When considering resource sharing, Beam et al. proposed to share data in dedicated biobanks in order to guarantee suitable data governance [12]. In case data sharing would raise privacy concerns, they suggested to sharing data according to a "walled-garden" approach whereby reviewers might be provided access to a private area and use the data for reproducibility analyses during the review period. Code release on any of the most common repositories (e.g., Github, Bitbucket, or GitLab) is the most common recommendations [57,91,146], also mentioning containerization to avoid dependences to software environment.</p>
<p>As far as development strategies are concerned, the most complete recommendations came from Wojtusiak [146] and McDermott et al. [91]. They proposed to:</p>
<p>• integrate multi-institution datasets, in order to ensure the heterogeneity of data and representativeness of the dataset used to developed the AI solutions • prospectively collect data, in order to avoid developing AI models that might likely not generalize on real-world prospective data • develop new privacy-preserving analysis techniques, to ensure data and can be shared without disclosing any sensitive information while retaining the relevant information • adopt rigorous statistical approaches for the development and evaluation of AI models, based on statistical best practices, in particular when considering model comparisons, model calibration, identification of top predictors and sensitivity analyses.</p>
<p>Besides the aforementioned types of recommendations, some authors addressed their suggestions also to regulatory bodies, encouraging the enactment of specific policies for AI study in the medical field. For instance, McDermott and co-authors also suggested to adopt a pre-registration policy for AI-based studies, meaning that the experimenters or authors should report to regulatory bodies their goal and planned preprocessing/modelling scheme before they run any experiments [91]. This would intend to avoid intentional or unintentional statistical frauds.</p>
<p>Reproducibility of ML methods for physical artificial intelligence</p>
<p>Physical AI refers to using AI techniques to solve problems that involve direct interaction with the physical world, for example by observing the world through sensors or by modifying the world through actuators. The data is generated mostly from physical sensors, but might be combined with other sources, such as databases, the Internet or direct user input. Actuation may range from support to human decisions to managing automated devices (e.g., traffic lights, gates) and actively directing robots, autonomous cars, drones, etc. [34].</p>
<p>Whereas physical AI largely uses the same methods and algorithms that are already proven in other application areas, including the growing share of deep learning based solutions, the specific sources of data and the forms of output make this area particularly challenging with respect to reproducibility. Science and engineering in general are dealing with physical experiments routinely, and there are established guidelines and protocols that allow researchers to keep that random factors under scrutiny. The results are experimental outcomes that are at least statistically reproducible and comparable.</p>
<p>Therefore, we can think about reproducibility in physical AI in terms of casting the recommendations for reproducibility we devised specifically for AI and machine learning onto the well-established rules of conducting reproducible experiments in general science and engineering. However, there are problems specific to physical AI, particularly concerning robotics, that need to be tackled in order to ensure a common understanding of reproducibility and its standards among researchers. The number of publications we were able to survey that are concerned with reproducibility in physical AI seems to be much smaller than the number of those concerning other application areas, e.g. medical, while different aspects of physical AI are represented to a much different extent in this literature. The following paragraphs of this subsection deal with reproducibility issues in Reinforcement Learning and Deep Reinforcement Learning, which are the most promising approaches to learning in physical AI. We tackle also the problem of comparing new results to state-of-the-art baselines, as this is particularly problematic if physical systems are involved, while the lack of reproducibility makes it often impossible to asses the importance of newly published results. Problems in achieving reproducibility when ML/DL is applied to two specific categories of physical systems, namely, if we deal with physical robots, or we try to obtain reproducible simulations of physical systems, are considered as being beyond the main scope of the this survey. However, as these issues might be interesting to readers applying modern AI solutions in robotics, they are covered in section C of the supplementary material available with the electronic version of this paper. Reproducibility of reinforcement learning experiments. A well represented aspect is reinforcement learning (RL) [63], which recently achieved successes in solving complex tasks of physical AI, particularly in robotics [148]. These achievements are also attributed to enhancing the RL paradigm with deep learning, that gave rise to the branch of deep reinforcement learning. Unfortunately, reproducibility in RL is even harder to obtain than in other areas of ML/DL, because a RL learning agent needs to interact on-line with an environment by taking actions and receiving feedback of their consequences (a reward). This manner of gathering the learning data makes RL experiments prone to additional non-determinism due to stochastic environment [68] and physical issues, such as delayed sensor readings [88]. Khetarpal et al. [77] were one of the first who studied the importance of reproducibility in reinforcement learning. They pointed out the distinction between reproducibility of the algorithm and reproducibility of the evaluation procedure, focusing mainly on the latter issue. An evaluation pipeline for (deep) RL that could be standardised was proposed in [77], which was the basis for the procedures used in [88]. Whereas RL is often used in simulated environments or simplified worlds, such as arcade games, Lynnerup et al. [88] tackled the problem of reproducing deep RL algorithms on real robots. They used the SenseAct 9 framework with an Universal Robots' manipulator arm to solve a benchmark task in which the robot has to reach target points in a 2D plane with its end-effector. This paper contributed a demonstration of a rigorous method for documentation of parameters with experiment configuration files, and a statistics-based evaluation of common RL baseline algorithms on real-world robots. Moreover, [88] comments extensively the sources of non-determinism in machine learning, deep learning, and those specific to deep RL:</p>
<p>• environment -when dealing with physical systems, sensor delays, network/interface delays and other uncontrolled issues might be the prevailing source of indeterminism; • initialisation of the neural networks' weights in deep RL must be controlled to ensure reproducibility; • sampling randomly from the training data and from replay buffers (minibatch sampling).</p>
<p>In the light of these findings and the experimental results [88] proposes a list of recommendations for reproducible deep RL experiments, which are basically in-line with those devised by others for general-purpose DL. Lynnerup et al. suggest also frequent code reviews to ensure the integrity of code prior to conducting experiments, and acknowledge that reproducing RL results on a real robot introduces many technical issues which are difficult to debug, while they may contribute to uncontrolled randomness in the experiment.</p>
<p>The impact of non-determinism on reproducibility in deep Q-learning [98] was investigated in [103], with a conclusion that a non-deterministic implementation of reinforcement learning may not reproduce results of similar quality to published results, solely due to the large variance between runs. Investigating the sources of non-determinism, Nagarajan et al. [103] point out to issues related to initialisation and the use of GPU, which is consistent with the results reported for deep learning in general, but also emphasize that in RL the agents typically employ a stochastic policy during learning, i.e. the agent's action is drawn from a non-degenerate distribution over the available actions. Reproducibility for comparing to baselines and competitors. Achieving reproducibility is important when a researcher wants to compare her/his results to the existing baselines. When physical AI is considered, this factor becomes crucial, because small implementation decisions make a big difference, as demonstrated in [74] for the case of multi-task RL (MTRL). This paper shows that small implementation details can create absolute performance differences of up to 35% on meta-and multi-task RL benchmarks for a given algorithm. This results in a statistically-significant variance in the performance of the investigated algorithm that can exceed the reported performance differences between this algorithm and the baseline, thus confounding performance improvement claims of the researchers. Julian et al. [74] emphasize the role of widely-disseminated reference implementations and consistent evaluation protocols, providing multi-seed benchmark results on the available benchmarks for several most popular meta-RL and MTRL algorithms. The problems related to reproducibility of robotics research obviously mount up with the increasing complexity of the robotic system. In this context [117] proposed the SwarmRob toolkit 10 , which deals with the problem of sharing experimental artifacts in multi-robot systems. SwarmRob leverages virtualisation of robotics applications isolating the artifacts of experiments in containers that can be easily shared with others, facilitating reproducibility.</p>
<p>Recommendations for improving reproducibility in machine learning and its applications</p>
<p>The extended discussion of the reproducibility issues in machine learning based on our literature studies revealed that the techniques and tools of modern machine learning, particularly deep learning and reinforcement learning, introduce additional factors that compromise reproducibility. Whereas the general recommendations for reproducibility we summarised in section 3 are still valid for all areas of machine learning, the discussion shows that DL, RL and their emerging applications in medicine and physical AI require to take particular care about some specific factors. Therefore, in Table 4 we re-visit some of the general guidelines for reproducibility in the light of these factors, providing for each of the issues a very brief recommendation devised on the basis of our analysis, the area of ML where the issue is most pronounced, and references to example papers that offer a more detailed treatment of the given problem.</p>
<p>Feature</p>
<p>Recommendation Application context</p>
<p>Source guideline</p>
<p>Hyper-parameters Use tools that automatically capture data inputs and model hyper-parameters while training a DL model. Store the training parameters and the input data together with the AI model (e.g. dtoolAI)</p>
<p>All DL areas [59] Measure Document the measurement metrics, which is essential for reinforcement learning.</p>
<p>Deep RL [88] Workflow Use documentation frameworks to document training, documenting also software dependencies. Avoid algorithmic factors of irreproducibility setting the random seeds at the beginning of each run All ML areas [26], [89] Workflow execution Employ automatic versioning tools enabling versioning both the code and the binary artifacts (e.g. Deep Forge).</p>
<p>All DL areas [18] Data collection, annotation and quality Document carefully the data acquisition process for physical AI systems. Annotate the training data with metadata and give them a persistent identifiers (e.g., URIs).</p>
<p>Biomedical applications, Embodied agents, Deep RL Data collection, annotation and quality Report the inclusion and exclusion criteria for the considered cases. Provide descriptive statistics of data. Describe the acquisition protocols and parameters</p>
<p>Biomedical applications [146] Preprocessing cleaning and labelling Provide a detailed description of the calibration procedures that were applied to the physical system that produced te data  Table 4. General recommendations from Table 1 or Table 3 that need to be extended and/or clarified in the context of deep learning, DL/RL in embodied agents or biomedical applications. Note that items in the Feature column strictly match their counterparts in the first columns of Table 1 or Table 3 Our survey reveals also that there is a group of reproducibility issues specific to deep learning, reinforcement learning, biomedical applications or physical AI systems that are largely overlooked in the general recommendations. These issues are related to the specific algorithmic or implementation factors in DL/RL or to the way data collected from physical systems (either off-line or on-line) is treated for training of ML systems. Identifying these problems in the surveyed literature (often very recent conference or workshop papers) and confronting them to the list of general guidelines we identify gaps in the understanding and awareness of the AI community with respect to reproducibility. Table 5 summarises the recommendations that are specific to DL/RL methods and their application areas. Analogously as in the previous table, we provide for each issue a brief recommendation to improve the reproducibility level, define the area where these recommendations are most useful or important, and list the papers that consider the given problem and propose the recommendations.</p>
<p>Issue</p>
<p>Recommendations Application context</p>
<p>Source guideline</p>
<p>Randomness</p>
<p>Pre-set random seeds or use a record-and-replay technique to eliminate sources of randomness All DL areas [26] Algorithmic indeterminism Avoid the use of stochastic layers in neural networks, random initialisation of weights, random ordering of data shuffling, and stochastic data augmentation All DL areas [149] Implementation indeterminism (software) Chose deterministic implementations of primitive operations, avoid autotune mechanisms in libraries, avoid the use of multiple processes that cannot guarantee data order, use dockerization techniques All DL areas [26] Implementation indeterminism (hardware) Eliminate unsupported non-deterministic operations, use techniques facilitating reproducibility of GPU workloads, use advanced dockerization, such as DetTrace or apply a profile-and-patch technique All DL areas [105] Comparison to baselines and previous results</p>
<p>Use reference implementations and consistent evaluation protocols, use benchmarks that support reproducibility (e.g. ACRV, EGAD in manipulation)</p>
<p>Deep RL, embodied agents, biomedical applications [103] Evaluation procedures in RL The evaluation pipeline for reinforcement learning, including deep RL, should be standardised in order to obtain reprodcucible results</p>
<p>Deep RL [77] Using public datasets of sensor data When documenting data provenance, describe the methods and implementation stack for any preprocessing/transformation steps executed on the data. Document the version of dataset being used (e.g. using DOI), particularly if anything was changed with respect to the originally published one   Table 6. Recommendations specific to the development of AI models in biomedical applications. Note that items in the Issue column are different from the features identified in Table 1, Table 3, or in the extended versions in Table 4 5 FINAL REMARKS Reproducibility is one of the key dimensions that concur to create trustworthy and reliable AI and ML. It becomes paramount when demonstrating scientific outcomes or methods that result from experimental processes, not necessarily supported by known theories or models, as it happens with current data-science and learning methods.</p>
<p>Despite the awareness of this and the appearance of many works on reproducibility in recent years, the take-up of reproducibility-enabling practices in practical implementation is still insufficient. In particular, this applies to the most advanced ML and DL methods, which are at the core of the most recent successes of AI. As we have extensively debated in the previous sections, several conditions complicate those methods' reproducibility. They include: the complexity of models in terms of architecture, the high number of parameters to be tuned, the optimization strategies needed to make them perform as expected, the inner peculiarities of the stochastic processes, the characteristics of the data used to their training process and all the pre-processing steps to curate and prepare these data, the technical peculiarities of the underlying tool-chains, and the dependencies to hardware and software platforms.</p>
<p>Nevertheless, the more technical issues that hinder reproducibility in modern ML are often overlooked in the general literature, which is focused more on process documentation and code sharing. They are treated only in more specific technical papers, as we have analyzed in more details in Section 4. Moreover, specific application scenarios, such as the presented biomedical and physical AI fields, show peculiarities in terms of sensitive data sharing, data acquisition indeterminism, statistical soundness of results, evaluation procedures and detailed reporting, which further complicate the reproducibility setting. As we attempted to show in our paper, all these issues require more systematic approaches with solutions that touch upon best practices, new requirements, quality standards and configuration choices along the technical stack to better model the development process of ML methods and their reproducibility.</p>
<p>In our survey, we wanted to broaden the debate over reproducibility of AI and ML. We also attempted to homogenize and map the various guidelines and recommendations, making explicit the technical or procedural means to implement them, namely via metadata, platforms, or the scientific material, paper, or report. Moreover, we strongly advocated taking into account the additional factors that compromise reproducibility in the field of ML, DL and RL and critical application scenarios. In this respect, we dedicated the last section of this paper to propose new specific recommendations.</p>
<p>As a result of this process, we realized there are still several open issues that need to be taken into account for comprehensively enabling reproducibility. The most relevant ones pertain to:</p>
<p>• the limited functionalities of existing development platforms with respect to many of the presented recommendations, especially when considering procedures to handle the randomness and the variability of the training process in DL; • the insufficient standardization of metadata usage. Current proposals of metadata are still far from well-documenting AI experiments, as we discussed in Section 3; • the limited practice in containerizing the developed applications and sharing the entire digital artefacts (in case with the development notebooks); • the lack of standardized evaluation procedures and reporting, which would include the comparison with baselines, benchmarks and competitive models, especially in the field of RL, biomedical applications and physical AI; • the difficulties in sharing complete data in some application areas, such as the discussed biomedical field, due to suitable anonymization techniques;</p>
<p>• the open challenge of interpreting the inner functioning and the generalization capacity of complex ML, DL and RL models.</p>
<p>Considering the key role that reproducibility plays in increasing the overall transparency and accountability of AI and ML, we believe that further initiatives are in need to encourage researchers, authors, and industrial actors to comply with the proposed guidelines. The approach should be overarching and comprehensive, not limited to documentation and resource sharing, as it should also encompass best practices at the technical and implementation level.</p>
<p>A METHODOLOGY OF THE REVIEW</p>
<p>This article is written following a systematic survey methodology [129,147], with the aim to synthesise reproducibility guidelines from the existing AI literature into a comprehensive catalogue of recommendations that particularly foster reproducibility practices in modern machine learning.</p>
<p>There are several types of literature reviews that best suit different areas of science or engineering, and support different aims of the conducted review [110]. A type of review that is focused on revealing contradictions, inconsistencies, strengths and weaknesses on the given topic in the pre-existing literature is a critical review [130].</p>
<p>Considering the multi-faceted nature of modern artificial intelligence, the choice of the critical review formula appears natural, even if we limit our survey to the area of machine learning. However, as contemporary AI, and in particular, machine learning, is characterised by a rapid pace of development, resulting in important findings and recent ideas being reported in non-archival sources, we include in our survey also some elements of a narrative review [48]. This allows us to summarise what has been written recently about various aspects of reproducibility, thus providing entry points for further individual studies by the readers. In order to collect the literature for our survey, we have implemented the following steps.</p>
<p>• Search for the literature. We started with a systematic search for the relevant literature in the three most widely used bibliographical data bases: Elsevier Scopus, Clarivate Web of Science and Google Scholar. Recall that Scopus and Web of Science collect mostly archival publications from journals, books and established conferences and have well-defined inclusion criteria for the papers that are covered. These databases focus on fundamental and natural sciences, including medicine, but their coverage of engineering disciplines is less comprehensive. This is particularly visible with respect to the recent areas of computer science, where many important conference and workshop publications are not included. Therefore we decided to search also the Google Scholar database, which has much more relaxed inclusion criteria. Google Scholar indexes recent and emergent sources, which are particularly relevant in the context of the aims of our survey. In addition, it also covers many archiving services collecting the so-called pre-prints or reports, such as e.g. arXiv, the use of which has recently become extremely popular among computer scientists. • Inclusion criteria. We only included papers written in English that concern machine learning reproducibility. Although we included literature from all disciplines, ranging from computer science and information systems to medicine and natural sciences, we excluded studies on reproducibility for specific topics that did not focused on machine learning or artificial intelligence. Only papers published from 2017 to 2022 were included in the search, as we focused on the current state-of-the-art. Because each of the databases we considered has a different search mechanism, the queries were slightly different. For Scopus we started using the keywords reproducibility AND (machine learning OR artificial intelligence), searching only in the titles and abstracts. For each found paper, we initially determined the relevance by the title and abstract, excluding papers that we deemed irrelevant, e.g. those from medicine which mentioned AI or ML but in fact focused on the reproducibility of clinical results. After this initial pruning of Scopus results, a total of 35 papers were considered for the survey. For Web of Science, we run the query reproducibility AND artificial intelligence, matching the keywords in the title and the abstract of the papers published in the period 2017-2022. We obtained an initial set of 252 papers, whose list was exported and downloaded from the WoS system. Such a set was then checked by analysing the pertinence and relevance of each paper. Thirty-one papers out of the initial 252 turned out to be significant from this check. The excluded papers were either just brief abstract, tutorials or letters or were out of scope as dealing, for instance, with the application of AI in a specific domain (e.g. health and care) and the reproducibility was considered with respect to the generalizability of the domain results. For Google Scholar the search procedure was different due to the more limited possibilities of asking complex queries with additional conditions and a different way of presenting the results of the search. We asked independently two queries reproducibility of artificial intelligence and reproducibility of machine learning and received very long lists of results: e.g. 87 600 answers to the first query. As these lists are presented in the relevance order proposed by Google Scholar, we analysed the first 10 web pages -so approx. 100 proposed papers. As a results we identified 39 potentially interesting paper links. After an expert inspection of their full files, we finally selected 28 papers for the deeper analysis. Finally, after combining results from the three databases and excluding duplicated papers, we got a list of 81 relevant publications, among them 69 journal articles, with the rest being conference papers, book chapters or arXiv pre-prints. • Final selection of papers. In order to decide which of these papers should be covered in the study three of the four authors conducted independent evaluations of the 81 full-text papers, agreeing to select 18 of them as highly relevant to the reproducibility in machine learning problem. Any controversies while implementing this selection were discussed and resolved. All these 18 papers are reviewed in our study, while some of the remaining 63 relevant sources found in the databases are cited as secondary examples, particularly in the context of various applications of machine learning or artificial intelligence and the emerging reproducibility issues.</p>
<p>Besides the systematic search for relevant publications we used also a less formal method to get the papers for our survey, namely through backward and forward searches in the references of the papers we have identified in the first phase. Here the selection was less formal, as we were interested in highly influential papers on reproducibility with some relevance to machine learning or at least general artificial intelligence, in particular those papers that defined guidelines and recommendations about reproducibility practices. In particular, in this scenario we search more intensively the recent conference or journal guidelines or checkpoint lists (see section 3.1). Our aim in this phase of research was to investigate how and by whom the guidelines we can find in the current literature were proposed, and to find any recurring patterns, inconsistencies, or important gaps in those guidelines. Therefore, in this phase we didn't set a publication year census on the papers, including also older but seminal works.</p>
<p>The search for most relevant citations in the analysed papers revealed relations between the existing works on reproducibility. In order to further explore these relations and to identify the most influential papers and authors we explored a new tool to survey the literature and verify what we got. This web-based visual tool, named Connected Papers [135], allows the users to find papers relevant to a "seed" paper, which is entered as the search query. Related papers are retrieved from the Semantic Scholar Paper Corpus. The similarity metric in Connected Papers is based on the concepts of co-citation and bibliographic coupling [69]. Papers that have highly overlapping citations and references are considered as more similar, even if there are no direct citations between them. The visualised graph clusters the found papers according to their similarity and highlights the shortest path from each node (paper) to the "seed" paper in similarity space. Figure 2 shows an example Connected Papers graph, which was built starting the search from one of the most relevant surveys about reproducibility in AI, the work of Gundersen and Kjensmo [54]. In this case 40 papers were used by the program to build the graph, as those having the strongest connections to the origin paper. When we inspected these papers, we have found that 17 were those already included in our references, while the remaining 23 were less relevant, covering reproducibility Fig. 2. An example graph of connected papers concerning reproducibility, which was "seeded" by the work of Gundersen and Kjensmo [54]. Papers that have highly overlapping references are considered more similar, hence similar papers have strong connecting lines and cluster together. Node size is the number of citations, while node colour is the publishing year (darker nodes are more recent) or replicability topics in areas other that ML and AI, or not meeting our inclusion criteria. The graph reveals highly connected clusters of papers around the works co-authored by Gundersen, whom we identify as one of the most influential authors in the survey, and the works of Barba [11], Tatman et al. [136], and Lynnerup et al. [88]. Few papers, like the one by Konkol et al. [78] are highly connected, but not relevant in our context.</p>
<p>B TERMINOLOGY VARIETY IN BIOMEDICAL APPLICATIONS</p>
<p>In their work [121], Renard and co-authors referred to the definition by the National Academies of Science, Engineering, and Medicine [104].They focused explicitly on DL and the variability of DL model development, recommending researchers to detail the various sources on uncertainty, coming from the stochastic optimization, hyper-parameter setting, DL model architecture and the development middleware and/or infrastructure, as debated in the previous sections. In their work [9], Balagurunathan et al. focused mainly on repeatability (i.e., top quadrant of Figure 1), as they claimed its importance as a first step to quantify the variability of an AI model and to gain confidence in the use of such a model. Nonetheless, they did not provide any recommendations or guidelines in this respect.</p>
<p>McDermott and co-authors introduced an additional specification of reproducibility, by differentiating among the following concepts [91]:</p>
<p>• technical replicability, which corresponds to the possibility to reproduce technically the AI-based method, yielding the same results reported in the paper describing the method. This can be mapped on the top-left quadrants of Figure 1 and entails aspects related to the sharing code and datasets. • statistical replicability, which corresponds to the possibility to obtain the same results under resampled conditions that yield different technical configurations, but should not statistically affect the claimed result (e.g., a different set of random seeds, or train/test splits). This implication of reproducibility might be mapped onto the repeatability concept (i.e. top-left sub-quadrant), but adopting specific measures to cope with the variability and randomness of ML models. This is an issue of particular interest in the biomedical community as most of the scientific results are presented by considering their variance or confidence intervals, for instance, based on cross-validation strategies. • conceptual replicability, which corresponds to the possibility to reproduce the desired results under conditions that match the conceptual description of the original paper. This maps onto the two right quadrants of Figure 1 as it mainly corresponds to the generalizability of the scientific results.</p>
<p>In his conference paper [146], Wojtusiak considered the reproducibility in a broad sense, ranging from replicability to robustness and generalizability (or corroboration), thus covering the four quadrants of Figure 1. He provided valuable recommendations to foster the various dimensions of reproducibility and put emphasis on reporting the statistical variance of any outcomes (e.g., by reporting mean and variance of several runs or trials). This is because in biomedical applications the significance and acceptability of any results are usually considered from a statistical standpoint. Statistical analyses, actually, guide clinical researchers to draw reasonable and accurate inferences from the collected data and, thus, to make sound decisions in the presence of uncertainty, for instance with respect to the variability of response to treatment by diverse individuals.</p>
<p>Similarly to McDermott et al., Wojtusiak debated the importance of generalizability of AI-based scientific results. Noteworthy, replicability, conceptual replicability, robustness or generalizability represent a very critical issue in biomedical applications, as without this capability any AI-based solutions may appear useless in clinical practice [146]. The difficulties in ensuring this capacity come from peculiarities of the biomedical field, which is often referred to as the lack of robustness or reliability of the AI-powered tools, is due to the fact that AI models are often trained, tested and validated on hand-picked and limited datasets, often acquired by just one clinical institution and in a so-called retrospective fashion (i.e., from the clinical records or repositories of the clinical institution, dating back even several years). These datasets are hence not enough representative of the data that can be encountered in clinical practice, as they can differ a lot from new data coming from other institutions or from those prospectively acquired by the same institution (i.e., from the current clinical practice). These conditions are peculiar of the biomedical field and are discussed in the main part of the paper.</p>
<p>C REPRODUCIBILITY OF ML/DL RESULTS IN PHYSICAL AND SIMULATED</p>
<p>ROBOTICS RESEARCH. C.1 Reproducibility in robotics. In robotics, reproducible comparison to algorithmic baselines and previous results is particularly complicated because of the lack of reproducible experimental setups. Whereas research in other areas of ML/DL, e.g. image processing, is usually benchmarked on large, publicly available datasets, such as COCO 11 and The PASCAL Visual Object Classes 12 , the datasets concerning robotics need to be more specialised, and they are often provided along with a particular research method or algorithm. Such datasets, e.g. the KITTI Vision Benchmark Suite 13 [44], commonly used in research related to self-driving cars and SLAM (Simultaneous Localisation and Mapping) offer pre-processed data, while the actual algorithms used for this pre-processing are often not specified in details (e.g. compensation of the vehicle motion while acquiring the 3D laser scanner data in KITTI). Another problem hampering reproducibility in robotics research using datasets can be exemplified by the case of KAIST Multi-Spectral Day/Night Data Set [28], which is one of a very few datasets with visual spectrum and thermal images that can be applied for practical research in safety of autonomous cars and pedestrians. Hence, the KAIST dataset is widely used, but many works adopt modified variants, for example the "sanitized" version [83], from which known annotation errors have been removed, or a subset of images with human silhouettes taller than 50 pixels. As there is no apparent versioning of these modifications, research results using the KAIST dataset become hardly comparable in terms of quantitative metrics [123]. These examples demonstrate that ensuring data provenance and traceability is also crucial in physical AI, while the metadata describing a given dataset should include a detailed description of the sensory data acquisition process and any post-processing that was applied afterwards.</p>
<p>In this review we can provide only few illustrative examples of problems specific to the reproduction of physical AI results, but even these examples show clearly that there is an urgent need of reproducible experimental setups and datasets, as well as performance metrics that are robust to the common factors of non-determinism that are present in this domain. Examples of physical benchmarks designed to be reproducible can be found in robotic manipulation. The ACRV Picking Benchmark [80] consists of a set of 42 common objects, a widely available shelf, and exact guidelines for object arrangement using stencils. An evaluation protocol is also provided in [80] in order to facilitate comparison of complete manipulation systems, including both perception and control aspects. The more recent Evolved Grasping Analysis Dataset (EGAD) [102] serves the purpose of reproducible training and evaluation of robotic visual grasp algorithms. A set of 49 diverse 3D-printable evaluation objects is provided to encourage physical, yet reproducible testing of robotic grasping algorithms.</p>
<p>C.2 Reproducibility of simulations.</p>
<p>An alternative solution to the development of physical AI systems in real-world experiments is to use simulators [27]. In contrary to the physical world, a simulated environment can be initialised in a deterministic way, and it is possible to reset the robot with it's control and learning algorithms to the default parameters and run a large number of independent experiments, which is essential in some domains, such as self-driving vehicles [64]. However, these benefits come at the cost of possible overfitting of the trained ML/DL solution to the simulated environment. Models trained in simulation often transfer poorly to the physical world and fail on real robots, or their performance decreases substantially, which is known as the reality gap problem, while the efforts towards transferring data-driven algorithms trained in simulations to the physical world are known as sim2real [66,67]. A common approach to bypass the reality gap is to randomise these aspects of the feedback information that are challenging to simulate accurately. Although this is an effective method to obtain robust learned models transferable to real robots [73], the use of extensive randomisation may compromise reproducibility. Josifovski et al. [73] investigated and quantified the effects of randomisation on the learning of RL policies that transfer best to a real robot in a simple manipulation task, concluding that increasing the randomness of simulation parameters one can expect more robust models, but also a decrease in the ability of the algorithm to find a good policy in simulation. Thus, less convergent learning experiments may bring different outcomes, becoming less reproducible.</p>
<p>Simulations that are transferable to real robots, but still reproducible are researched in [39]. This paper introduces Gym-Ignition, a framework to create reproducible robotic simulations for reinforcement learning experiments. The software architecture of Gym-Ignition avoids the use of socket-based client-server communication between different modules, as sockets can be preempted depending on the load of the operating system, which leads to non-determinism in the simulated environment feedback. What is more, Gym-Ignition exposes to the users an interface to initialise all the random number generator seeds, which allows separate runs to be repeatable. Ferigo et al. [39] compare the features of their simulator to a number of alternative solutions, finding also the recent Nvidia Isaac Gym 14 [90] and Unity ML-Agents 15 as reproducible, while the OpenAI Gym [17] with OpenAI Robotic Environments 16 , which is commonly used to benchmark RL algorithms, does not fully support reproducibility.</p>
<p>Fig. 1 .
1Graphical summarization of terms and their main meaning according to the literature and the conceptual dimensions Team, Reasons, Workflow Components. Terms in bold are recommended for use, whereas those in grey are discouraged</p>
<p>•
If the used libraries apply autotune mechanisms, then deterministic implementations of primitive operations should be chosen setting appropriate configuration variables. • If GPU computations are used, then techniques that ensure deterministic execution of operations should be considered. Zhuang et al. explain the impact of particular choices in the DL toolchain on reproducibility in more detail, considering also the cost of controlling the implementation factors in deep learning.</p>
<p>Train/validation/test splits.For all datasets used, The details of train/validation/test splits.Feature 
Recommendation 
Where 
Source guideline </p>
<p>Data repository 
Share data in a community repository or 
the simulation enviroment </p>
<p>P </p>
<p>Gundersen et al. [55] 
Pineau's checklist v2 [114] 
IJCAI 22 Guideline [140] 
Data distribution 
How will the dataset will be distributed 
(e.g., tarball on website, API, GitHub)? </p>
<p>P,M 
Datasheets [43] </p>
<p>Data appendix 
All novel datasets introduced in this paper 
are included in a data appendix </p>
<p>S 
IJCAI 22 Guideline [140] </p>
<p>Dataset from literature 
All datasets drawn from the existing liter-
ature (potentially including authors' own 
previously published work) are publicly 
available </p>
<p>P,S,M 
IJCAI 22 Guideline [140] </p>
<p>Cite Data 
All datasets drawn from the existing liter-
ature (potentially including authors' own 
previously published work) are accompa-
nied by appropriate citations </p>
<p>P,S,M 
IJCAI 22 Guideline [140] </p>
<p>Data citeable 
Generate DOI or PURL. 
P,M 
Gundersen et al. [55] 
Datasheets [43] 
Data relevant statistic 
For all datasets used, The relevant statis-
tics, such as number of examples </p>
<p>P,S,M 
Pineau's checklist v2 [114] </p>
<p>Unavailable Dataset Description 
All datasets that are not publicly avail-
able (especially proprietary datasets) are 
described in detail </p>
<p>S 
IJCAI 22 Guideline [140] </p>
<p>Data collection, annotation and quality 
For all datasets used, For new data col-
lected, a complete description of the data 
collection process, such as instructions to 
annotators and methods for quality con-
trol </p>
<p>P,S,M 
Pineau's checklist v2 [114] 
Datasheets [43] </p>
<p>P,M 
Pineau's checklist v2 </p>
<p>Preprocessing softwareIs the software used to preprocess/clean/label the instances available? If so, please provide a link or other access point.P,S,M 
Datasheets [43] </p>
<p>P,S,M 
Datasheets [43] </p>
<p>Data metadata 
Include basic metadata describing the 
data. </p>
<p>P,M 
Gundersen et al. [55] </p>
<p>Dataset contacts 
How can the owner/curator/manager of 
the dataset be contacted (e.g., email ad-
dress)? </p>
<p>P,M 
Datasheets [43] </p>
<p>Data license, Intelectual property, term of use Give the data a license including Intelec-
tual property and use terms or regulatory 
restrictions </p>
<p>P,M 
Gundersen et al. [55] 
Datasheets [43] </p>
<p>Table 5 .
5Recommendations specific to deep learning. Note that items in the Issue column are different from the features identified inTable 1, Table 3, or in the extended versions inTable 4Issue 
Recommendations 
Application 
context </p>
<p>Source guideline </p>
<p>Data selection 
Integrate multi-institutional datasets. Prefer when possi-
ble prospective data. Always consider to validate models 
on prospective data </p>
<p>Biomedical 
applications </p>
<p>[91] </p>
<p>Data curation 
Adopt privacy-preserving analysis techniques, in order 
to enable data sharing </p>
<p>Biomedical 
applications </p>
<p>[91] </p>
<p>Model evaluation Perform model calibration and report calibration curves 
and calibration measures. Perform global sensitivity 
analysis for attributes, report sensitivity plots and ana-
lyze if models are stable. Perform global sensitivity anal-
ysis for continuous and discrete attributes and report 
sensitivity plots and analyze if models are stable </p>
<p>Biomedical 
applications </p>
<p>[146] </p>
<p>OECD AI Principles: https://oecd.ai/en/dashboards/ai-principles/P8
The full comparison is available at https://github.com/collab-uniba/Software-Solutions-for-Reproducible-ML-Experiments 3 https://github.com/tensorflow/models
https://en.wikipedia.org/wiki/Patch_(computing) 5 https://dvc.org/ 6 https://mlflow.org/ 7 https://github.com/JIC-CSB/dtoolai
https://deepforge.org/
https://github.com/kindredresearch/SenseAct/ 10 https://iot-lab-minden.github.io/SwarmRob/
https://cocodataset.org 12 http://host.robots.ox.ac.uk/pascal/VOC/ 13 http://www.cvlibs.net/datasets/kitti/
https://developer.nvidia.com/isaac-gym 15 https://unity.com/products/machine-learning-agents 16 https://openai.com/blog/ingredients-for-robotics-research/
ACKNOWLEDGMENTSThis research was partially supported by the two EU H2020 Projects TAILOR (GA 952215) and ProCAncer-I (GA 952159).
. ACM Artifact Review and Badging -Version. 1ACMv1 2020. ACM Artifact Review and Badging -Version 1. https://www.acm.org/publications/policies/artifact- review-badging</p>
<p>. Acmv1, ACM Artifact Review and Badging -Version. 12020ACMv1.1 2020. ACM Artifact Review and Badging -Version 1.1. https://www.acm.org/publications/policies/artifact- review-and-badging-current</p>
<p>Data Catalog Vocabulary (DCAT) -Version 2. W3C Recommendation. Riccardo Albertoni, David Browning, Simon Cox, Alejandra González Beltrán, Andrea Perego, Peter Winstanley, Riccardo Albertoni, David Browning, Simon Cox, Alejandra González Beltrán, Andrea Perego, and Peter Winstanley. 2020. Data Catalog Vocabulary (DCAT) -Version 2. W3C Recommendation. W3C. https://www.w3.org/TR/2020/REC- vocab-dcat-2-20200204/</p>
<p>Reproducibility. part of he TAILOR Handbook of Trustworthy AI. Riccardo Albertoni, Sara Colantonio, Piotr Skrzypczyński, Jerzy Stefanowski, Riccardo Albertoni, Sara Colantonio, Piotr Skrzypczyński, and Jerzy Stefanowski. 2022. Reproducibility. part of he TAILOR Handbook of Trustworthy AI. http://tailor.isti.cnr.it/handbookTAI/T3.4/L2.Reproducibility.html</p>
<p>Riccardo Albertoni, Antoine Isaac, Data on the Web Best Practices: Data Quality Vocabulary. W3C Note. W3C. Riccardo Albertoni and Antoine Isaac. 2016. Data on the Web Best Practices: Data Quality Vocabulary. W3C Note. W3C. https://www.w3.org/TR/2016/NOTE-vocab-dqv-20161215/</p>
<p>Introducing the Data Quality Vocabulary (DQV). Riccardo Albertoni, Antoine Isaac, 10.3233/SW-20038212Semantic WebRiccardo Albertoni and Antoine Isaac. 2021. Introducing the Data Quality Vocabulary (DQV). Semantic Web 12, 1 (2021), 81-97. https://doi.org/10.3233/SW-200382</p>
<p>FactSheets: Increasing trust in AI services through supplier's declarations of conformity. M Arnold, R K E Bellamy, M Hind, S Houde, S Mehta, A Mojsilović, R Nair, K Ramamurthy, A Olteanu, D Piorkowski, D Reimer, J Richards, J Tsay, K R Varshney, IBM Journal of Research and Development. 635M. Arnold, R. K. E. Bellamy, M. Hind, S. Houde, S. Mehta, A. Mojsilović, R. Nair, K. Natesan Ramamurthy, A. Olteanu, D. Piorkowski, D. Reimer, J. Richards, J. Tsay, and K. R. Varshney. 2019. FactSheets: Increasing trust in AI services through supplier's declarations of conformity. IBM Journal of Research and Development 63, 4/5 (2019), 6:1-6:13.</p>
<p>Best practices in machine learning for chemistry. Nongnuch Artrith, Keith T Butler, François Xavier Coudert, Seungwu Han, Olexandr Isayev, Anubhav Jain, Aron Walsh, Nature Chemistry. 136Nongnuch Artrith, Keith T. Butler, François Xavier Coudert, Seungwu Han, Olexandr Isayev, Anubhav Jain, and Aron Walsh. 2021. Best practices in machine learning for chemistry. Nature Chemistry 13, 6 (June 2021), 505-508.</p>
<p>Requirements and reliability of AI in the medical context. Yoganand Balagurunathan, Ross Mitchell, Issam El Naqa, 10.1016/j.ejmp.2021.02.024Physica Medica. 83Yoganand Balagurunathan, Ross Mitchell, and Issam El Naqa. 2021. Requirements and reliability of AI in the medical context. Physica Medica 83 (2021), 72-78. https://doi.org/10.1016/j.ejmp.2021.02.024</p>
<p>Vishnu Banna, Akhil Chinnakotla, Zhengxin Yan, Anirudh Vegesana, Naveen Vivek, Kruthi Krishnappa, Wenxin Jiang, Yung-Hsiang Lu, George K Thiruvathukal, James C Davis, arXiv:2107.00821An Experience Report on Machine Learning Reproducibility: Guidance for Practitioners and TensorFlow Model Garden Contributors. Vishnu Banna, Akhil Chinnakotla, Zhengxin Yan, Anirudh Vegesana, Naveen Vivek, Kruthi Krishnappa, Wenxin Jiang, Yung-Hsiang Lu, George K. Thiruvathukal, and James C. Davis. 2021. An Experience Report on Machine Learning Reproducibility: Guidance for Practitioners and TensorFlow Model Garden Contributors. arXiv:2107.00821</p>
<p>Lorena A Barba, arXiv:1802.03311Terminologies for Reproducible Research. Lorena A. Barba. 2018. Terminologies for Reproducible Research. arXiv:1802.03311</p>
<p>Challenges to the Reproducibility of Machine Learning Models in Health Care. Andrew L Beam, Arjun K Manrai, Marzyeh Ghassemi, 10.1001/jama.2019.20866JAMA. 3234Andrew L. Beam, Arjun K. Manrai, and Marzyeh Ghassemi. 2020. Challenges to the Reproducibility of Machine Learning Models in Health Care. JAMA 323, 4 (01 2020), 305-306. https://doi.org/10.1001/jama.2019.20866</p>
<p>Reproducibility in Science Improving the Standard for Basic and Preclinical Research. C Begley, John Ioannidis, 10.1161/CIRCRESAHA.114.303819Circulation Research. 116C. Begley and John Ioannidis. 2015. Reproducibility in Science Improving the Standard for Basic and Preclinical Research. Circulation Research 116 (01 2015), 116-126. https://doi.org/10.1161/CIRCRESAHA.114.303819</p>
<p>Machine and Deep Learning Prediction Of Prostate Cancer Aggressiveness Using Multiparametric MRI. Elena Bertelli, Laura Mercatelli, Chiara Marzi, Eva Pachetti, Michela Baccini, Andrea Barucci, Sara Colantonio, Luca Gherardini, Lorenzo Lattavo, Maria Antonietta Pascali, Simone Agostini, Vittorio Miele, Frontiers in Oncology. 11Elena Bertelli, Laura Mercatelli, Chiara Marzi, Eva Pachetti, Michela Baccini, Andrea Barucci, Sara Colantonio, Luca Gherardini, Lorenzo Lattavo, Maria Antonietta Pascali, Simone Agostini, and Vittorio Miele. 2022. Machine and Deep Learning Prediction Of Prostate Cancer Aggressiveness Using Multiparametric MRI. Frontiers in Oncology 11 (2022).</p>
<p>Rethinking data and metadata in the age of machine intelligence. Martin-Immanuel Bittner, 10.1016/j.patter.2021.1002082100208Martin-Immanuel Bittner. 2021. Rethinking data and metadata in the age of machine intelligence. Patterns 2, 2 (2021), 100208. https://doi.org/10.1016/j.patter.2021.100208</p>
<p>Unreproducible Research is Reproducible. Xavier Bouthillier, César Laurent, Pascal Vincent, PMLRProceedings of the 36th International Conference on Machine Learning (Proceedings of Machine Learning Research. Kamalika Chaudhuri and Ruslan Salakhutdinovthe 36th International Conference on Machine Learning ( Machine Learning Research97Xavier Bouthillier, César Laurent, and Pascal Vincent. 2019. Unreproducible Research is Reproducible. In Proceedings of the 36th International Conference on Machine Learning (Proceedings of Machine Learning Research, Vol. 97), Kamalika Chaudhuri and Ruslan Salakhutdinov (Eds.). PMLR, 725-734.</p>
<p>Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, arXiv:1606.01540Jie Tang, and Wojciech Zaremba. 2016. OpenAI Gym. Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba. 2016. OpenAI Gym. arXiv:1606.01540</p>
<p>DeepForge: An Open Source, Collaborative Environment for Reproducible Deep Learning. Brian Broll, Jimmy Whitaker, Reproducibility in Machine Learning Workshop at ICML. Brian Broll and Jimmy Whitaker. 2017. DeepForge: An Open Source, Collaborative Environment for Reproducible Deep Learning. In Reproducibility in Machine Learning Workshop at ICML 2017.</p>
<p>The IJMEDI checklist for assessment of medical AI. Federico Cabitza, Andrea Campagner, 10.5281/zenodo.4835800Federico Cabitza and Andrea Campagner. 2021. The IJMEDI checklist for assessment of medical AI. https: //doi.org/10.5281/zenodo.4835800</p>
<p>Putting oncology patients at risk. Bob Carlson, Biotechnology Healthcare. 9Bob Carlson. 2012. Putting oncology patients at risk. Biotechnology Healthcare 9 (2012), 17-21. https://pubmed.ncbi. nlm.nih.gov/23091430</p>
<p>Pragmatic considerations for fostering reproducible research in artificial intelligence. Rickey E Carter, Zachi I Attia, Francisco Lopez-Jimenez, Paul A Friedman, npj Digital Medicine. 2Rickey E. Carter, Zachi I. Attia, Francisco Lopez-Jimenez, and Paul A. Friedman. 2019. Pragmatic considerations for fostering reproducible research in artificial intelligence. npj Digital Medicine 2 (2019).</p>
<p>Causality matters in medical imaging. C Daniel, Ian Castro, Ben Walker, Glocker, 10.1038/s41467-020-17478-wNature Communications. 111Daniel C. Castro, Ian Walker, and Ben Glocker. 2020. Causality matters in medical imaging. Nature Communications 11, 1 (2020). https://doi.org/10.1038/s41467-020-17478-w</p>
<p>SPIRIT 2013: new guidance for content of clinical trial protocols. An-Wen Chan, Jennifer M Tetzlaff, G Douglas, Kay Altman, David Dickersin, Moher, The Lancet. 381An-Wen Chan, Jennifer M Tetzlaff, Douglas G Altman, Kay Dickersin, and David Moher. 2013. SPIRIT 2013: new guidance for content of clinical trial protocols. The Lancet 381 (2013), 91-92.</p>
<p>. Raja Chatila, Virginia Dignum, Michael Fisher, Fosca Giannotti, Katharina Morik, Stuart Russell, Karen Yeung, Trustworthy AI. Lecture Notes in Computer Science. 12600SpringerRaja Chatila, Virginia Dignum, Michael Fisher, Fosca Giannotti, Katharina Morik, Stuart Russell, and Karen Yeung. 2021. Trustworthy AI. Lecture Notes in Computer Science, Vol. 12600. Springer, 13-39.</p>
<p>Developments in MLflow: A System to Accelerate the Machine Learning Lifecycle. Andrew Chen, Andy Chow, Aaron Davidson, Arjun Dcunha, Ali Ghodsi, Sue Ann Hong, Andy Konwinski, Clemens Mewald, Siddharth Murching, Tomas Nykodym, Paul Ogilvie, Mani Parkhe, Avesh Singh, Fen Xie, Matei Zaharia, Richard Zang, Juntai Zheng, Corey Zumar, Proceedings of the Fourth International Workshop on Data Management for End-to-End Machine Learning (DEEM'20). the Fourth International Workshop on Data Management for End-to-End Machine Learning (DEEM'20)New York, NY, USAAssociation for Computing MachineryArticle 5, 4 pagesAndrew Chen, Andy Chow, Aaron Davidson, Arjun DCunha, Ali Ghodsi, Sue Ann Hong, Andy Konwinski, Clemens Mewald, Siddharth Murching, Tomas Nykodym, Paul Ogilvie, Mani Parkhe, Avesh Singh, Fen Xie, Matei Zaharia, Richard Zang, Juntai Zheng, and Corey Zumar. 2020. Developments in MLflow: A System to Accelerate the Machine Learning Lifecycle. In Proceedings of the Fourth International Workshop on Data Management for End-to-End Machine Learning (DEEM'20). Association for Computing Machinery, New York, NY, USA, Article 5, 4 pages.</p>
<p>Gopi Krishnan Rajbahadur, and Zhen Ming (Jack) Jiang. 2022. Towards Training Reproducible Deep Learning Models. Boyuan Chen, Mingzhi Wen, Yong Shi, Dayi Lin, Proceedings of the 44th International Conference on Software Engineering. the 44th International Conference on Software EngineeringNew York, NY, USAAssociation for Computing MachineryBoyuan Chen, Mingzhi Wen, Yong Shi, Dayi Lin, Gopi Krishnan Rajbahadur, and Zhen Ming (Jack) Jiang. 2022. Towards Training Reproducible Deep Learning Models. In Proceedings of the 44th International Conference on Software Engineering. Association for Computing Machinery, New York, NY, USA, 2202-2214.</p>
<p>On the use of simulation in robotics: Opportunities, challenges, and suggestions for moving forward. Heesun Choi, Cindy Crump, Christian Duriez, Asher Elmquist, Gregory Hager, David Han, Frank Hearl, Jessica Hodgins, Abhinandan Jain, Frederick Leve, Chen Li, Franziska Meier, Dan Negrut, Ludovic Righetti, Alberto Rodriguez, Jie Tan, Jeff Trinkle, Proceedings of the National Academy of Sciences. 1181907856118HeeSun Choi, Cindy Crump, Christian Duriez, Asher Elmquist, Gregory Hager, David Han, Frank Hearl, Jessica Hodgins, Abhinandan Jain, Frederick Leve, Chen Li, Franziska Meier, Dan Negrut, Ludovic Righetti, Alberto Rodriguez, Jie Tan, and Jeff Trinkle. 2021. On the use of simulation in robotics: Opportunities, challenges, and suggestions for moving forward. Proceedings of the National Academy of Sciences 118, 1 (2021), e1907856118.</p>
<p>KAIST Multi-Spectral Day/Night Data Set for Autonomous and Assisted Driving. Yukyung Choi, Namil Kim, Soonmin Hwang, Kibaek Park, 10.1109/TITS.2018.2791533IEEE Trans. Intell. Transp. Syst. 19Jae Shin Yoon, Kyounghwan An, and In So KweonYukyung Choi, Namil Kim, Soonmin Hwang, Kibaek Park, Jae Shin Yoon, Kyounghwan An, and In So Kweon. 2018. KAIST Multi-Spectral Day/Night Data Set for Autonomous and Assisted Driving. IEEE Trans. Intell. Transp. Syst. 19, 3 (2018), 934-948. https://doi.org/10.1109/TITS.2018.2791533</p>
<p>Deterministic Atomic Buffering. Yuan-Hsi Chou, Christopher Ng, Shaylin Cattell, Jeremy Intan, Matthew D Sinclair, Joseph Devietti, Timothy G Rogers, Tor M Aamodt, 53rd Annual IEEE/ACM International Symposium on Microarchitecture. Athens, Greece; Athens, GreeceIEEE2020Yuan-Hsi Chou, Christopher Ng, Shaylin Cattell, Jeremy Intan, Matthew D. Sinclair, Joseph Devietti, Timothy G. Rogers, and Tor M. Aamodt. 2020. Deterministic Atomic Buffering. In 53rd Annual IEEE/ACM International Symposium on Microarchitecture, MICRO 2020, Athens, Greece, October 17-21, 2020. IEEE, Athens, Greece, 981-995.</p>
<p>Electronic documents give reproducible research a new meaning. Jon F Claerbout, Martin Karrenbach, SEG Technical Program Expanded Abstracts 1992. Society of Exploration Geophysicists. New Orleans, USJon F. Claerbout and Martin Karrenbach. 2005. Electronic documents give reproducible research a new meaning. In SEG Technical Program Expanded Abstracts 1992. Society of Exploration Geophysicists, New Orleans, US, 601-604.</p>
<p>Threats of a Replication Crisis in Empirical Computer Science. Andy Cockburn, Pierre Dragicevic, Lonni Besançon, Carl Gutwin, 10.1145/3360311Commun. ACM. 63Andy Cockburn, Pierre Dragicevic, Lonni Besançon, and Carl Gutwin. 2020. Threats of a Replication Crisis in Empirical Computer Science. Commun. ACM 63, 8 (July 2020), 70-79. https://doi.org/10.1145/3360311</p>
<p>Protocol for development of a reporting guideline (TRIPOD-AI) and risk of bias tool (PROBAST-AI) for diagnostic and prognostic prediction model studies based on artificial intelligence. S Gary, Paula Collins, Constanza L Andaur Dhiman, Jie Navarro, Lotty Ma, Hooft, B Johannes, Patricia Reitsma, Logullo, L Andrew, Lily Beam, Ben Peng, Maarten Van Calster, Richard D Van Smeden, Riley, Moons, BMJ Open. 11Gary S Collins, Paula Dhiman, Constanza L Andaur Navarro, Jie Ma, Lotty Hooft, Johannes B Reitsma, Patricia Logullo, Andrew L Beam, Lily Peng, Ben Van Calster, Maarten van Smeden, Richard D Riley, and Karel GM Moons. 2021. Protocol for development of a reporting guideline (TRIPOD-AI) and risk of bias tool (PROBAST-AI) for diagnostic and prognostic prediction model studies based on artificial intelligence. BMJ Open 11, 7 (2021), 1-7.</p>
<p>Transparent reporting of a multivariable prediction model for individual prognosis or diagnosis (TRIPOD): the TRIPOD Statement. Gary S Collins, Johannes B Reitsma, Douglas G Altman, Moons, 10.1186/s12916-014-0241-zBMC Medicine. 13Gary S. Collins, Johannes B. Reitsma, Douglas G. Altman, and Karel GM Moons. 2015. Transparent reporting of a multivariable prediction model for individual prognosis or diagnosis (TRIPOD): the TRIPOD Statement. BMC Medicine 13 (2015). https://doi.org/10.1186/s12916-014-0241-z</p>
<p>A simple guide to Physical AI. Paulo João, Pedro Costeira, Lima, João Paulo Costeira and Pedro Lima. 2020. A simple guide to Physical AI. http://ai4eu.eu.June24</p>
<p>Data Version Control. Data Version Control 2022. https://dvc.org/doc</p>
<p>The Fienberg Problem: How to Allow Human Interactive Data Analysis in the Age of Differential Privacy. Cynthia Dwork, Jonathan Ullman, Journal of Privacy and Confidentiality. 8Cynthia Dwork and Jonathan Ullman. 2018. The Fienberg Problem: How to Allow Human Interactive Data Analysis in the Age of Differential Privacy. Journal of Privacy and Confidentiality 8, 1 (Dec. 2018), 1-10.</p>
<p>Interoperable Machine Learning Metadata using MEX. Diego Esteves, Diego Moussallem, Ciro Baron Neto, Jens Lehmann, Maria Cláudia Cavalcanti, Julio Cesar Duarte, CEUR-WS.orgProceedings of the ISWC 2015 Posters &amp; Demonstrations Track co-located with the 14th International Semantic Web Conference (ISWC-2015). Jeff Z. Pan, and Mauro Dragonithe ISWC 2015 Posters &amp; Demonstrations Track co-located with the 14th International Semantic Web Conference (ISWC-2015)Serena Villata; Bethlehem, PA, USA1486CEUR Workshop ProceedingsDiego Esteves, Diego Moussallem, Ciro Baron Neto, Jens Lehmann, Maria Cláudia Cavalcanti, and Julio Cesar Duarte. 2015. Interoperable Machine Learning Metadata using MEX. In Proceedings of the ISWC 2015 Posters &amp; Demonstrations Track co-located with the 14th International Semantic Web Conference (ISWC-2015), October 11, 2015 (CEUR Workshop Proceedings, Vol. 1486), Serena Villata, Jeff Z. Pan, and Mauro Dragoni (Eds.). CEUR-WS.org, Bethlehem, PA, USA.</p>
<p>Directorate General for Communications Networks, Content and Technology. 2020. The Assessment List for Trustworthy Artificial Intelligence (ALTAI) for self assessment. https:/data.europa.eu/doi/10.2759/002360Publications Office, LUEuropean CommissionEuropean Commission. Directorate General for Communications Networks, Content and Technology. 2020. The Assessment List for Trustworthy Artificial Intelligence (ALTAI) for self assessment. Publications Office, LU. https: //data.europa.eu/doi/10.2759/002360</p>
<p>Gym-Ignition: Reproducible Robotic Simulations for Reinforcement Learning. Diego Ferigo, Silvio Traversaro, Giorgio Metta, Daniele Pucci, 10.1109/SII46433.2020.9025951IEEE/SICE International Symposium on System Integration (SII). Honolulu, HI, USAIEEEDiego Ferigo, Silvio Traversaro, Giorgio Metta, and Daniele Pucci. 2020. Gym-Ignition: Reproducible Robotic Simulations for Reinforcement Learning. In IEEE/SICE International Symposium on System Integration (SII). IEEE, Honolulu, HI, USA, 885-890. https://doi.org/10.1109/SII46433.2020.9025951</p>
<p>Reproducible research environments with repo2docker. J Forde, T Head, C Holdgraf, Y Panda, G Nalvarete, B Ragan-Kelley, E Sundell, ICML workshop on Reproducible Machine Learning. J. Forde, T. Head, C. Holdgraf, Y. Panda, G. Nalvarete, B. Ragan-Kelley, and E. Sundell. 2018. Reproducible research environments with repo2docker. In In ICML workshop on Reproducible Machine Learning.</p>
<p>Augmenting PROV with Plans in P-PLAN: Scientific Processes as Linked Data. Daniel Garijo, Yolanda Gil, Proceedings of the 2nd International Workshop on Linked Science (CEUR Workshop Proceedings. the 2nd International Workshop on Linked Science (CEUR Workshop Proceedings951Daniel Garijo and Yolanda Gil. 2012. Augmenting PROV with Plans in P-PLAN: Scientific Processes as Linked Data. In Proceedings of the 2nd International Workshop on Linked Science (CEUR Workshop Proceedings, Vol. 951).</p>
<p>Abstract, link, publish, exploit: An end to end framework for workflow sharing. Daniel Garijo, Yolanda Gil, Óscar Corcho, 10.1016/j.future.2017.01.008Future Generation Comp. Syst. 75Daniel Garijo, Yolanda Gil, and Óscar Corcho. 2017. Abstract, link, publish, exploit: An end to end framework for workflow sharing. Future Generation Comp. Syst. 75 (2017), 271-283. https://doi.org/10.1016/j.future.2017.01.008</p>
<p>Timnit Gebru, Jamie Morgenstern, Briana Vecchione, Jennifer Wortman Vaughan, Hanna Wallach, Hal Daumé, Iii , Kate Crawford, arXiv:1803.09010Datasheets for Datasets. Timnit Gebru, Jamie Morgenstern, Briana Vecchione, Jennifer Wortman Vaughan, Hanna Wallach, Hal Daumé III, and Kate Crawford. 2020. Datasheets for Datasets. arXiv:1803.09010</p>
<p>Vision meets robotics: The KITTI dataset. A Geiger, P Lenz, C Stiller, R Urtasun, 10.1177/0278364913491297The International Journal of Robotics Research. 32A. Geiger, P. Lenz, C. Stiller, and R. Urtasun. 2013. Vision meets robotics: The KITTI dataset. The International Journal of Robotics Research 32, 11 (2013), 1231-1237. https://doi.org/10.1177/0278364913491297</p>
<p>Recomputation. org: Experiences of its first year and lessons learned. P Ian, Lars Gent, Kotthoff, 2014 IEEE/ACM 7th International Conference on Utility and Cloud Computing. London, UKIan P Gent and Lars Kotthoff. 2014. Recomputation. org: Experiences of its first year and lessons learned. In 2014 IEEE/ACM 7th International Conference on Utility and Cloud Computing. IEEE, London, UK, 968-973.</p>
<ol>
<li>Cardio-metabolic risk modeling and assessment through sensor-based measurements. Daniela Giorgi, Luca Bastiani, Maria Aurora Morales, Maria Antonietta Pascali, Sara Colantonio, Giuseppe Coppini, 10.1016/j.ijmedinf.2022.104823International Journal of Medical Informatics. 165Daniela Giorgi, Luca Bastiani, Maria Aurora Morales, Maria Antonietta Pascali, Sara Colantonio, and Giuseppe Coppini. 2022. Cardio-metabolic risk modeling and assessment through sensor-based measurements. International Journal of Medical Informatics 165 (2022), 104823. https://doi.org/10.1016/j.ijmedinf.2022.104823</li>
</ol>
<p>What does research reproducibility mean?. N Steven, Daniele Goodman, John P A Fanelli, Ioannidis, Science Translational Medicine. 8Steven N. Goodman, Daniele Fanelli, and John P. A. Ioannidis. 2016. What does research reproducibility mean? Science Translational Medicine 8, 341 (2016), 341ps12-341ps12.</p>
<p>Storylines of research in diffusion of innovation: a meta-narrative approach to systematic review. Trisha Greenhalgh, Glenn Robert, Fraser Macfarlane, Paul Bate, Olympia Kyriakidou, Richard Peacock, 10.1016/j.socscimed.2004.12.001Social Science &amp; Medicine. 61Trisha Greenhalgh, Glenn Robert, Fraser Macfarlane, Paul Bate, Olympia Kyriakidou, and Richard Peacock. 2005. Storylines of research in diffusion of innovation: a meta-narrative approach to systematic review. Social Science &amp; Medicine 61, 2 (2005), 417-430. https://doi.org/10.1016/j.socscimed.2004.12.001</p>
<p>Guides for deep learning submissions: IEEE Transactions on Information Forensics and Security. Guides for deep learning submissions: IEEE Transactions on Information Forensics and Security 2022.</p>
<p>A survey of methods for explaining black box models. Riccardo Guidotti, Anna Monreale, Salvatore Ruggieri, Franco Turini, Fosca Giannotti, Dino Pedreschi, ACM Computing Surveys (CSUR). 51Riccardo Guidotti, Anna Monreale, Salvatore Ruggieri, Franco Turini, Fosca Giannotti, and Dino Pedreschi. 2018. A survey of methods for explaining black box models. ACM Computing Surveys (CSUR) 51, 5 (2018), 1-42.</p>
<p>Standing on the Feet of Giants -Reproducibility in AI. Erik Odd, Gundersen, AI Magazine. 40Odd Erik Gundersen. 2019. Standing on the Feet of Giants -Reproducibility in AI. AI Magazine 40, 4 (2019), 9-23.</p>
<p>The fundamental principles of reproducibility. Erik Odd, Gundersen, 10.1098/rsta.2020.0210Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences. 37920200210Odd Erik Gundersen. 2021. The fundamental principles of reproducibility. Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences 379, 2197 (2021), 20200210. https://doi.org/10.1098/rsta.2020.0210</p>
<p>On Reproducible AI: Towards Reproducible Research, Open Science, and Digital Scholarship in AI Publications. Yolanda Odd Erik Gundersen, David W Gil, Aha, AI Magazine. 39Odd Erik Gundersen, Yolanda Gil, and David W. Aha. 2018. On Reproducible AI: Towards Reproducible Research, Open Science, and Digital Scholarship in AI Publications. AI Magazine 39, 3 (Sep. 2018), 56-68.</p>
<p>State of the Art: Reproducibility in Artificial Intelligence. Erik Odd, Sigbjørn Gundersen, Kjensmo, Proceedings of the the Thirty-Second AAAI Conference on Artificial Intelligence. the the Thirty-Second AAAI Conference on Artificial IntelligenceAAAI PressOdd Erik Gundersen and Sigbjørn Kjensmo. 2018. State of the Art: Reproducibility in Artificial Intelligence. In Proceedings of the the Thirty-Second AAAI Conference on Artificial Intelligence,. AAAI Press, 1644-1651.</p>
<p>Do machine learning platforms provide out-of-the-box reproducibility?. Saeid Odd Erik Gundersen, Richard Juul Shamsaliei, Isdahl, Future Generation Computer Systems. 126Odd Erik Gundersen, Saeid Shamsaliei, and Richard Juul Isdahl. 2022. Do machine learning platforms provide out-of-the-box reproducibility? Future Generation Computer Systems 126 (2022), 34-47.</p>
<p>The limits of replicability. Stephan Guttinger, European Journal for Philosophy of Science. 10Stephan Guttinger. 2020. The limits of replicability. European Journal for Philosophy of Science 10, 2 (2020), 1-17.</p>
<p>. Benjamin Haibe-Kains, George Alexandru Adam, Ahmed Hosny, Farnoosh Khodakarami, Thakkar Shraddha, Rebecca Kusko, Susanna-Assunta Sansone, Weida Tong, Russ D Wolfinger, Christopher E Mason, Wendell Jones, Joaquin Dopazo, Cesare Furlanello, Levi Waldron, Bo Wang, Chris McIntosh, Anna Goldenberg, Anshul Kundaje, Casey SBenjamin Haibe-Kains, George Alexandru Adam, Ahmed Hosny, Farnoosh Khodakarami, Thakkar Shraddha, Rebecca Kusko, Susanna-Assunta Sansone, Weida Tong, Russ D. Wolfinger, Christopher E. Mason, Wendell Jones, Joaquin Dopazo, Cesare Furlanello, Levi Waldron, Bo Wang, Chris McIntosh, Anna Goldenberg, Anshul Kundaje, Casey S.</p>
<p>Aerts, and Massive Analysis Quality Control (MAQC) Society Board of Directors. 2020. Transparency and reproducibility in artificial intelligence. Tamara Greene, Michael M Broderick, Jeffrey T Hoffman, Keegan Leek, Wolfgang Korthauer, Alvis Huber, Joelle Brazma, Robert Pineau, Trevor Tibshirani, Hastie, P A John, John Ioannidis, Quackenbush, J W L Hugo, 10.1038/s41586-020-2766-yNature. 586Greene, Tamara Broderick, Michael M. Hoffman, Jeffrey T. Leek, Keegan Korthauer, Wolfgang Huber, Alvis Brazma, Joelle Pineau, Robert Tibshirani, Trevor Hastie, John P. A. Ioannidis, John Quackenbush, Hugo J. W. L. Aerts, and Massive Analysis Quality Control (MAQC) Society Board of Directors. 2020. Transparency and reproducibility in artificial intelligence. Nature 586 (2020). https://doi.org/10.1038/s41586-020-2766-y</p>
<p>Beyond Methods Reproducibility in Machine Learning. Leif Hancox-Li, ML-Retrospectives, Surveys &amp; Meta-Analyses Workshop at NeurIPS 2020. Leif Hancox-Li. 2020. Beyond Methods Reproducibility in Machine Learning. In ML-Retrospectives, Surveys &amp; Meta- Analyses Workshop at NeurIPS 2020. https://ml-retrospectives.github.io/neurips2020/</p>
<p>. Matthew Hartley, S G Tjelvar, Olsson, dtoolAI: Reproducibility for Deep Learning. Patterns. 15Matthew Hartley and Tjelvar S.G. Olsson. 2020. dtoolAI: Reproducibility for Deep Learning. Patterns 1, 5 (2020).</p>
<p>Will Douglas Heaven. 2020. AI is wrestling with a replication crisis. MIT Technology Review November. 12Will Douglas Heaven. 2020. AI is wrestling with a replication crisis. MIT Technology Review November 12 (2020).</p>
<p>Reproducible Survival Prediction with SEER Cancer Data. Stefan Hegselmann, Leonard Gruelich, Julian Varghese, Martin Dugas, PMLRProceedings of the 3rd Machine Learning for Healthcare Conference (Proceedings of Machine Learning Research. Finale Doshi-Velez, Jim Fackler, Ken Jung, David Kale, Rajesh Ranganath, Byron Wallace, and Jenna Wiensthe 3rd Machine Learning for Healthcare Conference ( Machine Learning ResearchPalo Alto, California85Stefan Hegselmann, Leonard Gruelich, Julian Varghese, and Martin Dugas. 2018. Reproducible Survival Prediction with SEER Cancer Data. In Proceedings of the 3rd Machine Learning for Healthcare Conference (Proceedings of Machine Learning Research, Vol. 85), Finale Doshi-Velez, Jim Fackler, Ken Jung, David Kale, Rajesh Ranganath, Byron Wallace, and Jenna Wiens (Eds.). PMLR, Palo Alto, California, 49-66. https://proceedings.mlr.press/v85/hegselmann18a.html</p>
<p>Reproducibility standards for machine learning in the life sciences. J Benjamin, Michael M Heil, Florian Hoffman, Su-In Markowetz, Casey S Lee, Stephanie C Greene, Hicks, Nature Methods. 18Benjamin J. Heil, Michael M. Hoffman, Florian Markowetz, Su-In Lee, Casey S. Greene, and Stephanie C. Hicks. 2021. Reproducibility standards for machine learning in the life sciences. Nature Methods 18, 10 (Oct. 2021), 1132-1135.</p>
<p>Deep Reinforcement Learning That Matters. Peter Henderson, Riashat Islam, Philip Bachman, Joelle Pineau, Doina Precup, David Meger, Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18). the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18)New Orleans, Louisiana, USAAAAI PressPeter Henderson, Riashat Islam, Philip Bachman, Joelle Pineau, Doina Precup, and David Meger. 2018. Deep Reinforcement Learning That Matters. In Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18), New Orleans, Louisiana, USA, February 2-7, 2018. AAAI Press, 3207-3214.</p>
<p>Development of a Simulator for Prototyping Reinforcement Learning-Based Autonomous Cars. Martin Holen, Kristian Muri Knausgård, Morten Goodwin, Informatics. 9Martin Holen, Kristian Muri Knausgård, and Morten Goodwin. 2022. Development of a Simulator for Prototyping Reinforcement Learning-Based Autonomous Cars. Informatics 9, 2 (2022).</p>
<p>Artificial intelligence faces reproducibility crisis. Matthew Hutson, Science. 359Matthew Hutson. 2018. Artificial intelligence faces reproducibility crisis. Science 359 (2018), 725-726.</p>
<p>Sebastian Höfer, Kostas Bekris, Ankur Handa, Juan Camilo Gamboa, Florian Golemo, Melissa Mozifian, Chris Atkeson, Dieter Fox, Ken Goldberg, John Leonard, C Karen Liu, arXiv:2012.03806Welinder, and Martha White. 2020. Perspectives on Sim2Real Transfer for Robotics: A Summary of the R:SS 2020 Workshop. Peters, Shuran Song, PeterSebastian Höfer, Kostas Bekris, Ankur Handa, Juan Camilo Gamboa, Florian Golemo, Melissa Mozifian, Chris Atkeson, Dieter Fox, Ken Goldberg, John Leonard, C. Karen Liu, Jan Peters, Shuran Song, Peter Welinder, and Martha White. 2020. Perspectives on Sim2Real Transfer for Robotics: A Summary of the R:SS 2020 Workshop. arXiv:2012.03806</p>
<p>Sebastian Höfer, Kostas Bekris, Ankur Handa, Juan Camilo Gamboa, Melissa Mozifian, Florian Golemo, Chris Atkeson, Dieter Fox, Ken Goldberg, John Leonard, C Karen Liu, Jan Peters, Shuran Song, Peter Welinder, Martha White, 10.1109/TASE.2021.3064065Sim2Real in Robotics and Automation: Applications and Challenges. 18Sebastian Höfer, Kostas Bekris, Ankur Handa, Juan Camilo Gamboa, Melissa Mozifian, Florian Golemo, Chris Atkeson, Dieter Fox, Ken Goldberg, John Leonard, C. Karen Liu, Jan Peters, Shuran Song, Peter Welinder, and Martha White. 2021. Sim2Real in Robotics and Automation: Applications and Challenges. IEEE Transactions on Automation Science and Engineering 18, 2 (2021), 398-400. https://doi.org/10.1109/TASE.2021.3064065</p>
<p>Riashat Islam, Peter Henderson, Maziar Gomrokchi, Doina Precup, arXiv:1708.04133Reproducibility of Benchmarked Deep Reinforcement Learning Tasks for Continuous Control. Riashat Islam, Peter Henderson, Maziar Gomrokchi, and Doina Precup. 2017. Reproducibility of Benchmarked Deep Reinforcement Learning Tasks for Continuous Control. arXiv:1708.04133</p>
<p>Bibliographic coupling and its application to research-front and other core documents. Bo Jarneving, Journal of Informetrics. 1Bo Jarneving. 2007. Bibliographic coupling and its application to research-front and other core documents. Journal of Informetrics 1, 4 (2007), 287-307.</p>
<p>Reproducibility in critical care: a mortality prediction case study. E W Alistair, Tom J Johnson, Roger G Pollard, Mark, PMLRProceedings of the 2nd Machine Learning for Healthcare Conference (Proceedings of Machine Learning Research. Doshi-Velez, Jim Fackler, David Kale, Rajesh Ranganath, Byron Wallace, and Jenna Wiensthe 2nd Machine Learning for Healthcare Conference ( Machine Learning Research68Alistair E. W. Johnson, Tom J. Pollard, and Roger G. Mark. 2017. Reproducibility in critical care: a mortality prediction case study. In Proceedings of the 2nd Machine Learning for Healthcare Conference (Proceedings of Machine Learning Research, Vol. 68), Finale Doshi-Velez, Jim Fackler, David Kale, Rajesh Ranganath, Byron Wallace, and Jenna Wiens (Eds.). PMLR, 361-376. https://proceedings.mlr.press/v68/johnson17a.html</p>
<p>The International vocabulary of metrology -Basic and general concepts and associated terms -3rd edition with minor corrections. JCGM. Joint Committee for Guides in MetrologyJoint Committee for Guides in Metrology. 2012. The International vocabulary of metrology -Basic and general concepts and associated terms -3rd edition with minor corrections. JCGM (2012). https://www.bipm.org/utils/ common/documents/jcgm/JCGM_200_2012.pdf</p>
<p>GPUDet: a deterministic GPU architecture. Hadi Jooybar, W L Wilson, Fung, O&apos; Mike, Joseph Connor, Tor M Devietti, Aamodt, Architectural Support for Programming Languages and Operating Systems, ASPLOS 2013. Vivek Sarkar and Rastislav BodíkHouston, TX, USAACMHadi Jooybar, Wilson W. L. Fung, Mike O'Connor, Joseph Devietti, and Tor M. Aamodt. 2013. GPUDet: a deterministic GPU architecture. In Architectural Support for Programming Languages and Operating Systems, ASPLOS 2013, Houston, TX, USA, March 16-20, 2013, Vivek Sarkar and Rastislav Bodík (Eds.). ACM, 1-12.</p>
<p>Analysis of Randomization Effects on Sim2Real Transfer in Reinforcement Learning for Robotic Manipulation Tasks. Josip Josifovski, Mohammadhossein Malmir, Noah Klarmann, Nicolás Bare Luka Žagar, Alois Navarro-Guerrero, Knoll, arXiv:2206.06282Josip Josifovski, Mohammadhossein Malmir, Noah Klarmann, Bare Luka Žagar, Nicolás Navarro-Guerrero, and Alois Knoll. 2022. Analysis of Randomization Effects on Sim2Real Transfer in Reinforcement Learning for Robotic Manipulation Tasks. arXiv:2206.06282</p>
<p>Demystifying Reproducibility in Meta-and Multi-Task Reinforcement Learning. Ryan C Julian, K R Zentner, Avnish Narayan, Tsan Kwong Wong, Yonghyun Cho, Keren Zhu, Linda Wong, Chang Su, Zhanpeng He, Karol Hausman, Gaurav S Sukhatme, International Conference on Machine Learning. Ryan C. Julian, K. R. Zentner, Avnish Narayan, Tsan Kwong Wong, YongHyun Cho, Keren Zhu, Linda Wong, Chang Su, Zhanpeng He, Karol Hausman, and Gaurav S. Sukhatme. 2020. Demystifying Reproducibility in Meta-and Multi-Task Reinforcement Learning. In International Conference on Machine Learning.</p>
<p>Estimation of numerical reproducibility on CPU and GPU. Fabienne Jézéquel, Jean-Luc Lamotte, Issam Saïd, 10.15439/2015F29Federated Conference on Computer Science and Information Systems (FedCSIS). Fabienne Jézéquel, Jean-Luc Lamotte, and Issam Saïd. 2015. Estimation of numerical reproducibility on CPU and GPU. In Federated Conference on Computer Science and Information Systems (FedCSIS). 675-680. https://doi.org/10. 15439/2015F29</p>
<p>. Davinder Kaur, Suleyman Uslu, Kaley J Rittichier, Arjan Durresi, 10.1145/3491209Trustworthy Artificial Intelligence: A Review. ACM Comput. Surv., Article. 39Davinder Kaur, Suleyman Uslu, Kaley J. Rittichier, and Arjan Durresi. 2022. Trustworthy Artificial Intelligence: A Review. ACM Comput. Surv., Article 39 (Jan 2022), 38 pages. https://doi.org/10.1145/3491209</p>
<p>RE-EVALUATE: Reproducibility in Evaluating Reinforcement Learning Algorithms. Khimya Khetarpal, Zafarali Ahmed, Andre Cianflone, Riashat Islam, Joelle Pineau, 2nd Reproducibility in Machine Learning Workshop at. Khimya Khetarpal, Zafarali Ahmed, Andre Cianflone, Riashat Islam, and Joelle Pineau. 2018. RE-EVALUATE: Reproducibility in Evaluating Reinforcement Learning Algorithms. In 2nd Reproducibility in Machine Learning Workshop at ICML 2018.</p>
<p>Computational reproducibility in geoscientific papers: Insights from a series of studies with geoscientists and a reproduction study. Markus Konkol, Christian Kray, Max Pfeiffer, International Journal of Geographical Information Science. 33Markus Konkol, Christian Kray, and Max Pfeiffer. 2019. Computational reproducibility in geoscientific papers: Insights from a series of studies with geoscientists and a reproduction study. International Journal of Geographical Information Science 33, 2 (2019), 408-429.</p>
<p>Towards FAIR principles for research software. Anna-Lena Lamprecht, Leyla J García, Mateusz Kuzak, Carlos Martinez-Ortiz, Ricardo Arcila, Eva Martin Del Pico, Victoria Dominguez Del Angel, Stephanie Van De, Jon C Sandt, Paula Andrea Ison, Peter Martínez, Alfonso Mcquilton, Jennifer L Valencia, Fotis E Harrow, Josep Lluis Psomopoulos, Neil P Chue Gelpí, Carole A Hong, Salvador Goble, Capella-Gutiérrez, Data Sci. 3Anna-Lena Lamprecht, Leyla J. García, Mateusz Kuzak, Carlos Martinez-Ortiz, Ricardo Arcila, Eva Martin Del Pico, Victoria Dominguez Del Angel, Stephanie van de Sandt, Jon C. Ison, Paula Andrea Martínez, Peter McQuilton, Alfonso Valencia, Jennifer L. Harrow, Fotis E. Psomopoulos, Josep Lluis Gelpí, Neil P. Chue Hong, Carole A. Goble, and Salvador Capella-Gutiérrez. 2020. Towards FAIR principles for research software. Data Sci. 3, 1 (2020), 37-59.</p>
<p>The ACRV picking benchmark: A robotic shelf picking benchmark to foster reproducible research. Jürgen Leitner, Adam W Tow, Niko Sünderhauf, Jake E Dean, Joseph W Durham, Matthew Cooper, Markus Eich, Christopher Lehnert, Ruben Mangels, Christopher Mccool, Peter T Kujala, Lachlan Nicholson, Trung Pham, James Sergeant, Liao Wu, Fangyi Zhang, Ben Upcroft, Peter Corke, 10.1109/ICRA.2017.7989545IEEE International Conference on Robotics and Automation (ICRA). Jürgen Leitner, Adam W. Tow, Niko Sünderhauf, Jake E. Dean, Joseph W. Durham, Matthew Cooper, Markus Eich, Christopher Lehnert, Ruben Mangels, Christopher McCool, Peter T. Kujala, Lachlan Nicholson, Trung Pham, James Sergeant, Liao Wu, Fangyi Zhang, Ben Upcroft, and Peter Corke. 2017. The ACRV picking benchmark: A robotic shelf picking benchmark to foster reproducible research. In IEEE International Conference on Robotics and Automation (ICRA). 4705-4712. https://doi.org/10.1109/ICRA.2017.7989545</p>
<p>FUTURE-AI: Guiding Principles and Consensus Recommendations for Trustworthy Artificial Intelligence in Medical Imaging. Karim Lekadir, Richard Osuala, Catherine Gallin, Noussair Lazrak, Kaisar Kushibar, Gianna Tsakou, Susanna Aussó, Leonor Cerdá Alberich, Konstantinos Marias, Manolis Tsiknakis, Sara Colantonio, Nickolas Papanikolaou, Zohaib Salahuddin, Henry C Woodruff, Philippe Lambin, Luis Martí-Bonmatí, arXiv:2109.09658Karim Lekadir, Richard Osuala, Catherine Gallin, Noussair Lazrak, Kaisar Kushibar, Gianna Tsakou, Susanna Aussó, Leonor Cerdá Alberich, Konstantinos Marias, Manolis Tsiknakis, Sara Colantonio, Nickolas Papanikolaou, Zohaib Salahuddin, Henry C. Woodruff, Philippe Lambin, and Luis Martí-Bonmatí. 2021. FUTURE-AI: Guiding Principles and Consensus Recommendations for Trustworthy Artificial Intelligence in Medical Imaging. CoRR abs/2109.09658 (2021). arXiv:2109.09658</p>
<p>Bo Li, Peng Qi, Bo Liu, Shuai Di, Jingen Liu, Jiquan Pei, Jinfeng Yi, Bowen Zhou, 10.1145/3555803Trustworthy AI: From Principles to Practices. Bo Li, Peng Qi, Bo Liu, Shuai Di, Jingen Liu, Jiquan Pei, Jinfeng Yi, and Bowen Zhou. 2022. Trustworthy AI: From Principles to Practices. https://doi.org/10.1145/3555803 Just Accepted.</p>
<p>Multispectral Pedestrian Detection via Simultaneous Detection and Segmentation. Chengyang Li, British Machine Vision Conference. Newcastle, UKBMVA Press225Northumbria UniversityDan Song 0006, Ruofeng Tong, and Min Tang 0001Chengyang Li, Dan Song 0006, Ruofeng Tong, and Min Tang 0001. 2018. Multispectral Pedestrian Detection via Simultaneous Detection and Segmentation. In British Machine Vision Conference 2018, BMVC 2018, Northumbria University, Newcastle, UK, September 3-6, 2018. BMVA Press, 225.</p>
<p>On the Reproducibility and Replicability of Deep Learning in Software Engineering. Chao Liu, Cuiyun Gao, Xin Xia, David Lo, John C Grundy, Xiaohu Yang, ACM Trans. Softw. Eng. Methodol. 3146Chao Liu, Cuiyun Gao, Xin Xia, David Lo, John C. Grundy, and Xiaohu Yang. 2022. On the Reproducibility and Replicability of Deep Learning in Software Engineering. ACM Trans. Softw. Eng. Methodol. 31, 1 (2022), 15:1-15:46.</p>
<p>Reproducibility: Evaluating the Evaluations. Daniel Lopresti, George Nagy, Reproducible Research in Pattern Recognition -Third International Workshop, RRPR 2021, Virtual Event. Bertrand Kerautret, Miguel Colom, Adrien Krähenbühl, Daniel Lopresti, Pascal Monasse, and Hugues TalbotSpringer12636Revised Selected PapersDaniel Lopresti and George Nagy. 2021. Reproducibility: Evaluating the Evaluations. In Reproducible Research in Pattern Recognition -Third International Workshop, RRPR 2021, Virtual Event, January 11, 2021, Revised Selected Papers (Lecture Notes in Computer Science, Vol. 12636), Bertrand Kerautret, Miguel Colom, Adrien Krähenbühl, Daniel Lopresti, Pascal Monasse, and Hugues Talbot (Eds.). Springer, 12-23.</p>
<p>Are GANs Created Equal? A Large-Scale Study. Mario Lucic, Karol Kurach, Marcin Michalski, Sylvain Gelly, Olivier Bousquet, Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems. NeurIPS; Montréal, CanadaMario Lucic, Karol Kurach, Marcin Michalski, Sylvain Gelly, and Olivier Bousquet. 2018. Are GANs Created Equal? A Large-Scale Study. In Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montréal, Canada. 698-707.</p>
<p>Guidelines for Developing and Reporting Machine Learning Predictive Models in Biomedical Research: A Multidisciplinary View. Wei Luo, Dinh Phung, Truyen Tran, Sunil Gupta, Santu Rana, Chandan Karmakar, Alistair Shilton, John Yearwood, Nevenka Dimitrova, Tu Bao Ho, Svetha Venkatesh, Michael Berk, 10.2196/jmir.5870J Med Internet Res. 18Wei Luo, Dinh Phung, Truyen Tran, Sunil Gupta, Santu Rana, Chandan Karmakar, Alistair Shilton, John Yearwood, Nevenka Dimitrova, Tu Bao Ho, Svetha Venkatesh, and Michael Berk. 2016. Guidelines for Developing and Reporting Machine Learning Predictive Models in Biomedical Research: A Multidisciplinary View. J Med Internet Res 18, 12 (16 Dec 2016), e323. https://doi.org/10.2196/jmir.5870</p>
<p>A Survey on Reproducibility by Evaluating Deep Reinforcement Learning Algorithms on Real-World Robots. A Nicolai, Laura Lynnerup, Rasmus Nolling, John Hasle, Hallam, PMLRProceedings of the Conference on Robot Learning (Proceedings of Machine Learning Research. Kaelbling, Danica Kragic, and Komei Sugiurathe Conference on Robot Learning ( Machine Learning ResearchLeslie Pack100Nicolai A. Lynnerup, Laura Nolling, Rasmus Hasle, and John Hallam. 2020. A Survey on Reproducibility by Evaluating Deep Reinforcement Learning Algorithms on Real-World Robots. In Proceedings of the Conference on Robot Learning (Proceedings of Machine Learning Research, Vol. 100), Leslie Pack Kaelbling, Danica Kragic, and Komei Sugiura (Eds.). PMLR, 466-489. https://proceedings.mlr.press/v100/lynnerup20a.html</p>
<p>On Model Stability as a Function of Random Seed. Pranava Madhyastha, Rishabh Jain, 10.18653/v1/K19-1087Proceedings of the 23rd Conference on Computational Natural Language Learning. Mohit Bansal and Aline Villavicenciothe 23rd Conference on Computational Natural Language LearningHong Kong, ChinaPranava Madhyastha and Rishabh Jain. 2019. On Model Stability as a Function of Random Seed. In Proceedings of the 23rd Conference on Computational Natural Language Learning, CoNLL 2019, Hong Kong, China, November 3-4, 2019, Mohit Bansal and Aline Villavicencio (Eds.). 929-939. https://doi.org/10.18653/v1/K19-1087</p>
<p>Viktor Makoviychuk, Lukasz Wawrzyniak, Yunrong Guo, Michelle Lu, Kier Storey, Miles Macklin, David Hoeller, Nikita Rudin, Arthur Allshire, 10.48550/ARXIV.2108.10470Ankur Handa, and Gavriel State. 2021. Isaac Gym: High Performance GPU-Based Physics Simulation For Robot Learning. Viktor Makoviychuk, Lukasz Wawrzyniak, Yunrong Guo, Michelle Lu, Kier Storey, Miles Macklin, David Hoeller, Nikita Rudin, Arthur Allshire, Ankur Handa, and Gavriel State. 2021. Isaac Gym: High Performance GPU-Based Physics Simulation For Robot Learning. https://doi.org/10.48550/ARXIV.2108.10470</p>
<p>Reproducibility in machine learning for health research: Still a ways to go. B A Matthew, Shirly Mcdermott, Nikki Wang, Rajesh Marinsek, Luca Ranganath, Marzyeh Foschini, Ghassemi, 10.1126/scitranslmed.abb1655Science Translational Medicine. 13Matthew B. A. McDermott, Shirly Wang, Nikki Marinsek, Rajesh Ranganath, Luca Foschini, and Marzyeh Ghassemi. 2021. Reproducibility in machine learning for health research: Still a ways to go. Science Translational Medicine 13, 586 (2021), eabb1655. https://doi.org/10.1126/scitranslmed.abb1655</p>
<p>The reporting standards of randomised controlled trials in leading medical journals between 2019 and 2020: a systematic review. Mairead Mcerlean, Jack Samways, Peter J Godolphin, Yang Chen, 10.1007/s11845-022-02955-6Irish Journal of Medical Science. Mairead McErlean, Jack Samways, Peter J. Godolphin, and Yang Chen. 2022. The reporting standards of randomised controlled trials in leading medical journals between 2019 and 2020: a systematic review. Irish Journal of Medical Science (1971 -) (2022). https://doi.org/10.1007/s11845-022-02955-6</p>
<p>Deborah Mcguinness, Timothy Lebo, Satya Sahoo, PROV-O: The PROV Ontology. W3C Recommendation. W3C. Deborah McGuinness, Timothy Lebo, and Satya Sahoo. 2013. PROV-O: The PROV Ontology. W3C Recommendation. W3C. http://www.w3.org/TR/2013/REC-prov-o-20130430/</p>
<p>International evaluation of an AI system for breast cancer screening. Marcin Scott Mayer Mckinney, Varun Sieniek, Jonathan Godbole, Natasha Godwin, Hutan Antropova, Trevor Ashrafian, Mary Back, Greg S Chesus, Ara Corrado, Mozziyar Darzi, Florencia Etemadi, Fiona J Garcia-Vicente, Mark Gilbert, Demis Halling-Brown, Sunny Hassabis, Alan Jansen, Karthikesalingam, J Christopher, Dominic Kelly, King, R Joseph, David Ledsam, Hormuz Melnick, Lily Mostofi, Peng, Nature. Richard Sidebottom, Mustafa Suleyman, Daniel Tse, Kenneth C Young, Jeffrey De Fauw, and Shravya Shetty577Scott Mayer McKinney, Marcin Sieniek, Varun Godbole, Jonathan Godwin, Natasha Antropova, Hutan Ashrafian, Trevor Back, Mary Chesus, Greg S Corrado, Ara Darzi, Mozziyar Etemadi, Florencia Garcia-Vicente, Fiona J Gilbert, Mark Halling-Brown, Demis Hassabis, Sunny Jansen, Alan Karthikesalingam, Christopher J Kelly, Dominic King, Joseph R Ledsam, David Melnick, Hormuz Mostofi, Lily Peng, Joshua Jay Reicher, Bernardino Romera-Paredes, Richard Sidebottom, Mustafa Suleyman, Daniel Tse, Kenneth C Young, Jeffrey De Fauw, and Shravya Shetty. 2020. International evaluation of an AI system for breast cancer screening. Nature 577 (2020), 89-94.</p>
<p>Docker: Lightweight Linux Containers for Consistent Development and Deployment. Dirk Merkel, Linux J. 2392Dirk Merkel. 2014. Docker: Lightweight Linux Containers for Consistent Development and Deployment. Linux J. 2014, 239, Article 2 (mar 2014).</p>
<p>Replicability or reproducibility? On the replication crisis in computational neuroscience and sharing only relevant detail. Marcin Miłkowski, M Witold, Mateusz Hensel, Hohol, Journal of Computational Neuroscience. 45Marcin Miłkowski, Witold M Hensel, and Mateusz Hohol. 2018. Replicability or reproducibility? On the replication crisis in computational neuroscience and sharing only relevant detail. Journal of Computational Neuroscience 45, 3 (2018), 163-172.</p>
<p>Model Cards for Model Reporting. Margaret Mitchell, Simone Wu, Andrew Zaldivar, Parker Barnes, Lucy Vasserman, Ben Hutchinson, Elena Spitzer, Deborah Inioluwa, Timnit Raji, Gebru, 10.1145/3287560.3287596Proceedings of the Conference on Fairness, Accountability, and Transparency. the Conference on Fairness, Accountability, and TransparencyAtlanta GA USAACMMargaret Mitchell, Simone Wu, Andrew Zaldivar, Parker Barnes, Lucy Vasserman, Ben Hutchinson, Elena Spitzer, Inioluwa Deborah Raji, and Timnit Gebru. 2019. Model Cards for Model Reporting. In Proceedings of the Conference on Fairness, Accountability, and Transparency. ACM, Atlanta GA USA, 220-229. https://doi.org/10.1145/3287560.3287596</p>
<p>Human-level control through deep reinforcement learning. Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin A Riedmiller, Andreas Fidjeland, Georg Ostrovski, Stig Petersen, Charles Beattie, Amir Sadik, 10.1038/nature14236Nature. 518Ioannis Antonoglou. and Demis HassabisVolodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Bellemare, Alex Graves, Martin A. Riedmiller, Andreas Fidjeland, Georg Ostrovski, Stig Petersen, Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wierstra, Shane Legg, and Demis Hassabis. 2015. Human-level control through deep reinforcement learning. Nature 518, 7540 (2015), 529-533. https://doi.org/10.1038/nature14236</p>
<p>Checklist for Artificial Intelligence in Medical Imaging (CLAIM): A Guide for Authors and Reviewers. John Mongan, Linda Moy, Charles E Kahn, Radiology: Artificial Intelligence. 22John Mongan, Linda Moy, and Charles E. Kahn. 2020. Checklist for Artificial Intelligence in Medical Imaging (CLAIM): A Guide for Authors and Reviewers. Radiology: Artificial Intelligence 2, 2 (2020).</p>
<p>Marçal Mora-Cantallops, Salvador Sánchez-Alonso, Elena García-Barriocanal, and Miguel-Angel Sicilia. 2021. Traceability for Trustworthy AI: A Review of Models and Tools. Big Data and Cognitive Computing. 5Marçal Mora-Cantallops, Salvador Sánchez-Alonso, Elena García-Barriocanal, and Miguel-Angel Sicilia. 2021. Trace- ability for Trustworthy AI: A Review of Models and Tools. Big Data and Cognitive Computing 5, 2 (2021).</p>
<p>Miguel Morin, Matthew Willetts, arXiv:2001.11396Non-Determinism in TensorFlow ResNets. Miguel Morin and Matthew Willetts. 2020. Non-Determinism in TensorFlow ResNets. arXiv:2001.11396</p>
<p>EGAD! An Evolved Grasping Analysis Dataset for Diversity and Reproducibility in Robotic Manipulation. Douglas Morrison, Peter Corke, Jürgen Leitner, IEEE Robotics and Automation Letters. 5Douglas Morrison, Peter Corke, and Jürgen Leitner. 2020. EGAD! An Evolved Grasping Analysis Dataset for Diversity and Reproducibility in Robotic Manipulation. IEEE Robotics and Automation Letters 5, 3 (2020), 4368-4375.</p>
<p>The Impact of Nondeterminism on Reproducibility in Deep Reinforcement Learning. Prabhat Nagarajan, Garrett Warnell, Peter Stone, 2nd Reproducibility in Machine Learning Workshop at. Prabhat Nagarajan, Garrett Warnell, and Peter Stone. 2018. The Impact of Nondeterminism on Reproducibility in Deep Reinforcement Learning. In 2nd Reproducibility in Machine Learning Workshop at ICML 2018.</p>
<p>Reproducibility and Replicability in Science. 10.17226/25303The National Academies PressWashington, DCNational Academies of Sciences, Engineering, and MedicineNational Academies of Sciences, Engineering, and Medicine. 2019. Reproducibility and Replicability in Science. The National Academies Press, Washington, DC. https://doi.org/10.17226/25303</p>
<p>. Omar S Navarro Leija, Kelly Shiptoski, Ryan G Scott, Baojun Wang, Nicholas Renner, Ryan R Newton, Joseph Devietti, 10.1145/3373376.3378519162020. Reproducible Containers.Omar S. Navarro Leija, Kelly Shiptoski, Ryan G. Scott, Baojun Wang, Nicholas Renner, Ryan R. Newton, and Joseph Devietti. 2020. Reproducible Containers. , 16 pages. https://doi.org/10.1145/3373376.3378519</p>
<p>Minimum information about clinical artificial intelligence modeling: the MI-CLAIM checklist. Beau Norgeot, Giorgio Quer, K Brett, Ali Beaulieu-Jones, Raquel Torkamani, Milena Dias, Rima Gianfrancesco, Isaac S Arnaout, Suchi Kohane, Eric Saria, Ziad Topol, Bin Obermeyer, Yu, Butte, Nature Medicine. 26Beau Norgeot, Giorgio Quer, Brett K. Beaulieu-Jones, Ali Torkamani, Raquel Dias, Milena Gianfrancesco, Rima Arnaout, Isaac S. Kohane, Suchi Saria, Eric Topol, Ziad Obermeyer, Bin Yu, and Atul J Butte. 2020. Minimum information about clinical artificial intelligence modeling: the MI-CLAIM checklist. Nature Medicine 26 (2020).</p>
<p>Replicability, robustness, and reproducibility in psychological science. A Brian, Tom E Nosek, Hannah Hardwicke, Aurélien Moshontz, Katherine S Allard, Anna Corker, Fiona Dreber, Joe Fidler, Melissa Kline Hilgard, Struhl, Michèle B Nuijten, Annual Review of Psychology. 73Brian A Nosek, Tom E Hardwicke, Hannah Moshontz, Aurélien Allard, Katherine S Corker, Anna Dreber, Fiona Fidler, Joe Hilgard, Melissa Kline Struhl, Michèle B Nuijten, et al. 2022. Replicability, robustness, and reproducibility in psychological science. Annual Review of Psychology 73 (2022), 719-748.</p>
<p>Lightweight data management with dtool. Tjelvar Olsson, Matthew Hartley, PeerJ. 76562Tjelvar Olsson and Matthew Hartley. 2019. Lightweight data management with dtool. PeerJ 7 (03 2019), e6562.</p>
<p>AIR5: Five Pillars of Artificial Intelligence Research. Yew-Soon Ong, Abhishek Gupta, IEEE Transactions on Emerging Topics in Computational Intelligence. 3Yew-Soon Ong and Abhishek Gupta. 2019. AIR5: Five Pillars of Artificial Intelligence Research. IEEE Transactions on Emerging Topics in Computational Intelligence 3, 5 (2019), 411-415.</p>
<p>Synthesizing information systems knowledge: A typology of literature reviews. Guy Paré, Marie-Claude Trudel, Mirou Jaana, Spyros Kitsiou, Information &amp; Management. 52Guy Paré, Marie-Claude Trudel, Mirou Jaana, and Spyros Kitsiou. 2015. Synthesizing information systems knowledge: A typology of literature reviews. Information &amp; Management 52, 2 (2015), 183-199.</p>
<p>Explainable artificial intelligence models using real-world electronic healthÂ record data: a systematic scoping review. Zhaoyi Seyedeh Neelufar Payrovnaziri, Pablo Chen, Tim Rengifo-Moreno, Jiang Miller, Jonathan H Bian, Xiuwen Chen, Zhe Liu, He, Journal of the American Medical Informatics Association. 277Seyedeh Neelufar Payrovnaziri, Zhaoyi Chen, Pablo Rengifo-Moreno, Tim Miller, Jiang Bian, Jonathan H Chen, Xiuwen Liu, and Zhe He. 2020. Explainable artificial intelligence models using real-world electronic healthÂ record data: a systematic scoping review. Journal of the American Medical Informatics Association 27, 7 (05 2020), 1173-1185.</p>
<p>Reproducible research in computational science. D Roger, Peng, Science. 334Roger D. Peng. 2011. Reproducible research in computational science. Science 334, 6060 (2 Dec. 2011), 1226-1227.</p>
<p>Problems and Opportunities in Training Deep Learning Software Systems: An Analysis of Variance. Hung Viet Pham, Shangshu Qian, Jiannan Wang, Thibaud Lutellier, Jonathan Rosenthal, Lin Tan, Yaoliang Yu, Nachiappan Nagappan, 35th IEEE/ACM International Conference on Automated Software Engineering (ASE). New York, NY, USAAssociation for Computing MachineryHung Viet Pham, Shangshu Qian, Jiannan Wang, Thibaud Lutellier, Jonathan Rosenthal, Lin Tan, Yaoliang Yu, and Nachiappan Nagappan. 2020. Problems and Opportunities in Training Deep Learning Software Systems: An Analysis of Variance. In 35th IEEE/ACM International Conference on Automated Software Engineering (ASE). Association for Computing Machinery, New York, NY, USA, 771-783.</p>
<p>. Joelle Pineau, Philippe Vincent-Lamarre, Koustuv Sinha, Vincent Larivière, Alina Beygelzimer, Florence D&apos;alché, Emily Buc, Hugo Fox, Larochelle, arXiv:2003.12206Improving Reproducibility in Machine Learning Research (A Report from the NeurIPS 2019 Reproducibility ProgramJoelle Pineau, Philippe Vincent-Lamarre, Koustuv Sinha, Vincent Larivière, Alina Beygelzimer, Florence d'Alché Buc, Emily Fox, and Hugo Larochelle. 2020. Improving Reproducibility in Machine Learning Research (A Report from the NeurIPS 2019 Reproducibility Program). arXiv:2003.12206</p>
<p>Replicability: A Brief History of a Confused Terminology. Hans E Plesser, 10.3389/fninf.2017.00076Frontiers in Neuroinformatics. 1176Reproducibility vsHans E. Plesser. 2018. Reproducibility vs. Replicability: A Brief History of a Confused Terminology. Frontiers in Neuroinformatics 11 (Jan. 2018), 76. https://doi.org/10.3389/fninf.2017.00076</p>
<p>Gustavo Correa Publio, Diego Esteves, Agnieszka Ławrynowicz, Panče Panov, Larisa Soldatova, Tommaso Soru, Joaquin Vanschoren, Hamid Zafar, arXiv:1807.05351ML-Schema: Exposing the Semantics of Machine Learning with Schemas and Ontologies. Gustavo Correa Publio, Diego Esteves, Agnieszka Ławrynowicz, Panče Panov, Larisa Soldatova, Tommaso Soru, Joaquin Vanschoren, and Hamid Zafar. 2018. ML-Schema: Exposing the Semantics of Machine Learning with Schemas and Ontologies. arXiv:1807.05351</p>
<p>Aljoscha Pörtner, Martin Hoffmann, Matthias König, arXiv:1801.04199SwarmRob: A Toolkit for Reproducibility and Sharing of Experimental Artifacts in Robotics Research. Aljoscha Pörtner, Martin Hoffmann, and Matthias König. 2018. SwarmRob: A Toolkit for Reproducibility and Sharing of Experimental Artifacts in Robotics Research. arXiv:1801.04199</p>
<p>A Taxonomy of Tools for Reproducible Machine Learning Experiments. L Quaranta, F Calefato, F Lanubile, Proceedings of the AIxIA 2021 Discussion Papers Workshop (AIxIA DP 2021). the AIxIA 2021 Discussion Papers Workshop (AIxIA DP 2021)3078CEUR Workshop ProceedingsL. Quaranta, F. Calefato, and F. Lanubile. 2021. A Taxonomy of Tools for Reproducible Machine Learning Experiments. In Proceedings of the AIxIA 2021 Discussion Papers Workshop (AIxIA DP 2021), Vol. 3078. CEUR Workshop Proceedings, 65-76.</p>
<p>Edward Raff, arXiv:1909.06674Step Toward Quantifying Independently Reproducible Machine Learning Research. cs.LGEdward Raff. 2019. A Step Toward Quantifying Independently Reproducible Machine Learning Research. arXiv:1909.06674 [cs.LG]</p>
<p>Research Reproducibility as a Survival Analysis. Edward Raff, Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021, Thirty-Third Conference on Innovative Applications of Artificial Intelligence, IAAI 2021, The Eleventh Symposium on Educational Advances in Artificial Intelligence, EAAI 2021, Virtual Event. AAAI PressEdward Raff. 2021. Research Reproducibility as a Survival Analysis. In Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021, Thirty-Third Conference on Innovative Applications of Artificial Intelligence, IAAI 2021, The Eleventh Symposium on Educational Advances in Artificial Intelligence, EAAI 2021, Virtual Event, February 2-9, 2021. AAAI Press, 469-478.</p>
<p>Noel De Palma, and Nicolas Vuillerme. 2020. Variability and reproducibility in deep learning for medical image segmentation. Félix Renard, Soulaimane Guedria, 10.1038/s41598-020-69920-0Scientific Reports. 10Félix Renard, Soulaimane Guedria, Noel De Palma, and Nicolas Vuillerme. 2020. Variability and reproducibility in deep learning for medical image segmentation. Scientific Reports 10 (2020). https://doi.org/10.1038/s41598-020-69920-0</p>
<p>Guidelines for clinical trial protocols for interventions involving artificial intelligence: the SPIRIT-AI extension. Samantha Cruz Rivera, Xiaoxuan Liu, An-Wen Chan, Alastair K Denniston, Melanie J Calvert, Spirit-Ai The, Consort-Ai Working Group, The Lancet. 2Samantha Cruz Rivera, Xiaoxuan Liu, An-Wen Chan, Alastair K Denniston, Melanie J Calvert, The SPIRIT-AI, and CONSORT-AI Working Group. 2020. Guidelines for clinical trial protocols for interventions involving artificial intelligence: the SPIRIT-AI extension. The Lancet 2 (2020).</p>
<p>Adopting the YOLOv4 Architecture for Low-Latency Multispectral Pedestrian Detection in Autonomous Driving. Kamil Roszyk, Michal R Nowicki, Piotr Skrzypczynski, Sensors. 221082Kamil Roszyk, Michal R. Nowicki, and Piotr Skrzypczynski. 2022. Adopting the YOLOv4 Architecture for Low-Latency Multispectral Pedestrian Detection in Autonomous Driving. Sensors 22, 3 (2022), 1082.</p>
<p>Concept Drift. Claude Sammut, Michael Harries, 10.1007/978-0-387-30164-8_153Springer USBoston, MAClaude Sammut and Michael Harries. 2010. Concept Drift. Springer US, Boston, MA, 202-205. https://doi.org/10. 1007/978-0-387-30164-8_153</p>
<p>Machine learning pipelines: provenance, reproducibility and FAIR data principles. Sheeba Samuel, Frank Löffler, Birgitta König-Ries, Provenance and Annotation of Data and Processes. SpringerSheeba Samuel, Frank Löffler, and Birgitta König-Ries. 2020. Machine learning pipelines: provenance, reproducibility and FAIR data principles. In Provenance and Annotation of Data and Processes. Springer, 226-230.</p>
<p>Automatically tracking metadata and provenance of machine learning experiments. Sebastian Schelter, Joos-Hendrik Böse, Johannes Kirschnick, Thoralf Klein, Stephan Seufert, Sebastian Schelter, Joos-Hendrik Böse, Johannes Kirschnick, Thoralf Klein, and Stephan Seufert. 2017. Automatically tracking metadata and provenance of machine learning experiments. In NeurIPS 2017. https://www.amazon.science/ publications/automatically-tracking-metadata-and-provenance-of-machine-learning-experiments</p>
<p>Kenneth F Schulz, Douglas G Altman, David Moher, Group, CONSORT 2010 Statement: updated guidelines for reporting parallel group randomised trials. 8Kenneth F. Schulz, Douglas G. Altman, David Moher, and the CONSORT Group. 2010. CONSORT 2010 Statement: updated guidelines for reporting parallel group randomised trials. BMC Medicine 8 (2010).</p>
<p>Anti-Distillation: Improving reproducibility of deep networks. I Gil, Lorenzo Shamir, Coviello, arXiv:2010.09923Gil I. Shamir and Lorenzo Coviello. 2020. Anti-Distillation: Improving reproducibility of deep networks. (2020). arXiv:2010.09923</p>
<p>How to Do a Systematic Review: A Best Practice Guide for Conducting and Reporting Narrative Reviews, Meta-Analyses, and Meta-Syntheses. Andy P Siddaway, Alex M Wood, Larry V Hedges, 10.1146/annurev-psych-010418-102803Annual Review of Psychology. 70Andy P. Siddaway, Alex M. Wood, and Larry V. Hedges. 2019. How to Do a Systematic Review: A Best Practice Guide for Conducting and Reporting Narrative Reviews, Meta-Analyses, and Meta-Syntheses. Annual Review of Psychology 70, 1 (2019), 747-770. https://doi.org/10.1146/annurev-psych-010418-102803</p>
<p>Literature review as a research methodology: An overview and guidelines. Hannah Snyder, 10.1016/j.jbusres.2019.07.039Journal of Business Research. 104Hannah Snyder. 2019. Literature review as a research methodology: An overview and guidelines. Journal of Business Research 104 (2019), 333-339. https://doi.org/10.1016/j.jbusres.2019.07.039</p>
<p>Recommendations for Reporting Machine Learning Analyses in Clinical Research. Laura M Stevens, J Bobak, C Mortazavi, Lesley Deo, David P Curtis, Kao, 10.1161/CIRCOUTCOMES.120.006556Circulation: Cardiovascular Quality and Outcomes. 13Laura M. Stevens, Bobak J. Mortazavi, Rahul C. Deo, Lesley Curtis, and David P. Kao. 2020. Recommendations for Reporting Machine Learning Analyses in Clinical Research. Circulation: Cardiovascular Quality and Outcomes 13, 10 (2020), e006556. https://doi.org/10.1161/CIRCOUTCOMES.120.006556</p>
<p>Trust your science? Open your data and code. C Victoria, Stodden, Victoria C Stodden. 2011. Trust your science? Open your data and code. https://magazine.amstat.org/blog/2011/07/ 01/trust-your-science/</p>
<p>The reproducibility crisis in the age of digital medicine. Aaron Stupple, David Singerman, Leo Celi, 10.1038/s41746-019-0079-znpj Digital Medicine. 2Aaron Stupple, David Singerman, and Leo Celi. 2019. The reproducibility crisis in the age of digital medicine. npj Digital Medicine 2 (12 2019), 2. https://doi.org/10.1038/s41746-019-0079-z</p>
<p>Model-integrated computing. J Sztipanovits, G Karsai, Computer. 30J. Sztipanovits and G. Karsai. 1997. Model-integrated computing. Computer 30, 4 (1997), 110-111.</p>
<p>. A Tarnavsky-Eitan, E Smolyansky, S Knaan-Harpaz, Perets, A. Tarnavsky-Eitan, E. Smolyansky, I Knaan-Harpaz, and S. Perets. 2022. Connected Papers. https://ConnectedPapers. com</p>
<p>A Practical Taxonomy of Reproducibility for Machine Learning Research. Racheal Tatman, Jake Vanderplas, Sohier Dane, Reproducibility in Machine Learning Workshop at. Stockholm, SwedenRacheal Tatman, Jake VanderPlas, and Sohier Dane. 2018. A Practical Taxonomy of Reproducibility for Machine Learning Research. In Reproducibility in Machine Learning Workshop at ICML 2018. Stockholm, Sweden.</p>
<p>. Ethics Guidelines for Trustworthy Artificial Intelligence. European Commission the High-Level Expert Group on AIEuropean Commission the High-Level Expert Group on AI. 2019. Ethics Guidelines for Trustworthy Artificial Intelligence. https://digital-strategy.ec.europa.eu/en/library/ethics-guidelines-trustworthy-ai</p>
<p>Looking beyond the hype: Applied AI and machine learning in translational medicine. S Tzen, Frank Toh, Dennis Dondelinger, Wang, 10.1016/j.ebiom.2019.08.027EBioMedicine. 47Tzen S. Toh, Frank Dondelinger, and Dennis Wang. 2019. Looking beyond the hype: Applied AI and machine learning in translational medicine. EBioMedicine 47 (2019), 607-615. https://doi.org/10.1016/j.ebiom.2019.08.027</p>
<p>High-performance medicine: the convergence of human and artificial intelligence. J Eric, Topol, 10.1038/s41591-018-0300-7Nature Medicine. 25Eric J Topol. 2019. High-performance medicine: the convergence of human and artificial intelligence. Nature Medicine 25 (2019). https://doi.org/10.1038/s41591-018-0300-7</p>
<p>Boris Veytsman. 2022. IJCAI -Reproducibility Guidelines. Retrieved. Boris Veytsman. 2022. IJCAI -Reproducibility Guidelines. Retrieved jul 22, 2022 from https://ijcai-22.org/ reproducibility/#</p>
<p>Machine learning and artificial intelligence research for patient benefit: 20 critical questions on transparency, replicability, ethics, and effectiveness. Sebastian Vollmer, A Bilal, Gergo Mateen, Bohner, J Franz, Rayid Király, Pall Ghani, Sarah Jonsson, Adrian Cumbers, Jonas, S L Katherine, Puja Mcallister, David Myles, Mark Grainger, Richard Birse, Branson, G M Karel, Moons, S Gary, John P A Collins, Chris Ioannidis, Harry Holmes, Hemingway, 10.1136/bmj.l6927BMJ. 368Sebastian Vollmer, Bilal A Mateen, Gergo Bohner, Franz J Király, Rayid Ghani, Pall Jonsson, Sarah Cumbers, Adrian Jonas, Katherine S L McAllister, Puja Myles, David Grainger, Mark Birse, Richard Branson, Karel G M Moons, Gary S Collins, John P A Ioannidis, Chris Holmes, and Harry Hemingway. 2020. Machine learning and artificial intelligence research for patient benefit: 20 critical questions on transparency, replicability, ethics, and effectiveness. BMJ 368 (2020). https://doi.org/10.1136/bmj.l6927</p>
<p>Reporting of Model Performance and Statistical Methods in Studies That Use Machine Learning to Develop Clinical Prediction Models: Protocol for a Systematic Review. Colin George Wyllie Weaver, B Robert, Tyler Basmadjian, Kerry Williamson, Tolu Mcbrien, Devon Sajobi, Mohamed Boyne, Paul Everett Yusuf, Ronksley, 10.2196/30956JMIR Res Protoc. 11Colin George Wyllie Weaver, Robert B Basmadjian, Tyler Williamson, Kerry McBrien, Tolu Sajobi, Devon Boyne, Mohamed Yusuf, and Paul Everett Ronksley. 2022. Reporting of Model Performance and Statistical Methods in Studies That Use Machine Learning to Develop Clinical Prediction Models: Protocol for a Systematic Review. JMIR Res Protoc 11, 3 (3 Mar 2022), e30956. https://doi.org/10.2196/30956</p>
<p>Going on up to the SPIRIT in AI: will new reporting guidelines for clinical trials of AI interventions improve their rigour?. Paul Wicks, Xiaoxuan Liu, Alastair K Denniston, BMC Medicine. 18Paul Wicks, Xiaoxuan Liu, and Alastair K. Denniston. 2020. Going on up to the SPIRIT in AI: will new reporting guidelines for clinical trials of AI interventions improve their rigour? BMC Medicine 18 (2020).</p>
<p>Squib: Reproducibility in Computational Linguistics: Are We Willing to Share?. Martijn Wieling, Josine Rawee, Gertjan Van Noord, 10.1162/coli_a_00330Computational Linguistics. 44Martijn Wieling, Josine Rawee, and Gertjan van Noord. 2018. Squib: Reproducibility in Computational Linguistics: Are We Willing to Share? Computational Linguistics 44, 4 (Dec. 2018), 641-649. https://doi.org/10.1162/coli_a_00330</p>
<p>From transparency to accountability of intelligent systems: Moving beyond aspirations. Rebecca Williams, Richard Cloete, Jennifer Cobbe, Caitlin Cottrill, Peter Edwards, Milan Markovic, Iman Naja, Frances Ryan, Jatinder Singh, Wei Pang, 10.1017/dap.2021.37Data &amp; Policy. 4Rebecca Williams, Richard Cloete, Jennifer Cobbe, Caitlin Cottrill, Peter Edwards, Milan Markovic, Iman Naja, Frances Ryan, Jatinder Singh, Wei Pang, and et al. 2022. From transparency to accountability of intelligent systems: Moving beyond aspirations. Data &amp; Policy 4 (2022), e7. https://doi.org/10.1017/dap.2021.37</p>
<p>Transparency and Evaluation of Machine Learning in Health Applications. 10.5220/0010348306850692Proceedings of the 14th International Joint Conference on Biomedical Engineering Systems and Technologies -HEALTHINF. Janusz Wojtusiak. 2021. Reproducibilitythe 14th International Joint Conference on Biomedical Engineering Systems and Technologies -HEALTHINFVienna, AustriaINSTICC, SciTePressJanusz Wojtusiak. 2021. Reproducibility, Transparency and Evaluation of Machine Learning in Health Applications. In Proceedings of the 14th International Joint Conference on Biomedical Engineering Systems and Technologies -HEALTHINF,. INSTICC, SciTePress, Vienna, Austria, 685-692. https://doi.org/10.5220/0010348306850692</p>
<p>Guidance on Conducting a Systematic Literature Review. Yu Xiao, Maria Watson, 10.1177/0739456X17723971Journal of Planning Education and Research. 39Yu Xiao and Maria Watson. 2019. Guidance on Conducting a Systematic Literature Review. Journal of Planning Education and Research 39, 1 (2019), 93-112. https://doi.org/10.1177/0739456X17723971</p>
<p>Reinforcement learning for robot research: A comprehensive review and open issues. Tengteng Zhang, Hongwei Mo, 10.1177/17298814211007305International Journal of Advanced Robotic Systems. 183Tengteng Zhang and Hongwei Mo. 2021. Reinforcement learning for robot research: A comprehensive review and open issues. International Journal of Advanced Robotic Systems 18, 3 (2021). https://doi.org/10.1177/17298814211007305</p>
<p>Randomness in Neural Network Training: Characterizing the Impact of Tooling. Donglin Zhuang, Xingyao Zhang, Shuaiwen Song, Sara Hooker, Proceedings of Machine Learning and Systems. D. Marculescu, Y. Chi, and C. WuMachine Learning and SystemsSanta Clara, CA, USA4Donglin Zhuang, Xingyao Zhang, Shuaiwen Song, and Sara Hooker. 2022. Randomness in Neural Network Training: Characterizing the Impact of Tooling. In Proceedings of Machine Learning and Systems, D. Marculescu, Y. Chi, and C. Wu (Eds.), Vol. 4. Santa Clara, CA, USA, 316-336.</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3215 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3215</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3215</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-74.html">extraction-schema-74</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the agent, the memory mechanism, the tasks, and performance comparisons.</div>
                <p><strong>Paper ID:</strong> paper-71ae756c75ac89e2d731c9c79649562b5768ff39</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/71ae756c75ac89e2d731c9c79649562b5768ff39" target="_blank">Memory Networks</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Learning Representations</p>
                <p><strong>Paper TL;DR:</strong> This work describes a new class of learning models called memory networks, which reason with inference components combined with a long-term memory component; they learn how to use these jointly.</p>
                <p><strong>Paper Abstract:</strong> Abstract: We describe a new class of learning models called memory networks. Memory networks reason with inference components combined with a long-term memory component; they learn how to use these jointly. The long-term memory can be read and written to, with the goal of using it for prediction. We investigate these models in the context of question answering (QA) where the long-term memory effectively acts as a (dynamic) knowledge base, and the output is a textual response. We evaluate them on a large-scale QA task, and a smaller, but more complex, toy task generated from a simulated world. In the latter, we show the reasoning power of such models by chaining multiple supporting sentences to answer questions that require understanding the intension of verbs.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3215.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3215.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the agent, the memory mechanism, the tasks, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MemNN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Memory Network (MemNN / Memory Neural Network)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A neural architecture combining an explicit, compartmentalized long-term memory (array of slots) with learned modules I (input), G (generalization/write), O (output/read/inference) and R (response), trained to read/write memory and use retrieved facts for prediction and multi-hop reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>MemNN (Memory Network / Memory Neural Network)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Memory Network with an explicit memory array m of slots storing sentences or embeddings; modules I, G, O, R where I encodes inputs, G writes/updates memory slots, O retrieves k supporting memories via learned embedding scoring s_O (multi-hop retrieval, typically k up to 2), and R produces textual answers (either by returning retrieved sentences, ranking single-word answers, or an RNN conditioned on retrieved facts). Variants include time-feature augmented scoring, hashing-based retrieval for efficiency, segmentation for word-stream inputs, and unseen-word/context features.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>external compartmentalized memory (retrieval-augmented, multi-hop episodic-style memory stored in slots)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism_description</strong></td>
                            <td>Memories are stored as items in an array of slots (usually sentences or embeddings) via G (simple append or indexed slot H(x)); O retrieves relevant memories by scoring input and candidate memories with an embedding score s(x,y)=Φ_x(x)^T U^T U Φ_y(y), optionally in multiple hops (o1 = argmax s_O(x,m_i); o2 = argmax s_O([x,m_o1],m_i)). Time (write-order) features are added via s_{O_t}(x,y,y') that encodes relative ages; efficient lookup uses hashing (word-hash or clustered embedding buckets); unseen words are represented via left/right context bags and dropout-like training; responses are produced by R (word ranking or RNN/LSTM conditioned on [x,o1,o2]).</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Large-scale QA (Fader et al. dataset); Simulated-world story QA (synthetic simulation of actors, objects, rooms)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Large-scale QA: answer open-domain factual questions by re-ranking candidate answers from 14M extracted triples (REVERB extractions). Simulated-world QA: answer questions about short simulated stories requiring temporal and multi-step reasoning (e.g., locating objects after pick/drop actions), with controlled difficulty by varying how far back supporting facts lie.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>question answering</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Large-scale QA: F1 0.72 (embedding-only); F1 0.82 (embedding + BoW features). Hashing variants: cluster-hash F1 0.71 (embedding) / 0.80 (+BoW) with ~80x speedup (177k candidates); word-hash F1 0.63 / 0.68 with ~1000x speedup (13k candidates). Simulated-world QA (single-word answers): MemNN k=2 (+time) achieves ~100% accuracy across difficulty levels (e.g., Difficulty 5 actor and actor+object ≈ 100% / 99.9%). Multi-word answer setting: MemNN with LSTM response achieved 90.98% (vs baseline LSTM on raw words 14.01%).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Large-scale QA: baselines (non-MemNN) reported in paper: Fader et al. 0.54 F1; Bordes et al. (embedding baseline) 0.73 F1. Simulated-world QA: RNN baseline (no external memory) accuracy drops substantially with distance: e.g., Difficulty 5 actor+object 17.8% (RNN) and 29.0% (LSTM). MemNN ablations: MemNN k=1 (no multi-hop) performs poorly (e.g., Difficulty 5 actor+object 18.5%); MemNN k=1 (+time) improves but still below k=2.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Explicit external memory with learned retrieval (MemNN) enables accurate multi-hop, long-range reasoning on story QA where RNN/LSTM baselines struggle; multi-hop retrieval (k=2) plus write-time features are critical to solve tasks requiring chaining supporting facts and temporal order; hashing of embeddings provides large speedups with limited performance loss; unseen-word/context modeling is necessary for generalization to previously unseen nouns.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Linear-time lookup over large memories is slow (addressed via hashing at some accuracy cost); training used strong supervision (labeled supporting facts) which may not be available in weakly supervised settings; retrieval errors (s_O picking wrong facts) are primary failure mode; potential staleness if storing embeddings while embedding parameters change; forgetting/eviction strategies not experimentally explored.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Memory Networks', 'publication_date_yy_mm': '2014-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3215.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3215.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the agent, the memory mechanism, the tasks, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RNN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Recurrent Neural Network (language-modeling RNN baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Standard recurrent neural network language model used as a baseline; encodes history into hidden-state vectors and predicts outputs, but has limited compartmentalized long-term memory.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Recurrent neural network based language model</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>RNN (language-modeling recurrent neural network baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Standard RNN trained as a language model (backpropagation through time) and used as a baseline for QA; it processes input as a word stream and uses hidden states to capture history for prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>internal hidden-state memory (distributed, not compartmentalized)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism_description</strong></td>
                            <td>Memory implemented implicitly in the recurrent hidden state and network weights; trained via backprop through time to predict answer words; no explicit external memory or retrieval mechanism.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Simulated-world story QA (single-word and multi-word answer settings)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Baseline evaluated on the same synthetic story QA tasks; challenge is remembering facts that lie many sentences in the past and chaining multiple supporting facts.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>question answering</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Example from Table 3: Difficulty 1: actor w/o before 100%; actor 60.9%; actor+object 27.9%. Difficulty 5: actor 23.8%; actor+object 17.8%. Multi-word answer setting (Table 6 baseline 'Word features'): RNN 13.97%.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>RNNs can solve very short-range actor-only tasks but fail as temporal distance and reasoning complexity grow; poor performance attributed to inability to reliably encode long-range facts in hidden states compared to explicit memory models.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Difficulty retaining and using long-term facts (long-range dependencies); performance degrades quickly with increased temporal distance; no explicit retrieval leads to failure on multi-hop inference.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Memory Networks', 'publication_date_yy_mm': '2014-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3215.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3215.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the agent, the memory mechanism, the tasks, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LSTM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Long Short-Term Memory (LSTM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A recurrent neural network variant with gating mechanisms designed to improve learning of long-term dependencies; used as a stronger baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Long short-term memory</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>LSTM (baseline recurrent model with gated memory)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>LSTM recurrent network baseline that uses gated hidden-state mechanisms (input/forget/output gates) to better preserve information over longer time spans than vanilla RNNs; trained as a language model baseline and as the R component in some MemNN variants.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>internal gated hidden-state memory (improved capacity over vanilla RNN but still distributed)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism_description</strong></td>
                            <td>Memory realized in cell states with gating to alleviate vanishing gradients and retain longer-term information, but no external compartmentalized read/write operations; used both as an end-to-end baseline and as the R module conditioned on retrieved facts inside MemNN.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Simulated-world story QA; multi-word answer generation</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same simulated story QA tasks; LSTM is evaluated both as standalone language model baseline and as R (response) module inside MemNN.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>question answering</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Standalone baseline (Table 3): Difficulty 1 actor w/o before 100%; actor 64.8%; actor+object 49.1%. Difficulty 5: actor 35.2%; actor+object 29.0%. Multi-word answer baseline (Table 6 'Word features'): LSTM 14.01%. As MemNN's R module, MemNN+LSTM achieved 90.98% on multi-word answer task.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>LSTMs outperform vanilla RNNs on longer dependencies but still underperform MemNNs on multi-hop, long-distance story reasoning; using an LSTM as MemNN's R module yields strong generation performance, indicating benefit from coupling explicit retrieval with sequence generation.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Despite gating, LSTMs still struggle to reliably retrieve and chain facts that appear far back in the input stream; performance improves when combined with explicit retrieved memory (MemNN).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Memory Networks', 'publication_date_yy_mm': '2014-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3215.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e3215.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the agent, the memory mechanism, the tasks, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Neural Turing Machine</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Neural Turing Machine (NTM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A differentiable neural architecture with a large addressable memory that can be read and written by neural controllers; mentioned as related work and contrasted in experiments and scale.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Neural turing machines</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Neural Turing Machine (Graves et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A model combining a neural controller with an external differentiable memory and learned read/write heads (content- and location-based addressing) to perform algorithmic tasks requiring use of external storage.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>differentiable external addressable memory</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism_description</strong></td>
                            <td>Uses learned soft attention over a memory matrix with differentiable read/write operations controlled by the neural controller; designed to learn algorithms like copying/sorting via gradient descent.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Algorithmic sequence tasks (sorting, copying, recall) — mentioned as the primary tasks in the NTM paper</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>NTM was evaluated on synthetic algorithmic problems that require complex memory manipulations (e.g., sorting, copying), different from language QA evaluated by MemNN; in the MemNN paper the NTM is discussed in related work and contrasted in memory size and experimental focus.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>sequence/algorithmic tasks (related work mention)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>NTM is a closely related approach that also offers read/write external memory; MemNN notes differences in experimental focus and scale (NTM experiments used smaller memory sizes, MemNN scales to millions of text facts).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Memory Networks', 'publication_date_yy_mm': '2014-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3215.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e3215.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the agent, the memory mechanism, the tasks, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RNNSearch (Attention MT)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RNNSearch (Neural machine translation with learned alignment)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An encoder-decoder model with an attention/alignment mechanism over the input used in machine translation; presented as a related method that can be seen as a limited form of memory access (attention over a single sentence).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Neural machine translation by jointly learning to align and translate</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>RNNSearch (Bahdanau et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Sequence-to-sequence neural translation model that learns an alignment (attention) mechanism over encoder states to selectively access parts of the input sentence while generating translations.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>attention over encoder states (short-term input-focused memory)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism_description</strong></td>
                            <td>Learns soft attention weights over encoder hidden states for each output timestep, enabling dynamic selective access to parts of the (single-sentence) input; unlike MemNN's large external memory over many sentences.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Neural machine translation (related work)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Translating source sentences to target sentences using learned alignment to cope with long source sentences; cited as analogous to memory access but operating over a single sentence's encoder states.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>sequence-to-sequence translation (related work mention)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Attention/alignment can be interpreted as a short-range memory-access mechanism; cited to illustrate memory-like mechanisms in other recent neural models.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Memory Networks', 'publication_date_yy_mm': '2014-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Neural turing machines <em>(Rating: 2)</em></li>
                <li>Open question answering with weakly supervised embedding models <em>(Rating: 2)</em></li>
                <li>Paraphrase-driven learning for open question answering <em>(Rating: 2)</em></li>
                <li>Long short-term memory <em>(Rating: 1)</em></li>
                <li>Recurrent neural network based language model <em>(Rating: 1)</em></li>
                <li>Neural machine translation by jointly learning to align and translate <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3215",
    "paper_id": "paper-71ae756c75ac89e2d731c9c79649562b5768ff39",
    "extraction_schema_id": "extraction-schema-74",
    "extracted_data": [
        {
            "name_short": "MemNN",
            "name_full": "Memory Network (MemNN / Memory Neural Network)",
            "brief_description": "A neural architecture combining an explicit, compartmentalized long-term memory (array of slots) with learned modules I (input), G (generalization/write), O (output/read/inference) and R (response), trained to read/write memory and use retrieved facts for prediction and multi-hop reasoning.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "MemNN (Memory Network / Memory Neural Network)",
            "agent_description": "Memory Network with an explicit memory array m of slots storing sentences or embeddings; modules I, G, O, R where I encodes inputs, G writes/updates memory slots, O retrieves k supporting memories via learned embedding scoring s_O (multi-hop retrieval, typically k up to 2), and R produces textual answers (either by returning retrieved sentences, ranking single-word answers, or an RNN conditioned on retrieved facts). Variants include time-feature augmented scoring, hashing-based retrieval for efficiency, segmentation for word-stream inputs, and unseen-word/context features.",
            "memory_used": true,
            "memory_type": "external compartmentalized memory (retrieval-augmented, multi-hop episodic-style memory stored in slots)",
            "memory_mechanism_description": "Memories are stored as items in an array of slots (usually sentences or embeddings) via G (simple append or indexed slot H(x)); O retrieves relevant memories by scoring input and candidate memories with an embedding score s(x,y)=Φ_x(x)^T U^T U Φ_y(y), optionally in multiple hops (o1 = argmax s_O(x,m_i); o2 = argmax s_O([x,m_o1],m_i)). Time (write-order) features are added via s_{O_t}(x,y,y') that encodes relative ages; efficient lookup uses hashing (word-hash or clustered embedding buckets); unseen words are represented via left/right context bags and dropout-like training; responses are produced by R (word ranking or RNN/LSTM conditioned on [x,o1,o2]).",
            "task_name": "Large-scale QA (Fader et al. dataset); Simulated-world story QA (synthetic simulation of actors, objects, rooms)",
            "task_description": "Large-scale QA: answer open-domain factual questions by re-ranking candidate answers from 14M extracted triples (REVERB extractions). Simulated-world QA: answer questions about short simulated stories requiring temporal and multi-step reasoning (e.g., locating objects after pick/drop actions), with controlled difficulty by varying how far back supporting facts lie.",
            "task_type": "question answering",
            "performance_with_memory": "Large-scale QA: F1 0.72 (embedding-only); F1 0.82 (embedding + BoW features). Hashing variants: cluster-hash F1 0.71 (embedding) / 0.80 (+BoW) with ~80x speedup (177k candidates); word-hash F1 0.63 / 0.68 with ~1000x speedup (13k candidates). Simulated-world QA (single-word answers): MemNN k=2 (+time) achieves ~100% accuracy across difficulty levels (e.g., Difficulty 5 actor and actor+object ≈ 100% / 99.9%). Multi-word answer setting: MemNN with LSTM response achieved 90.98% (vs baseline LSTM on raw words 14.01%).",
            "performance_without_memory": "Large-scale QA: baselines (non-MemNN) reported in paper: Fader et al. 0.54 F1; Bordes et al. (embedding baseline) 0.73 F1. Simulated-world QA: RNN baseline (no external memory) accuracy drops substantially with distance: e.g., Difficulty 5 actor+object 17.8% (RNN) and 29.0% (LSTM). MemNN ablations: MemNN k=1 (no multi-hop) performs poorly (e.g., Difficulty 5 actor+object 18.5%); MemNN k=1 (+time) improves but still below k=2.",
            "has_performance_comparison": true,
            "key_findings": "Explicit external memory with learned retrieval (MemNN) enables accurate multi-hop, long-range reasoning on story QA where RNN/LSTM baselines struggle; multi-hop retrieval (k=2) plus write-time features are critical to solve tasks requiring chaining supporting facts and temporal order; hashing of embeddings provides large speedups with limited performance loss; unseen-word/context modeling is necessary for generalization to previously unseen nouns.",
            "limitations_or_challenges": "Linear-time lookup over large memories is slow (addressed via hashing at some accuracy cost); training used strong supervision (labeled supporting facts) which may not be available in weakly supervised settings; retrieval errors (s_O picking wrong facts) are primary failure mode; potential staleness if storing embeddings while embedding parameters change; forgetting/eviction strategies not experimentally explored.",
            "uuid": "e3215.0",
            "source_info": {
                "paper_title": "Memory Networks",
                "publication_date_yy_mm": "2014-10"
            }
        },
        {
            "name_short": "RNN",
            "name_full": "Recurrent Neural Network (language-modeling RNN baseline)",
            "brief_description": "Standard recurrent neural network language model used as a baseline; encodes history into hidden-state vectors and predicts outputs, but has limited compartmentalized long-term memory.",
            "citation_title": "Recurrent neural network based language model",
            "mention_or_use": "use",
            "agent_name": "RNN (language-modeling recurrent neural network baseline)",
            "agent_description": "Standard RNN trained as a language model (backpropagation through time) and used as a baseline for QA; it processes input as a word stream and uses hidden states to capture history for prediction.",
            "memory_used": true,
            "memory_type": "internal hidden-state memory (distributed, not compartmentalized)",
            "memory_mechanism_description": "Memory implemented implicitly in the recurrent hidden state and network weights; trained via backprop through time to predict answer words; no explicit external memory or retrieval mechanism.",
            "task_name": "Simulated-world story QA (single-word and multi-word answer settings)",
            "task_description": "Baseline evaluated on the same synthetic story QA tasks; challenge is remembering facts that lie many sentences in the past and chaining multiple supporting facts.",
            "task_type": "question answering",
            "performance_with_memory": "Example from Table 3: Difficulty 1: actor w/o before 100%; actor 60.9%; actor+object 27.9%. Difficulty 5: actor 23.8%; actor+object 17.8%. Multi-word answer setting (Table 6 baseline 'Word features'): RNN 13.97%.",
            "performance_without_memory": null,
            "has_performance_comparison": true,
            "key_findings": "RNNs can solve very short-range actor-only tasks but fail as temporal distance and reasoning complexity grow; poor performance attributed to inability to reliably encode long-range facts in hidden states compared to explicit memory models.",
            "limitations_or_challenges": "Difficulty retaining and using long-term facts (long-range dependencies); performance degrades quickly with increased temporal distance; no explicit retrieval leads to failure on multi-hop inference.",
            "uuid": "e3215.1",
            "source_info": {
                "paper_title": "Memory Networks",
                "publication_date_yy_mm": "2014-10"
            }
        },
        {
            "name_short": "LSTM",
            "name_full": "Long Short-Term Memory (LSTM)",
            "brief_description": "A recurrent neural network variant with gating mechanisms designed to improve learning of long-term dependencies; used as a stronger baseline.",
            "citation_title": "Long short-term memory",
            "mention_or_use": "use",
            "agent_name": "LSTM (baseline recurrent model with gated memory)",
            "agent_description": "LSTM recurrent network baseline that uses gated hidden-state mechanisms (input/forget/output gates) to better preserve information over longer time spans than vanilla RNNs; trained as a language model baseline and as the R component in some MemNN variants.",
            "memory_used": true,
            "memory_type": "internal gated hidden-state memory (improved capacity over vanilla RNN but still distributed)",
            "memory_mechanism_description": "Memory realized in cell states with gating to alleviate vanishing gradients and retain longer-term information, but no external compartmentalized read/write operations; used both as an end-to-end baseline and as the R module conditioned on retrieved facts inside MemNN.",
            "task_name": "Simulated-world story QA; multi-word answer generation",
            "task_description": "Same simulated story QA tasks; LSTM is evaluated both as standalone language model baseline and as R (response) module inside MemNN.",
            "task_type": "question answering",
            "performance_with_memory": "Standalone baseline (Table 3): Difficulty 1 actor w/o before 100%; actor 64.8%; actor+object 49.1%. Difficulty 5: actor 35.2%; actor+object 29.0%. Multi-word answer baseline (Table 6 'Word features'): LSTM 14.01%. As MemNN's R module, MemNN+LSTM achieved 90.98% on multi-word answer task.",
            "performance_without_memory": null,
            "has_performance_comparison": true,
            "key_findings": "LSTMs outperform vanilla RNNs on longer dependencies but still underperform MemNNs on multi-hop, long-distance story reasoning; using an LSTM as MemNN's R module yields strong generation performance, indicating benefit from coupling explicit retrieval with sequence generation.",
            "limitations_or_challenges": "Despite gating, LSTMs still struggle to reliably retrieve and chain facts that appear far back in the input stream; performance improves when combined with explicit retrieved memory (MemNN).",
            "uuid": "e3215.2",
            "source_info": {
                "paper_title": "Memory Networks",
                "publication_date_yy_mm": "2014-10"
            }
        },
        {
            "name_short": "Neural Turing Machine",
            "name_full": "Neural Turing Machine (NTM)",
            "brief_description": "A differentiable neural architecture with a large addressable memory that can be read and written by neural controllers; mentioned as related work and contrasted in experiments and scale.",
            "citation_title": "Neural turing machines",
            "mention_or_use": "mention",
            "agent_name": "Neural Turing Machine (Graves et al.)",
            "agent_description": "A model combining a neural controller with an external differentiable memory and learned read/write heads (content- and location-based addressing) to perform algorithmic tasks requiring use of external storage.",
            "memory_used": true,
            "memory_type": "differentiable external addressable memory",
            "memory_mechanism_description": "Uses learned soft attention over a memory matrix with differentiable read/write operations controlled by the neural controller; designed to learn algorithms like copying/sorting via gradient descent.",
            "task_name": "Algorithmic sequence tasks (sorting, copying, recall) — mentioned as the primary tasks in the NTM paper",
            "task_description": "NTM was evaluated on synthetic algorithmic problems that require complex memory manipulations (e.g., sorting, copying), different from language QA evaluated by MemNN; in the MemNN paper the NTM is discussed in related work and contrasted in memory size and experimental focus.",
            "task_type": "sequence/algorithmic tasks (related work mention)",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_comparison": false,
            "key_findings": "NTM is a closely related approach that also offers read/write external memory; MemNN notes differences in experimental focus and scale (NTM experiments used smaller memory sizes, MemNN scales to millions of text facts).",
            "limitations_or_challenges": null,
            "uuid": "e3215.3",
            "source_info": {
                "paper_title": "Memory Networks",
                "publication_date_yy_mm": "2014-10"
            }
        },
        {
            "name_short": "RNNSearch (Attention MT)",
            "name_full": "RNNSearch (Neural machine translation with learned alignment)",
            "brief_description": "An encoder-decoder model with an attention/alignment mechanism over the input used in machine translation; presented as a related method that can be seen as a limited form of memory access (attention over a single sentence).",
            "citation_title": "Neural machine translation by jointly learning to align and translate",
            "mention_or_use": "mention",
            "agent_name": "RNNSearch (Bahdanau et al.)",
            "agent_description": "Sequence-to-sequence neural translation model that learns an alignment (attention) mechanism over encoder states to selectively access parts of the input sentence while generating translations.",
            "memory_used": true,
            "memory_type": "attention over encoder states (short-term input-focused memory)",
            "memory_mechanism_description": "Learns soft attention weights over encoder hidden states for each output timestep, enabling dynamic selective access to parts of the (single-sentence) input; unlike MemNN's large external memory over many sentences.",
            "task_name": "Neural machine translation (related work)",
            "task_description": "Translating source sentences to target sentences using learned alignment to cope with long source sentences; cited as analogous to memory access but operating over a single sentence's encoder states.",
            "task_type": "sequence-to-sequence translation (related work mention)",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_comparison": false,
            "key_findings": "Attention/alignment can be interpreted as a short-range memory-access mechanism; cited to illustrate memory-like mechanisms in other recent neural models.",
            "limitations_or_challenges": null,
            "uuid": "e3215.4",
            "source_info": {
                "paper_title": "Memory Networks",
                "publication_date_yy_mm": "2014-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Neural turing machines",
            "rating": 2
        },
        {
            "paper_title": "Open question answering with weakly supervised embedding models",
            "rating": 2
        },
        {
            "paper_title": "Paraphrase-driven learning for open question answering",
            "rating": 2
        },
        {
            "paper_title": "Long short-term memory",
            "rating": 1
        },
        {
            "paper_title": "Recurrent neural network based language model",
            "rating": 1
        },
        {
            "paper_title": "Neural machine translation by jointly learning to align and translate",
            "rating": 1
        }
    ],
    "cost": 0.015657749999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>MEMORY NETWORKS</h1>
<p>Jason Weston, Sumit Chopra \&amp; Antoine Bordes<br>Facebook AI Research<br>770 Broadway<br>New York, USA<br>{jase, spchopra, abordes}@fb.com</p>
<h4>Abstract</h4>
<p>We describe a new class of learning models called memory networks. Memory networks reason with inference components combined with a long-term memory component; they learn how to use these jointly. The long-term memory can be read and written to, with the goal of using it for prediction. We investigate these models in the context of question answering (QA) where the long-term memory effectively acts as a (dynamic) knowledge base, and the output is a textual response. We evaluate them on a large-scale QA task, and a smaller, but more complex, toy task generated from a simulated world. In the latter, we show the reasoning power of such models by chaining multiple supporting sentences to answer questions that require understanding the intension of verbs.</p>
<h2>1 INTRODUCTION</h2>
<p>Most machine learning models lack an easy way to read and write to part of a (potentially very large) long-term memory component, and to combine this seamlessly with inference. Hence, they do not take advantage of one of the great assets of a modern day computer. For example, consider the task of being told a set of facts or a story, and then having to answer questions on that subject. In principle this could be achieved by a language modeler such as a recurrent neural network (RNN) (Mikolov et al., 2010; Hochreiter \&amp; Schmidhuber, 1997) as these models are trained to predict the next (set of) word(s) to output after having read a stream of words. However, their memory (encoded by hidden states and weights) is typically too small, and is not compartmentalized enough to accurately remember facts from the past (knowledge is compressed into dense vectors). RNNs are known to have difficulty in performing memorization, for example the simple copying task of outputting the same input sequence they have just read (Zaremba \&amp; Sutskever, 2014). The situation is similar for other tasks, e.g., in the vision and audio domains a long term memory is required to watch a movie and answer questions about it.</p>
<p>In this work, we introduce a class of models called memory networks that attempt to rectify this problem. The central idea is to combine the successful learning strategies developed in the machine learning literature for inference with a memory component that can be read and written to. The model is then trained to learn how to operate effectively with the memory component. We introduce the general framework in Section 2, and present a specific implementation in the text domain for the task of question answering in Section 3. We discuss related work in Section 4, describe our experiments in 5, and finally conclude in Section 6.</p>
<h2>2 MEMORY NETWORKS</h2>
<p>A memory network consists of a memory $\mathbf{m}$ (an array of objects ${ }^{1}$ indexed by $\mathbf{m}_{i}$ ) and four (potentially learned) components $I, G, O$ and $R$ as follows:</p>
<p>I: (input feature map) - converts the incoming input to the internal feature representation.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>G: (generalization) - updates old memories given the new input. We call this generalization as there is an opportunity for the network to compress and generalize its memories at this stage for some intended future use.
O: (output feature map) - produces a new output (in the feature representation space), given the new input and the current memory state.
R: (response) - converts the output into the response format desired. For example, a textual response or an action.</p>
<p>Given an input $x$ (e.g., an input character, word or sentence depending on the granularity chosen, an image or an audio signal) the flow of the model is as follows:</p>
<ol>
<li>Convert $x$ to an internal feature representation $I(x)$.</li>
<li>Update memories $\mathbf{m}<em i="i">{i}$ given the new input: $\mathbf{m}</em>\right), \forall i$.}=G\left(\mathbf{m}_{i}, I(x), \mathbf{m</li>
<li>Compute output features $o$ given the new input and the memory: $o=O(I(x), \mathbf{m})$.</li>
<li>Finally, decode output features $o$ to give the final response: $r=R(o)$.</li>
</ol>
<p>This process is applied at both train and test time, if there is a distinction between such phases, that is, memories are also stored at test time, but the model parameters of I, G, O and R are not updated. Memory networks cover a wide class of possible implementations. The components $I, G, O$ and $R$ can potentially use any existing ideas from the machine learning literature, e.g., make use of your favorite models (SVMs, decision trees, etc.).
$I$ component: Component $I$ can make use of standard pre-processing, e.g., parsing, coreference and entity resolution for text inputs. It could also encode the input into an internal feature representation, e.g., convert from text to a sparse or dense feature vector.
$G$ component: The simplest form of $G$ is to store $I(x)$ in a "slot" in the memory:</p>
<p>$$
\mathbf{m}_{H(x)}=I(x)
$$</p>
<p>where $H($.$) is a function selecting the slot. That is, G$ updates the index $H(x)$ of $\mathbf{m}$, but all other parts of the memory remain untouched. More sophisticated variants of $G$ could go back and update earlier stored memories (potentially, all memories) based on the new evidence from the current input $x$. If the input is at the character or word level one could group inputs (i.e., by segmenting them into chunks) and store each chunk in a memory slot.</p>
<p>If the memory is huge (e.g., consider all of Freebase or Wikipedia) one needs to organize the memories. This can be achieved with the slot choosing function $H$ just described: for example, it could be designed, or trained, to store memories by entity or topic. Consequently, for efficiency at scale, $G$ (and $O$ ) need not operate on all memories: they can operate on only a retrieved subset of candidates (only operating on memories that are on the right topic). We explore a simple variant of this in our experiments.</p>
<p>If the memory becomes full, a procedure for "forgetting" could also be implemented by $H$ as it chooses which memory is replaced, e.g., $H$ could score the utility of each memory, and overwrite the least useful. We have not explored this experimentally yet.
$O$ and $R$ components: The $O$ component is typically responsible for reading from memory and performing inference, e.g., calculating what are the relevant memories to perform a good response. The $R$ component then produces the final response given $O$. For example in a question answering setup $O$ finds relevant memories, and then $R$ produces the actual wording of the answer, e.g., $R$ could be an RNN that is conditioned on the output of $O$. Our hypothesis is that without conditioning on such memories, such an RNN will perform poorly.</p>
<h1>3 A MemNN Implementation For Text</h1>
<p>One particular instantiation of a memory network is where the components are neural networks. We refer to these as memory neural networks (MemNNs). In this section we describe a relatively simple implementation of a MemNN with textual input and output.</p>
<h1>3.1 BASIC MODEL</h1>
<p>In our basic architecture, the $I$ module takes an input text. Let us first assume this to be a sentence: either the statement of a fact, or a question to be answered by the system (later we will consider word-based input sequences). The text is stored in the next available memory slot in its original form ${ }^{2}$, i.e., $S(x)$ returns the next empty memory slot $N: \mathbf{m}_{N}=x, N=N+1$. The $G$ module is thus only used to store this new memory, so old memories are not updated. More sophisticated models are described in subsequent sections.</p>
<p>The core of inference lies in the $O$ and $R$ modules. The $O$ module produces output features by finding $k$ supporting memories given $x$. We use $k$ up to 2 , but the procedure is generalizable to larger $k$. For $k=1$ the highest scoring supporting memory is retrieved with:</p>
<p>$$
o_{1}=O_{1}(x, \mathbf{m})=\underset{i=1, \ldots, N}{\arg \max } s_{O}\left(x, \mathbf{m}_{i}\right)
$$</p>
<p>where $s_{O}$ is a function that scores the match between the pair of sentences $x$ and $\mathbf{m}_{i}$. For the case $k=2$ we then find a second supporting memory given the first found in the previous iteration:</p>
<p>$$
o_{2}=O_{2}(x, \mathbf{m})=\underset{i=1, \ldots, N}{\arg \max } s_{O}\left(\left[x, \mathbf{m}<em 1="1">{o</em>\right)
$$}}\right], \mathbf{m}_{i</p>
<p>where the candidate supporting memory $\mathbf{m}<em o__1="o_{1">{i}$ is now scored with respect to both the original input and the first supporting memory, where square brackets denote a list ${ }^{3}$. The final output $o$ is $\left[x, \mathbf{m}</em>}}, \mathbf{m<em 2="2">{o</em>\right]$, which is input to the module $R$.}</p>
<p>Finally, $R$ needs to produce a textual response $r$. The simplest response is to return $\mathbf{m}<em k="k">{o</em>$, i.e., to output the previously uttered sentence we retrieved. To perform true sentence generation, one can instead employ an RNN. In our experiments we also consider an easy to evaluate compromise approach where we limit textual responses to be a single word (out of all the words seen by the model) by ranking them:}</p>
<p>$$
r=\operatorname{argmax}<em R="R">{w \in W} s</em>}\left(\left[x, \mathbf{m<em 1="1">{o</em>}}, \mathbf{m<em 2="2">{o</em>\right], w\right)
$$}</p>
<p>where $W$ is the set of all words in the dictionary, and $s_{R}$ is a function that scores the match.
An example task is given in Figure 1. In order to answer the question $x=$ "Where is the milk now?", the $O$ module first scores all memories, i.e., all previously seen sentences, against $x$ to retrieve the most relevant fact, $\mathbf{m}<em 1="1">{o</em>}}=$ "Joe left the milk" in this case. Then, it would search the memory again to find the second relevant fact given $\left[x, \mathbf{m<em 1="1">{o</em>}}\right]$, that is $\mathbf{m<em 2="2">{o</em>}}=$ "Joe travelled to the office" (the last place Joe went before dropping the milk). Finally, the $R$ module using eq. (4) would score words given $\left[x, \mathbf{m<em 1="1">{o</em>}}, \mathbf{m<em 2="2">{o</em>\right]$ to output $r=$ "office".}</p>
<p>In our experiments, the scoring functions $s_{O}$ and $s_{R}$ have the same form, that of an embedding model:</p>
<p>$$
s(x, y)=\Phi_{x}(x)^{\top} U^{\top} U \Phi_{y}(y)
$$</p>
<p>where $U$ is a $n \times D$ matrix where $D$ is the number of features and $n$ is the embedding dimension. The role of $\Phi_{x}$ and $\Phi_{y}$ is to map the original text to the $D$-dimensional feature space. The simplest feature space to choose is a bag of words representation, we choose $D=3|W|$ for $s_{O}$, i.e., every word in the dictionary has three different representations: one for $\Phi_{y}($.$) and two for \Phi_{x}($.$) depending$ on whether the words of the input arguments are from the actual input $x$ or from the supporting memories so that they can be modeled differently. ${ }^{4}$ Similarly, we used $D=3|W|$ for $s_{R}$ as well. $s_{O}$ and $s_{R}$ use different weight matrices $U_{O}$ and $U_{R}$.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Figure 1: Example "story" statements, questions and answers generated by a simple simulation. Answering the question about the location of the milk requires comprehension of the actions "picked up" and "left". The questions also require comprehension of the time elements of the story, e.g., to answer "where was Joe before the office?".</p>
<p>Joe went to the kitchen. Fred went to the kitchen. Joe picked up the milk.
Joe travelled to the office. Joe left the milk. Joe went to the bathroom.
Where is the milk now? A: office
Where is Joe? A: bathroom
Where was Joe before the office? A: kitchen</p>
<p>Training We train in a fully supervised setting where we are given desired inputs and responses, and the supporting sentences are labeled as such in the training data (but not in the test data, where we are given only the inputs). That is, during training we know the best choice of both max functions in eq. (2) and (3) ${ }^{5}$. Training is then performed with a margin ranking loss and stochastic gradient descent (SGD). Specifically, for a given question $x$ with true response $r$ and supporting sentences $\mathbf{m}<em 1="1">{o</em>}}$ and $\mathbf{m<em 2="2">{o</em>$ :}}$ (when $k=2$ ), we minimize over model parameters $U_{O}$ and $U_{R</p>
<p>$$
\begin{gathered}
\sum_{\bar{f} \neq \mathbf{m}<em 1="1">{o</em>}}} \max \left(0, \gamma-s_{O}\left(x, \mathbf{m<em 1="1">{o</em>)\right)+ \
\sum_{\bar{f}^{\prime} \neq \mathbf{m}}}\right)+s_{O}(x, \bar{f<em 2="2">{o</em>}}} \max \left(0, \gamma-s_{O}\left(\left[x, \mathbf{m<em 1="1">{o</em>}}\right], \mathbf{m<em 2="2">{o</em>}}\right]\right)+s_{O}\left(\left[x, \mathbf{m<em 1="1">{o</em>\right])\right)+ \
\sum_{\bar{r} \neq r} \max \left(0, \gamma-s_{R}\left(\left[x, \mathbf{m}}}\right], \bar{f}^{\prime<em 1="1">{o</em>}}, \mathbf{m<em 2="2">{o</em>}}\right], r\right)+s_{R}\left(\left[x, \mathbf{m<em 1="1">{o</em>}}, \mathbf{m<em 2="2">{o</em>\right]\right))
\end{gathered}
$$}}\right], \bar{r</p>
<p>where $\bar{f}, \bar{f}^{\prime}$ and $\bar{r}$ are all other choices than the correct labels, and $\gamma$ is the margin. At every step of SGD we sample $\bar{f}, \bar{f}^{\prime}, \bar{r}$ rather than compute the whole sum for each training example, following e.g., Weston et al. (2011).</p>
<p>In the case of employing an RNN for the $R$ component of our MemNN (instead of using a single word response as above) we replace the last term with the standard log likelihood used in a language modeling task, where the RNN is fed the sequence $\left[x, o_{1}, o_{2}, r\right]$. At test time we output its prediction $r$ given $\left[x, o_{1}, o_{2}\right]$. In contrast the absolute simplest model, that of using $k=1$ and outputting the located memory $\mathbf{m}<em 1="1">{o</em>$ as response $r$, would only use the first term to train.}</p>
<p>In the following subsections we consider some extensions of our basic model.</p>
<h1>3.2 Word SEQUENCES as INPUT</h1>
<p>If input is at the word rather than sentence level, that is words arrive in a stream (as is often done, e.g., with RNNs) and not already segmented as statements and questions, we need to modify the approach we have so far described. We hence add a "segmentation" function, to be learned, which takes as input the last sequence of words that have so far not been segmented and looks for breakpoints. When the segmenter fires (indicates the current sequence is a segment) we write that sequence to memory, and can then proceed as before. The segmenter is modeled similarly to our other components, as an embedding model of the form:</p>
<p>$$
\operatorname{seg}(c)=W_{\text {seg }}^{\top} U_{S} \Phi_{\text {seg }}(c)
$$</p>
<p>where $W_{\text {seg }}$ is a vector (effectively the parameters of a linear classifier in embedding space), and $c$ is the sequence of input words represented as bag of words using a separate dictionary. If $\operatorname{seg}(c)&gt;\gamma$, where $\gamma$ is the margin, then this sequence is recognised as a segment. In this way, our MemNN has a learning component in its write operation. We consider this segmenter a first proof of concept: of course, one could design something much more sophisticated. Further details on the training mechanism are given in Appendix B.</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h1>3.3 Efficient Memory via Hashing</h1>
<p>If the set of stored memories is very large it is prohibitively expensive to score all of them as in equations (2) and (3). Instead we explore hashing tricks to speed up lookup: hash the input $I(x)$ into one or more buckets and then only score memories $\mathbf{m}<em i="i">{i}$ that are in the same buckets. We investigated two ways of doing hashing: (i) via hashing words; and (ii) via clustering word embeddings. For (i) we construct as many buckets as there are words in the dictionary, then for a given sentence we hash it into all the buckets corresponding to its words. The problem with (i) is that a memory $\mathbf{m}</em>$, thus giving $K$ buckets. We then hash a given sentence into all the buckets that its individual words fall into. As word vectors tend to be close to their synonyms, they cluster together and we thus also will score those similar memories as well. Exact word matches between input and memory will still be scored by definition. Choosing $K$ controls the speed-accuracy trade-off.}$ will only be considered if it shares at least one word with the input $I(x)$. Method (ii) tries to solve this by clustering instead. After training the embedding matrix $U_{O}$, we run $K$-means to cluster word vectors $\left(U_{O}\right)_{i</p>
<h3>3.4 Modeling Write Time</h3>
<p>We can extend our model to take into account when a memory slot was written to. This is not important when answering questions about fixed facts ("What is the capital of France?") but is important when answering questions about a story, see e.g., Figure 1. One obvious way to implement this is to add extra features to the representations $\Phi_{x}$ and $\Phi_{y}$ that encode the index $j$ of a given memory $\mathbf{m}<em O__t="O_{t">{j}$, assuming that $j$ follows write time (i.e., no memory slot rewriting). However, that requires dealing with absolute rather than relative time. We had more success empirically with the following procedure: instead of scoring input, candidate pairs with $s$ as above, learn a function on triples $s</em>\right)$ :}}\left(x, y, y^{\prime</p>
<p>$$
s_{O_{t}}\left(x, y, y^{\prime}\right)=\Phi_{x}(x)^{\top} U_{O_{t}}^{\top} U_{O_{t}}\left(\Phi_{y}(y)-\Phi_{y}\left(y^{\prime}\right)+\Phi_{t}\left(x, y, y^{\prime}\right)\right)
$$</p>
<p>$\Phi_{t}\left(x, y, y^{\prime}\right)$ uses three new features which take on the value 0 or 1 : whether $x$ is older than $y, x$ is older than $y^{\prime}$, and $y$ older than $y^{\prime}$. (That is, we extended the dimensionality of all the $\Phi$ embeddings by 3 , and set these three dimensions to zero when not used.) Now, if $s_{O_{t}}\left(x, y, y^{\prime}\right)&gt;0$ the model prefers $y$ over $y^{\prime}$, and if $s_{O_{t}}\left(x, y, y^{\prime}\right)&lt;0$ it prefers $y^{\prime}$. The argmax of eq. (2) and (3) are replaced by a loop over memories $i=1, \ldots, N$, keeping the winning memory ( $y$ or $y^{\prime}$ ) at each step, and always comparing the current winner to the next memory $\mathbf{m}_{i}$. This procedure is equivalent to the argmax before if the time features are removed. More details are given in Appendix C.</p>
<h3>3.5 Modeling Previously Unseen Words</h3>
<p>Even for humans who have read a lot of text, new words are continuously introduced. For example, the first time the word "Boromir" appears in Lord of The Rings (Tolkien, 1954). How should a machine learning model deal with this? Ideally it should work having seen only one example. A possible way would be to use a language model: given the neighboring words, predict what the word should be, and assume the new word is similar to that. Our proposed approach takes this idea, but incorporates it into our networks $s_{O}$ and $s_{R}$, rather than as a separate step.</p>
<p>Concretely, for each word we see, we store a bag of words it has co-occurred with, one bag for the left context, and one for the right. Any unknown word can be represented with such features. Hence, we increase our feature representation $D$ from $3|W|$ to $5|W|$ to model these contexts $(|W|$ features for each bag). Our model learns to deal with new words during training using a kind of "dropout" technique: $d \%$ of the time we pretend we have not seen a word before, and hence do not have a $n$-dimensional embedding for that word, and represent it with the context instead.</p>
<h3>3.6 EXACT MATCHES and UnSEEN WORDS</h3>
<p>Embedding models cannot efficiently use exact word matches due to the low dimensionality $n$. One solution is to score a pair $x, y$ with</p>
<p>$$
\Phi_{x}(x)^{\top} U^{\top} U \Phi_{y}(y)+\lambda \Phi_{x}(x)^{\top} \Phi_{y}(y)
$$</p>
<p>instead. That is, add the "bag of words" matching score to the learned embedding score (with a mixing parameter $\lambda$ ). Another, related way, that we propose is to stay in the $n$-dimensional embedding space, but to extend the feature representation $D$ with matching features, e.g., one per</p>
<p>word. A matching feature indicates if a word occurs in both $x$ and $y$. That is, we score with $\Phi_{x}(x)^{\top} U^{\top} U \Phi_{y}(y, x)$ where $\Phi_{y}$ is actually built conditionally on $x$ : if some of the words in $y$ match the words in $x$ we set those matching features to 1 . Unseen words can be modeled similarly by using matching features on their context words. This then gives a feature space of $D=8|W|$.</p>
<h1>4 Related Work</h1>
<p>Classical QA methods use a set of documents as a kind of memory, and information retrieval methods to find answers, see e.g., (Kolomiyets \&amp; Moens, 2011) and references therein. More recent methods try instead to create a graph of facts - a knowledge base (KB) - as their memory, and map questions to logical queries (Berant et al., 2013; 2014). Neural network and embedding approaches have also been recently explored (Bordes et al., 2014a; Iyyer et al., 2014; Yih et al., 2014). Compared to recent knowledge base approaches, memory networks differ in that they do not apply a two-stage strategy: (i) apply information extraction principles first to build the KB; followed by (ii) inference over the KB. Instead, extraction of useful information to answer a question is performed on-the-fly over the memory which can be stored as raw text, as well as other choices such as embedding vectors. This is potentially less brittle as the first stage of building the KB may have already thrown away the relevant part of the original data.</p>
<p>Classical neural network memory models such as associative memory networks aim to provide content-addressable memory, i.e., given a key vector to output a value vector, see e.g., Haykin (1994) and references therein. Typically this type of memory is distributed across the whole network of weights of the model rather than being compartmentalized into memory locations. Memory-based learning such as nearest neighbor, on the other hand, does seek to store all (typically labeled) examples in compartments in memory, but only uses them for finding closest labels. Memory networks combine compartmentalized memory with neural network modules that can learn how to (potentially successively) read and write to that memory, e.g., to perform reasoning they can iteratively read salient facts from the memory.</p>
<p>However, there are some notable models that have attempted to include memory read and write operations from the 90s. In particular (Das et al., 1992) designed differentiable push and pop actions called a neural network pushdown automaton. The work of Schmidhuber (1992) incorporated the concept of two neural networks where one has very fast changing weights which can potentially be used as memory. Schmidhuber (1993) proposed to allow a network to modify its own weights "selfreferentially" which can also be seen as a kind of memory addressing. Finally two other relevant works are the DISCERN model of script processing and memory (Miikkulainen, 1990) and the NARX recurrent networks for modeling long term dependencies (Lin et al., 1996).</p>
<p>Our work was submitted to arxiv just before the Neural Turing Machine work of Graves et al. (2014), which is one of the most relevant related methods. Their method also proposes to perform (sequence) prediction using a "large, addressable memory" which can be read and written to. In their experiments, the memory size was limited to 128 locations, whereas we consider much larger storage (up to 14 M sentences). The experimental setups are notably quite different also: whereas we focus on language and reasoning tasks, their paper focuses on problems of sorting, copying and recall. On the one hand their problems require considerably more complex models than the memory network described in Section 3. On the other hand, their problems have known algorithmic solutions, whereas (non-toy) language problems do not.</p>
<p>There are other recent related works. RNNSearch (Bahdanau et al., 2014) is a method of machine translation that uses a learned alignment mechanism over the input sentence representation while predicting an output in order to overcome poor performance on long sentences. The work of (Graves, 2013) performs handwriting recognition by dynamically determining "an alignment between the text and the pen locations" so that "it learns to decide which character to write next". One can view these as particular variants of memory networks where in that case the memory only extends back a single sentence or character sequence.</p>
<p>Table 1: Results on the large-scale QA task of (Fader et al., 2013).</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Method</th>
<th style="text-align: center;">F1</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">(Fader et al., 2013)</td>
<td style="text-align: center;">0.54</td>
</tr>
<tr>
<td style="text-align: left;">(Bordes et al., 2014b)</td>
<td style="text-align: center;">0.73</td>
</tr>
<tr>
<td style="text-align: left;">MemNN (embedding only)</td>
<td style="text-align: center;">0.72</td>
</tr>
<tr>
<td style="text-align: left;">MemNN (with BoW features)</td>
<td style="text-align: center;">0.82</td>
</tr>
</tbody>
</table>
<p>Table 2: Memory hashing results on the large-scale QA task of (Fader et al., 2013).</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Method</th>
<th style="text-align: center;">Embedding F1</th>
<th style="text-align: center;">Embedding + BoW F1</th>
<th style="text-align: center;">Candidates (speedup)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">MemNN (no hashing)</td>
<td style="text-align: center;">0.72</td>
<td style="text-align: center;">0.82</td>
<td style="text-align: center;">$14 \mathrm{M}(0 \mathrm{x})$</td>
</tr>
<tr>
<td style="text-align: left;">MemNN (word hash)</td>
<td style="text-align: center;">0.63</td>
<td style="text-align: center;">0.68</td>
<td style="text-align: center;">$13 \mathrm{k}(1000 \mathrm{x})$</td>
</tr>
<tr>
<td style="text-align: left;">MemNN (cluster hash)</td>
<td style="text-align: center;">0.71</td>
<td style="text-align: center;">0.80</td>
<td style="text-align: center;">$177 \mathrm{k}(80 \mathrm{x})$</td>
</tr>
</tbody>
</table>
<h1>5 EXPERIMENTS</h1>
<h3>5.1 LARGE-SCALE QA</h3>
<p>We perform experiments on the QA dataset introduced in Fader et al. (2013). It consists of 14M statements, stored as (subject, relation, object) triples, which are stored as memories in the MemNN model. The triples are REVERB extractions mined from the ClueWeb09 corpus and cover diverse topics such as (milne, authored, winnie-the-pooh) and (sheep, be-afraid-of, wolf). Following Fader et al. (2013) and Bordes et al. (2014b), training combines pseudo-labeled QA pairs made of a question and an associated triple, and 35M pairs of paraphrased questions from WikiAnswers like "Who wrote the Winnie the Pooh books?" and "Who is poohs creator?".</p>
<p>We performed experiments in the framework of re-ranking the top returned candidate answers by several systems measuring F1 score over the test set, following Bordes et al. (2014b). These answers have been annotated as right or wrong by humans, whereas other answers are ignored at test time as we do not know their label. We used a MemNN model of Section 3 with a $k=1$ supporting memory, which ends up being similar to the approach of Bordes et al. (2014b). ${ }^{6}$ We also tried adding the bag of words features of Section 3.6 as well. Time and unseen word modeling were not used. Results are given in Table 1. The results show that MemNNs are a viable approach for large scale QA in terms of performance. However, lookup is linear in the size of the memory, which with 14M facts is slow. We therefore implemented the memory hashing techniques of Section 3.3 using both hashing of words and clustered embeddings. For the latter we tried $K=1000$ clusters. The results given in Table 2 show that one can get significant speedups ( $\sim 80 \mathrm{x}$ ) while maintaining similar performance using the cluster-based hash. The string hash on the other hand loses performance (whilst being a lot faster) because answers which share no words are now no longer matched.</p>
<h3>5.2 SIMULATED WORLD QA</h3>
<p>Similar to the approach of Bordes et al. (2010) we also built a simple simulation of 4 characters, 3 objects and 5 rooms - with characters moving around, picking up and dropping objects. The actions are transcribed into text using a simple automated grammar, and labeled questions are generated in a similar way. This gives a QA task on simple "stories" such as in Figure 1. The overall difficulty of the task is that multiple statements have to be used to do inference when asking where an object is, e.g. to answer where is the milk in Figure 1 one has to understand the meaning of the actions "picked up" and "left" and the influence of their relative order. We generated 7 k statements and 3 k questions from the simulator for training ${ }^{7}$, and an identical number for testing and compare MemNNs to RNNs and LSTMs (long short term memory RNNs (Hochreiter \&amp; Schmidhuber, 1997)) on this task. To</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Table 3: Test accuracy on the simulation QA task.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Difficulty 1</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Difficulty 5</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Method</td>
<td style="text-align: center;">actor w/o before</td>
<td style="text-align: center;">actor</td>
<td style="text-align: center;">actor+object</td>
<td style="text-align: center;">actor</td>
<td style="text-align: center;">actor+object</td>
</tr>
<tr>
<td style="text-align: left;">RNN</td>
<td style="text-align: center;">$100 \%$</td>
<td style="text-align: center;">$60.9 \%$</td>
<td style="text-align: center;">$27.9 \%$</td>
<td style="text-align: center;">$23.8 \%$</td>
<td style="text-align: center;">$17.8 \%$</td>
</tr>
<tr>
<td style="text-align: left;">LSTM</td>
<td style="text-align: center;">$100 \%$</td>
<td style="text-align: center;">$64.8 \%$</td>
<td style="text-align: center;">$49.1 \%$</td>
<td style="text-align: center;">$35.2 \%$</td>
<td style="text-align: center;">$29.0 \%$</td>
</tr>
<tr>
<td style="text-align: left;">MemNN $k=1$</td>
<td style="text-align: center;">$97.8 \%$</td>
<td style="text-align: center;">$31.0 \%$</td>
<td style="text-align: center;">$24.0 \%$</td>
<td style="text-align: center;">$21.9 \%$</td>
<td style="text-align: center;">$18.5 \%$</td>
</tr>
<tr>
<td style="text-align: left;">MemNN $k=1$ (+time)</td>
<td style="text-align: center;">$99.9 \%$</td>
<td style="text-align: center;">$60.2 \%$</td>
<td style="text-align: center;">$42.5 \%$</td>
<td style="text-align: center;">$60.8 \%$</td>
<td style="text-align: center;">$44.4 \%$</td>
</tr>
<tr>
<td style="text-align: left;">MemNN $k=2$ (+time)</td>
<td style="text-align: center;">$100 \%$</td>
<td style="text-align: center;">$100 \%$</td>
<td style="text-align: center;">$100 \%$</td>
<td style="text-align: center;">$100 \%$</td>
<td style="text-align: center;">$99.9 \%$</td>
</tr>
</tbody>
</table>
<p>test with sequences of words as input (Section 3.2) the statements are joined together again with a simple grammar ${ }^{8}$, to produce sentences that may contain multiple statements, see e.g., Figure 2.</p>
<p>We control the complexity of the task by setting a limit on the number of time steps in the past the entity we ask the question about was last mentioned. We try two experiments: using a limit of 1 , and of 5 , i.e., if the limit is 5 then we pick a random sentence between 1-5 time steps in the past. If this chosen sentence only mentions an actor, e.g., "Bill is in the kitchen" then we generate the question "where is Bill?" or "where was Bill before the kitchen?". If the sentence mentions an object, e.g., "Bill dropped the football" then we ask the question "where is the football?". For the answers we consider two options: (i) single word answers; and (ii) a simple grammar for generating true answers in sentence form, e.g., "kitchen" for (i) and "He is in the kitchen I believe" (and other variants) for (ii). More details on the dataset generation are given in Appendix A. Note that in the object case the supporting statements necessary to deduce the answer may not lie in the last 5 sentences, e.g., in this example the answer depends on other sentences to find out where Bill actually was when he dropped the football. In fact, in the dataset we generated necessary supporting statements can be up to 65 sentences before (but are usually closer). For that reason, we also conducted two further types of experiments: where we only ask questions about actors (easier) and about actors and objects (harder). We also consider the actor-based questions without the "before" questions for the simplest possible task (i.e. "where is Bill?" but not "where was Bill before the kitchen?" questions).</p>
<p>Figure 2: Sample test set predictions (in red) for the simulation in the setting of word-based input and where answers are sentences and an LSTM is used as the $R$ component of the MemNN.</p>
<p>Joe went to the garden then Fred picked up the milk; Joe moved to the bathroom and Fred dropped the milk, and then Dan moved to the living_room.
Where is Dan? A: living room I believe
Where is Joe? A: the bathroom
Fred moved to the bedroom and Joe went to the kitchen then Joe took the milk there and Dan journeyed to the bedroom; Joe discarded the milk.
Where is the milk now ? A: the milk is in the kitchen
Where is Dan now? A: I think he is in the bedroom
Joe took the milk there, after that Mike travelled to the office, then Joe went to the living_room, next Dan went back to the kitchen and Joe travelled to the office.
Where is Joe now? A: I think Joe is in the office</p>
<p>For the baseline RNN and LSTM systems we perform language modeling with backpropagation through time (Mikolov et al., 2010), but where we backprop only on answer words ${ }^{9}$. We optimized the hyperparameters: size of the hidden layer, bptt steps, and learning rate for each dataset. For MemNNs we fixed the embedding dimension to 100 , learning rate to 0.01 and margin $\gamma$ to 0.1 and 10 epochs of training in all experiments.</p>
<p>Results The results for the single word answer setting (i) are given in Table 3. For the actor-only tasks, RNN and LSTMs solve the simpler difficulty level 1 task without before questions ("w/o</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Figure 3: An example story with questions correctly answered by a MemNN. The MemNN was trained on the simulation described in Section 5.2 and had never seen many of these words before, e.g., Bilbo, Frodo and Gollum.</p>
<p>Bilbo travelled to the cave. Gollum dropped the ring there. Bilbo took the ring.
Bilbo went back to the Shire. Bilbo left the ring there. Frodo got the ring.
Frodo journeyed to Mount-Doom. Frodo dropped the ring there. Sauron died.
Frodo went back to the Shire. Bilbo travelled to the Grey-havens. The End.
Where is the ring? A: Mount-Doom
Where is Bilbo now? A: Grey-havens
Where is Frodo now? A: Shire</p>
<p>Figure 4: An example dialogue with a MemNN system trained on both the simulation data and the large-scale QA data. The system is able to (attempt to) answer questions about general world knowledge and about specific story-based statements in the dialogue.</p>
<div class="codehilite"><pre><span></span><code><span class="nx">Fred</span><span class="w"> </span><span class="nx">went</span><span class="w"> </span><span class="nx">to</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">kitchen</span><span class="p">.</span><span class="w"> </span><span class="nx">Fred</span><span class="w"> </span><span class="nx">picked</span><span class="w"> </span><span class="nx">up</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">milk</span><span class="p">.</span><span class="w"> </span><span class="nx">Fred</span><span class="w"> </span><span class="nx">travelled</span><span class="w"> </span><span class="nx">to</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">office</span><span class="p">.</span>
<span class="nx">Where</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">milk</span><span class="p">?</span><span class="w"> </span><span class="nx">A</span><span class="p">:</span><span class="w"> </span><span class="nx">office</span>
<span class="nx">Where</span><span class="w"> </span><span class="nx">does</span><span class="w"> </span><span class="nx">milk</span><span class="w"> </span><span class="nx">come</span><span class="w"> </span><span class="nx">from</span><span class="p">?</span><span class="w"> </span><span class="nx">A</span><span class="p">:</span><span class="w"> </span><span class="nx">milk</span><span class="w"> </span><span class="nx">come</span><span class="w"> </span><span class="nx">from</span><span class="w"> </span><span class="nx">cow</span>
<span class="nx">What</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="nx">a</span><span class="w"> </span><span class="nx">cow</span><span class="w"> </span><span class="nx">a</span><span class="w"> </span><span class="k">type</span><span class="w"> </span><span class="nx">of</span><span class="p">?</span><span class="w"> </span><span class="nx">A</span><span class="p">:</span><span class="w"> </span><span class="nx">cow</span><span class="w"> </span><span class="nx">be</span><span class="w"> </span><span class="nx">female</span><span class="w"> </span><span class="nx">of</span><span class="w"> </span><span class="nx">cattle</span>
<span class="nx">Where</span><span class="w"> </span><span class="nx">are</span><span class="w"> </span><span class="nx">cattle</span><span class="w"> </span><span class="nx">found</span><span class="p">?</span><span class="w"> </span><span class="nx">A</span><span class="p">:</span><span class="w"> </span><span class="nx">cattle</span><span class="w"> </span><span class="nx">farm</span><span class="w"> </span><span class="nx">become</span><span class="w"> </span><span class="nx">widespread</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="nx">brazil</span>
<span class="nx">What</span><span class="w"> </span><span class="nx">does</span><span class="w"> </span><span class="nx">milk</span><span class="w"> </span><span class="nx">taste</span><span class="w"> </span><span class="k">like</span><span class="p">?</span><span class="w"> </span><span class="nx">A</span><span class="p">:</span><span class="w"> </span><span class="nx">milk</span><span class="w"> </span><span class="nx">taste</span><span class="w"> </span><span class="k">like</span><span class="w"> </span><span class="nx">milk</span>
<span class="nx">What</span><span class="w"> </span><span class="nx">does</span><span class="w"> </span><span class="nx">milk</span><span class="w"> </span><span class="nx">go</span><span class="w"> </span><span class="nx">well</span><span class="w"> </span><span class="nx">with</span><span class="p">?</span><span class="w"> </span><span class="nx">A</span><span class="p">:</span><span class="w"> </span><span class="nx">milk</span><span class="w"> </span><span class="nx">go</span><span class="w"> </span><span class="nx">with</span><span class="w"> </span><span class="nx">coffee</span>
<span class="nx">Where</span><span class="w"> </span><span class="nx">was</span><span class="w"> </span><span class="nx">Fred</span><span class="w"> </span><span class="nx">before</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">office</span><span class="p">?</span><span class="w"> </span><span class="nx">A</span><span class="p">:</span><span class="w"> </span><span class="nx">kitchen</span>
</code></pre></div>

<p>before"), but perform worse with before questions, and even worse on the difficulty 5 tasks. This demonstrates that the poor performance of the RNN is due to its failure to encode long(er)-term memory. This would likely deteriorate even further with higher difficulty levels (distances). LSTMs are however better than RNNs, as expected, as they are designed with a more sophisticated memory model, but still have trouble remembering sentences too far in the past. MemNNs do not have this memory limitation and its mistakes are instead due to incorrect usage of its memory, when the wrong statement is picked by $s_{O}$. Time features are necessary for good performance on before questions or difficulty $&gt;1$ (i.e., when the answer is not in the last statement), otherwise $s_{O}$ can pick a statement about a person's whereabouts but they have since moved. Finally, results on the harder actor+object task indicate that MemNN also successfully perform 2-stage inference using $k=2$, whereas MemNNs without such inference (with $k=1$ ) and RNNs and LSTMs fail.
We also tested MemNNs in the multi-word answer setting (ii) with similar results, whereby MemNNs outperform RNNs and LSTMs, which are detailed in Appendix F. Example test prediction output demonstrating the model in that setting is given in Figure 2.</p>
<h1>5.2.1 QA with Previously Unseen Words</h1>
<p>We then tested the ability of MemNNs to deal with previously unseen words at test time using the unseen word modeling approach of Sections 3.5 and 3.6. We trained the MemNN on the same simulated dataset as before and test on the story given in Figure 3. This story is generated using similar structures as in the simulation data, except that the nouns are unknowns to the system at training time. Despite never seeing any of the Lord of The Rings specific words before (e.g., Bilbo, Frodo, Sauron, Gollum, Shire and Mount-Doom), MemNNs are able to correctly answer the questions.
MemNNs can discover simple linguistic patterns based on verbal forms such as (X, dropped, Y), (X, took, Y) or (X, journeyed to, Y) and can successfully generalize the meaning of their instantiations using unknown words to perform 2-stage inference. Without the unseen word modeling described in Section 3.5, they completely fail on this task.</p>
<h1>5.3 COMBINING SIMULATED DATA AND LARGE-SCALE QA</h1>
<p>Combining simulated world learning with real-world data might be one way to show the power and generality of the models we design. We implemented a naive setup towards that goal: we took the two models from Sections 5.1 and 5.2, trained on large-scale QA and simulated data respectively, and built an ensemble of the two. We present the input to both systems and then for each question simply output the response of the two choices with the highest score. This allows us to perform simple dialogues with our combined MemNN system. The system is then capable of answering both general knowledge questions and specific statements relating to the previous dialogue. An example dialogue trace is given in Fig. 4. Some answers appear fine, whereas others are nonsensical. Future work should combine these models more effectively, for example by multitasking directly the tasks with a single model.</p>
<h2>6 CONCLUSIONS AND FUTURE WORK</h2>
<p>In this paper we introduced a powerful class of models, memory networks, and showed one instantiation for QA. Future work should develop MemNNs for text further, evaluating them on harder QA and open-domain machine comprehension tasks (Richardson et al., 2013). For example, large scale QA tasks that require multi-hop inference such as WebQuestions should also be tried Berant et al. (2013). More complex simulation data could also be constructed in order to bridge that gap, e.g., requiring coreference, involving more verbs and nouns, sentences with more structure and requiring more temporal and causal understanding. More sophisticated architectures should also be explored in order to deal with these tasks, e.g., using more sophisticated memory management via $G$ and more sophisticated sentence representations. Weakly supervised settings are also very important, and should be explored, as many datasets only have supervision in the form of question answer pairs, and not supporting facts as well as we used here. Finally, we believe this class of models is much richer than the one specific variant we detail here, and that we have currently only explored one specific variant of memory networks. Memory networks should be applied to other text tasks, and other domains, such as vision, as well.</p>
<h2>ACKNOWLEDGMENTS</h2>
<p>We thank Tomas Mikolov for useful discussions.</p>
<h2>REFERENCES</h2>
<p>Bahdanau, Dzmitry, Cho, Kyunghyun, and Bengio, Yoshua. Neural machine translation by jointly learning to align and translate. arXiv preprint arXiv:1409.0473, 2014.</p>
<p>Berant, Jonathan, Chou, Andrew, Frostig, Roy, and Liang, Percy. Semantic parsing on freebase from question-answer pairs. In EMNLP, pp. 1533-1544, 2013.</p>
<p>Berant, Jonathan, Srikumar, Vivek, Chen, Pei-Chun, Huang, Brad, Manning, Christopher D, Vander Linden, Abby, Harding, Brittany, and Clark, Peter. Modeling biological processes for reading comprehension. In Proc. EMNLP, 2014.</p>
<p>Bordes, Antoine, Usunier, Nicolas, Collobert, Ronan, and Weston, Jason. Towards understanding situated natural language. In AISTATS, 2010.</p>
<p>Bordes, Antoine, Chopra, Sumit, and Weston, Jason. Question answering with subgraph embeddings. In Proc. EMNLP, 2014a.</p>
<p>Bordes, Antoine, Weston, Jason, and Usunier, Nicolas. Open question answering with weakly supervised embedding models. ECML-PKDD, 2014b.</p>
<p>Das, Sreerupa, Giles, C Lee, and Sun, Guo-Zheng. Learning context-free grammars: Capabilities and limitations of a recurrent neural network with an external stack memory. In Proceedings of The Fourteenth Annual Conference of Cognitive Science Society. Indiana University, 1992.</p>
<p>Fader, Anthony, Zettlemoyer, Luke, and Etzioni, Oren. Paraphrase-driven learning for open question answering. In ACL, pp. 1608-1618, 2013.</p>
<p>Graves, Alex. Generating sequences with recurrent neural networks. arXiv preprint arXiv:1308.0850, 2013.</p>
<p>Graves, Alex, Wayne, Greg, and Danihelka, Ivo. Neural turing machines. arXiv preprint arXiv:1410.5401, 2014.</p>
<p>Haykin, Simon. Neural networks: A comprehensive foundation. 1994.
Hochreiter, Sepp and Schmidhuber, Jürgen. Long short-term memory. Neural computation, 9(8): $1735-1780,1997$.</p>
<p>Iyyer, Mohit, Boyd-Graber, Jordan, Claudino, Leonardo, Socher, Richard, and III, Hal Daumé. A neural network for factoid question answering over paragraphs. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 633-644, 2014.</p>
<p>Kolomiyets, Oleksandr and Moens, Marie-Francine. A survey on question answering technology from an information retrieval perspective. Information Sciences, 181(24):5412-5434, 2011.</p>
<p>Lin, Tsungnam, Horne, Bil G, Tiňo, Peter, and Giles, C Lee. Learning long-term dependencies in narx recurrent neural networks. Neural Networks, IEEE Transactions on, 7(6):1329-1338, 1996.</p>
<p>Miikkulainen, Risto. ${$ DISCERN $}:{\mathrm{A}}$ distributed artificial neural network model of script processing and memory. 1990.</p>
<p>Mikolov, Tomas, Karafiát, Martin, Burget, Lukas, Cernockỳ, Jan, and Khudanpur, Sanjeev. Recurrent neural network based language model. In Interspeech, pp. 1045-1048, 2010.</p>
<p>Richardson, Matthew, Burges, Christopher JC, and Renshaw, Erin. Mctest: A challenge dataset for the open-domain machine comprehension of text. In EMNLP, pp. 193-203, 2013.</p>
<p>Schmidhuber, Jürgen. Learning to control fast-weight memories: An alternative to dynamic recurrent networks. Neural Computation, 4(1):131-139, 1992.</p>
<p>Schmidhuber, Jürgen. A self-referentialweight matrix. In ICANN93, pp. 446-450. Springer, 1993.
Tolkien, John Ronald Reuel. The Fellowship of the Ring. George Allen \&amp; Unwin, 1954.
Weston, Jason, Bengio, Samy, and Usunier, Nicolas. Wsabie: Scaling up to large vocabulary image annotation. In Proceedings of the Twenty-Second international joint conference on Artificial Intelligence-Volume Volume Three, pp. 2764-2770. AAAI Press, 2011.</p>
<p>Yih, Wen-Tau, He, Xiaodong, and Meek, Christopher. Semantic parsing for single-relation question answering. In Proceedings of ACL. Association for Computational Linguistics, June 2014. URL http://research.microsoft.com/apps/pubs/default.aspx?id=214353.</p>
<p>Zaremba, Wojciech and Sutskever, Ilya. Learning to execute. arXiv preprint arXiv:1410.4615, 2014.</p>
<h1>A Simulation Data Generation</h1>
<p>Aim We have built a simple simulation which behaves much like a classic text adventure game. The idea is that generating text within this simulation allows us to ground the language used.</p>
<p>Some comments about our intent:</p>
<ul>
<li>Firstly, while this currently only encompasses a very small part of the kind of language and understanding we want a model to learn to move towards full language understanding, we believe it is a prerequisite that models should perform well on this kind of task for them to work on real-world environments.</li>
<li>Secondly, our aim is to make this simulation more complex and to release improved versions over time. Hopefully it can then scale up to evaluate more and more useful properties.</li>
</ul>
<p>Currently, tasks within the simulation are restricted to question answering tasks about the location of people and objects. However, we envisage other tasks should be possible, including asking the learner to perform actions within the simulation ("Please pick up the milk", "Please find John and give him the milk") and asking the learner to describe actions ("What did John just do?").</p>
<p>Actions The underlying actions in the simulation consist of the following:
go $&lt;$ location $&gt;$, get $&lt;$ object $&gt;$, get $&lt;$ object $1&gt;$ from $&lt;$ object $2&gt;$,
put $&lt;$ object $1&gt;$ in/on $&lt;$ object $2&gt;$, give $&lt;$ object $&gt;$ to $&lt;$ actor $&gt;$,
drop $&lt;$ object $&gt;$, look, inventory, examine $&lt;$ object $&gt;$.
There are a set of constraints on those actions. For example an actor cannot get something that they or someone else already has, they cannot go to a place they are already at, cannot drop something they do not already have, and so on.</p>
<p>Executing Actions and Asking Questions Using the underlying actions and their constraints, there is then a (hand-built) model that defines how actors act. Currently this is very simple: they try to make a random valid action, at the moment restricted to go or go, get and drop depending on the which of two types of experiments we are running: (i) actor; or (ii) actor + object.</p>
<p>If we write these actions down in text form this gives us a very simple "story" which is executable by the simulation, e.g., joe go kitchen; fred go kitchen; joe get milk; joe go office; joe drop milk; joe go bathroom. This example corresponds to the story given in Figure 1. The system can then ask questions about the state of the simulation e.g., where milk?, where joe?, where joe before office? It is easy to calculate the true answers for these questions as we have access to the underlying world. What remains is to convert both the statements and the questions to look more like natural language.</p>
<p>Simple Grammar For Generating Language In order to produce more natural looking text with lexical variety we built a simple automated grammar. Each verb is assigned a set of synonyms, e.g., the simulation command get is replaced with either picked up, got, grabbed or took, and drop is replace with either dropped, left, discarded or put down. Similarly, each object and actor can have a set of replacement synonyms as well, although currently there is no ambiguity there in our experiments, we simply add articles or not. We do add lexical variation to questions, e.g., "Where is John ?" or "Where is John now ?".</p>
<p>Joining Statements Finally, for the word sequence training setting, we join the statements above into compound sentences. To do this we simply take the set of statements and then join them randomly with one of the following: ".", "and", "then", ", then", ",", ", later", ", after that", ", and then", or ", next". Example output can be seen in Figure 2.</p>
<p>Issues There are a great many aspects of language not yet modeled. For example, currently coreference is not modeled (e.g., "He picked up the milk") and similarly there are no compound noun phrases ("John and Fred went to the kitchen"). Some of these seem easy to add to the simulation. The hope is that adding these complexities will help evaluate models in a controlled way, within the simulated environment, which is hard to do with real data. Of course, this is not a substitute for real data which our models should be applied to as well, but does serve as a useful testbed.</p>
<h1>B Word Sequence Training</h1>
<p>For segmenting an input word stream as generated in Appendix A we use a segmenter of the form:</p>
<p>$$
\operatorname{seg}(c)=W_{\text {seg }}^{\top} U_{S} \Phi_{\text {seg }}(c)
$$</p>
<p>where $W_{\text {seg }}$ is a vector (effectively the parameters of a linear classifier in embedding space). As we are already in the fully supervised setting, where for each question in the training set we are given the answer and the supporting facts from the input stream, we can also use that supervision for the segmenter as well. That is, for any known supporting fact, such as "Bill is in the Kitchen" for the question "Where is Bill?" we wish the segmenter to fire for such a statement, but not for unfinished statements such as "Bill is in the". We can thus write our training criterion for segmentation as the minimization of:</p>
<p>$$
\sum_{f \in \mathcal{F}} \max (0, \gamma-\operatorname{seg}(f))+\sum_{\bar{f} \in \mathcal{F}} \max (0, \gamma+\operatorname{seg}(\bar{f}))
$$</p>
<p>where $\mathcal{F}$ are all known supporting segments in the labeled training set, and $\overline{\mathcal{F}}$ are all other segments in the training set.</p>
<h2>C Write Time Feature Training</h2>
<p>The training procedure to take into account modeling write time is slightly different to that described in Section 3.1. Write time features are important so that the MemNN knows when each memory was written, and hence knows the ordering of statements that comprise a story or dialogue. Note that this is different to time information described in the text of a statement, such as the tense of a statement, or statements containing time expressions, e.g., "He went to the office yesterday". For such cases, write time features are not directly necessary, and they could (potentially) be modeled directly from the text.</p>
<p>As was described in Section 3.4 we add three write time features to the model and score triples using:</p>
<p>$$
s_{O_{t}}\left(x, y, y^{\prime}\right)=\Phi_{x}(x)^{\top} U_{O_{t}}{ }^{\top} U_{O_{t}}\left(\Phi_{y}(y)-\Phi_{y}\left(y^{\prime}\right)+\Phi_{t}\left(x, y, y^{\prime}\right)\right)
$$</p>
<p>If $s_{O}\left(x, y, y^{\prime}\right)&gt;0$ the model prefers $y$ over $y^{\prime}$, and if $s_{O}\left(x, y, y^{\prime}\right)&lt;0$ it prefers $y^{\prime}$. The argmax of eq. (2) and (3) are replaced by a loop over memories $i=1, \ldots, N$, keeping the winning memory ( $y$ or $y^{\prime}$ ) at each step, and always comparing the current winner to the next memory $\mathbf{m}<em 1="1">{i}$. That is, at inference time, for a $k=2$ model the $\arg \max$ functions of eq. (2) and (3) are replaced with $o</em>}=O_{t}(x, \mathbf{m})$ and $o_{2}=O_{t}\left(\left[x, \mathbf{m<em 1="1">{o</em>$ is defined in Algorithm 1 below.}}\right], \mathbf{m}\right)$ where $O_{t</p>
<div class="codehilite"><pre><span></span><code>Algorithm \(1 O_{t}\) replacement to \(\arg \max\) when using write time features
    function \(O_{t}(q, \mathbf{m})\)
        \(t \leftarrow 1\)
        for \(i=2, \ldots, N\) do
            if \(s_{O_{t}}\left(q, \mathbf{m}_{i}, \mathbf{m}_{t}\right)&gt;0\) then
                \(t \leftarrow i\)
            end if
        end for
        return \(t\)
    end function
</code></pre></div>

<p>$\Phi_{t}\left(x, y, y^{\prime}\right)$ uses three new features which take on the value 0 or 1 : whether $x$ is older than $y$, $x$ is older than $y^{\prime}$, and $y$ older than $y^{\prime}$. When finding the second supporting memory (computing $O_{t}\left(\left[x, \mathbf{m}<em 1="1">{o</em>}}\right], \mathbf{m}\right)$ ) we encode whether $\mathbf{m<em 1="1">{o</em>}}$ is older than $y, \mathbf{m<em 1="1">{o</em>$ are always older.}}$ is older than $y^{\prime}$, and $y$ older than $y^{\prime}$ to capture the relative age of the first supporting memory w.r.t. the second one in the first two features. Note that when finding the first supporting memory (i.e., for $O_{t}(x, \mathbf{m})$ ) the first two features are useless as $x$ is the last thing in the memory and hence $y$ and $y^{\prime</p>
<p>To train our model with write time features we need to replace the hinge loss in eqs. (6)-(7) with a loss that matches Algorithm 1. To do this, we instead minimize:</p>
<p>$$
\begin{gathered}
\sum_{\check{f} \neq \mathbf{m}<em 1="1">{o</em>}}} \max \left(0, \gamma-s_{O_{t}}\left(x, \mathbf{m<em 1="1">{o</em>}}, \check{f}\right)\right)+\sum_{f \neq \mathbf{m<em 1="1">{o</em>}}} \max \left(0, \gamma+s_{O_{t}}\left(x, \check{f}, \mathbf{m<em 1="1">{o</em>\right)\right)+ \
\sum_{\check{f}^{\prime} \neq \mathbf{m}}<em 2="2">{o</em>}}} \max \left(0, \gamma-s_{O_{t}}\left(\left[x, \mathbf{m<em 1="1">{o</em>}}\right], \mathbf{m<em 2="2">{o</em>}}, \check{f}^{\prime}\right)\right)+\sum_{\check{f}^{\prime} \neq \mathbf{m<em 2="2">{o</em>}}} \max \left(0, \gamma+s_{O_{t}}\left(\left[x, \mathbf{m<em 1="1">{o</em>}}\right], \check{f}^{\prime}, \mathbf{m<em 2="2">{o</em>\right)+\right. \
\left.\sum_{\bar{r} \neq r} \max \left(0, \gamma-s_{R}\left(\left[x, \mathbf{m}}<em 1="1">{o</em>}}, \mathbf{m<em 2="2">{o</em>}}\right], r\right)+s_{R}\left(\left[x, \mathbf{m<em 1="1">{o</em>}}, \mathbf{m<em 2="2">{o</em>\right]\right)\right)
\end{gathered}
$$}}\right], \bar{r</p>
<p>The last term is the same as in eq. (8) and is for the final ranking of words to return a response, which remains unchanged (as usual, this can also be replaced by an RNN for a more sophisticated model). Terms 1-4 replace eqs. (6)-(7) by considering triples directly. For both $\mathbf{m}<em 1="1">{o</em>}}$ and $\mathbf{m<em 2="2">{o</em>$ rather than compute the whole sum for each training example.}}$ we need to have two terms considering them as the second or third argument to $S_{O_{t}}$ as they may appear on either side during inference (via Algorithm 1). As before, at every step of SGD we sample $\check{f}, \check{f}^{\prime}, \bar{r</p>
<h1>D Word-SEQUENCE LEARNING CURVE EXPERIMENTS</h1>
<p>We computed the test accuracy of MemNNs $k=2$ (+ time) for varying amounts of training data: 100, 500, 1000 and 3000 training questions. The results are given in Table 4. These results can be compared with RNNs and LSTMs on the full data ( 3000 examples) by comparing with Figure 3. For example, on the difficulty 5 actor and actor + object tasks MemNNs outperform LSTMs even using 30 times less training examples.</p>
<p>Table 4: Test accuracy of MemNNs $k=2$ (+time) on the word-sequence simulation QA task for differing numbers of training examples (number of questions).</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Difficulty 1</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Difficulty 5</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Num. training <br> questions</td>
<td style="text-align: center;">actor</td>
<td style="text-align: center;">actor <br> + object</td>
<td style="text-align: center;">actor</td>
<td style="text-align: center;">actor <br> + object</td>
</tr>
<tr>
<td style="text-align: left;">100</td>
<td style="text-align: center;">$73.8 \%$</td>
<td style="text-align: center;">$64.9 \%$</td>
<td style="text-align: center;">$74.4 \%$</td>
<td style="text-align: center;">$49.8 \%$</td>
</tr>
<tr>
<td style="text-align: left;">500</td>
<td style="text-align: center;">$99.9 \%$</td>
<td style="text-align: center;">$99.2 \%$</td>
<td style="text-align: center;">$99.8 \%$</td>
<td style="text-align: center;">$95.1 \%$</td>
</tr>
<tr>
<td style="text-align: left;">1000</td>
<td style="text-align: center;">$99.9 \%$</td>
<td style="text-align: center;">$100 \%$</td>
<td style="text-align: center;">$100 \%$</td>
<td style="text-align: center;">$98.4 \%$</td>
</tr>
<tr>
<td style="text-align: left;">3000</td>
<td style="text-align: center;">$100 \%$</td>
<td style="text-align: center;">$100 \%$</td>
<td style="text-align: center;">$100 \%$</td>
<td style="text-align: center;">$99.9 \%$</td>
</tr>
</tbody>
</table>
<h2>E SENTENCE-LEVEL EXPERIMENTS</h2>
<p>We conducted experiments where input was at the sentence-level, that is the data was already presegemented into statements and questions as input to the MemNN (as opposed to being input as a stream of words). Results comparing RNNs with MemNNs are given in Table 5. The conclusions are similar to those at the word level from Section 5.2. That is, MemNNs outperform RNNs, and that inference that finds $k=2$ supporting statements and time features are necessary for the actor $\mathrm{w} / \mathrm{o}$ before + object task.</p>
<p>Table 5: Test accuracy on the sentence-level simulation QA task.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Difficulty 1</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Difficulty 5</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Method</td>
<td style="text-align: center;">actor <br> w/o before</td>
<td style="text-align: center;">actor w/o before <br> + object</td>
<td style="text-align: center;">actor <br> w/o before</td>
<td style="text-align: center;">actor w/o before <br> + object</td>
</tr>
<tr>
<td style="text-align: left;">RNN</td>
<td style="text-align: center;">$100 \%$</td>
<td style="text-align: center;">$58 \%$</td>
<td style="text-align: center;">$29 \%$</td>
<td style="text-align: center;">$17 \%$</td>
</tr>
<tr>
<td style="text-align: left;">MemNN $k=1$</td>
<td style="text-align: center;">$90 \%$</td>
<td style="text-align: center;">$9 \%$</td>
<td style="text-align: center;">$46 \%$</td>
<td style="text-align: center;">$21 \%$</td>
</tr>
<tr>
<td style="text-align: left;">MemNN $k=1$ (+time)</td>
<td style="text-align: center;">$100 \%$</td>
<td style="text-align: center;">$73 \%$</td>
<td style="text-align: center;">$100 \%$</td>
<td style="text-align: center;">$73 \%$</td>
</tr>
<tr>
<td style="text-align: left;">MemNN $k=2$ (+time)</td>
<td style="text-align: center;">$100 \%$</td>
<td style="text-align: center;">$99.95 \%$</td>
<td style="text-align: center;">$100 \%$</td>
<td style="text-align: center;">$99.4 \%$</td>
</tr>
</tbody>
</table>
<h1>F Multi-word Answer Setting Experiments</h1>
<p>We conducted experiments for the simulation data in the case where the answers are sentences (see Appendix A and Figure 2). As the single word answer model can no longer be used, we simply compare MemNNs using either RNNs or LSTMs for the response module $R$. As baselines we can still use RNNs and LSTMs in the standard setting of being fed words only including the statements and the question as a word stream. In contrast, the MemNN RNN and LSTMs are effectively fed the output of the $O$ module (see Section 3.1). In these experiments we only consider the difficulty 5 actor+object setting in the case of MemNNs with $k=2$ iterations (eq. (3)), which means the module $R$ is fed the features $\left[x, \mathbf{m}<em 1="1">{o</em>}}, \mathbf{m<em 2="2">{o</em>\right]$ after the modules $I, G$ and $O$ have run.}</p>
<p>The sentence generation is performed on the test data, and the evaluation we chose is as follows. A correct generation has to contain the correct location answer, and can optionally contain the subject or a correct pronoun referring to it. For example the question "Where is Bill?" allows the correct answers "Kitchen", "In the kitchen", "Bill is in the kitchen", "He is in the kitchen" and "I think Bill is in the kitchen". However incorrect answers contain an incorrect location or subject reference, for example "Joe is in the kitchen", "It is in the kitchen" or "Bill is in the bathroom I believe". We can then measure the percentage of text examples that are correct using this metric.</p>
<p>The numerical results are given in Table 6, and example output is given in Figure 2. The results indicate that MemNNs with LSTMs perform quite strongly, outperforming MemNNs using RNNs. However, both MemNN variant outperform both RNNs and LSTMs by some distance.</p>
<p>Table 6: Test accuracy on the multi-word answer simulation QA task. We compare conventional RNN and LSTMs with MemNNs using an RNN or LSTM module $R$ (i.e., where $R$ is fed features $\left[x, \mathbf{m}<em 1="1">{o</em>}}, \mathbf{m<em 2="2">{o</em>\right]$ after the modules $I, G$ and $O$ have run).}</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">MemNN: IGO features $\left[x, \mathbf{m}<em 1="1">{o</em>}}, \mathbf{m<em 2="2">{o</em>\right]$}</th>
<th style="text-align: center;">Word features</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">RNN</td>
<td style="text-align: center;">$68.83 \%$</td>
<td style="text-align: center;">$13.97 \%$</td>
</tr>
<tr>
<td style="text-align: left;">LSTM</td>
<td style="text-align: center;">$90.98 \%$</td>
<td style="text-align: center;">$14.01 \%$</td>
</tr>
</tbody>
</table>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{8}$ We also tried the same kind of experiments with sentence-level rather than word-sequence input, without joining sentences, giving results with similar overall conclusions, see Appendix E.
${ }^{9}$ We tried using standard language modeling on the questions as well, with slightly worse results.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
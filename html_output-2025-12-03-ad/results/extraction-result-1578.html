<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1578 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1578</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1578</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-31.html">extraction-schema-31</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of genetic/evolutionary algorithms that use crossover and mutation operations on code, programs, or literature to generate new solutions, with particular attention to measures of novelty, diversity, executability, and functionality.</div>
                <p><strong>Paper ID:</strong> paper-264203016</p>
                <p><strong>Paper Title:</strong> Applying Artificial Neural Network Hadron - Hadron Collisions at LHC</p>
                <p><strong>Paper Abstract:</strong> High Energy Physics (HEP) targeting on particle physics, searches for the fundamental par‐ ticles and forces which construct the world surrounding us and understands how our uni‐ verse works at its most fundamental level. Elementary particles of the Standard Model are gauge Bosons (force carriers) and Fermions which are classified into two groups: Leptons (i.e. Muons, Electrons, etc) and Quarks (Protons, Neutrons, etc).</p>
                <p><strong>Cost:</strong> 0.012</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1578.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1578.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of genetic/evolutionary algorithms that use crossover and mutation operations on code, programs, or literature to generate new solutions, with particular attention to measures of novelty, diversity, executability, and functionality.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GA-ANN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Genetic Algorithm - Artificial Neural Network hybrid model</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A hybrid system in which a standard genetic algorithm is used to search for optimal feed-forward neural network architectures, weights and biases; fitness is the sum-squared error (SSE) between network outputs and experimental particle-physics data.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Applying Artificial Neural Network Hadron -Hadron Collisions at LHC</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>GA-ANN hybrid model (standard GA optimizing ANN architectures/weights)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>The paper implements a hybrid pipeline where a population-based Genetic Algorithm (GA) encodes neural-network architecture features, connection structure, weights and biases as vectors and evolves them to minimize a fitness function defined as the summed squared error (SSE) between ANN predictions and experimental charged-particle multiplicity data. The GA runs for multiple generations and proposes candidate network designs which are trained/adjusted (Levenberg–Marquardt is used for weight training as part of the workflow) and re-evaluated; selection, recombination and mutation operators are used in the evolutionary loop. The goal is to find networks with low SSE and reduced connection count (i.e., more efficient architectures) that generalize to withheld test data.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type</strong></td>
                            <td>neural network architectures and numerical parameters (weights and biases); i.e., numerical vectors encoding topology and connection weights</td>
                        </tr>
                        <tr>
                            <td><strong>crossover_operation</strong></td>
                            <td>Paper reports use of a crossover operator selected with probability 0.9 but does not specify the internal crossover mechanism (e.g., bitstring one-point, real-valued vector crossover, or subtree recombination). The GA description states that a genetic operator is selected and applied to parents to produce children, but the exact representation-to-operator mapping and crossover-point selection mechanism are not detailed.</td>
                        </tr>
                        <tr>
                            <td><strong>mutation_operation</strong></td>
                            <td>Paper reports a mutation probability of 0.001 but gives no implementation details; mutation is stated generically as a perturbation operator applied to genes, with no precise description of how numerical weights or architecture genes are altered (e.g., Gaussian perturbation, uniform reset, or bit-flip).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_literature</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>uses_code</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>executability_metric</strong></td>
                            <td>Fitness measured as SSE (summed squared error) between predicted and observed charged-particle multiplicity; model complexity implicitly measured via number of connections in the network (reported as a secondary metric).</td>
                        </tr>
                        <tr>
                            <td><strong>executability_results</strong></td>
                            <td>Reported minimum SSE = 0.0001 for the GA-ANN optimized network. Comparison with baseline ANN: baseline ANN (3x15x15x1) reported SSE ≈ 0.01 versus GA-ANN SSE = 0.0001. Connection counts reported in the paper: baseline ANN 285 connections; GA-ANN model reported as 229 connections (the text contains one instance mentioning 225 but the table and appendix consistently report 229).</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>diversity_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_executability_tradeoff</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>frontier_characterization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_domain</strong></td>
                            <td>High-energy physics modeling: prediction of charged-particle multiplicity / pseudo-rapidity distributions for proton–proton collisions at LHC energies (0.9, 2.36, 7 TeV).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared against standard ANN trained with Levenberg–Marquardt (ANN architecture 3x15x15x1). Baseline: ANN SSE ≈ 0.01 with 285 connections; GA-ANN SSE = 0.0001 with 229 connections.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Using a GA to optimize ANN architecture and parameters produced models that matched experimental multiplicity distributions more closely (SSE reduced from ≈0.01 to 0.0001) while reducing reported connection count (285 → 229). The paper reports GA control settings (generations=1000, population=4000, crossover probability=0.9, mutation probability=0.001) but does not analyze the specific roles of crossover or mutation on novelty, diversity, or executability beyond showing that the GA-optimized networks achieve lower SSE and fewer connections than the baseline ANN.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1578.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1578.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of genetic/evolutionary algorithms that use crossover and mutation operations on code, programs, or literature to generate new solutions, with particular attention to measures of novelty, diversity, executability, and functionality.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Genetic Algorithm</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A population-based evolutionary optimization method using selection, crossover and mutation to search a solution space; in this paper used as the evolutionary engine within GA-ANN to optimize neural network topology and parameters.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Applying Artificial Neural Network Hadron -Hadron Collisions at LHC</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Standard Genetic Algorithm (as used in the GA-ANN hybrid)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>The paper describes and uses a standard GA workflow: initialize population, evaluate fitness (SSE), select parents, apply genetic operators (crossover and mutation) to generate offspring, and iterate until termination. The GA is applied to encoding choices for NN topology (hidden units, connectivity) and network parameters (weights/biases), with fitness guiding selection toward low-error network designs.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type</strong></td>
                            <td>encoded neural-network genotypes (architecture descriptors and numerical genes for weights/biases)</td>
                        </tr>
                        <tr>
                            <td><strong>crossover_operation</strong></td>
                            <td>Crossover is used with reported probability 0.9. The paper explains selection of a genetic operator and selection of the required number of parents, then application to generate children, but does not specify the exact crossover mechanism (e.g., one-point, two-point, arithmetic or subtree recombination).</td>
                        </tr>
                        <tr>
                            <td><strong>mutation_operation</strong></td>
                            <td>Mutation is used with reported probability 0.001; described generically as perturbation of individuals to introduce variety. No specific mutation operator (e.g., Gaussian perturbation for real-valued genes or bit-flip) is specified.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_literature</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>uses_code</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>executability_metric</strong></td>
                            <td>SSE used as the fitness measure; network size (number of connections) reported as a complexity/efficiency indicator.</td>
                        </tr>
                        <tr>
                            <td><strong>executability_results</strong></td>
                            <td>Minimum SSE reported = 0.0001 for GA-optimized networks; GA reduced reported connection count to 229 vs baseline 285.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>diversity_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_executability_tradeoff</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>frontier_characterization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_domain</strong></td>
                            <td>Neural network architecture/weight optimization for modeling LHC proton–proton multiplicity distributions.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Standard ANN trained with Levenberg–Marquardt algorithm (no GA).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>GA, when applied to NN architecture and parameter search with high crossover probability and very low mutation rate, produced a network with substantially lower SSE and fewer connections than the baseline ANN; however, the paper does not provide experimental analysis isolating the effects of crossover vs mutation or measuring novelty/diversity of evolved solutions.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1578.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1578.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of genetic/evolutionary algorithms that use crossover and mutation operations on code, programs, or literature to generate new solutions, with particular attention to measures of novelty, diversity, executability, and functionality.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Genetic Programming</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An evolutionary computation technique that evolves computer programs, often represented as tree-structured expressions (abstract syntax trees), using selection, crossover and mutation; cited in this paper as a related evolutionary technique but not used in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Applying Artificial Neural Network Hadron -Hadron Collisions at LHC</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Genetic Programming (tree-based variety)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>The paper references GP in the related-work context, describing the idea of representing programs as abstract syntax trees and evolving them via genetic operators. It cites foundational GP work (Koza et al.) and notes GP can be used to evolve programs/expressions and learning rules, but GP itself is not implemented in the experiments of this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type</strong></td>
                            <td>programs / tree-structured expressions (general description only; not applied in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>crossover_operation</strong></td>
                            <td>Paper notes the tree-based GP implements genetic operators differently from fixed-length GAs; typical GP crossover is subtree exchange between parent program trees, but the paper does not provide implementation details beyond citing GP literature.</td>
                        </tr>
                        <tr>
                            <td><strong>mutation_operation</strong></td>
                            <td>Paper generically references mutation as a GP operator (e.g., subtree mutation or node replacement in tree-based GP) but provides no implementation details in this work.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_literature</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>uses_code</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>executability_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>executability_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>diversity_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>diversity_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_executability_tradeoff</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>frontier_characterization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_domain</strong></td>
                            <td>Mentioned as a method for evolving programs/learning rules in related work; not benchmarked here.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Mentioned alongside GEP and GA in related work; not compared experimentally in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>GP is cited as an approach to evolve programs and learning rules; the paper does not evaluate GP or measure its effects on novelty, diversity, or executability within the presented experiments.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1578.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1578.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of genetic/evolutionary algorithms that use crossover and mutation operations on code, programs, or literature to generate new solutions, with particular attention to measures of novelty, diversity, executability, and functionality.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GEP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Gene Expression Programming</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An evolutionary algorithm that evolves mathematical models or programs using linear chromosomes that are expressed as expression trees; cited in the paper as an alternative evolutionary technique but not used in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Applying Artificial Neural Network Hadron -Hadron Collisions at LHC</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Gene Expression Programming (GEP)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>The paper references GEP as an evolutionary method used in modeling and related HEP applications (cites Ferreira). GEP represents candidate solutions as fixed-length linear chromosomes which map to expression trees (phenotypes) and applies genetic operators such as recombination and mutation to these chromosomes. The paper does not implement GEP but lists it among evolutionary alternatives.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type</strong></td>
                            <td>mathematical expressions / programs (expression trees) — mentioned in literature but not applied in this work</td>
                        </tr>
                        <tr>
                            <td><strong>crossover_operation</strong></td>
                            <td>GEP typically uses recombination between linear chromosomes (e.g., one-point, two-point crossover) and other GEP-specific recombination operators; the paper only cites GEP and does not detail the operators used.</td>
                        </tr>
                        <tr>
                            <td><strong>mutation_operation</strong></td>
                            <td>GEP employs mutation on linear chromosomes (e.g., symbol replacement), but the paper does not provide implementation specifics.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_literature</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>uses_code</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>executability_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>executability_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>diversity_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>diversity_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_executability_tradeoff</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>frontier_characterization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_domain</strong></td>
                            <td>Mentioned as an evolutionary modelling approach in related HEP literature; not evaluated in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Listed among evolutionary methods (GP, GA) in related work; not experimentally compared in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>GEP is noted as a related evolutionary modeling approach; no experimental details or metrics on novelty, diversity or executability are provided in this paper.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Genetic Programming: On the Programming of Computers by means of Natural Selection <em>(Rating: 2)</em></li>
                <li>Gene Expression Programming: Mathematical Modeling by an Artificial Intelligence <em>(Rating: 2)</em></li>
                <li>Adaptation in Natural and Artificial Systems <em>(Rating: 2)</em></li>
                <li>Training feedforward networks with the Marquardt algorithm <em>(Rating: 1)</em></li>
                <li>Training Feedforward Neural Networks Using Genetic Algorithms <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1578",
    "paper_id": "paper-264203016",
    "extraction_schema_id": "extraction-schema-31",
    "extracted_data": [
        {
            "name_short": "GA-ANN",
            "name_full": "Genetic Algorithm - Artificial Neural Network hybrid model",
            "brief_description": "A hybrid system in which a standard genetic algorithm is used to search for optimal feed-forward neural network architectures, weights and biases; fitness is the sum-squared error (SSE) between network outputs and experimental particle-physics data.",
            "citation_title": "Applying Artificial Neural Network Hadron -Hadron Collisions at LHC",
            "mention_or_use": "use",
            "system_name": "GA-ANN hybrid model (standard GA optimizing ANN architectures/weights)",
            "system_description": "The paper implements a hybrid pipeline where a population-based Genetic Algorithm (GA) encodes neural-network architecture features, connection structure, weights and biases as vectors and evolves them to minimize a fitness function defined as the summed squared error (SSE) between ANN predictions and experimental charged-particle multiplicity data. The GA runs for multiple generations and proposes candidate network designs which are trained/adjusted (Levenberg–Marquardt is used for weight training as part of the workflow) and re-evaluated; selection, recombination and mutation operators are used in the evolutionary loop. The goal is to find networks with low SSE and reduced connection count (i.e., more efficient architectures) that generalize to withheld test data.",
            "input_type": "neural network architectures and numerical parameters (weights and biases); i.e., numerical vectors encoding topology and connection weights",
            "crossover_operation": "Paper reports use of a crossover operator selected with probability 0.9 but does not specify the internal crossover mechanism (e.g., bitstring one-point, real-valued vector crossover, or subtree recombination). The GA description states that a genetic operator is selected and applied to parents to produce children, but the exact representation-to-operator mapping and crossover-point selection mechanism are not detailed.",
            "mutation_operation": "Paper reports a mutation probability of 0.001 but gives no implementation details; mutation is stated generically as a perturbation operator applied to genes, with no precise description of how numerical weights or architecture genes are altered (e.g., Gaussian perturbation, uniform reset, or bit-flip).",
            "uses_literature": false,
            "uses_code": false,
            "novelty_metric": null,
            "novelty_results": null,
            "executability_metric": "Fitness measured as SSE (summed squared error) between predicted and observed charged-particle multiplicity; model complexity implicitly measured via number of connections in the network (reported as a secondary metric).",
            "executability_results": "Reported minimum SSE = 0.0001 for the GA-ANN optimized network. Comparison with baseline ANN: baseline ANN (3x15x15x1) reported SSE ≈ 0.01 versus GA-ANN SSE = 0.0001. Connection counts reported in the paper: baseline ANN 285 connections; GA-ANN model reported as 229 connections (the text contains one instance mentioning 225 but the table and appendix consistently report 229).",
            "diversity_metric": null,
            "diversity_results": null,
            "novelty_executability_tradeoff": null,
            "frontier_characterization": null,
            "benchmark_or_domain": "High-energy physics modeling: prediction of charged-particle multiplicity / pseudo-rapidity distributions for proton–proton collisions at LHC energies (0.9, 2.36, 7 TeV).",
            "comparison_baseline": "Compared against standard ANN trained with Levenberg–Marquardt (ANN architecture 3x15x15x1). Baseline: ANN SSE ≈ 0.01 with 285 connections; GA-ANN SSE = 0.0001 with 229 connections.",
            "key_findings": "Using a GA to optimize ANN architecture and parameters produced models that matched experimental multiplicity distributions more closely (SSE reduced from ≈0.01 to 0.0001) while reducing reported connection count (285 → 229). The paper reports GA control settings (generations=1000, population=4000, crossover probability=0.9, mutation probability=0.001) but does not analyze the specific roles of crossover or mutation on novelty, diversity, or executability beyond showing that the GA-optimized networks achieve lower SSE and fewer connections than the baseline ANN.",
            "uuid": "e1578.0"
        },
        {
            "name_short": "GA",
            "name_full": "Genetic Algorithm",
            "brief_description": "A population-based evolutionary optimization method using selection, crossover and mutation to search a solution space; in this paper used as the evolutionary engine within GA-ANN to optimize neural network topology and parameters.",
            "citation_title": "Applying Artificial Neural Network Hadron -Hadron Collisions at LHC",
            "mention_or_use": "use",
            "system_name": "Standard Genetic Algorithm (as used in the GA-ANN hybrid)",
            "system_description": "The paper describes and uses a standard GA workflow: initialize population, evaluate fitness (SSE), select parents, apply genetic operators (crossover and mutation) to generate offspring, and iterate until termination. The GA is applied to encoding choices for NN topology (hidden units, connectivity) and network parameters (weights/biases), with fitness guiding selection toward low-error network designs.",
            "input_type": "encoded neural-network genotypes (architecture descriptors and numerical genes for weights/biases)",
            "crossover_operation": "Crossover is used with reported probability 0.9. The paper explains selection of a genetic operator and selection of the required number of parents, then application to generate children, but does not specify the exact crossover mechanism (e.g., one-point, two-point, arithmetic or subtree recombination).",
            "mutation_operation": "Mutation is used with reported probability 0.001; described generically as perturbation of individuals to introduce variety. No specific mutation operator (e.g., Gaussian perturbation for real-valued genes or bit-flip) is specified.",
            "uses_literature": false,
            "uses_code": false,
            "novelty_metric": null,
            "novelty_results": null,
            "executability_metric": "SSE used as the fitness measure; network size (number of connections) reported as a complexity/efficiency indicator.",
            "executability_results": "Minimum SSE reported = 0.0001 for GA-optimized networks; GA reduced reported connection count to 229 vs baseline 285.",
            "diversity_metric": null,
            "diversity_results": null,
            "novelty_executability_tradeoff": null,
            "frontier_characterization": null,
            "benchmark_or_domain": "Neural network architecture/weight optimization for modeling LHC proton–proton multiplicity distributions.",
            "comparison_baseline": "Standard ANN trained with Levenberg–Marquardt algorithm (no GA).",
            "key_findings": "GA, when applied to NN architecture and parameter search with high crossover probability and very low mutation rate, produced a network with substantially lower SSE and fewer connections than the baseline ANN; however, the paper does not provide experimental analysis isolating the effects of crossover vs mutation or measuring novelty/diversity of evolved solutions.",
            "uuid": "e1578.1"
        },
        {
            "name_short": "GP",
            "name_full": "Genetic Programming",
            "brief_description": "An evolutionary computation technique that evolves computer programs, often represented as tree-structured expressions (abstract syntax trees), using selection, crossover and mutation; cited in this paper as a related evolutionary technique but not used in experiments.",
            "citation_title": "Applying Artificial Neural Network Hadron -Hadron Collisions at LHC",
            "mention_or_use": "mention",
            "system_name": "Genetic Programming (tree-based variety)",
            "system_description": "The paper references GP in the related-work context, describing the idea of representing programs as abstract syntax trees and evolving them via genetic operators. It cites foundational GP work (Koza et al.) and notes GP can be used to evolve programs/expressions and learning rules, but GP itself is not implemented in the experiments of this paper.",
            "input_type": "programs / tree-structured expressions (general description only; not applied in this paper)",
            "crossover_operation": "Paper notes the tree-based GP implements genetic operators differently from fixed-length GAs; typical GP crossover is subtree exchange between parent program trees, but the paper does not provide implementation details beyond citing GP literature.",
            "mutation_operation": "Paper generically references mutation as a GP operator (e.g., subtree mutation or node replacement in tree-based GP) but provides no implementation details in this work.",
            "uses_literature": false,
            "uses_code": true,
            "novelty_metric": null,
            "novelty_results": null,
            "executability_metric": null,
            "executability_results": null,
            "diversity_metric": null,
            "diversity_results": null,
            "novelty_executability_tradeoff": null,
            "frontier_characterization": null,
            "benchmark_or_domain": "Mentioned as a method for evolving programs/learning rules in related work; not benchmarked here.",
            "comparison_baseline": "Mentioned alongside GEP and GA in related work; not compared experimentally in this paper.",
            "key_findings": "GP is cited as an approach to evolve programs and learning rules; the paper does not evaluate GP or measure its effects on novelty, diversity, or executability within the presented experiments.",
            "uuid": "e1578.2"
        },
        {
            "name_short": "GEP",
            "name_full": "Gene Expression Programming",
            "brief_description": "An evolutionary algorithm that evolves mathematical models or programs using linear chromosomes that are expressed as expression trees; cited in the paper as an alternative evolutionary technique but not used in experiments.",
            "citation_title": "Applying Artificial Neural Network Hadron -Hadron Collisions at LHC",
            "mention_or_use": "mention",
            "system_name": "Gene Expression Programming (GEP)",
            "system_description": "The paper references GEP as an evolutionary method used in modeling and related HEP applications (cites Ferreira). GEP represents candidate solutions as fixed-length linear chromosomes which map to expression trees (phenotypes) and applies genetic operators such as recombination and mutation to these chromosomes. The paper does not implement GEP but lists it among evolutionary alternatives.",
            "input_type": "mathematical expressions / programs (expression trees) — mentioned in literature but not applied in this work",
            "crossover_operation": "GEP typically uses recombination between linear chromosomes (e.g., one-point, two-point crossover) and other GEP-specific recombination operators; the paper only cites GEP and does not detail the operators used.",
            "mutation_operation": "GEP employs mutation on linear chromosomes (e.g., symbol replacement), but the paper does not provide implementation specifics.",
            "uses_literature": false,
            "uses_code": true,
            "novelty_metric": null,
            "novelty_results": null,
            "executability_metric": null,
            "executability_results": null,
            "diversity_metric": null,
            "diversity_results": null,
            "novelty_executability_tradeoff": null,
            "frontier_characterization": null,
            "benchmark_or_domain": "Mentioned as an evolutionary modelling approach in related HEP literature; not evaluated in this paper.",
            "comparison_baseline": "Listed among evolutionary methods (GP, GA) in related work; not experimentally compared in this paper.",
            "key_findings": "GEP is noted as a related evolutionary modeling approach; no experimental details or metrics on novelty, diversity or executability are provided in this paper.",
            "uuid": "e1578.3"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Genetic Programming: On the Programming of Computers by means of Natural Selection",
            "rating": 2,
            "sanitized_title": "genetic_programming_on_the_programming_of_computers_by_means_of_natural_selection"
        },
        {
            "paper_title": "Gene Expression Programming: Mathematical Modeling by an Artificial Intelligence",
            "rating": 2,
            "sanitized_title": "gene_expression_programming_mathematical_modeling_by_an_artificial_intelligence"
        },
        {
            "paper_title": "Adaptation in Natural and Artificial Systems",
            "rating": 2,
            "sanitized_title": "adaptation_in_natural_and_artificial_systems"
        },
        {
            "paper_title": "Training feedforward networks with the Marquardt algorithm",
            "rating": 1,
            "sanitized_title": "training_feedforward_networks_with_the_marquardt_algorithm"
        },
        {
            "paper_title": "Training Feedforward Neural Networks Using Genetic Algorithms",
            "rating": 1,
            "sanitized_title": "training_feedforward_neural_networks_using_genetic_algorithms"
        }
    ],
    "cost": 0.0123185,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Applying Artificial Neural Network Hadron -Hadron Collisions at LHC</p>
<p>Amr Radi 
Samy K Hindawi 
Applying Artificial Neural Network Hadron -Hadron Collisions at LHC
BB625939441A2070CAEE93F868211EE310.5772/51273</p>
<p>Introduction</p>
<p>High Energy Physics (HEP) targeting on particle physics, searches for the fundamental particles and forces which construct the world surrounding us and understands how our universe works at its most fundamental level.Elementary particles of the Standard Model are gauge Bosons (force carriers) and Fermions which are classified into two groups: Leptons (i.e.Muons, Electrons, etc) and Quarks (Protons, Neutrons, etc).</p>
<p>The study of the interactions between those elementary particles requests enormously high energy collisions as in LHC [1][2][3][4][5][6][7][8], up to the highest energy hadrons collider in the world s =14 Tev.Experimental results provide excellent opportunities to discover the missing particles of the Standard Model.As well as, LHC possibly will yield the way in the direction of our awareness of particle physics beyond the Standard Model.</p>
<p>The proton-proton (p-p) interaction is one of the fundamental interactions in high-energy physics.In order to fully exploit the enormous physics potential, it is important to have a complete understanding of the reaction mechanism.The particle multiplicity distributions, as one of the first measurements made at LHC, used to test various particle production models.It is based on different physics mechanisms and also provide constrains on model features.Some of these models are based on string fragmentation mechanism [9][10][11] and some are based on Pomeron exchange [12].</p>
<p>Recently, different modeling methods, based on soft computing systems, include the application of Artificial Intelligence (AI) Techniques.Those Evolution Algorithms have a physical powerful existence in that field [13][14][15][16][17].The behavior of the p-p interactions is complicated due to the nonlinear relationship between the interaction parameters and the output.To understand the interactions of fundamental particles, multipart data analysis are needed and AI techniques are vital.Those techniques are becoming useful as alternate approaches to conventional ones [18].In this sense, AI techniques, such as Artificial Neural Network (ANN) [19], Genetic Algorithm (GA) [20], Genetic Programming (GP) [21 and Gene Expression Programming (GEP) [22], can be used as alternative tools for the simulation of these interactions [13][14][15][16][17][21][22][23].</p>
<p>The motivation of using a NN approach is its learning algorithm that learns the relationships between variables in sets of data and then builds models to explain these relationships (mathematically dependant).</p>
<p>In this chapter, we have discovered the functions that describe the multiplicity distribution of the charged shower particles of p-p interactions at different values of high energies using the GA-ANN technique.This chapter is organized on five sections.Section 2, gives a review to the basics of the NN &amp; GA technique.Section 3 explains how NN &amp; GA is used to model the p-p interaction.Finally, the results and conclusions are provided in sections 4 and 5 respectively.</p>
<p>An overview of Artificial Neural Networks (ANN)</p>
<p>An ANN is a network of artificial neurons which can store, gain and utilize knowledge.Some researchers in ANNs decided that the name <code>neuron'' was inappropriate and used other terms, such as</code>node''.However, the use of the term neuron is now so deeply established that its continued general use seems assured.A way to encompass the NNs studied in the literature is to regard them as dynamical systems controlled by synaptic matrixes (i.e.Parallel Distributed Processes (PDPs)) [24].</p>
<p>In the following sub-sections we introduce some of the concepts and the basic components of NNs:</p>
<p>Neuron-like Processing Units</p>
<p>A processing neuron based on neural functionality which equals to the summation of the products of the input patterns element {x 1 , x 2 ,..., x p } and its corresponding weights {w 1 , w 2 ,..., w p } plus the bias θ.Some important concepts associated with this simplified neuron are defined below.</p>
<p>A single-layer network is an area of neurons while a multilayer network consists of more than one area of neurons.</p>
<p>Let u i ℓ be the i th neuron in ℓ th layer.The input layer is called the x th layer and the output layer is called the O th layer.Let nℓ be the number of neurons in the ℓ th layer.The weight of the link between neuron u j ℓ in layer ℓ and neuron u i ℓ+1 in layer ℓ+1 is denoted by w ij ℓ .Let {x 1 , x 2 ,..., x p } be the set of input patterns that the network is supposed to learn its classification and let {d 1 , d 2 ,..., d p }be the corresponding desired output patterns.It should be noted that x p is an n dimension vector {x 1p , x 2p ,..., x np } and d p is an n dimension vector {d 1p ,d 2p ,...,d np }.The pair (x p , d p ) is called a training pattern.</p>
<p>The output of a neuron u i 0 is the input x ip (for input pattern p).For the other layers, the network input net pi ℓ+1 to a neuron u i ℓ+1 for the input x pi ℓ+1 is usually computed as follows:
1 1 1 n pi ij pj i j net w o i i i i i q + + = = - å(1)
where O pj ℓ = x pi ℓ+1 is the output of the neuron u j ℓ of layer ℓ and θ i ℓ+1 is the neuron's bias value of neuron u i ℓ+1 of layer ℓ+1.For the sake of a homogeneous representation, θ i is often substituted by a ``bias neuron'' with a constant output 1.This means that biases can be treated like weights, which is done throughout the remainder of the text.</p>
<p>Activation Functions</p>
<p>The activation function converts the neuron input to its activation (i.e. a new state of activation) by f (net p ).This allows the variation of input conditions to affect the output, usually included as O p .</p>
<p>The sigmoid function, as a non-linear function, is also often used as an activation function.The logistic function is an example of a sigmoid function of the following form:
1 ( ) 1 pi pj pi net o f net e i i i b - = = +(2)
where β determines the steepness of the activation function.In the rest of this chapter we assume that β=1.</p>
<p>Network Architectures</p>
<p>Network architectures have different types (single-layer feedforward, multi-layer feedforward, and recurrent networks) [25].In this chapter the Multi-layer Feedforward Networks are considered, these contain one or more hidden layers.Hidden layers are placed between input and output layers.Those hidden layers enable extraction of higher-order features.The input layer receives an external activation vector, and passes it via weighted connections to the neurons in the first hidden layer [25].An example of this arrangement, a three layer NN, is shown in Fig 1 .This is a common form of NN.</p>
<p>Neural Networks Learning</p>
<p>To use a NN, it is essential to have some form of training, through which the values of the weights in the network are adjusted to reflect the characteristics of the input data.When the network is trained sufficiently, it will obtain the most nearest correct output for a presented set of input data.</p>
<p>A set of well-defined rules for the solution of a learning problem is called a learning algorithm.No unique learning algorithm exists for the design of NN.Learning algorithms differ from each other in the way in which the adjustment of Δw ij to the synaptic weight w ij is formulated.In other words, the objective of the learning process is to tune the weights in the network so that the network performs the desired mapping of input to output activation.</p>
<p>NNs are claimed to have the feature of generalization, through which a trained NN is able to provide correct output data to a set of previously (unseen) input data.Training determines the generalization capability in the network structure.</p>
<p>Supervised learning is a class of learning rules for NNs.In which a teaching is provided by telling the network output required for a given input.Weights are adjusted in the learning system so as to minimize the difference between the desired and actual outputs for each input training data.An example of a supervised learning rule is the delta rule which aims to minimize the error function.This means that the actual response of each output neuron, in the network, approaches the desired response for that neuron.This is illustrated in Fig 2.</p>
<p>The error ε pi for the i th neuron u i o of the output layer o for the training pair (x p , t p ) is computed as:
o pi pi pi t o e = - (3)
This error is used to adjust the weights in such a way that the error is gradually reduced.The training process stops when the error for every training pair is reduced to an acceptable level, or when no further improvement is obtained.A method, known as "learning by epoch", first sums gradient information for the whole pattern set and then updates the weights.This method is also known as "batch learning" and most researchers use it for its good performance [25].Each weight-update tries to minimize the summed error of the pattern set.The error function can be defined for one training pattern pair (x p , d p ) as:
1 1/ 2 o n p pi i E e = = å(4)
Then, the error function can be defined for all the patterns (Known as the Total Sum of Squared, (TSS) errors as:
1 1 1 2 m n pi p i E e = = = åå(5)
The most desirable condition that we could achieve in any learning algorithm training is ε pi ≥0.Obviously, if this condition holds for all patterns in the training set, we can say that the algorithm found a global minimum.</p>
<p>The weights in the network are changed along a search direction, to drive the weights in the direction of the estimated minimum.The weight updating rule for the batch mode is given by: ( ) ( )
s 1 ij ij ij w w s w s + = D + l l(6)
where w ij s+1 is the update weight of w ij ℓ of layer ℓ in the s th learning step, and s is the step number in the learning process.</p>
<p>In training a network, the available input data set consists of many facts and is normally divided into two groups.One group of facts is used as the training data set and the second group is retained for checking and testing the accuracy of the performance of the network after training.The proposed ANN model was trained using Levenberg-Marquardt optimization technique [26].</p>
<p>Data collected from experiments are divided into two sets, namely, training set and testing set.The training set is used to train the ANN model by adjusting the link weights of network model, which should include the data covering the entire experimental space.This means that the training data set has to be fairly large to contain all the required information and must include a wide variety of data from different experimental conditions, including different formulation composition and process parameters.</p>
<p>Linearly, the training error keeps dropping.If the error stops decreasing, or alternatively starts to rise, the ANN model starts to over-fit the data, and at this point, the training must be stopped.In case over-fitting or over-learning occurs during the training process, it is usually advisable to decrease the number of hidden units and/or hidden layers.In contrast, if the network is not sufficiently powerful to model the underlying function, over-learning is not likely to occur, and the training errors will drop to a satisfactory level.</p>
<p>An overview of Genetic Algorithm</p>
<p>Introduction</p>
<p>Evolutionary Computation (EC) uses computational models of evolutionary processes based on concepts in biological theory.Varieties of these evolutionary computational models have been proposed and used in many applications, including optimization of NN parameters and searching for new NN learning rules.We will refer to them as Evolutionary Algorithms (EAs) [27][28][29] EAs are based on the evolution of a population which evolves according to rules of selection and other operators such as crossover and mutation.Each individual in the population is given a measure of its fitness in the environment.Selection favors individual with high fitness.These individuals are perturbed using the operators.This provides general heuristics for exploration in the environment.This cycle of evaluation, selection, crossover, mutation and survival continues until some termination criterion is met.Although, it is very simple from a biological point of view, these algorithms are sufficiently complex to provide strong and powerful adaptive search mechanisms.</p>
<p>Genetic Algorithms (GAs) were developed in the 70s by John Holland [30], who strongly stressed recombination as the energetic potential of evolution [32].The notion of using abstract syntax trees to represent programs in GAs, Genetic Programming (GP), was suggested in [33], first implemented in [34] and popularised in [35][36][37].The term Genetic Programming is used to refer to both tree-based GAs and the evolutionary generation of programs [38,39].Although similar at the highest level, each of the two varieties implements genetic operators in a different manner.This thesis concentrates on the tree-based variety.We will discuss GP further in Section 3.4.In the following two sections, whose descriptions are mainly based on [30,32,33,35,36,37], we give more background information about natural and artificial evolution in general, and on GAs in particular.</p>
<p>Natural and Artificial Evolution</p>
<p>As described by Darwin [40], evolution is the process by which a population of organisms gradually adapt over time to enhance their chances of surviving.This is achieved by ensuring that the stronger individuals in the population have a higher chance of reproducing and creating children (offspring).</p>
<p>In artificial evolution, the members of the population represent possible solutions to a particular optimization problem.The problem itself represents the environment.We must apply each potential solution to the problem and assign it a fitness value, indicating its performance on the problem.The two essential features of natural evolution which we need to maintain are propagation of more adaptive features to future generations (by applying a selective pressure which gives better solutions a greater opportunity to reproduce) and the heritability of features from parent to children (we need to ensure that the process of reproduction keeps most of the features of the parent solution and yet allows for variety so that new features can be explored) [30].</p>
<p>The Genetic Algorithm</p>
<p>GAs is powerful search and optimization techniques, based on the mechanics of natural selection [31].Some basic terms used are:</p>
<p>• A phenotype is a possible solution to the problem;</p>
<p>• A chromosome is an encoding representation of a phenotype in a form that can be used;</p>
<p>• A population is the variety of chromosomes that evolves from generation to generation;</p>
<p>• A generation (a population set) represents a single step toward the solution;</p>
<p>• Fitness is the measure of the performance of an individual on the problem;</p>
<p>• Evaluation is the interpretation of the genotype into the phenotype and the computation of its fitness;</p>
<p>• Genes are the parts of data which make up a chromosome.</p>
<p>The advantage of GAs is that they have a consistent structure for different problems.Accordingly, one GA can be used for a variety of optimization problems.GAs are used for a number of different application areas [30].GA is capable of finding good solutions quickly [32].Also, the GA is inherently parallel, since a population of potential solutions is maintained.</p>
<p>To solve an optimization problem, a GA requires four components and a termination criterion for the search.The components are: a representation (encoding) of the problem, a fitness evaluation function, a population initialization procedure and a set of genetic operators.</p>
<p>In addition, there are a set of GA control parameters, predefined to guide the GA, such as the size of the population, the method by which genetic operators are chosen, the probabilities of each genetic operator being chosen, the choice of methods for implementing probability in selection, the probability of mutation of a gene in a selected individual, the method used to select a crossover point for the recombination operator and the seed value used for the random number generator.</p>
<p>The structure of a typical GA can be described as follows [41] In the algorithm, an initial population is generated in line 2.Then, the algorithm computes the fitness for each member of the initial population in line 3. Subsequently, a loop is entered based on whether or not the algorithm's termination criteria are met in line 4. Line 6 contains the control code for the inner loop in which a new generation is created.Lines 7 through 10 contain the part of the algorithm in which new individuals are generated.First, a genetic operator is selected.The particular numbers of parents for that operator are then selected.The operator is then applied to generate one or more new children.Finally, the new children are added to the new generation.The most significant differences in GAs are:</p>
<p>• GAs search a population of points in parallel, not a single point</p>
<p>• GAs do not require derivative information (unlike gradient descending methods, e.g.SBP) or other additional knowledge -only the objective function and corresponding fitness levels affect the directions of search</p>
<p>• GAs use probabilistic transition rules, not deterministic ones</p>
<p>• GA can provide a number of potential solutions to a given problem</p>
<p>• GAs operate on fixed length representations.</p>
<p>The Proposed Hybrid GA -ANN Modeling</p>
<p>Genetic connectionism combines genetic search and connectionist computation.GAs have been applied successfully to the problem of designing NNs with supervised learning processes, for evolving the architecture suitable for the problem [42][43][44][45][46][47].However, these applications do not address the problem of training neural networks, since they still depend on other training methods to adjust the weights.</p>
<p>GAs for Training NNs</p>
<p>GAs have been used for training NNs either with fixed architectures or in combination with constructive/destructive methods.This can be made by replacing traditional learning algorithms such as gradient-based methods [48].Not only have GAs been used to perform weight training for supervised learning and for reinforcement learning applications, but they have also been used to select training data and to translate the output behavior of NNs [49][50][51].GAs have been applied to the problem of finding NN architectures [52][53][54][55][56][57], where an architecture specification indicates how many hidden units a network should have and how these units should be connected.</p>
<p>The process key in the evolutionary design of neural architectures is shown in Fig.</p>
<p>The topologies of the network have to be distinct before any training process.The definition of the architecture has great weight on the network performance, the effectiveness and efficiency of the learning process.As discussed in [58], the alternative provided by destructive and constructive techniques is not satisfactory.</p>
<p>The network architecture designing can be explained as a search in the architecture space that each point represents a different topology.The search space is huge, even with a limited number of neurons, and a controlled connectivity.Additionally, the search space makes things even more difficult in some cases.For instance when networks with different topologies may show similar learning and generalization abilities, alternatively, networks with similar structures may have different performances.In addition, the performance evaluation depends on the training method and on the initial conditions (weight initialization) [59].</p>
<p>Building the architectures by means of GAs is strongly reliant on how the features of the network are encoded in the genotype.Using a bitstring is not essentially the best approach to evolve the architecture.Therefore, a determination has to be made concerning how the information about the architecture should be encoded in the genotype.</p>
<p>To find good NN architectures using GAs, we should know how to encode architectures (neurons, layers, and connections) in the chromosomes that can be manipulated by the GA.Encoding of NNs onto a chromosome can take many different forms.</p>
<p>Modeling by Using ANN and GA</p>
<p>This study proposed a hybrid model combined of ANN and GA (We called it "GA-ANN hybrid model") for optimization of the weights of feed-forward neural networks to improve the effectiveness of the ANN model.Assuming that the structure of these networks has been decided.Genetic algorithm is run to have the optimal parameters of the architectures, weights and biases of all the neurons which are joined to create vectors.We construct a genetic algorithm, which can search for the global optimum of the number of hidden units and the connection structure between the inputs and the output layers.During the weight training and adjusting process, the fitness functions of a neural network can be defined by considering two important factors: the error is the different between target and actual outputs.</p>
<p>In this work, we defined the fitness function as the mean square error (SSE).The approach is to use the GA-ANN model that is enough intelligent to discover functions for p-p interactions (mean multiplicity distribution of charged particles with respects of the total center of mass energy).The model is trained/predicated by using experimental data to simulate the pp interaction.GA-ANN has the potential to discover a new model, to show that the data sets are subdivided into two sets (training and predication).GA-ANN discovers a new model by using the training set while the predicated set is used to examine their generalization capabilities.To measure the error between the experimental data and the simulated data we used the statistic measures.The total deviation of the response values from the fit to the response values.It is also called the summed square of residuals and is usually labeled as SSE.The statistical measures of sum squared error (SSE),
SSE = ∑ i=1 n (y i − y ^i) 2(7)
where y ^i = b 0 + b 1 x i is the predicted value for x i and y i is the observed data value occurring at x i .The proposed GA-ANN hybrid model has been used to model the multiplicity distribution of the charged shower particles.The proposed model was trained using Levenberg-Marquardt optimization technique [26].The architecture of GA-ANN has three inputs and one output.The inputs are the charged particles multiplicity (n), the total center of mass energy ( s ), and the pseudo rapidity (η).The output is the charged particles multiplicity distribution (P n ). Figure 1 shows the schematic of GA-ANN model.</p>
<p>Results and discussion</p>
<p>The input patterns of the designed GA-ANN hybrid have been trained to produce target patterns that modeling the pseudo-rapidity distribution.The fast Levenberg-Marquardt algorithm (LMA) has been employed to train the ANN.In order to obtain the optimal structure of ANN, we have used GA as hybrid model.</p>
<p>Simulation results based on both ANN and GA-ANN hybrid model, to model the distribution of shower charged particle produced for P-P at different the total center of mass energy, s 0.9 TeV, 2.36 Tev and 7 TeV, are given in Figure 2-a, b, and c respectively.We notice that the curves obtained by the trained GA-ANN hybrid model show an exact fitting to the experimental data in the three cases.</p>
<p>Then, the GA-ANN Hybrid model is able to exactly model for the charge particle multiplicity distribution.The total sum of squared error SSE, the weights and biases which used for the designed network are provided in the Appendix A. In this model we have obtained the minimum error (=0.0001) by using GA.Table 1 shows a comparison between the ANN model and the GA-ANN model for the prediction of the pseudo-rapidity distribution.In the 3x15x15x1 ANN structure, we have used 285 connections and obtained an error equal to 0.0001, while the connection in GA-ANN model is 225.Therefore, we noticed in the ANN model that by increasing the number of connections to 285 the error decreases to 0.01, but this needs more calculations.By using GA optimization search, we have obtained the structure which minimizes the number of connections equals to 229 only and the error (= 0.0001).This indicates that the GA-ANN hybrid model is more efficient than the ANN model.</p>
<p>Conclusions</p>
<p>The chapter presents the GA-ANN as a new technique for constructing the functions of the multiplicity distribution of charged particles, P n (n, η, s ) of p-p interaction.The discovered models show good match to the experimental data.Moreover, they are capable of testing experimental data for P n (n, η, s ) that are not used in the training session.</p>
<p>Consequence, the testing values of P n (n, η, s ) in terms of the same parameters are in good agreement with the experimental data from Particle Data Group.Finally, we conclude that GA-ANN has become one of important research areas in the field of high Energy physics.</p>
<p>Appendices</p>
<p>The The optimized GA-ANN:</p>
<p>The standard GA has been used.The parameters are given as follows: Generation = 1000, Population = 4000, probability of crossover = 0.9, probability of mutation = 0.001, Fitness function is SSE.A neural network had been optimized as 229 of neurons.</p>
<p>Figure 1 .
1
Figure 1. the three layers (input, hidden and output) of neurons are fully interconnected.</p>
<p>Figure 2 .
2
Figure 2. Example of Supervised Learning.</p>
<p>Lines 11 and 12
12
serve to close the outer loop of the algorithm.Fitness values are computed for each individual in the new generation.These values are used to guide simulated natural selection in the new generation.The termination criterion is tested and the algorithm is either repeated or terminated.</p>
<p>Figure 3 .
3
Figure 3. Overview of GA-ANN hybrid model.</p>
<p>Figure 4 .
4
Figure 4. ANN and GA-ANN simulation results for charge particle Multiplicity distribution of shower p-p.</p>
<p>Data collected from experiments are divided into two sets, namely, training set and testing set.The training set is used to train the GA-ANN hybrid model.The testing data set is used to confirm the accuracy of the proposed model.It ensures that the relationship between inputs and outputs, based on the training and test sets are real.The data set is divided into two groups 80% for training and 20% for testing.For work completeness, the final weights and biases after training are given in Appendix A.</p>
<p>Table 1 .
1
Comparison between the different training algorithms (ANN and GA-ANN) for the for charge particle Multiplicity distribution.
StructureNumber of connectionsError valuesLearning ruleANN: 3 x15x15x12850.01LMAGA optimization structure 2290.0001GA</p>
<p>efficient ANN structure is given as follows: [3x15x15x1] or [ixjxkxm].Wmk = [0.92831.6321 0.0356 -0.4147 -0.8312 -3.0722 -1.9368 1.7113 0.0100 -0.4066 0.0721 0.1362 0.4692 -0.9749 1.7950].
Columns 11 through 15-0.61230.4833-0.04570.3927-0.3694Wkj = -0.0746[0.3294 -0.0978 0.50060.0421 0.07100.3603 -0.76100.5147 0.1412-0.33730.5506 0.4167 0.24980.2678 0.34210.2670 -0.05770.3568 0.21090.24220.3951 0.2013 0.25290.2169 -0.13840.4323 -0.37000.0683 -0.44640.08680.1875 -0.5964 0.29480.2705 -0.08370.2209 -0.79710.1928 -0.4299-0.65000.2207 -1.1315 0.61210.0693 -0.45570.0125 1.61690.4214 -0.32050.22050.4698 1.0185 0.06970.4795 0.47520.0425 -0.41550.2387 0.16141.23110.1975 0.0061 0.14410.2947 -0.05390.1347 0.68130.0403 0.9395-0.42950.0745 -0.3083 0.23450.1572 0.27680.2792 -0.11510.3784 0.08020.1043 0.2346 0.4784 Weights coefficient after training are: -0.6988 0.5578 0.7176 -0.0601 0.05270.2899 -0.3455 0.3619 0.35190.2012 0.0432 0.2601 0.35200.4270 0.1663 0.2738 -0.7821-0.62410.1081 -0.1201 0.24120.0074 -0.43170.3967 0.74410.2235 0.7305Wji =0.5433[3.5001 0.0407 -0.6909 0.0466 0.4848 0.0592-1.0299 0.3128 -0.38881.6118 0.1570 0.3710-0.69200.7565 0.4505 -0.0190 0.4321 -0.4892 0.0313-2.2408 0.5976 0.16783.2605 0.0851 0.0808-0.3752-1.4374 0.4887 -0.1745 0.4295 -0.7304 0.06941.1033 0.3939 0.0462-3.1349 0.0354 -0.3883].0.19722.0116 0.14160.17062.8137 0.1719-1.7322 0.0761-3.6012-1.5717-0.2805-1.6741-2.58442.7109-2.0600-3.15191.2488Columns 6 through 10 0.2102 0.0185 bi = [-4.7175 -2.2157 3.6932 ].-0.1986 2.6272 -0.16581.0028 0.8254 -0.1943-4.0855 3.6292 -0.4253-2.3420 0.4946 bj = [-4.1756 -3.8559 3.9766 -3.3430 2.7598 2.5040 2.1326 1.9297 3.0259 0.2685 0.4724 -0.3538-1.9551 0.1559-3.2561 0.5657 -0.6547 0.7272 0.5859 -1.1575 0.3029 0.3486 -0.4088]. 0.3198 0.12070.4683 -0.38943.0896 0.14971.2442 -3.2589 -1.0889 0.5570 0.2453 bk = [ 1.7214 -1.7100 1.5000 -1.2915 1.1448 1.0033 -0.6584 -0.4397 -0.8996 -1.1887 -1.2080 -0.5528 0.4031 0.4562 0.3498 -0.3870 0.4581 0.2047 -0.0802 0.1584 0.2806 -0.4963 -0.3211 0.2594 -0.1649 0.0603 -0.1078].-3.4896 2.0875 4.3688 -0.5802 0.2430 -0.2790bm = [-0.2071].0.0981-0.5055-2.7820 0.2559-1.4291 -0.02972.3577 -0.2058-0.3498-0.55133.1861 0.0022-0.6309 -0.30342.0691 0.2156-0.6226-0.40853.4979 0.43380.2456 -0.0441-2.6633 -0.4801-0.00930.0875-0.4889 0.08152.4145 0.3935-2.8041 0.18400.00630.27902.1091 0.7558-0.1359 0.3383-3.4762 0.5882-0.5506-0.0518-0.1010 0.56254.1758 0.2459-0.2120 -0.06120.00360.44043.5538 -0.3268-1.5615 -0.5626-1.4795 -0.22530.5591-0.2797-3.4153 -0.04081.2517 0.13022.1415 -0.43612.6232-3.07570.08311.76321.9749-2.55197.69870.05260.4267].
Artificial Neural Networks -Architectures and Applications
Applying Artificial Neural Network Hadron -Hadron Collisions at LHC http://dx.doi.org/10.5772/51273
AcknowledgementsThe authors highly acknowledge and deeply appreciate the supports of the Egyptian Academy of Scientific Research and Technology (ASRT) and the Egyptian Network for High Energy Physics (ENHEP).Author detailsAmr Radi 1<em> and Samy K. Hindawi 2 </em>Address all correspondence to: Amr.radi@cern.ch 1 Department of Physics, Faculty of Sciences, Ain Shams University, Abbassia, Cairo, Egypt / Center of Theoretical Physics at the British University in Egypt (BUE), Egypt 2 Department of Physics, Faculty of Sciences, Ain Shams University, Abbassia, Cairo, Egypt
. J. High Energy Phys. 01792011</p>
<p>. J. High Energy Phys. 081412011</p>
<p>. Phys. Rev. Lett. 105220022010</p>
<p>. Phys. Lett. B. 7072012</p>
<p>. Phys. Lett. B. 2011</p>
<p>. Phys. Lett. B. 6932010</p>
<p>. Eur. Phys. J. C. 2010</p>
<p>. M Jacob, R Slansky, Phys. Rev. 518471972</p>
<p>. R Hwa, Phys. Rev. 117901970</p>
<p>. R Hwa, Phys. Rev. Lett. 2611431971</p>
<p>. R Engel, Z Phys, &amp; , C , Phys. Rev. R. Engel, J. Ranft and S. RoeslerD521995</p>
<p>. L Teodorescu, D Sherwood, Comput. Phys. Commun. 1784092008</p>
<p>. L Teodorescu, IEEE T. Nucl. Sci. 5322212006</p>
<p>. J M Link, Nucl. Instrum. Meth. A. 5515042005</p>
<p>. S El-Bakry, Yaseen, Amr Radi, Int. J. Mod. Phys. C. 183512007</p>
<p>. E El-Dahshan, A Radi, M Y El-Bakry, Int. J. Mod. Phys. C. 18172009</p>
<p>. S Whiteson, D Whiteson, Eng. Appl. Artif. Intel. 2212032009</p>
<p>S Haykin, Artificial Neural Networks -Architectures and Applications. Prentice Hall1999Neural networks a comprehensive foundation</p>
<p>Adaptation in Natural and Artificial Systems. J H Holland, 1975University of Michigan PressAnn Arbor</p>
<p>Genetic Programming: On the Programming of Computers by means of Natural Selection. J R Koza, 1992The MIT PressCambridge, MA</p>
<p>Gene Expression Programming: Mathematical Modeling by an Artificial Intelligence. C Ferreira, 2006Springer-VerlagGermany2nd Edition</p>
<p>Introduction to Evolutionary Algorithms. A E Eiben, J E Smith, 2003SpringerBerlin</p>
<p>Discovery of Neural network learning rules using genetic programming. PHD, the School of computers Sciences. Amr Radi, 2000Birmingham University</p>
<p>High energy physics data analysis with gene expression programming. L Teodorescu, IEEE Nuclear Science Symposium Conference Record. 12005. 2005</p>
<p>Training feedforward networks with the Marquardt algorithm. M T Hagan, M B Menhaj, IEEE Transactions on Neural Networks. 61994</p>
<p>T Back, Evolutionary Algorithms in Theory and Practice. New YorkOxford University Press1996</p>
<p>An Introduction to Simulated Evolutionary Optimization. D B Fogel, IEEE Trans. Neural Networks. 511994</p>
<p>Evolutionary Computation: Comments on the History and Current State. T Back, U Hammel, H P Schwefel, IEEE Trans. Evolutionary Computation. 111997</p>
<p>Adaptation in Natural and Artificial Systems The University of. . H Holland, 1975Michigan PressAnn Arbor, Michigan</p>
<p>Evolutionary Computation: Toward a New Philosophy of Machine Intelligence. D B Fogel, 1995IEEE PressPiscataway, NJ</p>
<p>D E Goldberg, Genetic Algorithm in Search Optimization and Machine Learning. New YorkAddison-Wesley1989</p>
<p>BEAGLE A Darwinian Approach to Pattern Recognition. Richard, Forsyth, 1981Kybernetes10</p>
<p>A representation for the Adaptive Generation of Simple Sequential Programs. Nichael Cramer, Lynn, Proceedings of an International Conference on Genetic Algorithms and the Applications. J John, an International Conference on Genetic Algorithms and the ApplicationsGrefenstette1985CMU</p>
<p>Genetic Programming: On the Programming of Computers by Means of Natural Selection. J R Koza, 1992MIT Press</p>
<p>Genetic Programming II: Automatic Discovery of Reusable Programs. J R Koza, 1994MIT Press</p>
<p>Genetic Programming III: Darwinian Invention and Problem Solving. J R Koza, F H Bennett, D Andre, M A Keane, 1999Morgan Kaufmann</p>
<p>Genetic Programming: An Introduction: On the Automatic Evolution of Computer Programs and its Applications. W Banzhaf, P Nordin, R E Keller, F D Francone, 1998Morgan Kaufmann</p>
<p>An Introduction to Genetic Algorithms. M Mitchell, 1996MIT Press</p>
<p>The Autobiography of Charles Darwin: With original omissions restored, edited with appendix and notes by his grand-daughter. C Darwin, 1959Nora Barlow, Norton</p>
<p>A genetic algorithm tutorial. Darrel Whitley, Statistics and Computing. 41994</p>
<p>A new algorithm for developing dynamic radial basis function neural network models based on genetic algorithms. </p>
<p>. H Sarimveis, A Alexandridis, S Mazarkakis, G Bafas, Computers &amp; Chemical Engineering. 2004</p>
<p>Shifei Ding, Chunyang Su, Artificial Intelligence Review. 2010</p>
<p>G G Yen, H Lu, IEEE Symposium on Combinations of Evolutionary Computation and Neural Networks. 2000</p>
<p>Genetic Algorithm based Selective Neural Network Ensemble. </p>
<p>Z H Zhou, J X Wu, Y Jiang, S F Chen, Proceedings of the 17th International Joint Conference on Artificial Intelligence. the 17th International Joint Conference on Artificial Intelligence2001</p>
<p>Modified backpropagation algorithms for training the multilayer feedforward neural networks with hard-limiting neurons. </p>
<p>Xiangui Yu, N K Loh, G A Jullien, W C Miller, Proceedings of Canadian Conference on Electrical and Computer Engineering. Canadian Conference on Electrical and Computer Engineering1993</p>
<p>. Training Feedforward Neural Networks Using Genetic Algorithms. </p>
<p>David J Montana, Lawrence Davis, Machine Learning. 1989</p>
<p>A J F Van Rooij, L C Jain, R P Johnson, Neural network training using genetic algorithms. SingaporeWorld Scientific1996</p>
<p>Genetic Evolution of the Topology and Weight Distribution of Neural Networks. Vittorio Maniezzo, IEEE Transactions of Neural Networks. 511994</p>
<p>General Asymmetric Neural Networks and Structure Design by Genetic Algorithms. Stefan Bornholdt, Dirk Graudenz, Neural Networks. 51992Pergamon Press</p>
<p>Designing Neural Networks Using Genetic Algorithms with Graph Generation Systems. Hiroaki Kitano, Complex Systems. 1990a</p>
<p>Desired answers do not correspond to good teaching inputs in ecological neural networks. S Nolfi, D Parisi, Neural processing letters. 11994</p>
<p>Learning to adapt to changing environments in evolving neural networks. S Nolfi, D Parisi, Adaptive Behavior. 51996</p>
<p>Learning and evolution in neural networks. S Nolfi, D Parisi, J L Elman, Adaptive Behavior. 311994</p>
<p>Efficient evolution of asymmetric recurrent neural networks using a two-dimensional representation. J C F Pujol, R Poli, Proceedings of the first European workshop on genetic programming (EUROGP). the first European workshop on genetic programming (EUROGP)1998</p>
<p>Designing neural networks using genetic algorithms. G F Miller, P M Todd, S U Hedge, Proceedings of the third international conference on genetic algorithms and their applications. the third international conference on genetic algorithms and their applications1989</p>
<p>Representation and evolution of neural networks. Paper presented at Artificial neural nets and genetic algorithms proceedings of the international conference at Innsbruck. M Mandischer, 1993SpringerAustria; Wien and New York</p>
<p>Evolution of Artificial Neural Networks Using a Two-dimensional Representation. Figueira Pujol, Joao Carlos, 1999UKSchool of Computer Science, University of BirminghamPhD thesis</p>
<p>Evolutionary artificial neural networks. X Yao, Also appearing in Encyclopedia of Library and Information Science. A Kent, J G Williams, New YorkMarcel Dekker Inc1995f33Encyclopedia of Computer Science and Technology</p>            </div>
        </div>

    </div>
</body>
</html>
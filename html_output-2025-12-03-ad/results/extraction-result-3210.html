<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3210 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3210</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3210</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-74.html">extraction-schema-74</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the agent, the memory mechanism, the tasks, and performance comparisons.</div>
                <p><strong>Paper ID:</strong> paper-80980cd10d19f021c14a6b7eee871b6a5d328024</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/80980cd10d19f021c14a6b7eee871b6a5d328024" target="_blank">Augmenting Language Models with Long-Term Memory</a></p>
                <p><strong>Paper Venue:</strong> Neural Information Processing Systems</p>
                <p><strong>Paper TL;DR:</strong> Experiments show that the proposed framework, Language Models Augmented with Long-Term Memory (LongMem), outperforms strong long-context models on ChapterBreak, a challenging long- context modeling benchmark, and achieves remarkable improvements on memory-augmented in-context learning over LLMs.</p>
                <p><strong>Paper Abstract:</strong> Existing large language models (LLMs) can only afford fix-sized inputs due to the input length limit, preventing them from utilizing rich long-context information from past inputs. To address this, we propose a framework, Language Models Augmented with Long-Term Memory (LongMem), which enables LLMs to memorize long history. We design a novel decoupled network architecture with the original backbone LLM frozen as a memory encoder and an adaptive residual side-network as a memory retriever and reader. Such a decoupled memory design can easily cache and update long-term past contexts for memory retrieval without suffering from memory staleness. Enhanced with memory-augmented adaptation training, LongMem can thus memorize long past context and use long-term memory for language modeling. The proposed memory retrieval module can handle unlimited-length context in its memory bank to benefit various downstream tasks. Typically, LongMem can enlarge the long-form memory to 65k tokens and thus cache many-shot extra demonstration examples as long-form memory for in-context learning. Experiments show that our method outperforms strong long-context models on ChapterBreak, a challenging long-context modeling benchmark, and achieves remarkable improvements on memory-augmented in-context learning over LLMs. The results demonstrate that the proposed method is effective in helping language models to memorize and utilize long-form contents. Our code is open-sourced at https://aka.ms/LongMem.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3210.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3210.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the agent, the memory mechanism, the tasks, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LongMEM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Language Models Augmented with Long-Term Memory (LongMEM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A decoupled memory-augmented LLM that freezes a pretrained backbone as a memory encoder and trains a lightweight residual SideNet to retrieve and fuse cached attention key/value pairs, enabling long-term memory (up to 65k tokens) for language modeling and many-shot in-context learning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>LongMEM</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Architecture: frozen backbone Transformer LLM (reproduced GPT-2* used in experiments) that encodes current and past inputs; a trainable residual SideNet (fewer layers) acts as memory retriever/reader; a non-differentiable Cache Memory Bank stores attention key/value pairs (from backbone m-th layer). Retrieval is token-to-chunk via a FAISS index; retrieved key/value pairs are fused in a special memory-augmented SideNet layer via joint-attention with a trainable gating vector. Cross-network residual connections inject differences of backbone hidden states into SideNet to transfer knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>retrieval-augmented long-term memory (cached attention key/value bank)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism_description</strong></td>
                            <td>During backbone forward passes, attention keys and values from a chosen decoder layer (18th in experiments) are cached into a per-GPU Cache Memory Bank (head-wise key/value queue) at token granularity but indexed and retrieved at chunk granularity (chunk-size csz, mean-pooled). Retrieval: for each current token, compute dot-product between query and chunk keys (FAISS exact search), retrieve top-K/csz chunks, squeeze to K token-level key/value pairs (K=64 in experiments). Fusion: SideNet's memory-augmented layer computes local attention A and memory attention M and combines them per-head with sigmoid gating: H = sigmoid(g)*A + (1-sigmoid(g))*M. Memory is updated by appending current sequences and removing oldest to preserve sequence-level causality. Decoupled design (frozen backbone, trainable SideNet) avoids memory staleness.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Long-text language modeling (PG-22, ArXiv), ChapterBreak suffix identification, Memory-augmented in-context learning (SST-2, MR, Subj, SST-5, MPQA, SQuAD)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Long-text language modeling: predict tokens over book/paper-length documents (challenge: long-range dependency modeling). ChapterBreak: suffix identification given long prefix (choose correct next-chapter segment among hard negatives). Memory-augmented in-context learning: attend to thousands of demonstration examples loaded into memory to improve few/ many-shot NLU and QA tasks (challenge: unlimited demonstration examples beyond context window).</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>language modeling; suffix identification (long-context understanding); in-context learning / question answering</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>PG-22 perplexities (token-level PPL): 5K-10K: 21.29 PPL; 10K-100K: 23.01 PPL; 100K-500K: 22.55 PPL; 500K-1M: 23.35 PPL; >1M: 16.71 PPL; ArXiv: 10.05 PPL. ChapterBreak (AO3) zero-shot suffix identification accuracy: ctx-4k: 37.7%; ctx-6k: 39.4%; ctx-8k: 40.5%. Memory-augmented ICL (with 2000 examples loaded in memory): 4-shot average acc: 58.4% (avg over 5 NLU datasets); 20-shot average acc: 66.7%. Reported SQuAD open-ended EM improvement: +4.5 EM (compared to baselines).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>No-memory baseline (reproduced GPT-2*): PG-22 PPLs: 5K-10K: 22.78; 10K-100K: 24.39; 100K-500K: 24.12; 500K-1M: 24.97; >1M: 18.07; ArXiv: 11.05. ChapterBreak accuracy (GPT-2*): 18.4% (ctx-4k/6k/8k). ICL (no in-memory demos): 4-shot avg: 55.6%; 20-shot avg: 58.7%.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Decoupled long-term memory via cached attention key/values + SideNet yields consistent improvements over both a no-memory baseline (GPT-2*) and a coupled memory baseline (MemTRM): reduces perplexity by ~1.38–1.62 on PG-22 splits and ~1.0 on ArXiv; achieves SOTA 40.5% accuracy on ChapterBreak (AO3) surpassing MemTRM and much larger GPT-3; substantially improves many-shot in-context learning (20-shot average +8.0 points over GPT-2* and MemTRM when 2000 demos are loaded into memory).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Requires design/tuning of chunk-size and memory size to downstream average document length; retrieval latency and resource cost (reported retrieval ~15 ms per 1k tokens, ~55% of backbone forward time); engineering constraints for batchfying (must disable global shuffling and preserve sequence-level causality); memory/index maintenance per GPU; reliance on FAISS exact index (scales with memory) and possibility to trade accuracy for speed via approximate search; training still requires memory-augmented adaptation (26B tokens in experiments) though backbone is frozen to avoid catastrophic forgetting.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Augmenting Language Models with Long-Term Memory', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3210.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3210.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the agent, the memory mechanism, the tasks, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MemTRM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Memorizing Transformer (MemTRM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A memory-augmented Transformer that approximates sparse in-context attention by retrieving memorized key/value representations from a non-differentiable external memory and attending over them; used here as a baseline for long-term memory augmentation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Memorizing transformers.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>MemTRM</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Original MemTRM caches attention key/value vectors into an external non-differentiable memory and performs dense attention over both local context tokens and retrieved memorized tokens (kNN retrieval). In the paper's reproduction the knn-augmented layer is inserted at layer 18 of the decoder and trained under the same memory-augmented adaptation regime.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>retrieval-augmented external memory (cached attention key/value pairs)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism_description</strong></td>
                            <td>Caches attention key/value pairs produced by the model and retrieves relevant memorized tokens at inference/training time via kNN in the key space; retrieved values are fused into the model's attention computation to approximate large-context attention. In the original design encoding and fusion are performed by the same model (coupled design).</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Long-text language modeling (PG-22, ArXiv), ChapterBreak suffix identification, Memory-augmented in-context learning (SST-2, MR, Subj, SST-5, MPQA)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same set of long-context modeling and in-context learning tasks as evaluated for LongMEM, used to test ability of the memory mechanism to supply long-range context / many demonstrations.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>language modeling; suffix identification; in-context learning</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>PG-22 perplexities: 5K-10K: 21.77 PPL; 10K-100K: 23.56 PPL; 100K-500K: 23.23 PPL; 500K-1M: 24.16 PPL; >1M: 17.39 PPL; ArXiv: 10.81 PPL. ChapterBreak (AO3) suffix identification accuracies: ctx-4k: 28.3%; ctx-6k: 28.7%; ctx-8k: 28.7%. Memory-augmented ICL (2000 in-memory demos): 4-shot average acc: 55.6%; 20-shot average acc: 58.6%.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>No-memory baseline (GPT-2*): PG-22 PPLs and ChapterBreak/ICL numbers as reported for GPT-2* (see GPT-2* entry).</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>MemTRM provides improvements over no-memory baselines by approximating long-range attention through retrieval, but in this paper LongMEM's decoupled memory and SideNet design outperform MemTRM across language modeling, ChapterBreak, and in-context learning tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Coupled memory design can suffer from memory staleness: cached older representations may become mismatched as model parameters update; adapting the entire LLM with memory augmentations is computationally inefficient and can lead to catastrophic forgetting (issues highlighted by the authors and motivating LongMEM's decoupled design).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Augmenting Language Models with Long-Term Memory', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3210.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3210.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the agent, the memory mechanism, the tasks, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-2*</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reproduced GPT-2 (GPT-2*)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A reproduced GPT-2 backbone model (407M params) with Alibi positional embeddings used as the frozen encoder baseline; it has a fixed input length (1k) and no external long-term memory.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Language models are unsupervised multitask learners</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>GPT-2*</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>24-layer autoregressive Transformer with 16 heads and 1024 hidden dimension (reproduced GPT-2* used as backbone). Alibi positional embeddings are used to enable better extrapolation to longer contexts. In experiments GPT-2* is used as the frozen encoding backbone and as the no-memory baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>context window only (fixed-length input)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism_description</strong></td>
                            <td>Standard Transformer self-attention limited to a fixed in-context window (1k tokens in experiments); no external cache of past attention key/value pairs for long-term retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Long-text language modeling (PG-22, ArXiv), ChapterBreak suffix identification, In-context learning (SST-2, MR, Subj, SST-5, MPQA, SQuAD)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Baseline evaluations: modeling/predicting over long documents within the fixed context window; suffix identification given long prefix truncated to the model's max input; few-shot in-context learning using only local demonstrations within the input window.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>language modeling; suffix identification; in-context learning / QA</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>PG-22 perplexities: 5K-10K: 22.78 PPL; 10K-100K: 24.39 PPL; 100K-500K: 24.12 PPL; 500K-1M: 24.97 PPL; >1M: 18.07 PPL; ArXiv: 11.05 PPL. ChapterBreak (AO3) zero-shot accuracy: 18.4% (ctx-4k/6k/8k). ICL: 4-shot avg acc: 55.6%; 20-shot avg acc: 58.7%.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Standard fixed-window Transformers are limited by input length and cannot utilize long-form past context; memory-augmented methods (LongMEM, MemTRM) provide measurable improvements over GPT-2* across long-context language modeling, suffix identification, and many-shot in-context learning.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Fixed context window (1k tokens) prevents access to long-range context and limits number of in-context demonstration examples; naive scaling of input length is computationally expensive due to quadratic self-attention.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Augmenting Language Models with Long-Term Memory', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Memorizing transformers. <em>(Rating: 2)</em></li>
                <li>Improving language models by retrieving from trillions of tokens <em>(Rating: 2)</em></li>
                <li>Transformer-xl: Attentive language models beyond a fixed-length context <em>(Rating: 2)</em></li>
                <li>Compressive transformers for long-range sequence modelling <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3210",
    "paper_id": "paper-80980cd10d19f021c14a6b7eee871b6a5d328024",
    "extraction_schema_id": "extraction-schema-74",
    "extracted_data": [
        {
            "name_short": "LongMEM",
            "name_full": "Language Models Augmented with Long-Term Memory (LongMEM)",
            "brief_description": "A decoupled memory-augmented LLM that freezes a pretrained backbone as a memory encoder and trains a lightweight residual SideNet to retrieve and fuse cached attention key/value pairs, enabling long-term memory (up to 65k tokens) for language modeling and many-shot in-context learning.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "LongMEM",
            "agent_description": "Architecture: frozen backbone Transformer LLM (reproduced GPT-2* used in experiments) that encodes current and past inputs; a trainable residual SideNet (fewer layers) acts as memory retriever/reader; a non-differentiable Cache Memory Bank stores attention key/value pairs (from backbone m-th layer). Retrieval is token-to-chunk via a FAISS index; retrieved key/value pairs are fused in a special memory-augmented SideNet layer via joint-attention with a trainable gating vector. Cross-network residual connections inject differences of backbone hidden states into SideNet to transfer knowledge.",
            "memory_used": true,
            "memory_type": "retrieval-augmented long-term memory (cached attention key/value bank)",
            "memory_mechanism_description": "During backbone forward passes, attention keys and values from a chosen decoder layer (18th in experiments) are cached into a per-GPU Cache Memory Bank (head-wise key/value queue) at token granularity but indexed and retrieved at chunk granularity (chunk-size csz, mean-pooled). Retrieval: for each current token, compute dot-product between query and chunk keys (FAISS exact search), retrieve top-K/csz chunks, squeeze to K token-level key/value pairs (K=64 in experiments). Fusion: SideNet's memory-augmented layer computes local attention A and memory attention M and combines them per-head with sigmoid gating: H = sigmoid(g)*A + (1-sigmoid(g))*M. Memory is updated by appending current sequences and removing oldest to preserve sequence-level causality. Decoupled design (frozen backbone, trainable SideNet) avoids memory staleness.",
            "task_name": "Long-text language modeling (PG-22, ArXiv), ChapterBreak suffix identification, Memory-augmented in-context learning (SST-2, MR, Subj, SST-5, MPQA, SQuAD)",
            "task_description": "Long-text language modeling: predict tokens over book/paper-length documents (challenge: long-range dependency modeling). ChapterBreak: suffix identification given long prefix (choose correct next-chapter segment among hard negatives). Memory-augmented in-context learning: attend to thousands of demonstration examples loaded into memory to improve few/ many-shot NLU and QA tasks (challenge: unlimited demonstration examples beyond context window).",
            "task_type": "language modeling; suffix identification (long-context understanding); in-context learning / question answering",
            "performance_with_memory": "PG-22 perplexities (token-level PPL): 5K-10K: 21.29 PPL; 10K-100K: 23.01 PPL; 100K-500K: 22.55 PPL; 500K-1M: 23.35 PPL; &gt;1M: 16.71 PPL; ArXiv: 10.05 PPL. ChapterBreak (AO3) zero-shot suffix identification accuracy: ctx-4k: 37.7%; ctx-6k: 39.4%; ctx-8k: 40.5%. Memory-augmented ICL (with 2000 examples loaded in memory): 4-shot average acc: 58.4% (avg over 5 NLU datasets); 20-shot average acc: 66.7%. Reported SQuAD open-ended EM improvement: +4.5 EM (compared to baselines).",
            "performance_without_memory": "No-memory baseline (reproduced GPT-2*): PG-22 PPLs: 5K-10K: 22.78; 10K-100K: 24.39; 100K-500K: 24.12; 500K-1M: 24.97; &gt;1M: 18.07; ArXiv: 11.05. ChapterBreak accuracy (GPT-2*): 18.4% (ctx-4k/6k/8k). ICL (no in-memory demos): 4-shot avg: 55.6%; 20-shot avg: 58.7%.",
            "has_performance_comparison": true,
            "key_findings": "Decoupled long-term memory via cached attention key/values + SideNet yields consistent improvements over both a no-memory baseline (GPT-2*) and a coupled memory baseline (MemTRM): reduces perplexity by ~1.38–1.62 on PG-22 splits and ~1.0 on ArXiv; achieves SOTA 40.5% accuracy on ChapterBreak (AO3) surpassing MemTRM and much larger GPT-3; substantially improves many-shot in-context learning (20-shot average +8.0 points over GPT-2* and MemTRM when 2000 demos are loaded into memory).",
            "limitations_or_challenges": "Requires design/tuning of chunk-size and memory size to downstream average document length; retrieval latency and resource cost (reported retrieval ~15 ms per 1k tokens, ~55% of backbone forward time); engineering constraints for batchfying (must disable global shuffling and preserve sequence-level causality); memory/index maintenance per GPU; reliance on FAISS exact index (scales with memory) and possibility to trade accuracy for speed via approximate search; training still requires memory-augmented adaptation (26B tokens in experiments) though backbone is frozen to avoid catastrophic forgetting.",
            "uuid": "e3210.0",
            "source_info": {
                "paper_title": "Augmenting Language Models with Long-Term Memory",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "MemTRM",
            "name_full": "Memorizing Transformer (MemTRM)",
            "brief_description": "A memory-augmented Transformer that approximates sparse in-context attention by retrieving memorized key/value representations from a non-differentiable external memory and attending over them; used here as a baseline for long-term memory augmentation.",
            "citation_title": "Memorizing transformers.",
            "mention_or_use": "use",
            "agent_name": "MemTRM",
            "agent_description": "Original MemTRM caches attention key/value vectors into an external non-differentiable memory and performs dense attention over both local context tokens and retrieved memorized tokens (kNN retrieval). In the paper's reproduction the knn-augmented layer is inserted at layer 18 of the decoder and trained under the same memory-augmented adaptation regime.",
            "memory_used": true,
            "memory_type": "retrieval-augmented external memory (cached attention key/value pairs)",
            "memory_mechanism_description": "Caches attention key/value pairs produced by the model and retrieves relevant memorized tokens at inference/training time via kNN in the key space; retrieved values are fused into the model's attention computation to approximate large-context attention. In the original design encoding and fusion are performed by the same model (coupled design).",
            "task_name": "Long-text language modeling (PG-22, ArXiv), ChapterBreak suffix identification, Memory-augmented in-context learning (SST-2, MR, Subj, SST-5, MPQA)",
            "task_description": "Same set of long-context modeling and in-context learning tasks as evaluated for LongMEM, used to test ability of the memory mechanism to supply long-range context / many demonstrations.",
            "task_type": "language modeling; suffix identification; in-context learning",
            "performance_with_memory": "PG-22 perplexities: 5K-10K: 21.77 PPL; 10K-100K: 23.56 PPL; 100K-500K: 23.23 PPL; 500K-1M: 24.16 PPL; &gt;1M: 17.39 PPL; ArXiv: 10.81 PPL. ChapterBreak (AO3) suffix identification accuracies: ctx-4k: 28.3%; ctx-6k: 28.7%; ctx-8k: 28.7%. Memory-augmented ICL (2000 in-memory demos): 4-shot average acc: 55.6%; 20-shot average acc: 58.6%.",
            "performance_without_memory": "No-memory baseline (GPT-2*): PG-22 PPLs and ChapterBreak/ICL numbers as reported for GPT-2* (see GPT-2* entry).",
            "has_performance_comparison": true,
            "key_findings": "MemTRM provides improvements over no-memory baselines by approximating long-range attention through retrieval, but in this paper LongMEM's decoupled memory and SideNet design outperform MemTRM across language modeling, ChapterBreak, and in-context learning tasks.",
            "limitations_or_challenges": "Coupled memory design can suffer from memory staleness: cached older representations may become mismatched as model parameters update; adapting the entire LLM with memory augmentations is computationally inefficient and can lead to catastrophic forgetting (issues highlighted by the authors and motivating LongMEM's decoupled design).",
            "uuid": "e3210.1",
            "source_info": {
                "paper_title": "Augmenting Language Models with Long-Term Memory",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "GPT-2*",
            "name_full": "Reproduced GPT-2 (GPT-2*)",
            "brief_description": "A reproduced GPT-2 backbone model (407M params) with Alibi positional embeddings used as the frozen encoder baseline; it has a fixed input length (1k) and no external long-term memory.",
            "citation_title": "Language models are unsupervised multitask learners",
            "mention_or_use": "use",
            "agent_name": "GPT-2*",
            "agent_description": "24-layer autoregressive Transformer with 16 heads and 1024 hidden dimension (reproduced GPT-2* used as backbone). Alibi positional embeddings are used to enable better extrapolation to longer contexts. In experiments GPT-2* is used as the frozen encoding backbone and as the no-memory baseline.",
            "memory_used": false,
            "memory_type": "context window only (fixed-length input)",
            "memory_mechanism_description": "Standard Transformer self-attention limited to a fixed in-context window (1k tokens in experiments); no external cache of past attention key/value pairs for long-term retrieval.",
            "task_name": "Long-text language modeling (PG-22, ArXiv), ChapterBreak suffix identification, In-context learning (SST-2, MR, Subj, SST-5, MPQA, SQuAD)",
            "task_description": "Baseline evaluations: modeling/predicting over long documents within the fixed context window; suffix identification given long prefix truncated to the model's max input; few-shot in-context learning using only local demonstrations within the input window.",
            "task_type": "language modeling; suffix identification; in-context learning / QA",
            "performance_with_memory": null,
            "performance_without_memory": "PG-22 perplexities: 5K-10K: 22.78 PPL; 10K-100K: 24.39 PPL; 100K-500K: 24.12 PPL; 500K-1M: 24.97 PPL; &gt;1M: 18.07 PPL; ArXiv: 11.05 PPL. ChapterBreak (AO3) zero-shot accuracy: 18.4% (ctx-4k/6k/8k). ICL: 4-shot avg acc: 55.6%; 20-shot avg acc: 58.7%.",
            "has_performance_comparison": true,
            "key_findings": "Standard fixed-window Transformers are limited by input length and cannot utilize long-form past context; memory-augmented methods (LongMEM, MemTRM) provide measurable improvements over GPT-2* across long-context language modeling, suffix identification, and many-shot in-context learning.",
            "limitations_or_challenges": "Fixed context window (1k tokens) prevents access to long-range context and limits number of in-context demonstration examples; naive scaling of input length is computationally expensive due to quadratic self-attention.",
            "uuid": "e3210.2",
            "source_info": {
                "paper_title": "Augmenting Language Models with Long-Term Memory",
                "publication_date_yy_mm": "2023-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Memorizing transformers.",
            "rating": 2
        },
        {
            "paper_title": "Improving language models by retrieving from trillions of tokens",
            "rating": 2
        },
        {
            "paper_title": "Transformer-xl: Attentive language models beyond a fixed-length context",
            "rating": 2
        },
        {
            "paper_title": "Compressive transformers for long-range sequence modelling",
            "rating": 1
        }
    ],
    "cost": 0.015281,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Augmenting Language Models with Long-Term Memory</h1>
<p>Weizhi Wang ${ }^{\dagger}$, Li Dong ${ }^{\ddagger}$, Hao Cheng ${ }^{\ddagger}$, Xiaodong Liu ${ }^{\ddagger}$, Xifeng Yan ${ }^{\dagger}$, Jianfeng Gao ${ }^{\ddagger}$, Furu Wei ${ }^{\S}$<br>${ }^{\dagger}$ University of California, Santa Barbara ${ }^{\ddagger}$ Microsoft Research<br>weizhiwang@ucsb.edu, {lidong1, haocheng}@microsoft.com</p>
<h4>Abstract</h4>
<p>Existing large language models (LLMs) can only afford fix-sized inputs due to the input length limit, preventing them from utilizing rich long-context information from past inputs. To address this, we propose a framework, Language Models Augmented with Long-Term Memory (LongMEM), which enables LLMs to memorize long history. We design a novel decoupled network architecture with the original backbone LLM frozen as a memory encoder and an adaptive residual side-network as a memory retriever and reader. Such a decoupled memory design can easily cache and update long-term past contexts for memory retrieval without suffering from memory staleness. Enhanced with memory-augmented adaptation training, LongMEM can thus memorize long past context and use long-term memory for language modeling. The proposed memory retrieval module can handle unlimited-length context in its memory bank to benefit various downstream tasks. Typically, LONGMEM can enlarge the long-form memory to 65 k tokens and thus cache many-shot extra demonstration examples as long-form memory for in-context learning. Experiments show that our method outperforms strong longcontext models on ChapterBreak, a challenging long-context modeling benchmark, and achieves remarkable improvements on memory-augmented in-context learning over LLMs. The results demonstrate that the proposed method is effective in helping language models to memorize and utilize long-form contents. Our code is open-sourced at https://aka.ms/LongMem.</p>
<h2>1 Introduction</h2>
<p>Large language models (LLMs) have revolutionized natural language processing with great successes in advancing the state-of-the-art on various understanding and generation tasks [DCLT19, RWC ${ }^{+} 19$, $\mathrm{LOG}^{+} 19, \mathrm{YDY}^{+} 19, \mathrm{BMR}^{+} 20, \mathrm{RSR}^{+} 20$ ]. Most LLMs benefit from self-supervised training over large corpora via harvesting knowledge from fix-sized local context, showing emergent abilities, e.g., zero-shot prompting [ $\mathrm{RWC}^{+} 19$ ], in-context learning [ $\mathrm{BMR}^{+} 20$ ], and Chain-of-Thought (CoT) reasoning [ $\mathrm{WWS}^{+} 22$ ]. Nevertheless, the input length limit of existing LLMs prevents them from generalizing to real-world scenarios where the capability of processing long-form information beyond a fix-sized session is critical, e.g., long horizontal planning.
To address the length limit issue, the most straightforward method is to simply scale up the input context length. For instance, GPT-3 [BMR ${ }^{+} 20$ ] increases the input length from 1 k of GPT-2 [RWC $^{+} 19$ ] to 2 k tokens for capturing better long-range dependencies. However, this approach typically incurs computation-intensive training from scratch and the in-context dense attention is still heavily constrained by the quadratic computation complexity of Transformer self-attention [VSP ${ }^{+} 17$ ]. Another recent line of work [BPC20, ZGD $^{+} 20$ ] instead focuses on developing in-context sparse attention to avoid the quadratic cost of self-attention, which still largely requires training from scratch. In contrast, the prominent work, Memorizing Transformer (MemTRM) [WRHS22], approximates in-context</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Overview of the memory caching and retrieval flow of LONGMEM. The long text sequence is split into fix-length segments, then each segment is forwarded through large language models and the attention key and value vectors of $m$-th layer are cached into the long-term memory bank. For future inputs, via attention query-key based retrieval, the top- $k$ attention key-value pairs of long-term memory are retrieved and fused into language modeling.
sparse attention via dense attention over both in-context tokens and memorized tokens retrieved from a non-differentiable memory for Transformers. Thus, MemTRM scales up the resulting language model to handle up to 65 k tokens and achieves substantial perplexity gains in modeling full-length books or long papers. However, MemTRM faces the memory staleness challenge during training due to its coupled memory design, which uses a single model for encoding memory and fusing memory for language modeling. In other words, as the model parameters are updated, cached older representations in memory may have distributional shifts from those from the latest model, thereby limiting the effectiveness of the memory augmentation.
In this paper, we propose a framework for Language Models Augmented with Long-Term Memory (LONGMEM), which enables language models to cache long-form previous context or knowledge into the non-differentiable memory bank, and further take advantage of them via a decoupled memory module to address the memory staleness problem. To achieve decoupled memory, we design a novel residual side-network (SideNet). Paired attention keys and values of the previous context are extracted using a frozen backbone LLM into the memory bank. In the memory-augmented layer of the SideNet, the generated attention query of the current input is used to retrieve cached (keys, values) of previous contexts from the memory, and the corresponding memory augmentations are then fused into learned hidden states via a joint-attention mechanism. Furthermore, newly designed cross-network residual connections between the SideNet and the frozen backbone LLM enable better knowledge transfer from the pretrained backbone LLM. By continually training the residual SideNet to retrieve and fuse memory-augmented long-context, the pre-trained LLM can be adapted to leverage long-contextual memory for improved modeling. The detailed memory cache, retrieval and fusion process is illustrated in Figure 1.
Our decoupled memory design leads to two main benefits. First, our proposed architecture decouples the process of encoding previous inputs into memory and the process of memory retrieval and fusion by decoupled frozen backbone LLM and SideNet. In this way, the backbone LLM only works as the long-context knowledge encoder, while the residual SideNet works as the memory retriever and reader, which effectively resolves the issue of memory staleness. Second, directly adapting the entire LLM with memory augmentations is computationally inefficient, and also suffers from catastrophic forgetting. As the backbone LLM is frozen during the efficient memory-augmented adaptation stage, LONGMEM can not only tap into the pretrained knowledge but also avoid catastrophic forgetting.
LONGMEM is capable of taking various types of long-form text and knowledge into the memory bank based on downstream tasks. Here, we consider two representative cases, language modeling with full-length book contexts, and memory-augmented in-context learning with thousands of task-relevant demonstration examples. Specifically, we evaluate the effectiveness of the proposed LONGMEM on various long-text language modeling, and memory-augmented in-context learning for language understanding. Experimental results demonstrate that our model consistently outperforms the strong baselines in terms of long-text modeling and in-context learning abilities. Our method substantially</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Overview of LongMEM architecture. "MemAug" represents Memory-Augmented Layer.
improves LLM's long-context language modeling capabilities by $-1.38 \sim-1.62$ perplexity over different length splits of Gutenberg-2022 corpus. Remarkably, our model achieves the state-of-the-art performance of 40.5\% identification accuracy on ChapterBreak, a challenging long-context modeling benchmark, significantly surpassing existing strong x-former baselines. Lastly, with 2 k demonstration examples in memory, LongMEM shows pronounced in-context learning improvements on popular NLU tasks, compared with MemTRM and non-memory-augmented baselines.</p>
<h1>2 Methods</h1>
<p>To enable LLMs to harvest relevant information from the past long context in memory, we propose to augment the frozen backbone LLM with a decoupled memory module. To fuse the memory context information, we design a novel lightweight residual SideNet, which can be continually trained in an efficient way. In the following, we first discuss the problem formulation of language modeling with memory augmentations. Then, we formally introduce our efficient residual SideNet for adapting the frozen pretrained LLM to jointly attend over local input context and retrieved memory context. Lastly, we provide our designed processes of how past memory is encoded, stored, recalled and fused for language modeling.</p>
<h3>2.1 Language Models Augmented with Long-Term Memory</h3>
<p>Here, we focus on the high-level problem setup and defer more component details to later sections. Given its wide adoption for pretrained LLMs, our LongMEM model is built on the Transformer architecture [VSP ${ }^{+} 17$ ]. For LongMEM, there are three key components: the frozen backbone LLM, SideNet, and Cache Memory Bank. As most existing pretrained LLMs can only take a fix-sized input, only the input segment of a long sequence (e.g., a book) that can fit in the length limit is denoted as the current input as done for most existing autoregressive language models. Those previous segments that can not fit are denoted as previous inputs, which are used for memory augmentations. To tap into the learned knowledge of the pretrained LLM, both previous and current inputs are encoded using the frozen backbone LLM but different representations are extracted. For previous inputs, the key-value pairs from the Transformer self-attention at $m$-th layer are stored in Cache Memory Bank, whereas the hidden states from each LLM decoder layer for the current inputs are retained and transferred to SideNet. For each current input token, top relevant key-value vector pairs are retrieved as memory augmentations for language modeling. The SideNet module can be viewed as an efficient adaption model that is trained to fuse the current input context and relevant cached previous contexts in the decoupled memory.</p>
<p>Formally, for a fix-sized input text sequence $\left{\mathbf{x}<em i="1">{i}\right}</em>}^{|x|}$ (the current input), LongMEM first performs a forward pass using the backbone LLM (marked in Blue in Figure 2) without any gradient calculation. The embedding layer of the backbone LLM first encodes the input $\left{\mathbf{x<em i="1">{i}\right}</em>$, where $E$ is the hidden dimension. Then each successive Transformer decoder layer of the frozen backbone LLM computes the new hidden states}^{|x|}$ into embedding space and outputs the initial hidden states, $\mathbf{H}_{\mathrm{LLM}}^{0} \in \mathbb{R}^{|x| \times E</p>
<p>using the hidden states from the previous layer, $\mathbf{H}<em _theta__mathrm_LLM="\theta_{\mathrm{LLM">{\mathrm{LLM}}^{l^{\prime}}=f</em>$ is the total # layers for the backbone LLM. During the forward pass with the backbone LLM for all previous inputs, the key-value pairs used for self-attention at the $m$-th Transformer decoder layer are stored in Cached Memory Bank (marked in Orange in Upper-Left corner of Figure2), which are later recalled as memory augmentations for future inputs.}}^{l^{\prime}}}(\mathbf{H}_{\mathrm{LLM}}^{l^{\prime}-1}),\forall l^{\prime}\in\left[1, L^{\prime}\right]$ and $L^{\prime</p>
<p>Cached Memory Bank is a cached head-wise vector queue $\mathcal{Z}<em v="v">{k}, \mathcal{Z}</em>$, where $H, d$ denotes the number of attention heads and per-head dimension respectively. After memory retrieval and fusion (§2.3), the memory bank removes the key-value pairs of the oldest sequences and appends the current sequences to the cached vector bank. Thus such an update mechanism ensures the language modeling causality at the sequences level and enables the memory bank to always keep records of the nearest previous context for the current inputs.} \in \mathbb{R}^{H \times M \times d}$, which maintains attention key-value pairs of latest $M$ previous inputs $\widetilde{\mathbf{K}}, \widetilde{\mathbf{V}} \in \mathbb{R}^{H \times|x| \times d</p>
<p>After the forward pass with the backbone LLM, the SideNet module then takes all current input hidden states from the backbone LLM $\left{\mathbf{H}<em l_prime="l^{\prime">{\mathrm{LLM}}^{l^{\prime}}\right}</em>$ into memory-augmented contextual representation via $(L-1)$ normal Transformer decoder layers and a special memory-augmented layer.}=1}^{L^{\prime}}$ and the past key-value pairs in Cached Memory Bank for computing memory-augmented representations. Specifically, our SideNet of LONGMEM consists of $(L-1)$ normal Transformer decoder layers and one special memory-augmented decoder layer. For efficient purposes, we mainly consider the case where #layers $L$ of the SideNet is smaller than that of the backbone LLM, i.e., $L&lt;L^{\prime}$. Our SideNet encodes $\mathbf{H}^{0</p>
<p>The memory-augmented layer is an extension of the vanilla Transformer decoder layer that takes a memory-augmented input, including both top relevant key-value pairs in memory and the hidden states from the current input. Here, the cached key-value pairs are recalled using a token-based memory retrieval module (§2.3). For each current input token, the memory retrieval module $s_{r t}(:)$ retrieves top- $K$ relevant key-value pairs in the memory bank $\left{\widetilde{\mathbf{k}}<em i="i" j="j">{i j}, \widetilde{\mathbf{v}}</em>\right}<em r="r" t="t">{j=1}^{K}=s</em>}\left(\mathbf{x<em _Side="{Side" _text="\text">{i}\right)$. Then SideNet computes the output using the memory-augmented input, $\mathbf{H}</em>}}^{m_{s}}=f_{\theta_{\text {Max }}}\left(\mathbf{H<em s="s">{\text {Side }}^{m</em>}-1},\left{\left{\widetilde{\mathbf{k}<em i="i" j="j">{i j}, \widetilde{\mathbf{v}}</em>\right}<em i="1">{j=1}^{K}\right}</em>$ is the layer index where we inject the memory-augmentation layer.
Finally, the token probability is computed using the last SideNet hidden states $P\left(\mathbf{x}}^{|x|}\right)$, where $m_{s<em 1="1">{i} \mid \mathbf{x}</em>}, \cdots, \mathbf{x<em _in="\in" _mathcal_D="\mathcal{D" x="x">{i-1}\right)=$ $\operatorname{softmax}\left(W \mathbf{H}^{L}\right)$, where $W$ is the frozen output embedding weight shared by both the backbone LLM and SideNet. We perform a memory-augmented adaptation training for LONGMEM to utilize the decoupled memory. Following the generative unsupervised pre-training [RNSS18], the training objective of LONGMEM is the standard left-to-right language modeling objective, which maximizes the likelihood of the next token based on the left context: $\max \sum</em>}} \sum_{i=1}^{|\mathbf{x}|} \log P\left(\mathbf{x<em 1="1">{i} \mid \mathbf{x}</em>$.}, \cdots, \mathbf{x}_{i-1}\right)$, where $x$ is a randomly sampled sentence from the pre-training text corpus $\mathcal{D</p>
<h1>2.2 Residual SideNet</h1>
<p>SideNet Architecture and Initialization. Here, we again implement SideNet based on Transformer [VSP ${ }^{+} 17$ ]. Here, the number of decoder layers $L$ in SideNet is equal to the number of layers $L^{\prime}$ in the backbone LLM divided by a reduction factor (a layer reduction factor of 2 throughout this work $L^{\prime}=2 L$ ). The weights of each decoder layer in SideNet are initialized from the corresponding pre-trained decoder layer of the backbone LLM with the same depth: $\Theta_{\text {Side }}^{N}=\Theta_{\mathrm{LLM}}^{l}$. As illustrated in Figure 2, the SideNet takes the output of backbone LLM's embedding layer and reuses the language modeling head layer of backbone LLM, which is also frozen during the continual adaption stage. During the memory-augmented adaptation stage, all other parameters of SideNet are updated accordingly based on the training signal. In this way, the lightweight SideNet achieves fast convergence with knowledge transferred from pre-trained parameters.</p>
<p>Cross-Network Residual Connections. To tap into knowledge from the pretrained backbone LLM, we resort to proposed cross-network residual connections for fusing representations from the backbone LLM into SideNet. Specifically, we add the difference between output hidden states at $2 l$-th and $(2 l-2)$-th layers of the backbone LLM as the residual connections to the output hidden states at $l$-th layer of SideNet. Then, the input to the next $(l+1)$-th layer of SideNet is the sum of the original hidden state forwarded through the previous layer $f_{\Theta_{\text {Side }}^{l}}\left(\mathbf{H}_{\text {Side }}^{l-1}\right)$ and the cross-network</p>
<p>residual connection of the hidden state difference from the backbone LLM</p>
<p>$\mathbf{H}<em _Theta__text_Side="\Theta_{\text{Side">{\text{Side}}^{l}=f</em>}}^{l}}(\mathbf{H<em _text_LLM="\text{LLM">{\text{Side}}^{l-1})+(\mathbf{H}</em>),\forall l\in[1,L],$ (1)}}^{2l}-\mathbf{H}_{\text{LLM}}^{2l-2</p>
<p>where $\mathbf{H}^{0}$ is the output of embedding layer. It is worth noting that the residual connections after the self-attention and feed-forward network of a decoder layer [VSP+17] will be performed as normal in $f_{\Theta_{\text{Side}}^{l}}(\mathbf{H}_{\text{Side}}^{l-1})$ and parallel to the proposed cross-network residual connections.</p>
<h1>2.3 Memory Retrieval and Fusion</h1>
<p>The long-term memory capability of LONGMEM is achieved via a memory-augmentation module for retrieval and fusion.</p>
<p>Token-to-Chunk Memory Retrieval. Instead of performing token-to-token retrieval, we focus on token-to-chunk retrieval for acceleration and integrity. A text-chunk refers to an n-gram structure of chunk-size $c s z$ number of contiguous tokens. The memory bank stores cached key-value pairs at the level of token chunks. We divide the memory bank into $M / c s z$ attention key-value paired chunks and use the mean-pooled vector on the chunk-size dimension to get the key vector for retrieval. Then we retrieve the top- $(K / c s z)$ attention key-value chunks w.r.t the dot product between the attention query of the current input token and the mean-pooled attention key of a candidate chunk. Finally, we squeeze the chunk-size dimension for retrieved key-value paired chunks and flatten them into $K$ key-value pairs at token-level $\left{\tilde{\mathbf{K}}<em j="j">{j}, \tilde{\mathbf{V}}</em>$. Adopting token-to-chunk retrieval reduces the size of the retrieval index and accelerates the process. Meanwhile, the retrieval accuracy can be further improved, which is also observed in [LGW+23] and [BMH+21]. The hyperparameter chunk-size $c s z$ controls the granularity of retrieved contexts, which can be empirically adjusted based on downstream tasks. For instance, in-context learning requires more fine-grained label tokens from demonstration examples cached in memory, where a smaller $c s z$ is helpful.}\right}_{j=1}^{K</p>
<p>Memory Fusion. The memory fusion is performed within a special memory-augmented layer. As the conventional Transformer decoder layer uses the multi-head self-attention [VSP+17], we follow [WRHS22] to extend it to a joint-attention mechanism and propose a long-term memory fusion process to enable each token to attend on both local contexts and retrieved memory contexts. With the head-wise hidden state output from previous layer $\mathbf{H}^{l-1} \in \mathbb{R}^{|x| \times d}$ and the corresponding retrieved attention key-value pairs are $\left{\tilde{\mathbf{K}}<em i="i">{i}, \tilde{\mathbf{V}}</em>$ is computed as:}\right}_{i=1}^{|x|} \in \mathbb{R}^{|x| \times \mathrm{K} \times \mathrm{d}}$, the output hidden state for the $l$-th memory-augmented layer $\mathbf{H}^{l</p>
<p>$$
\begin{aligned}
&amp; \mathbf{A}=\operatorname{softmax}\left(\frac{\mathbf{Q K}^{T}}{\sqrt{d}}\right) \mathbf{V}, \quad \mathbf{M}=\operatorname{Concat}\left{\operatorname{softmax}\left(\frac{\mathbf{Q}, \tilde{\mathbf{K}}<em i="i">{i}^{T}}{\sqrt{d}}\right) \tilde{\mathbf{V}}</em> \
&amp; \mathbf{H}^{l}=\operatorname{sigmoid}(g) \cdot \mathbf{A}+(1-\operatorname{sigmoid}(g)) \cdot \mathbf{M}
\end{aligned}
$$}\right}_{i=1}^{|x|</p>
<p>where $\mathbf{Q}, \mathbf{K}, \mathbf{V}, \mathbf{A}, \mathbf{M} \in \mathbb{R}^{|x| \times \mathrm{d}}, \mathrm{K}$ is the number of retrieved attention key-value pairs in cached memory for each token, and $g$ is a trainable head-wise gating vector. The hidden state output from previous layer $\mathbf{H}^{(l-1)}$ is linearly projected into attention queries, keys, and values $\mathbf{Q}, \mathbf{K}, \mathbf{V}$ separately via three matrices $W^{Q}, W^{K}, W^{V} \in \mathbb{R}^{d \times d}$. It is worth noting that the retrieved attention key-value pairs in cached memory are distinct to each token.</p>
<h2>3 Experiments</h2>
<p>We evaluate our proposed LONGMEM model on different tasks based on the demanded in-memory long-contexts: a) long-text language modeling and language understanding when loading the past long-context into cached memory; b) infinite-length in-context learning when loading large number of demonstration examples into cached memory.</p>
<h3>3.1 Training Setup</h3>
<p>Batchfying the training corpora. The conventional batchyfing process for large corpora truncates the whole corpora into consecutive fix-length text segments without padding and shuffles all segments to construct mini-batches [RWC+19]. In contrast, LONGMEM must disable global shuffling and ensure the global causality at segment level. Firstly, we divide all long documents in training corpora</p>
<p>into batch-size number of document groups with equivalent length and then perform a document-level shuffling within each group. Then, we concatenate shuffled documents within one group and truncate them into ordered segments. In order to ensure that two consecutive segments of one long document are distributed in two consecutive input batches after batchfying, we select one segment from batch-size number of document groups with the same inner-group index. Thus a mini-batch with batch-size number of segments are constructed from exactly batch-size number of document groups. In this way, as the training iteration steps, the cached attention key-value pairs in memory bank are exactly previous context of current inputs within the same document. The batchfying process is illustrated in Figure 3.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Batchfying the large text corpora into batches to ensure that each consecutive segments within each document is distributed in consecutive batches.</p>
<p><strong>Training Corpus and Hyperparameters.</strong> We sample a subset of the Pile [GBB+20] as the training corpus, including BookCorpus2, Books3, OpenWebText2, Stack Exchange, Wikipedia, Gutenberg (PG-19), NIH ExPorter, and Pile-CC datasets. We reproduce GPT-2 (407M-params) as the pre-trained backbone LLM with Alibi [PSL21] position embedding because original GPT-2 [RWC+19] adopts absolute position embedding, which is found to perform poorly to enable LLM to learn long-distance dependencies [DYY+19]. The backbone LLM holds a <em>L</em> = 24, <em>H</em> = 16, <em>d</em> = 64 architecture. The SideNet holds a <em>L</em> = 12, <em>H</em> = 16, <em>d</em> = 64 architecture. The training for memory-augmented adaptation iterates on 26B tokens, with a global 256 batch-size and 1024 sequence length. The chunk-size <em>csz</em> is 4 tokens and the memory size <em>M</em> is 65k key-value pairs of tokens. For each token, we retrieve <em>K</em>=64 attention key-value pairs for augmentation, which are <em>K/csz</em>=16 text chunks. The memory-augmentation layer is the 9-th layer of SideNet. The attention keys and values from 18-th layer of backbone LLM is cached into memory and used for future retrieval. Other training details are presented in Appendix C.</p>
<p><strong>Memory Retrieval Module.</strong> The fixed memory-size of cached memory bank in one GPU is 65536 key-value pairs of tokens. We enable each GPU to construct and update their own memory retrieval module for efficiency. For the implementation of the efficient token-to-chunk retrieval, we use the faiss [JDJ21] toolkit to construct an exact-search index on GPU to store the mean-pooled attention keys of text chunks and perform efficient retrieval. The faiss index maintains a fixed <em>M/csz</em> keys and provides the efficient exact search w.r.t. inner product. The retrieval takes about 15ms per 1k tokens, which is 55% timecost of backbone LLM forwarding pass. We can easily adapt the exact search index to approximate search index to gain more the retrieval efficiency.</p>
<p><strong>Baselines.</strong> In addition to the baseline of pre-trained GPT-2*, we reproduce Memorizing Transformer (MemTRM) [WRHS22] as another memory-augmented adaptation baseline. The MemTRM can be easily adapted to tune a pre-trained LLM to use external memory. We insert the knn-augmented layer proposed by MemTRM as the same 18-th layer in the LLM decoder. The MemTRM baseline is also trained for the same number of tokens under the same hyperparameter setting.</p>
<h3>3.2 Long-Context Language Modeling</h3>
<p>The long-context language modeling can easily benefit from the augmented decoupled memory of past long-contexts, in which the knowledge stored in retrieved attention key-values can play a useful</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Dataset <br> Splits</th>
<th style="text-align: center;">PG-22</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">ArXiv</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">S1</td>
<td style="text-align: center;">S2</td>
<td style="text-align: center;">S3</td>
<td style="text-align: center;">S4</td>
<td style="text-align: center;">S5</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Len. Range</td>
<td style="text-align: center;">$5 \mathrm{~K}-10 \mathrm{~K}$</td>
<td style="text-align: center;">$10 \mathrm{~K}-100 \mathrm{~K}$</td>
<td style="text-align: center;">$100 \mathrm{~K}-500 \mathrm{~K}$</td>
<td style="text-align: center;">$500 \mathrm{~K}-1 \mathrm{M}$</td>
<td style="text-align: center;">$&gt;1 \mathrm{M}$</td>
<td style="text-align: center;">$&lt;60 \mathrm{~K}$</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">#Documents</td>
<td style="text-align: center;">500</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">30</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Avg. #tokens</td>
<td style="text-align: center;">7.6 K</td>
<td style="text-align: center;">47.6 K</td>
<td style="text-align: center;">140 K</td>
<td style="text-align: center;">640 K</td>
<td style="text-align: center;">1.2 M</td>
<td style="text-align: center;">15.4 K</td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Table 1: Dataset Statistics of five splits of PG-22 based on length range and ArXiv.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">In-Context <br> Len.</th>
<th style="text-align: center;">In-Memory <br> Len.</th>
<th style="text-align: center;">5K-10K</th>
<th style="text-align: center;">10K-100K</th>
<th style="text-align: center;">PG-22 <br> 100K-500K</th>
<th style="text-align: center;">500K-1M</th>
<th style="text-align: center;">$&gt;1 \mathrm{M}$</th>
<th style="text-align: center;">ArXiv</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">GPT-2*</td>
<td style="text-align: center;">1 k</td>
<td style="text-align: center;">N/A</td>
<td style="text-align: center;">22.78</td>
<td style="text-align: center;">24.39</td>
<td style="text-align: center;">24.12</td>
<td style="text-align: center;">24.97</td>
<td style="text-align: center;">18.07</td>
<td style="text-align: center;">11.05</td>
</tr>
<tr>
<td style="text-align: left;">MemTRM</td>
<td style="text-align: center;">1 k</td>
<td style="text-align: center;">65 K</td>
<td style="text-align: center;">21.77</td>
<td style="text-align: center;">23.56</td>
<td style="text-align: center;">23.23</td>
<td style="text-align: center;">24.16</td>
<td style="text-align: center;">17.39</td>
<td style="text-align: center;">10.81</td>
</tr>
<tr>
<td style="text-align: left;">LongMEM</td>
<td style="text-align: center;">1 k</td>
<td style="text-align: center;">65 k</td>
<td style="text-align: center;">$\mathbf{2 1 . 2 9}$</td>
<td style="text-align: center;">$\mathbf{2 3 . 0 1}$</td>
<td style="text-align: center;">$\mathbf{2 2 . 5 5}$</td>
<td style="text-align: center;">$\mathbf{2 3 . 3 5}$</td>
<td style="text-align: center;">$\mathbf{1 6 . 7 1}$</td>
<td style="text-align: center;">$\mathbf{1 0 . 0 5}$</td>
</tr>
</tbody>
</table>
<p>Table 2: Evaluation results on long-context language modeling datasets. We report token-level perplexity (PPL) (lower the better) on all datasets.
role in providing significant background and contextual information to help models perform better on long-context language modeling. For instance, when trying to model a long-text book accurately, acquiring knowledge from previous background and character relationships can be helpful to model the consequent stories.</p>
<p>Evaluation Setting. We first compare LONGMEM and baselines on 3 long-context modeling datasets, Project Gutenberg 2020-2022, ArXiv, and ChapterBreak. The majority of included books or papers in these datasets have the length of at least 16 k tokens. All listed datasets are evaluated in zero-shot manner without any task-specific tuning. The detailed evaluation settings on 3 datasets are as follows:</p>
<ul>
<li>Project Gutenberg 2020-2022 Language Modeling Dataset. We crawled and cleaned the books published between 2020 and 2022 under Project Gutenberg Library ${ }^{1}$ to build up a completely new long-text modeling dataset, named PG-22. It is highly differentiated from our training subset PG-19 in domains and writing styles, because books in PG-19 [RPJL19] are published before 1919. We provide different validation splits of PG-22 based on length range, and data statistics are presented in Table 1.</li>
<li>ArXiv Dataset. ArXiv dataset involves papers in the areas of Math, Computer Science, and Physics. We select a validation split of ArXiv paper subset in the Pile corpus [GBB+20]. ArXiv subset of the Pile is excluded from our training and is an out-of-distribution dataset. We report the token-level language modeling perplexity on the long-context language modeling benchmarks of PG-22 and ArXiv.</li>
<li>ChapterBreak Benchmark. ChapterBreak is proposed in [STI22] as a challenging suffix identification dataset that requires LLMs to distinguish the beginning of the ground-truth next chapter from a set of hard negative segments sampled from the same book, given the long context of previous chapters. ChapterBreak requires processing global long-context to comprehend and identify the correct suffix. [STI22] demonstrated that even state-of-the-art x-formers for longtext processing fail to effectively leverage long-range context to perform well on ChapterBreak. We select the Archive of Our Own (AO3) subset of ChapterBreak which contains fan-fictions extracted from AO3. ChapterBreak provides 8 splits based on the prefix length from 0.5 k to 8 k tokens to fit the length limit of different models. The splits of $4 \mathrm{k}, 6 \mathrm{k}$, and 8 k prefix are selected for evaluation. For LLMs that cannot process over 4 k tokens, we abandon the front prefix to fulfill the maximum input length of LLMs. For MemTRM and LONGMEM model, we firstly load the given $4 \mathrm{k} / 6 \mathrm{k} / 8 \mathrm{k}$ prefix contexts into the cached memory and then do the scoring. we use the perplexity as the scorer for each candidate suffix segment in zero-shot evaluation manner. Then the suffix segment with lower perplexity is selected as the label. The suffix identification accuracy is used as the evaluation metric.</li>
</ul>
<p>Results. The main results on evaluated long-context datasets are summarized in Table 2. The proposed LONGMEM model significantly outperform all considered baselines on long-text language modeling</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">#Params</th>
<th style="text-align: center;">In-Context <br> Len.</th>
<th style="text-align: center;">In-Memory <br> Len.</th>
<th style="text-align: center;">ChapterBreak ${ }_{\text {av3 }}$</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">ctx-4k</td>
<td style="text-align: center;">ctx-6k</td>
<td style="text-align: center;">ctx-8k</td>
</tr>
<tr>
<td style="text-align: center;">GPT-2-XL ${ }^{\dagger}\left[\mathrm{RWC}^{+} 19\right]$</td>
<td style="text-align: center;">1.5B</td>
<td style="text-align: center;">1 K</td>
<td style="text-align: center;">N/A</td>
<td style="text-align: center;">$24 \%$</td>
<td style="text-align: center;">$24 \%$</td>
<td style="text-align: center;">$24 \%$</td>
</tr>
<tr>
<td style="text-align: center;">GPT-3 ${ }^{\dagger}\left[\mathrm{BMR}^{+} 20\right]$</td>
<td style="text-align: center;">175B</td>
<td style="text-align: center;">2 K</td>
<td style="text-align: center;">N/A</td>
<td style="text-align: center;">$28 \%$</td>
<td style="text-align: center;">$28 \%$</td>
<td style="text-align: center;">$28 \%$</td>
</tr>
<tr>
<td style="text-align: center;">LocalTRM ${ }^{\dagger}$ [RSVG21]</td>
<td style="text-align: center;">516 M</td>
<td style="text-align: center;">8 K</td>
<td style="text-align: center;">N/A</td>
<td style="text-align: center;">$24 \%$</td>
<td style="text-align: center;">$24 \%$</td>
<td style="text-align: center;">$24 \%$</td>
</tr>
<tr>
<td style="text-align: center;">RoutTRM ${ }^{\dagger}$ [RSVG21]</td>
<td style="text-align: center;">490 M</td>
<td style="text-align: center;">8 K</td>
<td style="text-align: center;">N/A</td>
<td style="text-align: center;">$25 \%$</td>
<td style="text-align: center;">$24 \%$</td>
<td style="text-align: center;">$24 \%$</td>
</tr>
<tr>
<td style="text-align: center;">Bigbird ${ }^{\dagger}\left[\mathrm{ZGD}^{+} 20\right]$</td>
<td style="text-align: center;">128 M</td>
<td style="text-align: center;">4 K</td>
<td style="text-align: center;">N/A</td>
<td style="text-align: center;">$26 \%$</td>
<td style="text-align: center;">$26 \%$</td>
<td style="text-align: center;">$26 \%$</td>
</tr>
<tr>
<td style="text-align: center;">GPT-2*</td>
<td style="text-align: center;">407 M</td>
<td style="text-align: center;">1 K</td>
<td style="text-align: center;">N/A</td>
<td style="text-align: center;">$18.4 \%$</td>
<td style="text-align: center;">$18.4 \%$</td>
<td style="text-align: center;">$18.4 \%$</td>
</tr>
<tr>
<td style="text-align: center;">MemTRM</td>
<td style="text-align: center;">407 M</td>
<td style="text-align: center;">1 K</td>
<td style="text-align: center;">$\infty$</td>
<td style="text-align: center;">$28.3 \%$</td>
<td style="text-align: center;">$28.7 \%$</td>
<td style="text-align: center;">$28.7 \%$</td>
</tr>
<tr>
<td style="text-align: center;">LongMEM</td>
<td style="text-align: center;">558 M</td>
<td style="text-align: center;">1 K</td>
<td style="text-align: center;">$\infty$</td>
<td style="text-align: center;">$37.7 \%$</td>
<td style="text-align: center;">$39.4 \%$</td>
<td style="text-align: center;">$40.5 \%$</td>
</tr>
</tbody>
</table>
<p>Table 3: Zero-shot Suffix Identification Accuracy on AO3 subset of ChapterBreak. Baselines marked with ${ }^{\dagger}$ are directly cited from [STI22]. The MemTRM and LongMEM loads the given $4 \mathrm{k} / 6 \mathrm{k} / 8 \mathrm{k}$ prefix contexts into cached memory, while the input length to local context is still 1 k tokens.
datasets, with improvements of -1.38 to -1.62 perplexity on different length splits of $P G$-22, and -1.0 ppl on ARXIV datasets. Surprisingly, the proposed method achieves the state-of-the-art performance of $40.5 \%$ accuracy on ChapterBreak ${ }_{\text {AO3 }}$ suffix identification benchmark and outperforms both the strong long-context transformers and latest LLM GPT-3 with 313x larger parameters. The substantial improvements on these datasets demonstrate that LongMEM can comprehend past long-context in cached memory to well complete the language modeling towards future inputs.</p>
<h1>3.3 Memory-Augmented In-Context Learning</h1>
<p>LLMs have the emerging capability of in-context learning (ICL) via learning knowledge nonparametrically from few-shot demonstration examples in the local context. However, conventional in-context learning is heavily restricted by input context length, rendering it ineffective to absorb supervision from sufficient demonstration examples in the training set. With the proposed unlimitedlength memory augmentation, our LongMEM method can overcome the limitation of the number of demonstration examples in the local context and even attend on the whole training set by loading it into the cached memory. In this way, LongMEM goes beyond the conventional few-shot in-context learning and realized memory-augmented in-context learning with thousands of auxiliary demonstration examples.
Evaluation Setting. Here, we evaluate the in-context learning capability of baselines and the proposed LongMEM model on five Natural Language Understanding (NLU) datasets, SST-2 [SPW ${ }^{+} 13$ ], MPQA [WWC05], MR [ABK ${ }^{+} 07$ ], Subj [PL04] and SST-5 [SPW ${ }^{+} 13$ ]. We evaluate models on two few-shot settings, 4 -shot and 20 -shot. The 4 -shot demonstrations are data-insufficient scenario, while the 20-shot demonstrations can almost fulfill the 1 k input length and provide sufficient contextual self-supervisions. We transform the k -shot examples to semantically meaningful demonstration examples via fixed text template, i.e., $d_{i}=$ "Review: $x_{i}$ Sentiment: $y_{i}$ ", $\forall\left{\left(x_{i}, y_{i}\right)\right}<em _text="\text" _train="{train">{i=1}^{k} \in \mathcal{D}</em>$ for sentiment analysis tasks. Additionally, we evaluate the 3-shot ICL on question-answering tasks of SQuAD [RZLL16] under an open-ended generation setting. The details of all prompt templates are presented in Appendix D. Then we concatenate the demonstration examples with newlines to delimit them. The prediction label is directly generated using greedy decoding given the demonstration examples and test cases in context. The prediction accuracy is used as the evaluation metric. We report the mean and standard deviation of 6 runs with different random seeds to overcome the randomness in selecting k-shot demonstration examples. As illustrated before, the chunk size controls the granularity of retrieved text chunks. As the select NLU datasets require to retrieve fine-grained labels from cached memory, we perform an hypperparameter selection on the validation set of SST-2, and the best chunk-size 2 is used to report the results for MemTRM and our model.}</p>
<p>Results. The results on in-context learning are summarized in Table 5 and Table 4. LongMEM achieves remarkable improvements on all NLU tasks in 20-shot sufficient in-context setting, with +8.0 average scores increase over pretrained GPT-2* and MemTRM. Meanwhile, LongMEM also brings performance improvements on the scenario of 4-shot demonstrations in local context. Additionally, LongMEM improves the in-context learning capabilities of LLMs on open-ended generation tasks, with +4.5 EM score increase on SQuAD.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">In-Context <br> #Demons.</th>
<th style="text-align: center;">In-Memory <br> #Demons.</th>
<th style="text-align: center;">SST-2 <br> ACC $\uparrow$</th>
<th style="text-align: center;">MR <br> ACC $\uparrow$</th>
<th style="text-align: center;">Subj <br> ACC $\uparrow$</th>
<th style="text-align: center;">SST-5 <br> ACC $\uparrow$</th>
<th style="text-align: center;">MPQA <br> ACC $\uparrow$</th>
<th style="text-align: center;">Avg.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Majority</td>
<td style="text-align: center;">N/A</td>
<td style="text-align: center;">N/A</td>
<td style="text-align: center;">50.9</td>
<td style="text-align: center;">50.0</td>
<td style="text-align: center;">50.0</td>
<td style="text-align: center;">20.0</td>
<td style="text-align: center;">50.0</td>
<td style="text-align: center;">44.2</td>
</tr>
<tr>
<td style="text-align: left;">GPT-2*</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">N/A</td>
<td style="text-align: center;">$68.3_{11.6}$</td>
<td style="text-align: center;">$64.7_{12.5}$</td>
<td style="text-align: center;">$51.9_{4.2}$</td>
<td style="text-align: center;">$31.4_{4.4}$</td>
<td style="text-align: center;">$61.5_{11.8}$</td>
<td style="text-align: center;">55.6</td>
</tr>
<tr>
<td style="text-align: left;">MemTRM</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">2000</td>
<td style="text-align: center;">$67.5_{12.4}$</td>
<td style="text-align: center;">$64.6_{11.3}$</td>
<td style="text-align: center;">$53.2_{6.0}$</td>
<td style="text-align: center;">$29.6_{4.4}$</td>
<td style="text-align: center;">$63.0_{12.1}$</td>
<td style="text-align: center;">55.6</td>
</tr>
<tr>
<td style="text-align: left;">LongMEM</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">2000</td>
<td style="text-align: center;">$\mathbf{7 1 . 8}_{14.0}$</td>
<td style="text-align: center;">$\mathbf{6 5 . 1}_{11.0}$</td>
<td style="text-align: center;">$\mathbf{5 3 . 8}_{3.7}$</td>
<td style="text-align: center;">$\mathbf{3 6 . 0}_{6.8}$</td>
<td style="text-align: center;">$\mathbf{6 5 . 4}_{12.8}$</td>
<td style="text-align: center;">$\mathbf{5 8 . 4}$</td>
</tr>
<tr>
<td style="text-align: left;">GPT-2*</td>
<td style="text-align: center;">20</td>
<td style="text-align: center;">N/A</td>
<td style="text-align: center;">$68.2_{11.5}$</td>
<td style="text-align: center;">$63.4_{5.2}$</td>
<td style="text-align: center;">$57.6_{10.2}$</td>
<td style="text-align: center;">$33.6_{6.0}$</td>
<td style="text-align: center;">$70.8_{7.6}$</td>
<td style="text-align: center;">58.7</td>
</tr>
<tr>
<td style="text-align: left;">MemTRM</td>
<td style="text-align: center;">20</td>
<td style="text-align: center;">2000</td>
<td style="text-align: center;">$65.1_{9.6}$</td>
<td style="text-align: center;">$65.1_{9.3}$</td>
<td style="text-align: center;">$58.2_{10.6}$</td>
<td style="text-align: center;">$31.9_{6.3}$</td>
<td style="text-align: center;">$72.7_{7.4}$</td>
<td style="text-align: center;">58.6</td>
</tr>
<tr>
<td style="text-align: left;">LongMEM</td>
<td style="text-align: center;">20</td>
<td style="text-align: center;">2000</td>
<td style="text-align: center;">$\mathbf{7 8 . 0}_{14.1}$</td>
<td style="text-align: center;">$\mathbf{7 8 . 6}_{3.3}$</td>
<td style="text-align: center;">$\mathbf{6 5 . 6}_{5.5}$</td>
<td style="text-align: center;">$\mathbf{3 6 . 5}_{7.5}$</td>
<td style="text-align: center;">$\mathbf{7 4 . 6}_{7.3}$</td>
<td style="text-align: center;">$\mathbf{6 6 . 7}$</td>
</tr>
</tbody>
</table>
<p>Table 5: Accuracy [\%] of 4-shot and 20-shot ICL on 5 NLU tasks (SST-2, mr, subj, SST-5, mpqa). We sample 2000 extra demonstration examples and load them into cached memory. The subscript is the standard deviation across 6 runs. Avg. refers to the average accuracy on 5 datasets.</p>
<p>The results indicate that the demonstration examples loaded in cached memory can be regarded as auxiliary contextual demonstrations to attend to and be helpful for in-context learning. LongMEM model can harvest both the taskrelevant knowledge in both local contextual demonstrations and in-memory augmented demonstrations for better incontext learning.</p>
<h3>3.4 Ablation Studies</h3>
<p>So far, we empirically verify the effectiveness and superiority of LongMEM in utilizing cached memory for longcontext modeling, long-context understanding, and manyshot in-context learning. As the design of cached memory bank involves many hyperparameters like memory size $m s z$ and chunk-size $c s z$, we perform a series of ablation studies to evaluate the effects of these hyperparameters on task performance.</p>
<p>Effects of Chunk-Size. As analyzed before, the chunk-size $c s z$ controls the granularity of retrieval and thus it may make a difference to tasks with requirements of fine-grained retrieval like in-context learning. We perform an ablation study on the effects of various chunk-size $c s z \in{2,4,8}$ on in-context learning and the results are presented in Figure 4(a). The chunk size of 2 yields the best performance on in-context learning tasks on five NLU datasets, which is consistent with the property of NLU tasks with the requirement of fine-grained retrieval and fusion towards classification label tokens.</p>
<p>Effects of Memory Size. The memory size (msz) controls the capacity of the memory bank. In general, the memory size should be compatible with the average length of documents or contexts, i.e., , a set of books with average 16 k tokens should deploy the memory size of 16 k tokens in cached memory. The training $m s z$ of 65 tokens is excessive for downstream tasks such as ChapterBreak as the whole prefix context length does not exceed 65 k tokens. Thus, we perform an ablation study on the effects of memory size $m s z \in{8 k, 16 k, 32 k, 65 k}$ during the inference stage on the PG-22 language modeling datasets and the results are shown in Figure 4(b). To model the books with average $8 \mathrm{k}-50 \mathrm{k}$ length, the smaller memory size $16 k$ which is consistent with the average length of target books yields the best perplexity.</p>
<h2>4 Related Work</h2>
<p>Large Language Models. Large Language Models, i.e., GPT-2 [RWC ${ }^{+}$19], GPT-3 [BMR ${ }^{+}$20], OPT [ZRG ${ }^{+}$22], and BLOOM [SFA ${ }^{+}$22], significantly revolutionized NLP research and promoted the state-of-the-art of various language understanding, language generation [WZG ${ }^{+}$22], and even vision-language tasks [WDC ${ }^{+}$22]. Additionally, via scaling the model parameters, LLMs exhibit "emergent abilities" [WTB ${ }^{+}$22] like few-shot in-context learning [BMR ${ }^{+}$20], multi-step reasoning [WWS ${ }^{+}$22], code completion, etc.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: (a) Accuracy on 5 NLU datasets given different chunk size during inference; (b) $\Delta$ Perplexity on 4 splits of PG-22 given different memory size during inference, in which the perplexity when $m s z=65 \mathrm{k}$ is used as baseline.
x-formers. To enable transformers to attend on longer context, many variants of "x-formers" are proposed. Transformer-XL [DYY ${ }^{+} 19$ ] proposes to cache attention keys and values of past segment and reuse them in recurrent manner. Recent seminal works of x-formers, including LinFormer [WLK ${ }^{+} 20$ ], LongFormer [BPC20], Routing Transformer [RSVG21], proposed various sparse attention mechanisms for decreasing $O\left(n^{2}\right)$ complexity to $O(n \log n)$ or even $O(n)$. BigBird [ZGD ${ }^{+} 20$ ] achieves a 4 k sequence length via attending on a subset of context tokens. Although these x -formers achieve substantial efficiency improvements, such efficiency gains are not remarkable when modeling sequences that spans book-level length. Moreover, the largest sequence length of these methods is still upper-bounded by 16 k tokens, making them invalid in modeling long-sequences at the book or wikipedia-page level (i.e., average 70k tokens for full-length books in PG19 dataset [RPJL19]).
Side-Tuning. The method of Side-Tuning [ZSZ ${ }^{+} 20$, SCB22] is a task-specific tuning method for pre-trained models via training a lightweight side-network that is fused with the fixed pre-trained network via summation. Our method inherits the idea of adopting a side-network but distinguishes the side-tuning method in terms of learning objective and cross-network fusion ways. LONGMEM proposes to augment LLMs with decoupled memory for memorizing long past inputs, which does not involve any task-specific tuning. The cross-network residual connections proposed by LONGMEM is novel and distincts from the vanilla summation of Side-Tuning.</p>
<h1>5 Conclusion</h1>
<p>In this paper, we propose to augment LLMs with long-term memory for enabling them to memorize long-form context and gain long-form memory. The designed decoupled memory module can cache attention key and value pairs of past inputs for future retrieval and fusion. A decoupled residual SideNet is introduced as the memory retriever and reader, meanwhile the LLM itself is frozen and works as knowledge and memory encoder. Experiments on various long-contextual language modeling datasets demonstrate the effectiveness of our model over other memory-augmentation baselines. The proposed method can also enable in-context learning of LLMs to overcome the limited number of demonstration examples in context, which is constrained by the contextual length, via caching thousands of auxiliary demonstration examples in memory.</p>
<h2>References</h2>
<p>[ABK ${ }^{+} 07$ ] Sören Auer, Christian Bizer, Georgi Kobilarov, Jens Lehmann, Richard Cyganiak, and Zachary Ives. Dbpedia: A nucleus for a web of open data. In The semantic web, pages 722-735. Springer, 2007.
[BMH ${ }^{+} 21$ ] Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican, George van den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, Diego de Las Casas, Aurelia Guy, Jacob Menick, Roman Ring, T. W. Hennigan, Saffron Huang, Lorenzo Maggiore, Chris Jones, Albin Cassirer, Andy Brock, Michela Paganini, Geoffrey Irving, Oriol Vinyals, Simon Osindero, Karen Simonyan, Jack W. Rae, Erich Elsen, and L. Sifre. Improving language models by retrieving from trillions of tokens. ArXiv, abs/2112.04426, 2021.
[BMR ${ }^{+} 20$ ] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel</p>
<p>Herbert-Voss, Gretchen Krueger, T. J. Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeff Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. ArXiv, abs/2005.14165, 2020.
[BPC20] Iz Beltagy, Matthew E Peters, and Arman Cohan. Longformer: The long-document transformer. arXiv preprint arXiv:2004.05150, 2020.
[DCLT19] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In NAACL, 2019.
$\left[\right.$ DYY $\left.^{+} 19\right]$ Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V Le, and Ruslan Salakhutdinov. Transformer-xl: Attentive language models beyond a fixed-length context. arXiv preprint arXiv:1901.02860, 2019.
[GBB+20] Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, et al. The pile: An 800gb dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027, 2020.
[JDJ21] Jeff Johnson, Matthijs Douze, and Hervé Jégou. Billion-scale similarity search with gpus. IEEE Transactions on Big Data, 7:535-547, 2021.
[KB15] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. CoRR, abs/1412.6980, 2015.
[LGW+23] Rui Lv, Junliang Guo, Rui Wang, Xu Tan, Qi Liu, and Tao Qin. N-gram nearest neighbor machine translation. arXiv preprint arXiv:2301.12866, 2023.
$\left[\right.$ LOG $\left.^{+} 19\right]$ Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. RoBERTa: A robustly optimized bert pretraining approach. ArXiv, abs/1907.11692, 2019.
[PL04] Bo Pang and Lillian Lee. A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts. arXiv preprint cs/0409058, 2004.
[PSL21] Ofir Press, Noah A Smith, and Mike Lewis. Train short, test long: Attention with linear biases enables input length extrapolation. arXiv preprint arXiv:2108.12409, 2021.
[RNSS18] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language understanding with unsupervised learning. 2018.
[RPJL19] Jack W Rae, Anna Potapenko, Siddhant M Jayakumar, and Timothy P Lillicrap. Compressive transformers for long-range sequence modelling. arXiv preprint arXiv:1911.05507, 2019.
[RSR+20] Colin Raffel, Noam M. Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. ArXiv, abs/1910.10683, 2020.
[RSVG21] Aurko Roy, Mohammad Saffar, Ashish Vaswani, and David Grangier. Efficient content-based sparse attention with routing transformers. Transactions of the Association for Computational Linguistics, 9:53-68, 2021.
$\left[\right.$ RWC $\left.^{+} 19\right]$ Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. 2019.
[RZLL16] Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. SQuAD: 100,000+ Questions for Machine Comprehension of Text. arXiv e-prints, page arXiv:1606.05250, 2016.
[SCB22] Yi-Lin Sung, Jaemin Cho, and Mohit Bansal. Lst: Ladder side-tuning for parameter and memory efficient transfer learning. arXiv preprint arXiv:2206.06522, 2022.
[SFA+22] Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilić, Daniel Hesslow, Roman Castagné, Alexandra Sasha Luccioni, François Yvon, Matthias Gallé, et al. Bloom: A 176bparameter open-access multilingual language model. arXiv preprint arXiv:2211.05100, 2022.
[SPW+13] Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Y Ng, and Christopher Potts. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the 2013 conference on empirical methods in natural language processing, pages 1631-1642, 2013.</p>
<p>[STI22] Simeng Sun, Katherine Thai, and Mohit Iyyer. Chapterbreak: A challenge dataset for long-range language models. arXiv preprint arXiv:2204.10878, 2022.
[VSP ${ }^{+}$17] Ashish Vaswani, Noam M. Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NIPS, 2017.
$\left[\mathrm{WDC}^{+}\right.$22] Weizhi Wang, Li Dong, Hao Cheng, Haoyu Song, Xiaodong Liu, Xifeng Yan, Jianfeng Gao, and Furu Wei. Visually-augmented language modeling. arXiv preprint arXiv:2205.10178, 2022.
[WLK ${ }^{+}$20] Sinong Wang, Belinda Z Li, Madian Khabsa, Han Fang, and Hao Ma. Linformer: Self-attention with linear complexity. arXiv preprint arXiv:2006.04768, 2020.
[WRHS22] Yuhuai Wu, Markus N. Rabe, DeLesley S. Hutchins, and Christian Szegedy. Memorizing transformers. ArXiv, abs/2203.08913, 2022.
[WTB ${ }^{+}$22] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al. Emergent abilities of large language models. arXiv preprint arXiv:2206.07682, 2022.
[WWC05] Janyce Wiebe, Theresa Wilson, and Claire Cardie. Annotating expressions of opinions and emotions in language. Language resources and evaluation, 39(2):165-210, 2005.
[WWS ${ }^{+}$22] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language models. arXiv preprint arXiv:2201.11903, 2022.
[WZG ${ }^{+}$22] Weizhi Wang, Zhirui Zhang, Junliang Guo, Yinpei Dai, Boxing Chen, and Weihua Luo. Taskoriented dialogue system as natural language generation. In Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 2698-2703, 2022.
[YDY ${ }^{+}$19] Zhilin Yang, Zihang Dai, Yiming Yang, Jaime G. Carbonell, Ruslan Salakhutdinov, and Quoc V. Le. XLNet: Generalized autoregressive pretraining for language understanding. In NeurIPS, 2019.
[ZGD ${ }^{+}$20] Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, et al. Big bird: Transformers for longer sequences. Advances in neural information processing systems, 33:17283-17297, 2020.
[ZRG ${ }^{+}$22] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068, 2022.
[ZSZ ${ }^{+}$20] Jeffrey O Zhang, Alexander Sax, Amir Zamir, Leonidas Guibas, and Jitendra Malik. Side-tuning: a baseline for network adaptation via additive side networks. In Computer Vision-ECCV 2020: 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part III 16, pages 698-714. Springer, 2020.</p>
<h1>A Inference Efficiency and GPU-Memory Efficiency</h1>
<p>When the model is required to comprehend long sequences, the proposed method LongMEM can load the out-of-boundary inputs into the cached memory as previous context. Thus, the memory usage and inference speed can be significantly improved compared with vanilla self-attention-based models. The detailed statistics in terms of the efficiency is presented in Table 6.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">In-Context <br> Len.</th>
<th style="text-align: center;">In-Memory <br> Len.</th>
<th style="text-align: center;">Inference Speed <br> (tokens/s) $\uparrow$</th>
<th style="text-align: center;">GPU-Memory Usage <br> (MBs) $\downarrow$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">GPT-2*</td>
<td style="text-align: center;">4 k</td>
<td style="text-align: center;">N/A</td>
<td style="text-align: center;">14666</td>
<td style="text-align: center;">20671</td>
</tr>
<tr>
<td style="text-align: left;">LONGMEM</td>
<td style="text-align: center;">1 k</td>
<td style="text-align: center;">3 k</td>
<td style="text-align: center;">22638</td>
<td style="text-align: center;">13335</td>
</tr>
<tr>
<td style="text-align: left;">GPT-2*</td>
<td style="text-align: center;">8 k</td>
<td style="text-align: center;">N/A</td>
<td style="text-align: center;">8417</td>
<td style="text-align: center;">54195</td>
</tr>
<tr>
<td style="text-align: left;">LONGMEM</td>
<td style="text-align: center;">1 k</td>
<td style="text-align: center;">7 k</td>
<td style="text-align: center;">21343</td>
<td style="text-align: center;">13437</td>
</tr>
</tbody>
</table>
<p>Table 6: The superiority of our method over fully dense self-attention (GPT-2*) in terms of inference speed and GPU-memory utilization.</p>
<h2>B Training Details</h2>
<p>The pre-training of reproduced GPT-2* iterates on 117B tokens in total, with 512 batch-size and 1024-token fixed segment-length. The Adam optimizer [KB15] is adopted in memory-augmented adaptation training. The pre-training and adaptation are trained on 16 32GB-Tesla-V100 GPUs. Other detailed training hypperparamters and settings are presented in Table 7.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Hyperparameter</th>
<th style="text-align: center;">LONGMEM</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Reproduced GPT-2* Backbone LLM Hyperparameters</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Parameters</td>
<td style="text-align: center;">407 M</td>
</tr>
<tr>
<td style="text-align: center;">Precision</td>
<td style="text-align: center;">float16</td>
</tr>
<tr>
<td style="text-align: center;">Layers</td>
<td style="text-align: center;">24</td>
</tr>
<tr>
<td style="text-align: center;">Hidden dim.</td>
<td style="text-align: center;">1024</td>
</tr>
<tr>
<td style="text-align: center;">Attention heads</td>
<td style="text-align: center;">16</td>
</tr>
<tr>
<td style="text-align: center;">Head Dim</td>
<td style="text-align: center;">64</td>
</tr>
<tr>
<td style="text-align: center;">Vocab size</td>
<td style="text-align: center;">52 k</td>
</tr>
<tr>
<td style="text-align: center;">Sequence length</td>
<td style="text-align: center;">1024</td>
</tr>
<tr>
<td style="text-align: center;">Position emb.</td>
<td style="text-align: center;">Alibi</td>
</tr>
<tr>
<td style="text-align: center;">Tied embedding</td>
<td style="text-align: center;">False</td>
</tr>
<tr>
<td style="text-align: center;">SideNet Hyperparameters</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Parameters</td>
<td style="text-align: center;">151 M</td>
</tr>
<tr>
<td style="text-align: center;">Precision</td>
<td style="text-align: center;">float16</td>
</tr>
<tr>
<td style="text-align: center;">Layers</td>
<td style="text-align: center;">12</td>
</tr>
<tr>
<td style="text-align: center;">Hidden dim.</td>
<td style="text-align: center;">1024</td>
</tr>
<tr>
<td style="text-align: center;">Attention heads</td>
<td style="text-align: center;">16</td>
</tr>
<tr>
<td style="text-align: center;">Head Dim</td>
<td style="text-align: center;">64</td>
</tr>
<tr>
<td style="text-align: center;">Sequence length</td>
<td style="text-align: center;">1024</td>
</tr>
<tr>
<td style="text-align: center;">Memory-Augmented Adaptation Hyperparameters</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Global Batch Size</td>
<td style="text-align: center;">256</td>
</tr>
<tr>
<td style="text-align: center;">Learning rate</td>
<td style="text-align: center;">$2.0 \mathrm{e}-4$</td>
</tr>
<tr>
<td style="text-align: center;">Total tokens</td>
<td style="text-align: center;">26B</td>
</tr>
<tr>
<td style="text-align: center;">Warmup tokens</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: center;">LR Decay style</td>
<td style="text-align: center;">polynomial</td>
</tr>
<tr>
<td style="text-align: center;">Adam $\left(\beta_{1}, \beta_{2}\right)$</td>
<td style="text-align: center;">$(0.9,0.98)$</td>
</tr>
<tr>
<td style="text-align: center;">Adam eps</td>
<td style="text-align: center;">$1 \mathrm{e}-06$</td>
</tr>
<tr>
<td style="text-align: center;">Weight decay</td>
<td style="text-align: center;">0.01</td>
</tr>
<tr>
<td style="text-align: center;">Gradient clipping</td>
<td style="text-align: center;">2.0</td>
</tr>
</tbody>
</table>
<p>Table 7: Memory-Augmented Adaptation and Architectural Hyperparameters.</p>
<h1>C Prompting Templates</h1>
<p>We present all hand-crafted in-context learning prompting templates and labels for 5 NLU datasets and Squad QA dataset in Tabel 8.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Task</th>
<th style="text-align: left;">Prompt</th>
<th style="text-align: left;">Labels</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">SST-2</td>
<td style="text-align: left;">Review: [Sentence] Sentiment: [Label]</td>
<td style="text-align: left;">[positive, negative]</td>
</tr>
<tr>
<td style="text-align: left;">MR</td>
<td style="text-align: left;">Review: [Sentence] Sentiment: [Label]</td>
<td style="text-align: left;">[positive, negative]</td>
</tr>
<tr>
<td style="text-align: left;">MPQA</td>
<td style="text-align: left;">Review: [Sentence] Sentiment: [Label]</td>
<td style="text-align: left;">[positive, negative]</td>
</tr>
<tr>
<td style="text-align: left;">SST-5</td>
<td style="text-align: left;">input: [Sentence] type: [Label]</td>
<td style="text-align: left;">[terrible,bad,okay,good,great]</td>
</tr>
<tr>
<td style="text-align: left;">Subj</td>
<td style="text-align: left;">input: [Sentence] type: [Label]</td>
<td style="text-align: left;">[objective, subjective]</td>
</tr>
<tr>
<td style="text-align: left;">Squad</td>
<td style="text-align: left;">Passage: [Passage]\n Question: [Question] Answer: [Answer]</td>
<td style="text-align: left;"></td>
</tr>
</tbody>
</table>
<p>Table 8: The hand-crafted prompts used to query the model predictions on the zero-shot evaluation of 5 NLU datasets and one question-answering dataset Squad.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ https://www.gutenberg.org/&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
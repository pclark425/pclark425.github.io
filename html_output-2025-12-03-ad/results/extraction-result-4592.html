<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4592 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4592</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4592</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-100.html">extraction-schema-100</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <p><strong>Paper ID:</strong> paper-272827223</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2409.13740v1.pdf" target="_blank">L ANGUAGE AGENTS ACHIEVE SUPERHUMAN SYNTHESIS OF SCIENTIFIC KNOWLEDGE</a></p>
                <p><strong>Paper Abstract:</strong> These authors jointly supervise technical work at FutureHouse</p>
                <p><strong>Cost:</strong> 0.025</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4592.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4592.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PaperQA2</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PaperQA2</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An agentic, retrieval-augmented-generation (RAG) system designed to search, extract, and synthesize information from the full scientific literature using multi-step tools (paper search, gather evidence with reranking+summarization, citation traversal, and generate-answer). It is optimized for factuality and evaluated on retrieval, summarization, and contradiction-detection tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>PaperQA2</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>An agent-based RAG system that decomposes literature search and answer synthesis into explicit tools orchestrated by an LLM agent. Key components are: (1) Paper Search — agent-generated keyword queries to external search APIs (e.g., Semantic Scholar) that return candidate papers; (2) Document parsing (Grobid or PyMuPDF) and chunking; (3) Embedding-based top-k dense retrieval over chunks; (4) RCS (Reranking and Contextual Summarization) — map LLM completions produce structured JSON summaries and relevance scores for each chunk; (5) Citation Traversal — one-hop traversal of past references and future citers to expand candidate set with overlap filtering; (6) Generate Answer — a final LLM prompt that consumes the top contextual summaries to produce a cited, Wikipedia-style answer. The agent can iteratively call tools (search → gather evidence → traverse citations → re-gather → generate) and revise queries.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Agent orchestration: gpt-4-turbo-2024-04-09; RCS (summarization & reranking): GPT-4-Turbo (best-performing); Generate Answer: tested with Claude-3-Opus, Gemini-1.5-Pro, GPT-4-Turbo, GPT-3.5-Turbo; ContraCrow/contradiction step used Claude 3.5 Sonnet for some experiments; embedding model: OpenAI text-embedding-large-3 (hybrid with sparse keyword encoding).</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Hybrid embedding-based dense vector retrieval (top-k) over parsed document chunks + LLM-based reranking and structured contextual summarization (RCS) that outputs JSON summaries and relevance scores; chunking via Grobid/PyMuPDF; citation-graph traversal to expand candidate documents.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Multi-step agentic RAG: combine top-ranked contextual summaries into a final context for LLM generation; citation-traversal hierarchical indexing; iterative query refinement by the agent; generation constrained to cite only provided contexts (Wikipedia-style output).</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>On LitQA2 questions, PaperQA2 parsed and utilized an average of 14.5 ± 0.6 papers per question (runs n=3). Experiment corpus: LitQA2 (248 Qs).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>General scientific literature with evaluation focused on biology papers and human protein-coding gene literature (biomedical domain emphasized in summarization/contradiction tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Cited, Wikipedia-style summaries (articles), direct answers to retrieval questions (multiple-choice mapping), and contradiction detection reports (claim-level reasoning with Likert score).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Precision (fraction correct when answered), Accuracy (fraction correct overall), DOI recall, human expert ratings for summaries (cited & supported vs cited & unsupported), AUC/ROC for contradiction benchmark, human validation rates for contradictions.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>On LitQA2 (248 Qs): precision 85.2% ± 1.1% (mean ± SD, n=3 runs), accuracy 66.0% ± 1.2%; chose 'insufficient information' in 21.9% ± 0.9% of answers. On WikiCrow summarization: generated 240 gene articles, WikiCrow precision (cited & supported / cited) = 86.1% vs Wikipedia 71.2%; cited & unsupported 13.5% (WikiCrow) vs 24.9% (Wikipedia).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared against human domain experts (PhD students/postdocs) using full internet and institutional access, previous PaperQA configuration, commercial RAG systems (Perplexity, Elicit), and frontier non-RAG LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>PaperQA2 achieved superhuman precision vs experts on LitQA2 (human precision 73.8% ± 9.6%; PaperQA2 precision 85.2% ± 1.1%; t-test p=0.0036) and matched human accuracy (human accuracy 67.7% ± 11.9%; PaperQA2 66.0%, not significantly different). It outperformed the prior PaperQA config (accuracy 36.7%, precision 76.5%) and other RAG systems tested on LitQA2.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Agentic multi-step RAG with deep LLM-based reranking/summarization (RCS) and citation traversal substantially improves retrieval precision/recall and factuality compared to simpler RAG or non-agentic pipelines; using a high-quality LLM for RCS (GPT-4-Turbo) and a deep RCS list (>10) were crucial; agentic ability to reformulate searches improves recall over fixed pipelines.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>High computational and API cost (PaperQA2 reported $1–$3 per query; WikiCrow ~$4.48 per article); dependency on access to full text repositories and licensing; hallucination/reasoning errors remain possible; performance sensitive to LLM choice and RCS depth; smaller LLMs performing RCS can degrade accuracy; infrastructure complexity for large-scale deployment.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Performance improves with deeper RCS ranking depth up to diminishing returns (notable gains from 1→10, marginal gains 10→30); better parsing (Grobid) reduced tokenization and aided summarization of tables for WikiCrow; higher-capacity LLMs for RCS increase accuracy (GPT-4-Turbo > smaller models); citation traversal and iterative search increase DOI recall and accuracy; tradeoff observed between number of contexts injected and precision vs accuracy (more contexts → higher precision, fewer contexts → higher accuracy in some settings).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'L ANGUAGE AGENTS ACHIEVE SUPERHUMAN SYNTHESIS OF SCIENTIFIC KNOWLEDGE', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4592.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4592.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PaperQA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PaperQA (original)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prior retrieval-augmented generative agent for scientific research that inspired PaperQA2; used RAG with an RCS-like pipeline but with smaller RCS models and less tooling (no citation traversal, fewer reranking depth).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Paperqa: Retrieval-augmented generative agent for scientific research</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>PaperQA (original)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A RAG agent that performs literature search, document parsing, chunking, top-k retrieval and LLM-based generation. PaperQA provided the foundational agentic tool abstraction (paper_search, gather_evidence, generate_answer) and a PromptCollection mechanism; PaperQA2 extends and optimizes this pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Originally used smaller RCS models (GPT-3.5-Turbo in prior config) and fewer deep reranking steps (per paper's ablation description).</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Embedding-based retrieval and LLM summarization of chunks; earlier RCS used a cheaper model for reranking/summarization.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Single-pass RAG generation from selected chunks without the expanded agentic citation traversal and deeper RCS depth used in PaperQA2.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Not explicitly specified in this paper for original runs; used as baseline on LitQA2 benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>General scientific literature; prior work targeted literature-enabled tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Answers to literature questions and summaries.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>LitQA2 accuracy and precision as reported for comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Prior PaperQA configuration: LitQA2 accuracy 36.7%, precision 76.5% (reported in paper as baseline).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared as a baseline against PaperQA2 and other systems like Perplexity and Elicit.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>PaperQA2 substantially improved accuracy and precision relative to the original PaperQA config by using larger RCS LLMs, citation traversal, and deeper RCS depth.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Shows that improvements in RCS model quality, deeper reranking, and added citation traversal materially improve retrieval accuracy and factual output quality.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Earlier limitations included lower RCS model capacity and missing tools (e.g., citation traversal), resulting in lower accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Not extensively detailed here beyond implication that scaling model capacity for the reranker and deeper ranking lists improves performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'L ANGUAGE AGENTS ACHIEVE SUPERHUMAN SYNTHESIS OF SCIENTIFIC KNOWLEDGE', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4592.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4592.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>WikiCrow</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>WikiCrow</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A pipeline built on PaperQA2 to generate cited, Wikipedia-style articles about human protein-coding genes by issuing multiple PaperQA2 calls per article section and stitching outputs into full articles.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>WikiCrow</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A multi-query orchestration over PaperQA2: for each target gene, WikiCrow issues a set of section-specific PaperQA2 queries (structure, function, interactions, clinical significance, etc.), uses Grobid parsing and RCS prompts tailored to extract gene names and tabular data, then stitches the section outputs into a single Wikipedia-style article. Article generation includes structured citation insertion and post-processing for consistency.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>PaperQA2 backend models (agent orchestration gpt-4-turbo-2024-04-09), RCS summarizers (GPT-4-Turbo), and an overview LLM call (GPT-4-Turbo); generation used the same Generate Answer tool settings.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Targeted PaperQA2 queries per article section, Grobid-based parsing for higher-fidelity section/table extraction, RCS extraction of gene names and structured JSON outputs for reliable citation association.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Section-wise multi-query generation followed by programmatic stitching into a single article; reliance on citation-aware RAG outputs to ensure factual support.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Generated 240 articles on genes for evaluation (paired with 240 Wikipedia articles). Underlying corpus size varied; Grobid/pyMuPDF token comparisons reported across 5,363 papers sample for parser token statistics.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Human protein-coding gene literature (biomedical domain).</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Cited, Wikipedia-style full-length articles (structured sections).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Human expert grading of sampled statements for 'Is it cited?' and 'Is information correct as cited?'; counts of 'cited and unsupported' statements; precision defined as cited-and-supported / cited.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>WikiCrow articles averaged 1219.0 ± 275.0 words; cost ~ $4.48 ± $1.02 per article; in evaluation (375 statements from 240 paired articles) WikiCrow had 13.5% cited-and-unsupported vs Wikipedia 24.9% (p=0.0075). WikiCrow had fewer uncited statements (3.5% vs 13.6%) and higher precision (86.1% vs 71.2%, p=0.0013).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared against existing human-written Wikipedia articles on the same genes and evaluated by blinded expert graders.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>WikiCrow produced significantly fewer cited-but-unsupported statements and higher cited-precision than Wikipedia in the sampled set.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Agentic RAG with Grobid parsing and targeted RCS metadata extraction (gene name) reduces hallucinations and attribution errors relative to Wikipedia articles; LLM-based summarization can be more factually accurate than crowd-written summaries in this evaluation setting.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Depends on access to full texts, Grobid parsing availability; cost per article non-negligible; potential attribution issues and remaining hallucination/reasoning errors (though fewer than Wikipedia in this task).</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Parser choice (Grobid) reduced token usage (approx. 8,903 tokens/paper vs 16,040 tokens for PyMuPDF in a sample), enabling more efficient scaling; RCS depth and high-quality summarizer remain important for fidelity when synthesizing multi-paper sections.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'L ANGUAGE AGENTS ACHIEVE SUPERHUMAN SYNTHESIS OF SCIENTIFIC KNOWLEDGE', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4592.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4592.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ContraCrow</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ContraCrow</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A system built on PaperQA2 to detect contradictions in the scientific literature by extracting claims from papers, searching literature for counter-evidence, and scoring contradictions on an 11-point Likert scale with LLM reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>ContraCrow</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Two-stage system: (1) claim extraction — split paper sections (≤5000 characters per chunk) and run LLM completions to propose candidate claims; filter claims with an LLM-based quality/generalizability scorer (keep claims scoring ≥8). (2) contradiction search — feed each claim into PaperQA2 with a contradiction-detection prompt instructing literature search, evidence gathering, and an LLM-generated Likert score (0–10) plus reasoning. Uses PaperQA2 tools (paper_search, gather_evidence with RCS, citation traversal) for evidence collection; outputs include model reasoning, evidence citations, and Likert score.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Claim extraction used gpt-4-turbo-2024-04-09 for generation and filtering; contradiction detection used Claude 3.5 Sonnet (for many experiments) with temperature 0 and chunk size 7000 characters; underlying retrieval uses embeddings (text-embedding-large-3).</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>LLM-based claim extraction from parsed chunks, followed by an LLM filter scoring claims; retrieval via PaperQA2 embedding-based top-k ranking and RCS summarization of chunks to assemble evidence for each claim.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Compare each claim against assembled evidence using LLM reasoning; map natural-language Likert outputs to integer scores and apply decision thresholds to flag contradictions; supports 'contradiction validation' pipelines where human experts review model-cited chunks and reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Applied to 93 randomly selected biology papers (from a 100-paper sample where 7 failed to parse). Processed 3,180 claims total (mean 35.16 ± 21.72 claims per paper).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Biology literature (random biology papers sample) and general scientific claim verification tasks (benchmarked with ContraDetect derived from LitQA2).</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Claim-level contradiction flags, LLM reasoning text, Likert-scale scores (0–10), and cited evidence chunks.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>ROC/AUC on ContraDetect benchmark (AUC 0.842), precision/accuracy at threshold, human validation agreement rates, F1 for validated contradictions.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>On ContraDetect: AUC = 0.842; at threshold 8: 73% accuracy, 88% precision, false positive rate 7%. Applied to 93 papers: 6.85% of 3,180 claims flagged as contradicted (scores 8–10); average 2.34 ± 1.99 contradictions per paper (Likert≥8). Human expert validation: sampled claims (50 score=8, 50 score≥9) — 70% agreement with ContraCrow (F1 = 0.82), yielding ~1.64 human-validated contradictions/paper in validation sample.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Benchmarked on ContraDetect (derived from LitQA2) and compared to human expert annotation (validation and detection tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>High precision in benchmarked setting; human validation agreed with 70% of model-flagged contradictions. Humans agreed with each other more than with ContraCrow on a separate detection task (humans inter-agreement 75.5% ± 13.43% vs agreement with ContraCrow 60.42% ± 5.99%).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>LLM-driven claim extraction plus PaperQA2-backed literature search can detect literature contradictions at scale with high precision and interpretable scoring; however the system exhibits overconfidence on some Likert-scale ratings and can disagree with exhaustive human review.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Possible overconfidence in Likert outputs relative to human judgments; dependence on quality of parsed chunks and access to full-text; contradictions may be contextual (not necessarily 'wrong'); claim-extraction quality and filtering thresholds affect recall/precision tradeoffs.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Capable of scaling to thousands of claims (it processed 3,180 claims in batched runs), but performance and cost depend on LLM choice and depth of evidence gathered; batching and parallel RCS help efficiency; human validation remains necessary for high-stakes deployment.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'L ANGUAGE AGENTS ACHIEVE SUPERHUMAN SYNTHESIS OF SCIENTIFIC KNOWLEDGE', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4592.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e4592.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LitQA2</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LitQA2</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A hard benchmark introduced in this paper consisting of 248 multiple-choice questions designed to require retrieval from the main body (not abstract) of scientific papers; intended to evaluate retrieval and citation-accurate answering across the literature.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>LitQA2 (benchmark)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A manually curated dataset of 248 multiple-choice questions where correct answers must be grounded in specific passages in the main body of scientific papers (often obscure experimental findings); creators ensured answers weren't present in abstracts and excluded questions where alternative sources could answer them. Includes 'insufficient information' label for unanswerable items. Used to evaluate precision, accuracy, and DOI recall by matching system-cited DOI to the creator's DOI.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Not an LLM system; used to evaluate LLM-based systems including PaperQA2. Question generation used experts and iterative testing with ChatGPT for distractor design, but LitQA2 itself is a benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>N/A (benchmark); evaluation depends on systems' extraction techniques (DOI-level matching required).</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>N/A (benchmark used to evaluate synthesis and retrieval of systems).</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Questions were generated about obscure findings from very recent papers; corpus size implicit but LitQA2 comprises 248 Qs referencing specific papers (47 original LitQA + 100 + 101 additions).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Scientific literature broadly; many questions focused on biology.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Multiple-choice answers with associated cited DOI (used to compute precision/accuracy/recall).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Precision (fraction correct when answered), Accuracy (fraction correct overall), DOI recall (fraction of answers attributed to the correct DOI), and abstention rate ('insufficient information').</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Used to measure PaperQA2: precision 85.2% ±1.1, accuracy 66.0% ±1.2, abstain rate 21.9% ±0.9. Human expert performance reported: precision 73.8% ±9.6, accuracy 67.7% ±11.9.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Human experts, other RAG systems (Perplexity, Elicit), frontier non-RAG LLMs, and prior PaperQA configuration.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>PaperQA2 outperformed humans in precision on LitQA2 and outperformed other RAG and non-RAG baselines in the experiments reported.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>LitQA2 is a challenging, high-fidelity benchmark that forces systems to retrieve specific main-text evidence; it exposed differences in DOI recall and demonstrated utility of agentic retrieval+RCS approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Creation is expert-intensive (manual curation); potential indexation leakage for human evaluators when questions get indexed online (noted contamination in one human quiz round).</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Designed as a static benchmark; scaling concerns relate to dataset expansion and potential automation of question generation (emergent ideas mentioned).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'L ANGUAGE AGENTS ACHIEVE SUPERHUMAN SYNTHESIS OF SCIENTIFIC KNOWLEDGE', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4592.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e4592.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RAG</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Retrieval-Augmented Generation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A general approach where an LLM is augmented with externally retrieved context (snippets or documents) to ground generated outputs; widely used as the paradigm for literature-enabled LLM systems.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Retrieval-Augmented Generation (RAG)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>High-level approach: retrieve relevant documents or passages for a query using vector/sparse retrieval, then include retrieved context in the prompt for an LLM to generate a grounded answer. In this paper, RAG is the baseline paradigm that PaperQA2 builds on but decomposes into agentic tools and augments with RCS and citation traversal.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Applicable to many LLMs; in experiments the generate-answer step used Claude-3-Opus, GPT-4-Turbo, Gemini-1.5-Pro, GPT-3.5-Turbo in various ablations.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Embedding-based retrieval and top-k selection of chunks, then direct insertion of chunks into generation context (though PaperQA2 modifies this with RCS).</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Direct RAG generation or multi-step RAG with reranking and summarization; can be single-pass or agentic iterative.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Scales with corpus size — in practice used over entire accessible literature in experiments (no fixed corpus).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>General scientific literature (used here for biology-focused tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Grounded answers, summaries, and documents produced by LLMs conditioned on retrieved context.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Precision/accuracy on LitQA2, citation recall, human expert evaluation on summarization tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>PaperQA2 (an agentic RAG variant) outperforms standard RAG systems (Perplexity, Elicit) on LitQA2 in reported experiments; non-RAG frontier LLMs underperformed compared to RAG systems.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared to agentic RAG (PaperQA2) and non-RAG LLMs; PaperQA2 modified RAG with RCS and citation traversal to improve performance.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Standard RAG without deep LLM reranking/summarization performed worse than the PaperQA2 pipeline on LitQA2; some RAG systems still outperformed smaller non-RAG LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>RAG reduces hallucination by grounding generation but quality depends on retrieval accuracy and the relevance/conciseness of contexts; enhancing RAG with LLM reranking and contextual summarization (RCS) and iterative search improves results.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Challenge of selecting correct snippets from very large literature, tradeoff between context size and model distraction, and sensitivity to retrieval errors.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>RAG effectiveness depends on retrieval depth and reranking — deeper retrieval lists and stronger rerankers improve recall but increase cost; hierarchical/citation-based indexing can improve scalability and recall.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'L ANGUAGE AGENTS ACHIEVE SUPERHUMAN SYNTHESIS OF SCIENTIFIC KNOWLEDGE', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4592.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e4592.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RCS</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reranking and Contextual Summarization (RCS)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A map-style LLM-driven reranking and summarization step applied to top-k retrieved chunks to produce concise structured summaries and relevance scores, used to filter and condense evidence before final generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Reranking and Contextual Summarization (RCS)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>After top-k vector retrieval returns candidate chunks, RCS runs an LLM completion on each chunk with the user query, prompting the model to output a JSON object containing a short summary (200–400 tokens) and an integer relevance_score (0–10). These structured summaries are parsed, reranked by relevance_score, and only top contextual summaries are passed to the final Generate Answer tool. RCS also extracts metadata (e.g., gene names) used to reduce hallucination and improve citation attribution.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Best results obtained using GPT-4-Turbo for RCS; experiments showed smaller models (GPT-3.5-Turbo, Llama3 70B) sometimes degraded accuracy when used for RCS.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>LLM-based map-style summarization of retrieved chunks into structured JSON (summary + relevance score + optional metadata), performed in parallel over top-k chunks.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>RCS condenses many long chunks into concise structured inputs that are then combined for the final LLM synthesis; acts as a filter to prevent irrelevant context from entering generation prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Applied to top-k chunks per query; default experiments used consider_sources (top-k) = 30 chunks, with performance gains up to depth >10.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>General scientific literature; used in LitQA2, WikiCrow, and ContraCrow pipelines.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Structured contextual summaries (JSON) with relevance scores and optional metadata for downstream synthesis.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>LitQA2 accuracy and precision with/without RCS; ablations measuring accuracy improvement when RCS is included.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>No RCS ablation significantly decreased retrieval accuracy (t(3.92)=9.29, p<0.001). Increasing RCS depth from 1 to 10 significantly increased accuracy (t(2.15)=5.44, p=0.014); default RCS depth = 30 chunks in LitQA experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared to pipelines that inject top-k raw chunks directly into generation context (no RCS).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>RCS substantially increases DOI recall and LitQA2 accuracy compared to no-RCS; effectiveness depends on the RCS model quality (stronger LLMs yield better reranking/summaries).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Structured, LLM-based per-chunk summarization + reranking is critical to scale injection of many documents into the final prompt while reducing irrelevant distraction; RCS enables examination of far more text per query at tolerable token cost.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>RCS adds API cost and latency (parallelizable but resource-intensive); small/insufficient LLMs used for RCS can reduce overall accuracy; requires prompt engineering to produce reliable structured JSON outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>RCS depth (number of chunks summarized) has diminishing returns after ~10–30; using a high-performing LLM for RCS scales better (improves accuracy) than using cheaper/smaller models.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'L ANGUAGE AGENTS ACHIEVE SUPERHUMAN SYNTHESIS OF SCIENTIFIC KNOWLEDGE', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4592.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e4592.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CitationTraversal</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Citation Traversal Tool</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A retrieval enhancement that expands candidate documents by traversing one degree of the citation graph (past references and future citers) from highly-scored evidence summaries, using overlap filtering to select likely-relevant papers.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Citation Traversal</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Given a set of highly-scored contextual summaries (RCS score ≥ threshold), the tool queries citation APIs (Semantic Scholar, Crossref) for past references and future citers, merges deduplicated DOIs, bins candidate citations by overlap across source papers, applies an overlap-fraction threshold α (default 1/3) and a per-traversal limit ℓ (default 12) to select top candidates (preferentially those cited by multiple source papers and with more future citers). Selected papers are then parsed and fed back into the Gather Evidence flow.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>No direct LLM required for traversal logic itself; traversal integrates with PaperQA2 which uses LLMs for downstream RCS and generation.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Graph-based expansion of candidate paper set using citation metadata from APIs; overlap-based filtering and prioritization by future-citer counts.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Hierarchical indexing via citation graph to discover related work that may not surface via keyword retrieval, then synthesizing evidence from the expanded set through RCS and generation.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Traversal produces variable numbers; paper reports many bins with one or more overlaps — practical traversal limited by ℓ (12) per invocation. In LitQA runs default consider_sources and traversal stats reported across experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>General scientific literature.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Expanded set of candidate DOIs/papers to be considered for evidence and final synthesis.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Measured effect on DOI recall and accuracy on LitQA2 when ablated (No Cit.Trav.).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Ablating Citation Traversal increased accuracy in one test (t(2.55)=2.14, p=0.069) and significantly increased DOI recall (t(3)=3.4, p=0.022) across PaperQA2 stages. The tool mirrors typical human literature exploration and improved retrieval metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared to pipelines without citation-based expansion.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Citation traversal increased DOI recall and generally improved retrieval completeness compared to not using traversal.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Citation-based hierarchical indexing is an effective complement to keyword/vector retrieval for finding relevant papers that may be missed by initial searches; overlap filtering (citations appearing in multiple source papers) helps prioritize high-value candidates.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Depends on coverage and metadata quality from citation APIs (Semantic Scholar, Crossref), which can return partial metadata and require best-effort deduplication; access/licensing constraints may prevent retrieval of full text for some traversed DOIs.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Traversal is bounded by configured limits (ℓ) and overlap thresholds; effectiveness increases when multiple source papers point to overlapping citations, but broad traversal can increase API calls and parsing costs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'L ANGUAGE AGENTS ACHIEVE SUPERHUMAN SYNTHESIS OF SCIENTIFIC KNOWLEDGE', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4592.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e4592.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ClaimExtractionPipeline</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-based Claim Extraction and Filtering Pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Procedure that extracts candidate claims from paper chunks via LLM completions, filters them with an LLM-based quality/generalizability scorer, and forwards high-quality claims for contradiction search.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Claim Extraction + LLM Filter</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Papers are split into section-constrained chunks (≤5000 characters). Each chunk is passed to an LLM completion to enumerate candidate claims. A secondary LLM filter then scores each claim (0–10) for quality and generalizability; only claims scoring ≥8 are considered for contradiction detection. This pipeline aims to produce actionable, testable claims for downstream literature comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Claim generation and filtering used gpt-4-turbo-2024-04-09 in the described experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Structured prompting to extract candidate claims from text chunks; LLM-based scoring to filter low-quality or non-generalizable claims.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Produces a set of high-quality claims which are then independently searched and evaluated against the literature by PaperQA2/ContraCrow.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Applied to 93 biology papers in the contradiction-detection study; generated ~3,180 total claims.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Scientific papers (biology sample in experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Filtered list of candidate claims (with quality scores) for contradiction analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Downstream contradiction detection performance (precision/recall, human validation of flagged contradictions); internal claim-quality score thresholding.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Filtering to claims scored ≥8 reduced candidate set to claims considered high-quality for contradiction detection; overall contradiction validation showed 70% human agreement on sampled flagged claims (indirect evidence of pipeline effectiveness).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared implicitly to brute-force claim extraction approaches (no explicit baseline reported).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>No direct baseline numbers given for claim extraction alone, but using this pipeline enabled ContraCrow to operate at scale and maintain human-validated precision.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Filtering extracted claims with an LLM quality scorer is an effective gating mechanism to reduce noisy or trivial claims and focus contradiction search on generalizable assertions.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Claim definition ambiguity (not all extracted statements are good claims), sensitivity to chunk boundaries and parsing quality; potential for missing multi-sentence or implicit claims.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Scales to thousands of claims with batching and parallel LLM calls, but cost and latency increase with corpus size and claim-generation depth.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'L ANGUAGE AGENTS ACHIEVE SUPERHUMAN SYNTHESIS OF SCIENTIFIC KNOWLEDGE', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Paperqa: Retrieval-augmented generative agent for scientific research <em>(Rating: 2)</em></li>
                <li>Retrieval augmentation reduces hallucination in conversation <em>(Rating: 1)</em></li>
                <li>Retrieval-augmented generation for knowledgeintensive nlp tasks <em>(Rating: 2)</em></li>
                <li>Generationaugmented retrieval for open-domain question answering <em>(Rating: 2)</em></li>
                <li>Averitec: A dataset for real-world claim verification with evidence from the web <em>(Rating: 2)</em></li>
                <li>Retrieval-augmented generation for large language models: A survey <em>(Rating: 2)</em></li>
                <li>Exploring the challenges of open domain multi-document summarization <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4592",
    "paper_id": "paper-272827223",
    "extraction_schema_id": "extraction-schema-100",
    "extracted_data": [
        {
            "name_short": "PaperQA2",
            "name_full": "PaperQA2",
            "brief_description": "An agentic, retrieval-augmented-generation (RAG) system designed to search, extract, and synthesize information from the full scientific literature using multi-step tools (paper search, gather evidence with reranking+summarization, citation traversal, and generate-answer). It is optimized for factuality and evaluated on retrieval, summarization, and contradiction-detection tasks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "PaperQA2",
            "system_description": "An agent-based RAG system that decomposes literature search and answer synthesis into explicit tools orchestrated by an LLM agent. Key components are: (1) Paper Search — agent-generated keyword queries to external search APIs (e.g., Semantic Scholar) that return candidate papers; (2) Document parsing (Grobid or PyMuPDF) and chunking; (3) Embedding-based top-k dense retrieval over chunks; (4) RCS (Reranking and Contextual Summarization) — map LLM completions produce structured JSON summaries and relevance scores for each chunk; (5) Citation Traversal — one-hop traversal of past references and future citers to expand candidate set with overlap filtering; (6) Generate Answer — a final LLM prompt that consumes the top contextual summaries to produce a cited, Wikipedia-style answer. The agent can iteratively call tools (search → gather evidence → traverse citations → re-gather → generate) and revise queries.",
            "llm_model_used": "Agent orchestration: gpt-4-turbo-2024-04-09; RCS (summarization & reranking): GPT-4-Turbo (best-performing); Generate Answer: tested with Claude-3-Opus, Gemini-1.5-Pro, GPT-4-Turbo, GPT-3.5-Turbo; ContraCrow/contradiction step used Claude 3.5 Sonnet for some experiments; embedding model: OpenAI text-embedding-large-3 (hybrid with sparse keyword encoding).",
            "extraction_technique": "Hybrid embedding-based dense vector retrieval (top-k) over parsed document chunks + LLM-based reranking and structured contextual summarization (RCS) that outputs JSON summaries and relevance scores; chunking via Grobid/PyMuPDF; citation-graph traversal to expand candidate documents.",
            "synthesis_technique": "Multi-step agentic RAG: combine top-ranked contextual summaries into a final context for LLM generation; citation-traversal hierarchical indexing; iterative query refinement by the agent; generation constrained to cite only provided contexts (Wikipedia-style output).",
            "number_of_papers": "On LitQA2 questions, PaperQA2 parsed and utilized an average of 14.5 ± 0.6 papers per question (runs n=3). Experiment corpus: LitQA2 (248 Qs).",
            "domain_or_topic": "General scientific literature with evaluation focused on biology papers and human protein-coding gene literature (biomedical domain emphasized in summarization/contradiction tasks).",
            "output_type": "Cited, Wikipedia-style summaries (articles), direct answers to retrieval questions (multiple-choice mapping), and contradiction detection reports (claim-level reasoning with Likert score).",
            "evaluation_metrics": "Precision (fraction correct when answered), Accuracy (fraction correct overall), DOI recall, human expert ratings for summaries (cited & supported vs cited & unsupported), AUC/ROC for contradiction benchmark, human validation rates for contradictions.",
            "performance_results": "On LitQA2 (248 Qs): precision 85.2% ± 1.1% (mean ± SD, n=3 runs), accuracy 66.0% ± 1.2%; chose 'insufficient information' in 21.9% ± 0.9% of answers. On WikiCrow summarization: generated 240 gene articles, WikiCrow precision (cited & supported / cited) = 86.1% vs Wikipedia 71.2%; cited & unsupported 13.5% (WikiCrow) vs 24.9% (Wikipedia).",
            "comparison_baseline": "Compared against human domain experts (PhD students/postdocs) using full internet and institutional access, previous PaperQA configuration, commercial RAG systems (Perplexity, Elicit), and frontier non-RAG LLMs.",
            "performance_vs_baseline": "PaperQA2 achieved superhuman precision vs experts on LitQA2 (human precision 73.8% ± 9.6%; PaperQA2 precision 85.2% ± 1.1%; t-test p=0.0036) and matched human accuracy (human accuracy 67.7% ± 11.9%; PaperQA2 66.0%, not significantly different). It outperformed the prior PaperQA config (accuracy 36.7%, precision 76.5%) and other RAG systems tested on LitQA2.",
            "key_findings": "Agentic multi-step RAG with deep LLM-based reranking/summarization (RCS) and citation traversal substantially improves retrieval precision/recall and factuality compared to simpler RAG or non-agentic pipelines; using a high-quality LLM for RCS (GPT-4-Turbo) and a deep RCS list (&gt;10) were crucial; agentic ability to reformulate searches improves recall over fixed pipelines.",
            "limitations_challenges": "High computational and API cost (PaperQA2 reported $1–$3 per query; WikiCrow ~$4.48 per article); dependency on access to full text repositories and licensing; hallucination/reasoning errors remain possible; performance sensitive to LLM choice and RCS depth; smaller LLMs performing RCS can degrade accuracy; infrastructure complexity for large-scale deployment.",
            "scaling_behavior": "Performance improves with deeper RCS ranking depth up to diminishing returns (notable gains from 1→10, marginal gains 10→30); better parsing (Grobid) reduced tokenization and aided summarization of tables for WikiCrow; higher-capacity LLMs for RCS increase accuracy (GPT-4-Turbo &gt; smaller models); citation traversal and iterative search increase DOI recall and accuracy; tradeoff observed between number of contexts injected and precision vs accuracy (more contexts → higher precision, fewer contexts → higher accuracy in some settings).",
            "uuid": "e4592.0",
            "source_info": {
                "paper_title": "L ANGUAGE AGENTS ACHIEVE SUPERHUMAN SYNTHESIS OF SCIENTIFIC KNOWLEDGE",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "PaperQA",
            "name_full": "PaperQA (original)",
            "brief_description": "Prior retrieval-augmented generative agent for scientific research that inspired PaperQA2; used RAG with an RCS-like pipeline but with smaller RCS models and less tooling (no citation traversal, fewer reranking depth).",
            "citation_title": "Paperqa: Retrieval-augmented generative agent for scientific research",
            "mention_or_use": "use",
            "system_name": "PaperQA (original)",
            "system_description": "A RAG agent that performs literature search, document parsing, chunking, top-k retrieval and LLM-based generation. PaperQA provided the foundational agentic tool abstraction (paper_search, gather_evidence, generate_answer) and a PromptCollection mechanism; PaperQA2 extends and optimizes this pipeline.",
            "llm_model_used": "Originally used smaller RCS models (GPT-3.5-Turbo in prior config) and fewer deep reranking steps (per paper's ablation description).",
            "extraction_technique": "Embedding-based retrieval and LLM summarization of chunks; earlier RCS used a cheaper model for reranking/summarization.",
            "synthesis_technique": "Single-pass RAG generation from selected chunks without the expanded agentic citation traversal and deeper RCS depth used in PaperQA2.",
            "number_of_papers": "Not explicitly specified in this paper for original runs; used as baseline on LitQA2 benchmark.",
            "domain_or_topic": "General scientific literature; prior work targeted literature-enabled tasks.",
            "output_type": "Answers to literature questions and summaries.",
            "evaluation_metrics": "LitQA2 accuracy and precision as reported for comparison.",
            "performance_results": "Prior PaperQA configuration: LitQA2 accuracy 36.7%, precision 76.5% (reported in paper as baseline).",
            "comparison_baseline": "Compared as a baseline against PaperQA2 and other systems like Perplexity and Elicit.",
            "performance_vs_baseline": "PaperQA2 substantially improved accuracy and precision relative to the original PaperQA config by using larger RCS LLMs, citation traversal, and deeper RCS depth.",
            "key_findings": "Shows that improvements in RCS model quality, deeper reranking, and added citation traversal materially improve retrieval accuracy and factual output quality.",
            "limitations_challenges": "Earlier limitations included lower RCS model capacity and missing tools (e.g., citation traversal), resulting in lower accuracy.",
            "scaling_behavior": "Not extensively detailed here beyond implication that scaling model capacity for the reranker and deeper ranking lists improves performance.",
            "uuid": "e4592.1",
            "source_info": {
                "paper_title": "L ANGUAGE AGENTS ACHIEVE SUPERHUMAN SYNTHESIS OF SCIENTIFIC KNOWLEDGE",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "WikiCrow",
            "name_full": "WikiCrow",
            "brief_description": "A pipeline built on PaperQA2 to generate cited, Wikipedia-style articles about human protein-coding genes by issuing multiple PaperQA2 calls per article section and stitching outputs into full articles.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "WikiCrow",
            "system_description": "A multi-query orchestration over PaperQA2: for each target gene, WikiCrow issues a set of section-specific PaperQA2 queries (structure, function, interactions, clinical significance, etc.), uses Grobid parsing and RCS prompts tailored to extract gene names and tabular data, then stitches the section outputs into a single Wikipedia-style article. Article generation includes structured citation insertion and post-processing for consistency.",
            "llm_model_used": "PaperQA2 backend models (agent orchestration gpt-4-turbo-2024-04-09), RCS summarizers (GPT-4-Turbo), and an overview LLM call (GPT-4-Turbo); generation used the same Generate Answer tool settings.",
            "extraction_technique": "Targeted PaperQA2 queries per article section, Grobid-based parsing for higher-fidelity section/table extraction, RCS extraction of gene names and structured JSON outputs for reliable citation association.",
            "synthesis_technique": "Section-wise multi-query generation followed by programmatic stitching into a single article; reliance on citation-aware RAG outputs to ensure factual support.",
            "number_of_papers": "Generated 240 articles on genes for evaluation (paired with 240 Wikipedia articles). Underlying corpus size varied; Grobid/pyMuPDF token comparisons reported across 5,363 papers sample for parser token statistics.",
            "domain_or_topic": "Human protein-coding gene literature (biomedical domain).",
            "output_type": "Cited, Wikipedia-style full-length articles (structured sections).",
            "evaluation_metrics": "Human expert grading of sampled statements for 'Is it cited?' and 'Is information correct as cited?'; counts of 'cited and unsupported' statements; precision defined as cited-and-supported / cited.",
            "performance_results": "WikiCrow articles averaged 1219.0 ± 275.0 words; cost ~ $4.48 ± $1.02 per article; in evaluation (375 statements from 240 paired articles) WikiCrow had 13.5% cited-and-unsupported vs Wikipedia 24.9% (p=0.0075). WikiCrow had fewer uncited statements (3.5% vs 13.6%) and higher precision (86.1% vs 71.2%, p=0.0013).",
            "comparison_baseline": "Compared against existing human-written Wikipedia articles on the same genes and evaluated by blinded expert graders.",
            "performance_vs_baseline": "WikiCrow produced significantly fewer cited-but-unsupported statements and higher cited-precision than Wikipedia in the sampled set.",
            "key_findings": "Agentic RAG with Grobid parsing and targeted RCS metadata extraction (gene name) reduces hallucinations and attribution errors relative to Wikipedia articles; LLM-based summarization can be more factually accurate than crowd-written summaries in this evaluation setting.",
            "limitations_challenges": "Depends on access to full texts, Grobid parsing availability; cost per article non-negligible; potential attribution issues and remaining hallucination/reasoning errors (though fewer than Wikipedia in this task).",
            "scaling_behavior": "Parser choice (Grobid) reduced token usage (approx. 8,903 tokens/paper vs 16,040 tokens for PyMuPDF in a sample), enabling more efficient scaling; RCS depth and high-quality summarizer remain important for fidelity when synthesizing multi-paper sections.",
            "uuid": "e4592.2",
            "source_info": {
                "paper_title": "L ANGUAGE AGENTS ACHIEVE SUPERHUMAN SYNTHESIS OF SCIENTIFIC KNOWLEDGE",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "ContraCrow",
            "name_full": "ContraCrow",
            "brief_description": "A system built on PaperQA2 to detect contradictions in the scientific literature by extracting claims from papers, searching literature for counter-evidence, and scoring contradictions on an 11-point Likert scale with LLM reasoning.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "ContraCrow",
            "system_description": "Two-stage system: (1) claim extraction — split paper sections (≤5000 characters per chunk) and run LLM completions to propose candidate claims; filter claims with an LLM-based quality/generalizability scorer (keep claims scoring ≥8). (2) contradiction search — feed each claim into PaperQA2 with a contradiction-detection prompt instructing literature search, evidence gathering, and an LLM-generated Likert score (0–10) plus reasoning. Uses PaperQA2 tools (paper_search, gather_evidence with RCS, citation traversal) for evidence collection; outputs include model reasoning, evidence citations, and Likert score.",
            "llm_model_used": "Claim extraction used gpt-4-turbo-2024-04-09 for generation and filtering; contradiction detection used Claude 3.5 Sonnet (for many experiments) with temperature 0 and chunk size 7000 characters; underlying retrieval uses embeddings (text-embedding-large-3).",
            "extraction_technique": "LLM-based claim extraction from parsed chunks, followed by an LLM filter scoring claims; retrieval via PaperQA2 embedding-based top-k ranking and RCS summarization of chunks to assemble evidence for each claim.",
            "synthesis_technique": "Compare each claim against assembled evidence using LLM reasoning; map natural-language Likert outputs to integer scores and apply decision thresholds to flag contradictions; supports 'contradiction validation' pipelines where human experts review model-cited chunks and reasoning.",
            "number_of_papers": "Applied to 93 randomly selected biology papers (from a 100-paper sample where 7 failed to parse). Processed 3,180 claims total (mean 35.16 ± 21.72 claims per paper).",
            "domain_or_topic": "Biology literature (random biology papers sample) and general scientific claim verification tasks (benchmarked with ContraDetect derived from LitQA2).",
            "output_type": "Claim-level contradiction flags, LLM reasoning text, Likert-scale scores (0–10), and cited evidence chunks.",
            "evaluation_metrics": "ROC/AUC on ContraDetect benchmark (AUC 0.842), precision/accuracy at threshold, human validation agreement rates, F1 for validated contradictions.",
            "performance_results": "On ContraDetect: AUC = 0.842; at threshold 8: 73% accuracy, 88% precision, false positive rate 7%. Applied to 93 papers: 6.85% of 3,180 claims flagged as contradicted (scores 8–10); average 2.34 ± 1.99 contradictions per paper (Likert≥8). Human expert validation: sampled claims (50 score=8, 50 score≥9) — 70% agreement with ContraCrow (F1 = 0.82), yielding ~1.64 human-validated contradictions/paper in validation sample.",
            "comparison_baseline": "Benchmarked on ContraDetect (derived from LitQA2) and compared to human expert annotation (validation and detection tasks).",
            "performance_vs_baseline": "High precision in benchmarked setting; human validation agreed with 70% of model-flagged contradictions. Humans agreed with each other more than with ContraCrow on a separate detection task (humans inter-agreement 75.5% ± 13.43% vs agreement with ContraCrow 60.42% ± 5.99%).",
            "key_findings": "LLM-driven claim extraction plus PaperQA2-backed literature search can detect literature contradictions at scale with high precision and interpretable scoring; however the system exhibits overconfidence on some Likert-scale ratings and can disagree with exhaustive human review.",
            "limitations_challenges": "Possible overconfidence in Likert outputs relative to human judgments; dependence on quality of parsed chunks and access to full-text; contradictions may be contextual (not necessarily 'wrong'); claim-extraction quality and filtering thresholds affect recall/precision tradeoffs.",
            "scaling_behavior": "Capable of scaling to thousands of claims (it processed 3,180 claims in batched runs), but performance and cost depend on LLM choice and depth of evidence gathered; batching and parallel RCS help efficiency; human validation remains necessary for high-stakes deployment.",
            "uuid": "e4592.3",
            "source_info": {
                "paper_title": "L ANGUAGE AGENTS ACHIEVE SUPERHUMAN SYNTHESIS OF SCIENTIFIC KNOWLEDGE",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "LitQA2",
            "name_full": "LitQA2",
            "brief_description": "A hard benchmark introduced in this paper consisting of 248 multiple-choice questions designed to require retrieval from the main body (not abstract) of scientific papers; intended to evaluate retrieval and citation-accurate answering across the literature.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "LitQA2 (benchmark)",
            "system_description": "A manually curated dataset of 248 multiple-choice questions where correct answers must be grounded in specific passages in the main body of scientific papers (often obscure experimental findings); creators ensured answers weren't present in abstracts and excluded questions where alternative sources could answer them. Includes 'insufficient information' label for unanswerable items. Used to evaluate precision, accuracy, and DOI recall by matching system-cited DOI to the creator's DOI.",
            "llm_model_used": "Not an LLM system; used to evaluate LLM-based systems including PaperQA2. Question generation used experts and iterative testing with ChatGPT for distractor design, but LitQA2 itself is a benchmark.",
            "extraction_technique": "N/A (benchmark); evaluation depends on systems' extraction techniques (DOI-level matching required).",
            "synthesis_technique": "N/A (benchmark used to evaluate synthesis and retrieval of systems).",
            "number_of_papers": "Questions were generated about obscure findings from very recent papers; corpus size implicit but LitQA2 comprises 248 Qs referencing specific papers (47 original LitQA + 100 + 101 additions).",
            "domain_or_topic": "Scientific literature broadly; many questions focused on biology.",
            "output_type": "Multiple-choice answers with associated cited DOI (used to compute precision/accuracy/recall).",
            "evaluation_metrics": "Precision (fraction correct when answered), Accuracy (fraction correct overall), DOI recall (fraction of answers attributed to the correct DOI), and abstention rate ('insufficient information').",
            "performance_results": "Used to measure PaperQA2: precision 85.2% ±1.1, accuracy 66.0% ±1.2, abstain rate 21.9% ±0.9. Human expert performance reported: precision 73.8% ±9.6, accuracy 67.7% ±11.9.",
            "comparison_baseline": "Human experts, other RAG systems (Perplexity, Elicit), frontier non-RAG LLMs, and prior PaperQA configuration.",
            "performance_vs_baseline": "PaperQA2 outperformed humans in precision on LitQA2 and outperformed other RAG and non-RAG baselines in the experiments reported.",
            "key_findings": "LitQA2 is a challenging, high-fidelity benchmark that forces systems to retrieve specific main-text evidence; it exposed differences in DOI recall and demonstrated utility of agentic retrieval+RCS approaches.",
            "limitations_challenges": "Creation is expert-intensive (manual curation); potential indexation leakage for human evaluators when questions get indexed online (noted contamination in one human quiz round).",
            "scaling_behavior": "Designed as a static benchmark; scaling concerns relate to dataset expansion and potential automation of question generation (emergent ideas mentioned).",
            "uuid": "e4592.4",
            "source_info": {
                "paper_title": "L ANGUAGE AGENTS ACHIEVE SUPERHUMAN SYNTHESIS OF SCIENTIFIC KNOWLEDGE",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "RAG",
            "name_full": "Retrieval-Augmented Generation",
            "brief_description": "A general approach where an LLM is augmented with externally retrieved context (snippets or documents) to ground generated outputs; widely used as the paradigm for literature-enabled LLM systems.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "Retrieval-Augmented Generation (RAG)",
            "system_description": "High-level approach: retrieve relevant documents or passages for a query using vector/sparse retrieval, then include retrieved context in the prompt for an LLM to generate a grounded answer. In this paper, RAG is the baseline paradigm that PaperQA2 builds on but decomposes into agentic tools and augments with RCS and citation traversal.",
            "llm_model_used": "Applicable to many LLMs; in experiments the generate-answer step used Claude-3-Opus, GPT-4-Turbo, Gemini-1.5-Pro, GPT-3.5-Turbo in various ablations.",
            "extraction_technique": "Embedding-based retrieval and top-k selection of chunks, then direct insertion of chunks into generation context (though PaperQA2 modifies this with RCS).",
            "synthesis_technique": "Direct RAG generation or multi-step RAG with reranking and summarization; can be single-pass or agentic iterative.",
            "number_of_papers": "Scales with corpus size — in practice used over entire accessible literature in experiments (no fixed corpus).",
            "domain_or_topic": "General scientific literature (used here for biology-focused tasks).",
            "output_type": "Grounded answers, summaries, and documents produced by LLMs conditioned on retrieved context.",
            "evaluation_metrics": "Precision/accuracy on LitQA2, citation recall, human expert evaluation on summarization tasks.",
            "performance_results": "PaperQA2 (an agentic RAG variant) outperforms standard RAG systems (Perplexity, Elicit) on LitQA2 in reported experiments; non-RAG frontier LLMs underperformed compared to RAG systems.",
            "comparison_baseline": "Compared to agentic RAG (PaperQA2) and non-RAG LLMs; PaperQA2 modified RAG with RCS and citation traversal to improve performance.",
            "performance_vs_baseline": "Standard RAG without deep LLM reranking/summarization performed worse than the PaperQA2 pipeline on LitQA2; some RAG systems still outperformed smaller non-RAG LLMs.",
            "key_findings": "RAG reduces hallucination by grounding generation but quality depends on retrieval accuracy and the relevance/conciseness of contexts; enhancing RAG with LLM reranking and contextual summarization (RCS) and iterative search improves results.",
            "limitations_challenges": "Challenge of selecting correct snippets from very large literature, tradeoff between context size and model distraction, and sensitivity to retrieval errors.",
            "scaling_behavior": "RAG effectiveness depends on retrieval depth and reranking — deeper retrieval lists and stronger rerankers improve recall but increase cost; hierarchical/citation-based indexing can improve scalability and recall.",
            "uuid": "e4592.5",
            "source_info": {
                "paper_title": "L ANGUAGE AGENTS ACHIEVE SUPERHUMAN SYNTHESIS OF SCIENTIFIC KNOWLEDGE",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "RCS",
            "name_full": "Reranking and Contextual Summarization (RCS)",
            "brief_description": "A map-style LLM-driven reranking and summarization step applied to top-k retrieved chunks to produce concise structured summaries and relevance scores, used to filter and condense evidence before final generation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Reranking and Contextual Summarization (RCS)",
            "system_description": "After top-k vector retrieval returns candidate chunks, RCS runs an LLM completion on each chunk with the user query, prompting the model to output a JSON object containing a short summary (200–400 tokens) and an integer relevance_score (0–10). These structured summaries are parsed, reranked by relevance_score, and only top contextual summaries are passed to the final Generate Answer tool. RCS also extracts metadata (e.g., gene names) used to reduce hallucination and improve citation attribution.",
            "llm_model_used": "Best results obtained using GPT-4-Turbo for RCS; experiments showed smaller models (GPT-3.5-Turbo, Llama3 70B) sometimes degraded accuracy when used for RCS.",
            "extraction_technique": "LLM-based map-style summarization of retrieved chunks into structured JSON (summary + relevance score + optional metadata), performed in parallel over top-k chunks.",
            "synthesis_technique": "RCS condenses many long chunks into concise structured inputs that are then combined for the final LLM synthesis; acts as a filter to prevent irrelevant context from entering generation prompts.",
            "number_of_papers": "Applied to top-k chunks per query; default experiments used consider_sources (top-k) = 30 chunks, with performance gains up to depth &gt;10.",
            "domain_or_topic": "General scientific literature; used in LitQA2, WikiCrow, and ContraCrow pipelines.",
            "output_type": "Structured contextual summaries (JSON) with relevance scores and optional metadata for downstream synthesis.",
            "evaluation_metrics": "LitQA2 accuracy and precision with/without RCS; ablations measuring accuracy improvement when RCS is included.",
            "performance_results": "No RCS ablation significantly decreased retrieval accuracy (t(3.92)=9.29, p&lt;0.001). Increasing RCS depth from 1 to 10 significantly increased accuracy (t(2.15)=5.44, p=0.014); default RCS depth = 30 chunks in LitQA experiments.",
            "comparison_baseline": "Compared to pipelines that inject top-k raw chunks directly into generation context (no RCS).",
            "performance_vs_baseline": "RCS substantially increases DOI recall and LitQA2 accuracy compared to no-RCS; effectiveness depends on the RCS model quality (stronger LLMs yield better reranking/summaries).",
            "key_findings": "Structured, LLM-based per-chunk summarization + reranking is critical to scale injection of many documents into the final prompt while reducing irrelevant distraction; RCS enables examination of far more text per query at tolerable token cost.",
            "limitations_challenges": "RCS adds API cost and latency (parallelizable but resource-intensive); small/insufficient LLMs used for RCS can reduce overall accuracy; requires prompt engineering to produce reliable structured JSON outputs.",
            "scaling_behavior": "RCS depth (number of chunks summarized) has diminishing returns after ~10–30; using a high-performing LLM for RCS scales better (improves accuracy) than using cheaper/smaller models.",
            "uuid": "e4592.6",
            "source_info": {
                "paper_title": "L ANGUAGE AGENTS ACHIEVE SUPERHUMAN SYNTHESIS OF SCIENTIFIC KNOWLEDGE",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "CitationTraversal",
            "name_full": "Citation Traversal Tool",
            "brief_description": "A retrieval enhancement that expands candidate documents by traversing one degree of the citation graph (past references and future citers) from highly-scored evidence summaries, using overlap filtering to select likely-relevant papers.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Citation Traversal",
            "system_description": "Given a set of highly-scored contextual summaries (RCS score ≥ threshold), the tool queries citation APIs (Semantic Scholar, Crossref) for past references and future citers, merges deduplicated DOIs, bins candidate citations by overlap across source papers, applies an overlap-fraction threshold α (default 1/3) and a per-traversal limit ℓ (default 12) to select top candidates (preferentially those cited by multiple source papers and with more future citers). Selected papers are then parsed and fed back into the Gather Evidence flow.",
            "llm_model_used": "No direct LLM required for traversal logic itself; traversal integrates with PaperQA2 which uses LLMs for downstream RCS and generation.",
            "extraction_technique": "Graph-based expansion of candidate paper set using citation metadata from APIs; overlap-based filtering and prioritization by future-citer counts.",
            "synthesis_technique": "Hierarchical indexing via citation graph to discover related work that may not surface via keyword retrieval, then synthesizing evidence from the expanded set through RCS and generation.",
            "number_of_papers": "Traversal produces variable numbers; paper reports many bins with one or more overlaps — practical traversal limited by ℓ (12) per invocation. In LitQA runs default consider_sources and traversal stats reported across experiments.",
            "domain_or_topic": "General scientific literature.",
            "output_type": "Expanded set of candidate DOIs/papers to be considered for evidence and final synthesis.",
            "evaluation_metrics": "Measured effect on DOI recall and accuracy on LitQA2 when ablated (No Cit.Trav.).",
            "performance_results": "Ablating Citation Traversal increased accuracy in one test (t(2.55)=2.14, p=0.069) and significantly increased DOI recall (t(3)=3.4, p=0.022) across PaperQA2 stages. The tool mirrors typical human literature exploration and improved retrieval metrics.",
            "comparison_baseline": "Compared to pipelines without citation-based expansion.",
            "performance_vs_baseline": "Citation traversal increased DOI recall and generally improved retrieval completeness compared to not using traversal.",
            "key_findings": "Citation-based hierarchical indexing is an effective complement to keyword/vector retrieval for finding relevant papers that may be missed by initial searches; overlap filtering (citations appearing in multiple source papers) helps prioritize high-value candidates.",
            "limitations_challenges": "Depends on coverage and metadata quality from citation APIs (Semantic Scholar, Crossref), which can return partial metadata and require best-effort deduplication; access/licensing constraints may prevent retrieval of full text for some traversed DOIs.",
            "scaling_behavior": "Traversal is bounded by configured limits (ℓ) and overlap thresholds; effectiveness increases when multiple source papers point to overlapping citations, but broad traversal can increase API calls and parsing costs.",
            "uuid": "e4592.7",
            "source_info": {
                "paper_title": "L ANGUAGE AGENTS ACHIEVE SUPERHUMAN SYNTHESIS OF SCIENTIFIC KNOWLEDGE",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "ClaimExtractionPipeline",
            "name_full": "LLM-based Claim Extraction and Filtering Pipeline",
            "brief_description": "Procedure that extracts candidate claims from paper chunks via LLM completions, filters them with an LLM-based quality/generalizability scorer, and forwards high-quality claims for contradiction search.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Claim Extraction + LLM Filter",
            "system_description": "Papers are split into section-constrained chunks (≤5000 characters). Each chunk is passed to an LLM completion to enumerate candidate claims. A secondary LLM filter then scores each claim (0–10) for quality and generalizability; only claims scoring ≥8 are considered for contradiction detection. This pipeline aims to produce actionable, testable claims for downstream literature comparison.",
            "llm_model_used": "Claim generation and filtering used gpt-4-turbo-2024-04-09 in the described experiments.",
            "extraction_technique": "Structured prompting to extract candidate claims from text chunks; LLM-based scoring to filter low-quality or non-generalizable claims.",
            "synthesis_technique": "Produces a set of high-quality claims which are then independently searched and evaluated against the literature by PaperQA2/ContraCrow.",
            "number_of_papers": "Applied to 93 biology papers in the contradiction-detection study; generated ~3,180 total claims.",
            "domain_or_topic": "Scientific papers (biology sample in experiments).",
            "output_type": "Filtered list of candidate claims (with quality scores) for contradiction analysis.",
            "evaluation_metrics": "Downstream contradiction detection performance (precision/recall, human validation of flagged contradictions); internal claim-quality score thresholding.",
            "performance_results": "Filtering to claims scored ≥8 reduced candidate set to claims considered high-quality for contradiction detection; overall contradiction validation showed 70% human agreement on sampled flagged claims (indirect evidence of pipeline effectiveness).",
            "comparison_baseline": "Compared implicitly to brute-force claim extraction approaches (no explicit baseline reported).",
            "performance_vs_baseline": "No direct baseline numbers given for claim extraction alone, but using this pipeline enabled ContraCrow to operate at scale and maintain human-validated precision.",
            "key_findings": "Filtering extracted claims with an LLM quality scorer is an effective gating mechanism to reduce noisy or trivial claims and focus contradiction search on generalizable assertions.",
            "limitations_challenges": "Claim definition ambiguity (not all extracted statements are good claims), sensitivity to chunk boundaries and parsing quality; potential for missing multi-sentence or implicit claims.",
            "scaling_behavior": "Scales to thousands of claims with batching and parallel LLM calls, but cost and latency increase with corpus size and claim-generation depth.",
            "uuid": "e4592.8",
            "source_info": {
                "paper_title": "L ANGUAGE AGENTS ACHIEVE SUPERHUMAN SYNTHESIS OF SCIENTIFIC KNOWLEDGE",
                "publication_date_yy_mm": "2024-09"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Paperqa: Retrieval-augmented generative agent for scientific research",
            "rating": 2
        },
        {
            "paper_title": "Retrieval augmentation reduces hallucination in conversation",
            "rating": 1
        },
        {
            "paper_title": "Retrieval-augmented generation for knowledgeintensive nlp tasks",
            "rating": 2
        },
        {
            "paper_title": "Generationaugmented retrieval for open-domain question answering",
            "rating": 2
        },
        {
            "paper_title": "Averitec: A dataset for real-world claim verification with evidence from the web",
            "rating": 2
        },
        {
            "paper_title": "Retrieval-augmented generation for large language models: A survey",
            "rating": 2
        },
        {
            "paper_title": "Exploring the challenges of open domain multi-document summarization",
            "rating": 1
        }
    ],
    "cost": 0.02509575,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>LANGUAGE AGENTS ACHIEVE SUPERHUMAN SYNTHESIS OF SCIENTIFIC KNOWLEDGE
26 Sep 2024</p>
<p>Michael D Skarlinski 
FutureHouse Inc
San FranciscoCA</p>
<p>Sam Cox 
FutureHouse Inc
San FranciscoCA</p>
<p>University of Rochester
RochesterNY</p>
<p>Jon M Laurent 
FutureHouse Inc
San FranciscoCA</p>
<p>James D Braza 
FutureHouse Inc
San FranciscoCA</p>
<p>Michaela Hinks 
FutureHouse Inc
San FranciscoCA</p>
<p>Michael J Hammerling 
FutureHouse Inc
San FranciscoCA</p>
<p>Manvitha Ponnapati 
FutureHouse Inc
San FranciscoCA</p>
<p>Samuel G Rodriques 
FutureHouse Inc
San FranciscoCA</p>
<p>Francis Crick Institute
LondonUK</p>
<p>Andrew D White andrew@futurehouse.org 
FutureHouse Inc
San FranciscoCA</p>
<p>University of Rochester
RochesterNY</p>
<p>LANGUAGE AGENTS ACHIEVE SUPERHUMAN SYNTHESIS OF SCIENTIFIC KNOWLEDGE
26 Sep 202498498D73A874A2EDA78F3F8AB395A4B6arXiv:2409.13740v2[cs.CL]
Language models are known to "hallucinate" incorrect information, and it is unclear if they are sufficiently accurate and reliable for use in scientific research.We developed a rigorous human-AI comparison methodology to evaluate language model agents on real-world literature search tasks covering information retrieval, summarization, and contradiction detection tasks.We show that PaperQA2, a frontier language model agent optimized for improved factuality, matches or exceeds subject matter expert performance on three realistic literature research tasks without any restrictions on humans (i.e., full access to internet, search tools, and time).PaperQA2 writes cited, Wikipediastyle summaries of scientific topics that are significantly more accurate than existing, human-written Wikipedia articles.We also introduce a hard benchmark for scientific literature research called LitQA2 that guided design of PaperQA2, leading to it exceeding human performance.Finally, we apply PaperQA2 to identify contradictions within the scientific literature, an important scientific task that is challenging for humans.PaperQA2 identifies 2.34 ± 1.99 (mean ± SD, N = 93 papers) contradictions per paper in a random subset of biology papers, of which 70% are validated by human experts.These results demonstrate that language model agents are now capable of exceeding domain experts across meaningful tasks on scientific literature.</p>
<p>Introduction</p>
<p>Large language models (LLMs) have the potential to assist scientists with retrieving, synthesizing, and summarizing the literature 1,2,3 , but still have several limitations for use in research tasks.Firstly, factuality is essential in scientific research, and LLMs hallucinate 4 , confidently stating information that is not grounded in any existing source or evidence.Secondly, science requires extreme attention to detail, and LLMs can overlook or misuse details when faced with challenging reasoning problems 5 .Finally, benchmarks for retrieval and reasoning across the scientific literature today are underdeveloped.They do not consider the entire literature, but instead are restricted to abstracts 6 , retrieval on a fixed corpus 7 , or simply provide the relevant paper directly 8 .These benchmarks are not suitable as performance proxies for real scientific research tasks, and, more importantly, often lack a direct comparison to human performance.Thus, it remains unclear whether language models and agents are suitable for use in scientific research.</p>
<p>We therefore set out to develop a rigorous comparison between the performance of AI systems and humans on three real-world tasks: a retrieval task involving searching the entire literature to answer questions; a summarization task involving producing a cited, Wikipedia-style articles on scientific topics; and a contradiction-detection task, involving extracting all claims from papers and checking them for contradictions against all of literature.This is, to our knowledge, the first robust procedure for evaluating a single AI system on multiple real-world literature search tasks.Using our newly developed evaluations, we explored multiple designs leading to a system we call PaperQA2 (Figure 1A), which exceeds the performance of PhD students and postdocs on the retrieval and summarization tasks.Applying PaperQA2 to the contradiction detection task enables us to identify contradictions in biology papers at scale (Figure 1B).For example, a statement that the ZNF804A rs1344706 allele positively affects brain structure in schizophrenia patients 9 was found to be contradicted by a later publication which found that rs1344706's effects on cortical thickness, surface area, and cortical volume in the brain aggravate the risk of schizophrenia 10 .</p>
<p>Answering scientific questions</p>
<p>To evaluate AI systems on retrieval over the scientific literature, we first generated LitQA2, 11 a set of 248 multiple choice questions with answers that require retrieval from scientific literature (Figure 2A).LitQA2 questions are designed to have answers that appear in the main body of a paper, but not in the abstract, and ideally appear only once in the set of all scientific literature.These constraints enable us to evaluate response accuracy by matching the system's cited source DOI with the DOI originally assigned by the question creator.To enforce these criteria, we generated large numbers of questions about obscure intermediate findings from very recent papers, and then excluded any questions where either an existing AI system or a human annotator could answer the question using an alternative source (Section 8.2.1).These were generated entirely by experts, although there are emerging ideas about how to automate this process 12 .When answering LitQA2 questions, models can refuse to answer via selecting Insufficient information to answer this question.Similar to prior work 13 and matching actual scientific questions, some questions are intended to be unanswerable.We evaluate two metrics: precision, the fraction of questions answered correctly when a response is provided, and accuracy, the fraction of correct answers over all questions.We also consider recall, which is the total percentage of questions where the system attributed its answer to the correct source DOI denoted in LitQA2.</p>
<p>Having developed LitQA2, we then utilized it to design an AI system for the scientific literature.The current paradigm for eliciting factually-based responses from LLMs is to use retrieval-augmented generation (RAG) 14,15 .RAG provides additional context to the LLM (e.g., snippets from research papers) to ground the generated response.As scientific literature is quite large, identifying the correct snippet is a challenge.Strategies like using metadata or hierarchical indexing can improve retrieval in this setting 16 , but finding the correct paper for a task often requires iterating and revising queries.Inspired by PaperQA 17 , PaperQA2 is a RAG agent that treats retrieval and response generation as a multi-step agent task 18 instead of a direct procedure.PaperQA2 decomposes RAG into tools, allowing it to revise its search parameters and to generate and examine candidate answers before producing a final answer (Figure 1A).PaperQA2 has access to a "Paper Search" tool, where the agent model transforms the user request into a keyword search that is used to identify candidate papers.The candidate papers are parsed into machine readable text, and chunked for later usage by the agent.PaperQA2 uses the state-of-the-art document parsing algorithm (Grobid 19 ) that reliably parses sections, tables, and citations from papers.After finding candidates, PaperQA2 can use a "Gather Evidence" tool that first ranks paper chunks with a top-k dense vector retrieval step, followed by an LLM reranking and contextual summarization (RCS) step.RCS prevents irrelevant chunks from appearing in the RAG context by summarizing and scoring the relevance of each chunk, which is known to be critical for RAG 20 .The top ranked contextual summaries are stored in the agent's state for later steps.PaperQA2's design differs from similar RAG systems like Perplexity 21 , Elicit 22 , or Mao et al. 23 which deliver retrieved chunks without substantial transformation in the context of the user query.While RCS is more costly than retrieval without a contextual summary, it allows PaperQA2 to examine much more text per user question.The RCS step also injects metadata about the source paper, like its citation count and journal.Once the PaperQA2 state has summaries, it can call a "Generate Answer" tool which uses the top ranked evidence summaries inside a prompt to an LLM for the final response to the asked questions or assigned task.To further improve recall, PaperQA2 adds a new "Citation Traversal" tool (Section 8.1.1)that exploits the citation graph as a form of hierarchical indexing to add additional relevant sources.</p>
<p>In answering LitQA2 questions, PaperQA2 parsed and utilized an average of 14.5 ± 0.6 (mean ± SD, n = 3) papers per question.Running PaperQA2 on LitQA2 yielded a precision of 85.2% ± 1.1% (mean ± SD, n = 3), and an accuracy of 66.0% ± 1.2% (mean ± SD, n = 3) (Figure 2B), with the system choosing "insufficient information" in 21.9% ± 0.9% (mean ± SD, n = 3) of answers.To compare the performance of PaperQA2 to other retrieval systems, we evaluated the performance of PaperQA with original parameters, commercial systems like Perplexity 21 and Elicit 22 , and frontier (non-RAG models) on LitQA2.We found that PaperQA2 outperforms other RAG systems on the LitQA2 benchmark in both precision and accuracy.We also found that all RAG systems tested, with the exception of Elicit, outperform non-RAG frontier models in both precision and accuracy.</p>
<p>To ensure that we did not overfit PaperQA2 to achieve high performance on LitQA2, we generated a new set of 101 LitQA2 questions after making most of the engineering changes to PaperQA2.The accuracy of PaperQA2 on the original set of 147 questions did not differ significantly from its accuracy on the latter set of 101 questions, indicating that our optimizations in the first stage generalized well to new and unseen LitQA2 questions (Table 2).</p>
<p>To compare PaperQA2 performance to human performance on the same task, human annotators who either possessed a PhD in biology or a related science, or who were enrolled in a PhD program (see Section 8.2.1), were each provided a subset of LitQA2 questions and a performance-related financial incentive of $3-12 per question to answer as many questions correctly as possible within approximately one week, using any online tools and paper access provided by their institutions.Under these conditions, human annotators achieved 73.8% ± 9.6% (mean ± SD, n = 9) precision on LitQA2 and 67.7% ± 11.9% (mean ± SD, n = 9) accuracy (Figure 2A, green line).PaperQA2 thus achieved superhuman precision on this task (t(8.6)= 3.49, p = 0.0036) and did not differ significantly from humans in accuracy (t(8.5)= −0.42,p = 0.66).</p>
<p>Performance analysis of PaperQA2</p>
<p>We varied the parameters of PaperQA2 to understand which are responsible for its accuracy (Figure 2C).We created a non-agentic version (No Agent) which had a hard-coded sequence of actions (paper search, gather evidence, then generate answer).The non-agentic system had significantly lower accuracy (t(3.7)= 3.41, p = 0.015), validating the choice of using an agent.We attribute the performance difference to the agent's better recall because it can return to and change keyword searches (paper search tool calls) after observing the amount of relevant papers it finds.The highest We then explored the tools and their parameters available to PaperQA2.An important RAG parameter is the number of contexts or text chunks to include in the final text generation step ("Generate Answer" tool in Figure 1A).This is a balance because increasing the count improves the chance of including the key context needed to answer a question (improving accuracy), but also increases the amount of distracting irrelevant context that reduces precision 24 .We varied the amount of contexts from 15 to 5 in Figure S5 and Figure 1 to see this effect; 15 gives highest precision and 5 gives highest accuracy.</p>
<p>To improve relevant chunk retrieval, we hypothesized that papers found as either citers or citees of existing relevant chunks would be an effective form of hierarchical indexing.This was validated by ablating the "Citation Traversal" tool (No Cit.Trav.), which showed an increased accuracy (t(2.55)= 2.14, p = 0.069), and significantly increased DOI recall (t(3) = 3.4, p = 0.022) at all stages of the PaperQA2 flow.(Figure 1D) This tool's process mirrors the way that scientists interact with the literature.</p>
<p>To quantify retrieval accuracy changes across LLM implementations, we performed experiments which varied the model choice for our "Generate Answer" tool as well as the RCS step in our "Gather Evidence" tool.Both of these tools give LLMs the opportunity to correctly identify crucial information in our scientific corpus, and we wanted to evaluate if their combined usage would be more effective than an un-transformed insertion of the top chunks into the final context window.The No RCS Model ablation validates that adding RCS to a traditional RAG text generation step significantly increases retrieval accuracy (t(3.92)= 9.29, p &lt; 0.001).Interestingly, this is not true across all models tested, smaller models (GPT-3.5-Turbo(RCS), Llama3 (70B)) decrease overall accuracy when used for RCS, relative to not using a model at all.This indicated there is a comprehension threshold that must be met for effective summarization and relevance evaluation.GPT-4-Turbo significantly outperformed other models on LitQA2 accuracy when used in the RCS step (t(3.47)= 6.14, p = 0.003).Finally, Claude-Opus 25 had the highest LitQA2 precision (Figure 5), though it was not a significant increase over Gemini-1.5-Proand GPT-4-Turbo.</p>
<p>To reduce the large cost of generating contextual summaries, we examined the effect of lowering the top-k ranking depth, i.e. limiting the number of passages considered in the RCS step.We see a significant increase in accuracy with increasing depth (i.e. more document chunks entering into the RCS ranking) from 1 to 10 (t(2.15)= 5.44, p = 0.014), and a long tail of diminishing performance gains from 10 to 30 (the default).In summary, having a deep (&gt;10) RCS ranking list, with a high-performing LLM, was crucial in achieving human-level accuracy on LitQA2.</p>
<p>We had hypothesized that parsing quality would affect accuracy, but Grobid parsings and larger chunk sizes did not significantly increase precision, accuracy, or recall on LitQA2 (Figure 6).This is likely specific to being a retrieval task, as there is often only a single passage needed from a paper's body, which makes our result insensitive to parser changes.Anecdotally, we found better parsings to be crucial for extracting data from tables in WikiCrow (detailed in the Section 8.3).</p>
<p>Summarizing scientific topics</p>
<p>To evaluate PaperQA2 on summarization, we engineered a system called WikiCrow, which generates cited Wikipediastyle articles about human protein-coding genes by combining several PaperQA2 calls on topics such as the structure, function, interactions, and clinical significance of the gene (Figure 3A).There has been previous work on unconstrained document summarization, where the document must be found and then summarized, 26 and even writing Wikipedia-style articles with RAG 2 .These studies have not compared directly against Wikipedia with human evaluation.Instead, they used either LLMs to judge or compared ROGUE (text overlap) against ground-truth summaries.Here, we measure directly against human-generated Wikipedia with subject mater expert grading.</p>
<p>We used WikiCrow to generate 240 articles on genes that already have non-stub Wikipedia articles to have matched comparisons.WikiCrow articles averaged 1219.0 ± 275.0 words (mean ± SD, N = 240), longer than the corresponding Wikipedia articles (889.6 ± 715.3 words).The average article was generated in 491.5 ± 324.0 seconds, and had an average cost of $4.48 ± $1.02 per article (including costs for search and LLM APIs).We compared WikiCrow and Wikipedia on 375 statements sampled from the 240 paired articles.Statements were selected using cues from document formatting (Section 8.3).The initial article sampling excluded any Wikipedia articles that were "stubs" or incomplete articles.Statements were then shuffled and given, blinded, to human experts, who graded statements according to whether they were (1) cited and supported; (2) missing a citation; or (3) cited and unsupported.We found that WikiCrow had significantly fewer "cited and unsupported" statements than the paired Wikipedia articles (13.5% vs. 24.9%)(p = 0.0075, χ 2 (1), N = 375 for all tests in this section).WikiCrow failed to cite sources at a 3.9x lower rate than human written articles, as only 3.5% of WikiCrow statements were uncited, vs. 13.6% for Wikipedia (p &lt; 0.001).In addition, defining precision for WikiCrow as the ratio of cited and supported statements over all cited statements, we found that WikiCrow displayed significantly higher precision than human-written articles (86.1% vs. 71.2%,p = 0.0013).</p>
<p>The "cited and unsupported" evaluation category includes both inaccurate statements (e.g.true hallucinations or reasoning errors) and statements that are accurate with inappropriate citations.To investigate the nature of the errors in Wikipedia and WikiCrow further, we manually inspected all reported errors and attempted to classify the issues as follows: reasoning issues, i.e. the written information contradicts, over-extrapolates, or is unsupported by any included citations; attribution issues, i.e. the information is likely supported by another included source, but either the statement does not include the correct citation locally or the source is too broad (e.g. a database portal link); or trivial statements, which are true passages, but overly pedantic or unnecessary (Figure 3D).Surprisingly, we found that compared to Wikipedia, WikiCrow had significantly fewer reasoning errors (12 vs. 26, p = 0.0144, χ 2 (1), N = 375) but a similar number of attribution errors (10 vs. 16, p = 0.21), suggesting that the improved factuality of WikiCrow over Wikipedia was largely due to improvements in reasoning.Although language models are clearly prone to reasoning errors (or hallucinations), in our task at least they appear to be less prone to such errors than Wikipedia authors or editors.This statement is specific to the agentic RAG setting presented here: language models like GPT-4 on their own, if asked to generate Wikipedia articles, would still be expected to hallucinate at high rates.a 5 Detecting contradictions in the literature Because PaperQA2 can explore scientific literature at much higher throughput than human scientists, we reasoned that we could deploy it to systematically identify contradictions and inconsistencies in the literature at scale.Contradiction detection is a "one versus many" problem, which in principle involves comparing claims or statements in one paper with all other claims or statements in the literature.At scale, contradiction detection becomes a "many versus many" problem and loses feasibility for humans.Thus, we leveraged PaperQA2 to build a system called ContraCrow that automatically detects contradictions in the literature (Figure 4A).</p>
<p>Contradiction detection is also known as claim verification or colloquially as "fact checking" 27 .This task has been studied for over a decade, especially in the context of claims in the news or the internet 27,28 .Although originally restricted to context and a claim, the setting extended to be unconstrained 29 and recent work tries to work at the scale of the internet 30 .Some claim verification work has also focused on scientific claims 31,32 .The main novelty of this work is in detecting contradictions without a restricted corpus and evaluating with human experts, not against a benchmark.</p>
<p>ContraCrow first extracts claims from a provided paper using a series of LLM completion calls (similar to Schlichtkrull et al. (2024) 30 ), and then feeds those claims into PaperQA2 with a contradiction detection prompt.This prompt instructs the system to evaluate whether there are any contradictions in literature to the provided claim, providing both an answer and a choice from an 11-point Likert scale (Figure 4B, Methods Section 8.4).Utilizing a Likert Scale allows the system to give more reliable and interpretable scores when providing rank 33 .</p>
<p>To evaluate ContraCrow, we first derived a contradiction detection benchmark, ContraDetect, from LitQA2, as detailed in Section 8.4.2.Briefly, we converted half of the question-answer pairs in LitQA2 into declarative, incorrect statements that are contradicted by the papers referenced in the corresponding LitQA2 question.(For example, a question like "what color is grass?" would become "grass is purple.")We converted the other half of the LitQA2 questions into declarative, correct statements that are supported by the corresponding papers ("what color is grass" becomes "grass is green.").</p>
<p>We then evaluated ContraCrow on its ability to detect the contradictions in ContraDetect.By transforming the Likert scale output into integers, we were able to tune the detection threshold and obtain an ROC curve with an AUC of 0.842 (Figure 4C).Setting a threshold of 8 (contradiction), ContraCrow achieved 73% accuracy, 88% precision, and a false positive rate of only 7%.To further evaluate precision and to demonstrate ContraCrow's ability to handle "Has anybody ever done x?" questions, we then ran ContraCrow on 42 "no-evidence statements" that have never to our knowledge been reported on (detailed in Section 8.4.2).These claims were hand-generated by the authors within their fields of expertise, and ContraCrow correctly chose lack of evidence (5) as its response 98% of the time, indicating its ability to distinguish between real contradictions and lack of support.</p>
<p>We then applied ContraCrow to a set of 93 biology-related papers randomly selected from our database, identifying an average of 35.16 ± 21.72 (mean ± SD, N = 93) claims per paper.Of the 3,180 claims analyzed over the 93 papers, 6.85% were deemed by ContraCrow to be contradicted by the literature, with 2.89%, 3.77%, and 0.19% assigned scores 8, 9, and 10, respectively (Figure 4D).Setting a Likert scale threshold of 8, we detected an average of 2.34 ± 1.99 contradictions per paper (mean ± SD) (Figure 4E).As an example, one contradiction detected by our system concerned the prognostic implications of LEF1 expression in colorectal carcinomas.The source paper, Kriegl et al. (2010) 34 , finds using immunohistochemistry on a tissue microarray that LEF-1 correlates positively with longer overall survival.By contrast, a study published the following year also using tissue microarrays found LEF1 overexpression in colorectal cancer correlates negatively with longer overall survival and is also correlated with liver metastasis 35 , results that have been supported by other studies as well 36,37,38 , thus explicitly contradicting the original statement.</p>
<p>To evaluate the validity of the contradictions detected this way, expert human annotators evaluated 50 claims that were assigned ContraCrow scores of 8 and 50 claims that were assigned scores of 9 or 10, considering all evidence and reasoning cited by the model.Annotators agreed with ContraCrow's findings on 70% of evaluated claims, or 1.64 contradictions per paper, demonstrating significant agreement (p = 1e −4 , χ 2 (1)), with an F1 score of 0.82.Interestingly, claims scored as 8 by ContraCrow were not more likely to elicit human agreement (70%) than claims scored as 9 or 10 (70%) (Figure 4F).The final number, 1.64 validated contradictions per paper, serves as a lower bound on the abundance of human-validatable contradictions in the biology literature.</p>
<p>a All WikiCrow generated articles for this study are now available for download from a Google Cloud Storage bucket https://storage.googleapis.com/fh-public/wikicrow2/. A command line tool like gsutil can be used to list and bulk-access these files.All WikiCrow and LitQA evaluator responses are available via the following link.We were concerned that the annotators on the "contradiction validation" task might exhibit some bias toward agreeing with the model or otherwise be influenced by the model's reasoning and chosen sources.We therefore further evaluated ContraCrow by asking human experts to identify contradictions in the same literature considered by ContraCrow, without any access to ContraCrow's reasoning.This "contradiction detection" task (Section 8.4.2) is a much more challenging task for annotators than the contradiction validation task, because it requires them to exhaustively consider a significant amount of literature relevant to the claim in question, whereas the contradiction validation task only requires them to consider the specific sections cited by the model.In the contradiction detection task, human experts were provided with the top 15 paper chunks identified as relevant to the claim by PaperQA2 (the same chunks ContraCrow had access to), and were asked to respond whether the claim was contradicted by the available evidence on both the Likert scale and binary determination ("yes" or "no").Experts considered a mixture of claims rated as contradictions and non-contradictions by ContraCrow.We found that experts' binary responses agree with each other on 75.5% ± 13.43% of claims provided (mean ± SD, N = 10 pairwise comparisons between human annotators), whereas they agree with ContraCrow on 60.42% ± 5.99% of claims (mean ± SD, N = 5 human annotators), indicating that humans are significantly more correlated with each other than they are with ContraCrow (p = 0.015).Examining the Likert scale values over 30 claims evaluated by human annotators leads us to speculate that overconfidence on ContraCrow's part is the primary driver of its lack of agreement with human annotators, and indicates directions for future improvement (Figure 4G).</p>
<p>We are thus able to establish a lower bound for the average number of human-validatable contradictions per paper in biology literature at 1.64.Importantly, just because a claim is detected as a contradiction does not mean that the claim is wrong.For example, one detected contradiction is "GBP is only found in the cytosol of human fibroblasts." 39This example shows how scientific literature and findings can update and contradict over time, as ContraCrow points out: "...more recent research has shown that GBPs can localize to various cellular compartments and membranes 40,41,42,43 ."Thus, claims can contradict but still be contextually valid, and contradictory claims may be merely reflective of the iterative nature of research.</p>
<p>Conclusions</p>
<p>We developed a methodology to compare or validate AI systems against human performance in realistic tasks for scientific research.PaperQA2 outperforms human experts on answering questions across all scientific literature; produces summaries that are, on average, more factual than Wikipedia summaries; and can be deployed to identify contradictions in scientific literature at scale.The contradiction work, in particular, attests to the potential of systems such as PaperQA2 for science: notably, one human expert who performed this task, who was also tested on a large battery of other benchmarks, reported without solicitation that the contradiction detection task was the hardest task they were asked to perform.Although PaperQA2 is expensive compared to lower accuracy commercial systems, it is inexpensive in absolute terms, costing $1 to $3 per query.Scaling up PaperQA2 and other literature-enabled agents like WikiCrow and ContraCrow empowers us to take advantage of the latent knowledge in literature at much greater scale than is possible today.</p>
<p>Data Availability</p>
<p>The code necessary to replicate these results or modify the algorithm for further research is included on Github via paperqa.Data including all evaluator responses, contradiction detection claims, litQA questions, and WikiCrow candidate statements are available in the supplementary materials.All generated WikiCrow articles for this study are available in a public Google Cloud bucket here: https://storage.googleapis.com/fh-public/wikicrow2/. Note that while paperqa gives the ability to recreate this work, the experiments reported in this paper were performed using a more featureful HTTP server that takes advantage of bespoke infrastructure at the authors' institution.This infrastructure includes features such as user authentication, MongoDB request caching, Redis object caching and global-load balancing, several PostgreSQL DBs with associated ORM code, cost-monitoring modules, time-profiling modules, configuration storage and run orchestration (Dagster 2 and kubernetes 3 ), cloud bucket storage for PDFs, a CI pipeline with semi-automated deployments, and infrastructure code for deploying auto-scaling instances in the cloud.None of these features affect performance on a per-query basis, but provide increased scalability, measurability, and persistence.To run our same server infrastructure, users would need to provision all of these assets and configure the deployments themselves.paperqa should serve allow usage and customization sufficient for most research purposes, and should be sufficient to reproduce the results reported here.</p>
<p>Even within paperqa, the "Paper Search" tool is limited by access to full text repositories of scientific papers, often bound by licensing agreements.The included implementation only works from local files accessible to each user.Our implementation starts with a full or partial-text keyword search, where the keywords have been specified by the agent when selecting the paper search tool.The ranked results returned from these services are then matched to a user's existing paper repository or can be retrieved on-the-fly if open-access or partner links exist for these works.These matching papers are parsed, and pulled into our agent state for usage with other tools.Note that the search services will have access to a larger corpus of works than is available to us via our repository and accessible link traversal, in these cases the system will simply skip these papers and they are not used.A stub of the paper search tool is implemented in paperqa with directions for users to implement their own retrieval since it will be limited to their own access to full-text papers.</p>
<p>Ablations and configurations for workflows like WikiCrow are exposed in paperqa as nested configuration objects.All experiments performed in this work correspond to included configuration objects.Here we highlight the configuration variable descriptions corresponding to the salient features tested in this work, though all variable names are available in the included files.</p>
<p>• query: The main query task asked of the PaperQA agent, i.e. a LitQA question or a directive to write an article.</p>
<p>• llm: The LLM used in the generate answer tool can be a valid Anthropic, OpenAI, or Gemini model identifier.This parameter was varied for the model experiments in Figure 2.</p>
<p>• agent_llm: The LLM used for the agent orchestration, in this work, it was always fixed to gpt-4-turbo-2024-04-09.</p>
<p>• summary_llm: The LLM used for the RCS step in the gather evidence tool, must be a valid Anthropic, OpenAI, or Gemini model identifier.This parameter was varied for the model ablations in Figure 2.</p>
<p>• prompts: A PromptCollection object from PaperQA 4 , which allows for specification of prompts in each tool, as well as features like turning off RCS (via prompts.skip_summarization).The No RCS Model ablation used this input as well as the WikiCrow prompts.</p>
<p>• max_sources: The number of top ranked sources to be included in the generate answer tool, in Figure 1A, the 'filter top summaries" cutoff.This parameter was 5 in the top-performing Answer cutoff @ 5, but 15 for all other experiments.</p>
<p>• consider_sources: The top-k cutoff, i.e. the number of chunks that will be used in the RCS step.This parameter was set to 30 by default in LitQA experiments, save for the Top-k Rank @ X experiment where it was set to X. Additionally, for our WikiCrow prompts this parameter was set to 25.</p>
<p>• agent_tools: An ordered list of tool names that will be used by the agent, including gather_evidence, paper_search, generate_answer, and citations_traversal.This always included all four tools except for the No Cit.Trav.and No Agent runs where citations_traversal was excluded.</p>
<p>• docs_index_mmr_lambda: A pre-gather evidence MMR lambda parameter which can be used to pre-filter similar papers by name before gathering evidence.This was set to 0.9 for our WikiCrow run to promote diversity of sources, but 1.0 for LitQA experiments.</p>
<p>parsing_configuration.ordered_parser_preferences:A list of the parsing algorithm to use, either paperqa_default (PyMuPDF) or grobid.paperqa_default was the default for each ablation, and grobid was used for WikiCrow generation.This parameter was also varied in the experiments shown in Figure 6.</p>
<p>• parsing_configuration.chunksize: the chunk size (in characters) to be used when chunking parsed documents.This parameter was varied in the experiments shown in Figure 6.</p>
<p>• parsing_configuration.overlap: the overlap (in characters) that will be common between sequential chunks.This was fixed at 750 for this work.</p>
<p>• parsing_configuration.chunking_algorithm: the algorithm used to chunk documents, simple_overlap simply uses a sliding window with overlap, and sections uses semantic parsing by section (i.e. one chunk per section where possible), if sections need to be broken into multiple chunks the system will automatically handle this.</p>
<p>sections is only supported via parsing_configuration.ordered_parser_preferences=grobid.This parameter was varied in the experiments shown in Figure 6, and in our WikiCrow generation.</p>
<p>• temperature: temperature used for the LLM in the generate answer tool.This was set to 0 for all runs in this work.</p>
<p>• summary_temperature: temperature used for the LLM in the gather evidence tool's RCS step, this was set to 0 for all runs in this work.</p>
<p>Tool implementations</p>
<p>PaperQA2's agentic tools were implemented as in PaperQA Where variables like {status} are included to represent the current state to the agent.Tools were implemented with the following prompts and settings.</p>
<p>Paper Search Tool</p>
<p>Language Agents Achieve Superhuman Synthesis of Scientific Knowledge</p>
<p>The paper search tool uses an initial keyword search, generated by the agent in the context of the user query.The agent is prompted as follows:</p>
<p>A search query in this format: [query], [start year]-[end year].You may include years as the last word in the query, e.g.'machine learning 2020' or 'machine learning 2010-2020'.The current year is {get_year()}.The query portion can be a specific phrase, complete sentence, or general keywords, e.g.'machine learning for immunology'.</p>
<p>Our initial search relies on services like Semantic Scholar 5 , where candidate lists (default of 12) of relevant papers are generated then parsed.When parsed, the papers are first turned into text using either Grobid or PyMuPDF, then split into chunksize character sized pieces.If the sections parsing is used, then section chunks are split on header metadata provided by Grobid.An embedding vector is generated for each chunk using a hybrid implementation which concatenates a dense and sparse, keyword based embedding model.For the experiments included in this study, OpenAI's text-embedding-large-3 was used.It was concatenated with a normalized 256 dimension vector which used modulus-encoding to extract a hot-encoded keyword from the tokenization integers provided by OpenAI's tiktoken 6 library.These text chunks are put into a document context which is accessible by the agent for further manipulation with tools.The PaperQA entrypoint for these functions can be found on github.</p>
<p>Gather Evidence Tool</p>
<p>As detailed in github for PaperQA, the Gather Evidence tool begins with a top-k vector ranking step, using the embedding vectors created in the Paper Search tool.The user query is embedded with the same algorithm, and cosine similarity is used to match all document chunks in the agent's context with the user query.The top-k chunks are then selected for the RCS step.</p>
<p>The reranking and contextual summarization step most differentiates PaperQA's implementation relative to other RAG technologies.The tool's prior step, an top-k vector retrieval ranking, is a widely implemented 7,8 approach to identify relevant documents, however, the RCS second step, is unique to PaperQA (to the authors' knowledge).While performance improvements with both deep reranking (or LLM) models and map-reduced summarizations 9,10,11,12 are well documented, combining the reranking operation with a contextual summary provides novel benefits.</p>
<p>The step is implemented by mapping an LLM completion across each top-k chunk (system prompt):</p>
<p>Provide a summary of the relevant information that could help answer the question based on the excerpt.The excerpt may be irrelevant.Do not directly answer the question -only summarize relevant information.Respond with the following JSON format: {{ "summary": "...", "relevance_score": "..." }} where "summary" is relevant information from text -{summary_length} words and "relevance_score" is the relevance of "summary" to answer the question (integer out of 10)</p>
<p>Where each chunk is injected as follows:</p>
<p>Excerpt from citation --{text} --Query: {question} After completion, each JSON object is parsed and the passages are re-ranked according to the new relevance scores.When running with WikiCrow, gene names are also prompted to be extracted as additional JOSN keys, these are kept and injected in the final answering context.Advantages of the RCS step are as follows: 1. Token usage efficiency is vastly improved, a contextual summary will be 200-400 tokens compared with our standard document's chunk size of 2,250 tokens.This allows for a significantly more accessible document corpus for injection into PaperQA's answering context window.Furthermore, we see no decrease in summarization efficacy, using LitQA performance as a proxy, across document chunk sizes from 750-3,000 tokens.2. As a new feature in this work, the LLM can be prompted to provide its summary in a structured JSON or XML format to simplify its downstream data extraction.</p>
<p>In addition to a relevance score used for reranking, this structure can include metadata (such as a gene name) which will be retained through the PaperQA workflow.This is used to reduce hallucination and confusion in the final answer context.Since the RCS step is performed in an embarrassingly parallel fashion, it's highly efficient, and its utility can be applied to an arbitrarily deep ranking, up to the rate or cost limits of the LLM API.Our studies on the efficacy of the RCS depth led us to use a much deeper RCS depth, and to utilize the best performing model for the RCS operation.This differs from the intuition in prior work 4 , which utilized a cheaper model during the RCS step.</p>
<p>Generate Answer Tool</p>
<p>This tool answers questions by taking a subset of the top ranked sources (from the RCS ranking), and injects them into a final context for answering.The default in this study was to inject 15 contextual summaries, but we saw maximal accuracy with 5 at the cost of precision.LLMs were prompted to answer as follows:</p>
<p>Answer the question below with the context.Context: {context} --Question: {question} Write an answer based on the context.If the context provides insufficient information and the question cannot be directly answered, reply "I cannot answer."For each part of your answer, indicate which sources most support it via citation keys at the end of sentences, like (Example2012Example pages 3-4).Only cite from the context and only use the valid keys.Write in the style of a Wikipedia article, with concise sentences and coherent paragraphs.The context comes from a variety of sources and is only a summary, so there may inaccuracies or ambiguities.If quotes are present and relevant, use them in the answer.This answer will go directly onto Wikipedia, so do not add any extraneous information.Answer ({answer_length}):</p>
<p>Where contexts are injected by the generate answer code before output is returned to the agent.</p>
<p>Citation Traversal Tool</p>
<p>Atop the PaperQA 4 tools, we created an additional tool to traverse one degree of citations, both forward in time ("future citers") and backwards in time ("past references").This tool enables a fine-grained search around paper(s) containing relevant information.The traversal originates from any paper containing a highly-scored contextual summary (RCS score 0-10), and our minimum score threshold was eight (inclusive).The papers corresponding to highly-scored summaries are referred to as D prev in Algorithm 1. See Table 1a for the frequencies of various |D prev | when this tool was selected.To first acquire citations, Semantic Scholar 5 and Crossref 13 APIs are called for past references and Semantic Scholar APIs are called for future citers.To collect all citations for a given paper, we make one API call per provider per direction, totalling four API calls/paper.All three providers only provide partial paper details, meaning a large fraction of the time a title or DOI is not present in the response metadata.To merge citations across providers, a best-effort de-duplication is performed using casefolded title and lowercased DOI.In Algorithm 1, this logic takes place inside the GetCitations procedure.</p>
<p>Once citations have been acquired, bins of overlap B are computed.For example, traversing past references for the following six DOIs: To filter bins of overlap, a hyperparameter "overlap fraction" α was introduced to compute a threshold overlap θ o as a function of the number of source papers (|D prev |).For example, with an α = 2 5 and traversing from six source DOIs, all citations not appearing in at least three source DOIs were discarded.The default overlap fraction used in data collection was 1 3 .See Table 1b for a full distribution of overlaps seen during LitQA runs.Furthermore, a twelve paper limit ℓ was posed on the traversal, which meant in the above example only keeping six of the bin of 29 DOIs cited by two papers.To filter within a bin, we fall back on the count of future citers.This winnowing logic is detailed across Algorithm 1's FilterOverlap and TraverseCitations procedures.Lastly, we traverse both future citers and past references, feathering together the resultant DOIs before finding them.</p>
<p>Algorithm 1 Traverse Citations</p>
<p>Require: Set of summaries S, score threshold θ score , overlap fraction α, look future flag 1 fut , limit ℓ Ensure: Set of traversed papers D out , where papers are future citers if 1 fut else past references 1: procedure TRAVERSECITATIONS(S, θ score , α, 1 fut , ℓ)</p>
<p>Question Construction and Human Evaluation</p>
<p>LitQA questions were generated manually by a combination of the authors as well as contracted human experts (see 14 ).All human annotators were compensated, informed that their evaluations were being used in research of human-level performance, and consented to the use of their annotations and participation.Question authors were instructed to identify recent papers (published within the last 36 months), and develop a multiple-choice question that requires context within the main text of the paper to answer and is not answerable by the abstract or title alone.They were further advised that the question should require some amount of reasoning within the paper context, and not be a direct quote or statement from the paper.Distractors were instructed to be reasonable within the context, using either other information in the paper (e.g.other genes being discussed) or based on inherent or other knowledge.We also periodically tested question drafts with ChatGPT 3.5 or 4 (with logging disabled) to ensure questions were not easily answerable by models already, or to help design effective distractors by asking to provide plausible answers.Question drafts were also often searched against Google Scholar to ensure that it was not trivial to find the exact statement necessary to answer the question.This was an effective aid in revising question wording.</p>
<p>For contracted question authors or anyone new to drafting, the first 10 or so questions produced were carefully reviewed by one or more of the authors for quality, after which they were either asked to rework them, they were added to corpus, or they were removed from consideration.This feedback (referred to as calibration) was usually enough to ensure quality question generation going forward from contractors, though questions were reviewed by authors on an ongoing basis prior to merging into the main corpus.Contractors were compensated via multiple structures during different phases of generation as we iterated on effective strategies.Initially, they were paid on an hourly basis at $50 per hour.At later phases, they were paid on a completion basis such that they were paid per accepted question, which equated to variable hourly equivalents always $50 or more per hour.Using this methodology, LitQA2 was built up from LitQA (47 questions) in two stages of releases, first 100 questions (147), then an additional 101 questions, adding to the original subset to make 248 total questions.</p>
<p>Human evaluators were assigned questions from the LitQAv2 corpus in rounds of "quizzes" composed of 20-40 questions each.They were allocated up to a week to complete the quiz, but were given no other time constraints.They were also allowed to use tools such as internet search or journal collection search provided via their institutions.They were asked to explicitly refrain from using AI-based tools such as ChatGPT or Claude, though we did not have any method of enforcement of this request.In order to encourage completion as well as high performance, we attempted to design an incentive scheme that separately promoted both: In brief, we paid a base dollar amount (from $3-6) to each question for completion.We then added a bonus to each question that was based on overall performance:</p>
<p>• 80% or more overall score = bonus equal to base question amount for each correct question.</p>
<p>• 60% -80% overall score = bonus equal to half the base question amount for each correct question.</p>
<p>• Less than 60% overall score = bonus equal to $1 for each correct question.</p>
<p>We additionally paid a completion bonus for the entire quiz in some instances, amounting to $150.On average, evaluators were paid between $50 to $100 per hour based on self-estimation of time spent and total compensation.Quizzes were sometimes combined with similar evaluation quizzes for other evaluation benchmark categories we are developing.</p>
<p>For the human LitQA2 performance reported in this work, 2 rounds of quizzes (20 questions each) were given to 9 evaluators.In total, evaluators answered provided 266 unique answers across 248 unique questions.</p>
<p>A third round of quizzes was given with another set of 160 questions given to evaluators, with questions which overlapped from both the initial set of 147 questions and the remaining 101 questions.However, this quiz round had several outlier scores ( &gt;90%) in quizzes on the same subsets of questions which had been given previously to a much lower average (66.3%).Upon investigating, we found that a portion of the initial 147 LitQA2 questions that were available on GitHub had been parsed by google-indexed data aggregators, causing it to trivially return results with a Google search from evaluators.Interestingly, this did not impact PaperQA2 performance, because PaperQA2 does not use Google websearch.Thus, we chose to exclude evaluator answers to the initial 147 LitQA2 questions in the third set of quizzes as it was no longer a valid human comparison.</p>
<p>PaperQA measurement</p>
<p>When measuring LitQA2 performance, paperqa function calls were performed for each LitQA question.LitQA answer order was randomized for each call, and, for all configurations shown, 3 full runs of all LitQA questions were performed.3 runs were necessary to control for the inherent noise in LLM inference, even at 0 temperature (where all tests were performed).LitQA2 was automatically evaluated using an evaluation LLM call (GPT-4-0613), which extracted the letter answer from PaperQA2's output.It was prompted to extract as follows:</p>
<p>Extract the single letter answer from the following question and answer {prompt_output('QA')} Single Letter Answer:</p>
<p>The extraction was then parsed and graded against the LitQA2 benchmarks correct ideal answer.All questions were graded as "Unsure" if the "Insufficient information to answer this question" option was selected, except for the LitQA2 questions where the ideal answer is "null", then the "Insufficient information" was graded as "Correct".</p>
<p>LitQA2 was developed in two stages of 100 then 101 questions, adding to the original subset of LitQA (47) questions to make 248 questions.This gave us an opportunity to evaluate our system before and after adding the set of 101 previously unseen questions to evaluate overfitting.We can see that our system performed similarly in terms of accuracy between question sets, implying we were not overfitting to the original LitQA data.</p>
<p>When comparing PaperQA2 to other RAG technologies in Figure 2A, the author's manually entered each LitQA2 question (in the exact format given to PaperQA2) into the online interfaces for both Perplexity 15 and Elicit 16 .Perplexity was run using the "Pro" product, and GPT-4o, across all 248 LitQA2 questions.We recorded either the letter answer, or manually interpreted the response to assign the system's letter output.The performance metrics were calculated in an identical way to PaperQA.Elicit was not able to be run for all 248 questions, the numbers reported only represent the first tranche of 147 LitQA2 questions.On Elicit's final 101 LitQA2 questions, the authors noted a significant algorithm   change.Answers resulted in &gt; 90 % "unsure" answers, thus we are only reporting on the subset of LitQA2 where multiple choice answers could be extracted.We also compared PaperQA2 to PaperQA's prior published configuration 4 , which used a smaller RCS model (GPT 3.5 Turbo) and ranking depth, had no citation traversal tool, and did not have access to Grobid parsings.PaperQA's LitQA2 accuracy was 36.7% and its precision was 76.5%.Thus, PaperQA2 showed a large improvement in both metrics, particularly in accuracy.</p>
<p>WikiCrow</p>
<p>WikiCrow statements were generated using a linked set of 5 different queries, 4 to PaperQA2, and 1 to a frontier LLM model, GPT-4-Turbo.As detailed in Figure 3A, each PaperQA2 query created a different article section.The PaperQA2 prompts and settings used for these queries can be found in our paperqa repository.The primary PaperQA2 parameter differentiation between WikiCrow and LitQA2 runs was the usage of Grobid for document parsing, as well as the modification of the RCS step to extract a gene name that is the focus of each paper chunk being summarized.They were designed to reduce hallucinations and accurately read data from tables.</p>
<p>We compared WikiCrow and Wikipedia statements starting from a sample of 300 genes which were sampled from from a complete list of all protein encoding human genes 17 (n=19,255) after being filtered for only those having valid (non-stub) Wikipedia articles (n=3,639).Among the 300 articles, Wikipedia statements were sampled, first from the pool of 300 genes, then among each article's <p> HTML sections with more than 25 characters.500 total Wikipedia statements were sampled, which resulted in 240 unique genes.Matching WikiCrow articles were generated for each of the 240 genes for comparison.WikiCrow statements were broken at either a reference list (notated with parenthesis and PaperQA2 document names) or at a paragraph break, most closely matching Wikipedia's HTML structure.Each of the 500 Wikipedia statements were randomly matched with a WikiCrow statement from the same gene article to ensure that evaluators were given a similar gene-distribution of statements between WikiCrow and Wikipedia articles.</p>
<p>References were extracted from the Wikipedia statements using the <sup> link, and matched to the correct DOI or hyperlink in the article references.References were extracted from the WikiCrow statements by parsing the parenthetical insertions (via regular expressions) and matching the DOI to the document name given by the model.In this way, each statement was associated with a list of DOI links to the sources listed.Efforts were then made to structure each statement similarly, removing any HTML from the Wikipedia statements, and removing all inserted references into the WikiCrow statements.All references were replaced with random numbers between 1 and 30, but kept consistent if listed several times in the same statement.References were then uniformly reformatted as [x], to hide their origin.</p>
<p>The matching 500 Wikipedia and 500 WikiCrow statements were shuffled for each of the 4 evaluators.Each was given a list of 200 to grade while blinded to the statement origins.Graders were sent the questions with the following instructions:</p>
<p>Task overview</p>
<p>You'll be reviewing statements written about human genes.These statements are intended to provide information taken from the literature, and should thus provide an accurate citation such that the stated information matches information contained in the cited literature.You'll be evaluating each statement according to its accuracy as well as being cited appropriately.Please read the guidelines below carefully to ensure accurate work.</p>
<p>Scoring</p>
<p>You'll soon receive a link to a private Google Sheet with 200 statements, along with some associated information for each one, including links to any cited literature.You'll be grading the sentences in each statement according to the below metrics.Some statements may contain multiple sentences, each of which should be evaluated such that any sentence not satisfying our scoring criteria counts against the entire statement.</p>
<p>• Is it cited?In other words, does the statement contain one or more citations to published literature?• Is the information correct, as cited?In other words, is the information stated in the sentence correct according to the literature that it cites?</p>
<p>There will be a column for each of the above scores.Put TRUE in the appropriate column if true according to the metric, and FALSE if not.There will also be a column for "notes".Please use this column to include anything you think is important for your grading, or other flags we might want to know about.Below are some more detailed guidelines for scoring: You are only evaluating if the statements are correct as cited -not if the statement is globally correct or supported by other sources.Some common scenarios you may encounter:</p>
<p>• Simple statements: Statement has a single citation and the content is supported by the citation.Is Cited = TRUE, Is Info Correct As Cited = TRUE</p>
<p>• Simple unsupported statements: A statement is true, highly-specific (see "broad context" below, but not explicitly stated in the source, mark as FALSE for "correct as cited", but mark "is cited" as TRUE.• Unrelated, meta statements: Statement is a meta-statement or unrelated to biology.For example, the statement may be the start of a list like: "Gene XYZ has the following properties:", or an attribution like "This data has been provided by Y organization."These statements should be judged as "Not Applicable/NA".They do not need to be cited.• Broad context: Statement has a citation and the content is a broader context than the citation, but not explicitly explained in the citation.For example, an explanation of what a secondary structure is or what a protein sequence is.The broad content should be judged to be "undergraduate biology student common knowledge".If it is, then the info can be marked correct as cited, if it's more advanced than undergraduate bio knowledge (or outright incorrect), then the info is incorrect as cited.• Underspecified citation: The statement may be factually correct, but the citation is underspecified.An example might be that the citation is a link to a database, where a user needs to query the database and synthesize their own data to support the statement.If it requires synthesis by the user, than the statement should be judged as NOT correct as cited.If it's a database link to the a page about the gene, and the fact can be found there, then it can be judges as correct as cited.</p>
<p>• Inaccessible citation: The statement may be factually correct, but the citation is to an inaccessible source, like a closed source database or a textbook.These should be judged as "Not Applicable/NA".Though an attempt should be made to acquire the source to validate the information.• Multi-part citations: The statement has several sentences and citations.Each sentence should be correct as cited given all of the previous criteria (including the "Broad contexts" category).• Occasionally, it may not be clear from context which sentence applies to a source, there may be three sentences in a row with two sources trailing the sentences.In this case if the sentences are supported by either source, then it's correct as cited.If a single sentence is un-supported by either source, and they don't meet our broad context criteria, then mark as incorrect as cited.• If there are multiple citations, where one is inaccessible, but the accessible citations can support the statements by themselves, then mark as correct as cited.If the statement can NOT be validated by the accessible sources, then mark as "Not Applicable/NA".</p>
<p>Evaluators were incentivized to encourage both quality of review and quantity of statements reviewed.They were offered $10 per statement graded as a base payment, and were additionally offered a $10 per question bonus payment if they reviewed more than 50 questions by a specific deadline approximately five days after starting.</p>
<p>Using this criteria, 4 expert researchers were contracted as evaluators to grade 375 evaluations.They were instructed to only complete their statements in the order provided, and any statements that were not graded in a contiguous block from the initial statement were excluded from analysis (to avoid biasing towards simple statements).Among our 375 evaluations, 40 statements overlapped between 2 or more evaluators.Among those 40, 77.5% (31) had agreement, and 22.5% (9) had a disagreement in evaluation, showing good overall alignment in scoring.</p>
<p>We had previously reported 18 lower "cited and unsupported" percentages for both Wikipedia and WikiCrow, but the prior study differed in key ways: 1. this work limits the statement samples to be from the same subset of gene articles, eliminating selection bias for longer, higher quality articles; 2. this work uses statements built from sentences which may cite one or more sources as is typically present in articles, the prior work focused on only single sentences with one source which greatly limited the sample; and 3. this work used a much more robust evaluation pipeline, with blinded external evaluators.</p>
<p>Due to the complexity and time required to evaluate WikiCrow accuracy, we relied on heuristics to evaluate our summarization-centric features, adding metadata (gene name) extraction to our RCS step and Grobid-based parsings.These features were meant to mitigate gene name confusion between the RCS step and the generate answer tool, and misreading tabular data, which anecdotally were the two most frequent issues found by the authors while developing WikiCrow.Among the reasoning issues reported by the evaluators, only 2 / 171 statements had issues with gene name confusion, and there were no reported table extraction errors.Utilizing Grobid structured parsings also reduced PaperQA2's token usage.Across a sample of 5,363 papers, PyMuPDF's parsings resulted in an average of 16,040 tokens per paper, while Grobid's parsings resulted in an average 8,903 tokens.The savings can be attribution to a reduction in excess whitespace as well as the removal of reference sections, which we did not include in our Grobid parsings.</p>
<p>Contradiction Detection Methods</p>
<p>ContraCrow detects contradictions in literature through two steps.Unless provided, the first step in ContraCrow is to extract claims from a given paper Figure 4A.First, the provided paper is split into chunks.To maintain context for extracting claims, each chunk is only split within its respective section, up to 5000 characters (no overlap), and the section and paper titles are included.Each resulting chunk is then fed into a claim-extraction LLM to extract claims candidates.The resulting extracted claims are each evaluated by a filtering LLM that assesses the claims based on quality and generalizability, assigning a score out of 10.Only claims scoring 8 or higher are considered.</p>
<p>Once claims have been generated or are provided, each claim is fed into PaperQA2 with a special contradiction detection prompt, instructing the system to search for contradictions and respond with an appropriate format.The system considers each claim independently and identifies relevant papers across literature in order to identify possible contradictions to the claim.Finally, as instructed by the contradiction detection prompt, ContraCrow outputs its reasoning and a choice from an 11-point Likert scale (see Figure 4D).We map the natural language score to integers (0-10) in our calculations, which allows us to tune the decision threshold, as seen in 4B.For all experiments, this step uses Claude 3.5 Sonnet 19 as the LLM model, a chunk size of 7000 characters, a temperature of 0, and a simple overlap of 250.Unless otherwise specified, we use a decision boundary of 8 on this step.</p>
<p>ContraDetect</p>
<p>The ContraDetect benchmark was generated from the LitQA2 contradiction dataset, removing any questions with "null" responses as to not overlap with the no-evidence data.The resulting questions were randomly split into two groups.The questions and corresponding ideal answers from the first group were individually given to gpt-4-turbo-2024-09 20 with instructions to rephrase them into factual statements.The second group was similarly turned into incorrect statements (contradictions), given question and the first corresponding distractor answers.Each question corresponds to exactly 1 statement.These claims then proceeded into ContraCrow's second step, as described above.</p>
<p>We separately designed 42 no-evidence statements by hand.The authors carefully designed questions in their respective fields of expertise and did extensive literature review to ensure that no literature has reported on the claim.There is no overlap between the no-evidence statements and the LitQA2 questions.The resulting answers from ContraCrow were also evaluated by the authors to ensure that no evidence had been found.Any claims with found evidences were removed from this dataset.</p>
<p>Human Evaluation</p>
<p>We randomly selected 100 papers from our local database of biology papers.7 of these papers failed to parse, leaving our dataset at 93 papers.These papers have no overlap with the papers corresponding to LitQA2 questions.gpt-4-turbo-2024-04-09 20 was used for both LLM models in the claim-generation step.The resulting claims were then fed into the contradiction-detection step as described above, in batches of 1,000 claims.</p>
<p>For the human "contradiction validation" task, 50 claims scored at 8 and 50 claims at least 9 by ContraCrow were randomly chosen (distribution: 8 : 50, 9 : 48, 10 : 2).The resulting 100 contradictions were split over 5 expert evaluators evenly, with no overlap.The evaluators were provided with the claim, the original claim information (source paper reference and the chunk used to generate the claim), some background information (acronyms, definitions, etc.), ContraCrow's reasoning, and all chunks cited in the reasoning.The evaluators were tasked with giving two separate binary responses indicating their agreement with both the model's reasoning and conclusion, based on the provided cited chunks (up to 10).They were instructed to use additional provided resources sparingly, only for clarification, and to only consider the evidence chunks and ContraCrow's reasoning when making a final decision.Only the label for "agreement with the model's conclusion" was used for analysis.</p>
<p>For the human "contradiction detection" task, 10 claims scored 0-4, 10 claims scored 8, and 10 claims scored at least 9 by ContraCrow were randomly chosen (distribution: 0 : 1, 1 : 6, 2 : 2, 3 : 1, 8 : 10, 9 : 10).The resulting 30 claims were split over 5 evaluators.Each claim was evaluated by at least 2 evaluators.The evaluators were provided with the claim, the original claim information (source paper reference and the chunk used to generate the claim), background information (acronyms, definitions, etc.), and all literature chunks considered by ContraCrow (up to 15).The evaluators were tasked with assigning a Likert scale value and a binary ("yes" or "no") label to determine whether each claim contained a contradiction to the provided sources.For this task, the evaluators did not have access to ContraCrow's reasoning.Similar to the first task, they were instructed to use any provided materials for clarification, but to only use the provided literature when making a final decision.</p>
<p>In the both the "contradiction validation" task and the binary "contradiction detection" task, ContraCrow scores of &gt;= 8 were considered the positive case in order to compare to humans' binary outputs.For the Likert score tasks, raw scores were used.</p>
<p>Evaluators were compensated for their work on grading claims to encourage thorough review.For the "contradiction validation" task, they were paid $12 per claim completed.For "contradiction validation", they were paid $20 per claim completed.They were additionally paid a $200 'completion bonus' for completing grading on all claims by a deadline approximately one week after starting.</p>
<p>Evaluators were sent the questions with the following instructions:</p>
<p>Contradiction Detection Instructions Task overview</p>
<p>For this task, we are interested in identifying and rating contradicting research findings, or claims.We've assembled a set of claims from research papers that are potentially contradicted by other findings, either prior or later.Your task will be to both assess the claim itself, as well as review portions of text taken from research papers that may be related to and may or may not present contradicting information to the presented claim.We will not have an explicit calibration phase for this task, nor do we provide examples.More detailed instructions are provided below to help guide you.You may use any sources available to you in order to understand concepts, acronyms, etc. that you are unfamiliar with, but you should perform the evaluation itself by-hand and using only the claim and context provided.We also explicitly ask that you do not use ChatGPT or other AI tools to come to your conclusions, though they may be used if helpful to define terms or clarify concepts.Please do not put claims or chunks into ChatGPT and similar tools.Instructions This task will consist of two separate activities, Claim Evaluation and Contradiction Detection (type a and b).For each claim, you will be given the following information:</p>
<p>• Claim: a single sentence making some claim or finding.</p>
<p>• Chunk: the section of text from the source paper that was used to generate the claim.</p>
<p>• Section: the section of the paper that the chunk lives in, if available • Title: the title of the paper that the chunk lives in, if available • DOI: the DOI URL for the paper that the chunk lives in, if available Section 1: Claim Evaluation Your first task is to a determination of the claim being a valid claim.Some examples of good &amp; bad claims are shown below:</p>
<p>• Good claims are typically clear, testable, and generalizable -"Cells expressing ARG1 are more likely to undergo apoptosis."• Bad claims will come in different forms, like not having enough context, i.e. it does not make sense outside the context of the paper.</p>
<p>-"Cells in cluster 1 had higher expression of ARG1." -"Cells expressing this gene are more likely to undergo apoptosis."-"Method A performed better than Method B in our study."-Claims provided may also just not really be defined as a 'claim'.These might describe methods, descriptions, or obvious common knowledge • "We used method A to test the effects of ARG1 on cell death."</p>
<p>• "ARG1 is a protein."-Claims may also not be supported by the provided chunk (Mixed up gene names, off-topic discussion, etc.) To evaluate each claim, select all that apply from the following options:</p>
<p>• good claim</p>
<p>• not enough context • not generalizable</p>
<p>• not a claim • not supported Some important considerations:</p>
<p>• Multiple claims may come from the same chunk &amp; claims may be redundant in nature • Consider each claim independently • You may use the all provided information in this section to help you on this step (including the paper title, as it may provide context) • Avoid:</p>
<p>-Evaluating the claim for truth value -Evaluating the wording/syntax of the claim, unless it changes the meaning -Deep-diving into the paper outside of the claim -Spending more than a few minutes grading each claim, on average Section 2: Contradiction Detection This part will contain additional information for each claim, consisting of up to 15 "evidence chunks" which are sections of research paper text that may serve as evidence either supporting, contradicting or neutral to the claim to various degrees.Each chunk will itself contain (when available) the source paper's citation.This information can be used for reference and clarity (for defining acronyms, clarifying things, etc) but should not be considered when determining contradictions.Your job for this part of the task is to determine whether (and to what extent) the claim contradicts or agrees with the provided evidence chunks.You will provide a selection for both of the below categories (note there are two separate selections!) -YES -Claim contradicts the provided context -NO -Claim does not contradict the provided context Some important considerations:</p>
<p>• A claim may have multiple chunks from the same paper.</p>
<p>• Consider claims individually.If the claim is a bad claim, use your best judgement about whether or not and how to evaluate the contradiction.We ask that you attempt to evaluate it if possible, and if you feel you cannot proceed, please explain why then move on to the next claim.• Focus on the claim + context provided: Don't worry about other contradictions or agreements in the context, only focus on the specific claim.Additional information (publication information, background info) is provided for clarification where needed, but you should rely solely on the provided context and claim for labeling.• It is possible that the provided statement makes more than 1 claim.Use your best judgement in this case, considering the context provided.• Avoid:</p>
<p>-Evaluating the claim or evidences for truth value or quality (only evaluate in relation to each other).-Deep-diving into any text outside of the provided context (including the papers from which the context was derived).</p>
<p>Contradiction Validation Instructions Task overview</p>
<p>This task is very similar, but this time you will be given an AI model's reasoning on the evidence chunks, and only the evidence chunks the model considered (which should cut down the review time considerably).You will do the usual annotation, and additionally say whether you agree with the LLM's reasoning.Section 1 is unchanged, Section 2 is different.</p>
<p>We also explicitly ask that you do not use ChatGPT or other AI tools to come to your conclusions, though they may be used if helpful to define terms or clarify concepts.Please do not put claims or chunks into ChatGPT and similar tools.Instructions This task will consist of two separate activities, Claim Evaluation and Contradiction Detection (type a and b).For each claim, you will be given the following information:</p>
<p>• Claim: a single sentence making some claim or finding.</p>
<p>• Chunk: the section of text from the source paper that was used to generate the claim.</p>
<p>• Section: the section of the paper that the chunk lives in, if available • Title: the title of the paper that the chunk lives in, if available • DOI: the DOI URL for the paper that the chunk lives in, if available Section 1: Claim Evaluation Section 1 is the same as in the "contradiction detection" task.Section 2: Contradiction Detection -Model Evaluation This part will contain additional information for each claim.Each claim will have a model response, which is some reasoning to answer the question 'Are there any contradictions to this claim in literature?' The model response will reference up to 10 "evidence chunks" which are sections of research paper text.These referenced chunks will also be provided for you and can be linked via the citation key in the model response (for example, the model response might reference Song2022DEPTH2:pages 16-17.The chunk corresponding to this reference will also be found under Song2022DEPTH2:pages 16-17.Each chunk will itself contain (when available) the source paper's citation.This information can be used for reference and clarity (for defining acronyms, clarifying things, etc) but should not be considered when evaluating contradictions or model response.Your job for this task is to determine whether you agree with the model response and to what extent the claim contradicts or agrees with the evidence chunks provided.You will provide a selection for both of the below categories (note there are two separate selections!) -YES -Claim contradicts the provided context -NO -Claim does not contradict the provided context • Optional Explanation: optionally, you may provide a written explanation/justification of your scoring.</p>
<p>Some important considerations:</p>
<p>• A claim may have multiple chunks from the same paper.</p>
<p>• Consider claims individually.</p>
<p>• Because the model response is provided, we ask that you evaluate every provided claim.</p>
<p>• Additional information (publication information, background info) is provided for clarification where needed.• It is possible that the provided statement makes more than one claim.Use your best judgment in this case, considering the model response provided.• Avoid:</p>
<p>-Evaluating the claim or evidences for truth value or quality (only evaluate in relation to each other).</p>
<p>Figure 1 :
1
Figure 1: A. Schematic of PaperQA2's agentic toolset along with relevant action representations within each tool.B. PaperQA2 performance across question answering, cited article summarization, and contradiction detection.Error bars represent standard error.</p>
<p>Figure 2 :
2
Figure 2: A. Example LitQA2 question, PaperQA2 answer, and metadata.B. PaperQA2 performance on LitQA2 across technologies.C. PaperQA2 performance studies and ablations across component categories.Error bars are 95% CI.D. Aggregated LitQA2 DOI recall per PaperQA2 stage.Search Recall includes DOIs found via the "Paper Search" or "Citation traversal" tools, Top-k Ranking includes all DOIs with similarity rankings below the top-k ranking cutoff (30), RCS includes all DOIs selected by RCS, and Attribution includes all DOIs cited in the "Generate Answer" tool.</p>
<p>Figure 3 :
3
Figure 3: A. WikiCrow prompt graph, showing the initial seed variable (gene name), PaperQA2 prompts for each section, and overview LLM call.Each section feeds into a Python script which stitches the outputs together.B. Sample WikiCrow article for gene FAM83H, with truncated sections under each header.C. Performance statistics for a WikiCrow vs. Wikipedia comparison performed by evaluators.D. "Cited and unsupported" issue categorization counts.</p>
<p>Figure 4 :
4
Figure 4: A. Schematic of ContraCrow.B. Likert scale used to evaluate contradictions, along with the integer mapping.C. ROC curve and metrics (Likert scale threshold of 8) of ContraCrow's performance on ContraDetect benchmark.D. Bar plot showing percentage of all claims found in 93 papers that are given scores of 8, 9, and 10 by ContraCrow.E. Histogram of the number of contradictions found per paper over 93 papers.The table shows the average number of claims (35.16 ± 21.72 [mean ± SD, N = 93]) and contradictions (2.34 ± 1.99 [mean ± SD, N = 93]) per paper found.F. Proportion of ContraCrow contradictions with scores 8 and 9-10 validated by expert evaluators.G. Range of evaluator scores for each claim.Bars represent the range of Likert scale scores assigned by expert evaluators and stars represent the ContraCrow label on the same claims.The table shows a mean standard deviation per claim of 1.45 (N = 30 claims) over all evaluators and a mean Pearson correlation coefficient of 0.579 ± 0.197 (mean ± SD) between evaluators (N = 5).</p>
<p>Implementation and Parameters All reported figures and data in this work were built on the open source PaperQA package, available on GitHub at paperqa.While the core PaperQA repository provides the basic algorithms used, it does not include the Grobid parsing code, access to non-local full-text literature searches, or the citation traversal tool.The open source version of PaperQA utilizes LangChain 1 for its agentic and state update operations.The full configuration objects for all experiments run in this paper are included for further customization.</p>
<p>7 :B 8 :D out ← {} 9 :
789
= o, {d | D∈D 1(d ∈ D) = o} for o ∈ [|D|, . . ., 1] ▷ Bin papers according to decreasing overlap for o, D ∈ B do ▷ Highest overlapping citations come first 10: if o &lt; θ o ∨ |D out | ≥ ℓ then break 11: D ← {d | d ∈ D ∧ d / ∈ D prev } ▷ Filter out already present papers 12: if ℓ − |D out | &lt; |D| then ▷ If the entire bin won't fit within limit ℓ 13: D ← {d | i, d ∈ SORT ↓CITERS (D) ∧ i ≤ (ℓ − |D out |)} ▷ Keep subset with the most future citers 14: D out ← D out ∪ D</p>
<p>Figure 5 :
5
Figure 5: PaperQA2 precision performance on LitQA2 across all configuration categories included in Figure 2. All error bars are 95% CI intervals.</p>
<p>Figure 6 :
6
Figure 6: Precision, accuracy and recall for different chunk size and parsing algorithm choices on LitQA.Note that the gap between recall and precision is because the model can sometimes find an alternative source to rule out distractors, provide the same information, or make the model confident enough to guess.</p>
<p>Gathering evidence will do nothing if you have not done a new search or collected new papers.If you do not have enough evidence to generate a good answer, you can:-Search for more papers (preferred) -Collect papers cited by previous evidence (preferred) -Gather more evidence using a different phrase If you search for more papers or collect new papers cited by previous evidence, remember to gather evidence again.Once you have five or more pieces of evidence from multiple sources, or you have tried a few times, call {gen_answer_tool_name} tool.The {gen_answer_tool_name} tool output is visible to the user, so you do not need to restate the answer and can simply terminate if the answer looks sufficient.The current status of evidence/papers/cost is {status}</p>
<ol>
<li>Our agent was prompted with the following message to guide tool usage: Answer question: {question}.Search for papers, gather evidence, collect papers cited in evidence then re-gather evidence, and answer.</li>
</ol>
<p>Table 1 :
1
Various statistics on citation traversal.Table showing the frequencies of citation overlap o seen in LitQA, illustrating the percentage of traversed citations at stake when filtering with an overlap threshold θo .We chose to specify θo = ⌈α × |Dprev|⌉, where α is known as the overlap fraction and was defaulted to13 .The bolded values show what overlaps would have been preserved using an α = 1 3 .
|D prev |12345678910Count2147 941 530 386 307 216 154 67 29 12Frequency (%)44.8 19.6 11.1 8.1 6.4 4.5 3.2 1.4 0.6 0.3(a) Distribution of traversal starting paper count |Dprev|.|D prev |1234567+Frequency of 1 Overlap (%)100.0 91.3 91.7 90.8 90.8 88.7 87.5Frequency of 2 Overlaps (%)8.77.47.57.28.28.5Frequency of 3 Overlaps (%)0.91.41.42.02.4Frequency of 4 Overlaps (%)0.30.50.70.9Frequency of 5 Overlaps (%)0.10.30.4Frequency of 6 Overlaps (%)0.10.2Frequency of 7+ Overlaps (%)0.1(b)</p>
<p>2&lt;105::aid-glia1&gt;3.0.co;2-h, 10.1089/scd.2015.0244,10.1002/glia.22882, and 10.1042/an20120041, leads to one DOI cited by four papers, five DOIs cited by three papers, 29 DOIs cited by two papers, and 428 DOIs cited by just one paper.</p>
<p>10.1016/j.mcn.2006.08.007, 10.1002/cpsc.17,10.1002/(sici)1098-1136(200004)30:</p>
<p>Table 2 :
2
Table of model choices using both stages of LitQA2 development, all errors shown are CI intervals.
AblationAccuracy (Old 147) Accuracy (New 101) Precision (Old 147) Precision (New 101)Claude-3-Opus59.0% ±2.4%54.5% ±6.1%93.9% ±3.7%82.6% ±6.0%Gemini-1.5-Pro55.8% ±5.7%59.4% ±2.3%93.2% ±1.8%83.0% ±4.6%GPT-4-Turbo64.4% ±1.8%63.7% ±7.4%91.3% ±2.1%81.8% ±7.3%</p>
<p>• Contradiction grade (choose one of the following):
-Explicit Agreement-Strong Agreement-Agreement-Possibly an Agreement-Lack of Evidence-Possibly a Contradiction-Nuanced Contradiction-Contradiction-Strong Contradiction-Explicit Contradiction• Contradiction determination:
Acknowledgments Work at FutureHouse is supported by the generosity of Eric and Wendy Schmidt.We also acknowledge Matt Rubashkin for contributions to an earlier stage of the project, and we acknowledge all current members of FutureHouse for useful discussions, including Sid Narayanan, Ryan Rhys-Griffiths, Cade Gordon, Peter Chang, and Conor Igoe.We also acknowledge support and resources from the Semantic Scholar Project at the Allen Institute for AI.Competing interestsThe authors report no competing interests.FutureHouse, Inc. is a non-profit research organization.Author Contributions MDS led work on PaperQA2 and WikiCrow, in collaboration with JDB.SC led work on ContraCrow.JML led work on LitQA.MH, MJH, and MP contributed questions to the LitQA benchmark.SGR and ADW conceived of and supervised the project.-Evaluating the model response truth value, quality, syntax, or style, except where directly related to the claim.-Deep-diving into any text outside of the provided context (including the papers from which the context was derived).
Retrieval-augmented generation for large language models: A survey. Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Haofen Wang, arXiv:2312.109972023arXiv preprint</p>
<p>Yijia Shao, Yucheng Jiang, Theodore A Kanell, Peter Xu, Omar Khattab, Monica S Lam, arXiv:2402.14207Assisting in writing wikipedia-like articles from scratch with large language models. 2024arXiv preprint</p>
<p>The semantic reader project: Augmenting scholarly documents through ai-powered interactive reading interfaces. Kyle Lo, Joseph Chee Chang, Andrew Head, Jonathan Bragg, Amy X Zhang, Cassidy Trier, Chloe Anastasiades, Tal August, Russell Authur, Danielle Bragg, Erin Bransom, Isabel Cachola, Stefan Candra, Yoganand Chandrasekhar, Yen-Sung Chen, Evie ( Yu-Yen, ) Cheng, Yvonne Chou, Doug Downey, Rob Evans, Raymond Fok, F Q Hu, Regan Huff, Dongyeop Kang, Tae Soo Kim, Rodney Michael Kinney, Aniket Kittur, B Hyeonsu, Egor Kang, Bailey Klevak, Michael Kuehl, Matt Langan, Jaron Latzke, Kelsey Lochner, Eric Stuart Macmillan, Tyler C Marsh, Aakanksha Murray, Ngoc-Uyen Naik, Srishti Nguyen, Soya Palani, Caroline Park, Napol Paulic, Rachatasumrit, Paul Smita R Rao, Zejiang Sayre, Pao Shen, Luca Siangliulue, Huy Soldaini, Madeleine Tran, Lucy Lu Van Zuylen, Christopher Wang, Caroline M Wilhelm, Jiangjiang Wu, Angele Yang, Marti A Zamarron, Daniel S Hearst, Weld, ArXiv, abs/2303.143342023</p>
<p>Sm Tonmoy, Vinija Zaman, Anku Jain, Rani, Aman Vipula Rawte, Amitava Chadha, Das, arXiv:2401.01313A comprehensive survey of hallucination mitigation techniques in large language models. 2024arXiv preprint</p>
<p>Large legal fictions: Profiling legal hallucinations in large language models. Matthew Dahl, Varun Magesh, Mirac Suzgun, Daniel E Ho, Journal of Legal Analysis. 1612024</p>
<p>PubMedQA: A dataset for biomedical research question answering. Qiao Jin, Bhuwan Dhingra, Zhengping Liu, William Cohen, Xinghua Lu, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). Kentaro Inui, Jing Jiang, Vincent Ng, Xiaojun Wan, the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)Hong Kong, ChinaAssociation for Computational LinguisticsNovember 2019</p>
<p>Bioasq-qa: A manually curated corpus for biomedical question answering. Anastasia Krithara, Anastasios Nentidis, Konstantinos Bougiatiotis, and Georgios Paliouras. 202310170</p>
<p>Qiao Jin, Bhuwan Dhingra, Zhengping Liu, William W Cohen, Xinghua Lu, arXiv:1909.06146Pubmedqa: A dataset for biomedical research question answering. 2019arXiv preprint</p>
<p>Pleiotropic action of genetic variation in znf804a on brain structure: a meta-analysis of magnetic resonance imaging studies. Shuai Wang, Yi He, Zi Chen, Yanzhang Li, Jingping Zhao, Luxian Lyu, Neuropsychiatric Disease and Treatment. 2019</p>
<p>Znf804a rs1344706 is associated with cortical thickness, surface area, and cortical volume of the unmedicated first episode schizophrenia and healthy controls. Qinling Wei, Meng Li, Zhuang Kang, Leijun Li, Feici Diao, Ruibin Zhang, Junjing Wang, Liangrong Zheng, Xue Wen, Jinbei Zhang, American Journal of Medical Genetics Part B: Neuropsychiatric Genetics. 16842015</p>
<p>Jon M Laurent, Joseph D Janizek, Michael Ruzo, Michaela M Hinks, Michael J Hammerling, Siddharth Narayanan, Manvitha Ponnapati, Andrew D White, Samuel G Rodriques, arXiv:2407.10362Lab-bench: Measuring capabilities of language models for biology research. 2024arXiv preprint</p>
<p>Sciqag: A framework for auto-generated scientific question answering dataset with fine-grained evaluation. Yuwei Wan, Aswathy Ajith, Yixuan Liu, Ke Lu, Clara Grazian, Bram Hoex, Wenjie Zhang, Chunyu Kit, Tong Xie, Ian Foster, arXiv:2405.099392024arXiv preprint</p>
<p>Know what you don't know: Unanswerable questions for squad. Pranav Rajpurkar, Robin Jia, Percy Liang, arXiv:1806.038222018arXiv preprint</p>
<p>Retrieval-augmented generation for knowledgeintensive nlp tasks. Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-Tau Yih, Tim Rocktäschel, Advances in Neural Information Processing Systems. 202033</p>
<p>Retrieval augmentation reduces hallucination in conversation. Kurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela, Jason Weston, arXiv:2104.075672021arXiv preprint</p>
<p>Chat-rec: Towards interactive and explainable llms-augmented recommender system. Yunfan Gao, Tao Sheng, Youlin Xiang, Yun Xiong, Haofen Wang, Jiawei Zhang, arXiv:2303.145242023arXiv preprint</p>
<p>Paperqa: Retrieval-augmented generative agent for scientific research. Jakub Lála, O' Odhran, Aleksandar Donoghue, Sam Shtedritski, Samuel G Cox, Andrew D Rodriques, White, arXiv:2312.075592023arXiv preprint</p>
<p>Mrkl systems: A modular, neuro-symbolic architecture that combines large language models, external knowledge sources and discrete reasoning. Ehud Karpas, Omri Abend, Yonatan Belinkov, Barak Lenz, Opher Lieber, Nir Ratner, Yoav Shoham, Hofit Bata, Yoav Levine, Kevin Leyton-Brown, arXiv:2205.004452022arXiv preprint</p>
<p>Large language models can be easily distracted by irrelevant context. Freda Shi, Xinyun Chen, Kanishka Misra, Nathan Scales, David Dohan, Ed H Chi, Nathanael Schärli, Denny Zhou, International Conference on Machine Learning. PMLR2023</p>
<p>. Perplexity, </p>
<p>Generationaugmented retrieval for open-domain question answering. Yuning Mao, Pengcheng He, Xiaodong Liu, Yelong Shen, Jianfeng Gao, Jiawei Han, Weizhu Chen, arXiv:2009.085532020arXiv preprint</p>
<p>Lost in the middle: How language models use long contexts. Kevin Nelson F Liu, John Lin, Ashwin Hewitt, Michele Paranjape, Fabio Bevilacqua, Percy Petroni, Liang, Transactions of the Association for Computational Linguistics. 122024</p>
<p>Introducing the next generation of Claude -anthropic. 13-07-2024</p>
<p>Exploring the challenges of open domain multi-document summarization. John Giorgi, Luca Soldaini, Bo Wang, Gary D Bader, Kyle Lo, Lucy Lu Wang, Arman Cohan, ArXiv, abs/2212.105262022</p>
<p>Fact checking: Task definition and dataset construction. Andreas Vlachos, Sebastian Riedel, Proceedings of the ACL 2014 workshop on language technologies and computational social science. the ACL 2014 workshop on language technologies and computational social science2014</p>
<p>Emergent: a novel data-set for stance classification. William Ferreira, Andreas Vlachos, Proceedings of the 2016 conference of the North American chapter of the association for computational linguistics: Human language technologies. ACL. the 2016 conference of the North American chapter of the association for computational linguistics: Human language technologies. ACL2016</p>
<p>Where the truth lies: Explaining the credibility of emerging claims on the web and social media. Kashyap Popat, Subhabrata Mukherjee, Jannik Strötgen, Gerhard Weikum, Proceedings of the 26th international conference on world wide web companion. the 26th international conference on world wide web companion2017</p>
<p>Averitec: A dataset for real-world claim verification with evidence from the web. Michael Schlichtkrull, Zhijiang Guo, Andreas Vlachos, Advances in Neural Information Processing Systems. 202436</p>
<p>Fact or fiction: Verifying scientific claims. David Wadden, Shanchuan Lin, Kyle Lo, Lucy Lu Wang, Madeleine Van Zuylen, Arman Cohan, Hannaneh Hajishirzi, arXiv:2004.149742020arXiv preprint</p>
<p>Multivers: Improving scientific claim verification with weak supervision and full. David Wadden, Kyle Lo, Lucy Lu Wang, Arman Cohan, Iz Beltagy, Hannaneh Hajishirzi, arXiv:2112.016402021-document context. arXiv preprint</p>
<p>The challenges of evaluating llm applications: An analysis of automated, human, and llm-based approaches. Bhashithe Abeysinghe, Ruhan Circi, arXiv:2406.033392024arXiv preprint</p>
<p>Lef-1 and tcf4 expression correlate inversely with survival in colorectal cancer. Lydia Kriegl, David Horst, Jana A Reiche, Jutta Engel, Thomas Kirchner, Andreas Jung, Journal of translational medicine. 82010</p>
<p>Comparative profiling of primary colorectal carcinomas and liver metastases identifies lef1 as a prognostic biomarker. Mei-Sze Albert Y Lin, Yoon-La Chua, William Choi, Young H Yeh, Raymond Kim, Gregg A Azzi, Kristin Adams, Matt Sainani, Van De Rijn, Samuel K So, PloS one. 62e166362011</p>
<p>Increased lef1 expression and decreased notch2 expression are strong predictors of poor outcomes in colorectal cancer patients. Wen-Juan Wang, Yu Yao, Li-Li Jiang, Ting-Hua Hu, Jie-Qun Ma, Zhi-Ping Ruan, Tao Tian, Hui Guo, Shu-Hong Wang, Ke-Jun Nan, Disease markers. 3552013</p>
<p>Knockdown of lymphoid enhancer factor 1 inhibits colon cancer progression in vitro and in vivo. Wen-Juan Wang, Yu Yao, Li-Li Jiang, Ting-Hua Hu, Jie-Qun Ma, Zi-Jun Liao, Jun-Tao Yao, Dong-Fan Li, Shu-Hong Wang, Ke-Jun Nan, PLoS One. 810e765962013</p>
<p>Lef-1 is frequently expressed in colorectal carcinoma and not in other gastrointestinal tract adenocarcinomas: an immunohistochemical survey of 602 gastrointestinal tract neoplasms. Priya Taher R Kermanshahi, Daniel T Jayachandran, Reet Chang, Pai, Applied Immunohistochemistry &amp; Molecular Morphology. 22102014</p>
<p>Affinity purification of an interferoninduced human guanylate-binding protein and its characterization. Mary F Ys Cheng, Theresa P Becker-Manley, Deborah C Chow, Horan, Journal of Biological Chemistry. 260291985</p>
<p>Intracellular trafficking of guanylate-binding proteins is regulated by heterodimerization in a hierarchical manner. Nathalie Britzen-Laurent, Michael Bauer, Valeria Berton, Nicole Fischer, Adrian Syguda, Simone Reipschläger, Elisabeth Naschberger, Christian Herrmann, Michael Stürzl, PloS one. 512e142462010</p>
<p>Golgi targeting of human guanylate-binding protein-1 requires nucleotide binding, isoprenylation, and an ifn-γ-inducible cofactor. Nir Modiano, Yanping E Lu, Peter Cresswell, Proceedings of the National Academy of Sciences. 102242005</p>
<p>Human guanylate binding protein-1 is a secreted gtpase present in increased concentrations in the cerebrospinal fluid of patients with bacterial meningitis. Elisabeth Naschberger, Clara Lubeseder-Martellato, Nadine Meyer, Ruth Gessner, Elisabeth Kremmer, Andrè Gessner, Michael Stürzl, The American journal of pathology. 16932006</p>
<p>Functional cross-species conservation of guanylate-binding proteins in innate immunity. Luca Schelle, João Vasco Côrte-Real, Pedro José Esteves, Joana Abrantes, Hanna-Mari Baldauf, Medical Microbiology and Immunology. 21222023</p>
<p>. Harrison Chase, Lanchain, 2022</p>
<p>Dagster. </p>
<p>Paperqa: Retrieval-augmented generative agent for scientific research. Jakub Lála, O' Odhran, Aleksandar Donoghue, Sam Shtedritski, Samuel G Cox, Andrew D Rodriques, White, arXiv:2312.075592023arXiv preprint</p>
<p>The semantic scholar open data platform. Rodney Kinney, Chloe Anastasiades, Russell Authur, Iz Beltagy, Jonathan Bragg, Alexandra Buraczynski, Isabel Cachola, Stefan Candra, Yoganand Chandrasekhar, Arman Cohan, arXiv:2301.101402023arXiv preprint</p>
<p>. Tiktoken, </p>
<p>Query rewriting for retrieval-augmented large language models. Xinbei Ma, Yeyun Gong, Pengcheng He, Hai Zhao, Nan Duan, arXiv:2305.142832023arXiv preprint</p>
<p>Retrieval-augmented generation for large language models: A survey. Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Haofen Wang, arXiv:2312.109972023arXiv preprint</p>
<p>Parth Sarthi, Salman Abdullah, Aditi Tuli, Shubh Khanna, Anna Goldie, Christopher D Manning, arXiv:2401.18059Raptor: Recursive abstractive processing for tree-organized retrieval. 2024arXiv preprint</p>
<p>Open-source large language models are strong zero-shot query likelihood models for document ranking. Shengyao Zhuang, Bing Liu, Bevan Koopman, Guido Zuccon, arXiv:2310.132432023arXiv preprint</p>
<p>Chat-rec: Towards interactive and explainable llms-augmented recommender system. Yunfan Gao, Tao Sheng, Youlin Xiang, Yun Xiong, Haofen Wang, Jiawei Zhang, arXiv:2303.145242023arXiv preprint</p>
<p>Rankrag: Unifying context ranking with retrieval-augmented generation in llms. Yue Yu, Wei Ping, Zihan Liu, Boxin Wang, Jiaxuan You, Chao Zhang, Mohammad Shoeybi, Bryan Catanzaro, 20242407arXiv e-prints</p>
<p>Jon M Laurent, Joseph D Janizek, Michael Ruzo, Michaela M Hinks, Michael J Hammerling, Siddharth Narayanan, Manvitha Ponnapati, Andrew D White, Samuel G Rodriques, arXiv:2407.10362Lab-bench: Measuring capabilities of language models for biology research. 2024arXiv preprint</p>
<p>. Perplexity, </p>
<p>List of human protein-coding genes 1 -Wikipedia, the free encyclopedia. Wikipedia contributors. 2024. July-202411</p>
<p>Wikicrow: Automating synthesis of human scientific knowledge. Sam Cox, Michael Hammerling, Jakub Lála, Jon Laurent, Sam Rodriques, Matt Rubashkin, Andrew White, </p>
<p>Introducing the next generation of Claude -anthropic. 13-07-2024</p>
<p>Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, arXiv:2303.08774Shyamal Anadkat, et al. Gpt-4 technical report. 2023arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
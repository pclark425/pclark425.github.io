<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8635 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8635</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8635</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-155.html">extraction-schema-155</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <p><strong>Paper ID:</strong> paper-271516332</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2407.18897v1.pdf" target="_blank">Small Molecule Optimization with Large Language Models</a></p>
                <p><strong>Paper Abstract:</strong> Recent advancements in large language models have opened new possibilities for generative molecular drug design. We present Chemlactica and Chemma, two language models fine-tuned on a novel corpus of 110M molecules with computed properties, totaling 40B tokens. These models demonstrate strong performance in generating molecules with specified properties and predicting new molecular characteristics from limited samples. We introduce a novel optimization algorithm that leverages our language models to optimize molecules for arbitrary properties given limited access to a black box oracle. Our approach combines ideas from genetic algorithms, rejection sampling, and prompt optimization. It achieves state-of-the-art performance on multiple molecular optimization benchmarks, including an 8% improvement on Practical Molecular Optimization compared to previous methods. We publicly release the training corpus, the language models and the optimization algorithm.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8635.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8635.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Chemlactica-125M</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chemlactica (125M parameters)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 125M-parameter causal transformer language model fine-tuned on a novel SMILES-based corpus of ~110M PubChem-derived molecules and computed properties; used for property-conditioned SMILES generation, property prediction, and in a population-based LLM-enhanced optimization loop for drug-design benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Chemlactica-125M</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>causal transformer / large language model (SMILES-tokenized)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>125M parameters</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Novel JSONL corpus built from PubChem (cutoff Jan 26, 2023): ~110M molecules, computed properties (QED, SAS, CLogP, TPSA, MW, H-bond counts, ring counts), similarity pairs sampled and recomputed (ECFC4); training corpus ~40B tokens with property-tagged SMILES templates.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>drug discovery / small-molecule optimization (PMO benchmark, docking tasks, QED similarity-constrained optimization)</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Prompt-based conditional SMILES generation using tagged templates (e.g., [QED]v[/QED][START_SMILES]) with sampling controls (temperature, repetition penalty, undesired-token suppression, chain-of-thought trick) and integration in an LLM-enhanced population algorithm that generates candidates, evaluates with a black-box oracle, and fine-tunes on high-performing molecules.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>Generated SMILES validated with RDKit; novelty quantified only partly in examples (e.g., some conditional-generation runs reported 0–15% exact matches to PubChem in illustrative samples). The paper does not provide a single global percentage of molecules absent from training data; many generated molecules are unique across optimization runs but some overlap with PubChem entries.</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Conditioning via property tags in prompts ([QED], [CLOGP], [TPSA], [SIMILAR]) and iterative oracle-guided selection/fine-tuning ensure molecules are steered toward specific objectives (e.g., maximize docking reward, QED≥0.9 with similarity≥0.4).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>RMSE and corrected RMSE for property-conditioned generation and property prediction; Top-10 AUC (PMO benchmark) for optimization; generative yield and oracle burden for docking benchmarks; success rate for QED+similarity constrained design; synthesizability score (SAS), QED, CLogP, TPSA, molecular weight, Pearson correlations on ADMET/regression tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Chemlactica-125M outperformed several baselines on PMO (AUC Top-10 sum across 23 tasks = 17.170 vs prior Genetic-guided GFlowNets 16.213), achieved strong calibration and property-conditioned generation (low RMSE for many properties), excelled in oracle-burden metrics on docking tasks and reached 99.0% success on the QED similarity-constrained optimization while using fewer QED evaluations than baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Compared to REINVENT, Augmented Memory, Genetic-guided GFlowNets and beam-enumeration variants; Chemlactica-125M produced higher PMO AUC Top-10 and better or comparable docking oracle-burden metrics, and higher success rate on the QED similarity task versus RetMol and other baselines reported.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>Operates solely on SMILES (no 3D coordinate modeling), limited protein/biomolecular understanding, does not explicitly enforce synthetic accessibility during optimization, performance sensitive to numerical precision (bfloat16 caused degraded iterative optimization performance), and optimization hyperparameters not exhaustively tuned. Batched generation/padding and low-precision lead to cascaded sub-optimal fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Small Molecule Optimization with Large Language Models', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8635.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8635.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Chemlactica-1.3B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chemlactica (1.3B parameters)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 1.3B-parameter causal transformer LLM variant of Chemlactica trained/finetuned on the same SMILES/property corpus to improve exploitation in molecular optimization tasks and conditional generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Chemlactica-1.3B</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>causal transformer / large language model (SMILES-tokenized)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>1.3B parameters</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Same PubChem-derived JSONL corpus (~110M molecules, computed properties, similarity pairs) used for Chemlactica family training; tokens include special property tags and SMILES delimiters.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>drug discovery / molecular optimization (PMO benchmark, docking tasks, property prediction)</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Property-conditioned prompt-based SMILES generation incorporated into the population-based LLM-enhanced genetic-style optimization algorithm with periodic fine-tuning on high-performing molecules, and sampling hyperparameters tuned (dynamic temperature scheduling, repetition penalty, undesired-token suppression).</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>As for Chemlactica-125M, explicit global novelty fraction not reported; empirical results indicate many unique high-scoring molecules across optimization runs and improved yield/exploitation behavior versus smaller model, but some samples overlap PubChem entries in examples.</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Ensures tailoring via explicit property tags in prompts, using oracle scores to select top-P molecules for pool updates and to fine-tune the model when stagnation detected (K iterations), facilitating targeted exploitation of reward landscapes.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Same as Chemlactica-125M: RMSE for conditional generation, Top-10 AUC for PMO, generative yield and oracle burden for docking, Pearson correlations for regression tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Chemlactica-1.3B slightly improves PMO performance over the 125M variant (AUC Top-10 = 17.284) and showed higher exploitation capability in some docking yield metrics; larger model size shifted trade-off toward exploitation versus exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Outperforms or matches prior methods (REINVENT, Augmented Memory, Genetic-guided GFlowNets) on aggregate PMO metrics; relative to the smaller Chemlactica-125M it improves some yield-oriented docking metrics but may require more oracle calls to find diverse candidates.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>Same limitations as Chemlactica-125M (SMILES-only, no 3D, limited protein knowledge, lack of explicit synthetic accessibility constraints). Also observed exploration–exploitation trade-off where larger models exploit reward space more strongly but may reduce initial exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Small Molecule Optimization with Large Language Models', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8635.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8635.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Chemma-2B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chemma (2B parameters, Gemma-based)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 2B-parameter causal language model (based on Gemma) fine-tuned on the SMILES/property corpus; optimized for conditional generation and used inside the LLM-enhanced population optimization pipeline, excelling in generative yield for docking tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Chemma-2B</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>causal transformer / large language model (SMILES-tokenized)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>2B parameters (evaluated at different training token budgets, including 2.1B and up to 39B tokens fine-tuning reported)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Same PubChem-derived JSONL corpus (~110M molecules, computed properties, similarity pairs); uses Gemma tokenizer extended with chemistry tokens and property tags; Chemmma trained in bfloat16 for compute efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>drug discovery / molecular optimization (PMO benchmark, docking case studies, property prediction and conditional generation)</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Tagged-prompt conditional SMILES generation, sampling with repetition penalty and token suppression, integrated in the population-based LLM-enhanced genetic-style algorithm with periodic fine-tuning and explicit oracle modeling; also evaluated with chain-of-thought prompt variant for property precomputation.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>Novelty not globally quantified; experimental examples and docking tasks show Chemma produces many unique high-reward molecules and achieves high generative yield; some illustrative conditional generation samples include matches to PubChem but majority are new candidates for docking optimization.</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Conditioning via property tags and [SIMILAR] prompts; explicit oracle modeling by including [PROPERTY]O(m)[/PROPERTY] in fine-tuning samples enables the model to learn mapping between structure and oracle score for targeted generation (e.g., docking score minimization combined with QED and MW constraints).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>RMSE/corrected RMSE for conditional generation, Top-10 AUC for PMO, generative yield and oracle burden for docking tasks, Pearson correlations for regression benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Chemma-2B achieved the best generative yield across all evaluated docking receptors and the highest PMO AUC Top-10 among reported models (17.534), indicating superior capability in generating many high-reward molecules; calibration and property-prediction performance are strong when trained on sufficient tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Surpassed prior baselines (REINVENT, Augmented Memory, Genetic-guided GFlowNets) on aggregate PMO metrics and outperformed smaller Chemlactica variants in generative-yield metrics for docking; shows the expected model-size advantage in exploitation.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>Same methodological limitations: SMILES-only modeling, lack of explicit 3D/ protein representations, not enforcing synthetic accessibility, sensitivity to numeric precision and batching issues; in-context learning attempts did not improve performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Small Molecule Optimization with Large Language Models', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8635.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8635.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-enhanced genetic algorithm</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Population-based LLM-enhanced genetic-style optimization algorithm with explicit oracle modeling</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A novel optimization algorithm that replaces explicit crossover/mutation with LLM-based generation of molecules similar to a pool, incorporates oracle feedback via fine-tuning on high-performing molecules, and applies population selection and dynamic fine-tuning when progress stalls.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLM-enhanced genetic algorithm (uses Chemlactica/Chemma models as generators)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>algorithm combining population-based optimization, language-model generation, and fine-tuning (meta-algorithm rather than a single neural architecture)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>N/A (algorithm depends on generator model size; experiments reported with 125M, 1.3B, and 2B LLMs)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Does not require additional pretraining data beyond the models; when stagnation occurs, the algorithm fine-tunes the LLM on samples derived from the current pool of high-scoring molecules using prompts that include similarity tags and oracle scores.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>sample-efficient molecular optimization across drug-discovery benchmarks: Practical Molecular Optimization (PMO), docking-based multi-property optimization, similarity-constrained QED maximization, and property prediction/fine-tuning tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Iterative loop: maintain top-P pool; for N iterations sample S similar molecules from pool, build prompt via molecules2prompt with [SIMILAR] and optionally [PROPERTY] tags, generate N candidates with LLM, evaluate with black-box oracle O(m), update pool to top-P, and trigger fine-tuning on high-performing molecules if no improvement over K iterations. Uses dynamic temperature scheduling and sampling controls.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>Algorithm encourages exploration via temperature scheduling (temperature ramp from 1 to 1.5) and sampling from diverse prompts; novelty arises through LLM-driven recombination of pool molecules and fine-tuning on high-scoring examples, but exact novelty statistics are task-dependent and not globally reported.</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Explicit oracle modeling (prompting with [PROPERTY]O(m)[/PROPERTY] plus similarity-conditioned prompts) ensures the generator learns correlations between structures and target oracle scores; prompts can include known computed properties to bias generation toward application-specific objective functions (e.g., docking rewards plus QED and MW constraints).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Top-10 AUC (PMO), generative yield and oracle burden (docking), success rate under similarity+QED constraints, RMSE for conditional property generation; optimization-specific hyperparameters P (pool size), S (similar molecules), K (finetuning tolerance), N (batch generated) were tuned via grid search on PMO tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Algorithm yielded state-of-the-art results across multiple benchmarks: improved PMO aggregate AUC relative to prior SOTA (e.g., Chemlactica-125M AUC 17.170 vs Genetic-guided GFlowNets 16.213), produced higher generative yields in docking tasks (Chemma-2B), and achieved a 99.0% success rate on the QED similarity-constrained task while using fewer oracle evaluations than baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Directly compared against REINVENT, Augmented Memory, Genetic-guided GFlowNets, and beam-enumeration variants; shows superior sample efficiency and higher aggregate optimization metrics in reported experiments. The method integrates ideas from genetic algorithms, rejection sampling, and prompt optimization.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>Requires careful hyperparameter tuning (P, S, K, lr) though authors freeze less-sensitive ones; sensitive to numerical precision/ batching effects which can degrade iterative fine-tuning; does not enforce synthetic accessibility or enforce 3D structural constraints; in-context learning approaches did not yield benefit; potential unfair advantage when including known computed properties in prompts (partial oracle leakage).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Small Molecule Optimization with Large Language Models', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Chemformer: a pre-trained transformer for computational chemistry <em>(Rating: 2)</em></li>
                <li>BARTSmiles: Generative masked language models for molecular representations <em>(Rating: 2)</em></li>
                <li>MolT5 <em>(Rating: 1)</em></li>
                <li>Reinvent 2.0: an ai tool for de novo drug design <em>(Rating: 2)</em></li>
                <li>Genetic-guided gflownets: Advancing in practical molecular optimization benchmark <em>(Rating: 2)</em></li>
                <li>Beam enumeration: Probabilistic explainability for sample efficient selfconditioned molecular design <em>(Rating: 2)</em></li>
                <li>Augmented memory: Capitalizing on experience replay to accelerate de novo molecular design <em>(Rating: 2)</em></li>
                <li>Evoprompting: Language models for code-level neural architecture search <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8635",
    "paper_id": "paper-271516332",
    "extraction_schema_id": "extraction-schema-155",
    "extracted_data": [
        {
            "name_short": "Chemlactica-125M",
            "name_full": "Chemlactica (125M parameters)",
            "brief_description": "A 125M-parameter causal transformer language model fine-tuned on a novel SMILES-based corpus of ~110M PubChem-derived molecules and computed properties; used for property-conditioned SMILES generation, property prediction, and in a population-based LLM-enhanced optimization loop for drug-design benchmarks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Chemlactica-125M",
            "model_type": "causal transformer / large language model (SMILES-tokenized)",
            "model_size": "125M parameters",
            "training_data": "Novel JSONL corpus built from PubChem (cutoff Jan 26, 2023): ~110M molecules, computed properties (QED, SAS, CLogP, TPSA, MW, H-bond counts, ring counts), similarity pairs sampled and recomputed (ECFC4); training corpus ~40B tokens with property-tagged SMILES templates.",
            "application_domain": "drug discovery / small-molecule optimization (PMO benchmark, docking tasks, QED similarity-constrained optimization)",
            "generation_method": "Prompt-based conditional SMILES generation using tagged templates (e.g., [QED]v[/QED][START_SMILES]) with sampling controls (temperature, repetition penalty, undesired-token suppression, chain-of-thought trick) and integration in an LLM-enhanced population algorithm that generates candidates, evaluates with a black-box oracle, and fine-tunes on high-performing molecules.",
            "novelty_of_chemicals": "Generated SMILES validated with RDKit; novelty quantified only partly in examples (e.g., some conditional-generation runs reported 0–15% exact matches to PubChem in illustrative samples). The paper does not provide a single global percentage of molecules absent from training data; many generated molecules are unique across optimization runs but some overlap with PubChem entries.",
            "application_specificity": "Conditioning via property tags in prompts ([QED], [CLOGP], [TPSA], [SIMILAR]) and iterative oracle-guided selection/fine-tuning ensure molecules are steered toward specific objectives (e.g., maximize docking reward, QED≥0.9 with similarity≥0.4).",
            "evaluation_metrics": "RMSE and corrected RMSE for property-conditioned generation and property prediction; Top-10 AUC (PMO benchmark) for optimization; generative yield and oracle burden for docking benchmarks; success rate for QED+similarity constrained design; synthesizability score (SAS), QED, CLogP, TPSA, molecular weight, Pearson correlations on ADMET/regression tasks.",
            "results_summary": "Chemlactica-125M outperformed several baselines on PMO (AUC Top-10 sum across 23 tasks = 17.170 vs prior Genetic-guided GFlowNets 16.213), achieved strong calibration and property-conditioned generation (low RMSE for many properties), excelled in oracle-burden metrics on docking tasks and reached 99.0% success on the QED similarity-constrained optimization while using fewer QED evaluations than baselines.",
            "comparison_to_other_methods": "Compared to REINVENT, Augmented Memory, Genetic-guided GFlowNets and beam-enumeration variants; Chemlactica-125M produced higher PMO AUC Top-10 and better or comparable docking oracle-burden metrics, and higher success rate on the QED similarity task versus RetMol and other baselines reported.",
            "limitations_and_challenges": "Operates solely on SMILES (no 3D coordinate modeling), limited protein/biomolecular understanding, does not explicitly enforce synthetic accessibility during optimization, performance sensitive to numerical precision (bfloat16 caused degraded iterative optimization performance), and optimization hyperparameters not exhaustively tuned. Batched generation/padding and low-precision lead to cascaded sub-optimal fine-tuning.",
            "uuid": "e8635.0",
            "source_info": {
                "paper_title": "Small Molecule Optimization with Large Language Models",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "Chemlactica-1.3B",
            "name_full": "Chemlactica (1.3B parameters)",
            "brief_description": "A 1.3B-parameter causal transformer LLM variant of Chemlactica trained/finetuned on the same SMILES/property corpus to improve exploitation in molecular optimization tasks and conditional generation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Chemlactica-1.3B",
            "model_type": "causal transformer / large language model (SMILES-tokenized)",
            "model_size": "1.3B parameters",
            "training_data": "Same PubChem-derived JSONL corpus (~110M molecules, computed properties, similarity pairs) used for Chemlactica family training; tokens include special property tags and SMILES delimiters.",
            "application_domain": "drug discovery / molecular optimization (PMO benchmark, docking tasks, property prediction)",
            "generation_method": "Property-conditioned prompt-based SMILES generation incorporated into the population-based LLM-enhanced genetic-style optimization algorithm with periodic fine-tuning on high-performing molecules, and sampling hyperparameters tuned (dynamic temperature scheduling, repetition penalty, undesired-token suppression).",
            "novelty_of_chemicals": "As for Chemlactica-125M, explicit global novelty fraction not reported; empirical results indicate many unique high-scoring molecules across optimization runs and improved yield/exploitation behavior versus smaller model, but some samples overlap PubChem entries in examples.",
            "application_specificity": "Ensures tailoring via explicit property tags in prompts, using oracle scores to select top-P molecules for pool updates and to fine-tune the model when stagnation detected (K iterations), facilitating targeted exploitation of reward landscapes.",
            "evaluation_metrics": "Same as Chemlactica-125M: RMSE for conditional generation, Top-10 AUC for PMO, generative yield and oracle burden for docking, Pearson correlations for regression tasks.",
            "results_summary": "Chemlactica-1.3B slightly improves PMO performance over the 125M variant (AUC Top-10 = 17.284) and showed higher exploitation capability in some docking yield metrics; larger model size shifted trade-off toward exploitation versus exploration.",
            "comparison_to_other_methods": "Outperforms or matches prior methods (REINVENT, Augmented Memory, Genetic-guided GFlowNets) on aggregate PMO metrics; relative to the smaller Chemlactica-125M it improves some yield-oriented docking metrics but may require more oracle calls to find diverse candidates.",
            "limitations_and_challenges": "Same limitations as Chemlactica-125M (SMILES-only, no 3D, limited protein knowledge, lack of explicit synthetic accessibility constraints). Also observed exploration–exploitation trade-off where larger models exploit reward space more strongly but may reduce initial exploration.",
            "uuid": "e8635.1",
            "source_info": {
                "paper_title": "Small Molecule Optimization with Large Language Models",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "Chemma-2B",
            "name_full": "Chemma (2B parameters, Gemma-based)",
            "brief_description": "A 2B-parameter causal language model (based on Gemma) fine-tuned on the SMILES/property corpus; optimized for conditional generation and used inside the LLM-enhanced population optimization pipeline, excelling in generative yield for docking tasks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Chemma-2B",
            "model_type": "causal transformer / large language model (SMILES-tokenized)",
            "model_size": "2B parameters (evaluated at different training token budgets, including 2.1B and up to 39B tokens fine-tuning reported)",
            "training_data": "Same PubChem-derived JSONL corpus (~110M molecules, computed properties, similarity pairs); uses Gemma tokenizer extended with chemistry tokens and property tags; Chemmma trained in bfloat16 for compute efficiency.",
            "application_domain": "drug discovery / molecular optimization (PMO benchmark, docking case studies, property prediction and conditional generation)",
            "generation_method": "Tagged-prompt conditional SMILES generation, sampling with repetition penalty and token suppression, integrated in the population-based LLM-enhanced genetic-style algorithm with periodic fine-tuning and explicit oracle modeling; also evaluated with chain-of-thought prompt variant for property precomputation.",
            "novelty_of_chemicals": "Novelty not globally quantified; experimental examples and docking tasks show Chemma produces many unique high-reward molecules and achieves high generative yield; some illustrative conditional generation samples include matches to PubChem but majority are new candidates for docking optimization.",
            "application_specificity": "Conditioning via property tags and [SIMILAR] prompts; explicit oracle modeling by including [PROPERTY]O(m)[/PROPERTY] in fine-tuning samples enables the model to learn mapping between structure and oracle score for targeted generation (e.g., docking score minimization combined with QED and MW constraints).",
            "evaluation_metrics": "RMSE/corrected RMSE for conditional generation, Top-10 AUC for PMO, generative yield and oracle burden for docking tasks, Pearson correlations for regression benchmarks.",
            "results_summary": "Chemma-2B achieved the best generative yield across all evaluated docking receptors and the highest PMO AUC Top-10 among reported models (17.534), indicating superior capability in generating many high-reward molecules; calibration and property-prediction performance are strong when trained on sufficient tokens.",
            "comparison_to_other_methods": "Surpassed prior baselines (REINVENT, Augmented Memory, Genetic-guided GFlowNets) on aggregate PMO metrics and outperformed smaller Chemlactica variants in generative-yield metrics for docking; shows the expected model-size advantage in exploitation.",
            "limitations_and_challenges": "Same methodological limitations: SMILES-only modeling, lack of explicit 3D/ protein representations, not enforcing synthetic accessibility, sensitivity to numeric precision and batching issues; in-context learning attempts did not improve performance.",
            "uuid": "e8635.2",
            "source_info": {
                "paper_title": "Small Molecule Optimization with Large Language Models",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "LLM-enhanced genetic algorithm",
            "name_full": "Population-based LLM-enhanced genetic-style optimization algorithm with explicit oracle modeling",
            "brief_description": "A novel optimization algorithm that replaces explicit crossover/mutation with LLM-based generation of molecules similar to a pool, incorporates oracle feedback via fine-tuning on high-performing molecules, and applies population selection and dynamic fine-tuning when progress stalls.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LLM-enhanced genetic algorithm (uses Chemlactica/Chemma models as generators)",
            "model_type": "algorithm combining population-based optimization, language-model generation, and fine-tuning (meta-algorithm rather than a single neural architecture)",
            "model_size": "N/A (algorithm depends on generator model size; experiments reported with 125M, 1.3B, and 2B LLMs)",
            "training_data": "Does not require additional pretraining data beyond the models; when stagnation occurs, the algorithm fine-tunes the LLM on samples derived from the current pool of high-scoring molecules using prompts that include similarity tags and oracle scores.",
            "application_domain": "sample-efficient molecular optimization across drug-discovery benchmarks: Practical Molecular Optimization (PMO), docking-based multi-property optimization, similarity-constrained QED maximization, and property prediction/fine-tuning tasks.",
            "generation_method": "Iterative loop: maintain top-P pool; for N iterations sample S similar molecules from pool, build prompt via molecules2prompt with [SIMILAR] and optionally [PROPERTY] tags, generate N candidates with LLM, evaluate with black-box oracle O(m), update pool to top-P, and trigger fine-tuning on high-performing molecules if no improvement over K iterations. Uses dynamic temperature scheduling and sampling controls.",
            "novelty_of_chemicals": "Algorithm encourages exploration via temperature scheduling (temperature ramp from 1 to 1.5) and sampling from diverse prompts; novelty arises through LLM-driven recombination of pool molecules and fine-tuning on high-scoring examples, but exact novelty statistics are task-dependent and not globally reported.",
            "application_specificity": "Explicit oracle modeling (prompting with [PROPERTY]O(m)[/PROPERTY] plus similarity-conditioned prompts) ensures the generator learns correlations between structures and target oracle scores; prompts can include known computed properties to bias generation toward application-specific objective functions (e.g., docking rewards plus QED and MW constraints).",
            "evaluation_metrics": "Top-10 AUC (PMO), generative yield and oracle burden (docking), success rate under similarity+QED constraints, RMSE for conditional property generation; optimization-specific hyperparameters P (pool size), S (similar molecules), K (finetuning tolerance), N (batch generated) were tuned via grid search on PMO tasks.",
            "results_summary": "Algorithm yielded state-of-the-art results across multiple benchmarks: improved PMO aggregate AUC relative to prior SOTA (e.g., Chemlactica-125M AUC 17.170 vs Genetic-guided GFlowNets 16.213), produced higher generative yields in docking tasks (Chemma-2B), and achieved a 99.0% success rate on the QED similarity-constrained task while using fewer oracle evaluations than baselines.",
            "comparison_to_other_methods": "Directly compared against REINVENT, Augmented Memory, Genetic-guided GFlowNets, and beam-enumeration variants; shows superior sample efficiency and higher aggregate optimization metrics in reported experiments. The method integrates ideas from genetic algorithms, rejection sampling, and prompt optimization.",
            "limitations_and_challenges": "Requires careful hyperparameter tuning (P, S, K, lr) though authors freeze less-sensitive ones; sensitive to numerical precision/ batching effects which can degrade iterative fine-tuning; does not enforce synthetic accessibility or enforce 3D structural constraints; in-context learning approaches did not yield benefit; potential unfair advantage when including known computed properties in prompts (partial oracle leakage).",
            "uuid": "e8635.3",
            "source_info": {
                "paper_title": "Small Molecule Optimization with Large Language Models",
                "publication_date_yy_mm": "2024-07"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Chemformer: a pre-trained transformer for computational chemistry",
            "rating": 2,
            "sanitized_title": "chemformer_a_pretrained_transformer_for_computational_chemistry"
        },
        {
            "paper_title": "BARTSmiles: Generative masked language models for molecular representations",
            "rating": 2,
            "sanitized_title": "bartsmiles_generative_masked_language_models_for_molecular_representations"
        },
        {
            "paper_title": "MolT5",
            "rating": 1
        },
        {
            "paper_title": "Reinvent 2.0: an ai tool for de novo drug design",
            "rating": 2,
            "sanitized_title": "reinvent_20_an_ai_tool_for_de_novo_drug_design"
        },
        {
            "paper_title": "Genetic-guided gflownets: Advancing in practical molecular optimization benchmark",
            "rating": 2,
            "sanitized_title": "geneticguided_gflownets_advancing_in_practical_molecular_optimization_benchmark"
        },
        {
            "paper_title": "Beam enumeration: Probabilistic explainability for sample efficient selfconditioned molecular design",
            "rating": 2,
            "sanitized_title": "beam_enumeration_probabilistic_explainability_for_sample_efficient_selfconditioned_molecular_design"
        },
        {
            "paper_title": "Augmented memory: Capitalizing on experience replay to accelerate de novo molecular design",
            "rating": 2,
            "sanitized_title": "augmented_memory_capitalizing_on_experience_replay_to_accelerate_de_novo_molecular_design"
        },
        {
            "paper_title": "Evoprompting: Language models for code-level neural architecture search",
            "rating": 1,
            "sanitized_title": "evoprompting_language_models_for_codelevel_neural_architecture_search"
        }
    ],
    "cost": 0.013735250000000001,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Small Molecule Optimization with Large Language Models
26 Jul 2024</p>
<p>Philipp Guevorguian 
Menua Bedrosian 
Yerevann Tigran Fahradyan 
Chilingaryan Gayane 
Hrant Yerevann 
Khachatrian 
Armen Aghajanyan </p>
<p>YerevaNN Yerevan State University</p>
<p>YerevaNN American University
Armenia</p>
<p>YerevaNN Yerevan State University</p>
<p>Small Molecule Optimization with Large Language Models
26 Jul 20246418AFBD9AB07352845857871C036FE6arXiv:2407.18897v1[cs.LG]
Recent advancements in large language models have opened new possibilities for generative molecular drug design.We present Chemlactica and Chemma, two language models fine-tuned on a novel corpus of 110M molecules with computed properties, totaling 40B tokens.These models demonstrate strong performance in generating molecules with specified properties and predicting new molecular characteristics from limited samples.We introduce a novel optimization algorithm that leverages our language models to optimize molecules for arbitrary properties given limited access to a black box oracle.Our approach combines ideas from genetic algorithms, rejection sampling, and prompt optimization.It achieves stateof-the-art performance on multiple molecular optimization benchmarks, including an 8% improvement on Practical Molecular Optimization compared to previous methods.We publicly release the training corpus, the language models and the optimization algorithm.</p>
<p>Introduction</p>
<p>Molecular optimization is a cornerstone of drug discovery, involving the complex task of identifying compounds with specific desirable properties.This process traditionally requires extensive laboratory experimentation, making it time-consuming and costly.Computational methods have emerged as powerful tools to accelerate this process, yet they often need help with the vast and discrete nature of chemical space [Wu et al., 2018].</p>
<p>Large language models (LLMs) have recently demonstrated remarkable capabilities across various domains, from natural language processing to code generation [Brown et al., 2020, OpenAI, 2023].While there have been initial attempts to apply LLMs to chemical tasks [Irwin et al., 2022, Edwards et al., 2022, Chilingaryan et al., 2024], these efforts have often been limited in scope or performance.Our work represents a significant leap forward, leveraging the full power of LLMs to revolutionize molecular optimization for drug discovery.</p>
<p>We present a novel approach that harnesses LLMs to generate and optimize small molecules with unprecedented efficiency and accuracy.Our method uniquely combines LLMs' generative capabilities with evolutionary strategies, enabling more effective exploration of chemical space than traditional graph-based or SMILES-based models.Our training corpus, models and code can be found at https://github.com/yerevann/chemlactica.</p>
<p>Our research makes several contributions to the field:</p>
<p>Preprint.Under review.</p>
<p>Related Work</p>
<p>Language Models for Molecular Representation While graph-based representations are common for molecules, string-based representations, particularly Simplified Molecular Input Line Entry System (SMILES) [Weininger, 1988], have gained traction due to their compatibility with language models.This approach leverages the power of pre-trained language models and enables efficient processing of molecular data.Notable examples include ChemFormer [Irwin et al., 2022], MolT5 [Edwards et al., 2022], and BARTSmiles [Chilingaryan et al., 2024], which adapt traditional language model architectures to chemical tasks.These models demonstrate the potential of applying natural language processing techniques to molecular design and property prediction.</p>
<p>Molecular Optimization Techniques Molecular optimization, a key challenge in drug discovery, involves navigating a vast combinatorial space of potential drugs while satisfying multiple constraints.Traditional approaches include genetic algorithms adapted for molecular graphs, often incorporating domain-specific heuristics [Jensen, 2019].More recent methods leverage machine learning, particularly deep learning techniques.For instance, variational autoencoders [Kingma and Welling, 2013] have been applied to generate and optimize molecules in latent space.The GFlowNets [Bengio et al., 2021] represents a novel approach designed to sample compositional objects (like molecules) with reward-proportional probability, making it well-suited for optimization tasks.Extensions of GFlowNets [Kim et al., 2024] incorporating genetic search have shown promising results in molecular optimization.</p>
<p>Recurrent Neural Networks in Molecular Design Recurrent neural networks (RNNs) have also been applied to molecular optimization.A notable example is REINVENT [Olivecrona et al., 2017], which uses policy-based reinforcement learning to generate molecules with desired properties.Recent enhancements to REINVENT, such as augmented memory and beam enumeration [Guo and Schwaller, 2023b], have further improved its performance.These approaches combine molecular diversity filters, experience replay mechanisms, and substructure filtering to increase sample efficiency in molecular optimization tasks.</p>
<p>Large Language Models in Optimization The success of large language models (LLMs) has led to their application in various optimization tasks beyond text generation.For instance, Chen et al. [2023] combined prompt tuning with evolutionary algorithms to design neural network architectures, outperforming human experts on specific tasks.Similarly, EvoPrompt [Guo et al., 2023] developed a general evolutionary algorithm using language models, optimizing task-specific prompts for various downstream applications.These studies demonstrate the potential of LLMs in complex optimization problems, paving the way for their application in molecular design and optimization.</p>
<p>Our work builds upon these foundations, uniquely combining the strengths of large language models with evolutionary strategies for molecular optimization.We extend the application of LLMs beyond simple property prediction or generation, developing a comprehensive framework for navigating the complex landscape of molecular design.</p>
<p>Training Corpus</p>
<p>Molecular Database from PubChem We constructed a comprehensive SQL database using PubChem dumps, encompassing information on molecules, similar molecule pairs, experimental properties, and bioassays.Using rdkit [Landrum et al., 2013], we computed key molecular properties, including synthesizability score (SAS), quantitatively estimated drug-likeness (QED), molecular weight (MW), total polar surface area (TPSA), partition coefficient (CLogP), and various structural features such as hydrogen donors/acceptors and ring counts.Due to differences in SMILES canonicalization between PubChem and rdkit, we standardized all SMILES strings using rdkit's implementation.</p>
<p>Our dataset's cutoff date is January 26th, 2023, excluding any subsequent additions or modifications to PubChem.To ensure data integrity, molecules that failed rdkit's MolFromSmiles parsing were discarded.</p>
<p>To incorporate similarity information, we utilized PubChem's related molecule data, which includes pairs with Tanimoto similarity ≥0.8 based on PubChem fingerprints.From the resulting 200 billion pairs, we sampled 4 billion and recalculated their similarities using the ECFC4 fingerprint for improved accuracy and consistency with widely used methods.</p>
<p>JSONL Corpus Generation</p>
<p>We transformed our database into a corpus of JSONL files, with each molecule represented as a single JSON object.Below is an abbreviated example for aspirin: This representation includes molecular identifiers, computed properties, similarity data, synonyms, experimental properties, and the PubChem compound identifier (CID).</p>
<p>Text Generation Template</p>
<p>We developed a template system using paired tags to delimit each property and data point.For instance, a molecule's QED value is represented as
[QED]0.84[/QED].
To enhance the model's versatility in both property prediction and property-conditioned molecular generation, we randomized the property order and alternated the position of the primary molecule (start vs. in-between other tags) with equal probability.This carefully curated and structured corpus forms the foundation for training our language models, enabling them to learn complex relationships between molecular structures and properties.</p>
<p>Model Training and Evaluation</p>
<p>Selection of Pretrained Language Models We chose models for continued pretraining based on their general-purpose performance and domain-specific knowledge.At its release, Galactica outperformed models like OPT, Chinchilla, and BLOOM on tasks such as BIG-bench, MMLU, and TruthfulQA [Taylor et al., 2022].Its pretraining included two million PubChem molecules, SMILES-specific tagging, and a scientific corpus, making it well-suited for molecular data.Gemma, while not explicitly trained on molecular data, underwent extensive pretraining (2 trillion tokens for Gemma-2B) and demonstrated state-of-the-art performance on benchmarks like MMLU, HellaSwag, and Human eval, comparable to larger models like LLaMA 2 and Mistral [Team et al., 2024].</p>
<p>Tokenization and Sample Preparation</p>
<p>We utilized the original tokenizers from Gemma and Galactica, adding chemistry-specific tokens [START_SMILES] and [END_SMILES] to Gemma's tokenizer for consistency.To optimize training efficiency, we included all opening and closing tags as special tokens (e.g., [QED]).Samples of varying lengths were tokenized and grouped into blocks of 2048 tokens, separated by model-specific separator tokens (EOS "</s>" for Chemlactica, BOS "<bos>" for Chemma).</p>
<p>Training Methodology Both Chemma and Chemlactica were trained using the Adam optimizer [Kingma and Ba, 2014] with cross-entropy loss and a causal language modeling objective.We applied dropout only to Chemlactica, maintaining consistency with the original model architectures.Chemma-2B was trained in full bfloat16 for computational efficiency.We leveraged PyTorch's [Paszke et al., 2019] Fully Sharded Data Parallel (FSDP) [Zhao et al., 2023] and Flash Attention [Dao, 2024] for optimized training.The training was conducted locally at Yerevan State University (Chemlactica-125M: 306 A100 hours) and on Nebius.aicloud (Chemma-2B: 488 H100 GPU hours, Chemlactica-1.3B: 288 H100 GPU hours).Preparatory work before the final training runs consumed multiple thousands of A100 hours.</p>
<p>Evaluation of Computed Property Prediction and Conditional Generation</p>
<p>To assess our models' proficiency in learning computed properties, we conducted two comprehensive experiments:</p>
<p>Property Prediction We randomly sampled a fixed set of 100 molecules from the validation set.For each property, we prompted the models with
[START_SMILES]M i [END_SMILES][QED],
where M i represents the SMILES string of the molecule.We then calculated the Root Mean Square Error (RMSE) between predicted and actual property values to evaluate performance.</p>
<p>Conditional Generation</p>
<p>For each property, we sampled 100 values v i from the distribution of PubChem molecules.We then prompted the models to generate molecules with
[QED]v i [/QED][START_SMILES].
Using rdkit, we computed the actual property values of the generated SMILES and calculated the RMSE against the target v i .</p>
<p>Table 1 presents the results for both Property Prediction (PP) and Conditional Generation (CG) across various properties for our three model variants.For Chemma-2B, we provide evaluations at different training data volumes, including a compute-controlled run with 2.1B tokens to ensure fair comparison with Chemlactica-125M.</p>
<p>To account for potential invalid generations, we compute a corrected RMSE by substituting the property values of invalid SMILES with the mean value of the respective property's distribution in our dataset.</p>
<p>Our generation process incorporates several techniques to improve output quality:</p>
<p>• Chain-of-Thought (CoT): We omit [START_SMILES] from the initial prompt, enabling the model to generate more property values before the molecule itself.</p>
<p>• Repetition Penalty: Applied to discourage repetitive outputs [Keskar et al., 2019].</p>
<p>• Undesired Token Suppression: Employed to ensure the model eventually generates
[START_SMILES].
Table 2 provides an ablation study of these sampling components across our three models, demonstrating their individual and combined impacts on generation quality.Surprisingly, the best combinations of hyperparameters coincide for all three models.</p>
<p>These experiments comprehensively show our models' capabilities in predicting molecular properties and generating molecules with specified properties.These are crucial tasks in computational drug discovery and molecular design.Model calibration in language modeling refers to the alignment between a model's predicted probabilities for generating specific text and the actual likelihood of that text being correct.To assess the calibration of our models, we developed a suite of multiple-choice property prediction questions based on our training data format.</p>
<p>We generated 2000 questions for each computed property, resulting in 10,000 responses.Each question presented a SMILES string as input:</p>
<p>[START_SMILES]<SMILES> [END_SMILES] followed by five potential continuations, with only one being correct.This methodology is inspired by the calibration analysis in the GPT-4 technical report [OpenAI, 2023], which highlights calibration as a key indicator of high-quality pretraining.</p>
<p>For each response, we calculated the model's predicted probability based on the perplexity of the text, normalizing it against other responses for the same question.These probabilities were then aggregated and sorted into 10 equal-width bins.We plotted the fraction of correct responses for each bin, allowing us to visualize the relationship between the model's confidence and accuracy.</p>
<p>Results</p>
<p>Figures 1a and 1b present the calibration plots for Chemma-2B and Chemlactica-125M, respectively.The x-axis represents the 10 probability bins, while the left y-axis shows the correct response fraction.</p>
<p>The right y-axis and red bars indicate the number of occurrences within each bin.</p>
<p>Chemlactica and Chemma models demonstrate robust calibration, as evidenced by the near-linear relationship between assigned probabilities and correct outcomes across all computed properties.This relationship closely follows the diagonal grey line, which represents perfect calibration.</p>
<p>These results suggest that the perplexity scores generated by our models serve as reliable confidence indicators for molecular data predictions (averaged over a set of molecules), provided the data falls within the distribution of the training corpus.This calibration is crucial for practical applications, as it allows users to accurately gauge the reliability of the models' outputs in various molecular prediction and generation tasks.</p>
<p>Property Prediction</p>
<p>Supervised fine-tuning recipe.We designed and implemented a fine-tuning strategy to evaluate our model's adaptability to novel tasks not present in the initial training corpus.To this end, we fined-tuned our models on 6 tasks introduced by Fang et al.</p>
<p>[2023a] and 3 others by MoleculeNet Wu et al. [2018].Inspired by instruction tuning methodologies, we generated a specialized training corpus formatted as follows:
[START_SMILES]m smiles [END_SMILES][PROPERTY]<VALUE>[/PROPERTY].
We only trained the model on generated responses following the [PROPERTY] tag during the finetuning process.Our initial experiments indicated that a general fine-tuning recipe of 15 epochs yielded satisfactory results with a peak learning rate of 10e − 4 with 3 epochs of warmup and a NEFTune noise [Jain et al., 2023] of 5.However, we observed that our models could significantly benefit from a more rigorous hyperparameter optimization process.Consequently, we conducted an extensive hyperparameter tuning study, exploring a grid of values within the following ranges: Learning rate: [0.00001, 0.00005, 0.0001, 0.0002], Number of epochs: [10,15,20], Warmup epoch ratios: [0, 0.4, 1], NEFTune noise : [0.0, 5.0, 10.0].The results presented in Table 3 and 4 showcase the abilities of our models after the hyperparameter tuning stage.The details of hyperparameters selected per task and model can be found in the Appendix A.1.</p>
<p>Molecular Optimization Algorithm</p>
<p>We present a novel population-based algorithm for molecular optimization that leverages our trained language models.The algorithm addresses the challenging task of navigating the vast chemical space to find molecules with desired properties, subject to a limited evaluation budget.Formally, we define the molecular optimization problem as:
m * = arg max m∈M O(m)
where m represents a molecule, M is the constraint set of valid molecules (typically very large), and O : M → R is a black-box oracle function that evaluates molecular properties.This oracle could represent complex processes such as lab experiments or quantum simulations.</p>
<p>Our approach maintains a pool of P high-performing molecules and iteratively generates new candidates using a language model.It is built on three key innovations:</p>
<p>LLM-enhanced genetic algorithm We leverage our language models to generate molecules similar to the current pool.This can be viewed as a genetic algorithm where traditional crossover/mutation operations are replaced by language model generation.For S randomly selected molecules from the pool, we generate a new molecule using the prompt:</p>
<p>[SIMILAR]m if the best molecule (in terms of oracle score) has not improved for K iterations then 5. Take all the molecules from the P ool with their corresponding similar molecules (using which they have been generated), m i , (m i,1 , m i,2 , . . ., m i,S ), i = 1, . . ., P respectively.</p>
<p>train_samples i ← molecules2prompt((m i,1 , m i,2 , . . ., m i,S ), m i ), i = 1, . . ., P 6. Train LM on train_samples i , i = 1, . . ., P .end if until optim.problem stopping condition This approach allows for more intelligent exploration of the chemical space compared to traditional mutation operators.</p>
<p>Explicit oracle modeling Inspired by the rejection sampling technique [Bai et al., 2022, Touvron et al., 2023], we incorporate oracle feedback directly into the language model by fine-tuning on high-performing molecules.This is done using prompts of the form:
[PROPERTY]O(m)[/PROPERTY][START_SMILES]m smiles [END_SMILES]
This explicit modeling allows the language model to learn the relationship between molecular structure and oracle scores, enabling more targeted generation.</p>
<p>In-context learning</p>
<p>In early experiments we tried to use in-context learning during generation and fine-tuning by making our prompts shorter than the model's context length.This did not improve the results, and we abandoned the idea in further experiments.Note that there was no explicit training for in-context learning during the pretraining phase.</p>
<p>Algorithm 1 presents our complete optimization procedure, which includes initialization of an empty molecule pool, iterative generation of new molecules using the language model, evaluation of new molecules using the oracle function, updating the pool to maintain the top-P molecules, and periodic fine-tuning of the language model when progress stagnates.Algorithm 2 details our prompt construction process, which is crucial for effective molecule generation and model fine-tuning.</p>
<p>We employ a dynamic fine-tuning strategy to adapt the language model throughout the optimization process.Fine-tuning is triggered if the best molecule doesn't improve for K consecutive iterations, with the maximum number of fine-tuning rounds limited by the oracle budget.We use a learning rate scheduler with warm-up steps, and each fine-tuning step consists of multiple epochs with a portion of data reserved for validation to prevent overfitting.</p>
<p>Given the complexity of our algorithm, we adopt a focused hyperparameter tuning strategy, prioritizing the most sensitive parameters while keeping others fixed.This approach balances computational By combining these elements, our algorithm effectively leverages the power of large language models for molecular optimization, demonstrating strong performance across a range of tasks as detailed in Section 6.</p>
<p>Experiments</p>
<p>Practical Molecular Optimization</p>
<p>Problem formulation.Inspired by real-world molecular design setting Gao et al. [2022] propose a practical molecular optimization (PMO) benchmark consisting of 23 molecular optimization problems.PMO focuses on sample efficiency, generalizability to different optimization objectives, and robustness to hyperparameter selection of the molecular optimization algorithms.To assess the optimization ability and sample efficiency, Gao et al. [2022] put a limit on the number of oracle calls for each task to be 10000 and report the area under the curve (AUC) of the top-10 average property value versus the number of oracle calls as the performance metric.AUC values are calculated after every 100 oracle call, then combined and normalized to map the [0, 1] range.</p>
<p>Our approach.Using our proposed optimization algorithm we evaluate Chemlactica-125M, Chemlactica-1.3B and Chemma-2B models.The hyperparameters for the optimization algorithm are tuned for each model separately according to the hyperparameter tuning methodology.For this benchmark, we use the bfloat16 data type for the language model's parameters.</p>
<p>Results.Our method performs strongly, surpassing the existing approaches.Our algorithm powered by the smallest Chemlactica-125M model already improves over the state-of-the-art by a significant margin, with an AUC Top-10 of 17.170 (Chemlactica-125M) vs 16.213 (Genetic-guided GFlowNets).Additionally, strengthening the generator model improves the performance.Chemlactica-1.3B and Chemma-2B achieve AUC Top-10 of 17.284 and 17.534, respectively.For a more comprehensive Note that, unlike most of the other methods, our language models can leverage additional information about the oracle if the oracle internally calculates common molecular properties.These properties can be explicitly written in the prompts used in the optimization loop.In Appendix A.4 we show that such rich prompts can significantly improve the metrics on several PMO tasks.</p>
<p>Multi-property Optimization with Docking</p>
<p>Problem formulation.This benchmark, initially proposed in the REINVENT paper [Blaschke et al., 2020], evaluates a model's capability to generate viable molecules for practical drug discovery.Specifically, it assesses the model's ability to generate plausible molecules that optimize docking scores (minimize docking energy) against specified protein targets.The benchmark focuses on three targets with extensive real-world applications: the dopamine type 2 receptor (DRD2), MK2-kinase, and acetylcholinesterase.To ensure the generation of realistic molecules, the oracle reward function incorporates additional constraints, including the maximization of QED and a molecular weight limit of 500 Da.</p>
<p>The primary objective is to maximize the reward function with minimal oracle calls, emphasizing sample efficiency.We quantify this efficiency using two metrics: oracle burden and generative yield.Oracle burden measures the number of oracle calls required to generate N unique molecules above a predefined reward threshold.At the same time, generative yield represents the number of unique molecules generated above a reward threshold for a fixed number of oracle calls.To maintain consistency with recent implementations, we adopt the molecular preprocessing, conformational generation, docking parameters, and aggregate reward function from the Beam Enumeration paper [Guo and Schwaller, 2023b], specifically comparing our results with the beam structure 15 methods, which demonstrated superior average-case performance.</p>
<p>Results.We used the exact same hyperparameters as those selected in the PMO experiment.Table 6 presents our approach's performance on this benchmark, simulating real-world drug design scenarios.Chemma-2B consistently achieves the highest performance for the generative yield metric across all evaluated receptors.Conversely, Chemlactica-125M demonstrates superior performance in terms of oracle burden, except for MK2 at oracle burden 1, where Chemma outperforms it.Notably, Chemlactica-1.3B achieved even better yield scores on the DRD2 target.Appendix A.7 shows the set of molecules generated at the beginning and at the end of the optimization trajectory for DRD2 docking.</p>
<p>These results suggest that model size is crucial in balancing exploration and exploitation of the molecular space.Smaller models appear more adept at initial space exploration, while larger models excel in exploiting the reward space.This trade-off between oracle burden and generative yield could have significant implications for applied drug design, particularly when access to oracle functions is limited or costly.</p>
<p>Our findings validate the effectiveness of our approach, demonstrating that our models can leverage pre-training information and selective fine-tuning to optimize complex reward functions, even with limited data unseen during pre-training.Furthermore, the successful transfer of training parameters and sampling strategies from the molecular optimization benchmark to this task underscores our method's flexibility and robustness.This adaptability suggests that our approach could be particularly valuable in scenarios where extensive hyperparameter tuning is impractical or undesirable.</p>
<p>QED Maximization with Similarity Constrained Molecular Design</p>
<p>Problem formulation.The objective of this optimization problem is to generate a molecule that has a high QED and is similar to some given molecule.More formally, given a molecule M , the objective of the problem is to generate a new molecule M ′ such that sim(M ′ , M ) ≥ 0.4 and qed(M ′ ) ≥ 0.9.</p>
<p>Following Wang et al. [2023] 800 molecules are selected with QED in the range [0.7, 0.8] as the inputs to the optimization problem, and the performance is measured by the percentage of the molecules that have been optimized (satisfy the QED and similarity constraints).In addition, a maximum number of QED evaluations is chosen to optimize each lead molecule.</p>
<p>Our approach.Since this is a lead optimization problem, we add the lead molecule to all prompts in addition to the molecules added from the pool.The lead molecule is added by enclosing it in [SIMILAR] tag.For this task, we design an oracle function by combining the QED value of the generated molecule with the similarity value of the lead molecule and the generated molecule.Additionally, we decreased the maximum number of QED evaluations to 10000, compared to the baselines, which used 50000.</p>
<p>Results.For this task, we only evaluate the Chemlactica-125M model, which achieves better success rates compared to the best existing approaches, 99.0% (Chemlactica-125M) versus 94.6% (RetMol), while being constrained to use 5 times less QED evaluations at maximum.Since the performance of the Chemlactica-125M is very close to perfect, we have not evaluated other models for this task.Table 7 illustrates the performance of different algorithms.</p>
<p>Conclusion</p>
<p>This paper presents three language models: Chemlactica-125M, Chemlactica-1.3B, and Chemma-2B.These models were trained on a novel corpus encompassing over 100 million molecules and their properties.We demonstrate the efficacy of these models on multiple tasks in chemistry research, with a particular focus on molecular optimization.Our proposed optimization algorithm combines the capabilities of language models with concepts from genetic algorithms.This approach has shown strong performance across various benchmarks, indicating its potential for addressing complex molecular design challenges.We publicly release our training corpus, pretrained models, optimization algorithm, and associated training recipes to support reproducibility and further research in this area.</p>
<p>While our work demonstrates promising results in molecular optimization and related tasks, we acknowledge that it represents an early step in applying language models to chemical research.We hope our contributions will provide a valuable foundation for future work in this domain, potentially enabling new molecular design and analysis approaches.</p>
<p>Limitations</p>
<p>The language models introduced in this paper operate only on SMILES representations and do not support 3D coordinates of atoms, limiting their reliability in scenarios where 3D conformation is critical.Furthermore, the models have very limited understanding of other biological entities like proteins, which constrains their practical applicability in certain areas of biochemistry and drug discovery.While effective, the optimization algorithms presented in this paper have not been exhaustively tuned, suggesting potential room for improvement.Additionally, our current approach does not account for synthetic accessibility or other practical considerations in drug design, which may limit its immediate applicability in real-world drug discovery pipelines.</p>
<p>Broader Impact</p>
<p>The molecular optimization models presented in this work have the potential for both positive and negative societal impacts.On the positive side, these models could significantly benefit the drug discovery and healthcare industries by accelerating the development of new therapeutic compounds.This acceleration may lead to faster responses to emerging health challenges and potentially reduce the cost of drug development.</p>
<p>However, as with many dual-use technologies, there is a risk that sufficiently advanced versions of these models could lower the barriers for malicious actors attempting to develop chemical or biological weapons.This risk underscores the importance of responsible development and deployment of such technologies.</p>
<p>Given these potential impacts, we recommend that future work in this area include rigorous evaluation of these algorithms and language models in designing potentially harmful substances to better understand and mitigate risks.Additionally, developing safeguards and ethical guidelines for using and disseminating molecular optimization models is crucial.Collaboration with experts in biosecurity and ethics will be essential to ensure that the development of these technologies proceeds in a manner that maximizes benefits while minimizing the potential for harm.</p>
<p>A Appendix</p>
<p>A.1 Hyperparameters Table 8 lists the hyperparameters we used for pretraining the language models.</p>
<p>For supervised fine-tuning we did a grid search over the following hyperparameters: peak learning rate, number of epochs, warmup steps and the amount of Neftune noise.Table 9 shows the best values for all tasks and models.Warmup steps are written as a ratio of the total training steps here.</p>
<p>Methodology for Hyperparameter Tuning of the Optimization Algorithm Given the large number of hyperparameters in our optimization algorithm, we adopt a two-step approach.First, we identify and freeze the hyperparameters that empirically show less sensitivity to the algorithm's performance.Then, we focus on tuning the more sensitive hyperparameters using grid search.</p>
<p>For tuning, we utilize the perindopril_mpo and zaleplon_mpo tasks from the PMO benchmark, following the methodology in [Gao et al., 2022].We report the AUC Top-10 metric from three independent runs with different seeds for each hyperparameter configuration.The best-performing configuration is then applied across all benchmarks in our evaluation.Notably, we tune the hyperparameters separately for Chemlactica-125M, Chemlactica-1.3B, and Chemma-2B to account for model-specific optimal settings.</p>
<p>A key hyperparameter, N , which determines the number of molecules generated before updating the pool, is set to 200.We employ vanilla temperature sampling for molecule generation throughout the optimization process.To address the need for generating thousands of unique molecules in many optimization benchmarks, we implement a dynamic temperature scheduling strategy.The sampling temperature starts at 1 and linearly increases to 1.5 as the number of oracle evaluations grows.This gradual temperature increase promotes the generation of more diverse molecules over time, reducing repetition and encouraging exploration of the chemical space.</p>
<p>Grid search.We perform grid search on P (pool size), S (number of similar molecules), K (finetuning tolerance level) and lr (fine-tuning peak learning rate) with the following grid:
• P = [10, 30, 50] • S = [0, 1, 2, 5] • K = [3, 5, 7] • lr = [10 −4 , 10 −5 ]</p>
<p>A.2 Detailed Results for Practical Molecular Optimization</p>
<p>Table 10 shows the evaluations of Chemlactica-125M, Chemlactica-1.3B and Gemma-2B, along with other methods on 23 tasks of the PMO benchmark.There is no method that uniformly beats all others The reason is that the oracle has a binary multiplier term that is usually equal to zero, so there is no supervision signal for the entire generation process.</p>
<p>A.3 Ablation Study on the Optimization Algorithm</p>
<p>A key component of our proposed optimization algorithm is the fine-tuning step, which is activated when the algorithm's progress stagnates.To assess the impact of this fine-tuning step, we conducted a comparative analysis of optimization processes both with and without this feature.For this evaluation, we selected four representative tasks from the PMO benchmark: jnk3, median1, sitagliptin_mpo, and scaffold_hop.These tasks were chosen to provide a diverse set of challenges and to be representative of the broader benchmark.</p>
<p>Table 11 presents the quantitative results of these experiments.To provide a more comprehensive understanding of the fine-tuning effect, we visualize the optimization trajectories in Figures 6 through 8.These visualizations aggregate data from five independent runs, offering insights into both the mean performance and its variance across different initializations.</p>
<p>This ablation study allows us to isolate the impact of the fine-tuning step and understand its contribution to the overall performance of our optimization algorithm across different types of molecular optimization tasks.</p>
<p>A.4 Leveraging Known Molecular Properties in Optimization Tasks</p>
<p>Our language models possess knowledge of various molecular properties such as QED, CLogP, and TPSA.However, we deliberately avoid utilizing this information in Algorithm 1 to maintain fair comparison with other methods.This decision stems from the fact that our models have been trained on properties that are components of the oracle functions we optimize against (e.g., those in PMO).</p>
<p>Exploiting this partial oracle information could potentially give our method an unfair advantage.</p>
<p>We conducted a separate set of experiments to explore the models' capacity to utilize additional information in solving optimization problems.We selected four tasks from the PMO benchmark: jnk3, median1, sitagliptin_mpo, and scaffold_hop.For these tasks, we modified Algorithm 2 to incorporate relevant known properties into the prompt p between steps 2 and 3.</p>
<p>Table 12 presents a performance comparison between our standard approach and this propertyaugmented version.The specific syntax used for adding these properties to the prompts is detailed in Table 13.Notably, no additional properties were added for the jnk3 task as our models lack specific knowledge about its oracle function.</p>
<p>The results demonstrate a significant performance improvement across all models when these additional properties are incorporated.This finding suggests that our models can effectively leverage their pre-existing knowledge of molecular properties to enhance their performance in molecular design tasks.However, it's important to note that while this approach showcases the potential of our      Challenges in Batched Generation Molecular optimization pipelines require repeated model calls for generation, followed by oracle function scoring.While batched processing accelerates this process through GPU parallelization, it introduces complications.The necessary padding for batch processing alters matrix sizes, affecting multiply-accumulate operations within the model.These small errors accumulate as they propagate through the model's layers.Lower precision exacerbates these errors, leading to larger discrepancies in logit values and, consequently more significant impacts on the generated molecules.</p>
<p>Cascading Effects of Sub-optimal Generations In our approach, high-scoring generated molecules are used for both additional fine-tuning and identifying similar molecules to guide optimization.However, when lower precision leads to sub-optimal molecule generation, it creates a negative feedback loop.The model is fine-tuned on and guided by these lower-quality molecules, hindering the generation of higher-scoring molecules in subsequent iterations.This causal relationship between successive generations underlies the particularly adverse effects of low precision in molecular optimization pipelines.</p>
<p>Precision Ablation Study To quantify the impact of numerical precision on the optimization process, we conducted an ablation study comparing 32-bit floating point precision with bfloat16 precision.Table 14 presents the results of this comparison across all drug discovery case studies described in Section 6.2.Despite the potential computational costs, these results demonstrate the critical importance of maintaining higher numerical precision in molecular optimization tasks.</p>
<p>A.6 Visualization of the Model Outputs on Property Prediction and Conditional Generation Tasks</p>
<p>[</p>
<p>WEIGHT]180.16[/WEIGHT][TPSA]63.60[/TPSA][CLOGP]1.31[/CLOGP][START_SMILES]CC(=O)OC1=CC=CC=C1C(=O)O[END_SMILES] [SAS]1.58[/SAS][QED]0.92[/QED][SIMILAR]O=C(Oc1ccccc1C(=O)O)c1ccccc1O 0.59[/SIMILAR] [SYNONYM]aspirin[/SYNONYM] [PROPERTY]Vapor Pressure 2.52X10-5 mm Hg at 25 °C (calc)[/PROPERTY] [CID]2244[/CID]</p>
<p>Figure 1 :
1
Figure 1: Model calibration on synthetic multiple choice question where y=x represents perfect calibration.</p>
<p>m 1 , m 2 , . . ., m S ), m 1. Check if the outcome should be a molecule generation prompt or a training sample.if m is null then 1.1.Sample similarity values for molecules in the prompt, desirable oracle score and set the suffix for a molecule generation.v sim i ∼ U(0.4,0.9), i = 1, . . ., S v max ← the maximum oracle score achieved at this moment v prop ∼ U(v max , oracle_max) suf f ix ← [START_SMILES] else 1.3.Compute the correct similarity values for the molecules in the prompt and the correct oracle score, set the suffix for a training sample.v sim i = similar(m i , m), i = 1, . . ., S v prop = O(m) suf f ix ← [START_SMILES]m smiles [END_SMILES]eos end if 2. Concatenate all molecules in the prompt with their similarity values.p ← bos[SIMILAR]m smiles 1 if at least one fine-tuning has been performed then 2.1.Add the oracle score to the prompt.p ← concat(p, [PROPERTY]v prop [/PROPERTY]) end if 3. Add the appropriate suffix.return concat(p, suf f ix) efficiency with optimization performance.Detailed methodology and results of our hyperparameter tuning experiments are provided in Appendix A.1.</p>
<p>Figures</p>
<p>Figures 2e-2eshow the performance of Chemma-2B for property prediction and conditional molecular generations tasks.Each dot in the scatter plot corresponds to one molecule.The histogram in the background is the actual distribution of those properties in the database.The purple line shows RMSE error for the given value of the property.</p>
<p>(greedy sampling) 0/100 invalid SMILES, 7/100 from PubChem rmse 0.415 rmse_c 0.415 mape 0.105 corr: 0.786 (c) SAS-conditioned generation of molecules.(greedy sampling) 0/100 invalid SMILES, 15/100 from PubChem rmse 6.942 rmse_c 6.942 mape 0.054 corr: 0.985 (d) TPSA-conditioned generation of molecules.invalid SMILES rmse 0.140 mape 0.234 corr: 0.874 (f) Similarity-conditioned generation of molecules.</p>
<p>Figure 2 :
2
Figure 2: Illustration of errors made by Chemma-2B during property prediction and conditional generation for various properties.</p>
<p>Figure 3 :Figure 4 :
34
Figure 3: Optimization process visualization using Chemlactica-125M model for sitagliptin_mpo task with four different seeds.</p>
<p>Figure 5 :Figure 6 :
56
Figure 5: Optimization process visualization using Chemma-2B model for sitagliptin_mpo task with four different seeds.</p>
<p>Table 1 :
1
RMSE (RSME corrected for mean) ↓ for Property Prediction and Conditional Generation for different tasks and models.
QEDSIMSASPPCGPPCGPPCGChemlactica-125M 0.016 0.101 (0.108) 0.0460.1830.0780.315 (0.379)Chemlactica-1.3B0.004 0.050 (0.050) 0.0430.1670.0660.400 (0.400)Chemma-2B-2.1B0.016 0.100 (0.100) 0.0490.1260.0730.384 (0.382)Chemma-2B-39B0.004 0.075 (0.075) 0.0460.1400.0370.415 (0.415)CLOGPTPSAWEIGHTPPCGPPCGPPCGChemlactica-125M 0.106 0.568 (0.568) 1.322 5.216 (5.244) 9.350 30.276 (30.276)Chemlactica-1.3B0.100 0.405 (0.405) 0.893 5.543 (15.640) 3.576 16.877 (16.877)Chemma-2B-2.1B0.137 1.675 (1.675) 1.638 7.077 (7.077) 8.962 39.695 (41.109)Chemma-2B-39B0.034 0.461 (0.461) 0.959 6.942 (6.942) 1.931 18.933 (20.395)4.2 Model Calibration4.2.1 Methodology</p>
<p>Table 2 :
2
Ablation study on Conditional Generation hyperparameters.Each row represents one combination of Chain-of-Thought (CoT), repetition penalty (rep.), and suppression (supp.).All experiments are done on the molecular weight prediction task.
Chemlactica-125MChemlactica-1.3BChemma-2BCoT rep. supp.RMSE (c) ↓ Invalids ↓RMSE (c) ↓ Invalids ↓RMSE (c) ↓ Invalids ↓No 1.0No70.02 (70.02)0/10015.41 (65.22)1/10016.56 (65.58)1/100No 1.0No70.11 (70.11)0/10015.81 (65.32)1/10012.15 (64.54)1/100Yes 1.0No 112.52 (112.52)0/100 187.26 (187.26)0/100 198.48 (191.89)46/100Yes 1.010 No82.28 (82.28)0/100 137.19 (137.19)0/100 170.02 (170.02)0/100Yes 1.0 Yes33.46 (33.46)0/10018.53 (25.22)1/10031.98 (31.85)1/100Yes 1.005 Yes34.52 (34.52)0/10017.14 (17.14)0/10029.71 (29.71)0/100Yes 1.010 Yes30.27 (30.27)0/10016.87 (16.87)0/10018.93 (20.39)1/100Yes 1.015 Yes30.27 (30.27)0/10018.07 (19.61)1/10018.99 (20.44)1/100Yes 1.020 Yes31.17 (31.17)1/10016.33 (18.03)1/10024.16 (25.27)1/100Yes 1.050 Yes45.38 (45.38)1/10016.49 (34.48)1/100 74.78 (130.11)63/100Yes 1.100 Yes35.20 (35.20)0/10016.61 (32.37)1/100 740.28 (488.73)59/1001.01750000.2 0.4 0.6 0.8 P(correct)25000 50000 75000 100000 125000 150000 Number of Occurences0.0Bin Ranges (0.3, 0.4] (0.2, 0.3] (0.1, 0.2] (-0.001, 0.1] (0.4, 0.5] (0.5, 0.6] (0.6, 0.7] (0.7, 0.8] (0.8, 0.9] (0.9, 1.0]0</p>
<p>Table 3 :
3
Regression tasks from MoleculeNet, all values are RMSE ↓.
ESOLFreeSolvLipophilicityAvgMoleculeNet GC0.9701.4000.6551.008Chemformer0.6331.2300.5980.820MoLFormer-XL0.2790.2310.5290.346GROVER large0.8311.5440.5600.978MolCLR1.1102.2000.6501.320iMolCLR1.1302.0900.6401.287BARTSmiles0.3080.3380.5400.395Chemlactica-125M 0.270 ± 0.011 0.306 ± 0.011 0.533 ± 0.009 0.369 ± 0.000Chemlactica-1.3B0.281 ± 0.005 0.356 ± 0.009 0.557 ± 0.021 0.403 ± 0.013Chemma-2B0.298 ± 0.014 0.359 ± 0.040 0.563 ± 0.004 0.406 ± 0.012</p>
<p>Table 4 :
4
Regression tasks from the ADMET benchmark.All numbers are Pearson correlation ↑.
HLMMDR1-MDCK ERSolubilityMPNN2 (from the original paper)0.680.780.59Chemlactica-125M0.68 ± 0.0110.77 ± 0.0120.57 ± 0.035Chemlactica-1.3B0.68 ± 0.0040.77 ± 0.0090.54 ± 0.043Chemma-2B0.67 ± 0.0040.78 ± 0.0090.53 ± 0.024RLMhPPBrPPBMPNN2 (from the original paper)0.740.770.70Chemlactica-125M0.71 ± 0.0040.73 ± 0.0040.60 ± 0.098Chemlactica-1.3B0.65 ± 0.0040.74 ± 0.0010.62 ± 0.017Chemma-2B0.68 ± 0.0050.75 ± 0.0040.60 ± 0.030
Fang et al. [2023b] the results for three regression tasks from MoleculeNet[Wu et al., 2018].Fang et al. [2023b]introduces a new dataset for six ADMET targets.The authors provided training/test split but no validation set.We used a random 20% of the training set as a validation set to pick the best hyperparameters.Table4shows the results.</p>
<p>Initialize an empty P ool ← {} repeat 1. Generate prompts for molecule generation.for i = 1 to N do (m i,1 , m i,2 , . . ., m i,S ) ← random_subset(P ool) p i ← molecules2prompt((m i,1 , m i,2 , . . ., m i,S ), null) end for 2. Generate N new and unique molecules with the language model.m i ← LM (p i ), i = 1, . . ., N 3. Update the pool with m i s and keep only the top-P molecules.P ool ← P ool ∪ {m 1 , . . ., m N } P ool ← top-P (P ool) 4. Fine-tune if necessary.</p>
<p>smiles 1 0.8[/SIMILAR]...[SIMILAR]m smiles S 0.8[/SIMILAR][START_SMILES] Algorithm 1 molecular_optimization Input: P , S, N , K</p>
<p>Table 5 :
5
Kim et al. [2024] [2023a]ctica-125M, Chemlactica-1.3B and Chemma-2B in comparison with other methods.REINVENT results are taken fromGao et al. [2022], Augmented memory is taken fromGuo and Schwaller [2023a], and Genetic-guided (GG) GFlowNets are taken fromKim et al. [2024].Values are the average of 5 runs with different seeds, metric is Top-10 AUC ↑ ±
standard deviationjnk3median1scaffold_hop sitagliptin_mposum of 4sum of 23REINVENT0.783 ± 0.023 0.356 ± 0.009 0.560 ± 0.019 0.021 ± 0.0031.72014.196Augmented memory 0.739 ± 0.110 0.326 ± 0.013 0.567 ± 0.008 0.284 ± 0.0501.91615.002GG GFlowNets0.764 ± 0.069 0.379 ± 0.010 0.615 ± 0.100 0.634 ± 0.0392.39216.213Chemlactica-125M0.881 ± 0.058 0.359 ± 0.060 0.626 ± 0.016 0.649 ± 0.051 2.515 ± 0.119 17.170 ± 0.424Chemlactica-1.3B0.866 ± 0.021 0.382 ± 0.047 0.673 ± 0.080 0.586 ± 0.062 2.506 ± 0.155 17.284 ± 0.284Chemma-2B0.891 ± 0.032 0.382 ± 0.022 0.669 ± 0.110 0.613 ± 0.018 2.555 ± 0.099 17.534 ± 0.214understanding of the optimization dynamics, Figures 3-5 illustrate visualizations of the optimizationprocesses for sitagliptin_mpo task with different seeds for different models.</p>
<p>Table 6 :
6
Drug discovery case studies via docking function reward optimization.All experiments were run with a maximum oracle budget of 5000 oracle calls.Note that both oracle burden and generative yield values are reward-threshold dependent, and mean values from the reported baseline works are reported.The parentheses for oracle burden indicate how many unique molecules need to be generated for consideration.The best performance on each task-metric combination is bolded.Note that the hyperparameters of our models are not tuned for this task; instead, we used the best-performing hyperparameters on the PMO benchmark.
MetricTargetReinventBeam Chemlactica ChemlacticaChemmaBaseline Structure 15125M1.3B2BGenerative Yield 0.7 ↑DRD21879 ± 16 3474 ± 1583733 ± 512 3659 ± 2883848 ± 98MK2879 ± 10 3127 ± 1383772 ± 578 3660 ± 535 3578 ± 452AChE2437 ± 53 3824 ± 1624108 ± 67 4193 ± 128 4092 ± 284DRD2102 ± 6 1780 ± 4392827 ± 510 2621 ± 614 2985 ± 194Generative Yield 0.8 ↑MK22 ± 0987 ± 211 2569 ± 1156 2216 ± 522 1058 ± 465AChE147 ± 11 2059 ± 3273246 ± 168 3652 ± 349 3096 ± 372DRD2168 ± 149126 ± 9020 ± 2911 ± 1074 ± 62Oracle burden 0.8 (1) ↓MK21724 ± 802736 ± 166345 ± 31278 ± 125189 ± 278AChE83 ± 29105 ± 2922 ± 2815 ± 2374 ± 72DRD2883 ± 105582 ± 83114 ± 08160 ± 130240 ± 11Oracle burden 0.8 (10) ↓MK2Failed 1122 ± 154493 ± 418248 ± 261440 ± 548AChE481 ± 108462224 ± 1791 ± 103168 ± 94DRD24595 ± 01120 ± 25364 ± 119430 ± 250518 ± 41Oracle burden 0.8 (100) ↓MK2Failed 2189 ± 181865 ± 533486 ± 346934 ± 918AChE 3931 ± 286 1110 ± 265497 ± 58333 ± 131433 ± 143</p>
<p>Table 7 :
7
Performance comparison of different algorithms on QED and Similarity constrained molecular optimization problem.
Success Rate (%) ↑QMO92.8RetMol94.5Chemlactica-125M99.0</p>
<p>Table 8 :
8
Hyperparameters of our language models.All cross-entropy losses use mean reduction.None of our (and many other) methods get non-zero result on valsartan_smarts.
Chemlactica-125M Chemlactica-1.3B Chemma-2BPeak learning rate1.4e-31.0e-41.0e-3Warmup steps500500500Context length204820482048ADAM β 10.90.90.9ADAM β 20.950.950.95ADAM ϵ1e-81e-81e-8Weight Decay0.10.10.1Dropout0.10.1NoneAttention Dropout0.10.1NonePrecisionMixedMixedBF16Loss FunctionCE LossCE LossCE LossVocabulary Size5006650066256000Gradient Clipping1.01.01.0</p>
<p>Table 9 :
9
Selected hyperparameters for property prediction tasks as a result of the grid search.We report learning rate (LR), warmup ratio (WU), number of epochs (Ep.) and Neftune noise (Nef.).
Chemlactica-125MChemlactica-1BChemma-2BTaskLRWU Ep. Nef.LRWU Ep. Nef.LRWU Ep. Nef.</p>
<p>Table 10 :
10
Comparision of different methods on PMO.The values represent the AUC Top-10 ↑ metric averaged over five independent runs with different seeds.
OracleREINVENTAugmentedGeneticChemlacticaChemlacticaChemmaMemoryGFN125M1.3B2Balbuterol_similarity0.882 ± 0.006 0.913 ± 0.009 0.949 ± 0.010 0.951 ± 0.0110.947 ± 0.0120.951 ± 0.009amlodipine_mpo0.635 ± 0.035 0.691 ± 0.047 0.761 ± 0.019 0.772 ± 0.0910.769 ± 0.0830.766 ± 0.107celecoxib_rediscover0.713 ± 0.067 0.796 ± 0.008 0.802 ± 0.029 0.906 ± 0.0460.911 ± 0.0130.920 ± 0.011deco_hop0.666 ± 0.044 0.658 ± 0.024 0.733 ± 0.109 0.801 ± 0.1010.836 ± 0.1170.831 ± 0.123drd20.945 ± 0.007 0.963 ± 0.006 0.974 ± 0.006 0.965 ± 0.0070.968 ± 0.0050.972 ± 0.006fexofenadine_mpo0.784 ± 0.006 0.859 ± 0.009 0.856 ± 0.039 0.881 ± 0.0310.891 ± 0.0390.931 ± 0.014gsk30.865 ± 0.043 0.881 ± 0.021 0.881 ± 0.042 0.926 ± 0.0220.916 ± 0.0270.928 ± 0.021isomers_c7h8n2o20.852 ± 0.036 0.853 ± 0.087 0.969 ± 0.003 0.951 ± 0.0120.933 ± 0.0170.947 ± 0.009isomers_c9h10n2o2pf2cl 0.642 ± 0.054 0.736 ± 0.051 0.897 ± 0.007 0.927 ± 0.0060.929 ± 0.0120.914 ± 0.017jnk30.783 ± 0.023 0.739 ± 0.110 0.764 ± 0.069 0.881 ± 0.0580.866 ± 0.0210.891 ± 0.032median10.356 ± 0.009 0.326 ± 0.013 0.379 ± 0.010 0.359 ± 0.0600.382 ± 0.0470.382 ± 0.022median20.276 ± 0.008 0.291 ± 0.008 0.294 ± 0.007 0.328 ± 0.0320.329 ± 0.0160.366 ± 0.018mestranol_similarity0.618 ± 0.048 0.750 ± 0.049 0.708 ± 0.057 0.896 ± 0.0640.850 ± 0.0510.926 ± 0.023osimertinib_mpo0.837 ± 0.009 0.855 ± 0.004 0.860 ± 0.008 0.907 ± 0.0150.892 ± 0.0130.879 ± 0.016perindopril_mpo0.537 ± 0.016 0.613 ± 0.015 0.595 ± 0.014 0.709 ± 0.0520.755 ± 0.0660.711 ± 0.062qed0.941 ± 0.000 0.942 ± 0.000 0.942 ± 0.000 0.942 ± 0.0000.942 ± 0.0000.941 ± 0.000ranolazine_mpo0.760 ± 0.009 0.801 ± 0.006 0.819 ± 0.018 0.864 ± 0.0140.883 ± 0.0170.868 ± 0.015scaffold_hop0.560 ± 0.019 0.567 ± 0.008 0.615 ± 0.100 0.626 ± 0.0160.673 ± 0.0800.669 ± 0.110sitagliptin_mpo0.021 ± 0.003 0.284 ± 0.050 0.634 ± 0.039 0.649 ± 0.0510.586 ± 0.0620.613 ± 0.018thiothixene_rediscovery 0.534 ± 0.013 0.550 ± 0.041 0.583 ± 0.034 0.624 ± 0.1020.693 ± 0.1190.698 ± 0.121troglitazone_rediscovery 0.441 ± 0.032 0.540 ± 0.048 0.511 ± 0.054 0.734 ± 0.1300.765 ± 0.1380.824 ± 0.049valsartan_smarts0.178 ± 0.358 0.000 ± 0.000 0.135 ± 0.271 0.000 ± 0.0000.000 ± 0.0000.000 ± 0.000zaleplon_mpo0.358 ± 0.062 0.394 ± 0.026 0.552 ± 0.033 0.569 ± 0.0470.569 ± 0.0200.608 ± 0.055sum14.19615.00216.21317.170 ± 0.424 17.284 ± 0.284 17.534 ± 0.214</p>
<p>Table 11 :
11
Illustration of the results of ablation study on the fine-tuning step in the optimization algorithm.The values represent AUC Top-10 ↑ obtained from five independent runs.</p>
<p>Table 13 :
13
The descriptions of tasks used in the prompts in the extended version of our optimization algorithm.The results are in Table12.See Section A.4 for details.
the syntax of additional properties added to the promptsjnk3(nothing added)median1[SIMILAR]camphor_smiles 0.55[/SIMILAR][SIMILAR]menthol_smiles 0.55[/SIMILAR]scaffold_hop[SIMILAR]pharmacophor_smiles 0.80[/SIMILAR]sitagliptin_mpo [SIMILAR]sitagliptin_smiles 0.99[/SIMILAR][CLOGP]2.02[/CLOGP][TPSA]77.04[/TPSA]</p>
<p>Table 14 :
14
Impact of numerical precision on multi-property optimization with docking task.
MetricTarget Chemlactica-125M Chemlactica-125MBF16 FP32Generative Yield 0.7 ↑DRD23501 ± 2523733 ± 512MK23000 ± 803772 ± 578AChE4337 ± 1334108 ± 67DRD22574 ± 1032827 ± 510Generative Yield 0.8 ↑MK21223 ± 5192569 ± 1156AChE3877 ± 2723246 ± 168DRD2156 ± 10020 ± 29Oracle burden 0.8 (1) ↓MK2320 ± 83345 ± 312AChE10 ± 822 ± 28DRD2283 ± 61114 ± 08Oracle burden 0.8 (10) ↓MK2631 ± 100493 ± 418AChE123 ± 119224 ± 17DRD2577 ± 71364 ± 119Oracle burden 0.8 (100) ↓MK21134 ± 178865 ± 533AChE350 ± 137497 ± 58
AcknowledgementsWe would like to thank Garik Petrosyan and Zaven Navoyan for insightful discussions.We appreciate Nebius.aifor granting us access to their GPU cloud and providing excellent support.Philipp Guevorguian's research is supported by the Yandex Armenia fellowship.Chemlactica-125MChemlactica-1.3B Chemma-2B fine-tuning no fine-tuning fine-tuning no fine-tuning fine-tuning no fine-tuning jnk3 0.881 ± 0.058 0.878 ± 0.040 0.866 ± 0.021 0.867 ± 0.036 0.891 ± 0.032 0.869 ± 0.033 median1 0.359 ± 0.060 0.371 ± 0.006 0.382 ± 0.047 0.395 ± 0.027 0.382 ± 0.022 0.380 ± 0.034 scaffold_hop 0.626 ± 0.016 0.648 ± 0.017 0.673 ± 0.080 0.721 ± 0.121 0.669 ± 0.110 0.700 ± 0.122 sitagliptin_mpo 0.649 ± 0.051 0.607 ± 0.051 0.586 ± 0.062 0.576 ± 0.082 0.613 ± 0.018 0.563 ± 0.059 sum 2.515 ± 0.119 2.504 ± 0.068 2.506 ± 0.155 2.559 ± 0.062 2.555 ± 0.099 2.512 ± 0.160 models, it may not provide a fair comparison with methods that don't have access to such property information.A.5 The Impact of Floating Point Precision on Molecular OptimizationNumerical Precision in Model Training Lower precision training, including mixed and halfprecision methods, is commonly used to increase training throughput.These techniques, employed during our models' pretraining stages, typically have negligible impact on performance and may even provide a regularizing effect.However, in the context of molecular optimization involving multiple rounds of fine-tuning, lower numerical precision leads to significantly degraded performance.Several factors contribute to this phenomenon in the specific case of molecular optimization with language models.Table12: The performance of the extended version of our optimization algorithm on selected PMO tasks.The prompts used in the optimization contain the description of the tasks in the format our language models has seen during pretraining.See Table13for the additional tags used in the prompts.
Y Bai, S Kadavath, S Kundu, A Askell, J Kernion, A Jones, A Chen, A Goldie, A Mirhoseini, C Mckinnon, arXiv:2212.08073Constitutional ai: Harmlessness from ai feedback. 2022arXiv preprint</p>
<p>Flow network based generative models for non-iterative diverse candidate generation. E Bengio, M Jain, M Korablyov, D Precup, Y Bengio, Advances in Neural Information Processing Systems. 202134</p>
<p>Reinvent 2.0: an ai tool for de novo drug design. T Blaschke, J Arús-Pous, H Chen, C Margreitter, C Tyrchan, O Engkvist, K Papadopoulos, A Patronov, Journal of chemical information and modeling. 60122020</p>
<p>Language models are few-shot learners. T B Brown, B Mann, N Ryder, M Subbiah, J Kaplan, P Dhariwal, A Neelakantan, P Shyam, G Sastry, A Askell, arXiv:2005.141652020arXiv preprint</p>
<p>Evoprompting: Language models for code-level neural architecture search. A Chen, D Dohan, D R So, ArXiv, abs/2302.148382023257232765</p>
<p>Bartsmiles: Generative masked language models for molecular representations. G Chilingaryan, H Tamoyan, A Tevosyan, N Babayan, K Hambardzumyan, Z Navoyan, A Aghajanyan, H Khachatrian, L Khondkaryan, 10.1021/acs.jcim.4c00512Journal of Chemical Information and Modeling. 2024</p>
<p>Flashattention-2: Faster attention with better parallelism and work partitioning. T Dao, The Twelfth International Conference on Learning Representations. 2024</p>
<p>Translation between molecules and natural language. C N Edwards, T Lai, K Ros, G Honke, H Ji, ArXiv, abs/2204.118172022248376906</p>
<p>Prospective validation of machine learning algorithms for absorption, distribution, metabolism, and excretion prediction: An industrial perspective. C Fang, Y Wang, R Grater, S Kapadnis, C Black, P Trapa, S Sciabola, Journal of Chemical Information and Modeling. 63112023a</p>
<p>Prospective validation of machine learning algorithms for absorption, distribution, metabolism, and excretion prediction: An industrial perspective. C Fang, Y Wang, R Grater, S Kapadnis, C Black, P Trapa, S Sciabola, Journal of Chemical Information and Modeling. 63112023b</p>
<p>Sample efficiency matters: A benchmark for practical molecular optimization. W Gao, T Fu, J Sun, C W Coley, ArXiv, abs/2206.124112022</p>
<p>Augmented memory: Capitalizing on experience replay to accelerate de novo molecular design. J Guo, P Schwaller, ArXiv, abs/2305.161602023a</p>
<p>Beam enumeration: Probabilistic explainability for sample efficient selfconditioned molecular design. J Guo, P Schwaller, ArXiv, abs/2309.139572023b</p>
<p>Connecting large language models with evolutionary algorithms yields powerful prompt optimizers. Q Guo, R Wang, J Guo, B Li, K Song, X Tan, G Liu, J Bian, Y Yang, T University, M Research, ArXiv, abs/2309.085322023</p>
<p>Chemformer: a pre-trained transformer for computational chemistry. R Irwin, S Dimitriadis, J He, E J Bjerrum, Machine Learning: Science and Technology. 31150222022</p>
<p>N Jain, P -Y. Chiang, Y Wen, J Kirchenbauer, H.-M Chu, G Somepalli, B R Bartoldson, B Kailkhura, A Schwarzschild, A Saha, arXiv:2310.05914Noisy embeddings improve instruction finetuning. 2023arXiv preprint</p>
<p>A graph-based genetic algorithm and generative model/monte carlo tree search for the exploration of chemical space. J H Jensen, Chemical science. 10122019</p>
<p>N S Keskar, B Mccann, L R Varshney, C Xiong, R Socher, arXiv:1909.05858Ctrl: A conditional transformer language model for controllable generation. 2019arXiv preprint</p>
<p>Genetic-guided gflownets: Advancing in practical molecular optimization benchmark. H.-S Kim, M Kim, S Choi, J Park, abs/2402.059612024</p>
<p>Pubchem substance and compound databases. S Kim, P A Thiessen, E E Bolton, J Chen, G Fu, A Gindulyte, L Han, J He, S He, B A Shoemaker, J Wang, B Yu, J Zhang, S H Bryant, Nucleic Acids Research. 442015</p>
<p>D P Kingma, J Ba, arXiv:1412.6980Adam: A method for stochastic optimization. 2014arXiv preprint</p>
<p>Auto-encoding variational bayes. P Kingma, M Welling, CoRR, abs/1312.61142013</p>
<p>Rdkit: A software suite for cheminformatics, computational chemistry, and predictive modeling. G Landrum, 2013</p>
<p>Molecular de-novo design through deep reinforcement learning. M Olivecrona, T Blaschke, O Engkvist, H Chen, Journal of Cheminformatics. 929783112017</p>
<p>OpenAI. Gpt-4 technical report. 2023</p>
<p>Pytorch: An imperative style, high-performance deep learning library. A Paszke, S Gross, F Massa, A Lerer, J Bradbury, G Chanan, T Killeen, Z Lin, N Gimelshein, L Antiga, Advances in neural information processing systems. 201932</p>
<p>R Taylor, M Kardas, G Cucurull, T Scialom, A Hartshorn, E Saravia, A Poulton, V Kerkez, R Stojnic, arXiv:2211.09085Galactica: A large language model for science. 2022arXiv preprint</p>
<p>G Team, T Mesnard, C Hardin, R Dadashi, S Bhupatiraju, S Pathak, L Sifre, M Rivière, M S Kale, J Love, arXiv:2403.08295Open models based on gemini research and technology. 2024arXiv preprint</p>
<p>Llama 2: Open foundation and fine-tuned chat models. H Touvron, L Martin, K Stone, P Albert, A Almahairi, Y Babaei, N Bashlykov, S Batra, P Bhargava, S Bhosale, arXiv:2307.092882023arXiv preprint</p>
<p>Retrieval-based controllable molecule generation. Z Wang, W Nie, Z Qiao, C Xiao, R Baraniuk, A Anandkumar, International Conference on Learning Representations. 2023</p>
<p>Smiles, a chemical language and information system. 1. introduction to methodology and encoding rules. D Weininger, Journal of chemical information and computer sciences. 2811988</p>
<p>Moleculenet: a benchmark for molecular machine learning. Z Wu, B Ramsundar, E N Feinberg, J Gomes, C Geniesse, A S Pappu, K Leswing, V Pande, Chemical science. 922018</p>
<p>Pytorch fsdp: Experiences on scaling fully sharded data parallel. Y Zhao, A Gu, R Varma, L Luo, C.-C Huang, M Xu, L Wright, H Shojanazeri, M Ott, S Shleifer, A Desmaison, C Balioglu, P Damania, B Nguyen, G Chauhan, Y Hao, A Mathews, S Li, 10.14778/3611540.3611569Proc. VLDB Endow. VLDB Endowaug 202316</p>
<p>Cc1cccc(N2CCC3(CCC3C (=O)Nc3nnc(CC4CCCCC4 )s3)C2=O). 1</p>
<p>COCC1C(C(=O)Nc2nnc(C 3CCCCC3)s2)CC(=O)N1c. 1ccccn1 Score: 0.7572</p>
<p>O=C(Nc1nnc(CC2CCCCC2 )s1)C1CCc2c(ncccc2=O )C1. Score: 0.8523</p>
<p>=O)NCc2c cc3c(c2)OCO3)c2c1C=C (F)C(C. A.7.2 MK2 CO.Cc1ccc(CC2</p>
<p>)(C)=O) c(Nc2nc3ccccc3nc2NS( =O)(=O)c2cn(C)cn2)c1. A.7.3 AChE CO.Cc1ccc(P(CC2Cl Score: 0.0000 I.O=C1CC(NC(=O</p>
<p>C=CC3c4ccccc4C=CC23) =NCN1 Score: 0.0000 COCc1c2ccc(C3OCCO3)c c2c(C)c2cc(S(=O). =O</p>
<p>N3CCC4(CC3)OC(=O). C4</p>
<p>O=C(CCC1=CC=C2C(c3cc ccc3)=C3C(=CCc4ccccc 43)C2S1)NC1=NCCO1 Score: 0.7877 Cc1ccc2c(C)c(S(=O)(= O)C3CCC4(CC3)CC. 4</p>
<p>Cc1ccc2cc(S(=O)(=O)C 3(O)CCC4(CC3)NC. Score: 0.79754</p>
<p>=O)(=O)c3cc c4cc(C)ccc4c3C)=CC1). Score: 0.8045 C=C1NC(=O)CC12[CH]CC 1(C=CC(S(</p>
<p>. C( =o, Score: 0.8255C2</p>
<p>Score: 0.8295CC1CN=C(NC(=O)C2CCc3 ccccc3C23c2ccccc2C3C )O1 Score: 0.8338 Cc1ccc2cc(S(=O)(=O)C 3CCC4(CC3)NC(=O)OC43 C=C3. </p>
<p>Score: 0.8412Cc1cc(N)c2cc(S(=O). </p>
<p>O)c3=ccc4 , C=C3)CC(=O )NC4C. </p>
<p>CSC1=NC(NC(=O)Cc2ccc c(-c3cccc4c3C3=CC=C4C3) c2)COC1 Score: 0.8572 Cc1ccc2c(C)c(S(=O). </p>
<p>. O)c3=ccc4 , CC3)NC(=O) CC43C=C3)ccc2c1</p>            </div>
        </div>

    </div>
</body>
</html>
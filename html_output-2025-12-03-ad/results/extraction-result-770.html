<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-770 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-770</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-770</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-21.html">extraction-schema-21</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of agents that use external tools for planning in partially observable text environments, including how they maintain belief states, incorporate tool outputs, and perform navigation or path-finding tasks.</div>
                <p><strong>Paper ID:</strong> paper-267028086</p>
                <p><strong>Paper Title:</strong> <a href="https://ojs.aaai.org/index.php/AAAI/article/download/29754/31298" target="_blank">Large Language Models Are Neurosymbolic Reasoners</a></p>
                <p><strong>Paper Abstract:</strong> A wide range of real-world applications is characterized by their symbolic nature, necessitating a strong capability for symbolic reasoning. This paper investigates the potential application of Large Language Models (LLMs) as symbolic reasoners. We focus on text-based games, significant benchmarks for agents with natural language capabilities, particularly in symbolic tasks like math, map reading, sorting, and applying common sense in text-based worlds. To facilitate these agents, we propose an LLM agent designed to tackle symbolic challenges and achieve in-game objectives. We begin by initializing the LLM agent and informing it of its role. The agent then receives observations and a set of valid actions from the text-based games, along with a specific symbolic module. With these inputs, the LLM agent chooses an action and interacts with the game environments. Our experimental results demonstrate that our method significantly enhances the capability of LLMs as automated agents for symbolic reasoning, and our LLM agent is effective in text-based games involving symbolic tasks, achieving an average performance of 88% across all tasks.</p>
                <p><strong>Cost:</strong> 0.013</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e770.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e770.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of agents that use external tools for planning in partially observable text environments, including how they maintain belief states, incorporate tool outputs, and perform navigation or path-finding tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM agent</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prompted Large Language Model Agent (GPT-3.5-turbo)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A zero-shot, prompt-driven language agent that uses an LLM (GPT-3.5-turbo) to select actions in text-based games by querying and consuming outputs from external symbolic modules (calculator, navigator, sorter, knowledge-base). It operates by receiving observations, an inventory state, and a valid-action set in the prompt and returning one action from that set.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>LLM agent (GPT-3.5-turbo)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A single LLM instantiated as an agent via role-initialization and stepwise Action Query prompts. Key components: (1) role-initialization prompt that describes the task, action constraints and agent role; (2) per-timestep Action Query prompt that supplies the current observation, inventory state, score, and a constrained set of valid actions; (3) interfaces to external symbolic modules (Calculation, Navigation, Sorting, Knowledge-Base) which are included in the environment's action space and return text observations when called. The LLM chooses exactly one action from the provided valid-action set at each timestep and receives the resulting observation (from either the game or a symbolic module) which is fed back into the next prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Text-based games (TextWorld / TextWorldExpress benchmarks introduced in Wang et al. 2022b and executed with TextWorldExpress)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Text-based interactive games with symbolic sub-tasks (Arithmetic, MapReader, Sorting, Text World Common Sense). Formally modeled as POMDPs: the agent only receives partial textual observations each turn; tasks require long-horizon multi-step reasoning (reading math, extracting map routes, sorting by quantities, and commonsense placement). Challenges include partial observability, combinatorial action spaces, and the need for long-term memory and planning across turns.</td>
                        </tr>
                        <tr>
                            <td><strong>is_partially_observable</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tools_used</strong></td>
                            <td>Calculation Module (calculator for arithmetic ops), Navigation Module (map/route extractor that returns next steps and route sequences), Sorting Module (extracts quantities and sorts items ascending/descending), Knowledge-Base Module (commonsense lookups mapping objects to typical locations). Each module is callable via specific action tokens (e.g., 'mul 8 7', 'next step to pantry', 'sort ascending', or 'query clean brown shirt').</td>
                        </tr>
                        <tr>
                            <td><strong>tool_output_types</strong></td>
                            <td>Textual, structured textual responses: numeric results for calculations (e.g., 'Multiplying 8 and 7 results in 56'), route descriptions / ordered lists of rooms for navigation (e.g., 'you need go through canteen, pantry'), ordered lists of items for sorting (e.g., 'sorted items: ...'), and short knowledge-base text snippets that map objects to canonical locations (e.g., 'Clean brown shirt is expected to be located at wardrobe').</td>
                        </tr>
                        <tr>
                            <td><strong>belief_state_mechanism</strong></td>
                            <td>No explicit structured belief state is reported; the agent's state is maintained implicitly via the textual prompt context each timestep (current observation + inventory state + score + valid action set). Inventory items and module outputs are fed into prompts as the agent's immediate memory; there is no reported persistent graph or separate memory store beyond what is included in prompt/inventory.</td>
                        </tr>
                        <tr>
                            <td><strong>incorporates_tool_outputs_in_belief</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>belief_update_description</strong></td>
                            <td>Tool outputs are returned as the 'next observation' and included verbatim in the subsequent Action Query prompt (alongside inventory and score). Thus the LLM's implicit belief is updated by appending the returned textual observation to its prompt context/inventory state; there is no separate, structured belief-update algorithm described (no explicit memory network or belief-graph update), and limited memory/capacity issues (e.g., forgetting routes) are noted.</td>
                        </tr>
                        <tr>
                            <td><strong>planning_approach</strong></td>
                            <td>Zero-shot prompt-based neuro-symbolic decision-making: the LLM performs symbolic reasoning over the textual observation and valid-action set and selects actions; planning is implicit in the LLM's reasoning over prompts and by consulting symbolic modules on-demand rather than an explicit search algorithm. The agent's planning relies on returning tool outputs as observations and the LLM's internal pattern-based reasoning to decide next steps.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_shortest_path_planning</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>navigation_method</strong></td>
                            <td>Navigation is handled by a Navigation Module that extracts and returns textual route information (next-step directions and full sequences of intermediate rooms). The agent queries the module (e.g., 'next step to pantry') and receives a textual route (list of rooms to traverse). There is no explicit algorithm like A* described; navigation is driven by the module's route text and the LLM's use of that text in subsequent action selection. The paper notes the agent often 'forgets' returned routes, causing repeated queries and extra steps.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_tools</strong></td>
                            <td>Reported average normalized test score ≈ 0.887 (≈88.7%) across the four symbolic-task benchmarks when the LLM agent uses symbolic modules and constrained prompting (paper reports 'achieving an average performance of 88% across all tasks').</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_tools</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_tool_ablation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Integrating external symbolic modules into the LLM agent's action space substantially improves task performance: the LLM consumes module outputs as observations in prompts and uses them to select subsequent actions; constrained prompts further improve accuracy and reduce steps. However, because the agent's belief is maintained implicitly via prompt context (inventory + recent observations) rather than a structured belief store, it can forget module-provided plan details (notably routes), causing inefficiency (extra steps and repeated queries). The approach enables zero-shot neurosymbolic reasoning without expert demonstration training but remains sensitive to prompt design and LLM memory limits.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models Are Neurosymbolic Reasoners', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e770.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e770.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of agents that use external tools for planning in partially observable text environments, including how they maintain belief states, incorporate tool outputs, and perform navigation or path-finding tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Behavior Cloned Transformer</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>T5-based Behavior Cloned Transformer (Behavior Cloning baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An imitation-learning baseline that frames action selection as sequence-to-sequence prediction (taking task description, current observation, previous action, and prior observation as input) and can include symbolic-module actions in demonstrations (gold trajectories).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Behavior Cloned Transformer (T5-based)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A sequence-to-sequence transformer trained via behavior cloning on human (or gold) trajectories: inputs include task description, current observation, previous action, and previous observation. Symbolic-module calls are included in demonstrations for the variant that uses modules. During evaluation the model predicts the next action token from the candidate set; symbolic-module outputs are provided in the trajectories and can be injected into the action space.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Text-based games (same TextWorld/TextWorldExpress benchmarks used in the paper)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Partially observable text games with symbolic tasks (Arithmetic, MapReader, Sorting, TWC). Challenging due to partial observability, the need to interpret symbolic-module outputs, and long-horizon dependencies that require remembering quantities, routes, or sorted orders.</td>
                        </tr>
                        <tr>
                            <td><strong>is_partially_observable</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tools_used</strong></td>
                            <td>Symbolic modules (Calculation, Navigation, Sorting, Knowledge-Base) when included in demonstrations; the behavior-cloned model can use module actions injected into the action space and has access to module outputs in training/demo trajectories (gold trajectories include module calls).</td>
                        </tr>
                        <tr>
                            <td><strong>tool_output_types</strong></td>
                            <td>Textual outputs from modules (numeric results, route texts, sorted item lists, knowledge-base snippets) supplied as part of demonstration trajectories.</td>
                        </tr>
                        <tr>
                            <td><strong>belief_state_mechanism</strong></td>
                            <td>The model conditions on a sequence of prior observations and prior actions (sequence modeling); belief is represented implicitly as the transformer context window containing recent trajectory tokens rather than an explicit belief graph or memory structure.</td>
                        </tr>
                        <tr>
                            <td><strong>incorporates_tool_outputs_in_belief</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>belief_update_description</strong></td>
                            <td>During training and inference, module outputs appear as tokens in the trajectory/context; the transformer updates its implicit state by attending over the token sequence (previous observations/actions/module outputs) to predict the next action. No separate structured belief-update mechanism is described.</td>
                        </tr>
                        <tr>
                            <td><strong>planning_approach</strong></td>
                            <td>Imitation learning / learned policy via sequence modeling (behavior cloning); planning emerges from learned sequence-to-sequence predictions conditioned on prior trajectory context and examples including module usage.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_shortest_path_planning</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>navigation_method</strong></td>
                            <td>Uses navigation-module actions when available in the demonstration/action space. The specific internal navigation algorithm is unspecified; navigation behavior is learned from trajectories that include module outputs (route text), not via explicit search algorithms documented in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_tools</strong></td>
                            <td>Reported in paper comparisons: Behavior Cloned Transformer with symbolic modules achieves strong performance in the benchmarks (paper reports it as a competitive baseline; exact per-agent numbers are provided in the paper's tables for baselines).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_tools</strong></td>
                            <td>Baselines were also evaluated without symbolic modules (paper reports lower performance when modules are omitted for baselines), but exact per-model numbers should be read from the paper's tables.</td>
                        </tr>
                        <tr>
                            <td><strong>has_tool_ablation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Behavior cloning with demonstrations that include symbolic-module calls yields strong performance; including symbolic modules in the action space benefits learned policies. Compared to the zero-shot LLM agent, the behavior-cloned transformer achieves comparable or sometimes higher scores but requires costly expert/demo data. Both approaches rely on including module outputs into the model's context/trajectory to inform decisions.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models Are Neurosymbolic Reasoners', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Behavior Cloned Transformers are Neurosymbolic Reasoners <em>(Rating: 2)</em></li>
                <li>Learning dynamic belief graphs to generalize on text-based games <em>(Rating: 2)</em></li>
                <li>Inner monologue: Embodied reasoning through planning with language models <em>(Rating: 2)</em></li>
                <li>Language models are zero-shot planners: Extracting actionable knowledge for embodied agents <em>(Rating: 2)</em></li>
                <li>TextWorld: A learning environment for text-based games <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-770",
    "paper_id": "paper-267028086",
    "extraction_schema_id": "extraction-schema-21",
    "extracted_data": [
        {
            "name_short": "LLM agent",
            "name_full": "Prompted Large Language Model Agent (GPT-3.5-turbo)",
            "brief_description": "A zero-shot, prompt-driven language agent that uses an LLM (GPT-3.5-turbo) to select actions in text-based games by querying and consuming outputs from external symbolic modules (calculator, navigator, sorter, knowledge-base). It operates by receiving observations, an inventory state, and a valid-action set in the prompt and returning one action from that set.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "LLM agent (GPT-3.5-turbo)",
            "agent_description": "A single LLM instantiated as an agent via role-initialization and stepwise Action Query prompts. Key components: (1) role-initialization prompt that describes the task, action constraints and agent role; (2) per-timestep Action Query prompt that supplies the current observation, inventory state, score, and a constrained set of valid actions; (3) interfaces to external symbolic modules (Calculation, Navigation, Sorting, Knowledge-Base) which are included in the environment's action space and return text observations when called. The LLM chooses exactly one action from the provided valid-action set at each timestep and receives the resulting observation (from either the game or a symbolic module) which is fed back into the next prompt.",
            "environment_name": "Text-based games (TextWorld / TextWorldExpress benchmarks introduced in Wang et al. 2022b and executed with TextWorldExpress)",
            "environment_description": "Text-based interactive games with symbolic sub-tasks (Arithmetic, MapReader, Sorting, Text World Common Sense). Formally modeled as POMDPs: the agent only receives partial textual observations each turn; tasks require long-horizon multi-step reasoning (reading math, extracting map routes, sorting by quantities, and commonsense placement). Challenges include partial observability, combinatorial action spaces, and the need for long-term memory and planning across turns.",
            "is_partially_observable": true,
            "external_tools_used": "Calculation Module (calculator for arithmetic ops), Navigation Module (map/route extractor that returns next steps and route sequences), Sorting Module (extracts quantities and sorts items ascending/descending), Knowledge-Base Module (commonsense lookups mapping objects to typical locations). Each module is callable via specific action tokens (e.g., 'mul 8 7', 'next step to pantry', 'sort ascending', or 'query clean brown shirt').",
            "tool_output_types": "Textual, structured textual responses: numeric results for calculations (e.g., 'Multiplying 8 and 7 results in 56'), route descriptions / ordered lists of rooms for navigation (e.g., 'you need go through canteen, pantry'), ordered lists of items for sorting (e.g., 'sorted items: ...'), and short knowledge-base text snippets that map objects to canonical locations (e.g., 'Clean brown shirt is expected to be located at wardrobe').",
            "belief_state_mechanism": "No explicit structured belief state is reported; the agent's state is maintained implicitly via the textual prompt context each timestep (current observation + inventory state + score + valid action set). Inventory items and module outputs are fed into prompts as the agent's immediate memory; there is no reported persistent graph or separate memory store beyond what is included in prompt/inventory.",
            "incorporates_tool_outputs_in_belief": true,
            "belief_update_description": "Tool outputs are returned as the 'next observation' and included verbatim in the subsequent Action Query prompt (alongside inventory and score). Thus the LLM's implicit belief is updated by appending the returned textual observation to its prompt context/inventory state; there is no separate, structured belief-update algorithm described (no explicit memory network or belief-graph update), and limited memory/capacity issues (e.g., forgetting routes) are noted.",
            "planning_approach": "Zero-shot prompt-based neuro-symbolic decision-making: the LLM performs symbolic reasoning over the textual observation and valid-action set and selects actions; planning is implicit in the LLM's reasoning over prompts and by consulting symbolic modules on-demand rather than an explicit search algorithm. The agent's planning relies on returning tool outputs as observations and the LLM's internal pattern-based reasoning to decide next steps.",
            "uses_shortest_path_planning": true,
            "navigation_method": "Navigation is handled by a Navigation Module that extracts and returns textual route information (next-step directions and full sequences of intermediate rooms). The agent queries the module (e.g., 'next step to pantry') and receives a textual route (list of rooms to traverse). There is no explicit algorithm like A* described; navigation is driven by the module's route text and the LLM's use of that text in subsequent action selection. The paper notes the agent often 'forgets' returned routes, causing repeated queries and extra steps.",
            "performance_with_tools": "Reported average normalized test score ≈ 0.887 (≈88.7%) across the four symbolic-task benchmarks when the LLM agent uses symbolic modules and constrained prompting (paper reports 'achieving an average performance of 88% across all tasks').",
            "performance_without_tools": null,
            "has_tool_ablation": true,
            "key_findings": "Integrating external symbolic modules into the LLM agent's action space substantially improves task performance: the LLM consumes module outputs as observations in prompts and uses them to select subsequent actions; constrained prompts further improve accuracy and reduce steps. However, because the agent's belief is maintained implicitly via prompt context (inventory + recent observations) rather than a structured belief store, it can forget module-provided plan details (notably routes), causing inefficiency (extra steps and repeated queries). The approach enables zero-shot neurosymbolic reasoning without expert demonstration training but remains sensitive to prompt design and LLM memory limits.",
            "uuid": "e770.0",
            "source_info": {
                "paper_title": "Large Language Models Are Neurosymbolic Reasoners",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "Behavior Cloned Transformer",
            "name_full": "T5-based Behavior Cloned Transformer (Behavior Cloning baseline)",
            "brief_description": "An imitation-learning baseline that frames action selection as sequence-to-sequence prediction (taking task description, current observation, previous action, and prior observation as input) and can include symbolic-module actions in demonstrations (gold trajectories).",
            "citation_title": "",
            "mention_or_use": "use",
            "agent_name": "Behavior Cloned Transformer (T5-based)",
            "agent_description": "A sequence-to-sequence transformer trained via behavior cloning on human (or gold) trajectories: inputs include task description, current observation, previous action, and previous observation. Symbolic-module calls are included in demonstrations for the variant that uses modules. During evaluation the model predicts the next action token from the candidate set; symbolic-module outputs are provided in the trajectories and can be injected into the action space.",
            "environment_name": "Text-based games (same TextWorld/TextWorldExpress benchmarks used in the paper)",
            "environment_description": "Partially observable text games with symbolic tasks (Arithmetic, MapReader, Sorting, TWC). Challenging due to partial observability, the need to interpret symbolic-module outputs, and long-horizon dependencies that require remembering quantities, routes, or sorted orders.",
            "is_partially_observable": true,
            "external_tools_used": "Symbolic modules (Calculation, Navigation, Sorting, Knowledge-Base) when included in demonstrations; the behavior-cloned model can use module actions injected into the action space and has access to module outputs in training/demo trajectories (gold trajectories include module calls).",
            "tool_output_types": "Textual outputs from modules (numeric results, route texts, sorted item lists, knowledge-base snippets) supplied as part of demonstration trajectories.",
            "belief_state_mechanism": "The model conditions on a sequence of prior observations and prior actions (sequence modeling); belief is represented implicitly as the transformer context window containing recent trajectory tokens rather than an explicit belief graph or memory structure.",
            "incorporates_tool_outputs_in_belief": true,
            "belief_update_description": "During training and inference, module outputs appear as tokens in the trajectory/context; the transformer updates its implicit state by attending over the token sequence (previous observations/actions/module outputs) to predict the next action. No separate structured belief-update mechanism is described.",
            "planning_approach": "Imitation learning / learned policy via sequence modeling (behavior cloning); planning emerges from learned sequence-to-sequence predictions conditioned on prior trajectory context and examples including module usage.",
            "uses_shortest_path_planning": true,
            "navigation_method": "Uses navigation-module actions when available in the demonstration/action space. The specific internal navigation algorithm is unspecified; navigation behavior is learned from trajectories that include module outputs (route text), not via explicit search algorithms documented in the paper.",
            "performance_with_tools": "Reported in paper comparisons: Behavior Cloned Transformer with symbolic modules achieves strong performance in the benchmarks (paper reports it as a competitive baseline; exact per-agent numbers are provided in the paper's tables for baselines).",
            "performance_without_tools": "Baselines were also evaluated without symbolic modules (paper reports lower performance when modules are omitted for baselines), but exact per-model numbers should be read from the paper's tables.",
            "has_tool_ablation": true,
            "key_findings": "Behavior cloning with demonstrations that include symbolic-module calls yields strong performance; including symbolic modules in the action space benefits learned policies. Compared to the zero-shot LLM agent, the behavior-cloned transformer achieves comparable or sometimes higher scores but requires costly expert/demo data. Both approaches rely on including module outputs into the model's context/trajectory to inform decisions.",
            "uuid": "e770.1",
            "source_info": {
                "paper_title": "Large Language Models Are Neurosymbolic Reasoners",
                "publication_date_yy_mm": "2024-01"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Behavior Cloned Transformers are Neurosymbolic Reasoners",
            "rating": 2,
            "sanitized_title": "behavior_cloned_transformers_are_neurosymbolic_reasoners"
        },
        {
            "paper_title": "Learning dynamic belief graphs to generalize on text-based games",
            "rating": 2,
            "sanitized_title": "learning_dynamic_belief_graphs_to_generalize_on_textbased_games"
        },
        {
            "paper_title": "Inner monologue: Embodied reasoning through planning with language models",
            "rating": 2,
            "sanitized_title": "inner_monologue_embodied_reasoning_through_planning_with_language_models"
        },
        {
            "paper_title": "Language models are zero-shot planners: Extracting actionable knowledge for embodied agents",
            "rating": 2,
            "sanitized_title": "language_models_are_zeroshot_planners_extracting_actionable_knowledge_for_embodied_agents"
        },
        {
            "paper_title": "TextWorld: A learning environment for text-based games",
            "rating": 1,
            "sanitized_title": "textworld_a_learning_environment_for_textbased_games"
        }
    ],
    "cost": 0.01267575,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Large Language Models Are Neurosymbolic Reasoners</p>
<p>Meng Fang meng.fang@liverpool.ac.uk 
University of Liverpool
United Kingdom</p>
<p>Eindhoven University of Technology
Netherlands</p>
<p>Shilong Deng shilong.deng@liverpool.ac.uk 
University of Liverpool
United Kingdom</p>
<p>Yudi Zhang y.zhang5@tue.nl 
Eindhoven University of Technology
Netherlands</p>
<p>Zijing Shi zijing.shi@student.uts.edu.au 
University of Technology Sydney
Australia</p>
<p>Ling Chen ling.chen@uts.edu.au 
University of Technology Sydney
Australia</p>
<p>Mykola Pechenizkiy m.pechenizkiy@tue.nl 
Eindhoven University of Technology
Netherlands</p>
<p>Jun Wang j.wang@cs.ucl.ac.uk 
University College London
United Kingdom</p>
<p>Large Language Models Are Neurosymbolic Reasoners
682140E380B72F329A1846E8B9FDF624
A wide range of real-world applications is characterized by their symbolic nature, necessitating a strong capability for symbolic reasoning.This paper investigates the potential application of Large Language Models (LLMs) as symbolic reasoners.We focus on text-based games, significant benchmarks for agents with natural language capabilities, particularly in symbolic tasks like math, map reading, sorting, and applying common sense in text-based worlds.To facilitate these agents, we propose an LLM agent designed to tackle symbolic challenges and achieve in-game objectives.We begin by initializing the LLM agent and informing it of its role.The agent then receives observations and a set of valid actions from the text-based games, along with a specific symbolic module.With these inputs, the LLM agent chooses an action and interacts with the game environments.Our experimental results demonstrate that our method significantly enhances the capability of LLMs as automated agents for symbolic reasoning, and our LLM agent is effective in text-based games involving symbolic tasks, achieving an average performance of 88% across all tasks.</p>
<p>Introduction</p>
<p>The ability to perform reasoning is crucial for AI due to its significant impact on various real-world tasks.The widespread adoption of large language models (LLMs), such as ChatGPT and GPT-4 (OpenAI 2023), has led to a series of remarkable successes in reasoning tasks, ranging from question &amp; answering to solving math problems.Among these challenges, text-based games serve as important benchmarks for agents with natural language capabilities and have garnered significant attention in the realm of language-centric machine learning research (Narasimhan, Kulkarni, and Barzilay 2015;Côté et al. 2018;Xu et al. 2020;Ryu et al. 2022;Shi et al. 2022).In these games, an agent uses language to interpret various scenarios and make decisions.The complexity of such games arises from the need for language comprehension, common sense, managing action spaces with combinatorial complexity, and the crucial importance of long-term memory and planning (Côté et al. 2018;Wang et al. 2022a).The challenges escalate in text-based games that involve symbolic tasks (Wang et al. 2022b).For instance, contemporary agents might be tasked with a scenario where they are required to solve a mathematical problem and simultaneously gather a specified amount of fruits, with the quantity needed being the solution to the math problem.</p>
<p>Using symbolic modules or external tools for arithmetic, navigation, sorting, and knowledge-base lookup is crucial for language agents, especially in complex text-based games (Lample and Charton 2020; Poesia, Dong, and Goodman 2021; Wang et al. 2022b;Qian et al. 2023).However, effectively integrating these aspects into language agents remains a relatively unaddressed challenge.Solving such textbased games requires interactive multi-step reasoning, and agents have most commonly been modeled using reinforcement learning (Xu et al. 2020;Yao et al. 2020;Xu et al. 2021).These methods, however, face challenges such as delayed rewards and difficulty in exploring large action spaces.Recently, there has been an exploration of imitation learning approaches, which utilize human play data (Wang et al. 2022b).While Behavior Cloning (BC) shows potential in effectively addressing these challenges, it often necessitates substantial effort and resources.This is primarily due to the need for acquiring expert data.</p>
<p>Recently, large language models (LLMs) have demonstrated notable in-context generalization capabilities, suggesting the potential to elicit reasoning abilities by prompting these models (Brown et al. 2020;Min et al. 2022).However, the application of LLMs in performing symbolic reasoning remains an under-explored area.Models like GPT-3.5 and GPT-4 have shown the ability to encode extensive information (OpenAI 2023).A significant example of this is their acquisition of substantial knowledge during training, enabling them to approach human-level performance across a wide range of tasks (OpenAI 2023).This indicates the feasibility of utilizing LLMs as neurosymbolic reasoners without relying on labeled gold training data.However, there is currently limited research on utilizing these models for reasoning tasks that involve logic, graphs, or symbolic formulas.The exploration and development of methods that leverage LLMs for symbolic reasoning is highly intriguing and Figure 1: The LLM agent is capable of interacting with the game environment, leveraging its reasoning abilities to determine the most suitable actions.These actions alter the environment's state and contribute to achieving the given objective.The environment, along with its corresponding symbolic modules, offers a valid set of actions to the LLM agent.The agent's responsibility is to select an action from this set.</p>
<p>The chosen action will then dictate how the agent interacts with either the game environment or the symbolic module.</p>
<p>holds significant potential impact.</p>
<p>In this paper, our aim is to investigate the role of Large Language Models (LLMs) in symbolic reasoning within the context of text-based games.When engaging in games that involve symbolic tasks, our LLM agent generates the most rational actions based on the observed game state in a zeroshot manner, assisted by external symbolic modules such as calculators or navigators, as illustrated in Figure 1.The LLM agent employs both the text-based game environment and symbolic modules to generate a list of valid actions.These valid actions, along with the current observation, are integrated into the prompt to direct the LLM agent in selecting an appropriate action.Subsequently, the LLM agent executes this action, interacting with both the game environment and symbolic modules to complete the task.</p>
<p>In summary, our contributions include:</p>
<p>• We introduce the use of LLMs for symbolic reasoning and provide a framework for employing the LLM agent as a neurosymbolic reasoner.This achievement underscores the potential of LLMs, with the support of external modules, to function as neurosymbolic reasoners, capable of successfully completing complex tasks.</p>
<p>• We have developed the LLM agent with tailored prompts, enabling it to effectively utilize symbolic modules and enhance its performance in text-based games that involve symbolic tasks.</p>
<p>Related Work</p>
<p>Large Language Models for Decision Making.LLMs have demonstrated notable capabilities, enabling their application in tasks that extend beyond language generation (OpenAI 2023).Furthermore, they are increasingly being grounded as policy models for decision-making in interactive contexts (Yang et al. 2023).Current studies focus on enhancing the decision-making capacity of LLMs through techniques such as prompting and in-context learning.For instance, Wei et al. (2022)  Text-based Game.Text-based games can be formally characterized as partially observable Markov decision processes (POMDPs) (Côté et al. 2018).In recent years, there has been a notable increase in the design of reinforcement learning (RL) agents to solve these games (Liu et al. 2021;Hendrycks et al. 2021;Osborne, Nõmm, and Freitas 2022) More recently, with the advancement of LLMs, research has shifted towards using prompts to enable LLMs to solve textbased games (Yao et al. 2022;Shinn, Labash, and Gopinath 2023).However, these efforts have primarily focused on the LLMs' capability for in-context learning, while the exploration of their potential in symbolic reasoning has been rel-atively overlooked.</p>
<p>Neurosymbolic Reasoning.The field of neurosymbolic reasoning combines the capabilities of deep neural networks with symbolic reasoning, significantly reducing the search space associated with symbolic techniques.This approach has been used to tackle various complex multi-step inference challenges, including tasks like multi-hop question answering (Weber et al. 2019)</p>
<p>Preliminaries</p>
<p>Text-based Games as POMDPs.Text-based games can be formally defined as partially observable Markov decision processes (POMDPs), considering that the agent only observes partial information about the environment at each turn (Sutton and Barto 2018).In games with symbolic modules, at each discrete time step t, the agent is provided with an observation denoted as o t and is given a task description denoted as d.The symbolic module then produces a collection of valid actions, denoted as A t,SyM , while the text game environment concurrently establishes its own set of proper actions, denoted as A t,Env .Consequently, the set of acceptable actions at time step t is the union of these two sets, denoted as A t = A t,Env ∪ A t,SyM .The agent's goal is to select an action a t from the set of valid actions A t , given the observation o t and the task description d.If a t belongs to the set A t,SyM , the symbolic module generates the next observation o t+1 .Conversely, if a t is not part of A t,SyM , the text-based game environment processes a t and produces both the subsequent observation o t+1 and the reward r t .</p>
<p>Symbolic Tasks.There are four distinct tasks within textbased games, namely Arithmetic, MapReader, Sorting and Text World Common Sense (TWC) (Wang et al. 2022b).</p>
<p>Each task is equipped with its own symbolic modules designed to assist agents in successfully accomplishing the task.</p>
<p>Methodology</p>
<p>We introduce an LLM agent, namely a language agent, for employing LLMs2 to engage in text-based games by leveraging symbolic modules in a zero-shot manner.We begin with an overview of playing games using symbolic modules, followed by a detailed description of the key design features of our language agent, including its prompting mechanism.</p>
<p>Interaction Action Query</p>
<p>Role: Your first task is to solve the math problem.Then, pick up the item with the same quantity as the math problem answer, and place it in the box.</p>
<p>Role Initialization</p>
<p>Step 2:</p>
<p>Step 3:</p>
<p>LLM Agent</p>
<p>Step 1:</p>
<p>Action: read math problem</p>
<p>Game Symbolic Module</p>
<p>Observation: You are in the laundry room.In one part of the room you see a bench that has 23 peas, 936 squashes on it.There is also a math problem.You also see a box, that is empty.</p>
<p>Playing Games with Symbolic Tasks</p>
<p>We describe the process of playing games that involve symbolic tasks, using the LLM agent in conjunction with external symbolic modules.</p>
<p>Symbolic Modules.Symbolic modules play a crucial role in maximizing the reasoning capabilities of LLMs.For example, as shown in Figure 2, consider a scenario where a mathematical problem is presented, and a calculator is available.In such cases, the LLM's reasoning can effectively use the calculator to complete the task in a zero-shot manner.Furthermore, symbolic modules are adept at their functions, as employing an external tool like a calculator is considered an action in itself.</p>
<p>The scenarios include four distinct symbolic modules: the Calculation Module, Sorting Module, Knowledge Base Module, and Navigation Module.Table 1 shows examples of how these symbolic modules are utilized.The observation produced by a symbolic module indicates the current state of the game, while the action selected by the LLM agent serves as the input.Additionally, the Navigation Module requires the previous observation as input to accurately determine Your task is to pick up objects, then place them in their usual locations in the environment.</p>
<p>INPUT: query clean brown shirt RESPONSE: Clean brown shirt is expected to be located at wardrobe.</p>
<p>Table 1: Text-based games with symbolic tasks and their corresponding symbolic modules.INPUT refers to the current action that is sent to the symbolic modules.RESPONSE denotes the responses generated by the symbolic modules at the present time.the player's current position.For instance, in a mathematical task, the LLM agent may select a computational action such as "multiply 8 by 7" (mul 8 7).This action triggers the symbolic module to calculate the product, and the resulting observation, "Multiplying 8 and 7 results in 56," is then returned.</p>
<p>The process of engaging in text-based games with LLMs involves multiple stages.The specifics of these steps are detailed in Figure 2. As mentioned earlier, the comprehensive environment, comprising both the symbolic modules and the text-based game environment, presents the LLM agent with a list of allowable actions.Upon receiving an observation, the LLM agent uses its symbolic reasoning to select an action from this list.If the chosen action involves the symbolic module, the module provides the next observation; otherwise, the text-based game environment supplies the subsequent observation.</p>
<p>LLM as the Neurosymbolic Reasoner</p>
<p>We investigate whether the accumulated world knowledge of LLMs can aid in making accurate decisions for downstream symbolic tasks.To ground LLMs in text-based games, we employ a prompting approach, which eliminates the need for costly additional training.Therefore, we construct prompts in a way that incorporates external context, enabling the LLM agent to generate reasonable actions.</p>
<p>We describe the role of the agent, incorporating the observation, valid actions, and the constraints of executing the action in the prompt, as it is not easy for the LLM agent to understand the underlying rules of the environment through interacting with game environments.The key components of our approach include:</p>
<p>• Role initialization: We initialize the agent by providing them with task descriptions and action constraints.</p>
<p>• Action Query: This step is repeated at each timestep.We prompt the LLM agent with the current observation, inventory state, valid action set, and a question.</p>
<p>Role</p>
<p>Initialization</p>
<p>You are a robot.{TASK DESC}\n You are required to choose action from the valid action set to complete the task step by step.\nTo take action, respond with an action in the valid action set.\n Action Query {OBS}\n {INV STATE}\n Your current score is: {SCORE}\n The valid action set contains: {VALID ACT SET}.\nPlease choose one action from the valid action set to finish the task step by step.\nDo NOT respond with any other text, and you cannot decline to take an action.</p>
<p>Table 2: The prompting format for role initialization and action query for each time step.{TASK DESC} is the task description.{OBS} is the current observation.{INV STATE} describes the items in your inventory.{SCORE} is the obtained reward.{VALID ACT SET} is a set of valid actions at the current time step.</p>
<p>• Answer by the LLM agent: The LLM agent chooses an action from the valid action set to complete the task.</p>
<p>Role Initialization.We initialize the role and provide instructions for a functional agent assigned to a task.This process informs the agent about its role, the task description, and the actions it can take, along with their explanations and constraints.These actions are necessary for interacting with text-based games or calling the symbolic module.The agent is instructed to choose from a valid set of actions, such as reading the map, getting paths to specific locations, and recalling the task.Additionally, the agent is advised to utilize the external symbolic module and to avoid unnecessary actions during the task.</p>
<p>Action Query.At each timestep, we inform the LLM agent of the current game state, as outlined in To sort the items one by one, please follow the instruction:\n 1) choose 'sort ascending' or 'sort descending' to know which one should be sort next.\n2) take the items.\n3) put the items in box.\nTWC 1) When you take the item, you will get positive score.\n2) When you put the item in the right place, you will get higher positive score.Otherwise you get 0.\n 3) You are supposed to get as much score as possible.\nTable 3: The prompting format for adding constraints on the actions of an agent.</p>
<p>the inventory, the reward, and the valid action set.The inventory state refers to the current possessions of the agent.For instance, in mathematical tasks, the inventory state may consist of a mathematical problem, while in the MapReader task, it could include a map.Additionally, the inventory state can encompass tangible objects, such as toothpaste or a quantity of 18 avocados, acquired by the agent within the environment.The LLM agent is then tasked with selecting one action from the valid action set to continue with the task.</p>
<p>It is important to note that the LLM agent is not allowed to decline or provide any text beyond the prescribed response.</p>
<p>We also limit the number of valid actions provided by the symbolic module.</p>
<p>In addition, it is essential to develop appropriate prompts that effectively restrict the agent's actions according to the information provided in Table 3.It is not feasible for the agent to acquire knowledge and infer the rules within trajectories solely through its interaction with the environment.In all tasks, there is typically a specific order of events, where the object is first taken and then placed in a designated lo-cation.This strategy is adopted to prevent scenarios where the object is placed before it is acquired, which would be considered unacceptable in the given context.</p>
<p>Experiments</p>
<p>We demonstrate the potential of LLMs in serving as neurosymbolic reasoners for text-based games.In particular, we present experimental results on four text-based games that involve different symbolic tasks.In these tasks, we observe that LLMs can effectively function as symbolic reasoners.</p>
<p>Setup</p>
<p>We follow the evaluation framework and game environments in Wang et al. (2022b).These games are developed using the TextWorldExpress game engine (Jansen and Cote 2023).For our LLM agent, we use GPT-3.5-turbo.The LLM agent can interact with game environments and symbolic modules.The task descriptions and examples of how the symbolic modules are called are provided in Table 1.The evaluation includes four text-based games involving symbolic tasks.Each task is divided into "Train", "Dev", and "Test" sets.All evaluations are conducted on the "Test" set.</p>
<p>The evaluation metric is based on two factors: the average score achieved at the end of each game, and the average number of steps taken within a single episode.</p>
<p>Environments</p>
<p>We use four text-based game benchmark environments (Wang et al. 2022b):</p>
<p>Arithmetic.The task at hand involves a mathematical component, wherein an agent is required to read and solve a mathematical problem.This process determines the specific object from a given set of objects that they should select and place.The arithmetic game includes a calculator module equipped with the capability to perform basic mathematical operations, including addition, subtraction, multiplication, and division.</p>
<p>MapReader.A pick-and-place game with a navigation theme, similar to the Coin Collector game (Yuan et al. 2018).The agent is equipped with a map that may be exploited to optimize route planning.The map provides information on the connections between rooms, such as the lounge connecting to the cookery and supermarket.The navigation symbolic module has the capability to extract location information from the observation space.This includes specific information relating to the present location and geographical features leading to the intended destination.For instance, the instructions sent to the agent might indicate that in order to get from the cooking area to the recreation zone, one must pass through the bar, steam room, library, and finally reach the recreation zone.</p>
<p>Sorting.This game involves an agent initially situated in a room containing a variable number of objects, ranging from three to five.The agent's task is to sequentially place these objects into a designated box, adhering to a specific sorting criterion based on increasing quantity.In this game, units related to volume, mass, or length are used, as exemplified by items such as 25g of oak, 12ml of marble, and 6cm of cedar.The sorting game includes a module capable of extracting information from the observation space.This module is specifically designed to identify items that include quantities and can arrange these objects in either ascending or descending order, following the user's instructions.</p>
<p>Text World Common Sense (TWC).The challenges provided in this game serve as a baseline for evaluating common sense reasoning abilities (Murugesan et al. 2021).In this game, agents are required to gather objects from their surroundings, such as a clean brown shirt, and subsequently place these objects in their appropriate and commonly recognized locations, like a wardrobe.The incorporation of a symbolic module within this game enables agents to engage in knowledge-based queries.For instance, it allows them to deduce that a clean brown shirt is typically found in a wardrobe.</p>
<p>Baselines</p>
<p>We also compare our LLM agent with two baselines, namely the Deep Reinforcement Relevance Network (DRRN) (He et al. 2016) and the T5-based Behavior Cloned Transformer (Raffel et al. 2020;Wang et al. 2022b), as follows:</p>
<p>• DRRN: The primary concept of the DRRN is based on Q-learning.The candidate action with the highest anticipated Q-value is chosen as the next action, based on the current observation.The DRRN employs a Deep Q-Network (Mnih et al. 2013) to estimate the Q-value for each observation-action pair.Xu et al. (2020) note that the DRRN is a fast and robust reinforcement learning baseline, frequently used to produce near state-of-the-art performance in a variety of text-based games.</p>
<p>• Behavior Cloned Transformer: This method adopts an imitation learning approach, conceptualizing reinforcement learning as a sequence-to-sequence problem, similar to the Decision Transformer (Chen et al. 2021).It predicts the subsequent action based on a sequence of previous observations.This baseline aligns with the approach described in Ammanabrolu et al. (2021), where the model input at timestep t includes the task description, current state observation, previous action, and previous state observation.Symbolic modules are utilized in the demonstrations, specifically employing gold trajectories.</p>
<p>Following Wang et al. (2022b), both baseline models include two variants: one with symbolic modules and one without.When using symbolic modules, we inject actions from these modules into the action space of each game for the baseline models.</p>
<p>Results</p>
<p>Based on the results presented in Table 4, it is evident that the use of the symbolic module in conjunction with the LLM agent yields a favorable average performance compared to other baseline approaches.When comparing the outcomes of the Behavior Cloned Transformer with a symbolic module to those of the LLM agent, the performance of the LLM agent is observed to be slightly lower.However, the LLM agent demonstrates a similar level of competency in interacting with the game environment.Furthermore, unlike the Behavior Cloned Transformer models, the LLM agent does not require extensive training with a large volume of expert data.As a result, this approach saves significant training resources.</p>
<p>Table 5 demonstrates that the LLM agent possesses a robust capacity for reasoning, enabling effective handling of tasks involving symbolic tasks.It shows exceptional performance, particularly in mathematics.In the MapReader benchmark, the agent achieves commendable scores, though it requires a considerable number of steps to complete the task.This inefficiency is mainly due to the agent's tendency to forget the route obtained from the symbolic module, leading to the risk of reaching incorrect locations and necessitating repeated route queries.The complexity of map logic, which involves determining one's current location and desired destination, adds to the probabilistic nature of this task.In contrast, the Sorting task reveals suboptimal performance, as the LLM agent's understanding of sorting logic is not fully developed.This issue is largely attributed to the agent's limited memory capacity, hindering its ability to remember the ascending order of all objects.</p>
<p>In Table 6, it compares the performance of the model with constrained prompts to that of the model without constrained prompts.The results indicate that when the LLM agent is provided with the prompts outlined in Table 3, there is an improvement in performance across all tasks.Additionally, a reduction in the average number of steps required to interact with the game environment is observed.This demonstrates the effectiveness of our constrained prompts in these tasks.Furthermore, experimental results using GPT-4, as shown in  3.</p>
<p>Table 7, reveal that it significantly outperforms the GPT-3.5 agent in the MapReader and Sorting tasks, while showing weaker performance in the TWC task.</p>
<p>Discussion.Our results demonstrate that the incorporation of external symbolic modules by the LLM agent leads to enhanced average accuracy compared to other baselines.This capability is achieved by leveraging the underlying patterns present in the training data.Instead of relying on symbolic thinking or explicit rules, this approach acquires knowledge by recognizing patterns and associations from the extensive corpus of text to which it has been exposed during its training phase, as exemplified by GPT-3.5 and GPT-4 (OpenAI 2023).Although the LLM agent has the capability to connect with a symbolic module for specific tasks, it still exhibits uncertainty and is prone to making mistakes.</p>
<p>Conclusion</p>
<p>This paper has demonstrated the effective application of Large Language Models (LLMs) in complex text-based games involving symbolic tasks.Utilizing a prompting approach, we have guided the LLM agent to efficiently engage with symbolic modules within these games.The efficacy of our method, leveraging LLMs, has shown superior performance compared to alternative benchmarks, highlighting the potential of LLMs to enhance training procedures in text-based games.Consequently, it can be posited that Large Language Models can be considered as Neurosymbolic Reasoners, possessing significant potential for performing sym- The performance of the LLM agent using GPT-3.5 and GPT-4 on the "Test" set.</p>
<p>bolic tasks in real-world applications.</p>
<p>Limitations</p>
<p>The addition of more detailed prompts could offer greater control over the actions of the LLM agent.This would be particularly beneficial in tasks like Sorting, where providing essential information beforehand is advantageous.Acknowledging and addressing these limitations could significantly enhance the system's performance.For future progress, it is crucial to extend the model's application to more complex domains, going beyond the scope of straightforward textbased games.Integrating more sophisticated symbolic modules would be necessary to tackle the complexities of diverse scenarios, thereby facilitating a more efficient problemsolving approach.</p>
<p>, language contextualization (Zellers et al. 2021), and semantic analysis (Cambria et al. 2022).Text-based games that involve symbolic tasks serve as a valuable test-bed for addressing such challenges.Previous approaches have employed traditional optimization techniques or reinforcement learning agents.For example, Kimura et al. (2021a) decompose text-based games into collections of logical rules, which are then integrated with deep reinforcement learning.Basu et al. (2022) use Integer Linear Programming (ILP) to substantially improve agent performance, providing an interpretable framework for understanding agents' selection of specific actions.</p>
<p>Your task is to solve the following math problem: multiply 26 and 36 .Then, pick up the item with the same quantity as the answer, and place it in the box.</p>
<p>Figure 2 :
2
Figure 2: An overview of how an LLM agent plays textbased games with external symbolic modules.The following procedural steps are involved in utilizing the LLM agent for engaging in a text-based game.Initially, the LLM agent is provided with a role initialization prompt.The first observation received by the LLM agent comes from the text game environment.As depicted in the diagram, the selection of actions, determined by the LLM's reasoning, activates the symbolic module.Subsequently, the symbolic module provides output, including observations related to the module.Then the next action chosen by the LLM agent is influenced by the outcome from the symbolic module.This process is executed repeatedly until the end of the game.</p>
<p>Your first task is to solve the math problem.Then, pick up the item with the same quantity as the math problem answer, and place it in the box.Your task is to take the coin located in the pantry, and put it into the box found in the chamber.A map is provided, that you may find helpful.
Task(SymbolicDescriptionSymbolic ModuleModule)Arithmetic (Calculation Module)INPUT: mul 8 7 RESPONSE: Multiplying 8 and 7 results in 56.MapReader (Navigation Module)INPUT: next step to pantry RESPONSE: The next location to go to is canteen. If you want to go to pantry from chamber, you need go through canteen, pantry.Sorting (Sorting Module)Your task is to sort objects by quantity. First, place the ob-ject with the smallest quantity in the box. Then, place the objects with the next smallest quantity in the box, and re-peat until all objects have been placed in the box.INPUT: sort ascending RESPONSE: The observed items, sorted in order of in-creasing quantity, are: 25 g of oak, 47 g of brick, 15 kg of cedar, 21 kg of marble.TWC(KnowledgeBase Module)</p>
<p>Table 2
2. This</p>
<p>Table 4 :
4
The average performance of the model across a set of 100 games in the unseen test set."+symbolic module" indicates the utilization of symbolic modules within the action space of the models.
DRRNBehavior Cloned TransformerLLM AgentBaseline+symbolic moduleBaseline+symbolic moduleBenchmark Score Steps ScoreStepsScore Steps ScoreStepsScore StepsArithmetic0.17100.1470.5651.0051.004MapReader 0.02500.02500.71271.00100.8615Sorting0.03210.03180.7270.9880.717TWC0.57270.37340.9060.9730.944Average0.20270.14270.72110.9970.887</p>
<p>Table 6 :
6
The performance of the LLM agent with and without constrained prompts on the "Test" set.The constrained prompts are shown in Table
TaskTrain Score Steps Score Steps Score Steps Dev TestArithmetic1.0030.9541.004MapReader 0.84150.84140.8615Sorting0.7070.6360.717TWC0.9340.835 50.944Average0.8770.8170.887Table 5: The performance of the LLM agent on differentsets of the game, including "Train", "Dev", and "Test". Thescores are subjected to normalization, resulting in valuesranging from 0 to 1, with higher values indicating greaterperformance. On the other hand, the steps quantify the num-ber of actions taken by an agent inside the environment, withlower values indicating more efficient behavior.Taskw/ Constraints w/o Constrains Score Steps Score StepsArithmetic 1.0040.963MapReader 0.86150.6412Sorting0.7170.3510TWC0.9440.737Average0.8870.678</p>
<p>Table 7 :
7Taskw/ GPT-3.5 Score Steps Score Steps w/ GPT-4Arithmetic1.0041.004MapReader 0.86150.997Sorting0.7170.938TWC0.9440.7116Average0.8870.918
The Thirty-Eighth AAAI Conference on Artificial Intelligence 
We utilize LLMs from OpenAI: https://chat.openai.com/.</p>
<p>Learning dynamic belief graphs to generalize on text-based games. A Adhikari, X Yuan, M.-A Côté, M Zelinka, M.-A Rondeau, R Laroche, P Poupart, J Tang, A Trischler, W Hamilton, Advances in Neural Information Processing Systems. 2020</p>
<p>Graph constrained reinforcement learning for natural language action spaces. P Ammanabrolu, M Hausknecht, International Conference on Learning Representations. 2020</p>
<p>How to Motivate Your Dragon: Teaching Goal-Driven Agents to Speak and Act in Fantasy Worlds. P Ammanabrolu, J Urbanek, M Li, A Szlam, T Rocktäschel, J Weston, K Basu, K Murugesan, M Atzeni, P Kapanipathi, K Talamadupula, T Klinger, M Campbell, M Sachan, G Gupta, Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies2021. 2022Combining Learning and Reasoning: Programming Languages, Formalisms, and Representations</p>
<p>E Brooks, L Walls, R L Lewis, S Singh, arXiv:2210.03821context policy iteration. 2022arXiv preprint</p>
<p>Language models are few-shot learners. T Brown, B Mann, N Ryder, M Subbiah, J D Kaplan, P Dhariwal, A Neelakantan, P Shyam, G Sastry, A Askell, Advances in Neural Information Processing Systems. 2020</p>
<p>SenticNet 7: A commonsense-based neurosymbolic AI framework for explainable sentiment analysis. E Cambria, Q Liu, S Decherchi, F Xing, K Kwok, Proceedings of the Thirteenth Language Resources and Evaluation Conference. the Thirteenth Language Resources and Evaluation Conference2022</p>
<p>Decision transformer: Reinforcement learning via sequence modeling. L Chen, K Lu, A Rajeswaran, K Lee, A Grover, M Laskin, P Abbeel, A Srinivas, I Mordatch, Advances in Neural Information Processing Systems. 2021</p>
<p>Textworld: A learning environment for textbased games. M.-A Côté, A Kádár, X Yuan, B Kybartas, T Barnes, E Fine, J Moore, M Hausknecht, L El Asri, M Adada, Computer Games: 7th Workshop. 2018. 2018Held in Conjunction with the 27th International Conference on Artificial Intelligence</p>
<p>Deep Reinforcement Learning with a Combinatorial Action Space for Predicting Popular Reddit Threads. J He, M Ostendorf, X He, J Chen, J Gao, L Li, L Deng, Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. the 2016 Conference on Empirical Methods in Natural Language Processing2016</p>
<p>What would jiminy cricket do? Towards agents that behave morally. D Hendrycks, M Mazeika, A Zou, S Patel, C Zhu, J Navarro, D Song, B Li, J Steinhardt, Advances in Neural Information Processing Systems. 2021</p>
<p>Language models as zero-shot planners: Extracting actionable knowledge for embodied agents. W Huang, P Abbeel, D Pathak, I Mordatch, International Conference on Machine Learning. 2022a</p>
<p>W Huang, F Xia, T Xiao, H Chan, J Liang, P Florence, A Zeng, J Tompson, I Mordatch, Y Chebotar, arXiv:2207.05608Inner monologue: Embodied reasoning through planning with language models. 2022barXiv preprint</p>
<p>TextWorldExpress: Simulating Text Games at One Million Steps Per Second. P Jansen, M Cote, Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics: System Demonstrations. the 17th Conference of the European Chapter of the Association for Computational Linguistics: System Demonstrations2023</p>
<p>LOA: Logical Optimal Actions for Text-based Interaction Games. D Kimura, S Chaudhury, M Ono, M Tatsubori, D J Agravante, A Munawar, A Wachi, R Kohita, A Gray, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: System Demonstrations. the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: System Demonstrations2021a</p>
<p>Neuro-Symbolic Reinforcement Learning with First-Order Logic. D Kimura, M Ono, S Chaudhury, R Kohita, A Wachi, D J Agravante, M Tatsubori, A Munawar, A Gray, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. the 2021 Conference on Empirical Methods in Natural Language Processing2021b</p>
<p>Reward design with language models. M Kwon, S M Xie, K Bullard, D Sadigh, International Conference on Learning Representations. Lample, G.; and Charton, F. 2020. Deep Learning For Symbolic Mathematics. 2023International Conference on Learning Representations</p>
<p>Code as policies: Language model programs for embodied control. J Liang, W Huang, F Xia, P Xu, K Hausman, B Ichter, P Florence, A Zeng, 2023 IEEE International Conference on Robotics and Automation. 2023</p>
<p>Learning object-oriented dynamics for planning from text. G Liu, A Adhikari, A.-M Farahmand, P Poupart, International Conference on Learning Representations. 2021</p>
<p>Self-refine: Iterative refinement with selffeedback. A Madaan, N Tandon, P Gupta, S Hallinan, L Gao, S Wiegreffe, U Alon, N Dziri, S Prabhumoye, Y Yang, arXiv:2303.176512023arXiv preprint</p>
<p>MetaICL: Learning to Learn In Context. S Min, M Lewis, L Zettlemoyer, H Hajishirzi, Proceedings of the 2022 Conference of the North American Chapter. the 2022 Conference of the North American ChapterHuman Language Technologies2022</p>
<p>V Mnih, K Kavukcuoglu, D Silver, A Graves, I Antonoglou, D Wierstra, M Riedmiller, arXiv:1312.5602Playing atari with deep reinforcement learning. 2013arXiv preprint</p>
<p>Text-based rl agents with commonsense knowledge: New challenges, environments and baselines. K Murugesan, M Atzeni, P Kapanipathi, P Shukla, S Kumaravel, G Tesauro, K Talamadupula, M Sachan, M Campbell, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202135</p>
<p>Language Understanding for Text-based Games using Deep Reinforcement Learning. K Narasimhan, T Kulkarni, R Barzilay, ArXiv, abs/2303.08774Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, 1-11. OpenAI. 2023. GPT-4 Technical Report. the 2015 Conference on Empirical Methods in Natural Language Processing, 1-11. OpenAI. 2023. GPT-4 Technical Report2015</p>
<p>A survey of text games for reinforcement learning informed by natural language. P Osborne, H Nõmm, A Freitas, Transactions of the Association for Computational Linguistics. 2022</p>
<p>Contrastive reinforcement learning of symbolic reasoning domains. G Poesia, W Dong, N Goodman, Advances in Neural Information Processing Systems. 2021</p>
<p>Limitations of Language Models in Arithmetic and Symbolic Induction. J Qian, H Wang, Z Li, S Li, X Yan, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. the 61st Annual Meeting of the Association for Computational Linguistics20231</p>
<p>Exploring the limits of transfer learning with a unified text-to-text transformer. C Raffel, N Shazeer, A Roberts, K Lee, S Narang, M Matena, Y Zhou, W Li, P J Liu, The Journal of machine learning Research. 2112020</p>
<p>Sword Cuts: Commonsense Inductive Bias for Exploration in Text-based Games. D Ryu, E Shareghi, M Fang, Y Xu, S Pan, R Haf, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Short Papers. the 60th Annual Meeting of the Association for Computational Linguistics20222</p>
<p>Stay moral and explore: Learn to behave morally in text-based games. Z Shi, M Fang, Y Xu, L Chen, Y Du, International Conference on Learning Representations. 2022</p>
<p>Reflexion: an autonomous agent with dynamic memory and selfreflection. N Shinn, B Labash, A Gopinath, arXiv:2303.113662023arXiv preprint</p>
<p>Progprompt: Generating situated robot task plans using large language models. I Singh, V Blukis, A Mousavian, A Goyal, D Xu, J Tremblay, D Fox, J Thomason, A Garg, IEEE. 2023. 2023</p>
<p>Reinforcement learning: An introduction. R S Sutton, A G Barto, 2018MIT press</p>
<p>ChatGPT for Robotics: Design Principles and Model Abilities. S Vemprala, R Bonatti, A Bucker, A Kapoor, R Wang, P Jansen, M.-A Côté, P Ammanabrolu, MSR-TR-2023-8Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language ProcessingMicrosoft2023. 2022aTechnical ReportScienceWorld: Is your Agent Smarter than a 5th Grader?</p>
<p>Behavior Cloned Transformers are Neurosymbolic Reasoners. R Wang, P A Jansen, M.-A Côté, P Ammanabrolu, Conference of the European Chapter. the Association for Computational Linguistics2022b</p>
<p>NLProlog: Reasoning with Weak Unification for Question Answering in Natural Language. L Weber, P Minervini, J Münchmeyer, U Leser, T Rocktäschel, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. the 57th Annual Meeting of the Association for Computational Linguistics2019</p>
<p>Generalization in Text-based Games via Hierarchical Reinforcement Learning. J Wei, X Wang, D Schuurmans, M Bosma, F Xia, E H Chi, Q V Le, D Zhou, Findings of the Association for Computational Linguistics: EMNLP 2021. Xu, Y2022. 2021Advances in Neural Information Processing Systems</p>
<p>Perceiving the World: Question-guided Reinforcement Learning for Text-based Games. Y Xu, M Fang, L Chen, Y Du, J Zhou, C Zhang, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. the 60th Annual Meeting of the Association for Computational Linguistics2022</p>
<p>Deep reinforcement learning with stacked hierarchical attention for text-based games. Y Xu, M Fang, L Chen, Y Du, J T Zhou, C Zhang, Advances in Neural Information Processing Systems. 2020</p>
<p>Keep CALM and Explore: Language Models for Action Generation in Text-based Games. S Yang, O Nachum, Y Du, J Wei, P Abbeel, D Schuurmans, S Yao, R Rao, M Hausknecht, K Narasimhan, arXiv:2303.04129Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing. the 2020 Conference on Empirical Methods in Natural Language Processing2023. 2020arXiv preprintFoundation models for decision making: Problems, methods, and opportunities</p>
<p>React: Synergizing reasoning and acting in language models. S Yao, J Zhao, D Yu, N Du, I Shafran, K Narasimhan, Y Cao, X Yin, J May, arXiv:2210.03629IEEE Conference on Games. 2022. 2019arXiv preprintComprehensible context-driven text game playing</p>
<p>PIGLeT: Language Grounding Through Neuro-Symbolic Interaction in a 3D World. X Yuan, M.-A Côté, A Sordoni, R Laroche, R T D Combes, M Hausknecht, A Trischler, R Zellers, A Holtzman, M Peters, R Mottaghi, A Kembhavi, A Farhadi, Y Choi, arXiv:1806.11525Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. Long Papers. the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing2018. 20211arXiv preprintCounting to explore and generalize in text-based games</p>            </div>
        </div>

    </div>
</body>
</html>
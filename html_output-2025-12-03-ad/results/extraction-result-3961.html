<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3961 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3961</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3961</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-92.html">extraction-schema-92</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how LLM-generated scientific theories are evaluated, including evaluation criteria, methods, benchmarks, metrics, human involvement, limitations, and examples.</div>
                <p><strong>Paper ID:</strong> paper-51000d9f79be0eefd7972fe94e3c71dddc90d2c6</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/51000d9f79be0eefd7972fe94e3c71dddc90d2c6" target="_blank">Evaluating the Text-to-SQL Capabilities of Large Language Models</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> An empirical evaluation of Text-to-SQL capabilities of the Codex language model is performed and it is demonstrated on the GeoQuery and Scholar benchmarks that a small number of in-domain examples provided in the prompt enables Codex to perform better than state-of-the-art models finetuned on such few-shot examples.</p>
                <p><strong>Paper Abstract:</strong> We perform an empirical evaluation of Text-to-SQL capabilities of the Codex language model. We find that, without any finetuning, Codex is a strong baseline on the Spider benchmark; we also analyze the failure modes of Codex in this setting. Furthermore, we demonstrate on the GeoQuery and Scholar benchmarks that a small number of in-domain examples provided in the prompt enables Codex to perform better than state-of-the-art models finetuned on such few-shot examples.</p>
                <p><strong>Cost:</strong> 0.012</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3961.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3961.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how LLM-generated scientific theories are evaluated, including evaluation criteria, methods, benchmarks, metrics, human involvement, limitations, and examples.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Spider</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Spider benchmark (cross-domain Text-to-SQL)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large human-labeled cross-domain Text-to-SQL dataset used as the primary automatic benchmark in this paper; evaluated via SQL validity, execution accuracy, and test-suite execution accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Spider: A largescale human-labeled dataset for complex and crossdomain semantic parsing and text-to-sql task</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Validity of generated SQL (executable in SQLite), execution correctness (execution accuracy: whether running the generated SQL returns the same result as gold), and semantic equivalence measured by test-suite execution accuracy (accounts for different-but-equivalent SQL forms).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>Automatic execution of generated SQL against the database; comparison of execution outputs to gold answers; use of distilled test suites (per Zhong et al.) to judge semantic equivalence beyond surface query equality.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Spider: large, cross-domain human-labeled Text-to-SQL dataset providing schemas and questions; used as the main zero-shot evaluation dataset (development set used to avoid sending held-out test to API).</td>
                        </tr>
                        <tr>
                            <td><strong>metrics_reported</strong></td>
                            <td>Percentage of predictions that are valid SQL (VA), execution accuracy (EX) as %, and test-suite execution accuracy (TS) as % (reported per model and prompt, e.g. davinci-codex achieving up to 67% EX and 56.5% TS on Spider dev).</td>
                        </tr>
                        <tr>
                            <td><strong>human_involvement</strong></td>
                            <td>Minimal for bulk scoring (automatic execution); human involvement for targeted manual annotation of sample errors (see separate entry) but not for the bulk Spider dev set scoring reported.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Gold queries may use conventions (aliasing, selection of columns, argmax tie-breaking) different from model outputs, causing correct-but-different queries to be penalized by exact or naive execution comparison; possible memorization concerns because Spider data is publicly available (addressed in Appendix A.5); cannot evaluate held-out Spider test offline without sending examples to the API (risk of data leakage).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_theory_example</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Codex (davinci-codex) with schema + small sample rows prompts achieved competitive performance on Spider dev (up to 67% execution accuracy, 56.5% test-suite accuracy); prompt format and inclusion of schema/database content strongly affect performance; many model 'errors' are due to stylistic or convention differences rather than semantic failures.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating the Text-to-SQL Capabilities of Large Language Models', 'publication_date_yy_mm': '2022-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3961.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3961.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how LLM-generated scientific theories are evaluated, including evaluation criteria, methods, benchmarks, metrics, human involvement, limitations, and examples.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Test-suite (Zhong et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Test-suite execution accuracy (semantic evaluation for Text-to-SQL)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A semantic equivalence evaluation methodology that runs a distilled set of test cases to determine whether generated SQL is semantically equivalent to the gold query, reducing false negatives from syntactic differences.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Semantic evaluation for text-to-sql with distilled test suites</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Semantic equivalence of generated SQL to gold as judged by behavior on a distilled set of execution tests (i.e., does generated query produce same outputs across targeted inputs?).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>Run distilled test-suite cases against generated and gold queries; mark a generation correct if it matches gold behavior across the suite (automated).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Applied on Spider (and Spider-Realistic) to provide a more semantically aware accuracy measure than raw execution/equality.</td>
                        </tr>
                        <tr>
                            <td><strong>metrics_reported</strong></td>
                            <td>Test-suite execution accuracy (TS) reported as percentage of examples where generated SQL passes the distilled test-suite (e.g., davinci-codex TS reported per prompt).</td>
                        </tr>
                        <tr>
                            <td><strong>human_involvement</strong></td>
                            <td>None required for the automatic test-suite; the paper additionally uses human annotation to estimate how many test-suite failures are actually acceptable (see manual annotation entry).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Distilled test suites must be well-designed to capture semantic equivalence; they may still miss valid alternative formulations or conversely, be too strict/lenient; constructing such suites requires care and resources.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_theory_example</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Test-suite metric reduces penalization of semantically equivalent but syntactically different queries relative to naive execution/equality, but the paper still finds a substantial fraction of model outputs judged wrong by the test-suite are accepted by human annotators (~31% of valid-but-erroneous predictions were annotated as 'Ambiguous Correct').</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating the Text-to-SQL Capabilities of Large Language Models', 'publication_date_yy_mm': '2022-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3961.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3961.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how LLM-generated scientific theories are evaluated, including evaluation criteria, methods, benchmarks, metrics, human involvement, limitations, and examples.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Automatic metrics (VA/EX/TS)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Automatic evaluation metrics: Valid SQL (VA), Execution accuracy (EX), Test-suite accuracy (TS)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper's core quantitative evaluation signals: VA measures syntactic/execution validity, EX measures whether outputs produce correct results, and TS measures semantic equivalence via distilled test suites.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Syntactic validity (executable SQLite), exact correctness by execution result, and semantic equivalence via test-suite behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>For each generated SQL: (1) check whether it is valid/executable in SQLite (VA), (2) execute it and compare results to gold output for execution accuracy (EX), (3) run distilled test-suite cases and compare behaviors for test-suite accuracy (TS).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Applied across Spider development set and Spider-Realistic; also used to report performance on GeoQuery and Scholar in few-shot experiments (test-suite accuracy reported for those datasets).</td>
                        </tr>
                        <tr>
                            <td><strong>metrics_reported</strong></td>
                            <td>VA, EX, and TS values reported as percentages per model and prompt configuration; also exact-match reported for finetuned T5 in some baselines (e.g., 71% exact-match for T5-3B on Spider validation in baseline training).</td>
                        </tr>
                        <tr>
                            <td><strong>human_involvement</strong></td>
                            <td>Automatic metrics require no humans; human review was performed on a random sample of 100 valid-but-erroneous predictions to better interpret automatic metric failures.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Automatic metrics can misclassify acceptable alternative queries as incorrect; VA is coarse (valid SQL may still be semantically wrong); EX can miss semantic equivalence when queries differ structurally; TS mitigates but does not eliminate these issues.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_theory_example</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Reported VA/EX/TS numbers reveal strong sensitivity to prompt design (e.g., Create Table + Select 3 prompt increased EX substantially), diminishing returns from larger Codex size beyond context window benefits, and scenarios where adding more database rows to prompts harms EX/TS despite increasing VA.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating the Text-to-SQL Capabilities of Large Language Models', 'publication_date_yy_mm': '2022-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3961.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e3961.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how LLM-generated scientific theories are evaluated, including evaluation criteria, methods, benchmarks, metrics, human involvement, limitations, and examples.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Human error analysis</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Manual annotation and error analysis of model outputs</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Human annotators manually inspected a random sample of model outputs that were valid SQL but judged incorrect by automated evaluation, categorizing errors into semantic-incorrect and ambiguous-correct classes and subtypes.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Human judgement of whether a generated SQL is a correct/acceptable translation of the natural language question (semantic correctness and acceptability despite differences from gold).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>Randomly sample 100 valid SQL predictions labeled incorrect by test-suite; a human annotator assigns categories (Test-Suite Correct, Semantic Incorrect with subtypes like Shortcuts, GROUP BY convention errors, Ambiguous Correct with subtypes like SELECT Extra Columns, Argmax, etc., and Invalid SQL subtypes).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Applied to davinci-codex outputs on Spider development set with Create Table + Select 3 prompt; categories and percentages reported in a table (Table 3) and examples in Figures 1 and 3.</td>
                        </tr>
                        <tr>
                            <td><strong>metrics_reported</strong></td>
                            <td>Annotated breakdown percentages over sample: e.g., 55.1% Test-Suite Correct (of that sample?), 25.2% Semantic Incorrect (with 5.1% Shortcuts), 11.3% Ambiguous Correct (with 2.9% SELECT Extra Columns), 8.4% Invalid SQL; also E% column reporting distribution within manually annotated erroneous queries.</td>
                        </tr>
                        <tr>
                            <td><strong>human_involvement</strong></td>
                            <td>High — a human annotator performed detailed labeling and qualitative analysis for the sampled predictions; used to interpret automatic metric failures and guide future prompt/control strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Small sample size (100 predictions) so annotations provide indicative rather than exhaustive statistics; single annotator or limited annotation process may introduce subjectivity; labor-intensive and not scalable to full test sets.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_theory_example</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Human annotation revealed that about 31% of valid but automatically judged erroneous predictions were considered acceptable by a human (Ambiguous Correct), indicating that automatic metrics undercount acceptable outputs; identified common error modes (shortcuts, grouping conventions, selection differences) which inform prompt design and potential fixes.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating the Text-to-SQL Capabilities of Large Language Models', 'publication_date_yy_mm': '2022-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3961.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e3961.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how LLM-generated scientific theories are evaluated, including evaluation criteria, methods, benchmarks, metrics, human involvement, limitations, and examples.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Few-shot adaptation evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Few-shot prompt-based vs finetuning-based adaptation (GeoQuery & Scholar)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Comparative evaluation of LLM few-shot prompting (Codex) against finetuning a smaller model (T5-3B) on small numbers of in-domain examples, measuring adaptation effectiveness on GeoQuery and Scholar.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Test-suite execution accuracy on target dataset (GeoQuery or Scholar) as the number of support examples (n-shot) varies; also compare zero-shot performance and full-dataset finetuned baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>For Codex: append n question-query examples in prompt (n-shot) and generate with greedy decoding; for baseline: finetune T5-3B on the same support examples and evaluate; vary n and plot test-suite accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>GeoQuery and Scholar repurposed into n-shot tasks by selecting one example per most frequent templates; GeoQuery allows up to 40-shot in davinci-codex due to context limit; Scholar has shorter prompt capacity.</td>
                        </tr>
                        <tr>
                            <td><strong>metrics_reported</strong></td>
                            <td>Test-suite accuracy (TS) reported vs number of support examples; baseline full-training exact/test accuracy for comparison (e.g., T5-3B baseline reaches 85.7% on GeoQuery when trained on full set).</td>
                        </tr>
                        <tr>
                            <td><strong>human_involvement</strong></td>
                            <td>None described for bulk few-shot evaluation; humans involved earlier for error analysis but not in routine few-shot metric computation.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>API context window limits how many support examples can be included (varies by model), affecting few-shot performance; dataset/query length limits fit; zero-shot performance varies by dataset conventions (argmax convention differences), so transfer depends on shared conventions; finetuning may be more sample-efficient for some datasets (e.g., T5 outperforms Codex zero-shot on Scholar).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_theory_example</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Codex (davinci-codex) outperforms a T5-3B finetuned on the same small support sets in many few-shot settings (especially GeoQuery up to 40-shot), showing strong adaptation via prompting; however T5 can outperform Codex in some zero-shot settings (Scholar), and Codex performance depends on prompt size and dataset conventions.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating the Text-to-SQL Capabilities of Large Language Models', 'publication_date_yy_mm': '2022-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Semantic evaluation for text-to-sql with distilled test suites <em>(Rating: 2)</em></li>
                <li>Spider: A largescale human-labeled dataset for complex and crossdomain semantic parsing and text-to-sql task <em>(Rating: 2)</em></li>
                <li>Improving text-to-SQL evaluation methodology <em>(Rating: 2)</em></li>
                <li>Structure-grounded pretraining for text-to-sql <em>(Rating: 1)</em></li>
                <li>Exploring unexplored generalization challenges for cross-database semantic parsing <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3961",
    "paper_id": "paper-51000d9f79be0eefd7972fe94e3c71dddc90d2c6",
    "extraction_schema_id": "extraction-schema-92",
    "extracted_data": [
        {
            "name_short": "Spider",
            "name_full": "Spider benchmark (cross-domain Text-to-SQL)",
            "brief_description": "A large human-labeled cross-domain Text-to-SQL dataset used as the primary automatic benchmark in this paper; evaluated via SQL validity, execution accuracy, and test-suite execution accuracy.",
            "citation_title": "Spider: A largescale human-labeled dataset for complex and crossdomain semantic parsing and text-to-sql task",
            "mention_or_use": "use",
            "evaluation_criteria": "Validity of generated SQL (executable in SQLite), execution correctness (execution accuracy: whether running the generated SQL returns the same result as gold), and semantic equivalence measured by test-suite execution accuracy (accounts for different-but-equivalent SQL forms).",
            "evaluation_methods": "Automatic execution of generated SQL against the database; comparison of execution outputs to gold answers; use of distilled test suites (per Zhong et al.) to judge semantic equivalence beyond surface query equality.",
            "benchmark_or_dataset": "Spider: large, cross-domain human-labeled Text-to-SQL dataset providing schemas and questions; used as the main zero-shot evaluation dataset (development set used to avoid sending held-out test to API).",
            "metrics_reported": "Percentage of predictions that are valid SQL (VA), execution accuracy (EX) as %, and test-suite execution accuracy (TS) as % (reported per model and prompt, e.g. davinci-codex achieving up to 67% EX and 56.5% TS on Spider dev).",
            "human_involvement": "Minimal for bulk scoring (automatic execution); human involvement for targeted manual annotation of sample errors (see separate entry) but not for the bulk Spider dev set scoring reported.",
            "limitations_or_challenges": "Gold queries may use conventions (aliasing, selection of columns, argmax tie-breaking) different from model outputs, causing correct-but-different queries to be penalized by exact or naive execution comparison; possible memorization concerns because Spider data is publicly available (addressed in Appendix A.5); cannot evaluate held-out Spider test offline without sending examples to the API (risk of data leakage).",
            "llm_theory_example": null,
            "evaluation_results": "Codex (davinci-codex) with schema + small sample rows prompts achieved competitive performance on Spider dev (up to 67% execution accuracy, 56.5% test-suite accuracy); prompt format and inclusion of schema/database content strongly affect performance; many model 'errors' are due to stylistic or convention differences rather than semantic failures.",
            "uuid": "e3961.0",
            "source_info": {
                "paper_title": "Evaluating the Text-to-SQL Capabilities of Large Language Models",
                "publication_date_yy_mm": "2022-03"
            }
        },
        {
            "name_short": "Test-suite (Zhong et al.)",
            "name_full": "Test-suite execution accuracy (semantic evaluation for Text-to-SQL)",
            "brief_description": "A semantic equivalence evaluation methodology that runs a distilled set of test cases to determine whether generated SQL is semantically equivalent to the gold query, reducing false negatives from syntactic differences.",
            "citation_title": "Semantic evaluation for text-to-sql with distilled test suites",
            "mention_or_use": "use",
            "evaluation_criteria": "Semantic equivalence of generated SQL to gold as judged by behavior on a distilled set of execution tests (i.e., does generated query produce same outputs across targeted inputs?).",
            "evaluation_methods": "Run distilled test-suite cases against generated and gold queries; mark a generation correct if it matches gold behavior across the suite (automated).",
            "benchmark_or_dataset": "Applied on Spider (and Spider-Realistic) to provide a more semantically aware accuracy measure than raw execution/equality.",
            "metrics_reported": "Test-suite execution accuracy (TS) reported as percentage of examples where generated SQL passes the distilled test-suite (e.g., davinci-codex TS reported per prompt).",
            "human_involvement": "None required for the automatic test-suite; the paper additionally uses human annotation to estimate how many test-suite failures are actually acceptable (see manual annotation entry).",
            "limitations_or_challenges": "Distilled test suites must be well-designed to capture semantic equivalence; they may still miss valid alternative formulations or conversely, be too strict/lenient; constructing such suites requires care and resources.",
            "llm_theory_example": null,
            "evaluation_results": "Test-suite metric reduces penalization of semantically equivalent but syntactically different queries relative to naive execution/equality, but the paper still finds a substantial fraction of model outputs judged wrong by the test-suite are accepted by human annotators (~31% of valid-but-erroneous predictions were annotated as 'Ambiguous Correct').",
            "uuid": "e3961.1",
            "source_info": {
                "paper_title": "Evaluating the Text-to-SQL Capabilities of Large Language Models",
                "publication_date_yy_mm": "2022-03"
            }
        },
        {
            "name_short": "Automatic metrics (VA/EX/TS)",
            "name_full": "Automatic evaluation metrics: Valid SQL (VA), Execution accuracy (EX), Test-suite accuracy (TS)",
            "brief_description": "The paper's core quantitative evaluation signals: VA measures syntactic/execution validity, EX measures whether outputs produce correct results, and TS measures semantic equivalence via distilled test suites.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_criteria": "Syntactic validity (executable SQLite), exact correctness by execution result, and semantic equivalence via test-suite behavior.",
            "evaluation_methods": "For each generated SQL: (1) check whether it is valid/executable in SQLite (VA), (2) execute it and compare results to gold output for execution accuracy (EX), (3) run distilled test-suite cases and compare behaviors for test-suite accuracy (TS).",
            "benchmark_or_dataset": "Applied across Spider development set and Spider-Realistic; also used to report performance on GeoQuery and Scholar in few-shot experiments (test-suite accuracy reported for those datasets).",
            "metrics_reported": "VA, EX, and TS values reported as percentages per model and prompt configuration; also exact-match reported for finetuned T5 in some baselines (e.g., 71% exact-match for T5-3B on Spider validation in baseline training).",
            "human_involvement": "Automatic metrics require no humans; human review was performed on a random sample of 100 valid-but-erroneous predictions to better interpret automatic metric failures.",
            "limitations_or_challenges": "Automatic metrics can misclassify acceptable alternative queries as incorrect; VA is coarse (valid SQL may still be semantically wrong); EX can miss semantic equivalence when queries differ structurally; TS mitigates but does not eliminate these issues.",
            "llm_theory_example": null,
            "evaluation_results": "Reported VA/EX/TS numbers reveal strong sensitivity to prompt design (e.g., Create Table + Select 3 prompt increased EX substantially), diminishing returns from larger Codex size beyond context window benefits, and scenarios where adding more database rows to prompts harms EX/TS despite increasing VA.",
            "uuid": "e3961.2",
            "source_info": {
                "paper_title": "Evaluating the Text-to-SQL Capabilities of Large Language Models",
                "publication_date_yy_mm": "2022-03"
            }
        },
        {
            "name_short": "Human error analysis",
            "name_full": "Manual annotation and error analysis of model outputs",
            "brief_description": "Human annotators manually inspected a random sample of model outputs that were valid SQL but judged incorrect by automated evaluation, categorizing errors into semantic-incorrect and ambiguous-correct classes and subtypes.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_criteria": "Human judgement of whether a generated SQL is a correct/acceptable translation of the natural language question (semantic correctness and acceptability despite differences from gold).",
            "evaluation_methods": "Randomly sample 100 valid SQL predictions labeled incorrect by test-suite; a human annotator assigns categories (Test-Suite Correct, Semantic Incorrect with subtypes like Shortcuts, GROUP BY convention errors, Ambiguous Correct with subtypes like SELECT Extra Columns, Argmax, etc., and Invalid SQL subtypes).",
            "benchmark_or_dataset": "Applied to davinci-codex outputs on Spider development set with Create Table + Select 3 prompt; categories and percentages reported in a table (Table 3) and examples in Figures 1 and 3.",
            "metrics_reported": "Annotated breakdown percentages over sample: e.g., 55.1% Test-Suite Correct (of that sample?), 25.2% Semantic Incorrect (with 5.1% Shortcuts), 11.3% Ambiguous Correct (with 2.9% SELECT Extra Columns), 8.4% Invalid SQL; also E% column reporting distribution within manually annotated erroneous queries.",
            "human_involvement": "High — a human annotator performed detailed labeling and qualitative analysis for the sampled predictions; used to interpret automatic metric failures and guide future prompt/control strategies.",
            "limitations_or_challenges": "Small sample size (100 predictions) so annotations provide indicative rather than exhaustive statistics; single annotator or limited annotation process may introduce subjectivity; labor-intensive and not scalable to full test sets.",
            "llm_theory_example": null,
            "evaluation_results": "Human annotation revealed that about 31% of valid but automatically judged erroneous predictions were considered acceptable by a human (Ambiguous Correct), indicating that automatic metrics undercount acceptable outputs; identified common error modes (shortcuts, grouping conventions, selection differences) which inform prompt design and potential fixes.",
            "uuid": "e3961.3",
            "source_info": {
                "paper_title": "Evaluating the Text-to-SQL Capabilities of Large Language Models",
                "publication_date_yy_mm": "2022-03"
            }
        },
        {
            "name_short": "Few-shot adaptation evaluation",
            "name_full": "Few-shot prompt-based vs finetuning-based adaptation (GeoQuery & Scholar)",
            "brief_description": "Comparative evaluation of LLM few-shot prompting (Codex) against finetuning a smaller model (T5-3B) on small numbers of in-domain examples, measuring adaptation effectiveness on GeoQuery and Scholar.",
            "citation_title": "",
            "mention_or_use": "use",
            "evaluation_criteria": "Test-suite execution accuracy on target dataset (GeoQuery or Scholar) as the number of support examples (n-shot) varies; also compare zero-shot performance and full-dataset finetuned baselines.",
            "evaluation_methods": "For Codex: append n question-query examples in prompt (n-shot) and generate with greedy decoding; for baseline: finetune T5-3B on the same support examples and evaluate; vary n and plot test-suite accuracy.",
            "benchmark_or_dataset": "GeoQuery and Scholar repurposed into n-shot tasks by selecting one example per most frequent templates; GeoQuery allows up to 40-shot in davinci-codex due to context limit; Scholar has shorter prompt capacity.",
            "metrics_reported": "Test-suite accuracy (TS) reported vs number of support examples; baseline full-training exact/test accuracy for comparison (e.g., T5-3B baseline reaches 85.7% on GeoQuery when trained on full set).",
            "human_involvement": "None described for bulk few-shot evaluation; humans involved earlier for error analysis but not in routine few-shot metric computation.",
            "limitations_or_challenges": "API context window limits how many support examples can be included (varies by model), affecting few-shot performance; dataset/query length limits fit; zero-shot performance varies by dataset conventions (argmax convention differences), so transfer depends on shared conventions; finetuning may be more sample-efficient for some datasets (e.g., T5 outperforms Codex zero-shot on Scholar).",
            "llm_theory_example": null,
            "evaluation_results": "Codex (davinci-codex) outperforms a T5-3B finetuned on the same small support sets in many few-shot settings (especially GeoQuery up to 40-shot), showing strong adaptation via prompting; however T5 can outperform Codex in some zero-shot settings (Scholar), and Codex performance depends on prompt size and dataset conventions.",
            "uuid": "e3961.4",
            "source_info": {
                "paper_title": "Evaluating the Text-to-SQL Capabilities of Large Language Models",
                "publication_date_yy_mm": "2022-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Semantic evaluation for text-to-sql with distilled test suites",
            "rating": 2
        },
        {
            "paper_title": "Spider: A largescale human-labeled dataset for complex and crossdomain semantic parsing and text-to-sql task",
            "rating": 2
        },
        {
            "paper_title": "Improving text-to-SQL evaluation methodology",
            "rating": 2
        },
        {
            "paper_title": "Structure-grounded pretraining for text-to-sql",
            "rating": 1
        },
        {
            "paper_title": "Exploring unexplored generalization challenges for cross-database semantic parsing",
            "rating": 1
        }
    ],
    "cost": 0.012034749999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Evaluating the Text-to-SQL Capabilities of Large Language Models</h1>
<p>Nitarshan Rajkumar ${ }^{1 *}$, Raymond $\mathrm{Li}^{2}$, Dzmitry Bahdanau ${ }^{2345}$<br>${ }^{1}$ University of Cambridge, ${ }^{2}$ ServiceNow, ${ }^{3}$ Mila, ${ }^{4}$ McGill University, ${ }^{5}$ Canada CIFAR AI Chair<br>nr500@cam.ac.uk, {raymond.li,dzmitry.bahdanau}@servicenow.com<br>https://github.com/nitarshan/codex-text2sql</p>
<h4>Abstract</h4>
<p>We perform an empirical evaluation of Text-toSQL capabilities of the Codex language model. We find that, without any finetuning, Codex is a strong baseline on the Spider benchmark; we also analyze the failure modes of Codex in this setting. Furthermore, we demonstrate on the GeoQuery and Scholar benchmarks that a small number of in-domain examples provided in the prompt enables Codex to perform better than state-of-the-art models finetuned on such few-shot examples.</p>
<h2>1 Introduction</h2>
<p>Translating natural language questions to SQL queries (Text-to-SQL) is an important business problem which has seen significant research interest. A common approach to this task involves training a model to produce a SQL query when given a question, a database schema, and possibly database content as inputs. A clear trend in this area is to finetune models pretrained on natural language; notably, performance significantly improves as larger pretrained models are used (Shaw et al., 2021; Scholak et al., 2021).</p>
<p>Recent results from the broader field demonstrate that simply scaling training data and model size for generative language models brings advanced capabilities, such as few-shot learning without finetuning (GPT-3, Brown et al., 2020) and code generation (Codex, Chen et al., 2021). In this work we study if such models are already competitive Text-to-SQL solutions without any further finetuning on task-specific training data, evaluating Codex and GPT-3 models of different sizes with varied prompts on Text-to-SQL benchmarks.</p>
<p>We find that Codex achieves a competitive performance of up to $67 \%$ execution accuracy on the Spider development set. We analyze the predicted queries that automatic evaluation judged as wrong</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>| Model | VA | EX | TS |
| :-- | :--: | :--: | :--: |
| Finetuned |  |  |  |
| T5-base | 72.7 | 57.9 | 54.5 |
| T5-large | 84.1 | 67.2 | 61.4 |
| T5-3B | 87.6 | 71.4 | 65.7 |
| T5-3B ${ }^{<em>}$ | 88.2 | 74.4 | 68.3 |
| T5-3B + PICARD ${ }^{</em>}$ | 97.8 | 79.1 | 71.7 |
| BRIDGE v2 ${ }^{<em>}$ | - | 68.0 | - |
| Inference-only |  |  |  |
| GPT-3 ada | 33.8 | 2.3 | 0.3 |
| GPT-3 babbage | 48.8 | 5.7 | 3.9 |
| GPT-3 curie | 70.9 | 12.6 | 8.3 |
| GPT-3 davinci | 65.0 | 26.3 | 21.7 |
| Codex cushman ${ }^{</em>}$ | 86.3 | 63.7 | 53.0 |
| Codex davinci ${ }^{*}$ | 91.6 | 67.0 | 55.1 |</p>
<p>Table 1: Best Spider development set performance across models, as measured by percentage of predictions which are valid SQL (VA), execution accuracy (EX), test-suite accuracy (TS). Models marked with * use database content. T5 results are from Scholak et al. (2021), BRIDGE v2 results are from Lin et al. (2020).
and find that many of them would be judged correct by humans, whereas others could likely be fixed within the no-finetuning paradigm. Lastly, using GeoQuery and Scholar benchmarks we show that adapting Codex to a specific domain by prompting it with few examples can be more effective than fine-tuning a smaller language model on the same examples.</p>
<h2>2 Experimental Setup</h2>
<p>Models Our evaluation focuses on the models accessible via the OpenAI API: GPT-3 (in the ascending ada, babbage, curie and davinci sizes) and Codex (in the ascending cushman-codex and davinci-codex sizes) ${ }^{1}$. These are generative language models which perform next-token prediction during training and inference; GPT-3 is trained on a diverse set of sources from the internet, and Codex is further finetuned on code from GitHub. We compare GPT-3 and Codex against methods from Shaw et al. (2021) using the T5 encoder-decoder</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<table>
<thead>
<tr>
<th>Prompt</th>
<th>VA</th>
<th>EX</th>
<th>TS</th>
</tr>
</thead>
<tbody>
<tr>
<td>Question</td>
<td>14.0</td>
<td>8.3</td>
<td>8.2</td>
</tr>
<tr>
<td>API Docs</td>
<td>83.8</td>
<td>56.8</td>
<td>47.5</td>
</tr>
<tr>
<td>Select 1</td>
<td>86.3</td>
<td>60.9</td>
<td>52.0</td>
</tr>
<tr>
<td>Select 3</td>
<td>85.8</td>
<td>60.3</td>
<td>52.2</td>
</tr>
<tr>
<td>Select 5</td>
<td>85.2</td>
<td>60.5</td>
<td>51.5</td>
</tr>
<tr>
<td>Select 10</td>
<td>86.0</td>
<td>60.8</td>
<td>51.2</td>
</tr>
<tr>
<td>Create Table</td>
<td>89.8</td>
<td>59.9</td>
<td>50.0</td>
</tr>
<tr>
<td>+ Select 1</td>
<td>92.5</td>
<td>64.8</td>
<td>53.7</td>
</tr>
<tr>
<td>+ Select 3</td>
<td>91.6</td>
<td>67.0</td>
<td>55.1</td>
</tr>
<tr>
<td>+ Select 5</td>
<td>91.0</td>
<td>65.3</td>
<td>53.9</td>
</tr>
<tr>
<td>+ Select 10</td>
<td>91.2</td>
<td>63.3</td>
<td>52.4</td>
</tr>
</tbody>
</table>
<p>model. Starting from public checkpoints pretrained on Common Crawl, the T5 model is finetuned on Spider to predict the output SQL, conditioned on the question and schema. The 3B parameter T5 model is currently the state-of-the-art on Spider when combined with constrained inference using the PICARD algorithm <em>Scholak et al. (2021)</em>. We also compare to BRIDGE v2 <em>Lin et al. (2020)</em>, a sequence-to-sequence model based on BERT.</p>
<p>Zero-Shot Experiments We use the Spider benchmark <em>Yu et al. (2019)</em> for cross-domain Text-to-SQL. We report performance using percentage of development set predictions which are valid (executable) SQLite SQL, execution accuracy, and test-suite execution accuracy. The latter metric was proposed by <em>Zhong et al. (2020)</em> to measure semantic equivalence of SQL queries written in different styles, which is essential when comparing Codex to models trained on Spider. We address concerns around possible memorization of Spider data by Codex in Appendix A.5.</p>
<p>Few-Shot Experiments We re-purpose the question-splits of the GeoQuery and Scholar datasets <em>Zelle and Mooney (1996); Iyer et al. (2017); Finegan-Dollak et al. (2018)</em> to perform experiments in a few-shot setting. The examples in these datasets are grouped by query templates. Examples corresponding to the same template have the same SQL query structure, but may have different English questions and SQL literals. To define the few-shot task, we first sort the templates by their frequency in the training set. In the $n$-shot setting we then use one random example for each of the $n$ most frequent templates.</p>
<p>Prompts We use six prompt structures in our experiments (examples provided in Appendix C). Question provides no database information and just includes the question as a SQL comment. API Docs follows the style of the Text-to-SQL example in Codex documentation and includes a schema in a comment style which does not conform to SQLite standards. Select X includes in comments the results of executing a SELECT * FROM T LIMIT X query on each table, including schemas via column headers. Create Table includes the CREATE TABLE commands for each table, including column type and foreign key declarations. Create Table + Select $\mathbf{X}^{2}$ is a combination of the preceding two prompt formats. Finally, Fewshot additionally includes question-query pairs.</p>
<h2>3 Zero-Shot Results</h2>
<p>We present results for different model sizes in Table 1 and for different prompt styles in Table 2. Full results are available in Table 4 in Appendix B.</p>
<p>Codex provides a strong baseline for Text-toSQL tasks In Table 1 the best performing model (davinci-codex, Create Table + Select 3) achieves 67% execution accuracy and 56.5% test suite execution accuracy on Spider. This is comparable to the performance of the BRIDGE v2 <em>Lin et al. (2020)</em> model which achieved a (then) state-of-the-art 68% execution accuracy in December 2020.</p>
<p>Prompt design is critical for performance As seen in Table 2, providing the question alone results in a low 8.3% execution accuracy. There is a progressive improvement to 56.8% as schema information is introduced in API Docs, to 59.9% when valid SQL and foreign key information is used in Create Table, and to 67.0% when database content is introduced with Create Table + Select 3.</p>
<p>More database content can harm performance In Table 2 we observe that for the Select Limit X prompts there is a negligible change in performance when adding more rows. By contrast, Create Table + Select Limit X prompt accuracy peaks with 3 rows before significantly decreasing in performance as more rows are added.</p>
<p>Diminishing returns for Codex model size While GPT-3 performance significantly benefits from increased model size, the davinci-codex model does not perform drastically better than cess whitespace tokens less efficiently than Codex models, and therefore cannot evaluate Create Table + Select X prompts at all.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">14\%</th>
<th style="text-align: center;">Shortcuts</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">NL</td>
<td style="text-align: center;">What is the number of car models created by the car maker American Motor Company?</td>
</tr>
<tr>
<td style="text-align: center;">Gold</td>
<td style="text-align: center;">SELECT count (*) FROM CAR_MAKERS AS T1 JOIN MODEL_LIST AS T2 ON T1.Id = T2.Maker WHERE T1.FullName = 'American Motor Company';</td>
</tr>
<tr>
<td style="text-align: center;">Pred</td>
<td style="text-align: center;">SELECT COUNT (Model) FROM model_list WHERE Maker = 1;</td>
</tr>
<tr>
<td style="text-align: center;">NL</td>
<td style="text-align: center;">Give the city and country for the Alton airport.</td>
</tr>
<tr>
<td style="text-align: center;">Gold</td>
<td style="text-align: center;">SELECT City, Country FROM AIRPORTS WHERE AirportName = "Alton"</td>
</tr>
<tr>
<td style="text-align: center;">Pred</td>
<td style="text-align: center;">SELECT City, Country FROM airports WHERE AirportCode = 'ALN';</td>
</tr>
<tr>
<td style="text-align: center;">8\%</td>
<td style="text-align: center;">SELECT Extra Columns</td>
</tr>
<tr>
<td style="text-align: center;">NL</td>
<td style="text-align: center;">List names of conductors in descending order of years of work.</td>
</tr>
<tr>
<td style="text-align: center;">Gold</td>
<td style="text-align: center;">SELECT Name FROM conductor ORDER BY Year_of_Work DESC</td>
</tr>
<tr>
<td style="text-align: center;">Pred</td>
<td style="text-align: center;">SELECT Name, Year_of_Work FROM conductor ORDER BY Year_of_Work DESC;</td>
</tr>
<tr>
<td style="text-align: center;">5\%</td>
<td style="text-align: center;">SELECT Convention</td>
</tr>
<tr>
<td style="text-align: center;">NL</td>
<td style="text-align: center;">What are all the makers and models?</td>
</tr>
<tr>
<td style="text-align: center;">Gold</td>
<td style="text-align: center;">SELECT Maker , Model FROM MODEL_LIST;</td>
</tr>
<tr>
<td style="text-align: center;">Pred</td>
<td style="text-align: center;">SELECT DISTINCT car_makers.Maker, model_list.Model FROM car_makers JOIN model_list ON car_makers.Id = model_list.Maker;</td>
</tr>
</tbody>
</table>
<p>Figure 1: Examples of error types, as made by the davinci-codex model with Create Table + Select 3 prompt. NL stands for natural language question. Percentage indicates the percentage of errors which are of the given type. Further examples are provided in Figure 3 in Appendix B.
cushman-codex. Full results in Table 4 in Appendix B show cushman-codex generally being within 1 percentage point of davinci-codex for the same prompt style; it even performs 3 percentage points better for the Create Table prompt. These results suggest that davinci-codex's longer context window may be a greater contributor to its peak performance than increased parameter count.</p>
<h3>3.1 Error Analysis</h3>
<p>We focus our error analysis on the davinci-codex model with Create Table + Select 3 prompt, and present a breakdown of prediction types in Table 3 and examples of errors in Figure 1. Our error categories were chosen to surface the most interesting Codex-specific behaviours we observed amongst the errors made. We randomly selected and annotated 100 predictions which were valid SQL yet were judged incorrect by test-suite evaluation.</p>
<p>We first consider Semantic Incorrect behaviours, which Spider evaluation and the human annotator both view as incorrect predictions. Shortcut errors are where Codex made use of either specific table values or "world knowledge" from GPT-3 pretraining, while the ground-truth query contained the exact literals from the question. GROUP BY Convention errors are where Codex incorrectly groups on a non-primary-key column (such as a name or title column).</p>
<p>We also consider Ambiguous Correct behaviours which are semantically different from the gold query and are therefore judged as incorrect by Spider evaluation, but which the human annotator viewed as being an acceptable SQL translation of</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Annotation</th>
<th style="text-align: left;">\%</th>
<th style="text-align: left;">E\%</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Test-Suite Correct</td>
<td style="text-align: left;">55.1</td>
<td style="text-align: left;">-</td>
</tr>
<tr>
<td style="text-align: left;">Semantic Incorrect</td>
<td style="text-align: left;">25.2</td>
<td style="text-align: left;">69</td>
</tr>
<tr>
<td style="text-align: left;">- Shortcuts</td>
<td style="text-align: left;">5.1</td>
<td style="text-align: left;">14</td>
</tr>
<tr>
<td style="text-align: left;">- GROUP BY Convention</td>
<td style="text-align: left;">1.5</td>
<td style="text-align: left;">4</td>
</tr>
<tr>
<td style="text-align: left;">- Other</td>
<td style="text-align: left;">18.6</td>
<td style="text-align: left;">51</td>
</tr>
<tr>
<td style="text-align: left;">Ambiguous Correct</td>
<td style="text-align: left;">11.3</td>
<td style="text-align: left;">31</td>
</tr>
<tr>
<td style="text-align: left;">- SELECT Extra Columns</td>
<td style="text-align: left;">2.9</td>
<td style="text-align: left;">8</td>
</tr>
<tr>
<td style="text-align: left;">- SELECT Convention</td>
<td style="text-align: left;">1.8</td>
<td style="text-align: left;">5</td>
</tr>
<tr>
<td style="text-align: left;">- Argmax</td>
<td style="text-align: left;">1.5</td>
<td style="text-align: left;">4</td>
</tr>
<tr>
<td style="text-align: left;">- Other</td>
<td style="text-align: left;">5.1</td>
<td style="text-align: left;">14</td>
</tr>
<tr>
<td style="text-align: left;">Invalid SQL</td>
<td style="text-align: left;">8.4</td>
<td style="text-align: left;">-</td>
</tr>
<tr>
<td style="text-align: left;">- Ambiguous column name</td>
<td style="text-align: left;">1.9</td>
<td style="text-align: left;">-</td>
</tr>
<tr>
<td style="text-align: left;">- No such column</td>
<td style="text-align: left;">4.5</td>
<td style="text-align: left;">-</td>
</tr>
</tbody>
</table>
<p>Table 3: Breakdown of prediction annotations over Spider development set for the davinci-codex model with Create Table + Select 3 prompt. \% is percentage of all predictions, E\% is percentage of manually annotated erroneous queries (see Section Section 3.1 for details).
the given question. SELECT Convention errors are where Codex selects a different column than the per-database convention of the gold queries (such as name instead of ID). SELECT Extra Columns errors are where Codex includes additional useful columns in its query beyond what the gold query includes. Argmax errors are where Codex differs from the gold query in how a min/max resolution (such as "youngest singer") is handled for ties.</p>
<p>We observe in Table 3 that a significant $31 \%$ of valid yet erroneous predictions are penalized by Spider evaluation as being incorrect though a human annotator viewed them as acceptable solutions. Future work could be to investigate to what extent one can control the behaviour of Codex. This could allow to fix these ambiguous errors, either by prompt design or using a few examples.</p>
<p>4 Few-Shot</p>
<p>We investigate whether Codex can perform few-shot Text-to-SQL. As described in Section 2, we re-purpose the GeoQuery and Sholar datasets in a few-shot setting. It is well known that models trained on Spider transfer poorly to other singledatabase Text-to-SQL datasets <em>Suhr et al. (2020)</em> in a zero-shot setting. Studying few-shot Text-to-SQL on GeoQuery and Scholar should show to what extent models are able to leverage a small amount of examples to effectively adapt to a new domain.</p>
<p>Baseline The baseline is a T5-3B model that was finetuned on Spider, reaching 71% exact-match accuracy on Spider validation set. The model is then further finetuned on the new domain – GeoQuery or Scholar. The learning rate for domain-specific-finetuning was selected in the 20-shot setting among $[0.1,0.2,0.5,1,2]\cdot 10^{-5}$, based on the best validation set performance after 300 steps. We use batch-size 1024, such that all the few-shot examples fit in the same batch.</p>
<p>Codex Building on the Create Table + Select X prompt, we append $n$ question-query examples to the input in an $n$-shot setting. An example of this prompt is provided in Figure 11. All samples are generated using greedy decoding, with temperature 0. Note that for a given $n$-shot setting, the baseline and Codex use the same set of support examples. These examples are in the prompt for Codex, and used to finetune the baseline on the new domain. Given the limited window-size of API models, on GeoQuery we can feed up to 40 support examples to davinci-codex, and up to 10 examples to cushman-codex and GPT-3 models. On Scholar the queries are longer and the schema more complex – we fit only 10 examples in the prompt of davinci-codex, 5 for cushman-codex, and none at all for GPT-3 models.</p>
<h3>4.1 Results</h3>
<p>Figure 2 shows test-suite accuracies on the Scholar and GeoQuery datasets. The baseline reaches 85.7% test-set performance when trained on the complete GeoQuery training set (549 examples). Respectively, it reaches 87.2% test accuracy when trained on the whole Scholar training set (499 examples). This simple baseline is a very competitive model when considering the entire datasets. However Figure 2 shows that it is largely beaten by Codex in few-shot settings. In a zero-shot setting, both davinci-codex and cushman-codex al-</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>(a) GeoQuery. When trained on the whole GeoQuery training set (549 examples), the finetuned T5 reaches 85.7% accuracy.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>(b) Scholar. When trained on the whole Scholar training set (499 examples), the finetuned T5 reaches 87.2% accuracy.</p>
<p>Figure 2: Test-suite accuracy with varying number of support examples. The x-axis shows the number of few-shot examples used.</p>
<p>ready beat the baseline on GeoQuery. We speculate that Codex performs well here because it uses the same argmax convention as the GeoQuery dataset, which is different than the convention used in Spider. With up to 40 examples in the prompt, davinci-codex outperforms a T5-3B model finetuned on these same examples by a large margin, whereas GPT-3 davinci performs quite poorly on this task. On the other hand, the T5 model outperforms Codex in a zero-shot setting on Scholar. In 5 and 10-shot settings, Codex shows better adaptation from these few samples and beats the T5 baseline.</p>
<h2>5 Conclusion</h2>
<p>We demonstrated that generative language models trained on code provide a strong baseline for Text-to-SQL. We also provided analysis of failure modes for these models, which we hope guides further prompt design (whether few-shot or through natural language instructions) in this setting. Finally, we showed that prompt-based few-shot learning with these models performs competitively with finetuning-based few-shot learning of smaller models. A clear direction for future work is to evaluate the benefits of finetuning with Codex models.</p>
<h2>Acknowledgements</h2>
<p>Nitarshan performed all zero-shot and finetuning experiments as well as error-analysis, and wrote most of the paper. Raymond performed all few-shot experiments and the associated writing. Dzmitry supervised, and contributed to paper editing.</p>
<p>We thank Dóra Jámbor for insightful discussions, Laurent Charlin for providing funding for Nitarshan and for providing feedback on this work, Fraser Kelton and Dave Cummings for support with the OpenAI API, and Ruiqi Zhong for assistance with Spider test suites. We also thank anonymous ARR reviewers for their feedback and criticism in the review process.</p>
<p>Nitarshan additionally thanks the city of Montréal and its cafés for providing inspirational settings in which to conduct this work.</p>
<h2>References</h2>
<p>Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners.</p>
<p>Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. 2021. Evaluating large language models trained on code.</p>
<p>Xiang Deng, Ahmed Hassan Awadallah, Christopher Meek, Oleksandr Polozov, Huan Sun, and Matthew Richardson. 2021. Structure-grounded pretraining for text-to-sql. Proceedings of the 2021 Conference of the North American Chapter of the Association
for Computational Linguistics: Human Language Technologies.</p>
<p>Catherine Finegan-Dollak, Jonathan K. Kummerfeld, Li Zhang, Karthik Ramanathan, Sesh Sadasivam, Rui Zhang, and Dragomir Radev. 2018. Improving text-to-SQL evaluation methodology. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 351-360, Melbourne, Australia. Association for Computational Linguistics.</p>
<p>Leo Gao. 2021. On the Sizes of OpenAI API Models.
Srinivasan Iyer, Ioannis Konstas, Alvin Cheung, Jayant Krishnamurthy, and Luke Zettlemoyer. 2017. Learning a neural semantic parser from user feedback. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 963-973, Vancouver, Canada. Association for Computational Linguistics.</p>
<p>Xi Victoria Lin, Richard Socher, and Caiming Xiong. 2020. Bridging textual and tabular data for crossdomain text-to-sql semantic parsing.</p>
<p>Torsten Scholak, Nathan Schucher, and Dzmitry Bahdanau. 2021. Picard: Parsing incrementally for constrained auto-regressive decoding from language models.</p>
<p>Peter Shaw, Ming-Wei Chang, Panupong Pasupat, and Kristina Toutanova. 2021. Compositional generalization and natural language variation: Can a semantic parsing approach handle both? In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 922-938, Online. Association for Computational Linguistics.</p>
<p>Alane Suhr, Ming-Wei Chang, Peter Shaw, and Kenton Lee. 2020. Exploring unexplored generalization challenges for cross-database semantic parsing. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 83728388, Online. Association for Computational Linguistics.</p>
<p>Tao Yu, Rui Zhang, Kai Yang, Michihiro Yasunaga, Dongxu Wang, Zifan Li, James Ma, Irene Li, Qingning Yao, Shanelle Roman, Zilin Zhang, and Dragomir Radev. 2019. Spider: A largescale human-labeled dataset for complex and crossdomain semantic parsing and text-to-sql task.</p>
<p>John M. Zelle and Raymond J. Mooney. 1996. Learning to parse database queries using inductive logic programming. In Proceedings of the Thirteenth National Conference on Artificial Intelligence - Volume 2, pages 1050-1055.</p>
<p>Ruiqi Zhong, Tao Yu, and Dan Klein. 2020. Semantic evaluation for text-to-sql with distilled test suites.</p>
<p>Albert Ziegler. 2021. Research recitation.</p>
<h2>A API Details</h2>
<p>At time of writing, the OpenAI API was accessible at https://openai.com/api/. The example from which our API Docs prompt draws from can be found at https://beta.openai.com/examples/ default-sql-translate.</p>
<h3>A.1 Hyperparameters</h3>
<p>We sample 200 tokens from GPT-3 and Codex with temperature 0, with the following strings used as stop tokens to halt generation: "--", "\n\n", ":", "#".</p>
<h2>A. 2 Parameter Counts</h2>
<p>Parameter counts for OpenAI API models are not openly available. Gao (2021) evaluated API GPT3 models across a variety of language modelling tasks to compare to published results in Brown et al. (2020), finding that "Ada, Babbage, Curie and Davinci line up closely with 350M, 1.3B, 6.7B, and 175B respectively". We presume that the davincicodex model is the same size as the GPT-3 davinci model; cushman-codex is a new model name so we can only guess that it is of a similar (but not the same) size to GPT-3 curie. Nevertheless these remain guesses which should not be relied on.</p>
<h2>A. 3 Model Versioning</h2>
<p>The exact models served through the OpenAI API may vary over time. We verified that for each model type, only a single model version was used to generate results. These versions are ada:2020-05-03, babbage:2020-05-03, curie:2020-05-03, davinci:2020-05-03, cushman-codex:2021-08-03, davinci-codex:2021-08-03.</p>
<h2>A. 4 Finetuning</h2>
<p>In Table 4 we include preliminary results from finetuning GPT-3 models on the Spider training set. We used the full training set, and the default finetuning settings of 4 epochs, a batch size of 8, and a learning rate multiplier of 0.1 . We did not perform a hyperparameter sweep due to the significant cost this would incur.</p>
<h2>A. 5 Memorization</h2>
<p>The Spider development set is available on GitHub, and is therefore possibly in the training set of Codex. We believe that this does not manifest as memorization for our results however, for the following reasons.</p>
<p>Evaluation data on Spider's repo is formatted differently to our prompts. Most related is the dev.sql file, which contains evaluation questionquery pairs in the following format:</p>
<div class="codehilite"><pre><span></span><code>Question 1: ...
SQL: ...
...
</code></pre></div>

<p>This resembles but isn't identical to our "Question" prompt. We prompted Codex with verbatim fragments of this file and generations failed to replicate any file contents. Our "Question" prompt has very poor performance - hardly an indication of memorization from dev.sql. Furthermore, most of Codex's performance is due to including in the prompt the schemas (see Table 2), which are not present in dev.sql.</p>
<p>As well, Codex prediction style is very different to evaluation gold queries. Gold queries make use of a consistent table aliasing strategy (using T1, T2, etc.) which we never see with Codex (see Figure 3 for example comparisons).</p>
<p>Furthermore, in Table 4 we reported performance for all models on spider-realistic (Deng et al., 2021), a modification of the spider evaluation set that removes column name references in questions. We observe a similar trend in performance across models as on spider (the consistent performance drop on spider-realistic is expected due to the difficulty of the updated dataset). Memorization cannot account for the performance observed, as spider-realistic is not publicly available on GitHub.</p>
<p>Finally, Ziegler (2021) studied memorization in Copilot, a derivative of the Codex models, and found that "Copilot can quote a body of code verbatim, but that it rarely does so, and when it does, it mostly quotes code that everybody quotes, and mostly at the beginning of a file". Spider evaluation data is rare on GitHub, and we use long contexts in our prompts that significantly differ from the files on GitHub.</p>
<h2>A. 6 Choice of Spider Evaluation Set</h2>
<p>We chose not to evaluate on the held-out test set of Spider, as this could not be done offline - it would instead require sending these held-out examples through the API to OpenAI, which risks inadvertently leaking them for retraining of Codex.</p>
<h1>B Additional Tables and Figures</h1>
<table>
<thead>
<tr>
<th style="text-align: center;">Engine</th>
<th style="text-align: center;">Prompt</th>
<th style="text-align: center;">VA</th>
<th style="text-align: center;">EX</th>
<th style="text-align: center;">TS</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">GPT-3</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">ada</td>
<td style="text-align: center;">Question</td>
<td style="text-align: center;">$1.2(1.0)$</td>
<td style="text-align: center;">$0.0(0.0)$</td>
<td style="text-align: center;">$0.0(0.0)$</td>
</tr>
<tr>
<td style="text-align: center;">ada</td>
<td style="text-align: center;">Docs</td>
<td style="text-align: center;">$3.4(2.2)$</td>
<td style="text-align: center;">$0.2(0.2)$</td>
<td style="text-align: center;">$0.1(0.0)$</td>
</tr>
<tr>
<td style="text-align: center;">ada</td>
<td style="text-align: center;">1 Row</td>
<td style="text-align: center;">40.1 (34.6)</td>
<td style="text-align: center;">$1.1(0.6)$</td>
<td style="text-align: center;">$0.2(0.0)$</td>
</tr>
<tr>
<td style="text-align: center;">ada</td>
<td style="text-align: center;">Schema</td>
<td style="text-align: center;">33.8 (33.9)</td>
<td style="text-align: center;">2.3 (3.5)</td>
<td style="text-align: center;">$0.3(0.0)$</td>
</tr>
<tr>
<td style="text-align: center;">babbage</td>
<td style="text-align: center;">Question</td>
<td style="text-align: center;">$4.4(2.0)$</td>
<td style="text-align: center;">$1.0(0.2)$</td>
<td style="text-align: center;">$1.0(0.2)$</td>
</tr>
<tr>
<td style="text-align: center;">babbage</td>
<td style="text-align: center;">Docs</td>
<td style="text-align: center;">22.5 (20.3)</td>
<td style="text-align: center;">$1.0(0.6)$</td>
<td style="text-align: center;">$0.7(0.2)$</td>
</tr>
<tr>
<td style="text-align: center;">babbage</td>
<td style="text-align: center;">1 Row</td>
<td style="text-align: center;">56.0 (49.8)</td>
<td style="text-align: center;">$5.1(1.6)$</td>
<td style="text-align: center;">$3.9(0.0)$</td>
</tr>
<tr>
<td style="text-align: center;">babbage</td>
<td style="text-align: center;">Schema</td>
<td style="text-align: center;">48.8 (44.9)</td>
<td style="text-align: center;">$5.7(0.8)$</td>
<td style="text-align: center;">$3.9(0.0)$</td>
</tr>
<tr>
<td style="text-align: center;">curie</td>
<td style="text-align: center;">Question</td>
<td style="text-align: center;">$9.0(6.7)$</td>
<td style="text-align: center;">$2.9(2.4)$</td>
<td style="text-align: center;">$2.5(1.8)$</td>
</tr>
<tr>
<td style="text-align: center;">curie</td>
<td style="text-align: center;">Docs</td>
<td style="text-align: center;">25.2 (25.0)</td>
<td style="text-align: center;">$7.4(5.5)$</td>
<td style="text-align: center;">$6.3(3.3)$</td>
</tr>
<tr>
<td style="text-align: center;">curie</td>
<td style="text-align: center;">1 Row</td>
<td style="text-align: center;">70.6 (67.3)</td>
<td style="text-align: center;">$10.8(7.3)$</td>
<td style="text-align: center;">$7.6(1.4)$</td>
</tr>
<tr>
<td style="text-align: center;">curie</td>
<td style="text-align: center;">Schema</td>
<td style="text-align: center;">70.9 (72.2)</td>
<td style="text-align: center;">12.6 (11.0)</td>
<td style="text-align: center;">$8.3(4.1)$</td>
</tr>
<tr>
<td style="text-align: center;">davinci</td>
<td style="text-align: center;">Schema</td>
<td style="text-align: center;">65.0 (65.4)</td>
<td style="text-align: center;">26.3 (23.2)</td>
<td style="text-align: center;">21.7 (14.2)</td>
</tr>
<tr>
<td style="text-align: center;">Finetuned GPT-3</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">ada</td>
<td style="text-align: center;">Schema</td>
<td style="text-align: center;">27.5 (21.3)</td>
<td style="text-align: center;">20.2 (14.0)</td>
<td style="text-align: center;">19.1 (13.0)</td>
</tr>
<tr>
<td style="text-align: center;">babbage</td>
<td style="text-align: center;">Schema</td>
<td style="text-align: center;">47.2 (38.0)</td>
<td style="text-align: center;">34.8 (23.6)</td>
<td style="text-align: center;">31.9 (20.9)</td>
</tr>
<tr>
<td style="text-align: center;">curie</td>
<td style="text-align: center;">Schema</td>
<td style="text-align: center;">66.9 (60.2)</td>
<td style="text-align: center;">51.3 (37.8)</td>
<td style="text-align: center;">46.9 (32.9)</td>
</tr>
<tr>
<td style="text-align: center;">Codex</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">cushman</td>
<td style="text-align: center;">Question</td>
<td style="text-align: center;">11.3 (8.1)</td>
<td style="text-align: center;">8.5 (3.9)</td>
<td style="text-align: center;">8.3 (3.9)</td>
</tr>
<tr>
<td style="text-align: center;">cushman</td>
<td style="text-align: center;">Docs</td>
<td style="text-align: center;">83.8 (80.5)</td>
<td style="text-align: center;">53.2 (45.1)</td>
<td style="text-align: center;">43.5 (32.3)</td>
</tr>
<tr>
<td style="text-align: center;">cushman</td>
<td style="text-align: center;">1 Row</td>
<td style="text-align: center;">84.7 (80.9)</td>
<td style="text-align: center;">59.6 (49.2)</td>
<td style="text-align: center;">48.5 (32.5)</td>
</tr>
<tr>
<td style="text-align: center;">cushman</td>
<td style="text-align: center;">3 Rows</td>
<td style="text-align: center;">82.9 (79.1)</td>
<td style="text-align: center;">60.3 (49.2)</td>
<td style="text-align: center;">49.4 (33.7)</td>
</tr>
<tr>
<td style="text-align: center;">cushman</td>
<td style="text-align: center;">5 Rows</td>
<td style="text-align: center;">83.6 (78.3)</td>
<td style="text-align: center;">61.5 (49.6)</td>
<td style="text-align: center;">50.4 (33.9)</td>
</tr>
<tr>
<td style="text-align: center;">cushman</td>
<td style="text-align: center;">Schema</td>
<td style="text-align: center;">88.3 (83.1)</td>
<td style="text-align: center;">62.1 (49.6)</td>
<td style="text-align: center;">53.1 (36.2)</td>
</tr>
<tr>
<td style="text-align: center;">cushman</td>
<td style="text-align: center;">+1 Row</td>
<td style="text-align: center;">86.3 (85.0)</td>
<td style="text-align: center;">63.7 (54.9)</td>
<td style="text-align: center;">53.0 (39.6)</td>
</tr>
<tr>
<td style="text-align: center;">davinci</td>
<td style="text-align: center;">Question</td>
<td style="text-align: center;">14.0 (8.9)</td>
<td style="text-align: center;">8.3 (4.5)</td>
<td style="text-align: center;">8.2 (4.1)</td>
</tr>
<tr>
<td style="text-align: center;">davinci</td>
<td style="text-align: center;">Docs</td>
<td style="text-align: center;">83.8 (87.4)</td>
<td style="text-align: center;">56.8 (51.8)</td>
<td style="text-align: center;">47.5 (39.0)</td>
</tr>
<tr>
<td style="text-align: center;">davinci</td>
<td style="text-align: center;">1 Row</td>
<td style="text-align: center;">86.3 (83.5)</td>
<td style="text-align: center;">60.9 (54.7)</td>
<td style="text-align: center;">52.0 (41.3)</td>
</tr>
<tr>
<td style="text-align: center;">davinci</td>
<td style="text-align: center;">3 Rows</td>
<td style="text-align: center;">85.8 (82.7)</td>
<td style="text-align: center;">60.3 (53.3)</td>
<td style="text-align: center;">52.2 (40.0)</td>
</tr>
<tr>
<td style="text-align: center;">davinci</td>
<td style="text-align: center;">5 Rows</td>
<td style="text-align: center;">85.2 (80.9)</td>
<td style="text-align: center;">60.5 (51.4)</td>
<td style="text-align: center;">51.5 (38.4)</td>
</tr>
<tr>
<td style="text-align: center;">davinci</td>
<td style="text-align: center;">10 Rows</td>
<td style="text-align: center;">86.0 (80.7)</td>
<td style="text-align: center;">60.8 (53.3)</td>
<td style="text-align: center;">51.2 (39.2)</td>
</tr>
<tr>
<td style="text-align: center;">davinci</td>
<td style="text-align: center;">Schema</td>
<td style="text-align: center;">89.8 (87.8)</td>
<td style="text-align: center;">59.9 (52.2)</td>
<td style="text-align: center;">50.0 (38.4)</td>
</tr>
<tr>
<td style="text-align: center;">davinci</td>
<td style="text-align: center;">+1 Row</td>
<td style="text-align: center;">92.5 (90.7)</td>
<td style="text-align: center;">64.8 (58.7)</td>
<td style="text-align: center;">53.7 (41.7)</td>
</tr>
<tr>
<td style="text-align: center;">davinci</td>
<td style="text-align: center;">+3 Rows</td>
<td style="text-align: center;">91.6 (90.6)</td>
<td style="text-align: center;">67.0 (60.2)</td>
<td style="text-align: center;">55.1 (42.9)</td>
</tr>
<tr>
<td style="text-align: center;">davinci</td>
<td style="text-align: center;">+5 Rows</td>
<td style="text-align: center;">91.0 (90.2)</td>
<td style="text-align: center;">65.3 (60.0)</td>
<td style="text-align: center;">53.9 (42.1)</td>
</tr>
<tr>
<td style="text-align: center;">davinci</td>
<td style="text-align: center;">+10 Rows</td>
<td style="text-align: center;">91.2 (90.6)</td>
<td style="text-align: center;">63.3 (58.3)</td>
<td style="text-align: center;">52.4 (40.2)</td>
</tr>
</tbody>
</table>
<p>Table 4: Performance on Spider across all evaluated models and prompts, as measured by percentage of predictions which are valid/executable SQL (VA), execution accuracy (EX), test-suite accuracy (TS). Main results are on the Spider development set, results in parantheses are on Spider-Realistic (Deng et al., 2021), a modified subset of the Spider development set with explicit references to column names removed from questions.</p>
<p>14\% Shortcuts
Q What are the template ids with template type description "Presentation".
Gold SELECT T2.template_id FROM Ref_template_types AS T1 JOIN Templates AS T2 ON T1.template_type_code = T2.template_type_code WHERE T1.template_type_description = "Presentation"
Pred SELECT Template_ID FROM Templates WHERE Template_Type_Code = 'PPT';
8\% SELECT Extra Columns
NL Which city has the most frequent destination airport?
Gold SELECT T1.City FROM AIRFORTS AS T1 JOIN FLIGHTS AS T2 ON T1.AirportCode = T2.DestAirport GROUP BY T1.City ORDER BY count $(<em>)$ DESC LIMIT 1
Pred SELECT City, COUNT (</em>) AS 'Number of Destinations' FROM airports JOIN flights ON airports.AirportCode = flights.DestAirport GROUP BY City ORDER BY COUNT (<em>) DESC LIMIT 1;
$5 \%$ SELECT Convention
NL Which airlines have a flight with source airport AHD?
Gold SELECT T1.Airline FROM AIRLINES AS T1 JOIN FLIGHTS AS T2 ON T1.uid = T2.Airline WHERE T2.SourceAirport = "AHD"
Pred SELECT Airline FROM flights WHERE SourceAirport = 'AHD';
4\% GROUP BY Convention
NL Which makers designed more than 3 car models? List full name and the id.
Gold SELECT T1.FullName, T1.Id FROM CAR_MAKERS AS T1 JOIN MODEL_LIST AS T2 ON T1.Id = T2.Maker GROUP BY T1.Id HAVING count (</em>) &gt; 3;
Pred SELECT car_makers.FullName, car_makers.Id FROM car_makers JOIN model_list ON car_makers.Id = model_list.Maker GROUP BY car_makers.FullName HAVING COUNT (model_list.Model) &gt; 3;
$4 \%$ Argmax
NL Give the name, year of independence, and surface area of the country that has the lowest population.
Gold SELECT Name, SurfaceArea, IndepYear FROM country ORDER BY Population LIMIT 1
Pred SELECT Name, IndepYear, SurfaceArea FROM country WHERE Population = (SELECT MIN (Population) FROM country);
$51 \%$ Other - Semantic Incorrect
NL Which countries have either English or Dutch as an official language?
Gold SELECT * FROM country AS T1 JOIN countrylanguage AS T2 ON T1.Code = T2.CountryCode WHERE T2.Language = "English" AND IsOfficial = "T" UNION SELECT * FROM country AS T1 JOIN countrylanguage AS T2 ON T1.Code = T2.CountryCode WHERE T2.Language = "Dutch" AND IsOfficial = "T"
Pred SELECT * FROM countrylanguage WHERE IsOfficial = 'T' OR IsOfficial = 'F';
NL Show the name and the release year of the song by the youngest singer.
Gold SELECT song_name, song_release_year FROM singer ORDER BY age LIMIT 1
Pred SELECT Name, Song_release_year FROM singer WHERE Age = (SELECT MIN (Age) FROM singer);
NL What is the maximum miles per gallon of the car with 8 cylinders or produced before 1980?
Gold select max (mpg) from cars_data where cylinders $=8$ or year $&lt;1980$
Pred SSELECT MAX (MPG) FROM cars_data WHERE Cylinders $=8$ AND Year $&lt;1980$;
$14 \%$ Other - Ambiguous Correct
NL Find the first name and age of students who have a pet.
Gold SELECT DISTINCT T1.fname, T1.age FROM student AS T1 JOIN has_pet AS T2 ON T1.stuid $=$ T2.stuid
Pred SELECT Fname, Age FROM Student WHERE StuID IN (SELECT StuID FROM Has_Pet);
NL How many countries speak both English and Dutch?
Gold SELECT COUNT (*) FROM (SELECT T1.Name FROM country AS T1 JOIN countrylanguage AS T2 ON T1.Code = T2.CountryCode WHERE T2.Language = "English" INTERSECT SELECT T1.Name FROM country AS T1 JOIN countrylanguage AS T2 ON T1.Code = T2.CountryCode WHERE T2.Language = "Dutch")</p>
<p>Pred SELECT COUNT (*) FROM countrylanguage WHERE Language = 'Dutch' AND CountryCode IN (SELECT CountryCode FROM countrylanguage WHERE Language = 'English');
NL What are the Asian countries which have a population larger than that of any country in Africa?
Gold SELECT Name FROM country WHERE Continent = "Asia" AND population &gt; (SELECT min (population) FROM country WHERE Continent = "Africa");
Pred SELECT Name FROM country WHERE Continent = 'Asia' AND Population &gt; (SELECT MAX (Population) FROM country WHERE Continent = 'Africa');</p>
<p>Figure 3: Additional examples of error types, as made by davinci-codex model with Create Table + Select 3 prompt. NL stands for natural language question. Percentage indicates the percentage of errors which are of the given type.</p>
<p>10-shot examples
what states does the missouri river run through SELECT
RIVERalias0.TRAVERSE FROM RIVER AS RIVERalias0 WHERE
RIVERalias0.RIVER_NAME = "missouri" ; - what is the size of texas SELECT
STATEalias0.AREA FROM STATE AS STATEalias0 WHERE STATEalias0.STATE_NAME
= "texas" ; - what are the major cities in texas SELECT
CITYalias0.CITY_NAME FROM CITY AS CITYalias0 WHERE CITYalias0.POPULATION</p>
<blockquote>
<p>150000 AND CITYalias0.STATE_NAME = "texas" ; - what is the capital
of pennsylvania SELECT STATEalias0.CAPITAL FROM STATE AS STATEalias0
WHERE STATEalias0.STATE_NAME = "pennsylvania" ; - what is the biggest
city in nebraska SELECT CITYalias0.CITY_NAME FROM CITY AS CITYalias0
WHERE CITYalias0.POPULATION = ( SELECT MAX( CITYalias1.POPULATION
) FROM CITY AS CITYalias1 WHERE CITYalias1.STATE_NAME = "nebraska"
) AND CITYalias0.STATE_NAME = "nebraska" ; - what is the population
of austin SELECT CITYalias0.POPULATION FROM CITY AS CITYalias0
WHERE CITYalias0.CITY_NAME = "austin" ; - which state is kalamazoo
in SELECT CITYalias0.STATE_NAME FROM CITY AS CITYalias0 WHERE
CITYalias0.CITY_NAME = "kalamazoo" ; - name all the rivers in
colorado SELECT RIVERalias0.RIVER_NAME FROM RIVER AS RIVERalias0 WHERE
RIVERalias0.TRAVERSE = "colorado" ; - what states border missouri SELECT
BORDER_INFOalias0.BORDER FROM BORDER_INFO AS BORDER_INFOalias0 WHERE
BORDER_INFOalias0.STATE_NAME = "missouri" ; - how many people live in
new mexico SELECT STATEalias0.POPULATION FROM STATE AS STATEalias0 WHERE
STATEalias0.STATE_NAME = "new mexico" ;
Very similar query in the few-shot prompt fixes the example
Question
which states border iowa
Gold SELECT BORDER_INFOalias0.BORDER FROM BORDER_INFO AS BORDER_INFOalias0 WHERE BORDER_INFOalias0.STATE_NAME = "iowa" ;
0-shot pred SELECT state_name FROM border WHERE border = 'iowa'
10-shot pred SELECT BORDER_INFOalias0.BORDER FROM BORDER_INFO AS BORDER_INFOalias0 WHERE BORDER_INFOalias0.STATE_NAME = "iowa"
Argmax convetion fixed
Question
what state has the smallest population
Gold SELECT STATEalias0.STATE_NAME FROM STATE AS STATEalias0 WHERE
STATEalias0.POPULATION = (SELECT MIN(STATEalias1.POPULATION) FROM STATE
AS STATEalias1) ;
0-shot pred SELECT state_name FROM state ORDER BY population LIMIT 1
10-shot pred SELECT STATEalias0.STATE_NAME FROM STATE AS STATEalias0 WHERE
STATEalias0.POPULATION = (SELECT MIN(STATEalias1.POPULATION) FROM STATE
AS STATEalias1)
SELECT extra columns fixed
Question
what is the population of the state with the largest area
Gold SELECT STATEalias0.POPULATION FROM STATE AS STATEalias0 WHERE
STATEalias0.AREA = (SELECT MAX(STATEalias1.AREA) FROM STATE AS
STATEalias1) ;
0-shot pred SELECT state_name, population FROM state WHERE area = (SELECT MAX(area)
10-shot pred SELECT STATEalias0.POPULATION FROM STATE AS STATEalias0 WHERE
STATEalias0.AREA = (SELECT MAX(STATEalias1.AREA) FROM STATE AS
STATEalias1)</p>
</blockquote>
<p>Figure 4: Cherry-picked examples of Codex improvements from 0-shot to 10-shot text-to-SQL on GeoQuery validation set. The style of the generated SQL changes a lot and is much closer to that of the gold SQL when few-shot examples are in the prompt. The few-shot examples were also useful to adapt the generated SQL to the conventions of the dataset, like the way argmax is done, or the selected columns.</p>
<h1>C Example Prompts</h1>
<div class="codehilite"><pre><span></span><code>What is Kyle’s id? | network_l | highschooler : id, name ( Kyle ), grade | friend :
    student_id, friend_id | likes : student_id, liked_id
</code></pre></div>

<p>Figure 5: Example input for baseline T5 models.</p>
<div class="codehilite"><pre><span></span><code>-- Using valid SQLite, answer the following questions.
-- What is Kyle’s id?
SELECT
</code></pre></div>

<p>Figure 6: Example prompt for Question.</p>
<div class="codehilite"><pre><span></span><code><span class="gu">##</span># SQLite SQL tables, with their properties:
<span class="gh">#</span>
# Highschooler(ID, name, grade)
<span class="gh">#</span> Friend(student_id, friend_id)
<span class="gh">#</span> Likes(student_id, liked_id)
<span class="gh">#</span>
### What is Kyle’s id?
SELECT
</code></pre></div>

<p>Figure 7: Example prompt for API Docs.</p>
<div class="codehilite"><pre><span></span><code><span class="o">/*</span>
<span class="mi">3</span><span class="w"> </span><span class="n">example</span><span class="w"> </span><span class="n">rows</span><span class="w"> </span><span class="n">from</span><span class="w"> </span><span class="n">table</span><span class="w"> </span><span class="n">Highschooler</span><span class="p">:</span>
<span class="n">SELECT</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">FROM</span><span class="w"> </span><span class="n">Highschooler</span><span class="w"> </span><span class="n">LIMIT</span><span class="w"> </span><span class="mi">3</span><span class="p">;</span>
<span class="n">Table</span><span class="p">:</span><span class="w"> </span><span class="n">Highschooler</span>
<span class="w">    </span><span class="n">ID</span><span class="w"> </span><span class="n">name</span><span class="w"> </span><span class="n">grade</span>
<span class="mi">1510</span><span class="w"> </span><span class="n">Jordan</span><span class="w"> </span><span class="mi">9</span>
<span class="mi">1689</span><span class="w"> </span><span class="n">Gabriel</span><span class="w"> </span><span class="mi">9</span>
<span class="mi">1381</span><span class="w"> </span><span class="n">Tiffany</span><span class="w"> </span><span class="mi">9</span>
<span class="o">*/</span>
<span class="o">/*</span>
<span class="mi">3</span><span class="w"> </span><span class="n">example</span><span class="w"> </span><span class="n">rows</span><span class="w"> </span><span class="n">from</span><span class="w"> </span><span class="n">table</span><span class="w"> </span><span class="n">Friend</span><span class="p">:</span>
<span class="n">SELECT</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">FROM</span><span class="w"> </span><span class="n">Friend</span><span class="w"> </span><span class="n">LIMIT</span><span class="w"> </span><span class="mi">3</span><span class="p">;</span>
<span class="n">Table</span><span class="p">:</span><span class="w"> </span><span class="n">Friend</span>
<span class="w">    </span><span class="n">student_id</span><span class="w"> </span><span class="n">friend_id</span>
<span class="w">        </span><span class="mi">1510</span><span class="w"> </span><span class="mi">1381</span>
<span class="w">        </span><span class="mi">1510</span><span class="w"> </span><span class="mi">1689</span>
<span class="w">        </span><span class="mi">1689</span><span class="w"> </span><span class="mi">1709</span>
<span class="o">*/</span>
<span class="o">/*</span>
<span class="mi">3</span><span class="w"> </span><span class="n">example</span><span class="w"> </span><span class="n">rows</span><span class="w"> </span><span class="n">from</span><span class="w"> </span><span class="n">table</span><span class="w"> </span><span class="n">Likes</span><span class="p">:</span>
<span class="n">SELECT</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">FROM</span><span class="w"> </span><span class="n">Likes</span><span class="w"> </span><span class="n">LIMIT</span><span class="w"> </span><span class="mi">3</span><span class="p">;</span>
<span class="n">Table</span><span class="p">:</span><span class="w"> </span><span class="n">Likes</span>
<span class="w">    </span><span class="n">student_id</span><span class="w"> </span><span class="n">liked_id</span>
<span class="w">        </span><span class="mi">1689</span><span class="w"> </span><span class="mi">1709</span>
<span class="w">        </span><span class="mi">1709</span><span class="w"> </span><span class="mi">1689</span>
<span class="w">        </span><span class="mi">1782</span><span class="w"> </span><span class="mi">1709</span>
<span class="o">*/</span>
<span class="o">--</span><span class="w"> </span><span class="n">Using</span><span class="w"> </span><span class="n">valid</span><span class="w"> </span><span class="n">SQLite</span><span class="p">,</span><span class="w"> </span><span class="n">answer</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">following</span><span class="w"> </span><span class="n">questions</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">tables</span><span class="w"> </span><span class="n">provided</span><span class="w"> </span><span class="n">above</span><span class="p">.</span>
<span class="o">--</span><span class="w"> </span><span class="n">What</span><span class="w"> </span><span class="n">is</span><span class="w"> </span><span class="n">Kyle</span>’<span class="n">s</span><span class="w"> </span><span class="n">id</span>?
<span class="n">SELECT</span>
</code></pre></div>

<p>Figure 8: Example prompt for Select 3.</p>
<div class="codehilite"><pre><span></span><code>CREATE TABLE Highschooler(
    ID int primary key,
    name text,
    grade int)
CREATE TABLE Friend(
    student_id int,
    friend_id int,
    primary key (student_id,friend_id),
    foreign key(student_id) references Highschooler(ID),
    foreign key (friend_id) references Highschooler(ID)
)
CREATE TABLE Likes(
    student_id int,
    liked_id int,
    primary key (student_id, liked_id),
    foreign key (liked_id) references Highschooler(ID),
    foreign key (student_id) references Highschooler(ID)
)
</code></pre></div>

<p>-- Using valid SQLite, answer the following questions for the tables provided above.
-- What is Kyle's id?
SELECT</p>
<p>Figure 9: Example prompt for Create Table.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">CREATE TABLE Highschooler(</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">ID int primary key,</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">name text,</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">grade int)</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">/*</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">3 example rows:</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">SELECT * FROM Highschooler LIMIT 3;</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">ID name grade</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">1510 Jordan 9</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">1689 Gabriel 9</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">1381 Tiffany 9</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">*</td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>CREATE TABLE Friend(
student_id int,
friend_id int,
primary key (student_id,friend_id),
foreign key(student_id) references Highschooler(ID),
foreign key (friend_id) references Highschooler(ID)
)
/<em>
3 example rows:
SELECT * FROM Friend LIMIT 3;
student_id friend_id
15101381
15101689
16891709
</em>f</p>
<p>CREATE TABLE Likes(
student_id int,
liked_id int,
primary key (student_id, liked_id),
foreign key (liked_id) references Highschooler(ID),
foreign key (student_id) references Highschooler(ID)
)
/<em>
3 example rows:
SELECT * FROM Likes LIMIT 3;
student_id liked_id
16891709
17091689
17821709
</em>f
-- Using valid SQLite, answer the following questions for the tables provided above.
-- What is Kyle's id?
SELECT</p>
<p>Figure 10: Example prompt for Create Table + Select 3.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 11: Example prompt for 5-shot. It starts with the schema and 3 rows per database (exactly as in Figure 10), followed by 5 few-shot examples, and finally the target question.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{*}$ Work partially done at Mila and the Université de Montréal.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{1}$ See Appendix A. 2 for a discussion on parameter counts.&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
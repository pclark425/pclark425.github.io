<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8308 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8308</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8308</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-152.html">extraction-schema-152</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <p><strong>Paper ID:</strong> paper-273098595</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2410.01952v2.pdf" target="_blank">TypedThinker: Diversify Large Language Model Reasoning with Typed Thinking</a></p>
                <p><strong>Paper Abstract:</strong> Large Language Models (LLMs) have demonstrated strong reasoning capabilities in solving complex problems. However, current approaches primarily enhance reasoning through the elaboration of thoughts while neglecting the diversity of reasoning types. LLMs typically employ deductive reasoning, proceeding step-by-step from given conditions, which limits their exploration during problem-solving. Our analysis reveals that certain problems are exclusively solvable through specific reasoning strategies like inductive, abductive, or analogical reasoning. However, incorporating diverse reasoning approaches presents two key challenges: identifying the appropriate reasoning type for each problem and exploiting this approach during problem-solving. Therefore, we propose the TypedThinker that predicts suitable reasoning types based on the problem and their previous effectiveness and provides relevant demonstrations to guide LLMs in applying these strategies. Experimental results show significant improvements across multiple benchmarks, with performance gains of 3.4% for Mistral 7B, 6.5% for LLaMA3 8B, and 7% for Qwen 2 7B on logical and mathematical reasoning tasks. TypedThinker enhances LLM reasoning without requiring knowledge distillation from larger models. It can be integrated into more advanced systems like GPT-4o or specialized models like MetaMath to diversify their reasoning approaches and improve their problem-solving capabilities.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8308.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8308.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TypedThinker</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>TypedThinker: Diversify Large Language Model Reasoning with Typed Thinking</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A framework that (1) predicts which high-level reasoning type(s) (deductive, inductive, abductive, analogical, or none) are likely to solve a given problem using a finetuned meta-thinker, (2) retrieves exemplar demonstrations for the predicted type from an explicit memory, and (3) uses a (optionally finetuned) LLM reasoner to generate solutions following that type; supports greedy selection, self-consistency, and weighted voting across types.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>TYPEDTHINKER: DIVERSIFY LARGE LANGUAGE MODEL REASONING WITH TYPED THINKING</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>TypedThinker (framework)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A three-component framework: (i) meta-thinker (finetuned LLM) that outputs an effectiveness score s_x,k ∈ [0,1] for each reasoning type f_k; (ii) explicit collection M of correct reasoning demonstrations grouped by reasoning type, retrieved by semantic similarity (SentenceTransformer cosine) with threshold δ; (iii) LLM reasoner (optionally finetuned) that is prompted with the problem, chosen type, and retrieved demonstrations to generate answers.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['Deductive reasoning', 'Inductive reasoning', 'Abductive reasoning', 'Analogical reasoning', 'Empty/None (no specific type)', 'Chain-of-Thought (CoT) style prompting', 'Self-Consistency (SC) majority voting', 'Mixture-of-Reasoning (MoR) / weighted vote across types', 'Retrieval of type-specific demonstrations', 'Meta-reasoning (meta-thinker predicting effectiveness scores)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Defines four typed reasoning modes (deduction/induction/abduction/analogy + empty). Meta-thinker assigns empirical effectiveness scores per instance (trained from success rates computed from sampled solutions). Explicit collection: sample up to 10 solutions per type (temp=1) on training set, filter for correctness and reverse-check that solution's method matches type; store the longest correct solution per (instance,type). At inference retrieve top-3 similar demos (cosine similarity, threshold δ=0.5). Two selection strategies: greedy use f* (highest s_x,k) and resample + self-consistency; or sample across effective set F and weighted-vote with s_x,k as coefficients. Reasoner may be instruction-finetuned per-type.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>both (framework enforces diverse typed reasoning while comparing to default/similar reasoning behaviour)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Explicitly instruct models to apply each reasoning type and sample multiple solutions per type (training sampling: up to 10 samples per type at temp=1). Define a problem as solvable by a type if any sampled solution for that type is correct. Compare baselines: few-shot, CoT Selection (model chooses type then solves), Self-Discover (task-level shared structure), Zero-shot MoR and Few-shot MoR (apply all types and majority vote), TypedThinker greedy (use predicted best f* + optional SC@5) and TypedThinker weighted vote on effective set F. Ablations: remove meta-thinker, remove explicit collection (replace with human-written few-shot), remove finetuned reasoner; also compare f* vs weighted voting and single-response vs SC@5.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Evaluated on four primary benchmarks: LogiQA (natural language logical reasoning, multi-choice), BBH (subset of BigBench hard tasks, multi-choice), GSM8K (grade-school math word problems), MATH (competition math); additional generalization tests on Contexthub (propositional logic) and LiveBench reasoning tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Across backbone LLMs, TypedThinker improves accuracy relative to baselines; example (single-response) numbers from paper: Mistral-7B TypedThinker (LogiQA 0.554, BBH 0.423, GSM8K 0.386, MATH 0.092, Avg 0.364) vs Few-shot (LogiQA 0.485, BBH 0.346, GSM8K 0.369, MATH 0.074, Avg 0.318). LLaMA3-8B TypedThinker (LogiQA 0.599, BBH 0.543, GSM8K 0.585, MATH 0.195, Avg 0.481) vs Few-shot (LogiQA 0.581, BBH 0.359, GSM8K 0.581, MATH 0.193, Avg 0.428). Qwen-2-7B reported ~7% improvement over few-shot baseline (detailed Qwen table in Appendix). Generalization: applying the Mistral-trained meta-thinker and collection to GPT-4o and MetaMath yields modest gains: GPT-4o baseline (LogiQA 0.76, BBH 0.84, GSM8K 0.97, MATH 0.89) → TypedThinker variants and +SC gave small improvements (examples: +SC@5 up to 0.81/0.90/0.96/0.91). MetaMath GSM8K 0.690→ TypedThinker 0.696 (single) and +SC up to 0.736. The authors also report aggregate improvement claims: +3.4% (Mistral 7B), +6.5% (LLaMA3 8B), +7% (Qwen 2 7B) on logical & math tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Key qualitative observations: (1) Different reasoning types uniquely solve particular problems — many instances are solvable exclusively by a single reasoning type even after high-temperature sampling; (2) Repeated sampling/high temperature mainly produces surface variation but not fundamentally different reasoning strategies; (3) Incorrect type selection often misleads models (CoT Selection often chooses 'None' or defaults to deductive reasoning >60%); (4) Explicit demonstrations (retrieval) significantly improve LLMs' ability to exploit a given type, especially for logical tasks; (5) Retrieved concrete numeric chains can mislead mathematical reasoning (retrieval helps logic more than math); (6) Meta-thinker accuracy (Kendall's τ and top-type accuracy ~68.3% averaged) matters: accurate type selection + SC on a correct type outperforms naive mixture voting when problems are dominated by a single effective type (e.g., MATH dominated by inductive solutions).</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>Diversifying LLM reasoning via explicit typed reasoning increases the set of solvable problems because some instances are solvable only by specific high-level reasoning types; selecting an appropriate type matters (meta-thinker improves selection); providing demonstrations of how to perform each type (explicit collection) and finetuning reasoners improves ability to follow the chosen strategy; simple diversity mechanisms like repeated sampling or temperature tuning rarely produce fundamentally different reasoning strategies and are insufficient.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'TypedThinker: Diversify Large Language Model Reasoning with Typed Thinking', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8308.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8308.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Mistral-7B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mistral 7B Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 7-billion-parameter open-source LLM (Mistral family) used as a backbone reasoner and for finetuning the meta-thinker and reasoner in TypedThinker experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>TYPEDTHINKER: DIVERSIFY LARGE LANGUAGE MODEL REASONING WITH TYPED THINKING</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Mistral 7B Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source 7B-parameter transformer model (Mistral family), instruction-tuned version used as backbone LLM for sampling, finetuning the meta-thinker and reasoner, and for direct evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['Default (mostly deductive) Chain-of-Thought', 'Typed reasoning enforced by TypedThinker (deductive/inductive/abductive/analogical/none)', 'Self-Consistency (SC) voting', 'Mixture-of-Reasoning (MoR) voting', 'Retrieval of type-specific demonstrations (explicit collection)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Baseline few-shot/CoT implies primarily deductive step-by-step CoT. Under TypedThinker, Mistral is guided by meta-thinker predicted type, and receives retrieved demonstrations; reasoner may be finetuned to follow type-specific prompts. Sampling regimes: training sampled up to 10 solutions per type (temp=1); inference uses greedy f* + SC@5 by default or weighted vote over effective set.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>both (default behaviour is similar/deductive, TypedThinker enforces diverse typed reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Mistral tested across Few-shot, CoT Selection, Self-Discover, Zero-shot MoR, Few-shot MoR, and TypedThinker. Ablations on TypedThinker for Mistral: remove meta-thinker, remove memory, remove finetuned reasoner. Retrieval uses SentenceTransformer embeddings; top-3 retrieved.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>LogiQA, BBH (16 tasks subset), GSM8K, MATH; Contexthub and LiveBench for generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>From Table 1 (single-response): Few-shot avg 0.318 (LogiQA 0.485, BBH 0.346, GSM8K 0.369, MATH 0.074). TypedThinker single-response avg 0.364 (LogiQA 0.554, BBH 0.423, GSM8K 0.386, MATH 0.092). Gains depend on benchmark; +SC@5 increased further (TypedThinker +SC@5 LogiQA 0.570, BBH 0.469, GSM8K 0.500, MATH 0.149, Avg 0.422). The paper reports a +3.4% improvement claim for Mistral-7B on logical & math tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Mistral's default tendency to choose deductive reasoning in CoT Selection leads to mismatches; TypedThinker's meta-thinker better predicts types and helps Mistral exploit non-deductive strategies; ablation shows meta-thinker is important for math benchmarks whereas explicit collection benefits logical benchmarks more; finetuned reasoner also contributes substantially.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>Mistral benefits from typed reasoning selection and demonstrations: explicit type selection and retrieval increase diversity and solve instances otherwise unsolvable by its default (mostly deductive) reasoning; selecting the right type yields larger gains than naive mixture voting when problems have dominant single-type solutions.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'TypedThinker: Diversify Large Language Model Reasoning with Typed Thinking', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8308.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8308.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaMA3-8B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaMA3 8B Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An 8-billion-parameter LLaMA3 instruction-tuned backbone LLM evaluated with and without TypedThinker finetuning and retrieval components.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>TYPEDTHINKER: DIVERSIFY LARGE LANGUAGE MODEL REASONING WITH TYPED THINKING</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA3 8B Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source 8B-parameter instruction-tuned transformer model from the LLaMA3 family used as a backbone in experiments; stronger baseline reasoning and instruction-following than Mistral 7B according to the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['Default Chain-of-Thought (predominantly deductive)', 'Typed reasoning (deductive/inductive/abductive/analogical via TypedThinker)', 'Self-Consistency (SC)', 'Weighted voting over effective set F', 'Retrieval of demonstrations']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Used same TypedThinker pipeline: meta-thinker predicts s_x,k; retrieve top-3 demos; reasoner (finetuned or ICL) follows prompts to produce type-specific CoT. For LLaMA3 paper reports both greedy f*+SC and weighted vote variants; LLaMA3 benefits more from the full TypedThinker + finetuned reasoner.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>both (default is similar/deductive; TypedThinker enforces and exploits multiple reasoning types)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Same experimental comparisons as for Mistral: Few-shot, CoT Selection, Self-Discover, MoR, TypedThinker; ablations and cross-domain meta-thinker training (logical-only, math-only, unified).</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>LogiQA, BBH, GSM8K, MATH (same splits as Mistral experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>From Table 1 (single-response): Few-shot avg 0.428 (LogiQA 0.581, BBH 0.359, GSM8K 0.581, MATH 0.193). TypedThinker single-response avg 0.481 (LogiQA 0.599, BBH 0.543, GSM8K 0.585, MATH 0.195). With +SC@5 TypedThinker reaches (LogiQA 0.637, BBH 0.591, GSM8K 0.753, MATH 0.267, Avg 0.562). Paper claims ~6.5% improvement for LLaMA3-8B on logical & math tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Stronger LLaMA3 models benefit more from typed reasoning and finetuned reasoners; weighted voting on effective set sometimes helps (e.g., LogiQA, GSM8K) but accurate f* selection + SC often outperforms weighted voting on benchmarks dominated by a single effective reasoning type (e.g., MATH dominated by inductive).</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>More capable LLMs (LLaMA3) gain larger benefits from TypedThinker; accurate selection of reasoning type combined with SC provides the biggest gains when a dominant reasoning type exists for many instances.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'TypedThinker: Diversify Large Language Model Reasoning with Typed Thinking', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8308.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8308.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Qwen-2-7B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Qwen 2-7B Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A Qwen-series 7B instruction-tuned LLM evaluated as an additional backbone; TypedThinker yields notable improvements on reasoning benchmarks for this model as well.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>TYPEDTHINKER: DIVERSIFY LARGE LANGUAGE MODEL REASONING WITH TYPED THINKING</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Qwen 2-7B Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source Qwen family 7B-parameter instruction-tuned transformer used as backbone in experiments (results reported in appendix).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['Default CoT (often deductive)', 'Typed reasoning (deductive/inductive/abductive/analogical) under TypedThinker', 'Self-Consistency (SC)', 'Retrieval and weighted vote']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Same pipeline: meta-thinker (finetuned on experiences) predicts effectiveness scores; explicit collection retrieved as few-shot demos; reasoner instructed to follow a type; sampling and SC applied similarly.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>both</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Qwen evaluated with same baselines and TypedThinker variants (details in Appendix A.5.1). Reported comparisons include single-response and +SC@5 majority-vote settings.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>LogiQA, BBH, GSM8K, MATH (Qwen-specific table in appendix).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Paper reports approximately a 7% improvement of TypedThinker over few-shot baseline for Qwen-2-7B across single-generation and +SC@5 settings (exact per-benchmark numbers provided in appendix Table 11).</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>TypedThinker generalizes across different open-source 7B LLMs including Qwen; improvements consistent with those for Mistral and LLaMA3, indicating the method's generality.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>TypedThinker improves smaller open-source LLMs like Qwen 2-7B by combining meta-type prediction and type-specific demonstrations; similar qualitative effects (unique-solution-by-type, retrieval helpful for logic) as observed for other backbones.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'TypedThinker: Diversify Large Language Model Reasoning with Typed Thinking', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8308.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8308.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4o (facilitated)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4o (facilitated with Mistral-trained meta-thinker & collection)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large commercial LLM (GPT-4o) that was not finetuned in this work but was evaluated by using the authors' finetuned meta-thinker and explicit collection to guide generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>TYPEDTHINKER: DIVERSIFY LARGE LANGUAGE MODEL REASONING WITH TYPED THINKING</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o (facilitated)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A very large-capacity LLM (GPT-4o) used in a transfer experiment: the finetuned Mistral-7B meta-thinker and explicit collection were used to select reasoning types and retrieve demonstrations, while GPT-4o served as the reasoner (no finetuning).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['Default high-capacity CoT', 'Typed reasoning via retrieved demonstrations and meta-thinker predictions (no finetuning)', 'Self-Consistency (SC)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>The paper applied the Mistral-trained meta-thinker to propose effectiveness scores and retrieve demonstrations; GPT-4o was then prompted with selected type and retrieved demos to produce answers; SC@5 also evaluated.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>both (GPT-4o baseline already versatile; TypedThinker transfers typed reasoning to it)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Transfer evaluation: randomly sample 100 examples per benchmark and run GPT-4o with meta-thinker/retrieval guidance; compare baseline GPT-4o, GPT-4o +SC, TypedThinker-guided GPT-4o, and TypedThinker +SC.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>LogiQA, BBH, GSM8K, MATH (100 examples per benchmark in transfer eval).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>From Table 5: GPT-4o baseline (single) LogiQA 0.76, BBH 0.84, GSM8K 0.97, MATH 0.89. TypedThinker-guided GPT-4o single: LogiQA 0.80, BBH 0.86, GSM8K 0.95, MATH 0.88. With SC@5 the best reported: 0.81 / 0.90 / 0.96 / 0.91. Gains are modest since GPT-4o strong baseline leaves little headroom.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Even when using a stronger LLM without further finetuning, applying a meta-thinker and retrieved type-specific demonstrations can provide small but measurable improvements on logic benchmarks; gains on math are limited due to already very high GPT-4o performance.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>TypedThinker components (meta-thinker + explicit collection) can transfer to larger LLMs without additional finetuning and still provide incremental improvements, notably on logical reasoning tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'TypedThinker: Diversify Large Language Model Reasoning with Typed Thinking', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8308.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8308.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MetaMath (facilitated)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MetaMath (7B math-specific model facilitated with TypedThinker)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A Mistral-7B-based math-specialized model trained with synthesized math data; evaluated with and without TypedThinker retrieval & selection to test transferability.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>TYPEDTHINKER: DIVERSIFY LARGE LANGUAGE MODEL REASONING WITH TYPED THINKING</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>MetaMath (7B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A math-specialized 7B LLM (Mistral-based) trained on ~400k synthesized math data distilled from GPT-3.5-Turbo; used as a reasoner in transfer experiments with the authors' meta-thinker and explicit collection.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['Math-focused CoT', 'Typed reasoning (guided by externally-trained meta-thinker and demonstrations)', 'Self-Consistency (SC)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>MetaMath used as reasoner without finetuning; it received meta-thinker outputs and retrieved demonstrations to prompt type-specific reasoning; SC@5 majority voting also reported.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>both</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Full test set evaluated; compared MetaMath baseline, MetaMath +SC, MetaMath with TypedThinker retrieval & selection, and TypedThinker +SC.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>GSM8K and MATH (full test sets for MetaMath experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>From Table 6: MetaMath baseline GSM8K 0.690, MATH 0.209. MetaMath +SC improves slightly. TypedThinker single-response: GSM8K 0.696, MATH 0.220. TypedThinker +SC@5 yields GSM8K 0.736 and MATH 0.246 (improvements modest but consistent).</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>TypedThinker yields small improvements for a math-specialized LLM; for math tasks retrieval must be used carefully since retrieved numeric chains can mislead calculations—masking or avoiding direct numeric reuse is suggested.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>A domain-specialized model can still benefit from typed reasoning selection and demonstrations; however, retrieval of concrete numeric chains can harm math reasoning unless handled (e.g., masked computations).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'TypedThinker: Diversify Large Language Model Reasoning with Typed Thinking', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Self-consistency improves chain of thought reasoning in language models <em>(Rating: 2)</em></li>
                <li>Large language monkeys: Scaling inference compute with repeated sampling <em>(Rating: 2)</em></li>
                <li>Tree of Thought: Deliberate problem solving with large language models <em>(Rating: 2)</em></li>
                <li>Self-Discover: Large language models self-compose reasoning structures <em>(Rating: 2)</em></li>
                <li>Complex CoT (Complex Chain-of-Thought) or Complexity-based prompting for multi-step reasoning <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8308",
    "paper_id": "paper-273098595",
    "extraction_schema_id": "extraction-schema-152",
    "extracted_data": [
        {
            "name_short": "TypedThinker",
            "name_full": "TypedThinker: Diversify Large Language Model Reasoning with Typed Thinking",
            "brief_description": "A framework that (1) predicts which high-level reasoning type(s) (deductive, inductive, abductive, analogical, or none) are likely to solve a given problem using a finetuned meta-thinker, (2) retrieves exemplar demonstrations for the predicted type from an explicit memory, and (3) uses a (optionally finetuned) LLM reasoner to generate solutions following that type; supports greedy selection, self-consistency, and weighted voting across types.",
            "citation_title": "TYPEDTHINKER: DIVERSIFY LARGE LANGUAGE MODEL REASONING WITH TYPED THINKING",
            "mention_or_use": "use",
            "model_name": "TypedThinker (framework)",
            "model_description": "A three-component framework: (i) meta-thinker (finetuned LLM) that outputs an effectiveness score s_x,k ∈ [0,1] for each reasoning type f_k; (ii) explicit collection M of correct reasoning demonstrations grouped by reasoning type, retrieved by semantic similarity (SentenceTransformer cosine) with threshold δ; (iii) LLM reasoner (optionally finetuned) that is prompted with the problem, chosen type, and retrieved demonstrations to generate answers.",
            "reasoning_methods": [
                "Deductive reasoning",
                "Inductive reasoning",
                "Abductive reasoning",
                "Analogical reasoning",
                "Empty/None (no specific type)",
                "Chain-of-Thought (CoT) style prompting",
                "Self-Consistency (SC) majority voting",
                "Mixture-of-Reasoning (MoR) / weighted vote across types",
                "Retrieval of type-specific demonstrations",
                "Meta-reasoning (meta-thinker predicting effectiveness scores)"
            ],
            "reasoning_methods_description": "Defines four typed reasoning modes (deduction/induction/abduction/analogy + empty). Meta-thinker assigns empirical effectiveness scores per instance (trained from success rates computed from sampled solutions). Explicit collection: sample up to 10 solutions per type (temp=1) on training set, filter for correctness and reverse-check that solution's method matches type; store the longest correct solution per (instance,type). At inference retrieve top-3 similar demos (cosine similarity, threshold δ=0.5). Two selection strategies: greedy use f* (highest s_x,k) and resample + self-consistency; or sample across effective set F and weighted-vote with s_x,k as coefficients. Reasoner may be instruction-finetuned per-type.",
            "reasoning_diversity": "both (framework enforces diverse typed reasoning while comparing to default/similar reasoning behaviour)",
            "reasoning_diversity_experimental_setup": "Explicitly instruct models to apply each reasoning type and sample multiple solutions per type (training sampling: up to 10 samples per type at temp=1). Define a problem as solvable by a type if any sampled solution for that type is correct. Compare baselines: few-shot, CoT Selection (model chooses type then solves), Self-Discover (task-level shared structure), Zero-shot MoR and Few-shot MoR (apply all types and majority vote), TypedThinker greedy (use predicted best f* + optional SC@5) and TypedThinker weighted vote on effective set F. Ablations: remove meta-thinker, remove explicit collection (replace with human-written few-shot), remove finetuned reasoner; also compare f* vs weighted voting and single-response vs SC@5.",
            "task_or_benchmark": "Evaluated on four primary benchmarks: LogiQA (natural language logical reasoning, multi-choice), BBH (subset of BigBench hard tasks, multi-choice), GSM8K (grade-school math word problems), MATH (competition math); additional generalization tests on Contexthub (propositional logic) and LiveBench reasoning tasks.",
            "performance_results": "Across backbone LLMs, TypedThinker improves accuracy relative to baselines; example (single-response) numbers from paper: Mistral-7B TypedThinker (LogiQA 0.554, BBH 0.423, GSM8K 0.386, MATH 0.092, Avg 0.364) vs Few-shot (LogiQA 0.485, BBH 0.346, GSM8K 0.369, MATH 0.074, Avg 0.318). LLaMA3-8B TypedThinker (LogiQA 0.599, BBH 0.543, GSM8K 0.585, MATH 0.195, Avg 0.481) vs Few-shot (LogiQA 0.581, BBH 0.359, GSM8K 0.581, MATH 0.193, Avg 0.428). Qwen-2-7B reported ~7% improvement over few-shot baseline (detailed Qwen table in Appendix). Generalization: applying the Mistral-trained meta-thinker and collection to GPT-4o and MetaMath yields modest gains: GPT-4o baseline (LogiQA 0.76, BBH 0.84, GSM8K 0.97, MATH 0.89) → TypedThinker variants and +SC gave small improvements (examples: +SC@5 up to 0.81/0.90/0.96/0.91). MetaMath GSM8K 0.690→ TypedThinker 0.696 (single) and +SC up to 0.736. The authors also report aggregate improvement claims: +3.4% (Mistral 7B), +6.5% (LLaMA3 8B), +7% (Qwen 2 7B) on logical & math tasks.",
            "qualitative_findings": "Key qualitative observations: (1) Different reasoning types uniquely solve particular problems — many instances are solvable exclusively by a single reasoning type even after high-temperature sampling; (2) Repeated sampling/high temperature mainly produces surface variation but not fundamentally different reasoning strategies; (3) Incorrect type selection often misleads models (CoT Selection often chooses 'None' or defaults to deductive reasoning &gt;60%); (4) Explicit demonstrations (retrieval) significantly improve LLMs' ability to exploit a given type, especially for logical tasks; (5) Retrieved concrete numeric chains can mislead mathematical reasoning (retrieval helps logic more than math); (6) Meta-thinker accuracy (Kendall's τ and top-type accuracy ~68.3% averaged) matters: accurate type selection + SC on a correct type outperforms naive mixture voting when problems are dominated by a single effective type (e.g., MATH dominated by inductive solutions).",
            "explicit_comparison": true,
            "key_claims_or_conclusions": "Diversifying LLM reasoning via explicit typed reasoning increases the set of solvable problems because some instances are solvable only by specific high-level reasoning types; selecting an appropriate type matters (meta-thinker improves selection); providing demonstrations of how to perform each type (explicit collection) and finetuning reasoners improves ability to follow the chosen strategy; simple diversity mechanisms like repeated sampling or temperature tuning rarely produce fundamentally different reasoning strategies and are insufficient.",
            "uuid": "e8308.0",
            "source_info": {
                "paper_title": "TypedThinker: Diversify Large Language Model Reasoning with Typed Thinking",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Mistral-7B",
            "name_full": "Mistral 7B Instruct",
            "brief_description": "A 7-billion-parameter open-source LLM (Mistral family) used as a backbone reasoner and for finetuning the meta-thinker and reasoner in TypedThinker experiments.",
            "citation_title": "TYPEDTHINKER: DIVERSIFY LARGE LANGUAGE MODEL REASONING WITH TYPED THINKING",
            "mention_or_use": "use",
            "model_name": "Mistral 7B Instruct",
            "model_description": "Open-source 7B-parameter transformer model (Mistral family), instruction-tuned version used as backbone LLM for sampling, finetuning the meta-thinker and reasoner, and for direct evaluation.",
            "reasoning_methods": [
                "Default (mostly deductive) Chain-of-Thought",
                "Typed reasoning enforced by TypedThinker (deductive/inductive/abductive/analogical/none)",
                "Self-Consistency (SC) voting",
                "Mixture-of-Reasoning (MoR) voting",
                "Retrieval of type-specific demonstrations (explicit collection)"
            ],
            "reasoning_methods_description": "Baseline few-shot/CoT implies primarily deductive step-by-step CoT. Under TypedThinker, Mistral is guided by meta-thinker predicted type, and receives retrieved demonstrations; reasoner may be finetuned to follow type-specific prompts. Sampling regimes: training sampled up to 10 solutions per type (temp=1); inference uses greedy f* + SC@5 by default or weighted vote over effective set.",
            "reasoning_diversity": "both (default behaviour is similar/deductive, TypedThinker enforces diverse typed reasoning)",
            "reasoning_diversity_experimental_setup": "Mistral tested across Few-shot, CoT Selection, Self-Discover, Zero-shot MoR, Few-shot MoR, and TypedThinker. Ablations on TypedThinker for Mistral: remove meta-thinker, remove memory, remove finetuned reasoner. Retrieval uses SentenceTransformer embeddings; top-3 retrieved.",
            "task_or_benchmark": "LogiQA, BBH (16 tasks subset), GSM8K, MATH; Contexthub and LiveBench for generalization.",
            "performance_results": "From Table 1 (single-response): Few-shot avg 0.318 (LogiQA 0.485, BBH 0.346, GSM8K 0.369, MATH 0.074). TypedThinker single-response avg 0.364 (LogiQA 0.554, BBH 0.423, GSM8K 0.386, MATH 0.092). Gains depend on benchmark; +SC@5 increased further (TypedThinker +SC@5 LogiQA 0.570, BBH 0.469, GSM8K 0.500, MATH 0.149, Avg 0.422). The paper reports a +3.4% improvement claim for Mistral-7B on logical & math tasks.",
            "qualitative_findings": "Mistral's default tendency to choose deductive reasoning in CoT Selection leads to mismatches; TypedThinker's meta-thinker better predicts types and helps Mistral exploit non-deductive strategies; ablation shows meta-thinker is important for math benchmarks whereas explicit collection benefits logical benchmarks more; finetuned reasoner also contributes substantially.",
            "explicit_comparison": true,
            "key_claims_or_conclusions": "Mistral benefits from typed reasoning selection and demonstrations: explicit type selection and retrieval increase diversity and solve instances otherwise unsolvable by its default (mostly deductive) reasoning; selecting the right type yields larger gains than naive mixture voting when problems have dominant single-type solutions.",
            "uuid": "e8308.1",
            "source_info": {
                "paper_title": "TypedThinker: Diversify Large Language Model Reasoning with Typed Thinking",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "LLaMA3-8B",
            "name_full": "LLaMA3 8B Instruct",
            "brief_description": "An 8-billion-parameter LLaMA3 instruction-tuned backbone LLM evaluated with and without TypedThinker finetuning and retrieval components.",
            "citation_title": "TYPEDTHINKER: DIVERSIFY LARGE LANGUAGE MODEL REASONING WITH TYPED THINKING",
            "mention_or_use": "use",
            "model_name": "LLaMA3 8B Instruct",
            "model_description": "Open-source 8B-parameter instruction-tuned transformer model from the LLaMA3 family used as a backbone in experiments; stronger baseline reasoning and instruction-following than Mistral 7B according to the paper.",
            "reasoning_methods": [
                "Default Chain-of-Thought (predominantly deductive)",
                "Typed reasoning (deductive/inductive/abductive/analogical via TypedThinker)",
                "Self-Consistency (SC)",
                "Weighted voting over effective set F",
                "Retrieval of demonstrations"
            ],
            "reasoning_methods_description": "Used same TypedThinker pipeline: meta-thinker predicts s_x,k; retrieve top-3 demos; reasoner (finetuned or ICL) follows prompts to produce type-specific CoT. For LLaMA3 paper reports both greedy f*+SC and weighted vote variants; LLaMA3 benefits more from the full TypedThinker + finetuned reasoner.",
            "reasoning_diversity": "both (default is similar/deductive; TypedThinker enforces and exploits multiple reasoning types)",
            "reasoning_diversity_experimental_setup": "Same experimental comparisons as for Mistral: Few-shot, CoT Selection, Self-Discover, MoR, TypedThinker; ablations and cross-domain meta-thinker training (logical-only, math-only, unified).",
            "task_or_benchmark": "LogiQA, BBH, GSM8K, MATH (same splits as Mistral experiments).",
            "performance_results": "From Table 1 (single-response): Few-shot avg 0.428 (LogiQA 0.581, BBH 0.359, GSM8K 0.581, MATH 0.193). TypedThinker single-response avg 0.481 (LogiQA 0.599, BBH 0.543, GSM8K 0.585, MATH 0.195). With +SC@5 TypedThinker reaches (LogiQA 0.637, BBH 0.591, GSM8K 0.753, MATH 0.267, Avg 0.562). Paper claims ~6.5% improvement for LLaMA3-8B on logical & math tasks.",
            "qualitative_findings": "Stronger LLaMA3 models benefit more from typed reasoning and finetuned reasoners; weighted voting on effective set sometimes helps (e.g., LogiQA, GSM8K) but accurate f* selection + SC often outperforms weighted voting on benchmarks dominated by a single effective reasoning type (e.g., MATH dominated by inductive).",
            "explicit_comparison": true,
            "key_claims_or_conclusions": "More capable LLMs (LLaMA3) gain larger benefits from TypedThinker; accurate selection of reasoning type combined with SC provides the biggest gains when a dominant reasoning type exists for many instances.",
            "uuid": "e8308.2",
            "source_info": {
                "paper_title": "TypedThinker: Diversify Large Language Model Reasoning with Typed Thinking",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Qwen-2-7B",
            "name_full": "Qwen 2-7B Instruct",
            "brief_description": "A Qwen-series 7B instruction-tuned LLM evaluated as an additional backbone; TypedThinker yields notable improvements on reasoning benchmarks for this model as well.",
            "citation_title": "TYPEDTHINKER: DIVERSIFY LARGE LANGUAGE MODEL REASONING WITH TYPED THINKING",
            "mention_or_use": "use",
            "model_name": "Qwen 2-7B Instruct",
            "model_description": "Open-source Qwen family 7B-parameter instruction-tuned transformer used as backbone in experiments (results reported in appendix).",
            "reasoning_methods": [
                "Default CoT (often deductive)",
                "Typed reasoning (deductive/inductive/abductive/analogical) under TypedThinker",
                "Self-Consistency (SC)",
                "Retrieval and weighted vote"
            ],
            "reasoning_methods_description": "Same pipeline: meta-thinker (finetuned on experiences) predicts effectiveness scores; explicit collection retrieved as few-shot demos; reasoner instructed to follow a type; sampling and SC applied similarly.",
            "reasoning_diversity": "both",
            "reasoning_diversity_experimental_setup": "Qwen evaluated with same baselines and TypedThinker variants (details in Appendix A.5.1). Reported comparisons include single-response and +SC@5 majority-vote settings.",
            "task_or_benchmark": "LogiQA, BBH, GSM8K, MATH (Qwen-specific table in appendix).",
            "performance_results": "Paper reports approximately a 7% improvement of TypedThinker over few-shot baseline for Qwen-2-7B across single-generation and +SC@5 settings (exact per-benchmark numbers provided in appendix Table 11).",
            "qualitative_findings": "TypedThinker generalizes across different open-source 7B LLMs including Qwen; improvements consistent with those for Mistral and LLaMA3, indicating the method's generality.",
            "explicit_comparison": true,
            "key_claims_or_conclusions": "TypedThinker improves smaller open-source LLMs like Qwen 2-7B by combining meta-type prediction and type-specific demonstrations; similar qualitative effects (unique-solution-by-type, retrieval helpful for logic) as observed for other backbones.",
            "uuid": "e8308.3",
            "source_info": {
                "paper_title": "TypedThinker: Diversify Large Language Model Reasoning with Typed Thinking",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "GPT-4o (facilitated)",
            "name_full": "GPT-4o (facilitated with Mistral-trained meta-thinker & collection)",
            "brief_description": "A large commercial LLM (GPT-4o) that was not finetuned in this work but was evaluated by using the authors' finetuned meta-thinker and explicit collection to guide generation.",
            "citation_title": "TYPEDTHINKER: DIVERSIFY LARGE LANGUAGE MODEL REASONING WITH TYPED THINKING",
            "mention_or_use": "use",
            "model_name": "GPT-4o (facilitated)",
            "model_description": "A very large-capacity LLM (GPT-4o) used in a transfer experiment: the finetuned Mistral-7B meta-thinker and explicit collection were used to select reasoning types and retrieve demonstrations, while GPT-4o served as the reasoner (no finetuning).",
            "reasoning_methods": [
                "Default high-capacity CoT",
                "Typed reasoning via retrieved demonstrations and meta-thinker predictions (no finetuning)",
                "Self-Consistency (SC)"
            ],
            "reasoning_methods_description": "The paper applied the Mistral-trained meta-thinker to propose effectiveness scores and retrieve demonstrations; GPT-4o was then prompted with selected type and retrieved demos to produce answers; SC@5 also evaluated.",
            "reasoning_diversity": "both (GPT-4o baseline already versatile; TypedThinker transfers typed reasoning to it)",
            "reasoning_diversity_experimental_setup": "Transfer evaluation: randomly sample 100 examples per benchmark and run GPT-4o with meta-thinker/retrieval guidance; compare baseline GPT-4o, GPT-4o +SC, TypedThinker-guided GPT-4o, and TypedThinker +SC.",
            "task_or_benchmark": "LogiQA, BBH, GSM8K, MATH (100 examples per benchmark in transfer eval).",
            "performance_results": "From Table 5: GPT-4o baseline (single) LogiQA 0.76, BBH 0.84, GSM8K 0.97, MATH 0.89. TypedThinker-guided GPT-4o single: LogiQA 0.80, BBH 0.86, GSM8K 0.95, MATH 0.88. With SC@5 the best reported: 0.81 / 0.90 / 0.96 / 0.91. Gains are modest since GPT-4o strong baseline leaves little headroom.",
            "qualitative_findings": "Even when using a stronger LLM without further finetuning, applying a meta-thinker and retrieved type-specific demonstrations can provide small but measurable improvements on logic benchmarks; gains on math are limited due to already very high GPT-4o performance.",
            "explicit_comparison": true,
            "key_claims_or_conclusions": "TypedThinker components (meta-thinker + explicit collection) can transfer to larger LLMs without additional finetuning and still provide incremental improvements, notably on logical reasoning tasks.",
            "uuid": "e8308.4",
            "source_info": {
                "paper_title": "TypedThinker: Diversify Large Language Model Reasoning with Typed Thinking",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "MetaMath (facilitated)",
            "name_full": "MetaMath (7B math-specific model facilitated with TypedThinker)",
            "brief_description": "A Mistral-7B-based math-specialized model trained with synthesized math data; evaluated with and without TypedThinker retrieval & selection to test transferability.",
            "citation_title": "TYPEDTHINKER: DIVERSIFY LARGE LANGUAGE MODEL REASONING WITH TYPED THINKING",
            "mention_or_use": "use",
            "model_name": "MetaMath (7B)",
            "model_description": "A math-specialized 7B LLM (Mistral-based) trained on ~400k synthesized math data distilled from GPT-3.5-Turbo; used as a reasoner in transfer experiments with the authors' meta-thinker and explicit collection.",
            "reasoning_methods": [
                "Math-focused CoT",
                "Typed reasoning (guided by externally-trained meta-thinker and demonstrations)",
                "Self-Consistency (SC)"
            ],
            "reasoning_methods_description": "MetaMath used as reasoner without finetuning; it received meta-thinker outputs and retrieved demonstrations to prompt type-specific reasoning; SC@5 majority voting also reported.",
            "reasoning_diversity": "both",
            "reasoning_diversity_experimental_setup": "Full test set evaluated; compared MetaMath baseline, MetaMath +SC, MetaMath with TypedThinker retrieval & selection, and TypedThinker +SC.",
            "task_or_benchmark": "GSM8K and MATH (full test sets for MetaMath experiments).",
            "performance_results": "From Table 6: MetaMath baseline GSM8K 0.690, MATH 0.209. MetaMath +SC improves slightly. TypedThinker single-response: GSM8K 0.696, MATH 0.220. TypedThinker +SC@5 yields GSM8K 0.736 and MATH 0.246 (improvements modest but consistent).",
            "qualitative_findings": "TypedThinker yields small improvements for a math-specialized LLM; for math tasks retrieval must be used carefully since retrieved numeric chains can mislead calculations—masking or avoiding direct numeric reuse is suggested.",
            "explicit_comparison": true,
            "key_claims_or_conclusions": "A domain-specialized model can still benefit from typed reasoning selection and demonstrations; however, retrieval of concrete numeric chains can harm math reasoning unless handled (e.g., masked computations).",
            "uuid": "e8308.5",
            "source_info": {
                "paper_title": "TypedThinker: Diversify Large Language Model Reasoning with Typed Thinking",
                "publication_date_yy_mm": "2024-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Self-consistency improves chain of thought reasoning in language models",
            "rating": 2,
            "sanitized_title": "selfconsistency_improves_chain_of_thought_reasoning_in_language_models"
        },
        {
            "paper_title": "Large language monkeys: Scaling inference compute with repeated sampling",
            "rating": 2,
            "sanitized_title": "large_language_monkeys_scaling_inference_compute_with_repeated_sampling"
        },
        {
            "paper_title": "Tree of Thought: Deliberate problem solving with large language models",
            "rating": 2,
            "sanitized_title": "tree_of_thought_deliberate_problem_solving_with_large_language_models"
        },
        {
            "paper_title": "Self-Discover: Large language models self-compose reasoning structures",
            "rating": 2,
            "sanitized_title": "selfdiscover_large_language_models_selfcompose_reasoning_structures"
        },
        {
            "paper_title": "Complex CoT (Complex Chain-of-Thought) or Complexity-based prompting for multi-step reasoning",
            "rating": 1,
            "sanitized_title": "complex_cot_complex_chainofthought_or_complexitybased_prompting_for_multistep_reasoning"
        }
    ],
    "cost": 0.0188755,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>TYPEDTHINKER: DIVERSIFY LARGE LANGUAGE MODEL REASONING WITH TYPED THINKING
23 Apr 2025</p>
<p>Danqing Wang danqingw@cs.cmu.edu 
Carnegie Mellon University</p>
<p>Jianxin Ma 
Qwen Team</p>
<p>Fei Fang 
Carnegie Mellon University</p>
<p>Lei Li 
Carnegie Mellon University</p>
<p>TYPEDTHINKER: DIVERSIFY LARGE LANGUAGE MODEL REASONING WITH TYPED THINKING
23 Apr 2025B70ECCFD22DC35525A561583563AC0C8arXiv:2410.01952v2[cs.CL]
Large Language Models (LLMs) have demonstrated strong reasoning capabilities in solving complex problems.However, current approaches primarily enhance reasoning through the elaboration of thoughts while neglecting the diversity of reasoning types.LLMs typically employ deductive reasoning, proceeding step-bystep from given conditions, which limits their exploration during problem-solving.Our analysis reveals that certain problems are exclusively solvable through specific reasoning strategies like inductive, abductive, or analogical reasoning.However, incorporating diverse reasoning approaches presents two key challenges: identifying the appropriate reasoning type for each problem and exploiting this approach during problem-solving.Therefore, we propose the TypedThinker that predicts suitable reasoning types based on the problem and their previous effectiveness and provides relevant demonstrations to guide LLMs in applying these strategies.Experimental results show significant improvements across multiple benchmarks, with performance gains of 3.4% for Mistral 7B, 6.5% for LLaMA3 8B, and 7% for Qwen 2 7B on logical and mathematical reasoning tasks.TypedThinker enhances LLM reasoning without requiring knowledge distillation from larger models.It can be integrated into more advanced systems like GPT-4o or specialized models like MetaMath to diversify their reasoning approaches and improve their problem-solving capabilities.</p>
<p>INTRODUCTION</p>
<p>Large Language Models (LLMs) exhibited promising capabilities in reasoning, such as solving logical reasoning and mathematical problems (Bai et al., 2022;OpenAI, 2023).Plenty of work has been done to improve the reasoning capabilities by adding reasoning thoughts (Wei et al., 2022) and making these thoughts more elaborated (Fu et al., 2023;Zheng et al., 2024).However, the exploration of diverse reasoning approaches remains severely limited.LLMs typically rely on a single reasoning pattern-usually deductive reasoning that proceeds step-by-step from given conditions.This narrow focus traps models in fixed thinking patterns, preventing them from solving problems that require different high-level reasoning approaches.</p>
<p>Current research fails to create truly diverse reasoning approaches.AlphaCode (Li et al., 2022;Leblond et al., 2023) tries to increase diversity by randomizing problem difficulty and tags.This method has limited scalability because it needs manual curation of attributes.It also works poorly beyond coding tasks.Increasing temperature settings offers another approach.This can generate outputs that appear different on the surface.Yet it rarely produces solutions with fundamentally different reasoning strategies.For example, repeated sampling (Brown et al., 2024) generated 100,000 solutions per problem using temperature 0.6.Their solutions 1 all follow the same basic approach.They start with problem conditions and work forward to deduce answers step-by-step.Humans, however, can use multiple reasoning strategies.One alternative approach is to propose a hypothesis first and then verify this hypothesis within the problem context.Current methods do not capture this diversity of thinking patterns.</p>
<p>Similarly, encouraging LLMs to explore diverse high-level thinking patterns leads to more varied solutions.Human cognitive research (Halpern, 2014;Bronkhorst et al., 2020) reveals multiple problem-solving approaches beyond deduction, including inductive (Flach and Kakas, 2000), abductive (Douven, 2011), and analogical reasoning (Bartha, 2013).These non-deductive strategies offer different directions through the solution space, often leading to more effective results.For example, abductive reasoning-proposing hypotheses and verifying them-is more suitable in multiple-choice scenarios.Humans demonstrate remarkable flexibility by switching from deductive to abductive reasoning when a free-response problem is reformatted with answer choices, despite unchanged problem content.This adaptive behavior shows how humans intuitively select optimal reasoning strategies for each problem format.</p>
<p>To examine how reasoning types affect LLM performance, we explicitly directed LLMs to apply specific reasoning strategies to problems.For each problem, the model is asked to follow a specific reasoning type and generate multiple solutions.We considered a problem "solvable" by a reasoning type if at least one of these solutions is correct.We calculate the percentage of problems solvable exclusively by one particular reasoning type.These represent cases where lacking the right reasoning approach makes problems nearly impossible to solve.We tested Mistral 7B instruct (Jiang et al., 2023) across four benchmarks: LogiQA (Liu et al., 2023a), BBH (Suzgun et al., 2022), GSM8k (Cobbe et al., 2021), and MATH (Hendrycks et al., 2021).Results in Figure 1 reveal that each reasoning type uniquely solves certain problems that other approaches cannot.This demonstrates that incorporating diverse reasoning strategies effectively expands the range of solvable problems.More detailed analysis is put in Section 4.2.However, incorporating reasoning types into LLM problem-solving faces two major challenges.First, identifying the right reasoning type is difficult.Figure 1 shows that incorrect reasoning types mislead LLMs with wrong thinking directions.Second, ensuring the model follows the chosen reasoning type during problem-solving is crucial.Therefore, we propose TypedThinker to address these challenges.It predicts suitable reasoning types based on previous successful experiences and uses explicit demonstrations to help LLMs effectively apply them.TypedThinker consists of three key components: the LLM reasoner, the meta-thinker and demonstration collection.The meta-thinker is fine-tuned on empirical effectiveness scores of each reasoning type from the training set.We use rejection sampling to collect successful solutions for each type as demonstrations, which enhance LLMs' ability to exploit specific reasoning strategies.During inference, TypedThinker uses the meta-thinker to identify the most suitable reasoning type and retrieves relevant demonstrations to guide the LLM reasoner in applying this approach to solve the problem.</p>
<p>Experimental results show that TypedThinker improves Mistral 7B instruct by 3.4%, LLaMA3 8B instruct (Touvron et al., 2023) by 6.5%, and Qwen 2 7B by 7% on two logical benchmarks and two mathematics benchmarks.We further demonstrate that TypedThinker can directly be applied to the benchmark Contexthub (Hua et al., 2024) and outperforms other baselines.Moreover, we show that the meta-thinker can be adapted in much larger LLMs such as GPT-4o or domain-specific LLMs such as MetaMath (Yu et al., 2023) and enhance their reasoning.</p>
<p>more creative and open-ended, involves forming hypotheses to explain observations, often generating the most plausible explanation rather than a guaranteed conclusion (Douven, 2011).Analogical reasoning is concerned with the comparison between two or more objects and drawing a conclusion based on the similarity (Bartha, 2013).Previous LLMs studies on logical reasoning mainly focus on benchmarking its performance in different reasoning types (Bang et al., 2023;Dougrez-Lewis et al., 2024;Luo et al., 2023;Yu et al., 2024), or applying one reasoning type to solve the corresponding reasoning problems, such as using inductive reasoning for inductive reasoning problems (Wang et al., 2023a;Shao et al., 2024;Yang et al., 2024).Instead, this paper mainly focuses on the selection and application of the appropriate reasoning type when solving a general logic or math problem.</p>
<p>Reasoning in Large Language Models Plenty of studies have been done to enhance the reasoning capability of LLMs.Chain-of-thoughts methods focus on creating better instructions to improve the quality of the reasoning process, such as Complex CoT (Fu et al., 2023), Tree of Thought (ToT) (Yao et al., 2023) and Graph of Thought (Besta et al., 2024).Refinement-based methods revise LLMs solutions by the feedback from themselves or others model (Akyürek et al., 2023;Wang and Li, 2023).Search-based methods use the reward model to search the best reasoning path (Lightman et al.; Liu et al., 2023b;Hao et al., 2023).While most focus on creating high-quality reasoning paths, the diversity of thinking attracted more attention recently.Studies have investigated the diversity brought by repeated sampling (Brown et al., 2024) or multi-agent discussion with different prompts (Du et al., 2023;Liang et al., 2023;Suzgun and Kalai, 2024).Our paper aims to diversify thinking by incorporating suitable reasoning types for each instance.</p>
<p>Self-improvement and Self-training in LLMs Recent works explore the self-improvement capability of LLMs, by finetuning LLMs on their high-quality generations (Wang et al., 2023b;Huang et al., 2023;Toshniwal et al., 2024).This process can be extended to multiple iterations Gülçehre et al. (2023); Aksitov et al.. Benefiting the LLMs' ability to follow instructions, researchers also ask LLMs to provide feedback themselves and improve their responses without finetuning (Peng et al., 2023;Shinn et al., 2023).This can be further enhanced by using their own feedback as the reward model to provide better signals for finetuning (Yuan et al., 2024;Kumar et al., 2024).In this paper, we focus on stimulating their capabilities to conduct various reasoning types and use these experiences to diversify their thinking in reasoning type selection and following.</p>
<p>TYPEDTHINKER: DIVERSIFY THINKING WITH TYPED REASONING</p>
<p>In this paper, we focus on four logical reasoning types: deductive, inductive, abductive, and analogical reasoning defined in (Nunes, 2012).For each reasoning type, we provide a short definition and a simple example to demonstrate the inference rules, which are listed in Table 8 in the Appendix.Based on that, we introduce a reasoning framework TypedThinker to diversify LLMs' thinking with different reasoning types.As shown in Figure 2, there are three components in TypedThinker: the meta-thinker to select reasoning type, explicit collection for demonstration, and the LLM reasoner to exploit one particular reasoning type.TypedThinker optimizes the implicit policy of the meta-thinker and updates the explicit collection of demonstration based on previous experiences.</p>
<p>TYPING REASONING WITH IMPLICIT POLICY AND EXPLICIT DEMONSTRATION</p>
<p>Let D = {(x 1 , y 1 ), • • • , (x N , y N )} be a set of N problems, where x i and y i is the problem and the ground-truth answer of the i−th instance.We define a reasoning type space F that includes an empty type and four types of reasoning: deductive, inductive, abductive, and analogical.The goal is to model the selection and implementation of various reasoning types as implicit and explicit collection of demonstration, thus enhancing LLMs' performance in reasoning tasks.</p>
<p>Meta-thinker for the reasoning type identification.Given a problem x, the goal of the meta-thinker is to select an appropriate set of reasoning types to solve the problem.Specifically, it predicts an effectiveness score s k ∈ [0, 1] for each reasoning type f k ∈ F, which can be represented as s x,k = π θ (x, f k ).s x,k = 0 indicates that the problem x can hardly be solved by the reasoning type f k with a limited sampling times2 .Note that the effectiveness scores of different types are independent of each other and their sums are not necessary to be 1.The most effective reasoning type is defined as f * (x) = arg max f k ∈F s x,k .Meanwhile, we can obtain a set of reasoning types with a non-zero effective ratio, which we call the effective set:
F (x) = {f k |s x,k &gt; 0}.
We initialize π θ with a pre-trained LLM.The prompt is listed in Appendix A.1.</p>
<p>Explicit collection of demonstration TypedThinker collects a set of demonstration M = f k ∈F M k for each type of reasoning.For each problem in the training set, we keep one correct solution per reasoning type if applicable, resulting in a set of at most |D| × |F| solutions.If multiple solutions exist for one problem, we keep the longest to get a more detailed context.The entry in the collection of demonstration M k is represented as a tuple of (x k r , sol k r ).Here sol k r is the concrete reasoning process of the reasoning type f k , including the predicted answer.During inference stage, given a new problem x and its reasoning type f k , it retrieves a set of relevant experience
d k x = {(x k r , sol k r ) ∈ M k |L(x k r , x) &lt; δ}.
L is the distance function measuring relevancy between two problems and δ ∈ [0, 1] is the relevancy threshold.We use the cosine similarity between the semantic embeddings as the distance function.The retrieved experiences are used as the few-shot examples of the reasoner.</p>
<p>Reasoner to perform the reasoning according to the type.The reasoner applies the reasoning type f k to the problem x and provides a detailed reasoning path for its predicted answer ŷ.The reasoner is based on LLM to conduct reasoning and the instruction is composed of (x, f k , d k ), where the d k is the retrieved relevant successful experience.The reasoner can be further optimized via instruction tuning to enhance the capability of conducting a specific type of reasoning.</p>
<p>To conclude, TypedThinker enhances LLMs' reasoning by estimating the effectiveness of each reasoning type and guiding the LLM reasoner with the demonstration of this reasoning type.Specifically, the meta-thinker π θ predicts an effective score s k for each reasoning type.TypedThinker then retrieves the most relevant reasoning demonstration d k from the fixed explicit collection corresponding to the reasoning type f k .Finally, the reasoners conduct the specific type of reasoning f k with the guidance of demonstration.In our experiments, we use two approaches to decide the reasoning types used in the problem-solving: One is to greedily resample several times based on the most effective reasoning type f * and use self-consistency (Wang et al., 2022) to enhance the answer; the other is to sample solutions for all effective reasoning types F , and apply a weighted vote with the effective score as the coefficient.By default, we use the greedy approach for TypedThinker, and we discuss the weighted vote in Section 4.4.</p>
<p>OPTIMIZE IMPLICIT POLICY FOR REASONING TYPE SELECTION AND EXPLOITING</p>
<p>We optimize the meta-thinker π θ and the reasoner while updating the explicit collection of demonstration with the collected experience.The pipeline is demonstrated in Figure 3.The green lines represent the parametric optimization process, while the blue line represents the non-parameter update.</p>
<p>Diversify Reasoning Experiences with Types</p>
<p>To inspire LLMs' knowledge of solving problems with different reasoning types, the definition (Table 8) and manually-written few-shot examples with detailed reasoning paths (Table 16) are used for prompting solutions for each reasoning type.For each problem in the training set, we use a temperature of 1 to sample 10 solutions per reasoning type.These solutions are then filtered by the correctness of the final answers.To guarantee that these solutions belong to their reasoning type, we apply a reverse check on the remaining solutions.For the experience (x, sol, y) of the reasoning type f k , we prompt the model to predict its reasoning type fk .If f k = fk , we think this experience indicates the methodology of this reasoning type and keep it.Otherwise, it will be removed.Finally, we collect an experience dataset D with multiple types of reasoning.The experiences are grouped by their reasoning type and are stored in the explicit collection of demonstration M .</p>
<p>Optimize the Implicit policy of Meta-thinker and Reasoner Given a problem x, the meta-thinker π θ predicts a score s x,k to indicate how likely this reasoning type can solve this problem.This can be estimated by the experience in the training set.We assume that if one reasoning type is more effective in solving this problem, it will generate more correct solutions among the same sampling times.Therefore, given there are n k successful experiences of the reasoning type f k among m samples, we define the empirical effectiveness score based on its success rate: s x,k = n k /m.This empirical effectiveness score calculated on the experience dataset D is then used for finetuning the meta-thinker.We reconstruct the tuple (x, f k , s x,k ) into the instruction-following pair via the prompt in Section A.1 for supervised finetuning.Meanwhile, we finetune a reasoner with the experience to enhance its capability to conduct a specific type of reasoning.</p>
<p>EXPERIMENTS</p>
<p>EXPERIMENT SETUP</p>
<p>We investigate two open-source LLMs Mistral 7B instruct (Jiang et al., 2023) and LLaMA3 8B instruct (Touvron et al., 2023) and Qwen 2-7B-Instruct (Bai et al., 2023) on two logical benchmarks (LogiQA, BBH) and two mathematics benchmarks (GSM8K and MATH).For each LLM, we set up the following baselines: (i) Few-shot baseline with 3 in-context examples.We use the few-shot examples provided in Suzgun et al. (2022) for BBH, and text-based few-shot examples in Toshniwal et al. (2024) for GSM8k and MATH since we do not consider the code interpreter in this paper.We also manually write few-shot examples for LogiQA, (ii) CoT Selection: Select the best reasoning type by prompting.We let the LLM identify the best reasoning type and then apply the selected type to the problem.(iii) Self-Discover (Zhou et al., 2024) generates a task-level reasoning structure by prompting LLMs to select relevant modules from a list of seed modules and adapt the selected module to task-specific descriptions.We follow their official implementation3 and use the backbone LLMs to generate one reasoning structure from an exemplar training instance of each task.This reasoning structure is then applied to all instances in this task.(iv) Zero-shot Mixture of Reasoning (MoR): apply all possible reasoning types and use the majority vote to get the final answer4 .The LLM is instructed with the definition and demonstration in Table 8.The temperature is set to 0.7 for all baselines as suggested by Wang et al. (2022).The maximum output length is set to 1000 tokens.We use SentenceTransformer5 (Reimers and Gurevych, 2019) to retrieve top-3 similar experiences and the threshold is set to δ = 0.5.We use accuracy as the measurement of task performance.The model response is compared with the ground truth based on the exact match for logic problems.The script in Toshniwal et al. (2024) calculates the mathematical equivalent of mathematics benchmarks.The training details are put in Appendix A.3.</p>
<p>HOW DO REASONING TYPES ENCOURAGE DIVERGENT THINKING DURING GENERATION?</p>
<p>We first investigate the role of reasoning types in LLMs' self-training.We group problems of the collected experience dataset D based on their empirical effective set F (x) = {f k |s x,k &gt; 0}, and the empirical effectiveness score is defined in Section 3.2.The problems in the same group can be solved with the same set of reasoning types.We focus on the effective set with only one reasoning type and count the size of these groups.The size indicates how many problems that can only be solved by one specific reasoning type, showing the advantage of including this reasoning type.We illustrate their percentage on the whole dataset in Figure 1.We find that even if we use temperature = 1 to sample 10 times to diversify the solutions, a lot of problems still have only one effective reasoning type.It indicates that given an inappropriate reasoning type, the diversity brought by repeated sampling with a high temperature cannot help the LLM solve this problem.</p>
<p>Meanwhile, although these reasoning types have similar performance on the whole dataset (shown in Table 6 in Appendix), the problems they can solve do not completely overlap.None of the percentages of the reasoning type is zero, indicating that for each reasoning type, there is a unique set of problems that can only solved by it.It indicates that these reasoning types have their advantages over different problems, highlighting the importance of considering the appropriate reasoning types during problem-solving.</p>
<p>We further compare the diversity of the solutions before and after adding the reasoning types in Table 10 of Appendix A.4.We can find that introducing different reasoning types can bring more diversity to the solution set than repeated sampling with a high temperature.Appropriate reasoning types improve the reasoning performance.The main difference between Fewshot and CoT Selection without the majority vote is the reasoning type selection.For CoT Selection, the model is first prompted to predict a reasoning type and then apply it, while the Fewshot baseline directly solves the problem.However, we find that the CoT Selection struggles with the reasoning type selection.Given the option to choose from four reasoning types or none, it chooses none over 60% of the time.The rest of the time, it selects more than 50% deductive, while only 34% of them can be effectively solved by deductive reasoning during the sampling.</p>
<p>The mismatch in reasoning types results in poor performance.Facilitating with a trained metathinker, TypedThinker is more accurate in selecting the reasoning type, which helps it improve performance under the single response setting.Self-Discover, which uses a shared reasoning structure for all instances of the task, performs poorly, especially for the weaker model Mistral 7B.This may be due to the difficulty these models face in identifying a reusable shared high-level reasoning structure for diverse instances, especially in datasets like LogiQA, where reasoning structures are highly varied.</p>
<p>Precise prediction is more effective than an inappropriate mixture.The zero-shot MoR and few-shot MoR apply all types of reasoning to the given problem and use a majority vote to get the final answer.Compared with the other two majority vote baselines Few-shot + SC @5 and CoT Selection + SC @5, these methods fall behind on several benchmarks, especially on MATH.We find that the performance drop typically happens when there are only one or two reasoning types that are effective for this problem.In such cases, the majority of incorrect answers dominate, resulting in fewer votes for the correct one.As we can see in Figure 1, plenty of problems on the MATH benchmark can only be solved by inductive reasoning.In such cases, if the CoT Selection correctly predicts the inductive reasoning for them, the CoT Selection + SC @5 can benefit from the majority vote and have a better performance.This highlights the importance of predicting the effectiveness of reasoning types before aggregating them.</p>
<p>Experience of how to conduct a specific type of reasoning is important.The performance difference between zero-shot and few-shot MoR illustrates the impact of the reasoning demonstration.</p>
<p>When prompted solely with the definition, LLMs struggle to understand how to apply the reasoning type to specific problems.It can be improved by human-written few-shot examples in few-shot MoR.However, it still falls behind the non-parametric retrieval and the parametric reasoner in TypedThinker, both of which enhance the capability of conducting a specific reasoning type.Additionally, poor performance in Self-Discover also indicates that, without demonstration, the complex reasoning structures will introduce excessive complexity, confusing the models.We conduct several investigations to enhance the understanding of our proposed method.</p>
<p>Ablation study</p>
<p>The ablation studies are conducted on three key components in TypedThinker.</p>
<p>Each time one component is removed.It includes (i) w/o Fine-tuned Reasoner: it is replaced with the base LLM (ii) w/o Meta-thinker: it is replaced with a CoT selection (iii) w/o collection of demonstration: the explicit collection is replaced with the human-written few-shot examples of each reasoning type.In Table 2, we can find the meta-thinker is the most important module for the math benchmarks, while the explicit collection is more effective on two logical benchmarks.The finetuned reasoner also contributes a lot to the performance improvement.We also observe that explicit collection does not always bring benefits: the performance on MATH even increases when we remove it.We find that the retrieved examples usually have a similar context but different numbers.The math calculation in the retrieved chain-of-thoughts solutions will mislead the reasoner.This is consistent with the observations of Toshniwal et al. ( 2024) that the solutions with masked computations are more beneficial to the math problems.For logical problems, there are fewer calculations and the retrieved solutions focus more on the reasoning process.</p>
<p>Meta-thinker's predictions achieve a high correlation with the empirical effectiveness score.We evaluate the performance of the meta-thinker by the correlation between the predicted effectiveness score and the empirical one (which we view as the ground truth).We split the collected experience dataset D by problems and use 0.9 of them to train the meta-thinker and 0.1 for testing.We use Kendall's τ coefficient to evaluate the correlation.It measures rank correlation, essentially assessing the similarity of orderings when data is ranked.A higher Kendall's τ coefficient indicates that when the ground truth assigns a high effectiveness score to a reasoning type, the meta-thinker also ranks it high, thereby validating the reliability of the predicted scores.We compare the performance under three settings: the meta-thinker trained only on the logical domain, only in the math domain, and jointly trained on the unified domains (including both logic and math data).The meta-thinker trained on the unified domain achieves the highest correlation.This suggests that training on a dataset with multiple domains enhances the meta-thinker's ability to accurately rank and predict suitable reasoning types, thereby improving its overall performance.We also calculate the accuracy between the predicted optimal reasoning type and the empirical one for the unified setting.The average accuracy on four benchmarks is 68.3% (LogiQA 75.4%, BBH 75.6%, GSM8k 72.1%, and MATH 47.7%).Note that the meta-thinker can predict an incorrect optimal reasoning type f k while still generating a correct solution.It is because the predicted reasoning type can belong to the effective set F = {f k |s x,k &gt; 0}, indicating the reasoning type can also help solve the problem.</p>
<p>Unified meta-thinkers perform well in most cases.We further investigate the effectiveness of these policies by facilitating TypedThinker with these meta-thinkers.The results are based on the Mistral 7B TypedThinker without SC.The results in Figure 5 show that the unified meta-thinker has the best performance in most cases.However, in the more difficult MATH dataset, the specific meta-thinker trained in the math domain can help it be more powerful.To conclude, the unified meta-thinker has reasonable performance in all its domains, while for difficult problems it may slightly underperform the specific meta-thinker trained in this domain.Optimal reasoning type v.s.weighted vote on the effective set In the main experiment, we use the optimal reasoning type f * which has the highest effectiveness score for reasoning.As discussed in Section 3.1, we can also use a majority vote on the effective set F with the effectiveness score as the coefficient.Specifically, if one solution is based on a reasoning type with a higher effectiveness score, its vote gets a larger weight.The results are shown in Table 3.We can see that the weighted vote can balance different reasoning types on LogiQA and GSM8k for the Mistral-7B-based model.However, on the other two benchmarks, the TypedThinker + SC @5 has a better performance.It indicates that accurate selection is more important if one or two reasoning types dominate the benchmark.For example, as we have shown in Figure 1, there are a lot of problems that can only be solved by inductive reasoning, indicating the other types will mislead the final answer.In such cases, the self-consistency of inductive reasoning is more powerful than the weighted vote.However, when we have a more accurate meta-thinker that can identify the correct reasoning type and a more powerful reasoner that can follow the specific reasoning type, for example, models initialized by LLaMA3, the advantage of TypedThinker + SC is more obvious.</p>
<p>CAN TY P E DTH I N K E R BE APPLIED TO NEW DOMAINS OR NEW LLMS WITHOUT FINETUNING?</p>
<p>It is essential to evaluate the generalization capability of TypedThinker.We assess it from two aspects: (i) TypedThinker's performance on new domains; and (ii) other LLMs' performance after facilitated with our finetuned meta-thinker and the explicit collection of demonstration.</p>
<p>TypedThinker generalizes well to the unseen domain.We use a new propositional logic benchmark Contexthub (Hua et al., 2024) for evaluation.It is a recently released dataset, which has never been seen by Mistral and LLaMA3 models during the pre-training.It contains problems from 12 categories with 4 levels of difficulty.We select the difficulty of level 4 to test the complex logic reasoning capabilities.We use the experiences collected from LogiQA as the explicit collection of demonstration.The meta-thinker and the reasoner are fine-tuned on four training benchmarks.The results in Table 4 show that TypedThinker outperforms other baselines on this unseen domain as well, indicating that it can generalize well to new domains.One interesting thing is Mistral 7B baselines significantly outperform LLaMA3 8B on this benchmark and its superior capabilities make it benefit more from our TypedThinker.</p>
<p>Facilitating LLMs with TypedThinker makes them more powerful.Our TypedThinker framework is orthogonal to the backbone LLMs and can be adapted to new LLMs.There are two ways to use a new LLM in the TypedThinker framework: one is to conduct the self-training process, like the two LLMs used in our main experiments (Mistral 7B and LLaMA3 8B); the other is to use our fine-tuned meta-thinker for reasoning type selection and the explicit collection of demonstration for retrieval while using the new LLM as the reasoner without finetuning.The first way can make the LLM more powerful (as shown in the performance comparison between TypedThinker and TypedThinker w/o Finetuned Reasoner in Table 2), but the latter one is more flexible.Here we use the second way to evaluate the direct transferability to new LLMs.We choose one of the most powerful LLMs GPT-4o and one math-specific 7B model MetaMath (Yu et al., 2023).MetaMath is a Mistral-7B-based model trained with more than 400k synthesized math data distilled from GPT-3.5-Turbo.We randomly sample 100 examples from each benchmark for GPT-4o.For MetaMath, we use the whole test set.The results are shown in Table 5 and Table 6.</p>
<p>Compared with Mistral 7B in Table 1, the high-quality and large scale of synthesized data from GPT-3.5-Turbo enhances MetaMath's capabilities in math problems.TypedThinker can further improve its performance by reasoning type selection and explicit collection.Meanwhile, although the superior performance of GPT-4o on two math datasets leaves little space for improvement, the results on logic benchmarks (LogiQA and BBH) demonstrate that the meta-thinker trained with the small 7B model also enhances its performance.These findings confirm that our approach is not only effective in improving smaller LLMs but also transferable to larger models, further validating the generalization capability of TypedThinker.</p>
<p>CASE STUDY</p>
<p>Here is one example of TypedThinker on the LogiQA benchmark in Table 7.This problem states a phenomenon that a higher altitude leads to a lower atmospheric pressure.Based on this observation, it is easy for humans to use inductive reasoning and get a general conclusion about the inverse cause-and-effect relationship.It is also natural for humans to use analogical reasoning and find the most similar options.The meta-thinker gives the highest effectiveness score for inductive reasoning, which is then chosen as the optimal reasoning type f * = inductive.Effectiveness scores are all larger than 0, so the effective set is all reasoning types.The reasoner gets the correct answer for deductive and inductive reasoning while doing wrong on the other reasoning types.If we use the majority vote over five answers, (A) and (C) will have the same votes, indicating that there is a 50% chance to be correct6 .However, with the effectiveness score predicted by the meta-thinker, TypedThinker can get the correct answer either by applying the optimal reasoning type or using the weighted vote on the four answers.Besides, without a specific reasoning type (which is 'Empty'), the model cannot arrive at the correct answer.This shows the limitation of the common few-shot baselines.It shows that TypedThinker improves the reasoning performance by the introduction of diverse reasoning types and the capability of selecting the appropriate type to apply.(C) The older a tree is, the more rings it has.The age of the locust tree in A's yard is older than that of B's family, so the locust tree of A's family has more rings than B's.</p>
<p>(D) The greater the vocabulary of a language, the more difficult it is to learn.English is harder to learn than Italian, so English has a larger vocabulary than Italian.</p>
<p>Ground</p>
<p>CONCLUSION AND LIMITATION</p>
<p>We investigate how reasoning types diversify LLMs' thinking and propose TypedThinker to incorporate different reasoning types into problem-solving.TypedThinker is inspired by human cognition processes during reasoning: it learns an implicit policy to select the appropriate reasoning types with the meta-thinker and to apply the selected type of reasoning with the reasoner.It also maintains an explicit memory to retrieve experiences to aid reasoning.The results show that TypedThinker enhances the reasoning capabilities of Mistral 7B, LLaMA3 8B and Qwen 2 7B on four benchmarks.Furthermore, TypedThinker shows good generalization capabilities in new domains and models.</p>
<p>Despite the promising results, TypedThinker has several limitations that need further investigation.Firstly, one problem may require different reasoning types at different steps, and applying one sole reasoning type can hardly find a correct solution.In that case, dividing the problems into multiple reasoning steps, and applying TypedThinker for each step could make the reasoning more diverse and effective.Additionally, this paper mainly focuses on logical and mathematical benchmarks.</p>
<p>Expanding to a broader range of tasks, such as code generation and creative problem-solving, could deepen the understanding of the role of reasoning types in various problems and provide a more comprehensive assessment of TypedThinker's capabilities.BBH (Suzgun et al., 2022) is a set of hard problems borrowed from Big Bench (Srivastava et al., 2022).They are also formatted as multi-choice problems.We pick the English tasks with more than 2 options, resulting in 16 tasks: date understanding disambiguation qa, geometric shapes, hyperbaton, logical deduction three, logical deduction five, logical deduction seven, movie recommendation, penguins in a Contexthub (Hua et al., 2024) is a new propositional logic benchmark.It contains abstract and contextualized logical problems from 12 categories with 4 levels of difficulty (Zhu et al., 2024).We follow the standard split of the original paper and use the subset of difficult level 4 to test the complex logic reasoning capabilities.The abstract logical problems only contain the symbolic variable without natural language description, which can be viewed as symbolic reasoning problems.</p>
<p>Livebench (White et al., 2024) is a recently proposed benchmark with 18 diverse tasks across 6 categories, specifically designed to minimize data contamination.All problems have verifiable, objective ground-truth answers, allowing hard questions to be scored accurately and automatically.We evaluate our models on three tasks (spatial, web of lies v2, zebra puzzle) from the reasoning category, splitting them 0.7/0.3 for training and testing.</p>
<p>A.2.2 DATASET EXAMPLES</p>
<p>We demonstrate one example for each dataset below.</p>
<p>A.3 TRAINING DETAILS</p>
<p>For self-training of TypedThinker, we use the splits in the original papers for LogiQA and follow the split of Toshniwal et al. (2024) for GSM8k and MATH.For BBH, we utilize 16 English multiple-choice tasks and randomly select 100 examples per task as the test set, with 20 examples as the hold-out validation set.The detailed statistics are listed in Table 9.Finally, the curated generation dataset covers 67.2% problems on the LogiQA benchmark, 69.7% on BBH, 74.88% on GSM8k, and 36.27% on MATH.We finetune a unified meta-thinker for both math and logical problems, and a unified reasoner for all reasoning types.We use Huggingface (Wolf et al., 2019) with deepspeed (Rasley et al., 2020).The finetuning is conducted on 2 A6000 GPUs.The batch size is 64 and the learning rate is 1e − 5.The maximum epoch is 3 for the meta-thinker and 2 for the reasoner.</p>
<p>A.4 ANALYSIS OF TYPED REASONING</p>
<p>Accuracy of Typed Reasoning We calculate the accuracy for each reasoning type on our empirical dataset D, shown in Figure 6.We can find that on LogiQA and MATH, the accuracy of different reasoning types is similar.However, deductive and analogical reasoning outperform the other two on BBH while inductive and abductive reasoning are more effective.The results illustrate that after our carefully designed demonstration for each reasoning type, LLM's capabilities in other reasoning types achieve comparable performance with deductive reasoning.This ensures the quality and the balance of our collected dataset on each reasoning type.</p>
<p>Comparing Figure 1 and Figure 6, we can see that if correctly selected, the specific reasoning type can enhance the model performance by handling problems that cannot be solved by other reasoning types, such as inductive on MATH.However, the unsuitable reasoning type can also mislead the model, leading to poor performance.Diversity of Typed Reasoning To further verify whether the reasoning types can make the solutions more diverse, we compare the diversity between solutions under different sampling settings in Table 10.We use Levenshtein Distance (Levenshtein, 1966) and the n-gram overlaps between sentences to evaluate diversity.Specifically, for K generations G = {g 1 , • • • , g K } of the same problem, we calculate the distance between each pair and normalize them with the sentence length.Then the average distance over these paired results is used as the distance of these K generations.If we denote the normalized Levenshtein Distance function as f ld , this process can be represented as:
f ld (G) = 2 K(K − 1) K i=0 K j=i+1
f ld (g i , g j ).</p>
<p>(1)</p>
<p>The calculation of the n-gram overlap is defined in the same way.For each setting, we present the average score over the problems in the test set in  We further evaluate our methods on LiveBench (White et al., 2024) to test the generalization capability of our method.The experimental settings are the same as described in Section 4.5.The results are shown in Table 12.It demonstrates that TypedThinker outperforms other baselines, further supporting its generalization capability across diverse tasks.</p>
<p>A.5.3 MORE ABLATION STUDIES</p>
<p>The primary reason for comparing our method with the few-shot baseline is that fine-tuning for specific reasoning types is an integral part of our approach.Therefore, we evaluate the impact of our fine-tuned reasoner through a separate ablation study.However, comparing TypedThinker to few-shot baselines without fine-tuning may not fully account for the benefits of fine-tuning.Therefore, we conduct two more experiments to verify the influence of the fine-tuned LLMs.</p>
<p>Comparison with base LLM + one module Ablation studies in Section 4.4 investigate the contribution of each component by removing one component each time.Here we provide additional ablation results by adding one component to the base LLM each time, resulting in two variants: Base LLM + Meta-thinker and Base LLM + Collection.For a more reliable conclusion, we ran experiments three times to calculate the average and std and present the result in Table 13 and 14. Results show that the retrieval component improves performance on logical tasks but may mislead models on mathematical datasets.This is consistent with our findings in the ablation study in Table 2: the retrieved solutions with digits may mislead the model.Meanwhile, compared with the ICL reasoner, our finetuned reasoner shows better capability in identifying the suitable reasoning type.</p>
<p>A.6 DISCUSSION ON MORE REASONING PROBLEMS</p>
<p>In this paper, we mainly focus on logical and math reasoning problems.However, our TypedThinker can also be extended to symbolic or commonsense reasoning without extra ef-</p>
<p>Figure 1 :
1
Figure 1: The percentage of problems solvable exclusively by one reasoning type.</p>
<p>Figure 2 :Figure 3 :
23
Figure 2: TypedThinker consists of three components: the meta-thinker to select the reasoning types, the explicit collection of demonstrations to retrieve relevant experience, and the reasoner to conduct the specific reasoning.The meta-thinker is fine-tuned to predict an effective score s ∈ [0, 1] for each reasoning type.</p>
<p>(v) Few-shot MoR: Similar to the zero-shot MoR except for each reasoning type, 3 few-shot examples are provided in the prompt.(vi) TypedThinker: use the most effective reasoning type f * .The +SC baselines indicate the majority vote over 5 responses.</p>
<p>Figure 4 :Figure 5 :
45
Figure 4: Kendall's τ coefficient between the prediction confidence score with the ground truth.All results have the p-value &lt; 0.05.The unified policy shows the best correlation on all reasoning types.</p>
<p>Inductive: 0.5; Analogical: 0.4; Abductive: 0.4; Empty: 0.4 and their answers Deductive: (C); Inductive: (C); Analogical:(A); Abductive: NULL; Empty: (A) Model Output MoR: (A); TypedThinker with f * (C); TypedThinker with F : (C)</p>
<p>Figure 6 :
6
Figure 6: Accuracy of the solutions on different reasoning types.It indicates that the effectiveness of reasoning types varies in different problems.</p>
<p>Table 1 :
1
TypedThinker achieves the best performance in both single response and the majority vote setting on two logical benchmarks and two math benchmarks.@5indicates the result is based on the majority vote over 5 responses.+SCindicatestheself-consistencymethod.MoR indicates the Mixture of Reasoning, which employs all reasoning types (including an empty type) and votes for the final output.Avg.indicates the average accuracy over four benchmarks.Qwen's results are put in Table11.As we can see in Table1, our TypedThinker achieves the best performance among baselines.The improvement is more obvious in LLaMA3 8B, which is more powerful than Mistral 7B.It shows that LLMs with a better capability in reasoning and instruction-following can benefit more from the self-training of TypedThinker.Additionally, there are several key insights from the detailed comparison with different baselines.
Mistral 7BLLaMA3 8BLogiQA BBH GSM8K MATH Avg. LogiQA BBH GSM8K MATH Avg.Few-shot0.485 0.346 0.3690.074 0.318 0.581 0.359 0.5810.193 0.428+ SC @50.532 0.441 0.4440.136 0.388 0.579 0.391 0.7690.250 0.497CoT Selection0.474 0.361 0.3720.095 0.325 0.564 0.392 0.5560.181 0.423+ SC @50.503 0.429 0.4660.132 0.382 0.562 0.426 0.7850.222 0.499Self-Discover0.386 0.340 0.1410.056 0.231 0.493 0.425 0.5870.200 0.426+ SC @ 50.476 0.391 0.2080.082 0.289 0.540 0.543 0.7010.278 0.516Zero-shot MoR @5 0.528 0.414 0.3130.108 0.341 0.556 0.463 0.6660.189 0.468Few-shot MoR @5 0.509 0.456 0.4600.127 0.388 0.599 0.543 0.5850.195 0.481TypedThinker0.554 0.423 0.3860.092 0.364 0.599 0.543 0.5850.195 0.481+ SC @50.570 0.469 0.5000.149 0.422 0.637 0.591 0.7530.267 0.5624.3 WHAT KINDS OF BENEFITS CAN TY P E DTH I N K E R BRING?</p>
<p>Table 2 :
2
Ablation Study on the Mistral 7B based TypedThinker's components.We remove one component each time.The results are based on the best reasoning type and calculated for the single response per query.The negative scores indicate the performance drop, and the largest scores are shown in bold.
LogiQA BBH GSM8K MATHTypedThinker0.5540.4230.3860.092w/o Finetuned Reasoner-0.076-0.041-0.102-0.018w/o Meta-thinker-0.025-0.036-0.152-0.024w/o Memory-0.082-0.051-0.0330.0134.4 WHAT CONTRIBUTES TO TY P E DTH I N K E R 'S EFFECTIVENESS?</p>
<p>Table 3 :
3
TypedThinker's performance with the most effective reasoning type f * v.s.weighted votes on the effective set F .
Table 4: TypedThinker performs best onthe unseen benchmark Contexthub. Here theresults are based on the majority vote over 5responses (+SC @5).LogicQA BBH GSM8K Math AverageMistral 7BMistral 7B LLaMA3 8BSC @5 on f  *  weighted on F0.570 0.469 0.500 0.149 0.422 0.581 0.453 0.501 0.127 0.416Few-shot CoT Selection Zero-shot MoR0.419 0.415 0.4150.378 0.390 0.403LLaMA3 8BFew-shot MoR0.4320.390SC @5 on f  *0.637 0.591 0.753 0.267 0.562Self-Discover0.3320.365weighted on F0.619 0.587 0.738 0.245 0.547TypedThinker 0.4520.423</p>
<p>Table 5 :
5
GPT-4o's performance is improved with our metareasoner.We use the finetuned Mistral 7B meta-thinker to predict the reasoning type.
LogiQA BBH GSM8k MATHGPT-4o0.760.840.970.89+ SC @ 50.800.850.980.90TypedThinker 0.800.860.950.88+ SC @50.810.900.960.91</p>
<p>Table 6 :
6
TypedThinker can also enhance the performance of the math-specific 7B model such as MetaMath.
GSM8k MATHMetaMath0.690 0.209+ SC @ 50.704 0.220TypedThinker 0.696 0.220+ SC @ 50.736 0.246</p>
<p>Table 7 :
7
One example from LogiQA.The correct answer and the reasoning type with the highest effectiveness score are underlined.MoR is the few-shot MoR baselines, which use the majority votes among reasoning types.ProblemThe higher the altitude, the smaller the atmospheric pressure.Because the altitude of Place A is higher than that of Place B, the atmospheric pressure of Place A is lower than that of Place B. Which of the following examples shows the same pattern of reasoning?(A) In a highly competitive market, the better the product quality and the more advertising investment, the greater the product sales.Company A invests more money in advertising than Company B. So Company A sells more products than Company B. (B) The older a person is, the more mature he becomes.Person A is older than his son, so Person B is more mature than his son.</p>
<p>Table 9 :
9
Toshniwal et al. (2024)mathematic benchmarks we used in this paper.We follow the standard train/test split on LogiQA and follow the split inToshniwal et al. (2024)for GSM8k and MATH.For BBH, we randomly split the dataset.The synthesized data is described in Section 3.1.BBH includes 16 tasks while MATH includes math problems of 7 categories.Policy indicates the data used to train the meta-reasoner and SFT indicates the instruction-following in reasoner finetuning.These reasoning categories are not orthogonal and one problem can belong to multiple categories.We follow the standard training/validation split and only keep examples with more than 3 reasoning categories.This makes the problem more diverse and difficult to solve.We take the validation set as the test set and randomly select 500 examples from the training set for validation.
BenchmarkEmpirical Dataset# Task # Train # Val # Test # Total # Meta-thinker # ReasonerLogiQA137575005114768~2k~6kBBH16190432016003824~1k~3.5kGSM8k16473100013198792~4k~4kMath765001000500012500~1k~1ksufficient conditional reasoning, necessary conditional reasoning, disjunctive reasoning, and conjunc-tive reasoning.</p>
<p>table, reasoning color, ruin names, snarks, temporal sequences, tracking shuffled three, tracking shuffled five, and tracking shuffled seven.For each task, we randomly select 100 examples as the test set and 20 examples as the validation.The rest are used as training examples.
GSM8k (Cobbe et al., 2021) is a commonly used math benchmark to evaluate LLMs' capabilityin math reasoning. It contains 8.5K grade school math word problems, which are split into 7.5ktraining examples and 1k test problems. Each problem usually takes between 2 and 8 steps to solve.MATH (Hendrycks et al., 2021) is also a popular math benchmark for LLMs. It contains 12,500challenging competition mathematics problems with 7 categories. There are 7.5k training examplesand 5k test problems. We follow Toshniwal et al. (2024) to process the dataset.</p>
<p>Table 10.A larger Levenshtein distance and a smaller overlap indicate a more diverse solution set.The zero-shot setting does not include examples</p>
<p>Table 12 :
12
TypedThinker outperforms other baselines on LiveBench without extra finetuning.Here the results are based on the majority vote over 5 responses (+SC @5).
Mistral 7B LLaMA3 8BFew-shot0.1780.200CoT Selection0.2440.200TypedThinker0.2670.267A.5.2 RESULTS ON MORE BENCHMARKS</p>
<p>Table 13 :
13
Mistral 7B results are based on three repetitive experiments.Avg.indicates the average accuracy over four benchmarks.±0.007 0.347 ± 0.01 0.372 ± 0.014 0.071 ± 0.003 0.321 ± 0.006CoT Selection 0.475 ± 0.009 0.361 ± 0.01 0.377 ± 0.011 0.104 ± 0.008 0.329 ± 0.004 LLM + Meta Thinker 0.512 ± 0.003 0.377 ± 0.006 0.379 ± 0.004 0.106 ± 0.009 0.343 ± 0.004 LLM + Collection 0.519 ± 0.007 0.398 ± 0.005 0.363 ± 0.004 0.086 ± 0.008 0.342 ± 0.001 TypedThinker 0.553 ± 0.004 0.430 ± 0.008 0.390 ± 0.012 0.103 ± 0.01 0.369 ± 0.006
LogiQABBHGSM8KMATHAvg.Few-shot0.493</p>
<p>Table 14 :
14
LLaMA 3 8B results are based on three repetitive experiments.Avg.indicates the average accuracy over four benchmarks.± 0.003 0.319 ± 0.006 0.476 ± 0.004 0.102 ± 0.005 0.366 ± 0.001 CoT Selection 0.558 ± 0.011 0.376 ± 0.007 0.360 ± 0.006 0.104 ± 0.009 0.349 ± 0.007 LLM + Meta Thinker 0.538 ± 0.005 0.434 ± 0.005 0.508 ± 0.005 0.118 ± 0.005 0.400 ± 0.001 LLM + Collection 0.574 ± 0.011 0.497 ± 0.004 0.438 ± 0.005 0.109 ± 0.006 0.404 ± 0.001 TypedThinker 0.546 ± 0.004 0.534 ± 0.003 0.535 ± 0.001 0.203 ± 0.009 0.455 ± 0.002
LogiQABBHGSM8KMATHAvg.Few-shot0.569
In this paper, we sample at most 10 times for one problem.
https://github.com/kailashsp/SELF-DISCOVER
For answers with the same votes, we choose the first one in alphabetical order.
https://www.sbert.net/
In our implementation, answers with the same votes are ranked based on their alphabetical order, so (A) will be chosen in this case.
A APPENDIXA.1 PROMPT We introduce the simple and practical definition of four reasoning types in Table8.Table16lists the few-shot examples of each reasoning type.The full few-shot examples can be found in the supplementary materials.We use the same few-shot examples for the logical problems and create another set of examples for the mathematics problems.Table8: Description of different reasoning types.We give informal definitions that are easy to follow and illustrate simple examples for each reasoning type.Type Definition ExampleDeduction Deduce conclusion based on the general rules and premise.From the premises 'all frogs are amphibians' and 'no cats are amphibians', we can infer the conclusion 'no cats are frogs' Induction Make broad generalizations from specific observations.Starting from the empirical observation that 'all ravens I have seen so far are black', inductive reasoning can be used to infer that 'all ravens are black'Abduction Assume one candidate is correct and check whether it meets the condition in the problem.Guess that it has rained to explain that the streets are wet.A tsunami could also explain why the streets are wet but this is usually not the best explanation.AnalogyRetrieve several relevant information and draw the conclusion of this problem based on the similarity.Infer information about humans from medical experiments on animals: (1) rats are similar to humans;(2) birth control pills affect the brain development of rats;(3) therefore they may also affect the brain development of humans.The prompt used by the meta-thinker is:Given the question below, please identify the type of reasoning required to provide a solution.You may choose the following reasoning types: Deductive, Inductive, Analogical, Abductive Reasoning, or None.None indicates that no specific reasoning type is needed for this problem.Please assign an effectiveness score for each reasoning type from 0 to 1, where 0 represents no effective and 1 represents full effective.Please return the reasoning types and their corresponding effectiveness scores in the JSON format.For instance, if you think the question can be solved using both deductive and inductive reasoning, with an effectiveness of 0.5 for deductive reasoning and 0.3 for inductive reasoning, you should return: [{"ReasoningType": "Deductive", "Effectiveness": 0.5},{"ReasoningType": "Inductive", "Effectiveness": 0.3},{"Rea-soningType": "Analogical", "Effectiveness": 0},{"ReasoningType": "Abductive", "Effectiveness": 0}, {"ReasoningType": "None", "Effectiveness": 0}].The prompt used by the reasoner is listed below.The definition is based on Table8.The dataset statistics of the four benchmarks are detailed in Table9.For multiple-choice questions, we calculate accuracy using the exact match criterion.For mathematics problems, we compare the model's response with the ground truth using mathematical equality.LogiQA(Liu et al., 2021;2023a)LiveBench: reasoning -zebra puzzleThere are 3 people standing in a line numbered 1 through 3 in a left-to-right order.Each person has a set of attributes: Nationality, Music-Genre, Transport.The attributes have the following possible values:-Nationality: spanish, argentine, canadian -Music-Genre: punk, rock, reggae -Transport: train, jet-ski, trike and exactly one person in the line has a given value for an attribute.Given the following premises about the line of people:-the person who is argentine avoids getting on a train -the person who is spanish is somewhere between the person who listens to punk and the person who listens to rock -the person who listens to punk is not anywhere to the right of the person that travels by trike -the person who listens to punk is on the immediate right of the person that travels by jet-ski Answer the following question: What is the nationality of the person who listens to rock?Return your answer as a single word, in the following format: <strong><em>X</em></strong>, where X is the answer.in the prompt, and the zero-shot setting + types only include the definition of the reasoning type (as listed in Table8).The few-shot setting has 5 examples, and the few-shot setting with types has different 6 examples for each type.For zero-shot / few-shot @5, we use repeated sampling with temperature = 1 for 5 times.For zero-shot / few-shot + 5 types, we sample one solution per reasoning type.From Table10, we can see that after adding the reasoning types, the diversity of both zero-shot and few-shot increases significantly.It indicates that the introduction of various reasoning types can make the LLM's reasoning more diverse.We can also find that in most cases, the few-shot with reasoning types has the highest diversity, while in BBH, the zero-shot setting can benefit more from the reasoning types.A.5.1 RESULTS ON MORE BACKBONE LLMSWe conducted further experiments using Qwen 2-7B-Instruct(Bai et al., 2023)as our backbone LLM.The Qwen series of open-source large language models have demonstrated comparable or even superior performance to the Mistral and LLaMA families across multiple tasks.The results are shown in Table11.Our method achieves approximately 7% improvement over the few-shot baseline in both single-generation and majority-vote settings (+SC @5).These results demonstrate that TypedThinker is a general and effective method for enhancing the reasoning capabilities of various LLMs.4, we present specific performance on the abstract category of symbolic reasoning in Table15.As we can see TypedThinker outperforms baseline methods, even without further fine-tuning the meta-thinker and reasoner for this symbolic reasoning task.While the LogiQA dataset contains problems requiring commonsense reasoning, the dataset lacks explicit annotations (such as a specific category) for such tasks.For example, here is one case that requires commonsense knowledge about manufacturing costs, market dynamics, and consumer preferences.LogiQA: A commonsense reasoning exampleTraditionally, the most highly sought cars have been the sports cars and similar two-door models.Nevertheless, Zincstone Motors has chosen to eliminate the last two-door models and produce only four-door models.Which of the following would, if true, most help to explain Zincstone Motors' strategy?Options: (A) In almost every instance, Zincstone Motors models lead all comparable models of competitors in fuel efficiency and have lower average maintenance costs as well.(B) After a spate of recent additional safety requirements, the cost of frame and doors of Zincstone Motors' standard two-door models are now three times as expensive as standard four-door frame and doors.(C) Many of Zincstone Motors models are exported and sold overseas, including in some countries like Japan, which import a significant number of cars into the United States.(D) As American consumers lose access to car manufacturers who produce two-door cars, and as two-door cars occupy smaller and smaller shares of the United States car market, American consumers' tastes tend to shift from two-door cars.For reasoning problems that significantly differ from the existing domains (logic and math), additional demonstrations tailored to the task are recommended to guide reasoning.For example, inductive reasoning might involve deriving a general program from exemplar input-output test cases in code generation(Shao et al., 2024;Yang et al., 2024).In such cases, providing a few task-specific examples or conducting light fine-tuning on the reasoner can further enhance performance.A.7 IMPACT STATEMENTThis work will enhance current LLMs with better reasoning capability, which can make them more useful in problem-solving.There might be some potential societal consequences of our work, none of which we feel must be specifically highlighted here.However, it might be misused as we release all our code and data for reproduction.We will try our best to avoid the potential misuse.Published as a conference paper at ICLR 2025 Abduction The integer m is between 30 and 80 and is a multiple of 6.When m is divided by 8, the remainder is 2. Similarly, when m is divided by 5, the remainder is 2. What is the value of m?To solve this problem using abductive reasoning, we assume that one possible value of m exists that abides by the constraints and check if this assumption holds.1.Fi rst, filter values of m that are multiples of 6 between 30 and 80. 2. Next, apply the condition that when m is divided by 8, the remainder is 2.Only 42, 66 fit this condition.3.Apply the third condition, that when divided by 5, m should leave a remainder of 2. Testing the applicable values so far and find 42 meets the requirement.So the answer is 42 .AnalogyJohn is 24 years younger than his dad.The sum of their ages is 68 years.How many years old is John?Retrieval: Question: Lisa is 10 years younger than her mom.The sum of their ages is 70 years.How old is Lisa?Answer: Lisa is 30 years old and her mom is 40 years old.These are solved using the same approach as the problem about John and his dad's ages, i.e., setting up two equations based on the information given and then solving for the two variables representing the ages.Therefore, for the given question, John is 22 years old.
Rest meets react: Selfimprovement for multi-step reasoning llm agent. Renat Aksitov, Sobhan Miryoosefi, Zonglin Li, Daliang Li, Sheila Babayan, Kavya Kopparapu, Zachary Fisher, Ruiqi Guo, Sushant Prakash, Pranesh Srinivasan, ICLR 2024 Workshop on Large Language Model (LLM) Agents. </p>
<p>Rl4f: Generating natural language feedback with reinforcement learning for repairing model outputs. Afra Feyza Akyürek, Ekin Akyürek, Ashwin Kalyan, Peter Clark, Derry Wijaya, Niket Tandon, Annual Meeting of the Association of Computational Linguistics 2023. Association for Computational Linguistics2023</p>
<p>Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, arXiv:2309.16609Qwen technical report. 2023arXiv preprint</p>
<p>Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron Mckinnon, arXiv:2212.08073Constitutional ai: Harmlessness from ai feedback. 2022arXiv preprint</p>
<p>A multitask, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and interactivity. Yejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wenliang Dai, Dan Su, Bryan Wilie, Holy Lovenia, Ziwei Ji, Tiezheng Yu, Willy Chung, Proceedings of the 13th International Joint Conference on Natural Language Processing and the 3rd Conference of the Asia-Pacific Chapter. Long Papers. the 13th International Joint Conference on Natural Language Processing and the 3rd Conference of the Asia-Pacific Chapterthe Association for Computational Linguistics20231</p>
<p>Analogy and analogical reasoning. Paul Bartha, 2013</p>
<p>Graph of thoughts: Solving elaborate problems with large language models. Maciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Michal Podstawski, Lukas Gianinazzi, Joanna Gajda, Tomasz Lehmann, Hubert Niewiadomski, Piotr Nyczyk, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202438</p>
<p>Logical reasoning in formal and everyday reasoning tasks. Hugo Bronkhorst, Gerrit Roorda, J M Cor, Martin J Suhre, Goedhart, International Journal of Science and Mathematics Education. 182020</p>
<p>Large language monkeys: Scaling inference compute with repeated sampling. Bradley Brown, Jordan Juravsky, Ryan Ehrlich, Ronald Clark, Christopher Quoc V Le, Azalia Ré, Mirhoseini, arXiv:2407.217872024arXiv preprint</p>
<p>Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, John Schulman, arXiv:2110.14168Training verifiers to solve math word problems. 2021arXiv preprint</p>
<p>A concise introduction to logic. Craig Delancey, Open SUNY Textbooks. 2017</p>
<p>Assessing the reasoning abilities of chatgpt in the context of claim verification. John Dougrez-Lewis, Mahmud Elahi Akhter, Yulan He, Maria Liakata, arXiv:2402.107352024arXiv preprint</p>
<p>. Igor Douven, 2011</p>
<p>Logical reasoning. Bradley H Dowden, 2018</p>
<p>Improving factuality and reasoning in language models through multiagent debate. Yilun Du, Shuang Li, Antonio Torralba, Joshua B Tenenbaum, Igor Mordatch, arXiv:2305.143252023arXiv preprint</p>
<p>A Peter, Antonis C Flach, Kakas, Abductive and inductive reasoning: background and issues. Abduction and induction: Essays on their relation and integration. 2000</p>
<p>Complexity-based prompting for multi-step reasoning. Yao Fu, Hao Peng, Ashish Sabharwal, Peter Clark, Tushar Khot, The Eleventh International Conference on Learning Representations. 2023</p>
<p>Reinforced self-training (rest) for language modeling. Çaglar Gülçehre, Tom Le Paine, Srivatsan Srinivasan, Ksenia Konyushkova, Lotte Weerts, Abhishek Sharma, Aditya Siddhant, Alex Ahern, Miaosen Wang, Chenjie Gu, 2023CoRR</p>
<p>Critical thinking across the curriculum: A brief edition of thought &amp; knowledge. Diane F Halpern, 2014Routledge</p>
<p>Reasoning with language model is planning with world model. Shibo Hao, Yi Gu, Haodi Ma, Joshua Hong, Zhen Wang, Daisy Wang, Zhiting Hu, 10.18653/v1/2023.emnlp-main.507Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language ProcessingSingaporeAssociation for Computational Linguistics2023</p>
<p>Measuring mathematical problem solving with the math dataset. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, Jacob Steinhardt, 2021NeurIPS</p>
<p>Wenyue Hua, Kaijie Zhu, Lingyao Li, Lizhou Fan, Shuhang Lin, Mingyu Jin, Haochen Xue, Zelong Li, Jindong Wang, Yongfeng Zhang, arXiv:2406.02787Disentangling logic: The role of context in large language model reasoning capabilities. 2024arXiv preprint</p>
<p>Large language models can self-improve. Jiaxin Huang, Shixiang Gu, Le Hou, Yuexin Wu, Xuezhi Wang, Hongkun Yu, Jiawei Han, 10.18653/v1/2023.emnlp-main.67Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language ProcessingSingaporeAssociation for Computational Linguistics2023</p>
<p>Alexandre Albert Q Jiang, Arthur Sablayrolles, Chris Mensch, Devendra Bamford, Diego Singh Chaplot, Florian De Las Casas, Gianna Bressand, Guillaume Lengyel, Lucile Lample, Saulnier, arXiv:2310.06825Mistral 7b. 2023arXiv preprint</p>
<p>Deductive reasoning. Phil Johnson-Laird, Wiley Interdisciplinary Reviews: Cognitive Science. 112010</p>
<p>Training language models to self-correct via reinforcement learning. Aviral Kumar, Vincent Zhuang, Rishabh Agarwal, Yi Su, John D Co-Reyes, Avi Singh, Kate Baumli, Shariq Iqbal, Colton Bishop, Rebecca Roelofs, arXiv:2409.129172024arXiv preprint</p>
<p>Alphacode 2 technical report. Rémi Leblond, 2023DeepMindTechnical report</p>
<p>Binary codes capable of correcting deletions, insertions, and reversals. Proceedings of the Soviet physics doklady. V Levenshtein, 1966</p>
<p>Competition-level code generation with alphacode. Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Rémi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, Science. 37866242022</p>
<p>Encouraging divergent thinking in large language models through multi-agent debate. Tian Liang, Zhiwei He, Wenxiang Jiao, Xing Wang, Yan Wang, Rui Wang, Yujiu Yang, Zhaopeng Tu, Shuming Shi, arXiv:2305.191182023arXiv preprint</p>
<p>Let's verify step by step. Vineet Hunter Lightman, Yuri Kosaraju, Harrison Burda, John Edwards ; Leike, Ilya Schulman, Karl Sutskever, Cobbe, The Twelfth International Conference on Learning Representations. Bowen Baker, Teddy LeeJan</p>
<p>Logiqa 2.0-an improved dataset for logical reasoning in natural language understanding. Hanmeng Liu, Jian Liu, Leyang Cui, Zhiyang Teng, Nan Duan, Ming Zhou, Yue Zhang, 10.1109/TASLP.2023.3293046Speech, and Language Processing. 2023a31</p>
<p>Making ppo even better: Value-guided monte-carlo tree search decoding. Jiacheng Liu, Andrew Cohen, Ramakanth Pasunuru, Yejin Choi, Hannaneh Hajishirzi, Asli Celikyilmaz, arXiv:2309.150282023barXiv preprint</p>
<p>Logiqa: a challenge dataset for machine reading comprehension with logical reasoning. Jian Liu, Leyang Cui, Hanmeng Liu, Dandan Huang, Yile Wang, Yue Zhang, Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence, IJCAI'20. the Twenty-Ninth International Joint Conference on Artificial Intelligence, IJCAI'202021</p>
<p>Towards logiglue: A brief survey and a benchmark for analyzing logical reasoning capabilities of language models. Man Luo, Shrinidhi Kumbhar, Mihir Parmar, Neeraj Varshney, Pratyay Banerjee, Somak Aditya, Chitta Baral, arXiv:2310.008362023arXiv preprint</p>
<p>Logical Reasoning and Learning. Terezinha Nunes, 10.1007/978-1-4419-1428-6_790OpenAI. 2023. Gpt-4 technical report. Boston, MASpringer US2012</p>
<p>Check your facts and try again: Improving large language models with external knowledge and automated feedback. Baolin Peng, Michel Galley, Pengcheng He, Hao Cheng, Yujia Xie, Yu Hu, Qiuyuan Huang, Lars Liden, Zhou Yu, Weizhu Chen, arXiv:2302.128132023arXiv preprint</p>
<p>Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters. Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, Yuxiong He, 10.1145/3394486.3406703Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining, KDD '20. the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining, KDD '20New York, NY, USAAssociation for Computing Machinery2020</p>
<p>Sentence-BERT: Sentence embeddings using Siamese BERTnetworks. Nils Reimers, Iryna Gurevych, 10.18653/v1/D19-1410Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)Hong Kong, ChinaAssociation for Computational Linguistics2019</p>
<p>Yunfan Shao, Linyang Li, Yichuan Ma, Peiji Li, Demin Song, Qinyuan Cheng, Shimin Li, Xiaonan Li, Pengyu Wang, Qipeng Guo, Hang Yan, 10.48550/arXiv.2407.12504Xipeng Qiu, Xuanjing Huang, and Dahua Lin. 2024. Case2code: Learning inductive reasoning with synthetic data. </p>
<p>Reflexion: an autonomous agent with dynamic memory and self-reflection. Noah Shinn, Beck Labash, Ashwin Gopinath, arXiv:2303.113662023arXiv preprint</p>
<p>Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal, Md Shoeb, Abubakar Abid, Adam Fisch, Adam Adam R Brown, Aditya Santoro, Adrià Gupta, Garriga-Alonso, arXiv:2206.04615Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. 2022arXiv preprint</p>
<p>Meta-prompting: Enhancing language models with task-agnostic scaffolding. Mirac Suzgun, Adam Tauman, Kalai , 2024</p>
<p>Challenging big-bench tasks and whether chain-of-thought can solve them. Mirac Suzgun, Nathan Scales, Nathanael Schärli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Ed H Quoc V Le, Denny Chi, Jason Zhou, Wei, arXiv:2210.092612022arXiv preprint</p>
<p>Openmathinstruct-1: A 1.8 million math instruction tuning dataset. Shubham Toshniwal, Ivan Moshkov, Sean Narenthiran, Daria Gitman, Fei Jia, Igor Gitman, arXiv:2402.101762024arXiv preprint</p>
<p>Thibaut Hugo Touvron, Gautier Lavril, Xavier Izacard, Marie-Anne Martinet, Timothée Lachaux, Baptiste Lacroix, Naman Rozière, Eric Goyal, Faisal Hambro, Azhar, arXiv:2302.13971Llama: Open and efficient foundation language models. 2023arXiv preprint</p>
<p>Learn from mistakes through cooperative interaction with study assistant. Danqing Wang, Lei Li, The 2023 Conference on Empirical Methods in Natural Language Processing. 20232023</p>
<p>Hypothesis search: Inductive reasoning with language models. Ruocheng Wang, Eric Zelikman, Gabriel Poesia, Yewen Pu, Nick Haber, Noah Goodman, The Twelfth International Conference on Learning Representations. 2023a</p>
<p>Self-consistency improves chain of thought reasoning in language models. Xuezhi Wang, Jason Wei, Dale Schuurmans, Ed H Quoc V Le, Sharan Chi, Aakanksha Narang, Denny Chowdhery, Zhou, The Eleventh International Conference on Learning Representations. 2022</p>
<p>Self-instruct: Aligning language models with self-generated instructions. Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, Hannaneh Hajishirzi, 10.18653/v1/2023.acl-long.754Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. the 61st Annual Meeting of the Association for Computational LinguisticsToronto, CanadaAssociation for Computational Linguistics2023b1</p>
<p>Psychology of reasoning: Structure and content. Peter Cathcart Wason and Philip Nicholas Johnson-Laird1972Harvard University Press86</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed H Chi, Denny Quoc V Le, Zhou, Advances in Neural Information Processing Systems. 2022</p>
<p>Colin White, Samuel Dooley, Manley Roberts, Arka Pal, Ben Feuer, Siddhartha Jain, Ravid Shwartz-Ziv, Neel Jain, Khalid Saifullah, Siddartha Naidu, arXiv:2406.19314Livebench: A challenging, contamination-free llm benchmark. 2024arXiv preprint</p>
<p>Huggingface's transformers: State-of-the-art natural language processing. Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, arXiv:1910.037712019arXiv preprint</p>
<p>Language models as inductive reasoners. Zonglin Yang, Li Dong, Xinya Du, Hao Cheng, Erik Cambria, Xiaodong Liu, Jianfeng Gao, Furu Wei, Yvette Graham, Matthew Purver, Proceedings of the 18th Conference of the European Chapter. the Association for Computational Linguistics. the 18th Conference of the European ChapterSt. Julian's, MaltaAssociation for Computational Linguistics20241Long Papers</p>
<p>Tree of Thoughts: Deliberate problem solving with large language models. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L Griffiths, Yuan Cao, Karthik Narasimhan, 2023</p>
<p>THOUGHT PROPAGATION: AN ANALOGICAL APPROACH TO COMPLEX REASONING WITH LARGE LANGUAGE MODELS. Junchi Yu, Ran He, Zhitao Ying, The Twelfth International Conference on Learning Representations. 2024</p>
<p>Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengying Liu, Yu Zhang, James T Kwok, Zhenguo Li, Adrian Weller, Weiyang Liu, arXiv:2309.12284Metamath: Bootstrap your own mathematical questions for large language models. 2023arXiv preprint</p>
<p>Weizhe Yuan, Richard Yuanzhe Pang, Kyunghyun Cho, Sainbayar Sukhbaatar, Jing Xu, Jason Weston, arXiv:2401.10020Self-rewarding language models. 2024arXiv preprint</p>
<p>Progressive-hint prompting improves reasoning in large language models. Chuanyang Zheng, Zhengying Liu, Enze Xie, Zhenguo Li, Yu Li, AI for Math Workshop @ ICML 2024. 2024</p>
<p>Self-discover: Large language models self-compose reasoning structures. Pei Zhou, Jay Pujara, Xiang Ren, Xinyun Chen, Heng-Tze Cheng, Quoc V Le, Ed H Chi, Denny Zhou, Swaroop Mishra, Huaixiu Steven, Zheng , 10.48550/arXiv.2402.036202024</p>
<p>Dyval: Dynamic evaluation of large language models for reasoning tasks. Kaijie Zhu, Jiaao Chen, Jindong Wang, Neil Zhenqiang Gong, Diyi Yang, Xing Xie, The Twelfth International Conference on Learning Representations. 2024</p>            </div>
        </div>

    </div>
</body>
</html>
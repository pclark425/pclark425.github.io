<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6814 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6814</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6814</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-131.html">extraction-schema-131</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <p><strong>Paper ID:</strong> paper-271974366</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2408.15778v1.pdf" target="_blank">L OGIC G AME : Benchmarking Rule-Based Reasoning Abilities of Large Language Models</a></p>
                <p><strong>Paper Abstract:</strong> Large Language Models (LLMs) have demonstrated notable capabilities across various tasks, showcasing complex problem-solving abilities. Understanding and executing complex rules, along with multi-step planning, are fundamental to logical reasoning and critical for practical LLM agents and decision-making systems. However, evaluating LLMs as effective rule-based executors and planners remains underexplored. In this paper, we introduce L OGIC G AME , a novel benchmark designed to evaluate the comprehensive rule understanding, execution, and planning capabilities of LLMs. Unlike traditional benchmarks, L OGIC G AME provides diverse games that contain a series of rules with an initial state, requiring models to comprehend and apply predefined regulations to solve problems. We create simulated scenarios in which models execute or plan operations to achieve specific outcomes. These game scenarios are specifically designed to distinguish logical reasoning from mere knowledge by relying exclusively on predefined rules. This separation allows for a pure assessment of rule-based reasoning capabilities. The evaluation considers not only final outcomes but also intermediate steps, providing a comprehensive assessment of model performance. Moreover, these intermediate steps are deterministic and can be automatically verified. L OGIC G AME defines game scenarios with varying difficulty levels, from simple rule applications to complex reasoning chains, in order to offer a precise evaluation of model performance on rule understanding and multi-step execution. Utilizing L OGIC G AME , we test various LLMs and identify notable shortcomings in their rule-based logical reasoning abilities.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6814.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6814.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LOGICGAME</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LOGICGAME benchmark</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A bilingual (zh/en) rule-based reasoning benchmark of games designed to isolate rule understanding, execution, and multi-step planning; evaluates both final answers and deterministic intermediate processes via a JSON process constraint across four difficulty levels.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LOGICGAME</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Benchmark introduced in this paper to evaluate LLMs' strict rule-following and logical reasoning via execution and planning game scenarios; processes are deterministic where possible and automatically verifiable.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>benchmark / evaluation suite (not a neural architecture)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>N/A (benchmark uses prompt: Rules + Question + JSON output format; supports few-shot exemplars to probe models' in‑context rule acquisition)</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>LOGICGAME</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Contains 304 problems spanning Execution (deterministic single/multi-step string and matrix operations, arithmetic, e.g., Reversi, Synthesis/Decomposition) and Planning (multi-step strategy puzzles, e.g., Minesweeper, Constrained Linear Arrangement, Logic Puzzles); en/zh versions; four difficulty levels (0–3); requires JSON answer+process when applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Rule-based execution and planning (deterministic step verification and multi-step planning)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>AP-Acc (Answer+Process Accuracy), A-Acc, P-Acc, NIJ-Acc; JSON/JSError/IFError reported for output conformity</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Benchmark description only; used to evaluate models. Paper reports best model NIJ-Acc: o1-preview 54.93% (zh) / 53.29% (en); many models score <<50%, with many under ~12% in category-level analyses.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>N/A (benchmark itself). Paper uses 0/1/2-shot experiments to compare in-context learning effects across models.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Designed to separate rule-based logical reasoning from knowledge; enables deterministic verification of intermediate steps and reveals large performance differences across models and task types.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Some planning problems are inherently less deterministic (paper inserts checkpoints to enable process checking); natural-language ambiguities were minimized but may not be entirely avoidable.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'L OGIC G AME : Benchmarking Rule-Based Reasoning Abilities of Large Language Models', 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6814.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6814.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>o1-preview</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>o1-preview (closed-source model evaluated)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A top-performing closed-source LLM in this study that achieved the highest reported NIJ-Acc on LOGICGAME among evaluated models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>o1-preview</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Closed-source, transformer-based large language model (details not specified in paper).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>transformer-based</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>not reported in this paper</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Prompting with rules + question + JSON output constraint; evaluated under 0/1/2-shot settings (few-shot exemplars inserted between rules and question)</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>LOGICGAME</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Rule-based games covering execution (deterministic operations) and planning (multi-step strategy) across four difficulty levels, bilingual.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Execution and Planning (rule-based reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>NIJ-Acc (aggregate correctness on non-IFError/non-JSError samples); AP-Acc reported elsewhere</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>NIJ-Acc Avg = 54.93% (zh) and 53.29% (en) as reported in the paper (leading model).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Outperforms other evaluated models by a large margin on NIJ-Acc; paper notes o1-preview and o1-mini as top two performers.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Top performer on LOGICGAME, stronger on Execution than Planning (per paper’s per-category analysis).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Despite leading scores, still far from perfect: failure modes exist especially on harder levels and planning tasks; paper does not provide internal architecture/training details.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'L OGIC G AME : Benchmarking Rule-Based Reasoning Abilities of Large Language Models', 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6814.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6814.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>o1-mini</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>o1-mini (closed-source model evaluated)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A second strong closed-source LLM in the evaluation that follows o1-preview in overall LOGICGAME performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>o1-mini</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Closed-source transformer-based LLM (paper does not provide architecture/training specifics).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>transformer-based</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>not reported in this paper</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Prompting with rules + JSON process constraint; tested with 0/1/2-shot exemplars to probe in-context learning.</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>LOGICGAME</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Execution and Planning rule-based games with deterministic intermediate steps where applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Execution and Planning</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>NIJ-Acc Avg</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>NIJ-Acc Avg = 51.97% (zh) and 49.67% (en) as reported in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Close behind o1-preview; shows more gradual decline across levels compared to some models (paper observation).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Performs robustly across difficulty levels relative to many other models; benefits from few-shot on execution tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Still fails many complex reasoning tasks; internal details not disclosed in paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'L OGIC G AME : Benchmarking Rule-Based Reasoning Abilities of Large Language Models', 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6814.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6814.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>claude-3.5-sonnet</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Anthropic Claude 3.5 Sonnet</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A Claude-series closed-source model included in the evaluation that shows moderate performance on execution tasks but lower on planning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>claude-3.5-sonnet</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Closed-source transformer-based model from Anthropic (paper cites Claude family but does not give technical specs).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>transformer-based</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>not reported in this paper</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Prompting with rules + JSON output constraint; tested with few-shot exemplars in some settings.</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>LOGICGAME</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Rule-following execution (string/matrix/operation tasks) and planning puzzles; requires process outputs for multi-step problems.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Execution and Planning</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>NIJ-Acc Avg</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>NIJ-Acc Avg = 30.26% (zh) and 29.28% (en) as reported in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Substantially below the top performers (o1-preview/mini); shows larger drops at higher difficulty levels.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Understands instructions/formatting often but fails in accurate rule application for multi-step tasks (case study evidence).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Fails detailed rule execution (e.g., Reversi piece flips) leading to incorrect intermediate states; lower process accuracy on planning tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'L OGIC G AME : Benchmarking Rule-Based Reasoning Abilities of Large Language Models', 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6814.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e6814.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>gpt-4o</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4o (OpenAI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An OpenAI GPT-4 family variant evaluated on LOGICGAME; shows mixed performance, better on some planning tasks than other GPT variants.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-4o</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Closed-source GPT-4 family transformer-based model (details not specified in paper).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>transformer-based</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>not reported in this paper</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Prompting with rules + JSON output constraint; few-shot experiments performed to assess in-context learning effects.</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>LOGICGAME</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Execution and Planning rule-based games with step-by-step process evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Execution and Planning</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>NIJ-Acc Avg</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>NIJ-Acc Avg = 27.06% (paper table).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Comparable to other strong closed-source models but below o1-preview/mini; within GPT family, relative strengths vary by category (paper: gpt-4-turbo outperforms gpt-4o on Planning in some comparisons).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Shows mixed category-level performance; benefits from few-shot for execution in some cases but not uniformly.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Drops significantly as difficulty increases; process-level errors and JSON formatting errors occasionally observed.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'L OGIC G AME : Benchmarking Rule-Based Reasoning Abilities of Large Language Models', 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6814.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e6814.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>gpt-4-turbo-0409</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4-Turbo-0409 (OpenAI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Another GPT-4 variant assessed; shows differing strengths from gpt-4o across Execution vs Planning tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-4-turbo-0409</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Closed-source GPT-4 family transformer-based model (paper does not provide parameter counts).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>transformer-based</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>not reported in this paper</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Prompted with Rules + question + JSON output format; few-shot studied.</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>LOGICGAME</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Execution (deterministic) and Planning (multi-step) tasks with intermediate process checking.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Execution and Planning</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>NIJ-Acc Avg</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>NIJ-Acc Avg = 26.09% (paper table).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Compared to gpt-4o, gpt-4-turbo performed differently by subtask: paper notes gpt-4-turbo-0409 outperforms gpt-4o on Planning, but gpt-4o better on Execution in some comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Model-level tradeoffs across Execution vs Planning observed; few-shot improves execution more than planning generally.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Performance degrades on higher difficulty levels; occasional JSON formatting/non-conformant outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'L OGIC G AME : Benchmarking Rule-Based Reasoning Abilities of Large Language Models', 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6814.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e6814.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>qwen2-72b-instruct</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Qwen2-72B-instruct</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source family model (Qwen) instruction-tuned variant evaluated; shows mid-range performance and notable few-shot improvement on execution.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>qwen2-72b-instruct</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source Qwen family model; 72B parameter variant (as indicated by name in the paper).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>72B</td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>transformer-based</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>not reported in this paper</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Prompting with rules + JSON output constraint; few-shot exemplars used in trials (0/1/2-shot experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>LOGICGAME</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Rule-based execution and planning tasks with intermediate process verification.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Execution and Planning (rule application & planning)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>NIJ-Acc Avg</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>NIJ-Acc Avg = 21.16% (paper table).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Stronger models (e.g., qwen2-72b-instruct vs smaller qwen2-7b) show larger gains from few-shot in Execution tasks per paper; qwen2-72b shows notable shot-driven improvements on execution.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Benefits from few-shot contexts on Execution; Planning often degraded by added exemplars for many models.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Still low absolute performance on complex planning tasks and higher difficulty levels; occasional errors in process formatting.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'L OGIC G AME : Benchmarking Rule-Based Reasoning Abilities of Large Language Models', 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6814.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e6814.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>llama-3-70b-chat</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Llama 3 70B Chat</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large open-source LLaMA-3 chat model evaluated; shows relatively low NIJ-Acc on LOGICGAME with varying performance between Execution and Planning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>llama-3-70b-chat</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source LLaMA-3 family chat model with 70B parameters (name indicates 70B in paper tables).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>70B</td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>transformer-based</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>not reported in this paper</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Prompting with rules + JSON output constraint; few-shot trials evaluated.</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>LOGICGAME</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Execution and Planning games with deterministic processes where applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Execution and Planning</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>NIJ-Acc Avg</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>NIJ-Acc Avg = 14.45% (paper table).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Underperforms compared to top closed-source models; within-family performance shows tradeoffs (llama-3-70b better at Planning vs Execution for some tasks per paper).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Performs poorly on many LOGICGAME tasks; stability issues (JSON/format errors) reported for some LLaMA variants.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Poor instruction-following on JSON constraint in some cases; low process accuracy especially on deterministic execution tasks like Reversi.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'L OGIC G AME : Benchmarking Rule-Based Reasoning Abilities of Large Language Models', 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6814.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e6814.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>llama-3-8b-chat</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Llama 3 8B Chat</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Smaller LLaMA-3 variant (8B) included in evaluation; shows very low NIJ-Acc and pronounced failures in instruction-following on LOGICGAME.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>llama-3-8b-chat</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source LLaMA-3 family chat model (8B parameters as indicated by name in paper tables).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>8B</td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>transformer-based</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>not reported in this paper</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Rules + question prompting with JSON output constraint; few-shot experiments included in study.</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>LOGICGAME</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Execution (deterministic) and Planning tasks; requires process outputs for multi-step verification.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Execution and Planning</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>NIJ-Acc Avg</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>NIJ-Acc Avg = 5.61% (paper table).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Performs worse than larger LLaMA-3 (70B) and most other evaluated models.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Consistently poor instruction-following and reasoning performance; in Reversi case study many models except this one adhered to instruction format but still failed logical reasoning—this variant performed particularly poorly.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Very low accuracy on both answer and process metrics; frequent JSON/formatting errors reported.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'L OGIC G AME : Benchmarking Rule-Based Reasoning Abilities of Large Language Models', 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6814.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e6814.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>glm-4-9b</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GLM-4 9B</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A GLM family open-source model evaluated; small- to mid-sized (9B) model that attains low NIJ-Acc on LOGICGAME.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>glm-4-9b</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source GLM-4 family transformer-based model with 9B parameters as indicated by the name in paper tables.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>9B</td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>transformer-based</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>not reported in this paper</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Prompting with rules + JSON process constraint; few-shot trials included.</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>LOGICGAME</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Rule-based execution and planning tasks with step-wise verification.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Execution and Planning</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>NIJ-Acc Avg</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>NIJ-Acc Avg = 9.62% (paper table).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Performs poorly compared to larger/stronger models; smaller models show smaller gains from few-shot prompting on planning tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Limited capability on strict rule-following and multi-step planning; few-shot helps execution for some models but not uniformly.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Low absolute accuracy; JSON/formatting and process errors present.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'L OGIC G AME : Benchmarking Rule-Based Reasoning Abilities of Large Language Models', 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6814.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e6814.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>mistral-7b-instruct</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mistral 7B Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 7B open-source instruct-tuned model included in evaluation that shows low NIJ-Acc and some JSON-formatting issues.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>mistral-7b-instruct</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source Mistral family instruct-tuned transformer (7B parameters inferred from name and cited Mistral paper).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>transformer-based</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>not reported in this paper</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Prompting with Rules + JSON constraint; few-shot experiments performed.</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>LOGICGAME</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Execution and Planning games where intermediate process steps can be automatically verified.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Execution and Planning</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>NIJ-Acc Avg</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>NIJ-Acc Avg = 4.39% (paper table).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Among the lower-performing models in the study; performed poorly on JSON-constrained outputs (paper flags high JSError/IFError for some smaller models).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Failed frequently on strict rule-following tasks; few-shot effects unstable.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>High error rates on JSON formatting; very low correct process/answer rates on many tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'L OGIC G AME : Benchmarking Rule-Based Reasoning Abilities of Large Language Models', 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6814.11">
                <h3 class="extraction-instance">Extracted Data Instance 11 (e6814.11)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>internlm-2.5-7b-chat</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>InternLM-2.5 7B Chat</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multilingual open-source chat model (7B-ish) evaluated; obtains low NIJ-Acc on LOGICGAME.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>internlm-2.5-7b-chat</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source InternLM family chat model (name indicates 2.5 / 7B variant; paper lists as evaluated).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>transformer-based</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>not reported in this paper</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Prompting with rules and JSON output constraints; few-shot contexts evaluated.</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>LOGICGAME</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Execution and Planning problems with process-level verification; bilingual dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Execution and Planning</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>NIJ-Acc Avg</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>NIJ-Acc Avg = 5.38% (paper table).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Performs poorly relative to larger models; small models have high variability and frequent formatting errors.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Low performance; few-shot effects minor or unstable.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Frequent JSON/formatting errors and low process correctness on deterministic tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'L OGIC G AME : Benchmarking Rule-Based Reasoning Abilities of Large Language Models', 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Chain-of-thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Language Models are Few-Shot Learners <em>(Rating: 2)</em></li>
                <li>Training language models to follow instructions with human feedback <em>(Rating: 2)</em></li>
                <li>Transformers as soft reasoners over language <em>(Rating: 1)</em></li>
                <li>A unified benchmark for mathematical reasoning <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6814",
    "paper_id": "paper-271974366",
    "extraction_schema_id": "extraction-schema-131",
    "extracted_data": [
        {
            "name_short": "LOGICGAME",
            "name_full": "LOGICGAME benchmark",
            "brief_description": "A bilingual (zh/en) rule-based reasoning benchmark of games designed to isolate rule understanding, execution, and multi-step planning; evaluates both final answers and deterministic intermediate processes via a JSON process constraint across four difficulty levels.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LOGICGAME",
            "model_description": "Benchmark introduced in this paper to evaluate LLMs' strict rule-following and logical reasoning via execution and planning game scenarios; processes are deterministic where possible and automatically verifiable.",
            "model_size": null,
            "architecture_type": "benchmark / evaluation suite (not a neural architecture)",
            "training_data": null,
            "reasoning_method": "N/A (benchmark uses prompt: Rules + Question + JSON output format; supports few-shot exemplars to probe models' in‑context rule acquisition)",
            "external_tool_used": false,
            "external_tool_description": null,
            "benchmark_name": "LOGICGAME",
            "benchmark_description": "Contains 304 problems spanning Execution (deterministic single/multi-step string and matrix operations, arithmetic, e.g., Reversi, Synthesis/Decomposition) and Planning (multi-step strategy puzzles, e.g., Minesweeper, Constrained Linear Arrangement, Logic Puzzles); en/zh versions; four difficulty levels (0–3); requires JSON answer+process when applicable.",
            "task_type": "Rule-based execution and planning (deterministic step verification and multi-step planning)",
            "performance_metric": "AP-Acc (Answer+Process Accuracy), A-Acc, P-Acc, NIJ-Acc; JSON/JSError/IFError reported for output conformity",
            "performance_value": "Benchmark description only; used to evaluate models. Paper reports best model NIJ-Acc: o1-preview 54.93% (zh) / 53.29% (en); many models score &lt;&lt;50%, with many under ~12% in category-level analyses.",
            "comparison_with_baseline": "N/A (benchmark itself). Paper uses 0/1/2-shot experiments to compare in-context learning effects across models.",
            "key_findings": "Designed to separate rule-based logical reasoning from knowledge; enables deterministic verification of intermediate steps and reveals large performance differences across models and task types.",
            "limitations": "Some planning problems are inherently less deterministic (paper inserts checkpoints to enable process checking); natural-language ambiguities were minimized but may not be entirely avoidable.",
            "uuid": "e6814.0",
            "source_info": {
                "paper_title": "L OGIC G AME : Benchmarking Rule-Based Reasoning Abilities of Large Language Models",
                "publication_date_yy_mm": "2024-08"
            }
        },
        {
            "name_short": "o1-preview",
            "name_full": "o1-preview (closed-source model evaluated)",
            "brief_description": "A top-performing closed-source LLM in this study that achieved the highest reported NIJ-Acc on LOGICGAME among evaluated models.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "o1-preview",
            "model_description": "Closed-source, transformer-based large language model (details not specified in paper).",
            "model_size": null,
            "architecture_type": "transformer-based",
            "training_data": "not reported in this paper",
            "reasoning_method": "Prompting with rules + question + JSON output constraint; evaluated under 0/1/2-shot settings (few-shot exemplars inserted between rules and question)",
            "external_tool_used": false,
            "external_tool_description": null,
            "benchmark_name": "LOGICGAME",
            "benchmark_description": "Rule-based games covering execution (deterministic operations) and planning (multi-step strategy) across four difficulty levels, bilingual.",
            "task_type": "Execution and Planning (rule-based reasoning)",
            "performance_metric": "NIJ-Acc (aggregate correctness on non-IFError/non-JSError samples); AP-Acc reported elsewhere",
            "performance_value": "NIJ-Acc Avg = 54.93% (zh) and 53.29% (en) as reported in the paper (leading model).",
            "comparison_with_baseline": "Outperforms other evaluated models by a large margin on NIJ-Acc; paper notes o1-preview and o1-mini as top two performers.",
            "key_findings": "Top performer on LOGICGAME, stronger on Execution than Planning (per paper’s per-category analysis).",
            "limitations": "Despite leading scores, still far from perfect: failure modes exist especially on harder levels and planning tasks; paper does not provide internal architecture/training details.",
            "uuid": "e6814.1",
            "source_info": {
                "paper_title": "L OGIC G AME : Benchmarking Rule-Based Reasoning Abilities of Large Language Models",
                "publication_date_yy_mm": "2024-08"
            }
        },
        {
            "name_short": "o1-mini",
            "name_full": "o1-mini (closed-source model evaluated)",
            "brief_description": "A second strong closed-source LLM in the evaluation that follows o1-preview in overall LOGICGAME performance.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "o1-mini",
            "model_description": "Closed-source transformer-based LLM (paper does not provide architecture/training specifics).",
            "model_size": null,
            "architecture_type": "transformer-based",
            "training_data": "not reported in this paper",
            "reasoning_method": "Prompting with rules + JSON process constraint; tested with 0/1/2-shot exemplars to probe in-context learning.",
            "external_tool_used": false,
            "external_tool_description": null,
            "benchmark_name": "LOGICGAME",
            "benchmark_description": "Execution and Planning rule-based games with deterministic intermediate steps where applicable.",
            "task_type": "Execution and Planning",
            "performance_metric": "NIJ-Acc Avg",
            "performance_value": "NIJ-Acc Avg = 51.97% (zh) and 49.67% (en) as reported in the paper.",
            "comparison_with_baseline": "Close behind o1-preview; shows more gradual decline across levels compared to some models (paper observation).",
            "key_findings": "Performs robustly across difficulty levels relative to many other models; benefits from few-shot on execution tasks.",
            "limitations": "Still fails many complex reasoning tasks; internal details not disclosed in paper.",
            "uuid": "e6814.2",
            "source_info": {
                "paper_title": "L OGIC G AME : Benchmarking Rule-Based Reasoning Abilities of Large Language Models",
                "publication_date_yy_mm": "2024-08"
            }
        },
        {
            "name_short": "claude-3.5-sonnet",
            "name_full": "Anthropic Claude 3.5 Sonnet",
            "brief_description": "A Claude-series closed-source model included in the evaluation that shows moderate performance on execution tasks but lower on planning.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "claude-3.5-sonnet",
            "model_description": "Closed-source transformer-based model from Anthropic (paper cites Claude family but does not give technical specs).",
            "model_size": null,
            "architecture_type": "transformer-based",
            "training_data": "not reported in this paper",
            "reasoning_method": "Prompting with rules + JSON output constraint; tested with few-shot exemplars in some settings.",
            "external_tool_used": false,
            "external_tool_description": null,
            "benchmark_name": "LOGICGAME",
            "benchmark_description": "Rule-following execution (string/matrix/operation tasks) and planning puzzles; requires process outputs for multi-step problems.",
            "task_type": "Execution and Planning",
            "performance_metric": "NIJ-Acc Avg",
            "performance_value": "NIJ-Acc Avg = 30.26% (zh) and 29.28% (en) as reported in the paper.",
            "comparison_with_baseline": "Substantially below the top performers (o1-preview/mini); shows larger drops at higher difficulty levels.",
            "key_findings": "Understands instructions/formatting often but fails in accurate rule application for multi-step tasks (case study evidence).",
            "limitations": "Fails detailed rule execution (e.g., Reversi piece flips) leading to incorrect intermediate states; lower process accuracy on planning tasks.",
            "uuid": "e6814.3",
            "source_info": {
                "paper_title": "L OGIC G AME : Benchmarking Rule-Based Reasoning Abilities of Large Language Models",
                "publication_date_yy_mm": "2024-08"
            }
        },
        {
            "name_short": "gpt-4o",
            "name_full": "GPT-4o (OpenAI)",
            "brief_description": "An OpenAI GPT-4 family variant evaluated on LOGICGAME; shows mixed performance, better on some planning tasks than other GPT variants.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "gpt-4o",
            "model_description": "Closed-source GPT-4 family transformer-based model (details not specified in paper).",
            "model_size": null,
            "architecture_type": "transformer-based",
            "training_data": "not reported in this paper",
            "reasoning_method": "Prompting with rules + JSON output constraint; few-shot experiments performed to assess in-context learning effects.",
            "external_tool_used": false,
            "external_tool_description": null,
            "benchmark_name": "LOGICGAME",
            "benchmark_description": "Execution and Planning rule-based games with step-by-step process evaluation.",
            "task_type": "Execution and Planning",
            "performance_metric": "NIJ-Acc Avg",
            "performance_value": "NIJ-Acc Avg = 27.06% (paper table).",
            "comparison_with_baseline": "Comparable to other strong closed-source models but below o1-preview/mini; within GPT family, relative strengths vary by category (paper: gpt-4-turbo outperforms gpt-4o on Planning in some comparisons).",
            "key_findings": "Shows mixed category-level performance; benefits from few-shot for execution in some cases but not uniformly.",
            "limitations": "Drops significantly as difficulty increases; process-level errors and JSON formatting errors occasionally observed.",
            "uuid": "e6814.4",
            "source_info": {
                "paper_title": "L OGIC G AME : Benchmarking Rule-Based Reasoning Abilities of Large Language Models",
                "publication_date_yy_mm": "2024-08"
            }
        },
        {
            "name_short": "gpt-4-turbo-0409",
            "name_full": "GPT-4-Turbo-0409 (OpenAI)",
            "brief_description": "Another GPT-4 variant assessed; shows differing strengths from gpt-4o across Execution vs Planning tasks.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "gpt-4-turbo-0409",
            "model_description": "Closed-source GPT-4 family transformer-based model (paper does not provide parameter counts).",
            "model_size": null,
            "architecture_type": "transformer-based",
            "training_data": "not reported in this paper",
            "reasoning_method": "Prompted with Rules + question + JSON output format; few-shot studied.",
            "external_tool_used": false,
            "external_tool_description": null,
            "benchmark_name": "LOGICGAME",
            "benchmark_description": "Execution (deterministic) and Planning (multi-step) tasks with intermediate process checking.",
            "task_type": "Execution and Planning",
            "performance_metric": "NIJ-Acc Avg",
            "performance_value": "NIJ-Acc Avg = 26.09% (paper table).",
            "comparison_with_baseline": "Compared to gpt-4o, gpt-4-turbo performed differently by subtask: paper notes gpt-4-turbo-0409 outperforms gpt-4o on Planning, but gpt-4o better on Execution in some comparisons.",
            "key_findings": "Model-level tradeoffs across Execution vs Planning observed; few-shot improves execution more than planning generally.",
            "limitations": "Performance degrades on higher difficulty levels; occasional JSON formatting/non-conformant outputs.",
            "uuid": "e6814.5",
            "source_info": {
                "paper_title": "L OGIC G AME : Benchmarking Rule-Based Reasoning Abilities of Large Language Models",
                "publication_date_yy_mm": "2024-08"
            }
        },
        {
            "name_short": "qwen2-72b-instruct",
            "name_full": "Qwen2-72B-instruct",
            "brief_description": "An open-source family model (Qwen) instruction-tuned variant evaluated; shows mid-range performance and notable few-shot improvement on execution.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "qwen2-72b-instruct",
            "model_description": "Open-source Qwen family model; 72B parameter variant (as indicated by name in the paper).",
            "model_size": "72B",
            "architecture_type": "transformer-based",
            "training_data": "not reported in this paper",
            "reasoning_method": "Prompting with rules + JSON output constraint; few-shot exemplars used in trials (0/1/2-shot experiments).",
            "external_tool_used": false,
            "external_tool_description": null,
            "benchmark_name": "LOGICGAME",
            "benchmark_description": "Rule-based execution and planning tasks with intermediate process verification.",
            "task_type": "Execution and Planning (rule application & planning)",
            "performance_metric": "NIJ-Acc Avg",
            "performance_value": "NIJ-Acc Avg = 21.16% (paper table).",
            "comparison_with_baseline": "Stronger models (e.g., qwen2-72b-instruct vs smaller qwen2-7b) show larger gains from few-shot in Execution tasks per paper; qwen2-72b shows notable shot-driven improvements on execution.",
            "key_findings": "Benefits from few-shot contexts on Execution; Planning often degraded by added exemplars for many models.",
            "limitations": "Still low absolute performance on complex planning tasks and higher difficulty levels; occasional errors in process formatting.",
            "uuid": "e6814.6",
            "source_info": {
                "paper_title": "L OGIC G AME : Benchmarking Rule-Based Reasoning Abilities of Large Language Models",
                "publication_date_yy_mm": "2024-08"
            }
        },
        {
            "name_short": "llama-3-70b-chat",
            "name_full": "Llama 3 70B Chat",
            "brief_description": "A large open-source LLaMA-3 chat model evaluated; shows relatively low NIJ-Acc on LOGICGAME with varying performance between Execution and Planning.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "llama-3-70b-chat",
            "model_description": "Open-source LLaMA-3 family chat model with 70B parameters (name indicates 70B in paper tables).",
            "model_size": "70B",
            "architecture_type": "transformer-based",
            "training_data": "not reported in this paper",
            "reasoning_method": "Prompting with rules + JSON output constraint; few-shot trials evaluated.",
            "external_tool_used": false,
            "external_tool_description": null,
            "benchmark_name": "LOGICGAME",
            "benchmark_description": "Execution and Planning games with deterministic processes where applicable.",
            "task_type": "Execution and Planning",
            "performance_metric": "NIJ-Acc Avg",
            "performance_value": "NIJ-Acc Avg = 14.45% (paper table).",
            "comparison_with_baseline": "Underperforms compared to top closed-source models; within-family performance shows tradeoffs (llama-3-70b better at Planning vs Execution for some tasks per paper).",
            "key_findings": "Performs poorly on many LOGICGAME tasks; stability issues (JSON/format errors) reported for some LLaMA variants.",
            "limitations": "Poor instruction-following on JSON constraint in some cases; low process accuracy especially on deterministic execution tasks like Reversi.",
            "uuid": "e6814.7",
            "source_info": {
                "paper_title": "L OGIC G AME : Benchmarking Rule-Based Reasoning Abilities of Large Language Models",
                "publication_date_yy_mm": "2024-08"
            }
        },
        {
            "name_short": "llama-3-8b-chat",
            "name_full": "Llama 3 8B Chat",
            "brief_description": "Smaller LLaMA-3 variant (8B) included in evaluation; shows very low NIJ-Acc and pronounced failures in instruction-following on LOGICGAME.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "llama-3-8b-chat",
            "model_description": "Open-source LLaMA-3 family chat model (8B parameters as indicated by name in paper tables).",
            "model_size": "8B",
            "architecture_type": "transformer-based",
            "training_data": "not reported in this paper",
            "reasoning_method": "Rules + question prompting with JSON output constraint; few-shot experiments included in study.",
            "external_tool_used": false,
            "external_tool_description": null,
            "benchmark_name": "LOGICGAME",
            "benchmark_description": "Execution (deterministic) and Planning tasks; requires process outputs for multi-step verification.",
            "task_type": "Execution and Planning",
            "performance_metric": "NIJ-Acc Avg",
            "performance_value": "NIJ-Acc Avg = 5.61% (paper table).",
            "comparison_with_baseline": "Performs worse than larger LLaMA-3 (70B) and most other evaluated models.",
            "key_findings": "Consistently poor instruction-following and reasoning performance; in Reversi case study many models except this one adhered to instruction format but still failed logical reasoning—this variant performed particularly poorly.",
            "limitations": "Very low accuracy on both answer and process metrics; frequent JSON/formatting errors reported.",
            "uuid": "e6814.8",
            "source_info": {
                "paper_title": "L OGIC G AME : Benchmarking Rule-Based Reasoning Abilities of Large Language Models",
                "publication_date_yy_mm": "2024-08"
            }
        },
        {
            "name_short": "glm-4-9b",
            "name_full": "GLM-4 9B",
            "brief_description": "A GLM family open-source model evaluated; small- to mid-sized (9B) model that attains low NIJ-Acc on LOGICGAME.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "glm-4-9b",
            "model_description": "Open-source GLM-4 family transformer-based model with 9B parameters as indicated by the name in paper tables.",
            "model_size": "9B",
            "architecture_type": "transformer-based",
            "training_data": "not reported in this paper",
            "reasoning_method": "Prompting with rules + JSON process constraint; few-shot trials included.",
            "external_tool_used": false,
            "external_tool_description": null,
            "benchmark_name": "LOGICGAME",
            "benchmark_description": "Rule-based execution and planning tasks with step-wise verification.",
            "task_type": "Execution and Planning",
            "performance_metric": "NIJ-Acc Avg",
            "performance_value": "NIJ-Acc Avg = 9.62% (paper table).",
            "comparison_with_baseline": "Performs poorly compared to larger/stronger models; smaller models show smaller gains from few-shot prompting on planning tasks.",
            "key_findings": "Limited capability on strict rule-following and multi-step planning; few-shot helps execution for some models but not uniformly.",
            "limitations": "Low absolute accuracy; JSON/formatting and process errors present.",
            "uuid": "e6814.9",
            "source_info": {
                "paper_title": "L OGIC G AME : Benchmarking Rule-Based Reasoning Abilities of Large Language Models",
                "publication_date_yy_mm": "2024-08"
            }
        },
        {
            "name_short": "mistral-7b-instruct",
            "name_full": "Mistral 7B Instruct",
            "brief_description": "A 7B open-source instruct-tuned model included in evaluation that shows low NIJ-Acc and some JSON-formatting issues.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "mistral-7b-instruct",
            "model_description": "Open-source Mistral family instruct-tuned transformer (7B parameters inferred from name and cited Mistral paper).",
            "model_size": "7B",
            "architecture_type": "transformer-based",
            "training_data": "not reported in this paper",
            "reasoning_method": "Prompting with Rules + JSON constraint; few-shot experiments performed.",
            "external_tool_used": false,
            "external_tool_description": null,
            "benchmark_name": "LOGICGAME",
            "benchmark_description": "Execution and Planning games where intermediate process steps can be automatically verified.",
            "task_type": "Execution and Planning",
            "performance_metric": "NIJ-Acc Avg",
            "performance_value": "NIJ-Acc Avg = 4.39% (paper table).",
            "comparison_with_baseline": "Among the lower-performing models in the study; performed poorly on JSON-constrained outputs (paper flags high JSError/IFError for some smaller models).",
            "key_findings": "Failed frequently on strict rule-following tasks; few-shot effects unstable.",
            "limitations": "High error rates on JSON formatting; very low correct process/answer rates on many tasks.",
            "uuid": "e6814.10",
            "source_info": {
                "paper_title": "L OGIC G AME : Benchmarking Rule-Based Reasoning Abilities of Large Language Models",
                "publication_date_yy_mm": "2024-08"
            }
        },
        {
            "name_short": "internlm-2.5-7b-chat",
            "name_full": "InternLM-2.5 7B Chat",
            "brief_description": "A multilingual open-source chat model (7B-ish) evaluated; obtains low NIJ-Acc on LOGICGAME.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "internlm-2.5-7b-chat",
            "model_description": "Open-source InternLM family chat model (name indicates 2.5 / 7B variant; paper lists as evaluated).",
            "model_size": "7B",
            "architecture_type": "transformer-based",
            "training_data": "not reported in this paper",
            "reasoning_method": "Prompting with rules and JSON output constraints; few-shot contexts evaluated.",
            "external_tool_used": false,
            "external_tool_description": null,
            "benchmark_name": "LOGICGAME",
            "benchmark_description": "Execution and Planning problems with process-level verification; bilingual dataset.",
            "task_type": "Execution and Planning",
            "performance_metric": "NIJ-Acc Avg",
            "performance_value": "NIJ-Acc Avg = 5.38% (paper table).",
            "comparison_with_baseline": "Performs poorly relative to larger models; small models have high variability and frequent formatting errors.",
            "key_findings": "Low performance; few-shot effects minor or unstable.",
            "limitations": "Frequent JSON/formatting errors and low process correctness on deterministic tasks.",
            "uuid": "e6814.11",
            "source_info": {
                "paper_title": "L OGIC G AME : Benchmarking Rule-Based Reasoning Abilities of Large Language Models",
                "publication_date_yy_mm": "2024-08"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Chain-of-thought prompting elicits reasoning in large language models",
            "rating": 2,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Language Models are Few-Shot Learners",
            "rating": 2,
            "sanitized_title": "language_models_are_fewshot_learners"
        },
        {
            "paper_title": "Training language models to follow instructions with human feedback",
            "rating": 2,
            "sanitized_title": "training_language_models_to_follow_instructions_with_human_feedback"
        },
        {
            "paper_title": "Transformers as soft reasoners over language",
            "rating": 1,
            "sanitized_title": "transformers_as_soft_reasoners_over_language"
        },
        {
            "paper_title": "A unified benchmark for mathematical reasoning",
            "rating": 1,
            "sanitized_title": "a_unified_benchmark_for_mathematical_reasoning"
        }
    ],
    "cost": 0.01801925,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>LOGICGAME: Benchmarking Rule-Based Reasoning Abilities of Large Language Models
12 Oct 2024</p>
<p>Jiayi Gui 
Tsinghua University</p>
<p>Yiming Liu 
Tsinghua University</p>
<p>Jiale Cheng 
Tsinghua University</p>
<p>Xiaotao Gu 
Tsinghua University</p>
<p>Xiao Liu 
Tsinghua University</p>
<p>Hongning Wang 
Tsinghua University</p>
<p>Yuxiao Dong 
Tsinghua University</p>
<p>Jie Tang 
Tsinghua University</p>
<p>Minlie Huang 
Tsinghua University</p>
<p>Zhipu Ai 
Tsinghua University</p>
<p>LOGICGAME: Benchmarking Rule-Based Reasoning Abilities of Large Language Models
12 Oct 2024D8A8E3025D0E63DFA18C57450A6528BAarXiv:2408.15778v4[cs.AI]1. X = 5: too low 2. X = 8: too high 3. X = 7: too high Determine X.Process claude-3.5-sonnet
Large Language Models (LLMs) have demonstrated notable capabilities across various tasks, showcasing complex problem-solving abilities.Understanding and executing complex rules, along with multi-step planning, are fundamental to logical reasoning and critical for practical LLM agents and decision-making systems.However, evaluating LLMs as effective rule-based executors and planners remains underexplored.In this paper, we introduce LOGICGAME, a novel benchmark designed to evaluate the comprehensive rule understanding, execution, and planning capabilities of LLMs.Unlike traditional benchmarks, LOGICGAME provides diverse games that contain a series of rules with an initial state, requiring models to comprehend and apply predefined regulations to solve problems.We create simulated scenarios in which models execute or plan operations to achieve specific outcomes.These game scenarios are specifically designed to distinguish logical reasoning from mere knowledge by relying exclusively on predefined rules.This separation allows for a pure assessment of rule-based reasoning capabilities.The evaluation considers not only final outcomes but also intermediate steps, providing a comprehensive assessment of model performance.Moreover, these intermediate steps are deterministic and can be automatically verified.LOGICGAME defines game scenarios with varying difficulty levels, from simple rule applications to complex reasoning chains, in order to offer a precise evaluation of model performance on rule understanding and multi-step execution.Utilizing LOGICGAME, we test various LLMs and identify notable shortcomings in their rule-based logical reasoning abilities.</p>
<p>Introduction</p>
<p>Large Language Models (LLMs) have shown notable abilities in a wide range of tasks and even complex problem-solving [4; 37; 7; 12].Their ability to reason, encompassing the understanding of intricate scenarios, strategic planning, and multi-step execution, is crucial for developing advanced AI agents and decision-making systems [20; 30; 6].These capabilities allow LLMs to understand complex user instructions, make logical decisions, and execute tasks accurately.</p>
<p>As alignment becomes integral to the application of LLMs [25; 1; 27; 5] , the primary goal is to align with human intentions and accurately execute their instructions.Simultaneously, these models must possess strong reasoning abilities to handle complicated scenarios.However, evaluating LLMs as</p>
<p>Reference:</p>
<p>Answer: 6, Process: [6,10]; [6,7] Difficulty Levels: 0 effective rule-based executors and planners remains underexplored.Traditional benchmarks usually focus solely on instruction-following or logical reasoning, neglecting the combination of both.Thus, they fail to comprehensively assess the model's reasoning capabilities after elaborate alignment.</p>
<p>Example of Cryptanalysis</p>
<p>In this paper, we introduce LOGICGAME, a novel benchmark crafted to evaluate the comprehensive rule understanding, planning, and execution capabilities of LLMs.LOGICGAME offers a set of carefully designed rule-based reasoning games.Each game contains a series of rules the model must follow to find the solution involving single or multiple steps.During the data construction process, we ensure that all the problems remain unavailable on the Internet to prevent data leakage.LOGICGAME covers two main scenarios: execution and planning, each divided into several subcategories.Execution problems include tasks related to string data manipulation, where models handle string data transformations, as well as arithmetic operations and manipulations, focusing on mathematical computations and sequential execution.Planning games encompass math puzzles, which require solving complex mathematical problems, and pure logic puzzles, involving abstract reasoning without numerical computation.Through these varied games, LOGICGAME aims to comprehensively evaluate the rule-based reasoning capabilities in LLMs. 1n LOGICGAME, our goal is to evaluate how well LLMs can reason according to given rules, so we ensure that no additional knowledge is required.The final answer and the process in these scenarios rely solely on the given rules, fostering a pure assessment of the models' rule-based reasoning capabilities.Moreover, the evaluation process in LOGICGAME involves not only the final answers but also the intermediate steps taken by the models, in order to offer a holistic view of the models' performance.Furthermore, process evaluation enables us to determine whether the model faithfully reasons based on established rules rather than merely guessing answers.In addition, these intermediate steps are deterministic and can be automatically verified.To thoroughly assess the rule comprehension and multi-step execution capabilities of various LLMs, LOGICGAME presents problems with multiple difficulty levels.We determine the complexity of each problem by the number of reasoning steps involved.Simple games may require only single-step reasoning, while more challenging game scenarios require multiple reasoning steps, reflecting the need for deeper understanding and more sophisticated reasoning.</p>
<p>By leveraging LOGICGAME, we have conducted extensive experiments across a wide range of LLMs, including api-based models like GPT and GLM families, as well as open-source models such as Qwen and Llama families.Our findings indicate that while LLMs showcase good performance in a variety of tasks, they still exhibit notable shortcomings in rule-based logical reasoning.Even the best-performing LLMs struggle with complex reasoning tasks with about 20% overall accuracy and less than 10% on level 3 tasks.Additionally, while few-shot demonstrations can help with execution tasks, they may damage the performance of planning tasks.</p>
<p>Our contributions can be summarized as follows:</p>
<p>• We introduce LOGICGAME, a novel benchmark for rule-based reasoning, including execution and planning tasks, with varying difficulty levels.• We design an automated assessment process for LOGICGAME, which not only checks the final answers but also analyzes the solution process to comprehensively evaluate LLMs' reasoning abilities.• We conduct extensive experiments on LOGICGAME across a wide range of LLMs, effectively exposing their deficiencies in rule-based reasoning with the best about 25% overall accuracy.</p>
<p>Related Work</p>
<p>The capability to reason has long been a crucial aspect of language models.Research [34] has demonstrated that as the size of models increases, their ability to reason emerges, making it a fundamental attribute of LLMs.To elicit this reasoning ability, techniques like chain-of-thought prompting [35] and specialized training [23] have become widely adopted.Multi-step reasoning, in particular, is essential for complex decision-making and planning tasks, such as those undertaken by LLM agents [20].[9] to Olympiad-level challenges [16] and encompass a variety of formats, from word problems to theory proving [19; 18].These problems often demand not just reasoning but also robust calculation abilities.Knowledge-based reasoning, particularly commonsense reasoning [22; 24], is another pivotal focus.These benchmarks are designed to determine whether models possess commonsense knowledge and can leverage it to reason effectively.Advancing further, theoryof-mind reasoning [14] examines whether models can understand and incorporate complex layers of human cognition, such as thoughts and beliefs.
✗ ✗ ✗ ✗ ✗ semi-synthetic GSM8K [9] ✓ ✓ ✗ ✗ ✗ human-annotated PRONTOQA [28] ✓ ✓ ✗ ✗ ✗ synthetic FOLIO [13] ✗ ✗ ✗ ✓ ✓ human-annotated BIG-Bench Hard [31] ✗ ✗ ✗ ✓ ✗ human-annotated STRATEGYQA [11] ✗ ✗ ✗ ✗ ✗ human-annotated Ours ✓ ✓ ✓ ✓ ✓ human-annotated
LLMs have undergone extensive alignment, with a key focus on following human instructions.However, reasoning with the capability of instruction-following remains underexplored.Thus, we propose LOGICGAME, a benchmark designed to assess rule-based reasoning, which is a natural integration of logical reasoning with instruction-following capabilities.We compare LOGICGAME with each benchmark in Table 1.</p>
<p>LOGICGAME</p>
<p>Data Construction</p>
<p>This section outlines our systematic approach to dataset construction, which comprises four key phases:</p>
<ol>
<li>
<p>Design rule-based problems inspired by real-world scenarios ( §3.1.1).</p>
</li>
<li>
<p>Develop output constraints to standardize evaluation formats ( §3.1.2).</p>
</li>
<li>
<p>Implement a spectrum of difficulty levels and incorporating exemplars ( §3.1.3).</p>
</li>
<li>
<p>Create bilingual versions through meticulous translation of problems and instructions ( §3.1.4).</p>
</li>
</ol>
<p>Problem Collection and Design</p>
<p>Collection and extraction of real-world scenarios.The integration of rule-following and reasoning is a critical aspect of many real-world tasks, yet existing benchmarks often fail to adequately capture this well.To address this gap, we developed a novel problem set through extensive research and crowdsourcing.We found that these tasks are similar to some game mechanics, since real-world tasks often share features with games, such as having specific rules to follow, requiring decision-making.This insight led us to adopt a gamification approach, allowing for a nuanced evaluation of models' rule-following reasoning capabilities.</p>
<p>Reference</p>
<p>Answer: 10, Process: 1 (for "AA") +1(for "AA") + 2 (for "BB") + 3 (for "CCCC") + 2 (for "BB") + 1 (for "AAAA")</p>
<p>Model Generation</p>
<p>Prompting</p>
<p>Rules + Question + Output Format Constrain</p>
<p>Response Collection</p>
<p>Step 1: score +1 (for "AA")</p>
<p>Step 2: score +2 (for "BB")</p>
<p>Step 3: score +3 (for "CCCC")</p>
<p>Step 4: score +2 (for "BB")</p>
<p>Step 5: score +1 (for "AAAA")</p>
<p>Step Execution domain.In the context of our benchmark, execution refers to a reasoning process characterized by deterministic, single-step inferences.Here, models must apply well-defined rules to manipulate strings or states, with each step yielding a predictable outcome based on the current state and the applied rule.These tasks often require models to execute the correct action from given information, simulating real-world scenarios where explicit instructions are given.</p>
<p>Planning domain.The planning domain in our benchmark represents a higher order of cognitive complexity, involving long-term strategic thinking and multi-step decision making within rulegoverned environments.Planning problems challenge models to analyze potential future states, formulate strategies, and determine a sequences of actions to reach a solution.Importantly, our focus in this domain is on identifying a correct solution path rather than optimizing for efficiency, mirroring many real-world scenarios where finding any valid solution is the primary goal.</p>
<p>Rule-based problem design and quality control.Following the establishment of our categories, a team of expert human annotators developed problems for each category with a focus on novelty and challenging out-of-domain reasoning, which makes it harder to overfit.To mitigate potential semantic ambiguities associated with natural language reasoning [10], we minimized reliance on natural language constructs.Our problems are designed such that the reasoning process does not necessitate natural language inference, allowing for a more direct evaluation of reasoning abilities.</p>
<p>In the execution domain, we ensured that every step is deterministic and verifiable, facilitating precise evaluation and preventing models from resorting to guesswork.For the inherently less deterministic planning problems, we introduced intermediate checkpoints or state variables where appropriate, allowing for a more granular assessment of the problem-solving process.Detailed specifications of these evaluation methods will be provided in Section 3.2.</p>
<p>Output Constraint Design</p>
<p>To facilitate precise evaluation and streamline the matching process, we mandated a structured JSON output format for model responses.Our evaluation criteria are tailored to the complexity of each problem.For single-step problems categorized as Level 0, models are only required to output the final answer, and evaluation is based solely on the correctness of this answer.However, for problems involving multiple steps or more complex reasoning, which include Levels 1, 2, 3, and certain Level 0 problems, we evaluate both the answer and the process.</p>
<p>In both cases, the output JSON structure includes 'answer', which is a list of strings representing the final solution(s), and for second cases the output also includes 'process', a list of strings detailing each step of the problem-solving process.The details of JSON constraints can be found in Appendix A</p>
<p>Difficulty Levels and Exemplars</p>
<p>To comprehensively assess models' reasoning capabilities, we have structured our benchmark with four distinct difficulty levels (0, 1, 2, and 3) for each task.The difficulty gradient is determined by two key factors: the complexity of the rules involved and the number of reasoning steps required to arrive at the solution.Each successive level systematically introduces additional rules and reasoning steps.In general, our problems are difficult for models, and some are also challenging for humans.</p>
<p>Furthermore, to evaluate models' capacity for rule acquisition and application, we have developed two distinct exemplars for each question.These exemplars consist of a given question, the correct answer, a step-by-step solution process, and detailed explanations.By providing these examples, we aim to test not only the models' baseline performance but also their ability to learn from demonstrations and apply newly acquired rules to similar problems.</p>
<p>Building Bilingual Benchmark</p>
<p>Initially, our questions were designed in Chinese.However, we recognized that this could potentially bias the benchmark against LLMs primarily trained on English data.To ensure fairness and broader applicability of our benchmark, we developed a comprehensive bilingual benchmark containing both zh(Chinese) and en(English) version.</p>
<p>Evaluation Protocol</p>
<p>Each model is prompted with a set of rules specific to the given problem, along with a corresponding question and a JSON format constraint for the output, encompassing both the answer and the process, as illustrated in Figure 2.For few-shot trials, example(s) are inserted between the rules and the question to assess the model's in-context learning capabilities.The model responses are then collected and subjected to automated evaluation.As previously mentioned, the evaluation protocol is designed to assess not only the correctness of the answer but also the correctness of the process that led to the answer.Scoring for each problem's answer is determined by comparing the model's response to the reference answer.Similarly, scoring for each problem's process, as defined by the JSON format constraint, is achieved by assessing the degree of alignment between the model's process and the reference process.Specifically, the LOGICGAME defines three metrics related to each problem for scoring:</p>
<ol>
<li>Answer Accuracy(A-Acc): This metric evaluates the correctness of the answers for all given questions, providing a binary assessment (0/1) for each answer to indicate whether it is correct or not.</li>
</ol>
<p>Process Accuracy(P-Acc):</p>
<p>This metric assesses the correctness of the process, measuring the percentage match based on character-level similarity between the provided process and the expected process.In rare cases where no process is provided in level 0 questions as single-step reasoning, process accuracy are considered equally with answer accuracy for scoring.</p>
<ol>
<li>Answer Process Accuracy(AP-Acc): This composite metric evaluates the overall accuracy of both the answer and the process.Its calculation involves an aggregate score derived by combining answer accuracy and process accuracy using a logical AND operation.</li>
</ol>
<p>Experiments</p>
<p>Experimental Setup</p>
<p>We evaluate 14 popular LLMs.Closed-source models included versions from Claude [1] and GPT [26] series.Open-source models encompassed LLaMA 3 [33], Qwen [2], GLM [12], Mistral [17],</p>
<p>and InternLM [32] variants.In the inference stage, we set temperature to 0, ensuring deterministic outputs.The maximum token number is set to 2048.Other parameters are set as their default values.</p>
<p>Main Results and Analysis</p>
<p>Table 3 presents the performance of 14 LLMs on LOGICGAME across our Chinese and English bilingual dataset, measured by AP-Acc.The o1-preview model leads with 54.93% and 53.29% overall accuracy for Chinese and English respectively, closely followed by o1-mini.These results underscore the persistent challenge of reasoning for LLMs, as even top performers barely exceed 50% accuracy.The substantial performance gap, ranging from over 50% to below 5%, not only highlights  the varying capabilities of current LLMs in complex reasoning tasks, but also emphasizes that despite recent advancements, logical reasoning remains a significant hurdle for most language models.</p>
<p>The performance drop from Level 0 to Level 3 is not uniform across models.Some models (e.g.o1-mini) show a more gradual decline, while others drop off sharply after Level 1.This may suggest that some models have better consistency across task complexities.</p>
<p>Interestingly, the performance of models in Execution and Planning tasks varies.Some top-performing models, such as o1-preview, demonstrates superior performance in Execution compared to Planning.Conversely, llama-3-70b-chat excels in Planning over Execution.Even within model families, the relative performance differences are evident.In the GPT family, gpt-4-turbo-0409 outperforms gpt-4o in Planning tasks, while gpt-4o shows better performance in Execution tasks.</p>
<p>During the evaluation, it was observed that some models occasionally failed to adhere to the requirement of producing JSON format output.Detailed results and analysis are provided in Appendix B. Despite this, the overall error rates are low across most models, resulting in minimal differences in rankings among those with similar performance.</p>
<p>Few-shot Results</p>
<p>We conducted experiments to analyze the change of model's performance on 0-shot, 1-shot and 2-shot settings.And models of gpt-4o, qwen2-72b-instruct and qwen2-7b-instruct, llama-3-70b-chat and llama-3-8b-chat, glm-4-9b, mistral-7b-instruct and internlm-2.5-7b-chatare chosen for this trial.The analysis demonstrated in Figure 4 and Figure 5 reveals mixed results of LOGICGAME's zh version in the "Planning" and "Execution" categories, difficulty levels across the various shot settings.Appendix D shows the results of LOGICGAME's en version.In the "Execution" category, models demonstrate notable improvements in accuracy with increased shot contexts demonstrated in Figure 4. Specifically, stronger models (as indicated in Table 3a and Table 3b) like gpt-4o, qwen2-72b-instruct shows a greater increase in the AP-Acc score when transitioning from 0-shot to 1-shot and 2-shot settings than weaker ones, indicating enhanced execution accuracy with more contextual information.However, the effects of 1-shot and 2-shot settings vary across models Performance variations by difficulty levels, as shown in Figure 5, indicate that models benefit most from 1-shot and 2-shot contexts at Level 0. And in general, the influence of shot contexts diminishes as the difficulty level increases.This consistency suggests that simpler tasks (Level 0) allow models to leverage additional context more effectively, enhancing their execution capabilities across the board.</p>
<p>Conversely, the "Planning" category presents more heterogeneous results.Models often show declines in performance when moving from 0-shot to 1-shot or 2-shot settings demonstrated in Figure 4.These results suggest that while additional context can enhance performance for some models, it can introduce noise for others, potentially obfuscating key elements necessary for planning tasks.Overall, these observations highlight that the added context's efficacy is highly contingent on the task and model characteristics.Furthermore, Figure 5 illustrates that the negative impact of 1-shot contexts is most pronounced at Level 0. As the difficulty level increases, the influence of 1-shot contexts diminishes, leading to smaller performance fluctuations, while 2-shot contexts are more unstable with no pattern found.</p>
<p>Discussion</p>
<p>Case study on Revesi game.From Appendix E, it is evident that all models perform poorly in the Reversi game.Consequently, we conducted a case study on this particular game scenario.</p>
<p>The responses from various models tasked with determining the outcome of a Reversi game are analyzed as shown in Figure 6.Despite all models except llama-3-8b-chat adhering to the instruction format and correctly interpreting the initial setup, all models failed to provide the correct answer, demonstrating various types of inaccuracies.The key reasons for failure include:</p>
<ol>
<li>Mismanagement of Details: For instance, claude-3.5-sonnetmisplaced markers or incorrectly transformed pieces, showing that while the general rules were understood, the model failed to apply specific game rules correctly.2. Inadequate Execution/Planning Understanding: Models like qwen2-72b-instruct produced incorrect board states following what should have been straightforward captures, revealing a fundamental misunderstanding of the game's piece-flipping mechanisms as well as the initial conditions outlined in the problem.3. Excessive Alterations: The llama-3-8b-chat model drastically altered the board state in an unrealistic manner, adding rows and altering more positions than the rules allow, suggesting a misinterpretation of the core principles of the game, particularly with regards to matrix operations and the understanding and execution of piece-flipping mechanisms.</li>
</ol>
<p>Conclusion</p>
<p>In this paper, we introduce LOGICGAME, a novel benchmark designed to evaluate the rule-based reasoning capabilities of LLMs.LOGICGAME encompasses multiple difficulty levels, focusing on assessing models' understanding of rules, execution based on these rules, and planning abilities.Moreover, we have developed methods to evaluate both outcomes and reasoning processes, ensuring that models follow the given rules faithfully rather than merely guessing answers.Extensive experiments indicate that current large models still exhibit significant deficiencies in rule-based reasoning tasks.More effort needs to be devoted to further enhancing models' abilities to handle complex reasoning scenarios.
S NIJ−Acc = S AP−Acc 1 − S IFError − S JSError(1)
The metrics presented in Table 3 and Table 4 provide a comprehensive assessment of the model's error frequency in en and zh version respectively.The NIJ-Acc metric evaluates the model's overall accuracy.In this study, the llama-3-8b-chat and llama-3-70b-chat model shows consistent poor instruction-following capabilities in both versions, while the mistral-7b-instruct model performs poorly in the metric of json format.Conversely, stronger models tend to perform more stably as observed in these metrics.Overall error probabilities (IFError, JSError) are low across most models, resulting in minimal variations in NIJ-Acc scores compared to the average scores in Table ??, with only minor rank adjustments among similarly performing models.</p>
<p>D Few shot results of en version</p>
<p>E Problems all models fail</p>
<p>Figure 10 categorizes the five areas where all models exhibit the poorest performance, presenting the average AP-ACC scores for each category in a heat map.The horizontal axis corresponds to the model capabilities as outlined in Table 3b and Table 3a.This figure highlights that models generally struggle most in two sub-categories within the execution scenario, particularly with 'Reversi', where many models score close to zero except for o1-mini and o1-preview models.Conversely, in planning scenarios such as 'Constrained Linear Arrangement', there is a slight variation in performance across different models.</p>
<p>Figure 1 :
1
Figure 1: Evaluation results and demonstrations of LOGICGAME.(Bottom) Case study on two examples from execution and planning category respectively.(Top) Performance of various models across execution and planning categories.The performance is arithmetic mean of LOGICGAME's Chinese and English version.Most models struggle on LOGICGAME getting less than 12% scores in both categories.Two top-performing models highlighted with pink stars stand out.</p>
<p>1 . 2 . 2 ) 1 .
1221
Answer &amp; Process Extraction: Answer: 10, Process: 1 (for "AA") + 2 (for "BB") + 3 (for "CCCC") + 2 (for "BB") + 1 (for "AAAA") Matching with Reference 3. Accuracy Calculation 1) Answer is correct The second of 6 steps are incorrect leading to 5Rules Start with a score of 0. For a string of A, B, C, and D, scan from left to right: Rule1 "AA" or more A → score +1 Rule2 "BB" or more B → score +2 Rule3 "CC" or more C → score +3 2. Question Total score for string "AABAABBCCCCBBAAAAC"?</p>
<p>6 :Figure 2 :
62
Figure 2: Illustration of taxonomy and evaluation protocol in LOGICGAME.Taxonomy illustration highlights categories involving mathematics in purple.Json format constrain in evaluation is ommitted due to space limitations and can be referred to Appendix A.</p>
<p>Figure 3 :
3
Figure 3: Performance comparison of 14 models on LOGICGAME measured by AP-Acc for both Chinese (zh) and English (en) versions.</p>
<p>Figure 4 :
4
Figure 4: Few-shot differences on execution and planning category of LOGICGAME's zh version."shot_diff_1_0" represents the difference in the P-Acc score between the 1-shot and 0-shot settings, calculated as the result of 1-shot minus the result of 0-shot, "shot_diff_2_0" representing the P-Acc score between the 2-shot and 0-shot settings similarly.</p>
<p>Figure 5 :
5
Figure 5: Few-shot differences on difficulty levels of LOGICGAME's zh version with shot difference settings similar with Figure 4.</p>
<p>Figure 6 :
6
Figure 6: An example of a Reversi game with model outputs, including the answer and process, is shown.The board states for initial, reference, and model outputs are visualized with errors highlighted in red.JSON constraints are omitted due to space, referenced in Figure 7.</p>
<p>Figure 8 and
8
Figure8and Figure9shows few-shot results of LOGICGAME's en version.For 'Execution' category, the observed conclusion is aligned with zh version.Shot contexts increases P-Acc score in most cases except glm-4-9b and llama-3-8b-chat.When difficulty levels are increased, the positive effect of shot contexts declines.For 'Planning' category, most models decrease or has no change when extra shot contexts are added except for llama-3-70b-chat and mistral-7b-instruct.No notable effect on shot contexts on different difficulty levels except for level 1.</p>
<p>Figure 8 :
8
Figure 8: Few-shot differences on execution and planning category of LOGICGAME's en version."shot_diff_1_0" represents the difference in the P-Acc score between the 1-shot and 0-shot settings, calculated as the result of 1-shot minus the result of 0-shot, "shot_diff_2_0" representing the P-Acc score between the 2-shot and 0-shot settings similarly.</p>
<p>Figure 9 :
9
Figure 9: Few-shot differences on difficulty levels of LOGICGAME's en version with shot difference settings similar with Figure 8.</p>
<p>Figure 10 :
10
Figure 10: Average AP-Acc scores for five categories with the poorest performance: Reversi and Synthesis and Decomposition from the 'Execution' category, and Minesweeper, Constrained Linear Arrangement, and Logic Puzzle from the 'Planning' category.</p>
<p>Table 1
1: Compare LOGICGAME with other logical reasoning benchmarks. BIG-Bench Hard com-parison limited to algorithmic part for relevance. Verifiable: Each step in process verifiable or not.Determinism: Each step in process determined or not. Verifiability and determinism guaranteeautomated evaluation of process. Exemplars: Examples provided or not.Numerous benchmarks have been established over time to rigorously evaluate the reasoning capabili-ties of neural network models. Early research has concentrated on logical reasoning [3; 8; 36]. Thesestudies cover various forms of logic, including inductive, deductive, and abductive reasoning, andaim to assess whether models can infer answers based on given conditions. Mathematical reasoningrepresents another critical area [15; 21]. Benchmarks in this domain range in difficulty from gradeschool problems</p>
<p>Performance of 14 models on LOGICGAME of zh version.The highest performance is bold.
ModelExecution Level 0 Level 1 Level 2 Level 3 Avg.Planning Level 0 Level 1 Level 2 Level 3 Avg.Overallo1-preview82.2266.6748.8942.2260.00 64.5258.0635.4832.2647.58 54.93o1-mini77.7857.7848.8942.2256.67 74.1958.0635.4812.9045.16 51.97claude-3-5-sonnet 68.8940.0022.2215.5636.67 48.3925.816.453.2320.97 30.26gpt-4o62.2231.1113.3313.3330.00 51.6122.589.686.4522.58 26.97gpt-4-turbo-040960.0015.5615.5611.1125.56 67.7422.589.683.2325.81 25.66glm-4-plus42.2226.6717.786.6723.33 48.3919.356.453.2319.36 21.71qwen2-72b53.3320.0015.562.2222.78 45.1616.133.233.2316.94 20.39llama-3-70b42.2211.116.672.2215.56 25.816.450.000.008.0712.50claude-3-haiku31.114.442.220.009.4432.266.450.000.009.689.54glm-4-9b24.448.892.220.008.8919.353.230.000.005.657.57internlm2-5-7b13.334.440.000.004.4416.133.230.000.004.844.61llama-3-8b11.112.220.000.003.339.686.450.000.004.033.62mistral-7b4.440.000.000.001.1119.353.230.000.005.652.96qwen2-7b4.440.000.000.001.1116.133.230.000.004.842.63(a) ModelExecution Level 0 Level 1 Level 2 Level 3 Avg.Planning Level 0 Level 1 Level 2 Level 3 Avg.Overallo1-preview71.1157.7848.8951.1157.22 70.9754.8441.9422.5847.58 53.29o1-mini73.3346.6744.4446.6752.78 70.9748.3938.7122.5845.16 49.67claude-3-5-sonnet 46.6740.0033.3313.3333.33 64.5216.136.456.4523.39 29.28gpt-4o57.7837.7831.1113.3335.00 48.3919.353.233.2318.55 28.29gpt-4-turbo-040942.2220.0017.788.8922.22 51.6112.909.683.2319.36 21.05glm-4-plus31.1115.5617.786.6717.78 48.399.689.683.2317.75 17.76qwen2-72b28.894.440.000.008.3322.586.456.453.239.688.88glm-4-9b22.226.672.222.228.3322.586.450.000.007.267.89internlm2-5-7b22.224.444.440.007.7812.903.230.000.004.036.25claude-3-haiku8.898.890.000.004.4522.580.000.000.005.654.93llama-3-70b8.898.898.890.006.673.233.230.000.001.624.61mistral-7b22.220.000.000.005.569.680.000.000.002.424.28qwen2-7b4.442.220.000.001.676.450.000.000.001.611.64llama-3-8b0.000.000.000.000.000.000.000.000.000.000.00
(b) Performance of 14 models on LOGICGAME of en version.The highest performance is bold.</p>
<p>Table 2 :
2
An overview of LOGICGAME, including Execution and Planning.Light red rows indicate sequential-based tasks, light blue rows indicate matrix-based tasks.Tasks involving math are denoted with the superscript
Categories Basic TasksApplication#SamplesCharacter search12String insertion8String synthesisSynthesis and decomposition24String deletion and modification8String rearrangement8String splitting8ExecutionString processingMahjong-type16Statistical counting λ8New operator calculation λ12Element operationsLights out20Pattern recognitionReversi16Matrix transformation2048 λ24Path movementPooling λ16Single-choice self-reasoning8Constrained Linear Arrangement16Mutual generation and restriction16Logical equations λ8Combinatorial calculation λ8PlanningEight Queens puzzleLetter logic diagram16Logic puzzle λ12Minesweeper λ8Standard Sudoku λSudoku with arithmetic rules λ 16Cryptanalysis λ16Total304
λ2. Instruction Following Error (IFError): This metric measures the number of instances in which a JSON format could not be successfully extracted.3.Answer Process Accuracy based on non-IFError and non-JSError (NIJ-Acc): This is the correctness metric, which evaluates the Answer Process Accuracy (AP-Acc) concerning non-IFError and non-JSError.The formula is provided in Equation 1.</p>
<p>Table 3 :
3
Model performance on JSError, ResNError, IFError and NIJ-Acc metrics of en version, with the Avg.calculated as the arithmetic mean NIJ-Acc value of both execution and planning.‡and †shows the worst and second worst performance in error metrics respectively.Underline shows the best performance in NIJ-Acc metric.
ModelIFError(%↓) Execution Planning Execution Planning Execution Planning JSError(%↓) NIJ-Acc(%↑)Avg.o1-preview0.000.000.000.0060.0047.58 54.93o1-mini0.560.810.560.0056.6745.16 51.97claude-3.5-sonnet0.000.000.000.0036.6720.97 30.26gpt-4o0.560.000.000.0030.1722.58 27.06gpt-4-turbo-04090.561.610.001.6125.7026.67 26.09glm-4-plus10.008.060.560.0026.0921.05 24.00qwen2-72b-instruct3.331.610.561.6123.7017.50 21.16llama-3-70b-chat12.78†13.710.000.8117.839.43 14.45claude-3-haiku3.890.810.000.819.839.849.83glm-4-9b25.00†13.711.110.8112.036.609.62llama-3-8b-chat‡40.56‡28.230.000.005.615.625.61internlm-2.5-7b-chat17.781.611.67‡5.655.525.225.38mistral-7b-instruct†36.6711.29‡7.22†4.841.986.734.39qwen2-7b-instruct3.892.42†2.224.031.185.172.81</p>
<p>Table 4 :
4
Model performance on JSError, ResNError, IFError and NIJ-Acc metrics of zh version, with the Avg.calculated as the arithmetic mean NIJ-Acc value of both execution and planning.‡and †shows the worst and second worst performance in error metrics respectively.Underline shows the best performance in NIJ-Acc metric.</p>
<p>We have released the dev set and the whole set of the LOGICGAME, and created a leaderboard on https://github.com/Hypatiaalegra/LogicGame-Data.You can refer to https://www.codabench.org/competitions/4140/ for a fair and fast evaluation of the LOGICGAME.
* 1 * * * * * * * 1 0 * * 0 1 0 * * * * * * * * * 1 1 * * 0 0 0 * 1 * * Answer qwen2-72b-instruct Process qwen2-72b-instruct * * * * * 1 0 * * 0 1 0 * 1 * * * * * * * 1 0 * * 0 1 0 * * * * * * * * * 1 0 * * 0 1 0 * 1 * *
Case Study of Reversi GameRules 1. Grid: n × n grid (where n is an even number) 2. Gameplay: Two players take turns(Black:0, White:1); Place one piece on an empty cell (*); Flip opponent's pieces to your color(0 →1 or 1→0) when you trap them in a line(horizontal/vertical/diagonal) between your new piece and an existing one.A JSON promptThe evaluation JSON constrain prompt template we used to evaluate the performance of different models is shown in Figure7.JSON Prompt TemplatePlease generate a JSON object that follows standard JSON formatting and indentation, containing a field named 'answer'.The 'answer' field should be a list of strings, where each string represents ... The 'process' field should be a list of strings, where each string ... , it should be represented as ["1", "0", "3", "0"].The 'process' field should be a list of strings, where each string records the instructions for each step from the initial state to the final state.First output the blocks that need to be synthesized, followed by the "-&gt;" symbol, then output the synthesized block, without adding any extra explanations.For example:{ "answer": ["0", "3", "6", "1"], "process": [C Additional evaluation metrics and error analysisDuring the evaluation process, it is observed that some models occasionally fail to adhere to instructions regarding the constraint of JSON format output.In this context, we have defined two error metrics and one correctness metric for thorough analysis:1. JSON Error (JSError): This metric tracks instances where there is an error in parsing the JSON format, usually due to incomplete or improperly formatted JSON outputs.
. Anthropic. Introducing claude. 2023</p>
<p>. J Bai, S Bai, Y Chu, Z Cui, K Dang, X Deng, Y Fan, W Ge, Y Han, F Huang, arXiv:2309.166092023Qwen technical report. arXiv preprint</p>
<p>S R Bowman, arXiv:1312.6192Can recursive neural tensor networks learn logical reasoning?. 2013arXiv preprint</p>
<p>Language models are few-shot learners. T Brown, B Mann, N Ryder, M Subbiah, J D Kaplan, P Dhariwal, A Neelakantan, P Shyam, G Sastry, A Askell, Advances in neural information processing systems. 202033</p>
<p>J Cheng, X Liu, K Zheng, P Ke, H Wang, Y Dong, J Tang, M Huang, arXiv:2311.04155Black-box prompt optimization: Aligning large language models without model training. 2023arXiv preprint</p>
<p>Autodetect: Towards a unified framework for automated weakness detection in large language models. J Cheng, Y Lu, X Gu, P Ke, X Liu, Y Dong, H Wang, J Tang, M Huang, arXiv:2406.167142024arXiv preprint</p>
<p>Palm: Scaling language modeling with pathways. A Chowdhery, S Narang, J Devlin, M Bosma, G Mishra, A Roberts, P Barham, H W Chung, C Sutton, S Gehrmann, Journal of Machine Learning Research. 242402023</p>
<p>P Clark, O Tafjord, K Richardson, arXiv:2002.05867Transformers as soft reasoners over language. 2020arXiv preprint</p>
<p>Training verifiers to solve math word problems. K Cobbe, V Kosaraju, M Bavarian, M Chen, H Jun, L Kaiser, M Plappert, J Tworek, J Hilton, R Nakano, arXiv:2110.141682021arXiv preprint</p>
<p>Language is primarily a tool for communication rather than thought. E Fedorenko, S T Piantadosi, E A Gibson, Nature. 63080172024</p>
<p>Did aristotle use a laptop? a question answering benchmark with implicit reasoning strategies. M Geva, D Khashabi, E Segal, T Khot, D Roth, J Berant, Transactions of the Association for Computational Linguistics. 92021</p>
<p>. T Glm, : , A Zeng, B Xu, B Wang, C Zhang, D Yin, D Rojas, G Feng, H Zhao, H Lai, H Yu, H Wang, J Sun, J Zhang, J Cheng, J Gui, J Tang, J Zhang, J Li, L Zhao, L Wu, L Zhong, M Liu, M Huang, P Zhang, Q Zheng, R Lu, S Duan, S Zhang, S Cao, S Yang, W L Tam, W Zhao, X Liu, X Xia, X Zhang, X Gu, X Lv, X Liu, X Liu, X Yang, X Song, X Zhang, Y An, Y Xu, Y Niu, Y Yang, Y Li, Y Bai, Y Dong, Z Qi, Z Wang, Z Yang, Z Du, Z Hou, Z Wang, 2024Chatglm: A family of large language models from glm-130b to glm-4 all tools</p>
<p>S Han, H Schoelkopf, Y Zhao, Z Qi, M Riddell, L Benson, L Sun, E Zubova, Y Qiao, M Burtell, arXiv:2209.00840Natural language reasoning with first-order logic. 2022arXiv preprint</p>
<p>Hi-tom: A benchmark for evaluating higher-order theory of mind reasoning in large language models. Y He, Y Wu, Y Jia, R Mihalcea, Y Chen, N Deng, arXiv:2310.167552023arXiv preprint</p>
<p>Measuring massive multitask language understanding. D Hendrycks, C Burns, S Basart, A Zou, M Mazeika, D Song, J Steinhardt, International Conference on Learning Representations. 2021</p>
<p>Z Huang, Z Wang, S Xia, X Li, H Zou, R Xu, R.-Z Fan, L Ye, E Chern, Y Ye, arXiv:2406.12753Benchmarking multi-discipline cognitive reasoning for superintelligent ai. 2024arXiv preprint</p>
<p>A Q Jiang, A Sablayrolles, A Mensch, C Bamford, D S Chaplot, D Casas, F Bressand, G Lengyel, G Lample, L Saulnier, arXiv:2310.06825Mistral 7b. 2023arXiv preprint</p>
<p>G Lample, F Charton, arXiv:1912.01412Deep learning for symbolic mathematics. 2019arXiv preprint</p>
<p>W Li, L Yu, Y Wu, L C Paulson, arXiv:2006.09265Isarstep: a benchmark for high-level mathematical reasoning. 2020arXiv preprint</p>
<p>X Liu, H Yu, H Zhang, Y Xu, X Lei, H Lai, Y Gu, H Ding, K Men, K Yang, arXiv:2308.03688Evaluating llms as agents. 2023arXiv preprint</p>
<p>S Mishra, M Finlayson, P Lu, L Tang, S Welleck, C Baral, T Rajpurohit, O Tafjord, A Sabharwal, P Clark, arXiv:2210.17517A unified benchmark for mathematical reasoning. 2022arXiv preprint</p>
<p>Numglue: A suite of fundamental yet challenging mathematical reasoning tasks. S Mishra, A Mitra, N Varshney, B Sachdeva, P Clark, C Baral, A Kalyan, arXiv:2204.056602022arXiv preprint</p>
<p>S Mukherjee, A Mitra, G Jawahar, S Agarwal, H Palangi, A Awadallah, arXiv:2306.02707Orca: Progressive learning from complex explanation traces of gpt-4. 2023arXiv preprint</p>
<p>Y Onoe, M J Zhang, E Choi, G Durrett, arXiv:2109.01653Creak: A dataset for commonsense reasoning over entity knowledge. 2021arXiv preprint</p>
<p>. OpenAI. Introducing chatgpt. 2022</p>
<p>R Openai, arXivGpt-4 technical report. 2023</p>
<p>Training language models to follow instructions with human feedback. L Ouyang, J Wu, X Jiang, D Almeida, C Wainwright, P Mishkin, C Zhang, S Agarwal, K Slama, A Ray, Advances in Neural Information Processing Systems. 202235</p>
<p>Language models are greedy reasoners: A systematic formal analysis of chain-of-thought. A Saparov, H He, arXiv:2210.012402022arXiv preprint</p>
<p>K Sinha, S Sodhani, J Dong, J Pineau, W L Hamilton, arXiv:1908.06177Clutrr: A diagnostic benchmark for inductive reasoning from text. 2019arXiv preprint</p>
<p>T R Sumers, S Yao, K Narasimhan, T L Griffiths, arXiv:2309.02427Cognitive architectures for language agents. 2023arXiv preprint</p>
<p>Challenging big-bench tasks and whether chain-of-thought can solve them. M Suzgun, N Scales, N Schärli, S Gehrmann, Y Tay, H W Chung, A Chowdhery, Q V Le, E H Chi, D Zhou, arXiv:2210.092612022arXiv preprint</p>
<p>Internlm: A multilingual language model with progressively enhanced capabilities. I Team, 2023</p>
<p>H Touvron, T Lavril, G Izacard, X Martinet, M.-A Lachaux, T Lacroix, B Rozière, N Goyal, E Hambro, F Azhar, A Rodriguez, A Joulin, E Grave, G Lample, Llama: Open and efficient foundation language models. 2023</p>
<p>J Wei, Y Tay, R Bommasani, C Raffel, B Zoph, S Borgeaud, D Yogatama, M Bosma, D Zhou, D Metzler, arXiv:2206.07682Emergent abilities of large language models. 2022arXiv preprint</p>
<p>Chain-ofthought prompting elicits reasoning in large language models. J Wei, X Wang, D Schuurmans, M Bosma, F Xia, E Chi, Q V Le, D Zhou, Advances in neural information processing systems. 202235</p>
<p>W Yu, Z Jiang, Y Dong, J Feng, arXiv:2002.04326Reclor: A reading comprehension dataset requiring logical reasoning. 2020arXiv preprint</p>
<p>Glm-130b: An open bilingual pre-trained model. A Zeng, X Liu, Z Du, Z Wang, H Lai, M Ding, Z Yang, Y Xu, W Zheng, X Xia, arXiv:2210.024142022arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
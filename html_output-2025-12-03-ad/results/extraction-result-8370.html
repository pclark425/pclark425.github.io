<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8370 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8370</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8370</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-149.html">extraction-schema-149</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <p><strong>Paper ID:</strong> paper-71ba5f845bd22d42003675b7cea970ca9e590bcc</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/71ba5f845bd22d42003675b7cea970ca9e590bcc" target="_blank">Editing Models with Task Arithmetic</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Learning Representations</p>
                <p><strong>Paper TL;DR:</strong> A new paradigm for steering the behavior of neural networks, centered around task vectors, is proposed, which shows that task arithmetic is a simple, efficient and effective way of editing models.</p>
                <p><strong>Paper Abstract:</strong> Changing how pre-trained models behave -- e.g., improving their performance on a downstream task or mitigating biases learned during pre-training -- is a common practice when developing machine learning systems. In this work, we propose a new paradigm for steering the behavior of neural networks, centered around \textit{task vectors}. A task vector specifies a direction in the weight space of a pre-trained model, such that movement in that direction improves performance on the task. We build task vectors by subtracting the weights of a pre-trained model from the weights of the same model after fine-tuning on a task. We show that these task vectors can be modified and combined together through arithmetic operations such as negation and addition, and the behavior of the resulting model is steered accordingly. Negating a task vector decreases performance on the target task, with little change in model behavior on control tasks. Moreover, adding task vectors together can improve performance on multiple tasks at once. Finally, when tasks are linked by an analogy relationship of the form ``A is to B as C is to D", combining task vectors from three of the tasks can improve performance on the fourth, even when no data from the fourth task is used for training. Overall, our experiments with several models, modalities and tasks show that task arithmetic is a simple, efficient and effective way of editing models.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8370.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8370.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Task vectors</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Task weight difference vectors</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A representation of a fine-tuning update computed as the element-wise difference between fine-tuned and pre-trained weights (τ = θ_ft − θ_pre), used as an additive edit in weight space to steer model behavior via scaling and linear combinations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>general (applied to CLIP, T5, GPT-2 in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Task vectors are architecture-agnostic in definition but in this paper were produced from models including CLIP (Vision Transformer variants ViT-B/32, ViT-B/16, ViT-L/14), T5-base (text-to-text transformer), and GPT-2 (various sizes, including GPT-2 Large). Each task vector is computed by taking the element-wise difference between the fine-tuned checkpoint and the shared pre-trained initialization.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>weight-space arithmetic (vector negation, addition, subtraction to form analogies); not numeric digit arithmetic</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>Explicit weight-space representation: τ_t = θ_ft^t − θ_pre; applying edits by θ_new = θ + λ τ_new where τ_new is a linear combination (e.g., −τ, Σ τ_i, τ_C + (τ_B − τ_A)). Observations: task vectors across different tasks are typically nearly orthogonal (cosine similarities near zero except for semantically related tasks), enabling addition with limited interference; intermediate task vectors early in fine-tuning already align with final direction.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Create task vectors by fine-tuning from a shared pre-trained initialization; apply interventions by element-wise addition/subtraction of task vectors to target model weights with scalar λ chosen on held-out validation data. Baselines/probes include: gradient-ascent fine-tuning (to increase loss), random vectors scaled per-layer to match magnitude, and ensembling comparisons. Analyses include cosine similarity matrices, learning-rate sweeps, interpolation/extrapolation along linear path, and comparing to ensembles (correlation of accuracy with ensembling, Pearson 0.99).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Not an arithmetic benchmark; summarized empirical behaviors: adding task vectors produces multi-task models with up to 91.2% average normalized accuracy when combining all eight image task vectors (relative to specialized fine-tuned models normalized to 1.0). Adding pairs reaches ~98.9% normalized accuracy on image tasks. Intermediate task vectors (few hundred steps) already yield high accuracy when added.</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>Limitations include requirement that models share same architecture and (in experiments) same pre-trained initialization; larger fine-tuning learning rates can degrade additivity and final accuracy; semantic overlap between target and control tasks can reduce effectiveness of negation/forgetting; random vectors do not produce the selective changes achieved by task vectors; averaging/extrapolation can fail when fine-tuned models do not share optimization trajectory.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Empirical evidence: monotonic performance along linear path between pre-trained and fine-tuned weights (cited MLI behavior), cosine similarity analysis (Figure 5) showing near-orthogonality, learning-rate experiments (Figure 6) demonstrating sensitivity, evolution of intermediate vectors (Figure 7) showing rapid convergence of direction, and high correlation between accuracy of added-vectors models and ensembles (Figure 8, Pearson 0.99). The algebraic identity τ = θ_ft − θ_pre explains how additions/subtractions produce linear combinations of original checkpoints.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>Counter-evidence/limitations: when tasks semantically overlap with control data (e.g., Cars vs. ImageNet), forgetting via negation is less effective; gradient-ascent fine-tuning (an alternative intervention) reduces target accuracy but severely degrades control-task performance; task-vector arithmetic was only evaluated for models sharing initialization (relaxing this remains future work).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Editing Models with Task Arithmetic', 'publication_date_yy_mm': '2022-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8370.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8370.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Negation (toxicity mitigation)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Negating a toxicity task vector in GPT-2 to reduce toxic generations</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An intervention that negates a task vector produced by fine-tuning GPT-2 on toxic data; applying −τ reduces toxic generations substantially while largely preserving perplexity on a control corpus.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-2 Large (and other GPT-2 sizes in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Transformer-based autoregressive language models (GPT-2 family). The paper uses GPT-2 Large for the toxicity mitigation experiment; task vectors are produced by fine-tuning from the same pre-trained GPT-2 initialization on toxic comments (CivilComments subset).</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>weight-space negation/extrapolation (apply θ_new = θ_pre + λ (−τ_tox)), not token-level arithmetic</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>Mechanism is an extrapolation in weight space away from the fine-tuned toxic-model direction: negation corresponds to moving from the fine-tuned checkpoint back across the pre-trained point in the opposite direction (effectively undoing that task's learned update). Representation: full-model weight vector difference.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Fine-tune GPT-2 on toxic examples to get τ_tox; apply −τ_tox (with scaling λ chosen on validation) to the pre-trained weights; evaluate toxicity of generated outputs with Detoxify classifier; control evaluation via perplexity on WikiText-103. Baselines: gradient-ascent fine-tuning to maximize toxicity loss, fine-tuning on non-toxic data, and adding random vectors of equal magnitude.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>GPT-2 Large: Pre-trained toxic generations 4.8%; fine-tuned (toxic) 57%; negative task vector reduced toxic generations to 0.8% (≈6× reduction from pre-trained 4.8% to 0.8% relative to fine-tuned behavior). Avg toxicity score reduced from 0.06 (pre) to 0.01 (negated vector). WikiText-103 perplexity: pre-trained 16.4, fine-tuned 16.6, negative-task-vector model 16.9 (control perplexity maintained within ~0.5 points). Gradient ascent baseline achieved 0% toxic generations but catastrophic control perplexity (>1e10). Fine-tuning on non-toxic data yielded 1.8% toxic generations but higher perplexity (17.2). Random vector had no effect (4.8%).</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>Negation reduces the targeted behavior but may require appropriate scaling λ to avoid degrading other behaviors; gradient-ascent alternatives can eliminate toxic generations at cost of catastrophic degradation on control tasks; random vectors do not selectively remove toxicity. The method assumes access to a fine-tuned toxic model (or its weights) and same initialization constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Empirical result: clear reduction in Detoxify-measured toxic generations with only small change in perplexity on WikiText-103; comparison with baselines shows selective effect is specific to negated fine-tuning direction rather than arbitrary perturbations. Interpretation relies on linear-extrapolation properties observed in weight-space interpolation experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>Gradient-ascent 'negation' obtained by fine-tuning with reversed loss can produce undesirable global degradation. Also, negation depends on having a task vector derived from the harmful behavior; if fine-tuned models are unavailable or initialization differs, applicability is limited. The paper does not provide mechanistic neuron-level explanations for why toxicity behavior is localized to that vector direction.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Editing Models with Task Arithmetic', 'publication_date_yy_mm': '2022-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8370.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8370.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Addition (NLP transfer via task vectors)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Adding external task vectors to improve NLP target-task performance (T5 experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Using external task vectors (downloaded fine-tuned checkpoints) added to a T5-base fine-tuned model to improve single-task performance or build multi-task capabilities without further training.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>T5-base</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Text-to-text Transformer (T5-base) models fine-tuned on GLUE tasks; external candidate checkpoints from Hugging Face Hub (427 candidates) used to construct external task vectors. Task vectors originate from models sharing the same pre-trained initialization.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>weight-space addition (θ_new = θ_pre + λ Σ τ_ext or θ_ft_target + λ τ_ext), used to transfer improvements to single target tasks</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>Representation: each external model yields τ_ext = θ_ext − θ_pre; addition accumulates directions corresponding to capabilities learned by external fine-tuning. The hypothesis is near-orthogonality of task vectors reduces destructive interference, allowing additive transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Search over available public checkpoints to identify candidate τ_ext; add τ_ext (with scaling coefficient chosen by held-out validation) to the fine-tuned target model; evaluate GLUE task performance. Baselines: zero-shot, fine-tuned only. Additional analyses include sensitivity to learning rate of the fine-tuned models used to build vectors.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>GLUE subset (MRPC, RTE, CoLA, SST-2): Fine-tuned baseline average 78.1; fine-tuned + best external task vector average 78.6 (+0.5 absolute). Per-task gains: MRPC 88.5 → 89.3 (+0.8), RTE 77.3 → 77.5 (+0.2), CoLA 52.3 → 53.0 (+0.7), SST-2 94.5 → 94.7 (+0.2).</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>Gains are modest and dependent on availability of compatible external checkpoints; larger learning rates during the source fine-tuning can reduce transferability; variance arises from differences in fine-tuning procedures of external checkpoints (hyperparameters, seeds). Addition may not outperform specialized fine-tuning in many cases.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Selected-best experiments (held-out validation) show improvements, and learning-rate ablations (Figure 6) show that lower source fine-tuning learning rates make task vectors more usable for addition. Correlations with ensemble behavior and orthogonality observations support the additive-transfer hypothesis.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>Significant reliance on finding a compatible external checkpoint; adding many task vectors requires careful scaling and can still yield compressed performance below an ensemble of specialized fine-tuned models (though in image tasks compressed models approached ensemble performance). The approach assumes shared architecture and (in experiments) shared initialization.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Editing Models with Task Arithmetic', 'publication_date_yy_mm': '2022-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8370.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8370.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Task analogies (domain generalization)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Task analogy composition via vector arithmetic: τ_C + (τ_B − τ_A)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Forming analogies among tasks via weight-space arithmetic to improve performance on a target task with little or no labeled data by combining three related task vectors.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>T5-base (NLP) and CLIP ViT variants (vision subpopulation experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>T5-base models for sentiment tasks and CLIP ViT models for image/sketch subpopulation tasks; task vectors computed from fine-tuning on supervised/unsupervised combinations and from unlabeled data.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>weight-space analogy arithmetic (τ_new = τ_C + (τ_B − τ_A)), applied to transfer properties (e.g., supervised/unsupervised domain shifts, sketch vs. real imagery) rather than numeric arithmetic</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>Mechanism: treat task-vectors as linear embeddings in weight space where vector differences correspond to changes in objective/dataset; adding the difference between two related tasks to a third task's vector yields a predicted vector for the fourth task due to presumed linear structure of task space and partial orthogonality.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Construct τ_A, τ_B, τ_C by fine-tuning on the three available tasks/subpopulations; compute τ_pred = τ_C + (τ_B − τ_A); apply to pre-trained weights (with scaling λ selected by validation where possible); evaluate on the held-out target domain (no labeled target data used in some experiments). Baselines include pre-trained model performance and fine-tuning with small amounts of target data.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Sentiment domain generalization: combining task vectors improved accuracy on target sentiment tasks (Yelp/Amazon examples) — paper reports improvements (Table 4 summarized), e.g., for T5 variants accuracy improved by multiple percentage points (example aggregate improvement: domain-generalization gains where analogies alone yield similar benefit as annotating ~100 target samples). Image subpopulation experiments: task analogies improved average accuracy by 3.4 percentage points over pre-trained across four target subpopulations and three CLIP models.</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>Effectiveness depends on existence of a valid analogy and semantic relatedness; if tasks are not related or if source vectors were learned with incompatible hyperparameters, analogy may fail; reliance on same architecture and initialization; no guarantee of success when tasks are highly dissimilar.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Empirical gains in multiple cross-domain experiments (sentiment, sketches vs. real images, subpopulation transfer) support that linear relationships in task vector space capture useful transfer structure. Cosine similarity analysis and the near-orthogonality property are invoked to explain low interference in composed vectors.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>When tasks lack strong structural analogy or when source vectors are noisy (e.g., from high learning-rate fine-tuning), predicted analogies can perform poorly. The method was not tested for arbitrary task pairs and requires careful selection/validation of scaling λ.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Editing Models with Task Arithmetic', 'publication_date_yy_mm': '2022-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Patching open-vocabulary models by interpolating weights <em>(Rating: 2)</em></li>
                <li>Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time <em>(Rating: 2)</em></li>
                <li>Eternal sunshine of the spotless net: Selective forgetting in deep networks <em>(Rating: 2)</em></li>
                <li>Fast model editing at scale <em>(Rating: 2)</em></li>
                <li>Extracting latent steering vectors from pretrained language models <em>(Rating: 1)</em></li>
                <li>Merging models with fisher-weighted averaging <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8370",
    "paper_id": "paper-71ba5f845bd22d42003675b7cea970ca9e590bcc",
    "extraction_schema_id": "extraction-schema-149",
    "extracted_data": [
        {
            "name_short": "Task vectors",
            "name_full": "Task weight difference vectors",
            "brief_description": "A representation of a fine-tuning update computed as the element-wise difference between fine-tuned and pre-trained weights (τ = θ_ft − θ_pre), used as an additive edit in weight space to steer model behavior via scaling and linear combinations.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "general (applied to CLIP, T5, GPT-2 in experiments)",
            "model_description": "Task vectors are architecture-agnostic in definition but in this paper were produced from models including CLIP (Vision Transformer variants ViT-B/32, ViT-B/16, ViT-L/14), T5-base (text-to-text transformer), and GPT-2 (various sizes, including GPT-2 Large). Each task vector is computed by taking the element-wise difference between the fine-tuned checkpoint and the shared pre-trained initialization.",
            "arithmetic_task_type": "weight-space arithmetic (vector negation, addition, subtraction to form analogies); not numeric digit arithmetic",
            "mechanism_or_representation": "Explicit weight-space representation: τ_t = θ_ft^t − θ_pre; applying edits by θ_new = θ + λ τ_new where τ_new is a linear combination (e.g., −τ, Σ τ_i, τ_C + (τ_B − τ_A)). Observations: task vectors across different tasks are typically nearly orthogonal (cosine similarities near zero except for semantically related tasks), enabling addition with limited interference; intermediate task vectors early in fine-tuning already align with final direction.",
            "probing_or_intervention_method": "Create task vectors by fine-tuning from a shared pre-trained initialization; apply interventions by element-wise addition/subtraction of task vectors to target model weights with scalar λ chosen on held-out validation data. Baselines/probes include: gradient-ascent fine-tuning (to increase loss), random vectors scaled per-layer to match magnitude, and ensembling comparisons. Analyses include cosine similarity matrices, learning-rate sweeps, interpolation/extrapolation along linear path, and comparing to ensembles (correlation of accuracy with ensembling, Pearson 0.99).",
            "performance_metrics": "Not an arithmetic benchmark; summarized empirical behaviors: adding task vectors produces multi-task models with up to 91.2% average normalized accuracy when combining all eight image task vectors (relative to specialized fine-tuned models normalized to 1.0). Adding pairs reaches ~98.9% normalized accuracy on image tasks. Intermediate task vectors (few hundred steps) already yield high accuracy when added.",
            "error_types_or_failure_modes": "Limitations include requirement that models share same architecture and (in experiments) same pre-trained initialization; larger fine-tuning learning rates can degrade additivity and final accuracy; semantic overlap between target and control tasks can reduce effectiveness of negation/forgetting; random vectors do not produce the selective changes achieved by task vectors; averaging/extrapolation can fail when fine-tuned models do not share optimization trajectory.",
            "evidence_for_mechanism": "Empirical evidence: monotonic performance along linear path between pre-trained and fine-tuned weights (cited MLI behavior), cosine similarity analysis (Figure 5) showing near-orthogonality, learning-rate experiments (Figure 6) demonstrating sensitivity, evolution of intermediate vectors (Figure 7) showing rapid convergence of direction, and high correlation between accuracy of added-vectors models and ensembles (Figure 8, Pearson 0.99). The algebraic identity τ = θ_ft − θ_pre explains how additions/subtractions produce linear combinations of original checkpoints.",
            "counterexamples_or_challenges": "Counter-evidence/limitations: when tasks semantically overlap with control data (e.g., Cars vs. ImageNet), forgetting via negation is less effective; gradient-ascent fine-tuning (an alternative intervention) reduces target accuracy but severely degrades control-task performance; task-vector arithmetic was only evaluated for models sharing initialization (relaxing this remains future work).",
            "uuid": "e8370.0",
            "source_info": {
                "paper_title": "Editing Models with Task Arithmetic",
                "publication_date_yy_mm": "2022-12"
            }
        },
        {
            "name_short": "Negation (toxicity mitigation)",
            "name_full": "Negating a toxicity task vector in GPT-2 to reduce toxic generations",
            "brief_description": "An intervention that negates a task vector produced by fine-tuning GPT-2 on toxic data; applying −τ reduces toxic generations substantially while largely preserving perplexity on a control corpus.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-2 Large (and other GPT-2 sizes in experiments)",
            "model_description": "Transformer-based autoregressive language models (GPT-2 family). The paper uses GPT-2 Large for the toxicity mitigation experiment; task vectors are produced by fine-tuning from the same pre-trained GPT-2 initialization on toxic comments (CivilComments subset).",
            "arithmetic_task_type": "weight-space negation/extrapolation (apply θ_new = θ_pre + λ (−τ_tox)), not token-level arithmetic",
            "mechanism_or_representation": "Mechanism is an extrapolation in weight space away from the fine-tuned toxic-model direction: negation corresponds to moving from the fine-tuned checkpoint back across the pre-trained point in the opposite direction (effectively undoing that task's learned update). Representation: full-model weight vector difference.",
            "probing_or_intervention_method": "Fine-tune GPT-2 on toxic examples to get τ_tox; apply −τ_tox (with scaling λ chosen on validation) to the pre-trained weights; evaluate toxicity of generated outputs with Detoxify classifier; control evaluation via perplexity on WikiText-103. Baselines: gradient-ascent fine-tuning to maximize toxicity loss, fine-tuning on non-toxic data, and adding random vectors of equal magnitude.",
            "performance_metrics": "GPT-2 Large: Pre-trained toxic generations 4.8%; fine-tuned (toxic) 57%; negative task vector reduced toxic generations to 0.8% (≈6× reduction from pre-trained 4.8% to 0.8% relative to fine-tuned behavior). Avg toxicity score reduced from 0.06 (pre) to 0.01 (negated vector). WikiText-103 perplexity: pre-trained 16.4, fine-tuned 16.6, negative-task-vector model 16.9 (control perplexity maintained within ~0.5 points). Gradient ascent baseline achieved 0% toxic generations but catastrophic control perplexity (&gt;1e10). Fine-tuning on non-toxic data yielded 1.8% toxic generations but higher perplexity (17.2). Random vector had no effect (4.8%).",
            "error_types_or_failure_modes": "Negation reduces the targeted behavior but may require appropriate scaling λ to avoid degrading other behaviors; gradient-ascent alternatives can eliminate toxic generations at cost of catastrophic degradation on control tasks; random vectors do not selectively remove toxicity. The method assumes access to a fine-tuned toxic model (or its weights) and same initialization constraints.",
            "evidence_for_mechanism": "Empirical result: clear reduction in Detoxify-measured toxic generations with only small change in perplexity on WikiText-103; comparison with baselines shows selective effect is specific to negated fine-tuning direction rather than arbitrary perturbations. Interpretation relies on linear-extrapolation properties observed in weight-space interpolation experiments.",
            "counterexamples_or_challenges": "Gradient-ascent 'negation' obtained by fine-tuning with reversed loss can produce undesirable global degradation. Also, negation depends on having a task vector derived from the harmful behavior; if fine-tuned models are unavailable or initialization differs, applicability is limited. The paper does not provide mechanistic neuron-level explanations for why toxicity behavior is localized to that vector direction.",
            "uuid": "e8370.1",
            "source_info": {
                "paper_title": "Editing Models with Task Arithmetic",
                "publication_date_yy_mm": "2022-12"
            }
        },
        {
            "name_short": "Addition (NLP transfer via task vectors)",
            "name_full": "Adding external task vectors to improve NLP target-task performance (T5 experiments)",
            "brief_description": "Using external task vectors (downloaded fine-tuned checkpoints) added to a T5-base fine-tuned model to improve single-task performance or build multi-task capabilities without further training.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "T5-base",
            "model_description": "Text-to-text Transformer (T5-base) models fine-tuned on GLUE tasks; external candidate checkpoints from Hugging Face Hub (427 candidates) used to construct external task vectors. Task vectors originate from models sharing the same pre-trained initialization.",
            "arithmetic_task_type": "weight-space addition (θ_new = θ_pre + λ Σ τ_ext or θ_ft_target + λ τ_ext), used to transfer improvements to single target tasks",
            "mechanism_or_representation": "Representation: each external model yields τ_ext = θ_ext − θ_pre; addition accumulates directions corresponding to capabilities learned by external fine-tuning. The hypothesis is near-orthogonality of task vectors reduces destructive interference, allowing additive transfer.",
            "probing_or_intervention_method": "Search over available public checkpoints to identify candidate τ_ext; add τ_ext (with scaling coefficient chosen by held-out validation) to the fine-tuned target model; evaluate GLUE task performance. Baselines: zero-shot, fine-tuned only. Additional analyses include sensitivity to learning rate of the fine-tuned models used to build vectors.",
            "performance_metrics": "GLUE subset (MRPC, RTE, CoLA, SST-2): Fine-tuned baseline average 78.1; fine-tuned + best external task vector average 78.6 (+0.5 absolute). Per-task gains: MRPC 88.5 → 89.3 (+0.8), RTE 77.3 → 77.5 (+0.2), CoLA 52.3 → 53.0 (+0.7), SST-2 94.5 → 94.7 (+0.2).",
            "error_types_or_failure_modes": "Gains are modest and dependent on availability of compatible external checkpoints; larger learning rates during the source fine-tuning can reduce transferability; variance arises from differences in fine-tuning procedures of external checkpoints (hyperparameters, seeds). Addition may not outperform specialized fine-tuning in many cases.",
            "evidence_for_mechanism": "Selected-best experiments (held-out validation) show improvements, and learning-rate ablations (Figure 6) show that lower source fine-tuning learning rates make task vectors more usable for addition. Correlations with ensemble behavior and orthogonality observations support the additive-transfer hypothesis.",
            "counterexamples_or_challenges": "Significant reliance on finding a compatible external checkpoint; adding many task vectors requires careful scaling and can still yield compressed performance below an ensemble of specialized fine-tuned models (though in image tasks compressed models approached ensemble performance). The approach assumes shared architecture and (in experiments) shared initialization.",
            "uuid": "e8370.2",
            "source_info": {
                "paper_title": "Editing Models with Task Arithmetic",
                "publication_date_yy_mm": "2022-12"
            }
        },
        {
            "name_short": "Task analogies (domain generalization)",
            "name_full": "Task analogy composition via vector arithmetic: τ_C + (τ_B − τ_A)",
            "brief_description": "Forming analogies among tasks via weight-space arithmetic to improve performance on a target task with little or no labeled data by combining three related task vectors.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "T5-base (NLP) and CLIP ViT variants (vision subpopulation experiments)",
            "model_description": "T5-base models for sentiment tasks and CLIP ViT models for image/sketch subpopulation tasks; task vectors computed from fine-tuning on supervised/unsupervised combinations and from unlabeled data.",
            "arithmetic_task_type": "weight-space analogy arithmetic (τ_new = τ_C + (τ_B − τ_A)), applied to transfer properties (e.g., supervised/unsupervised domain shifts, sketch vs. real imagery) rather than numeric arithmetic",
            "mechanism_or_representation": "Mechanism: treat task-vectors as linear embeddings in weight space where vector differences correspond to changes in objective/dataset; adding the difference between two related tasks to a third task's vector yields a predicted vector for the fourth task due to presumed linear structure of task space and partial orthogonality.",
            "probing_or_intervention_method": "Construct τ_A, τ_B, τ_C by fine-tuning on the three available tasks/subpopulations; compute τ_pred = τ_C + (τ_B − τ_A); apply to pre-trained weights (with scaling λ selected by validation where possible); evaluate on the held-out target domain (no labeled target data used in some experiments). Baselines include pre-trained model performance and fine-tuning with small amounts of target data.",
            "performance_metrics": "Sentiment domain generalization: combining task vectors improved accuracy on target sentiment tasks (Yelp/Amazon examples) — paper reports improvements (Table 4 summarized), e.g., for T5 variants accuracy improved by multiple percentage points (example aggregate improvement: domain-generalization gains where analogies alone yield similar benefit as annotating ~100 target samples). Image subpopulation experiments: task analogies improved average accuracy by 3.4 percentage points over pre-trained across four target subpopulations and three CLIP models.",
            "error_types_or_failure_modes": "Effectiveness depends on existence of a valid analogy and semantic relatedness; if tasks are not related or if source vectors were learned with incompatible hyperparameters, analogy may fail; reliance on same architecture and initialization; no guarantee of success when tasks are highly dissimilar.",
            "evidence_for_mechanism": "Empirical gains in multiple cross-domain experiments (sentiment, sketches vs. real images, subpopulation transfer) support that linear relationships in task vector space capture useful transfer structure. Cosine similarity analysis and the near-orthogonality property are invoked to explain low interference in composed vectors.",
            "counterexamples_or_challenges": "When tasks lack strong structural analogy or when source vectors are noisy (e.g., from high learning-rate fine-tuning), predicted analogies can perform poorly. The method was not tested for arbitrary task pairs and requires careful selection/validation of scaling λ.",
            "uuid": "e8370.3",
            "source_info": {
                "paper_title": "Editing Models with Task Arithmetic",
                "publication_date_yy_mm": "2022-12"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Patching open-vocabulary models by interpolating weights",
            "rating": 2
        },
        {
            "paper_title": "Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time",
            "rating": 2
        },
        {
            "paper_title": "Eternal sunshine of the spotless net: Selective forgetting in deep networks",
            "rating": 2
        },
        {
            "paper_title": "Fast model editing at scale",
            "rating": 2
        },
        {
            "paper_title": "Extracting latent steering vectors from pretrained language models",
            "rating": 1
        },
        {
            "paper_title": "Merging models with fisher-weighted averaging",
            "rating": 1
        }
    ],
    "cost": 0.014145999999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Editing Models with Task Arithmetic</h1>
<p>Gabriel Ilharco ${ }^{* 1}$ Marco Tulio Ribeiro ${ }^{2}$ Mitchell Wortsman ${ }^{1}$ Suchin Gururangan ${ }^{1}$<br>Ludwig Schmidt ${ }^{1,3}$ Hannaneh Hajishirzi ${ }^{1,3}$ Ali Farhadi ${ }^{1}$<br>${ }^{1}$ University of Washington ${ }^{2}$ Microsoft Research ${ }^{3}$ Allen Institute for AI</p>
<h4>Abstract</h4>
<p>Changing how pre-trained models behave-e.g., improving their performance on a downstream task or mitigating biases learned during pre-training-is a common practice when developing machine learning systems. In this work, we propose a new paradigm for steering the behavior of neural networks, centered around task vectors. A task vector specifies a direction in the weight space of a pre-trained model, such that movement in that direction improves performance on the task. We build task vectors by subtracting the weights of a pre-trained model from the weights of the same model after fine-tuning on a task. We show that these task vectors can be modified and combined together through arithmetic operations such as negation and addition, and the behavior of the resulting model is steered accordingly. Negating a task vector decreases performance on the target task, with little change in model behavior on control tasks. Moreover, adding task vectors together can improve performance on multiple tasks at once. Finally, when tasks are linked by an analogy relationship of the form " $A$ is to $B$ as $C$ is to $D$ ", combining task vectors from three of the tasks can improve performance on the fourth, even when no data from the fourth task is used for training. Overall, our experiments with several models, modalities and tasks show that task arithmetic is a simple, efficient and effective way of editing models.</p>
<h2>1 INTRODUCTION</h2>
<p>Pre-trained models are commonly used as backbones of machine learning systems. In practice, we often want to edit models after pre-training, ${ }^{1}$ to improve performance on downstream tasks [105; 100; 63; 39], mitigate biases or unwanted behavior [85; 59; 82; 71], align models with human preferences $[4 ; 74 ; 44 ; 32]$, or update models with new information $[104 ; 15 ; 69 ; 70]$.</p>
<p>In this work, we present a new paradigm for editing neural networks based on task vectors, which encode the information necessary to do well on a given task. Inspired by recent work on weight interpolation $[27 ; 100 ; 63 ; 99 ; 39 ; 55 ; 2 ; 20]$, we obtain such vectors by taking the weights of a model fine-tuned on a task and subtracting the corresponding pre-trained weights (Figure 1a).</p>
<p>We show that we can edit a variety of models with task arithmetic—performing simple arithmetic operations on task vectors (Figure 1b-d). For example, negating a vector can be used to remove undesirable behaviors or unlearn tasks, while adding task vectors leads to better multi-task models, or even improves performance on a single task. Finally, when tasks form an analogy relationship, task vectors can be combined to improve performance on tasks where data is scarce.</p>
<p>Forgetting via negation. Users can negate task vectors to mitigate undesirable behaviors (e.g., toxic generations), or even to forget specific tasks altogether, like OCR. In Section 3, we negate a task vector from a language model fine-tuned on toxic data [77; 8], reducing the proportion of generations classified as toxic, with little change in fluency. We also negate task vectors for image classification tasks, resulting in substantially lower accuracy on the task we wish to forget with little loss on ImageNet accuracy [16].</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: An illustration of task vectors and the arithmetic operations we study for editing models. (a) A task vector is obtained by subtracting the weights of a pre-trained model from the weights of the same model after fine-tuning (Section 2). (b) Negating a task vector degrades performance on the task, without substantial changes in control tasks (Section 3). (c) Adding task vectors together improves the performance of the pre-trained model on the tasks under consideration (Section 4). (d) When tasks form an analogy relationship such as supervised and unsupervised learning on two different data sources, it is possible to improve performance on a supervised target task using only vectors from the remaining three combinations of objectives and datasets (Section 5).</p>
<p>Learning via addition. Adding task vectors results in better multi-task models, or improved performance on a single task. In Section 4, we add task vectors from various image models (CLIP, Radford et al. [78]) and compare the performance of the resulting model with using multiple specialized fine-tuned models. We find that the single resulting model can be competitive with using multiple specialized models. Adding two task vectors maintains $98.9 \%$ of the accuracy, and the average performance on the entire set of tasks increases as more task vectors are added. Moreover, adding a task vector from a different task can improve performance on a target task using text models (T5, Raffel et al. [79]).</p>
<p>Task analogies. When we can form task analogies of the form " $A$ is to $B$ as $C$ is to $D$ ", combining task vectors from the first three tasks improves performance on the fourth, even when little or no training data is available. In Section 5, we show that we can improve domain generalization to a new target task without using labeled data from that task. More specifically, accuracy on a sentiment analysis task improves by combining a task vector from a second sentiment analysis dataset and task vectors produced using unlabeled data from both domains. We also use analogies between classifying pictures and sketches of objects to improve accuracy on subgroups where little or no data is available.
Overall, editing models with task arithmetic is simple, fast and effective. There is no extra cost at inference time in terms of memory or compute, since we only do element-wise operations on model weights. Moreover, vector operations are cheap, allowing users to experiment quickly with multiple task vectors. With task arithmetic, practitioners can reuse or transfer knowledge from models they create, or from the multitude of publicly available models all without requiring access to data or additional training. ${ }^{2}$</p>
<h1>2 TASK VECTORS</h1>
<p>For our purposes, a task is instantiated by a dataset and a loss function used for fine-tuning. Let $\theta_{\text {pre }} \in \mathbb{R}^{d}$ be the weights of a pre-trained model, and $\theta_{\mathrm{ft}}^{t} \in \mathbb{R}^{d}$ the corresponding weights after fine-tuning on task $t$. The task vector $\tau_{t} \in \mathbb{R}^{d}$ is given by the element-wise difference between $\theta_{\mathrm{ft}}^{t}$ and $\theta_{\text {pre }}$, i.e., $\tau_{t}=\theta_{\mathrm{ft}}^{t}-\theta_{\text {pre }}$. When the task is clear from context, we omit the identifier $t$, referring to the task vector simply as $\tau$.</p>
<p>Task vectors can be applied to any model parameters $\theta$ from the same architecture, via element-wise addition, with an optional scaling term $\lambda$, such that the resulting model has weights $\theta_{\text {new }}=\theta+\lambda \tau$. In our experiments, the scaling term is determined using held-out validation sets. Note that adding a single task vector to a pre-trained model with $\lambda=1$ results in the model fine-tuned on that task.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Following Ilharco et al. [39], we focus on open-ended models, where it is possible to fine-tune on a downstream task without introducing new parameters (e.g., open-vocabulary image classifiers [78; 42; 76; 3] and text-to-text models [79; 77; 9; 38]). In cases where fine-tuning introduces new parameters (e.g., a new classification head), we could follow Matena \&amp; Raffel [63] and merge only the shared weights, but this exploration is left for future work.</p>
<p>Editing models with task arithmetic. We focus on three arithmetic expressions over task vectors, as illustrated in Figure 1: negating a task vector, adding task vectors together, and combining task vectors to form analogies. All operations are applied element-wise to the weight vectors.</p>
<p>When negating a task vector $\tau$, applying the resulting vector $\tau_{\text {new }}=-\tau$ corresponds to extrapolating between the fine-tuned model and the pre-trained model. The resulting model is worse at the target task, with little change in performance on control tasks (Section 3). Adding two or more task vectors $\tau_{i}$ yields $\tau_{\text {new }}=\sum_{i} \tau_{i}$, and results in a multi-task model proficient in all tasks, sometimes even with gains over models fine-tuned on individual tasks (Section 4). Finally, when tasks $A, B, C$ and $D$ form an analogy in the form " $A$ is to $B$ as $C$ is to $D$ ", the task vector $\tau_{\text {new }}=\tau_{C}+\left(\tau_{B}-\tau_{A}\right)$ improves performance on task $D$, even if there is little or no data for that task (Section 5).</p>
<p>For all operations, the model weights obtained by applying $\tau_{\text {new }}$ are given by $\theta_{\text {new }}=\theta+\lambda \tau_{\text {new }}$, where the scaling term $\lambda$ is determined using held-out validation sets.</p>
<h1>3 Forgetting via Negation</h1>
<p>In this section, we show that negating a task vector is an effective way to reduce its performance on a target task, without substantially hurting performance elsewhere. Forgetting or "unlearning" can help mitigate undesired biases learned when pre-training; forgetting tasks altogether may be desirable to comply with regulations or for ethical reasons like preventing an image classifier to recognize faces, or to "read" personal information via OCR.</p>
<p>These interventions should not have a substantial effect on how models behave when processing data outside the scope of the edit [69; 39]. Accordingly, we measure accuracy on control tasks, in addition to evaluating on the target tasks from which the task vector originated. Our experiments showcase the effectiveness of negating task vectors for editing image classification and text generation models.</p>
<h3>3.1 IMAGE CLASSIFICATION</h3>
<p>For image classification, we use CLIP models [78] and task vectors from eight tasks studied by Ilharco et al. [39]; Radford et al. [78], ranging from satellite imagery recognition to classifying traffic signs: Cars [47], DTD [12], EuroSAT [36], GTSRB [87], MNIST [51], RESISC45 [10], SUN397 [101], and SVHN [72]. We explore additional tasks including OCR and person identification in Appendix B. For the control task, we use ImageNet [16]. We generate task vectors by fine-tuning on each of the target tasks, as detailed in Appendix B.1.</p>
<p>We compare against two additional baselines, fine-tuning by moving in the direction of increasing loss (i.e., with gradient ascent), as in Golatkar et al. [34]; Tarun et al. [90], and against using a random vector where each layer has the same magnitude as the corresponding layer of task vector. Additional details are in Appendix B.2.</p>
<p>As shown in Table 1, negating the task vectors is the most effective editing strategy for decreasing accuracy on the target task with little impact on the control task. For example, negative task vectors decrease the average target accuracy of ViT-L/14 by 45.8 percentage points with little change in accuracy on the control task. In contrast, using a random vector does not have much impact on target accuracy, while fine-tuning with gradient ascent severely deteriorates performance on control tasks. We present additional results in Appendix B.</p>
<h3>3.2 TEXT GENERATION</h3>
<p>We study whether we can mitigate a particular model behavior by negating a task vector trained to do that behavior. In particular, we aim to reduce the amount of toxic generations produced by GPT-2 models of various sizes [77]. We generate task vectors by fine-tuning on data from Civil</p>
<p>Table 1: Forgetting image classification tasks via negation. Results are shown for CLIP models, reporting average accuracy (\%) on the eight target tasks we wish to forget (Cars, DTD, EuroSAT, GTSRB, MNIST, RESISC45, SUN397 and SVHN), and the control task (ImageNet). Negating task vectors reduce the accuracy of a pre-trained ViT-L/14 by 45.8 percentage points on the target tasks, with little loss on the control task. Additional details and results are shown in Appendix B.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Method</th>
<th style="text-align: center;">ViT-B/32</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">ViT-B/16</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">ViT-L/14</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">Target $(\downarrow)$</td>
<td style="text-align: center;">Control $(\uparrow)$</td>
<td style="text-align: center;">Target $(\downarrow)$</td>
<td style="text-align: center;">Control $(\uparrow)$</td>
<td style="text-align: center;">Target $(\downarrow)$</td>
<td style="text-align: center;">Control $(\uparrow)$</td>
</tr>
<tr>
<td style="text-align: left;">Pre-trained</td>
<td style="text-align: center;">48.3</td>
<td style="text-align: center;">63.4</td>
<td style="text-align: center;">55.2</td>
<td style="text-align: center;">68.3</td>
<td style="text-align: center;">64.8</td>
<td style="text-align: center;">75.5</td>
</tr>
<tr>
<td style="text-align: left;">Fine-tuned</td>
<td style="text-align: center;">90.2</td>
<td style="text-align: center;">48.2</td>
<td style="text-align: center;">92.5</td>
<td style="text-align: center;">58.3</td>
<td style="text-align: center;">94.0</td>
<td style="text-align: center;">72.6</td>
</tr>
<tr>
<td style="text-align: left;">Gradient ascent</td>
<td style="text-align: center;">2.73</td>
<td style="text-align: center;">0.25</td>
<td style="text-align: center;">1.93</td>
<td style="text-align: center;">0.68</td>
<td style="text-align: center;">3.93</td>
<td style="text-align: center;">16.3</td>
</tr>
<tr>
<td style="text-align: left;">Random vector</td>
<td style="text-align: center;">45.7</td>
<td style="text-align: center;">61.5</td>
<td style="text-align: center;">53.1</td>
<td style="text-align: center;">66.0</td>
<td style="text-align: center;">60.9</td>
<td style="text-align: center;">72.9</td>
</tr>
<tr>
<td style="text-align: left;">Negative task vector</td>
<td style="text-align: center;">24.0</td>
<td style="text-align: center;">60.9</td>
<td style="text-align: center;">21.3</td>
<td style="text-align: center;">65.4</td>
<td style="text-align: center;">19.0</td>
<td style="text-align: center;">72.9</td>
</tr>
</tbody>
</table>
<p>Table 2: Making language models less toxic with negative task vectors. Results are shown for the GPT-2 Large model. Negative task vectors decrease the amount of toxic generations by $6 \times$, while resulting in a model with comparable perplexity on a control task (WikiText-103). Additional details and results are shown in Appendix C.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Method</th>
<th style="text-align: center;">\% toxic generations $(\downarrow)$</th>
<th style="text-align: center;">Avg. toxicity score $(\downarrow)$</th>
<th style="text-align: center;">WikiText-103 perplexity $(\downarrow)$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Pre-trained</td>
<td style="text-align: center;">4.8</td>
<td style="text-align: center;">0.06</td>
<td style="text-align: center;">16.4</td>
</tr>
<tr>
<td style="text-align: left;">Fine-tuned</td>
<td style="text-align: center;">57</td>
<td style="text-align: center;">0.56</td>
<td style="text-align: center;">16.6</td>
</tr>
<tr>
<td style="text-align: left;">Gradient ascent</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.45</td>
<td style="text-align: center;">$&gt;10^{10}$</td>
</tr>
<tr>
<td style="text-align: left;">Fine-tuned on non-toxic</td>
<td style="text-align: center;">1.8</td>
<td style="text-align: center;">0.03</td>
<td style="text-align: center;">17.2</td>
</tr>
<tr>
<td style="text-align: left;">Random vector</td>
<td style="text-align: center;">4.8</td>
<td style="text-align: center;">0.06</td>
<td style="text-align: center;">16.4</td>
</tr>
<tr>
<td style="text-align: left;">Negative task vector</td>
<td style="text-align: center;">0.8</td>
<td style="text-align: center;">0.01</td>
<td style="text-align: center;">16.9</td>
</tr>
</tbody>
</table>
<p>Comments [8] where the toxicity score is higher than 0.8 , and then negating such task vectors. As in Section 3.1, we also compare against baselines that use gradient ascent when fine-tuning [34; 90], and using a random task vector of the same magnitude. Additionally, we compare against fine-tuning on non-toxic samples from Civil Comments (toxicity scores smaller than 0.2), similar to Liu et al. [57]. We measure the toxicity of one thousand model generations with Detoxify [35]. For the control task, we measure the perplexity of the language models on WikiText-103 [66].
As shown in Table 2, editing with negative task vectors is effective, reducing the amount of generations classified as toxic from $4.8 \%$ to $0.8 \%$, while maintaining perplexity on the control task within 0.5 points of the pre-trained model. In contrast, fine-tuning with gradient ascent lowers toxic generations by degrading performance on the control task to an unacceptable level, while fine-tuning on non-toxic data is worse than task vectors both in reducing task generations and on the control task. As an experimental control, adding a random vector has little impact either on toxic generations or perplexity on WikiText-103. We present additional experimental details and results in Appendix C.</p>
<h1>4 LEARNING VIA ADDITION</h1>
<p>We now turn our attention to adding task vectors, either to build multi-task models that are proficient on multiple tasks simultaneously, or to improve single-task performance. This operation allows us to reuse and transfer knowledge either from in-house models, or from the multitude of publicly available fine-tuned models, without additional training or access to training data. We explore addition on various image classification and natural language processing tasks.</p>
<h3>4.1 IMAGE CLASSIFICATION</h3>
<p>We start with the same eight models used in Section 3, fine-tuned on a diverse set of image classification tasks (Cars, DTD, EuroSAT, GTSRB, MNIST, RESISC45, SUN397 and SVHN). In Figure 2, we</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Adding pairs of task vectors from image classification tasks. Adding task vectors from two tasks improves accuracy on both, resulting in a single model that is competitive with using two specialized fine-tuned models.
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Adding task vectors builds multi-task models for image classification tasks. Accuracy is averaged over all downstream tasks. When more task vectors are available, better multi-task vectors can be built. Each point represents an experiment with a subset of the eight tasks we study, and the solid line connects the average performance for each subset size. Recall that the average normalized accuracy of using multiple fine-tuned models is always one. Additional details and experiments are in Appendix D.
show the accuracy obtained by adding all pairs of task vectors from these tasks. To account for the difference in difficulty of the tasks, we normalize accuracy on each task by the accuracy of the model fine-tuned on that task. After normalizing, the performance of fine-tuned models on their respective tasks is one, and so the average performance of using multiple specialized models is also one. As shown in Figure 2, adding pairs of task vectors leads to a single model that outperforms the zero-shot model by a large margin, and is competitive with using two specialized models ( $98.9 \%$ normalized accuracy on average).
Beyond pairs of tasks, we explore adding task vectors for all possible subsets of the tasks ( $2^{\mathrm{a}}$ in total). In Figure 3, we show how the normalized accuracy of the resulting models, averaged over all the eight tasks. As the number of available task vectors increases, better multi-task models can be produced. When all task vectors are available, the best model produced by adding task vectors reaches an average performance of $91.2 \%$, despite compressing several models into one. Additional experiments and details are presented in Appendix D.</p>
<h1>4.2 NATURAL LANGUAGE PROCESSING</h1>
<p>In addition to building multi-task models, we explore whether adding task vectors is a useful way of improving performance on a single target task. Towards this goal, we first fine-tune T5-base models on four tasks from the GLUE benchmark [93], as in Wortsman et al. [99]. Then, we search for compatible checkpoints on Hugging Face Hub, finding 427 candidates in total. We try adding each of the corresponding task vectors to our fine-tuned models, choosing the best checkpoint and scaling coefficient based on held-out validation data. As shown in Table 3, adding task vectors can improve performance on target tasks, compared to fine-tuning. Additional details and experiments-including building multi-task models from public checkpoints from Hugging Face Hub—are presented in Appendix D.</p>
<p>Table 3: Improving performance on target tasks with external task vectors. For four text classification tasks from the GLUE benchmark, adding task vectors downloaded from the Hugging Face Hub can improve accuracy of fine-tuned T5 models. Appendix D. 6 shows additional details.</p>
<table>
<thead>
<tr>
<th>Method</th>
<th>MRPC</th>
<th>RTE</th>
<th>CoLA</th>
<th>SST-2</th>
<th>Average</th>
</tr>
</thead>
<tbody>
<tr>
<td>Zero-shot</td>
<td>74.8</td>
<td>52.7</td>
<td>8.29</td>
<td>92.7</td>
<td>57.1</td>
</tr>
<tr>
<td>Fine-tuned</td>
<td>88.5</td>
<td>77.3</td>
<td>52.3</td>
<td>94.5</td>
<td>78.1</td>
</tr>
<tr>
<td>Fine-tuned + task vectors</td>
<td>$89.3(=0.8)$</td>
<td>$77.5(=0.2)$</td>
<td>$53.0(=0.7)$</td>
<td>$94.7(=0.2)$</td>
<td>$78.6(=0.5)$</td>
</tr>
</tbody>
</table>
<p>Table 4: Improving domain generalization with task analogies. Using an auxiliary task for which labeled data is available and unlabeled data from both the auxiliary and the target datasets, task analogies improve the accuracy for multiple T5 models and two sentiment analysis target tasks [102; 65], without using any labeled data from the target tasks.</p>
<p>|  | target $=$ Yelp | | | target $=$ Amazon | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | |</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Learning about subpopulations via analogy. Combining task vectors from related subpopulations improves accuracy on the target subpopulation, when little or no data from the target supopulation is available. Accuracy is averaged over the four target subpopulations and three CLIP models. Additional details are in Appendix E.3.</p>
<p>Figure 5: Task vectors are typically close to orthogonal. The plot shows the cosine similarities between vectors for different tasks, using CLIP. The largest deviations from orthogonality are found when tasks are similar to each other, for instance, for MNIST, SVHN and GTSRB-where recognizing digits is either the task itself (MNIST and SVHN), or a capability needed to solve the task (GTSRB, where the task is traffic sign recognition)—and EuroSAT and RESISC45, two satellite imagery recognition datasets.
"real dog", "real lion", "sketch dog" and "sketch lion" as a running example. We present more details and samples in Appendix E.3.</p>
<p>Given a target subpopulation, we create task vectors by fine-tuning three models independently on the remaining subpopulations, and then combine them via task arithmetic, e.g., $\hat{\tau}<em _sketch="{sketch" _text="\text" dog="dog">{\text {sketch lion }}=$ $\tau</em>\right)$ for the target subpopulation "sketch lion". We show the results in Figure 4, averaged over the four target subpopulations. Compared to the pre-trained model, task vectors improve accuracy by 3.4 percentage points on average. Moreover, when some data from the target subpopulation is available for fine-tuning, starting from the edited model leads to consistently higher accuracy than starting from the pre-trained model. The gains from analogies alone (with no additional data) are roughly the same as that of collecting and annotating around one hundred training samples for the target subpopulation.}}+\left(\tau_{\text {real lion }}-\tau_{\text {real dog }</p>
<p>Kings and queens. We explore whether an image classifier can learn a new categories (e.g., "king") using data from three related classes that form an analogy relationship (e.g., "queen", "man" and "woman"). Our results are presented in Appendix E.2, showing that task analogies yield large gains in accuracy over pre-trained models on the new target category, despite having no training data for it.</p>
<h1>6 DISCUSSION</h1>
<p>In this section, we provide further insight into previous results by exploring the similarity between task vectors for different tasks, as well as the impact of different learning rates and random seeds. Additional analysis are presented in Appendix A, including discussions on the connection between ensembles and weight averaging. We conclude by discussing some limitations of our approach.</p>
<p>Similarity between task vectors. In Figure 5, we explore the cosine similarity between task vectors for different tasks, in an effort to understand how multiple models can be collapsed into a single multi-task model via addition (Section 4). We observe that vectors from different tasks are typically close to orthogonal, and speculate that this enables the combination of task vectors via addition with minimal interference. We also observe higher cosine similarities when tasks are semantically similar to each other. For example, the largest cosine similarities in Figure 5 (left) are between MNIST, SVHN and GTSRB, where recognizing digits is essential for the tasks, and between EuroSAT and RESISC45, which are both satellite imagery recognition datasets. This similarity in "task space" could help explain some results in Ilharco et al. [39], where interpolating the weights of a model</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Learning rate (log scale)
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: The impact of learning rate when fine-tuning. When adding task vectors from CLIP ViT-L/14 models finetuned on MNIST and EuroSAT, lower learning rates make the best use of the fine-tuned models, and also correspond to the highest accuracies of the fine-tuned models on the target task.</p>
<p>Accuracy of added intermediate task vectors
<img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Accuracy of added intermediate task vectors
<img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 7: How task vectors evolve throughout fine-tuning. Left: the cosine similarity between the final task vector and task vectors produced at intermediate points during fine-tuning. Right: Accuracy obtained by adding intermediate task vectors from MNIST and EuroSAT. Adding intermediate task vectors can lead to high accuracy, despite fine-tuning for substantially fewer steps.
fine-tuned on one task and the pre-trained model weights-in our terminology, applying a single task vector-sometimes improves accuracy on a different task for which no data is available (e.g., applying the MNIST task vector improves accuracy on SVHN).
The impact of the learning rate. In Figure 6, we observe that increasing the learning rate degrades accuracy both when using task vectors and when fine-tuning individual models, but the decrease is more gradual for individual models. These findings align with those of [99], who observed that accuracy decreases on the linear path between two fine-tuned models when using a larger learning rate. Thus, while larger learning rates may be acceptable when fine-tuning individual models, we recommend more caution when using task vectors. Further, we hypothesize that larger learning rates may explain some of the variance when adding vectors from natural language processing tasks, where we take models fine-tuned by others in the community.</p>
<p>The evolution of task vectors throughout fine-tuning. In Figure 7, we show how task vectors evolve throughout fine-tuning. Intermediate task vectors converge rapidly to the direction of the final task vector obtained at the end of fine-tuning. Moreover, the accuracy of the model obtained by adding intermediate task vectors from two image classification tasks saturates after just a few hundred steps. These results suggest that using intermediate task vectors can be a useful way of saving compute with little harm in accuracy.</p>
<p>Limitations. Task vectors are restricted to models with the same architecture, since they depend on element-wise operations on model weights. Further, in all of our experiments we perform arithmetic operations only on models fine-tuned from the same pre-trained initialization, although emerging work shows promise in relaxing this assumption [2]. We also note that some architectures are very popular, and have "standard" initializations-e.g., at the time of writing there are over 3,000 models on Hugging Face Hub fine-tuned from the same BERT-base initialization [17], and over 800 models fine-tuned from the same T5-small initialization.</p>
<h1>7 Related work</h1>
<p>The loss landscape and interpolating weights. The geometry of neural network loss surfaces has attracted the interest of several authors in recent years [54; 28; 21; 48; 25; 13; 98; 7; 23; 55; 60]. Despite neural networks being non-linear, previous work has empirically found that interpolations between the weights of two neural networks can maintain their high accuracy, provided these two neural networks share part of their optimization trajectory [27; 40; 73; 26; 99; 11; 39].</p>
<p>In the context of fine-tuning, accuracy increases steadily when gradually moving the weights of a pre-trained model in the direction of its fine-tuned counterpart [100; 63; 39]. Beyond a single task, Matena \&amp; Raffel [63]; Ilharco et al. [39] found that when multiple models are fine-tuned on different tasks from the same initialization, averaging their weights can improve accuracy on the fine-tuning tasks. Similar results were found by Li et al. [55] when averaging the parameters of language models fine-tuned on various domains. Choshen et al. [11] showed that "fusing" fine-tuned models by averaging their weights creates a better starting point for fine-tuning on a new downstream task. Wortsman et al. [99] found that averaging the weights of models fine-tuned on multiple tasks can increase accuracy on a new downstream task, without any further training. These findings are aligned with results shown in Section 4. In this work, we go beyond interpolating between models, examining extrapolating between models and additional ways of combining them (Sections 3 and 5).</p>
<p>Model interventions. Considering that re-training models is prohibitively expensive in most circumstances, several authors have studied more efficient methods for modifying a model's behavior with interventions after pre-training, referring to this process by different names, such as patching [33; 89; 39; 71], editing [85; 69; 70], aligning [74; 4; 44; 32], or debugging [82; 30]. In contrast to previous literature, our work provides a unique way of editing models, where capabilities can be added or deleted in a modular and efficient manner by re-using fine-tuned models. Closer to our work is that of Subramani et al. [88], who explore steering language models with vectors added to its hidden states. In contrast, our work applies vectors in the weight space of pre-trained models and does not modify the standard fine-tuning procedure.</p>
<p>Task embeddings. Achille et al. [1]; Vu et al. [91; 92], inter alia, explored strategies for representing tasks with continuous embeddings, in order to to predict task similarities and transferability, or to create taxonomic relations. While the task vectors we build could be used for such purposes, our main goal is to use them as tools for steering the behavior of pre-trained models. Additionally, Lampinen \&amp; McClelland [50] propose a framework for adapting models based on relationships between tasks. In contrast to their work, our framework uses only linear combinations of model weights.</p>
<h2>8 CONCLUSION</h2>
<p>In this paper we introduce a new paradigm for editing models based on arithmetic operations over task vectors. For various vision and NLP models, adding multiple specialized task vectors results in a single model that performs well on all target tasks, or even improves performance on a single task. Negating task vectors allows users to remove undesirable behaviors, e.g., toxic generations, or even forget specific tasks altogether, while retaining performance everywhere else. Finally, task analogies leverage existing data to improve performance on domains or subpopulations where data is scarce.</p>
<p>Arithmetic operations over task vectors only involve adding or subtracting model weights, and thus are efficient to compute, especially when compared to alternatives that involve additional finetuning. Thus, users can easily experiment with various model edits, recycling and transferring knowledge from large collections of publicly available fine-tuned models. Since these operations result in a single model of the same size, they incur no extra inference cost. Our code is available at https://github.com/mlfoundations/task_vectors.</p>
<h1>ACKNOWLEDGEMENTS</h1>
<p>We thank Alex Fang, Ari Holtzman, Colin Raffel, Dhruba Ghosh, Jesse Dodge, Margaret Li, Ofir Press, Sam Ainsworth, Sarah Pratt, Stephen Mussmann, Tim Dettmers, and Vivek Ramanujan for helpful discussion and comments on the paper. This work is in part supported by the NSF AI Institute for Foundations of Machine Learning (IFML), Open Philanthropy, NSF IIS 1652052, NSF IIS 17303166, NSF IIS 2044660, ONR N00014-18-1-2826, ONR MURI N00014- 18-1-2670, DARPA N66001-19-2-4031, DARPA W911NF-15-1-0543, the Sloan Fellowship and gifts from AI2.</p>
<h2>REFERENCES</h2>
<p>[1] Alessandro Achille, Michael Lam, Rahul Tewari, Avinash Ravichandran, Subhransu Maji, Charless C Fowlkes, Stefano Soatto, and Pietro Perona. Task2vec: Task embedding for meta-learning. In International Conference on Computer Vision (ICCV), 2019. https: //arxiv.org/abs/1902.03545.
[2] Samuel K Ainsworth, Jonathan Hayase, and Siddhartha Srinivasa. Git re-basin: Merging models modulo permutation symmetries, 2022. https://arxiv.org/abs/2209.04836.
[3] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katie Millican, Malcolm Reynolds, et al. Flamingo: a visual language model for few-shot learning, 2022. https://arxiv.org/abs/2204.14198.
[4] Amanda Askell, Yuntao Bai, Anna Chen, Dawn Drain, Deep Ganguli, Tom Henighan, Andy Jones, Nicholas Joseph, Ben Mann, Nova DasSarma, et al. A general language assistant as a laboratory for alignment, 2021. https://arxiv.org/abs/2112.00861.
[5] Roy Bar-Haim, Ido Dagan, Bill Dolan, Lisa Ferro, Danilo Giampiccolo, Bernardo Magnini, and Idan Szpektor. The second pascal recognising textual entailment challenge. In II PASCAL challenge, 2006.
[6] Luisa Bentivogli, Peter Clark, Ido Dagan, and Danilo Giampiccolo. The fifth pascal recognizing textual entailment challenge. In TAC, 2009. https://cris.fbk.eu/handle/11582/ 5351 .
[7] Gregory Benton, Wesley Maddox, Sanae Lotfi, and Andrew Gordon Gordon Wilson. Loss surface simplexes for mode connecting volumes and fast ensembling. In International Conference on Machine Learning (ICML), 2021. https://arxiv.org/abs/2102.13042.
[8] Daniel Borkan, Lucas Dixon, Jeffrey Sorensen, Nithum Thain, and Lucy Vasserman. Nuanced metrics for measuring unintended bias with real data for text classification. In Companion Proceedings of the 2019 World Wide Web Conference, 2019. https://arxiv.org/abs/ 1903.04561.
[9] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are fewshot learners. In Advances in Neural Information Processing Systems (NeurIPS), 2020. https://arxiv.org/abs/2005.14165.
[10] Gong Cheng, Junwei Han, and Xiaoqiang Lu. Remote sensing image scene classification: Benchmark and state of the art. Proceedings of the Institute of Electrical and Electronics Engineers (IEEE), 2017. https://ieeexplore.ieee.org/abstract/document/ 7891544 .
[11] Leshem Choshen, Elad Venezian, Noam Slonim, and Yoav Katz. Fusing finetuned models for better pretraining, 2022. https://arxiv.org/abs/2204.03044.</p>
<p>[12] Mircea Cimpoi, Subhransu Maji, Iasonas Kokkinos, Sammy Mohamed, and Andrea Vedaldi. Describing textures in the wild. In Conference on Computer Vision and Pattern Recognition (CVPR), 2014. https://openaccess.thecvf.com/content_cvpr_2014/ html/Cimpoi_Describing_Textures_in_2014_CVPR_paper.html.
[13] Wojciech Marian Czarnecki, Simon Osindero, Razvan Pascanu, and Max Jaderberg. A deep neural network's loss surface contains every low-dimensional pattern, 2019. https: //arxiv.org/abs/1912.07559.
[14] Ido Dagan, Oren Glickman, and Bernardo Magnini. The pascal recognising textual entailment challenge. In Machine Learning Challenges Workshop, 2005. https://link.springer. com/chapter/10.1007/11736790_9.
[15] Nicola De Cao, Wilker Aziz, and Ivan Titov. Editing factual knowledge in language models. In Conference on Empirical Methods in Natural Language Processing (EMNLP), 2021. https: //arxiv.org/abs/2104.08164.
[16] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A largescale hierarchical image database. In Conference on Computer Vision and Pattern Recognition (CVPR), 2009. https://ieeexplore.ieee.org/abstract/document/ 5206848 .
[17] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In Conference of the North American Chapter of the Association for Computational Linguistics (NAACL), 2019. https: //aclanthology.org/N19-1423.
[18] Jesse Dodge, Gabriel Ilharco, Roy Schwartz, Ali Farhadi, Hannaneh Hajishirzi, and Noah Smith. Fine-tuning pretrained language models: Weight initializations, data orders, and early stopping, 2020. https://arxiv.org/abs/2002.06305/.
[19] William B. Dolan and Chris Brockett. Automatically constructing a corpus of sentential paraphrases. In International Workshop on Paraphrasing, 2005. https://aclanthology. org/I05-5002.
[20] Shachar Don-Yehiya, Elad Venezian, Colin Raffel, Noam Slonim, Yoav Katz, and Leshem Choshen. Cold fusion: Collaborative descent for distributed multitask finetuning, 2022. https://arxiv.org/abs/2212.01378.
[21] Felix Draxler, Kambis Veschgini, Manfred Salmhofer, and Fred Hamprecht. Essentially no barriers in neural network energy landscape. In International Conference on Machine Learning (ICML), 2018. https://arxiv.org/abs/1803.00885.
[22] Mathias Eitz, James Hays, and Marc Alexa. How do humans sketch objects? ACM Transactions on graphics (TOG), 2012. https://dl.acm.org/doi/10.1145/2185520. 2185540 .
[23] Rahim Entezari, Hanie Sedghi, Olga Saukh, and Behnam Neyshabur. The role of permutation invariance in linear mode connectivity of neural networks. In International Conference on Learning Representations (ICLR), 2022. https://arxiv.org/abs/2110.06296.
[24] Alexander R. Fabbri, Irene Li, Tianwei She, Suyi Li, and Dragomir R. Radev. Multi-news: a large-scale multi-document summarization dataset and abstractive hierarchical model, 2019. https://arxiv.org/abs/1906.01749.
[25] Stanislav Fort, Huiyi Hu, and Balaji Lakshminarayanan. Deep ensembles: A loss landscape perspective, 2019. https://arxiv.org/abs/1912.02757.
[26] Stanislav Fort, Gintare Karolina Dziugaite, Mansheej Paul, Sepideh Kharaghani, Daniel M Roy, and Surya Ganguli. Deep learning versus kernel learning: an empirical study of loss landscape geometry and the time evolution of the neural tangent kernel. In Advances in Neural Information Processing Systems (NeurIPS), 2020. https://arxiv.org/abs/2010. 15110 .</p>
<p>[27] Jonathan Frankle, Gintare Karolina Dziugaite, Daniel Roy, and Michael Carbin. Linear mode connectivity and the lottery ticket hypothesis. In International Conference on Machine Learning (ICML), 2020. https://proceedings.mlr.press/v119/frankle20a. html.
[28] Timur Garipov, Pavel Izmailov, Dmitrii Podoprikhin, Dmitry Vetrov, and Andrew Gordon Wilson. Loss surfaces, mode connectivity, and fast ensembling of dnns. In Advances in Neural Information Processing Systems (NeurIPS), 2018. https://arxiv.org/abs/1802. 10026 .
[29] Samuel Gehman, Suchin Gururangan, Maarten Sap, Yejin Choi, and Noah A. Smith. RealToxicityPrompts: Evaluating neural toxic degeneration in language models. In Findings of the Association for Computational Linguistics: EMNLP 2020, 2020. https: //aclanthology.org/2020.findings-emnlp.301.
[30] Mor Geva, Avi Caciularu, Guy Dar, Paul Roit, Shoval Sadde, Micah Shlain, Bar Tamir, and Yoav Goldberg. Lm-debugger: An interactive tool for inspection and intervention in transformer-based language models, 2022. https://arxiv.org/abs/2204.12130.
[31] Danilo Giampiccolo, Bernardo Magnini, Ido Dagan, and Bill Dolan. The third pascal recognizing textual entailment challenge. In ACL-PASCAL Workshop on Textual Entailment and Paraphrasing, 2007. https://aclanthology.org/W07-1401/.
[32] Amelia Glaese, Nat McAleese, Maja Trebacz, John Aslanides, Vlad Firoiu, Timo Ewalds, Maribeth Rauh, Laura Weidinger, Martin Chadwick, Phoebe Thacker, Lucy Campbell-Gillingham, Jonathan Uesato, Po-Sen Huang, Ramona Comanescu, Fan Yang, Abigail See, Sumanth Dathathri, Rory Greig, Charlie Chen, Doug Fritz, Jaume Sanchez Elias, Richard Green, Sona Mokra, Nicholas Fernando, Boxi Wu, Rachel Foley, Susannah Young, Iason Gabriel, William Isaac, John Mellor, Demis Hassabis, Koray Kavukcuoglu, Lisa Anne Hendricks, and Geoffrey Irving. Improving alignment of dialogue agents via targeted human judgements, 2022. https://www.deepmind.com/blog/building-safer-dialogue-agents.
[33] Karan Goel, Albert Gu, Yixuan Li, and Christopher Ré. Model patching: Closing the subgroup performance gap with data augmentation, 2020. https://arxiv.org/abs/2008. 06775 .
[34] Aditya Golatkar, Alessandro Achille, and Stefano Soatto. Eternal sunshine of the spotless net: Selective forgetting in deep networks. In Conference on Computer Vision and Pattern Recognition (CVPR), 2020. https://arxiv.org/abs/1911.04933.
[35] Laura Hanu and Unitary team. Detoxify, 2020. https://github.com/unitaryai/ detoxify.
[36] Patrick Helber, Benjamin Bischke, Andreas Dengel, and Damian Borth. Eurosat: A novel dataset and deep learning benchmark for land use and land cover classification. Journal of Selected Topics in Applied Earth Observations and Remote Sensing, 2019. https:// arxiv.org/abs/1709.00029.
[37] Dan Hendrycks, Kevin Zhao, Steven Basart, Jacob Steinhardt, and Dawn Song. Natural adversarial examples. In Conference on Computer Vision and Pattern Recognition (CVPR), 2021.
[38] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal large language models, 2022. https://arxiv.org/abs/ 2203.15556.
[39] Gabriel Ilharco, Mitchell Wortsman, Samir Yitzhak Gadre, Shuran Song, Hannaneh Hajishirzi, Simon Kornblith, Ali Farhadi, and Ludwig Schmidt. Patching open-vocabulary models by interpolating weights. In Advances in Neural Information Processing Systems (NeurIPS), 2022. https://arXiv.org/abs/2208.05592.</p>
<p>[40] Pavel Izmailov, Dmitrii Podoprikhin, Timur Garipov, Dmitry Vetrov, and Andrew Gordon Wilson. Averaging weights leads to wider optima and better generalization. In Conference on Uncertainty in Artificial Intelligence (UAI), 2018. https://arxiv.org/abs/1803. 05407 .
[41] Arthur Jacot, Franck Gabriel, and Clément Hongler. Neural tangent kernel: Convergence and generalization in neural networks. In Advances in Neural Information Processing Systems (NeurIPS), 2018. https://arxiv.org/abs/1806.07572.
[42] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc V Le, Yunhsuan Sung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language representation learning with noisy text supervision. In International Conference on Machine Learning (ICML), 2021. https://arxiv.org/abs/2102.05918.
[43] Jeevesh Juneja, Rachit Bansal, Kyunghyun Cho, João Sedoc, and Naomi Saphra. Linear connectivity reveals generalization strategies, 2022. https://arxiv.org/abs/2205. 12411/.
[44] Atoosa Kasirzadeh and Iason Gabriel. In conversation with artificial intelligence: aligning language models with human values, 2022. https://arxiv.org/abs/2209.00731.
[45] Daniel Khashabi, Sewon Min, Tushar Khot, Ashish Sabharwal, Oyvind Tafjord, Peter Clark, and Hannaneh Hajishirzi. UNIFIEDQA: Crossing format boundaries with a single QA system. In Findings of the Association for Computational Linguistics (EMNLP), 2020. https: //aclanthology.org/2020.findings-emnlp.171.
[46] Tushar Khot, Peter Clark, Michal Guerquin, Peter Jansen, and Ashish Sabharwal. Qasc: A dataset for question answering via sentence composition, 2020. https://arxiv.org/ abs/1910.11473v2.
[47] Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei. 3d object representations for finegrained categorization. In International Conference on Computer Vision Workshops (ICML), 2013. https://www.cv-foundation.org/openaccess/content_iccv_ workshops_2013/W19/html/Krause_3D_Object_Representations_ 2013_ICCV_paper.html.
[48] Rohith Kuditipudi, Xiang Wang, Holden Lee, Yi Zhang, Zhiyuan Li, Wei Hu, Rong Ge, and Sanjeev Arora. Explaining landscape connectivity of low-cost solutions for multilayer nets. Advances in Neural Information Processing Systems (NeurIPS), 2019. https://arxiv. org/abs/1906.06247.
[49] Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, and Eduard Hovy. RACE: Largescale ReAding comprehension dataset from examinations. In Conference on Empirical Methods in Natural Language Processing (EMNLP), 2017. https://aclanthology. org/D17-1082.
[50] Andrew K Lampinen and James L McClelland. Transforming task representations to perform novel tasks. Proceedings of the National Academy of Sciences, 2020.
[51] Yann LeCun. The mnist database of handwritten digits, 1998. http://yann.lecun. com/exdb/mnist/.
[52] Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient prompt tuning. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 3045-3059, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.243. URL https://aclanthology.org/2021.emnlp-main. 243.
[53] Quentin Lhoest, Albert Villanova del Moral, Yacine Jernite, Abhishek Thakur, Patrick von Platen, Suraj Patil, Julien Chaumond, Mariama Drame, Julien Plu, Lewis Tunstall, Joe Davison, Mario Šaško, Gunjan Chhablani, Bhavitvya Malik, Simon Brandeis, Teven Le Scao, Victor Sanh, Canwen Xu, Nicolas Patry, Angelina McMillan-Major, Philipp Schmid, Sylvain Gugger, Clément Delangue, Théo Matussière, Lysandre Debut, Stas Bekman, Pierric Cistac, Thibault</p>
<p>Goehringer, Victor Mustar, François Lagunas, Alexander Rush, and Thomas Wolf. Datasets: A community library for natural language processing. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pp. 175-184, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-demo.21. URL https://aclanthology. org/2021.emnlp-demo.21.
[54] Hao Li, Zheng Xu, Gavin Taylor, Christoph Studer, and Tom Goldstein. Visualizing the loss landscape of neural nets. Advances in Neural Information Processing Systems (NeurIPS), 2018. https://arxiv.org/abs/1712.09913.
[55] Margaret Li, Suchin Gururangan, Tim Dettmers, Mike Lewis, Tim Althoff, Noah A Smith, and Luke Zettlemoyer. Branch-train-merge: Embarrassingly parallel training of expert language models, 2022. https://arxiv.org/abs/2208.03306.
[56] Bill Yuchen Lin, Wangchunshu Zhou, Ming Shen, Pei Zhou, Chandra Bhagavatula, Yejin Choi, and Xiang Ren. CommonGen: A constrained text generation challenge for generative commonsense reasoning. In Findings of the Association for Computational Linguistics: EMNLP, 2020. https://www.aclweb.org/anthology/2020.findings-emnlp.165.
[57] Alisa Liu, Maarten Sap, Ximing Lu, Swabha Swayamdipta, Chandra Bhagavatula, Noah A. Smith, and Yejin Choi. DExperts: Decoding-time controlled text generation with experts and anti-experts. In Annual Meeting of the Association for Computational Linguistics (ACL), 2021. https://aclanthology.org/2021.acl-long.522.
[58] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Conference on Learning Representations (ICLR), 2019. URL https://openreview. net/forum?id=Bkg6RiCqY7.
[59] Ximing Lu, Sean Welleck, Liwei Jiang, Jack Hessel, Lianhui Qin, Peter West, Prithviraj Ammanabrolu, and Yejin Choi. Quark: Controllable text generation with reinforced unlearning, 2022. https://arxiv.org/abs/2205.13636.
[60] Ekdeep Singh Lubana, Eric J Bigelow, Robert P Dick, David Krueger, and Hidenori Tanaka. Mechanistic mode connectivity, 2022. https://arxiv.org/abs/2211.08422.
[61] James Lucas, Juhan Bae, Michael R Zhang, Stanislav Fort, Richard Zemel, and Roger Grosse. Analyzing monotonic linear interpolation in neural network loss landscapes, 2021. https: //arxiv.org/abs/2104.11044.
[62] Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts. Learning word vectors for sentiment analysis. In Annual Meeting of the Association for Computational Linguistics (ACL), 2011. http://www.aclweb.org/anthology/ P11-1015.
[63] Michael Matena and Colin Raffel. Merging models with fisher-weighted averaging. In Advances in Neural Information Processing Systems (NeurIPS), 2021. https://arxiv. org/abs/2111.09832.
[64] Brian W Matthews. Comparison of the predicted and observed secondary structure of t4 phage lysozyme. Biochimica et Biophysica Acta (BBA)-Protein Structure, 1975. https://www. sciencedirect.com/science/article/abs/pii/0005279575901099.
[65] Julian McAuley and Jure Leskovec. Hidden factors and hidden topics: understanding rating dimensions with review text. In ACM Conference on Recommender Systems, 2013. https: //dl.acm.org/doi/10.1145/2507157.2507163.
[66] Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models, 2016. https://arxiv.org/abs/1609.07843.
[67] Sewon Min, Mike Lewis, Luke Zettlemoyer, and Hannaneh Hajishirzi. MetaICL: Learning to learn in context. In Conference of the North American Chapter of the Association for Computational Linguistics (NAACL), 2022. https://aclanthology.org/2022. naacl-main. 201.</p>
<p>[68] Swaroop Mishra, Daniel Khashabi, Chitta Baral, and Hannaneh Hajishirzi. Cross-task generalization via natural language crowdsourcing instructions. In Annual Meeting of the Association for Computational Linguistics (ACL), 2022. https://aclanthology.org/2022. acl-long. 244 .
[69] Eric Mitchell, Charles Lin, Antoine Bosselut, Chelsea Finn, and Christopher D Manning. Fast model editing at scale. In International Conference on Learning Representations (ICLR), 2021. https://arxiv.org/abs/2110.11309.
[70] Eric Mitchell, Charles Lin, Antoine Bosselut, Christopher D Manning, and Chelsea Finn. Memory-based model editing at scale. In International Conference on Machine Learning, 2022. https://arxiv.org/abs/2206.06520.
[71] Shikhar Murty, Christopher D Manning, Scott Lundberg, and Marco Tulio Ribeiro. Fixing model bugs with natural language patches. In ACL Workshop on Learning with Natural Language Supervision, 2022. https://openreview.net/forum?id=blJrg3WvvDV.
[72] Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y Ng. Reading digits in natural images with unsupervised feature learning. In Advances in Neural Information Processing Systems (NeurIPS) Workshops, 2011. https://storage.googleapis. com/pub-tools-public-publication-data/pdf/37648.pdf.
[73] Behnam Neyshabur, Hanie Sedghi, and Chiyuan Zhang. What is being transferred in transfer learning? In Advances in Neural Information Processing Systems (NeurIPS), 2020. https: //arxiv.org/abs/2008.11687.
[74] Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback, 2022. https://arxiv.org/abs/2203. 02155 .
[75] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. Advances in Neural Information Processing Systems (NeurIPS), 2019. https://arxiv.org/abs/1912.01703.
[76] Hieu Pham, Zihang Dai, Golnaz Ghiasi, Hanxiao Liu, Adams Wei Yu, Minh-Thang Luong, Mingxing Tan, and Quoc V Le. Combined scaling for zero-shot transfer learning, 2021. https://arxiv.org/abs/2111.10050.
[77] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language Models are Unsupervised Multitask Learners, 2019. https://openai.com/ blog/better-language-models/.
[78] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In International Conference on Machine Learning (ICML), 2021. https://arxiv.org/ abs/2103.00020.
[79] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research (JMLR), 2020. http: //jmlr.org/papers/v21/20-074.html.
[80] Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. SQuAD: 100,000+ questions for machine comprehension of text. In Conference on Empirical Methods in Natural Language Processing, 2016. https://aclanthology.org/D16-1264.
[81] Vinay Venkatesh Ramasesh, Aitor Lewkowycz, and Ethan Dyer. Effect of scale on catastrophic forgetting in neural networks. In International Conference on Learning Representations (ICLR), 2021. https://openreview.net/forum?id=GhVS8_yPeEa.</p>
<p>[82] Marco Tulio Ribeiro and Scott Lundberg. Adaptive testing and debugging of nlp models. In Annual Meeting of the Association for Computational Linguistics (ACL), 2022. https: //aclanthology.org/2022.acl-long.230/.
[83] Fereshteh Sadeghi, C Lawrence Zitnick, and Ali Farhadi. Visalogy: Answering visual analogy questions. In Advances in Neural Information Processing Systems (NeurIPS), 2015.
[84] Victor Sanh, Albert Webson, Colin Raffel, Stephen H Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja, et al. Multitask prompted training enables zero-shot task generalization. In International Conference on Learning Representations (ICLR), 2021. https://arxiv.org/abs/2110.08207.
[85] Shibani Santurkar, Dimitris Tsipras, Mahalaxmi Elango, David Bau, Antonio Torralba, and Aleksander Madry. Editing a classifier by rewriting its prediction rules. In Advances in Neural Information Processing Systems (NeurIPS), 2021. https://arxiv.org/abs/2112. 01008 .
[86] Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Ng, and Christopher Potts. Recursive deep models for semantic compositionality over a sentiment treebank. In Conference on Empirical Methods in Natural Language Processing (EMNLP), 2013. https://aclanthology.org/D13-1170/.
[87] Johannes Stallkamp, Marc Schlipsing, Jan Salmen, and Christian Igel. The german traffic sign recognition benchmark: a multi-class classification competition. In International Joint Conference on Neural Networks (IJCNN), 2011. https://ieeexplore.ieee.org/ document/6033395.
[88] Nishant Subramani, Nivedita Suresh, and Matthew Peters. Extracting latent steering vectors from pretrained language models. In Findings of the Association for Computational Linguistics (ACL), 2022. https://aclanthology.org/2022.findings-acl.48.
[89] Yi-Lin Sung, Varun Nair, and Colin A Raffel. Training neural networks with fixed sparse masks. In Advances in Neural Information Processing Systems (NeurIPS), 2021. https: //arxiv.org/abs/2111.09839.
[90] Ayush K Tarun, Vikram S Chundawat, Murari Mandal, and Mohan Kankanhalli. Fast yet effective machine unlearning, 2021. https://arxiv.org/abs/2111.08947.
[91] Tu Vu, Tong Wang, Tsendsuren Munkhdalai, Alessandro Sordoni, Adam Trischler, Andrew Mattarella-Micke, Subhransu Maji, and Mohit Iyyer. Exploring and predicting transferability across NLP tasks. In Conference on Empirical Methods in Natural Language Processing (EMNLP), 2020. https://aclanthology.org/2020.emnlp-main.635.
[92] Tu Vu, Brian Lester, Noah Constant, Rami Al-Rfou', and Daniel Cer. SPoT: Better frozen model adaptation through soft prompt transfer. In Annual Meeting of the Association for Computational Linguistics (ACL), 2022. https://aclanthology.org/2022.acl-long. 346 .
[93] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman. Glue: A multi-task benchmark and analysis platform for natural language understanding. In International Conference on Learning Representations (ICLR), 2018. https://arxiv. org/abs/1804.07461.
[94] Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei, Anjana Arunkumar, Arjun Ashok, Arut Selvan Dhanasekaran, Atharva Naik, David Stap, et al. Benchmarking generalization via in-context instructions on 1,600+ language tasks, 2022. https://arxiv.org/abs/2204.07705.
[95] Alex Warstadt, Amanpreet Singh, and Samuel R. Bowman. Neural network acceptability judgments. Transactions of the Association for Computational Linguistics (TACL), 2019. https://aclanthology.org/Q19-1040/.</p>
<p>[96] Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, and Quoc V Le. Finetuned language models are zero-shot learners. In International Conference on Learning Representations (ICLR), 2021. https://arxiv. org/abs/2109.01652/.
[97] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, et al. Huggingface's transformers: State-of-the-art natural language processing, 2019. https://arxiv.org/abs/ 1910.03771 .
[98] Mitchell Wortsman, Maxwell C Horton, Carlos Guestrin, Ali Farhadi, and Mohammad Rastegari. Learning neural network subspaces. In International Conference on Machine Learning (ICML), 2021. http://proceedings.mlr.press/v139/wortsman21a.html.
[99] Mitchell Wortsman, Gabriel Ilharco, Samir Yitzhak Gadre, Rebecca Roelofs, Raphael GontijoLopes, Ari S Morcos, Hongseok Namkoong, Ali Farhadi, Yair Carmon, Simon Kornblith, et al. Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time. In International Conference on Machine Learning (ICML), 2022. https://arxiv.org/abs/2203.05482.
[100] Mitchell Wortsman, Gabriel Ilharco, Mike Li, Jong Wook Kim, Hannaneh Hajishirzi, Ali Farhadi, Hongseok Namkoong, and Ludwig Schmidt. Robust fine-tuning of zero-shot models. In Conference on Computer Vision and Pattern Recognition (CVPR), 2022. https:// arxiv.org/abs/2109.01903.
[101] Jianxiong Xiao, Krista A Ehinger, James Hays, Antonio Torralba, and Aude Oliva. Sun database: Exploring a large collection of scene categories. International Journal of Computer Vision (IJCV), 2016. https://link.springer.com/article/10.1007/ s11263-014-0748-y.
[102] Xiang Zhang, Junbo Zhao, and Yann LeCun. Character-level convolutional networks for text classification. In Advances in Neural Information Processing Systems (NeurIPS), 2015. https://proceedings.neurips.cc/paper/2015/file/ 250cf8b51c773f3f8dc8b4be867a9a02-Paper.pdf.
[103] Ruiqi Zhong, Kristy Lee, Zheng Zhang, and Dan Klein. Adapting language models for zeroshot learning by meta-tuning on dataset and prompt collections. In Findings of the Association for Computational Linguistics (EMNLP), 2021. https://aclanthology.org/2021. findings-emnlp. 244.
[104] Chen Zhu, Ankit Singh Rawat, Manzil Zaheer, Srinadh Bhojanapalli, Daliang Li, Felix Yu, and Sanjiv Kumar. Modifying memories in transformer models, 2020. https://arxiv. org/abs/2012.00363.
[105] Fuzhen Zhuang, Zhiyuan Qi, Keyu Duan, Dongbo Xi, Yongchun Zhu, Hengshu Zhu, Hui Xiong, and Qing He. A comprehensive survey on transfer learning. Proceedings of the IEEE, 2020. https://arxiv.org/abs/1911.02685.</p>
<p><img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 8: When adding two task vectors, the performance of the resulting model approximates the performing of ensembling the corresponding fine-tuned models.</p>
<h1>A The LOSS LANDSCAPE, WEIGHT AVERAGING AND ENSEMBLES</h1>
<p>When two neural networks share part of their optimization trajectory-such as when fine-tuning from the same pre-trained initialization-previous work found that performance does not decrease substantially when linearly interpolating between their weights [27; 40; 73; 26; 99; 11; 39]. Applying a task vector-and any vectors produced via the arithmetic expressions we study in this workis equivalent to a linear combination of the pre-trained model and the fine-tuned models used to generate the task vectors, since only linear operations are used. Interpolating between the weights of a fine-tuned model and its pre-trained counterpart as in Wortsman et al. [100]; Ilharco et al. [39] is equivalent to applying a single task vector, and adding different task vectors is equivalent to a weighted average of all models, similar to experiments from Wortsman et al. [99]; Ilharco et al. [39]; Li et al. [55]. Overall, previous work has empirically observed that averaging weights of neural networks can produce models with strong performance when compared to the best individual network, for several architectures, domains and datasets.</p>
<p>Our motivation for studying task vectors is also well aligned with findings of Lucas et al. [61]; Ilharco et al. [39], who observed that performance steadily increases on the linear path between a model before and after training. ${ }^{3}$ This indicates that the direction from the pre-trained to the fine-tuned model is such that movement in that direction directly translates to performance gains on the fine-tuning task. Moreover, Ilharco et al. [39] found that linear interpolations between a pre-trained model and a fine-tuned model are able to preserve accuracy on tasks that are unrelated to fine-tuning, while greatly improving accuracy on the fine-tuning task compared to the pre-trained model. That accuracy on the fine-tuning task and on unrelated tasks are independent of each other along the linear path between pre-trained and fine-tuned models is well aligned with our results on from Section 3, where we find that extrapolating from the pre-trained model away from the fine-tuned model leads to worse performance on the fine-tuning task with little change in behavior on control tasks.</p>
<p>Finally, we highlight the connection between linear combinations of neural network weights and the well-established practice of ensembling their predictions. ${ }^{4}$ This connection is discussed in depth by Wortsman et al. [100; 99], and we briefly revisit it in the context of adding task vectors. First, recall that the arithmetic operations we study result in linear combinations of model weights. As shown by Wortsman et al. [100], in certain regimes, the result from linearly combining the weights of neural network approximate ensembling their outputs. This approximation holds whenever the</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>loss can be locally approximated by a linear expansion, which is referred to as the NTK regime [41]. Moreover, as shown by Fort et al. [26], this linear expansion becomes more accuracy in the later phase of training neural networks, which closely resembles fine-tuning. When the approximation holds exactly, weight averaging and ensembles are exactly equivalent [100]. This connection is further studied analytically and empirically by Wortsman et al. [99].
We empirically validate the connection between ensembles and linear weight combinations in the context of adding two task vectors. Note that the model resulting from adding two task vectors with a scaling coefficient $\lambda=0.5$ is equivalent to a uniform average of the weights of the fine-tuned models. ${ }^{5}$ We then investigate whether accuracy of the model obtained using the task vectors correlates with the accuracy of ensembling the fine-tuned models, as predicted by theory. As shown in Figure 8, we indeed observe that the accuracy of the model produced by adding two task vectors closely follows the accuracy of the corresponding ensemble. We observe a slight bias towards higher accuracy for the ensembles on average, and that the two quantities are also strongly correlated, with a Pearson correlation of 0.99 .</p>
<h1>B FORGETTING IMAGE CLASSIFICATION TASKS</h1>
<p>This section presents additional experimental details and results complementing the findings presented in Section 3.1, showcasing the effect of negating task vectors from image classification tasks.</p>
<h2>B. 1 EXPERIMENTAL DETAILS</h2>
<p>We follow the same procedure from [39] when fine-tune CLIP models [78]. Namely, we fine-tune for 2000 iterations with a batch size of 128, learning rate $1 \mathrm{e}-5$ and a cosine annealing learning rate schedule with 200 warm-up steps and the AdamW optimizer [58; 75], with weight decay 0.1 . When fine-tuning, we freeze the weights of the classification layer output by CLIP's text encoder, so that we do not introduce additional learnable parameters, as in [39]. As shown by [39], freezing the classification layer does not harm accuracy. After fine-tuning, we evaluate scaling coefficients $\lambda \in{0.0,0.05,0.1, \cdots, 1.0}$, choosing the highest value such that the resulting model still retains at least $95 \%$ of the accuracy of the pre-trained model on the control task.</p>
<h2>B. 2 BASELINES</h2>
<p>We contrast our results with two baselines, fine-tuning with gradient ascent as in Golatkar et al. [34]; Tarun et al. [90], and against using a random vector of the same magnitude as the task vector on a layer-by-layer basis.</p>
<p>In practice, for fine-tuning with gradient ascent, we use the same hyper-parameters as for standard finetuning. However, instead of optimizing to minimize the cross-entropy loss $\ell=\mathbb{E}<em y="y">{x, y \in \mathcal{D}}\left[-\log f(x)</em>}\right]$, we optimize to minimize its negative value, $\ell_{\text {neg }}=-\ell=\mathbb{E<em y="y">{x, y \in \mathcal{D}}\left[\log f(x)</em>$ is the probability assigned by the model $f$ that the inputs $x$ belong to label $y$. This is equivalent to performing gradient ascent on $\ell$.}\right]$, where $x, y$ are samples in the dataset $\mathcal{D}$ and $f(x)_{y</p>
<p>For the random vector baseline, we first compute the different between the parameters of the pretrained and fine-tuned models for each layer $L, \tau^{(L)}=\theta_{0}^{(L)}-\theta_{\text {pre }}^{(L)}$. Then, we draw a new vector $\tau_{\text {rand }}^{(L)} \sim \mathcal{N}(0, I)$ where each element is drawn from a normal distribution with mean 0 and variance 1. We then scale this vector so it has the same magnitude as $\tau^{(L)}$, resulting in $\tau_{\text {scaled }}^{(L)}=\tau_{\text {rand }}^{(L)}\left|\tau_{\text {rand }}^{(L)}\right|$. Finally, we concatenate all the vectors $\tau_{\text {scaled }}^{(L)}$ for all layers to form a new vector withe the same dimensionality as the model parameters $\theta$, which is used in the same way as task vectors.</p>
<h2>B. 3 BREAKDOWN PER TASK</h2>
<p>Tables 5, 6 and 7 show a breakdown of accuracy for the eight tasks and the three CLIP models we examine.</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Table 5: Forgetting via negation on image classification tasks. Results are shown for a CLIP ViT-L/14 model [78], reporting accuracy on both the target (T) and control (C) tasks.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Method</th>
<th style="text-align: center;">Cars</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">DTD</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">EuroSAT</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">GTSRB</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">MNIST</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">RESISC45</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">SUN397</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">SVHN</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\mathrm{T} \downarrow$</td>
<td style="text-align: center;">C $\uparrow$</td>
<td style="text-align: center;">$\mathrm{T} \downarrow$</td>
<td style="text-align: center;">C $\uparrow$</td>
<td style="text-align: center;">$\mathrm{T} \downarrow$</td>
<td style="text-align: center;">C $\uparrow$</td>
<td style="text-align: center;">$\mathrm{T} \downarrow$</td>
<td style="text-align: center;">C $\uparrow$</td>
<td style="text-align: center;">$\mathrm{T} \downarrow$</td>
<td style="text-align: center;">C $\uparrow$</td>
<td style="text-align: center;">$\mathrm{T} \downarrow$</td>
<td style="text-align: center;">C $\uparrow$</td>
<td style="text-align: center;">$\mathrm{T} \downarrow$</td>
<td style="text-align: center;">C $\uparrow$</td>
<td style="text-align: center;">$\mathrm{T} \downarrow$</td>
<td style="text-align: center;">C $\uparrow$</td>
</tr>
<tr>
<td style="text-align: center;">Pre-trained</td>
<td style="text-align: center;">77.8</td>
<td style="text-align: center;">75.5</td>
<td style="text-align: center;">55.4</td>
<td style="text-align: center;">75.5</td>
<td style="text-align: center;">60.2</td>
<td style="text-align: center;">75.5</td>
<td style="text-align: center;">50.6</td>
<td style="text-align: center;">75.5</td>
<td style="text-align: center;">76.4</td>
<td style="text-align: center;">75.5</td>
<td style="text-align: center;">71.0</td>
<td style="text-align: center;">75.5</td>
<td style="text-align: center;">68.3</td>
<td style="text-align: center;">75.5</td>
<td style="text-align: center;">58.6</td>
<td style="text-align: center;">75.5</td>
</tr>
<tr>
<td style="text-align: center;">Fine-tuned</td>
<td style="text-align: center;">92.8</td>
<td style="text-align: center;">73.1</td>
<td style="text-align: center;">83.7</td>
<td style="text-align: center;">72.3</td>
<td style="text-align: center;">99.2</td>
<td style="text-align: center;">70.5</td>
<td style="text-align: center;">99.3</td>
<td style="text-align: center;">73.1</td>
<td style="text-align: center;">99.8</td>
<td style="text-align: center;">72.9</td>
<td style="text-align: center;">96.9</td>
<td style="text-align: center;">73.8</td>
<td style="text-align: center;">82.4</td>
<td style="text-align: center;">72.7</td>
<td style="text-align: center;">98.0</td>
<td style="text-align: center;">72.6</td>
</tr>
<tr>
<td style="text-align: center;">Neg. gradients</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">4.82</td>
<td style="text-align: center;">2.13</td>
<td style="text-align: center;">0.10</td>
<td style="text-align: center;">9.26</td>
<td style="text-align: center;">1.07</td>
<td style="text-align: center;">1.19</td>
<td style="text-align: center;">0.07</td>
<td style="text-align: center;">9.80</td>
<td style="text-align: center;">67.0</td>
<td style="text-align: center;">2.14</td>
<td style="text-align: center;">0.07</td>
<td style="text-align: center;">0.25</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">6.70</td>
<td style="text-align: center;">57.2</td>
</tr>
<tr>
<td style="text-align: center;">Random vector</td>
<td style="text-align: center;">72.0</td>
<td style="text-align: center;">73.3</td>
<td style="text-align: center;">52.1</td>
<td style="text-align: center;">72.2</td>
<td style="text-align: center;">59.7</td>
<td style="text-align: center;">73.5</td>
<td style="text-align: center;">43.4</td>
<td style="text-align: center;">72.5</td>
<td style="text-align: center;">74.8</td>
<td style="text-align: center;">72.8</td>
<td style="text-align: center;">70.8</td>
<td style="text-align: center;">73.0</td>
<td style="text-align: center;">66.9</td>
<td style="text-align: center;">72.7</td>
<td style="text-align: center;">47.1</td>
<td style="text-align: center;">72.9</td>
</tr>
<tr>
<td style="text-align: center;">Neg. task vector</td>
<td style="text-align: center;">32.0</td>
<td style="text-align: center;">72.4</td>
<td style="text-align: center;">26.7</td>
<td style="text-align: center;">72.2</td>
<td style="text-align: center;">7.33</td>
<td style="text-align: center;">73.3</td>
<td style="text-align: center;">6.45</td>
<td style="text-align: center;">72.2</td>
<td style="text-align: center;">2.69</td>
<td style="text-align: center;">74.9</td>
<td style="text-align: center;">19.7</td>
<td style="text-align: center;">72.9</td>
<td style="text-align: center;">50.8</td>
<td style="text-align: center;">72.6</td>
<td style="text-align: center;">6.71</td>
<td style="text-align: center;">72.7</td>
</tr>
</tbody>
</table>
<p>Table 6: Forgetting via negation on image classification tasks. Results are shown for a CLIP ViT-B/16 model [78], reporting accuracy on both the target (T) and control (C) tasks.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Method</th>
<th style="text-align: center;">Cars</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">DTD</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">EuroSAT</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">GTSRB</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">MNIST</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">RESISC45</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">SUN397</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">SVHN</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\mathrm{T} \downarrow$</td>
<td style="text-align: center;">C $\uparrow$</td>
<td style="text-align: center;">$\mathrm{T} \downarrow$</td>
<td style="text-align: center;">C $\uparrow$</td>
<td style="text-align: center;">$\mathrm{T} \downarrow$</td>
<td style="text-align: center;">C $\uparrow$</td>
<td style="text-align: center;">$\mathrm{T} \downarrow$</td>
<td style="text-align: center;">C $\uparrow$</td>
<td style="text-align: center;">$\mathrm{T} \downarrow$</td>
<td style="text-align: center;">C $\uparrow$</td>
<td style="text-align: center;">$\mathrm{T} \downarrow$</td>
<td style="text-align: center;">C $\uparrow$</td>
<td style="text-align: center;">$\mathrm{T} \downarrow$</td>
<td style="text-align: center;">C $\uparrow$</td>
<td style="text-align: center;">$\mathrm{T} \downarrow$</td>
<td style="text-align: center;">C $\uparrow$</td>
</tr>
<tr>
<td style="text-align: center;">Pre-trained</td>
<td style="text-align: center;">64.6</td>
<td style="text-align: center;">68.3</td>
<td style="text-align: center;">44.9</td>
<td style="text-align: center;">68.3</td>
<td style="text-align: center;">53.9</td>
<td style="text-align: center;">68.3</td>
<td style="text-align: center;">43.4</td>
<td style="text-align: center;">68.3</td>
<td style="text-align: center;">51.6</td>
<td style="text-align: center;">68.3</td>
<td style="text-align: center;">65.8</td>
<td style="text-align: center;">68.3</td>
<td style="text-align: center;">65.5</td>
<td style="text-align: center;">68.3</td>
<td style="text-align: center;">52.0</td>
<td style="text-align: center;">68.3</td>
</tr>
<tr>
<td style="text-align: center;">Fine-tuned</td>
<td style="text-align: center;">87.0</td>
<td style="text-align: center;">61.9</td>
<td style="text-align: center;">82.3</td>
<td style="text-align: center;">57.5</td>
<td style="text-align: center;">99.1</td>
<td style="text-align: center;">56.0</td>
<td style="text-align: center;">99.0</td>
<td style="text-align: center;">54.7</td>
<td style="text-align: center;">99.7</td>
<td style="text-align: center;">55.2</td>
<td style="text-align: center;">96.4</td>
<td style="text-align: center;">62.2</td>
<td style="text-align: center;">79.0</td>
<td style="text-align: center;">61.7</td>
<td style="text-align: center;">97.7</td>
<td style="text-align: center;">56.8</td>
</tr>
<tr>
<td style="text-align: center;">Neg. gradients</td>
<td style="text-align: center;">0.36</td>
<td style="text-align: center;">0.11</td>
<td style="text-align: center;">2.13</td>
<td style="text-align: center;">0.09</td>
<td style="text-align: center;">9.26</td>
<td style="text-align: center;">0.14</td>
<td style="text-align: center;">0.71</td>
<td style="text-align: center;">0.10</td>
<td style="text-align: center;">0.04</td>
<td style="text-align: center;">1.20</td>
<td style="text-align: center;">2.60</td>
<td style="text-align: center;">0.10</td>
<td style="text-align: center;">0.25</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">0.08</td>
<td style="text-align: center;">3.69</td>
</tr>
<tr>
<td style="text-align: center;">Rand. task vector</td>
<td style="text-align: center;">61.0</td>
<td style="text-align: center;">65.6</td>
<td style="text-align: center;">43.9</td>
<td style="text-align: center;">66.3</td>
<td style="text-align: center;">51.7</td>
<td style="text-align: center;">66.2</td>
<td style="text-align: center;">43.1</td>
<td style="text-align: center;">65.0</td>
<td style="text-align: center;">51.6</td>
<td style="text-align: center;">68.3</td>
<td style="text-align: center;">63.6</td>
<td style="text-align: center;">65.6</td>
<td style="text-align: center;">63.7</td>
<td style="text-align: center;">65.2</td>
<td style="text-align: center;">46.2</td>
<td style="text-align: center;">65.5</td>
</tr>
<tr>
<td style="text-align: center;">Neg. task vector</td>
<td style="text-align: center;">30.8</td>
<td style="text-align: center;">65.4</td>
<td style="text-align: center;">26.5</td>
<td style="text-align: center;">65.6</td>
<td style="text-align: center;">12.3</td>
<td style="text-align: center;">65.8</td>
<td style="text-align: center;">9.53</td>
<td style="text-align: center;">65.8</td>
<td style="text-align: center;">9.55</td>
<td style="text-align: center;">65.4</td>
<td style="text-align: center;">26.5</td>
<td style="text-align: center;">65.1</td>
<td style="text-align: center;">48.6</td>
<td style="text-align: center;">65.1</td>
<td style="text-align: center;">6.43</td>
<td style="text-align: center;">65.4</td>
</tr>
</tbody>
</table>
<p>We observe qualitatively similar results in all cases. Similarly to what is observed in [39], we also see that results improve with scale: on average, the largest model, ViT-L/14, achieves lower accuracy on the target tasks, compared to the smaller models.</p>
<h1>B. 4 ADDITIONAL VISUALIZATIONS</h1>
<p>In Figure 9, we show how accuracy on the target and control tasks vary as we change the scaling coefficients $\lambda$, both for the task vector obtained by fine-tuning on the target task and for a random vector of the same magnitude.</p>
<p>As the scaling coefficient increases, the curves traced by the task vector and a random vector behave differently. For task vectors, performance on the target tasks ( $y$-axis) initially decreases faster than performance on the control task ( $x$-axis), so there exists models with high accuracy on the control task but low accuracy on the target task. In contrast, such points do not exist in the curves traced by random vectors, which move more linearly towards the origin. In practice, this means forgetting is effective for task vectors obtained by fine-tuning, but not for random vectors.</p>
<p>Table 7: Forgetting via negation on image classification tasks. Results are shown for a CLIP ViT-B/32 model [78], reporting accuracy on both the target (T) and control (C) tasks.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Method</th>
<th style="text-align: center;">Cars</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">DTD</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">EuroSAT</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">GTSRB</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">MNIST</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">RESISC45</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">SUN397</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">SVHN</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\mathrm{T} \downarrow$</td>
<td style="text-align: center;">C $\uparrow$</td>
<td style="text-align: center;">$\mathrm{T} \downarrow$</td>
<td style="text-align: center;">C $\uparrow$</td>
<td style="text-align: center;">$\mathrm{T} \downarrow$</td>
<td style="text-align: center;">C $\uparrow$</td>
<td style="text-align: center;">$\mathrm{T} \downarrow$</td>
<td style="text-align: center;">C $\uparrow$</td>
<td style="text-align: center;">$\mathrm{T} \downarrow$</td>
<td style="text-align: center;">C $\uparrow$</td>
<td style="text-align: center;">$\mathrm{T} \downarrow$</td>
<td style="text-align: center;">C $\uparrow$</td>
<td style="text-align: center;">$\mathrm{T} \downarrow$</td>
<td style="text-align: center;">C $\uparrow$</td>
<td style="text-align: center;">$\mathrm{T} \downarrow$</td>
<td style="text-align: center;">C $\uparrow$</td>
</tr>
<tr>
<td style="text-align: center;">Pre-trained</td>
<td style="text-align: center;">59.6</td>
<td style="text-align: center;">63.4</td>
<td style="text-align: center;">44.1</td>
<td style="text-align: center;">63.4</td>
<td style="text-align: center;">45.9</td>
<td style="text-align: center;">63.4</td>
<td style="text-align: center;">32.5</td>
<td style="text-align: center;">63.4</td>
<td style="text-align: center;">48.7</td>
<td style="text-align: center;">63.4</td>
<td style="text-align: center;">60.7</td>
<td style="text-align: center;">63.4</td>
<td style="text-align: center;">63.2</td>
<td style="text-align: center;">63.4</td>
<td style="text-align: center;">31.5</td>
<td style="text-align: center;">63.4</td>
</tr>
<tr>
<td style="text-align: center;">Fine-tuned</td>
<td style="text-align: center;">79.2</td>
<td style="text-align: center;">55.2</td>
<td style="text-align: center;">78.7</td>
<td style="text-align: center;">49.3</td>
<td style="text-align: center;">98.6</td>
<td style="text-align: center;">47.2</td>
<td style="text-align: center;">98.5</td>
<td style="text-align: center;">39.1</td>
<td style="text-align: center;">99.6</td>
<td style="text-align: center;">42.5</td>
<td style="text-align: center;">95.0</td>
<td style="text-align: center;">53.2</td>
<td style="text-align: center;">75.1</td>
<td style="text-align: center;">54.6</td>
<td style="text-align: center;">97.2</td>
<td style="text-align: center;">44.7</td>
</tr>
<tr>
<td style="text-align: center;">Neg. gradients</td>
<td style="text-align: center;">0.01</td>
<td style="text-align: center;">0.11</td>
<td style="text-align: center;">2.13</td>
<td style="text-align: center;">0.10</td>
<td style="text-align: center;">9.26</td>
<td style="text-align: center;">0.10</td>
<td style="text-align: center;">1.19</td>
<td style="text-align: center;">0.07</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">1.22</td>
<td style="text-align: center;">2.60</td>
<td style="text-align: center;">0.10</td>
<td style="text-align: center;">0.25</td>
<td style="text-align: center;">0.01</td>
<td style="text-align: center;">6.38</td>
<td style="text-align: center;">0.29</td>
</tr>
<tr>
<td style="text-align: center;">Rand. task vector</td>
<td style="text-align: center;">54.1</td>
<td style="text-align: center;">60.9</td>
<td style="text-align: center;">39.9</td>
<td style="text-align: center;">61.5</td>
<td style="text-align: center;">45.8</td>
<td style="text-align: center;">63.4</td>
<td style="text-align: center;">27.9</td>
<td style="text-align: center;">60.7</td>
<td style="text-align: center;">48.3</td>
<td style="text-align: center;">63.4</td>
<td style="text-align: center;">57.1</td>
<td style="text-align: center;">60.9</td>
<td style="text-align: center;">61.3</td>
<td style="text-align: center;">60.5</td>
<td style="text-align: center;">31.2</td>
<td style="text-align: center;">60.7</td>
</tr>
<tr>
<td style="text-align: center;">Neg. task vector</td>
<td style="text-align: center;">36.0</td>
<td style="text-align: center;">61.1</td>
<td style="text-align: center;">27.8</td>
<td style="text-align: center;">60.2</td>
<td style="text-align: center;">13.6</td>
<td style="text-align: center;">61.3</td>
<td style="text-align: center;">8.13</td>
<td style="text-align: center;">61.4</td>
<td style="text-align: center;">16.7</td>
<td style="text-align: center;">60.7</td>
<td style="text-align: center;">31.7</td>
<td style="text-align: center;">61.0</td>
<td style="text-align: center;">50.7</td>
<td style="text-align: center;">60.5</td>
<td style="text-align: center;">7.65</td>
<td style="text-align: center;">61.0</td>
</tr>
</tbody>
</table>
<p><img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>Figure 9: Comparison between task vectors and random vectors for forgetting image classification tasks.</p>
<p>Table 8: The effect of semantic overlap with the control task in forgetting experiments on image classification tasks. Results are shown for a CLIP ViT-L/14 model, reporting accuracy both on the target task and control task (Ctrl, ImageNet).</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Method</th>
<th style="text-align: center;">Without filtering</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">With filtering</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">Cars $(\downarrow)$</td>
<td style="text-align: center;">Ctrl $(\uparrow)$</td>
<td style="text-align: center;">SUN397 $(\downarrow)$</td>
<td style="text-align: center;">Ctrl $(\uparrow)$</td>
<td style="text-align: center;">Cars $(\downarrow)$</td>
<td style="text-align: center;">Ctrl $(\uparrow)$</td>
<td style="text-align: center;">SUN397 $(\downarrow)$</td>
<td style="text-align: center;">Ctrl $(\uparrow)$</td>
</tr>
<tr>
<td style="text-align: left;">Pre-trained</td>
<td style="text-align: center;">77.8</td>
<td style="text-align: center;">75.5</td>
<td style="text-align: center;">68.3</td>
<td style="text-align: center;">75.5</td>
<td style="text-align: center;">77.8</td>
<td style="text-align: center;">75.5</td>
<td style="text-align: center;">68.3</td>
<td style="text-align: center;">76.1</td>
</tr>
<tr>
<td style="text-align: left;">Fine-tuned</td>
<td style="text-align: center;">92.8</td>
<td style="text-align: center;">73.1</td>
<td style="text-align: center;">82.4</td>
<td style="text-align: center;">72.7</td>
<td style="text-align: center;">92.8</td>
<td style="text-align: center;">73.3</td>
<td style="text-align: center;">82.4</td>
<td style="text-align: center;">73.1</td>
</tr>
<tr>
<td style="text-align: left;">Neg. task vector</td>
<td style="text-align: center;">32.0</td>
<td style="text-align: center;">72.4</td>
<td style="text-align: center;">50.8</td>
<td style="text-align: center;">72.6</td>
<td style="text-align: center;">32.0</td>
<td style="text-align: center;">72.5</td>
<td style="text-align: center;">48.1</td>
<td style="text-align: center;">72.4</td>
</tr>
</tbody>
</table>
<h1>B. 5 THE EFFECT OF CLASS OVERLAP</h1>
<p>In Tables 5, 7, 6, we observe that the tasks where forgetting via task vectors is least effective are tasks where the distribution of images is closer to ImageNet, SUN397 [101], a scene understanding dataset with classes such as "church" and "tower", and Stanford Cars [47], a dataset with with many car categories such as "2012 Tesla Model S" or "2012 BMW M3 coupe". One reasonable hypothesis is that forgetting is less effective for those tasks due to the overlap with the images from the control tasks.</p>
<p>To better understand this effect, we measure accuracy on a subset of classes from ImageNet, such that the overlap is minimized. Concretely, we exclude nodes from the WordNet hierarchy from which the ImageNet classes are based. ${ }^{6}$ For the Cars dataset, we exclude the all subnodes under the node "wheeled vehicle" (e.g., "minivan", "jeep", "limousine"). For SUN397, we exclude all subnodes under the nodes "structure" and "geological formation". As shown in Table 8, we do not observe large differences after filtering.</p>
<h2>B. 6 INTERPOLATING WITH A MODEL FINE-TUNED WITH GRADIENT ASCENT</h2>
<p>One baseline explored in the experiments is fine-tuning with gradient ascent, as explored in Golatkar et al. [34]; Tarun et al. [90]. Our results show that this strategy is effective at reducing the accuracy on treatment tasks, but also substantially deteriorates accuracy on the control task, which is undesirable.</p>
<p>We further examine whether interpolations between the pre-trained model and the model fine-tuned with gradient ascent help with forgetting. Our results, shown in Figure 10, indicate that interpolations greatly mitigate the low accuracy on the control task of the fine-tuned model, leading to even better accuracy trade-offs than the solutions obtained by extrapolation with standard fine-tuning.</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{6} \mathrm{~A}$ visualization is available at https://observablehq.com/@mbostock/ imagenet-hierarchy&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
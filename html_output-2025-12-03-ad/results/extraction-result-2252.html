<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2252 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2252</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2252</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-62.html">extraction-schema-62</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models, representation learning methods, or predictive models that compare different levels of abstraction, discuss task-relevant vs task-irrelevant features, or evaluate transfer learning and multi-task performance.</div>
                <p><strong>Paper ID:</strong> paper-278740500</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2505.11528v6.pdf" target="_blank">LaDi-WM: A Latent Diffusion-based World Model for Predictive Manipulation</a></p>
                <p><strong>Paper Abstract:</strong> Predictive manipulation has recently gained considerable attention in the Embodied AI community due to its potential to improve robot policy performance by leveraging predicted states. However, generating accurate future visual states of robot-object interactions from world models remains a well-known challenge, particularly in achieving high-quality pixel-level representations. To this end, we propose LaDi-WM, a world model that predicts the latent space of future states using diffusion modeling. Specifically, LaDi-WM leverages the well-established latent space aligned with pre-trained Visual Foundation Models (VFMs), which comprises both geometric features (DINO-based) and semantic features (CLIP-based). We find that predicting the evolution of the latent space is easier to learn and more generalizable than directly predicting pixel-level images. Building on LaDi-WM, we design a diffusion policy that iteratively refines output actions by incorporating forecasted states, thereby generating more consistent and accurate results. Extensive experiments on both synthetic and real-world benchmarks demonstrate that LaDi-WM significantly enhances policy performance by 27.9\% on the LIBERO-LONG benchmark and 20\% on the real-world scenario. Furthermore, our world model and policies achieve impressive generalizability in real-world experiments.</p>
                <p><strong>Cost:</strong> 0.026</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2252.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2252.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models, representation learning methods, or predictive models that compare different levels of abstraction, discuss task-relevant vs task-irrelevant features, or evaluate transfer learning and multi-task performance.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LaDi-WM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Latent Diffusion-based World Model (LaDi-WM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A world model that predicts future states in a latent space aligned to pretrained Visual Foundation Models by using an interactive latent diffusion process over geometric (DINO) and semantic (SigLip) codes; used to provide imagined future latent states to a diffusion policy with iterative refinement for robotic manipulation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LaDi-WM</td>
                        </tr>
                        <tr>
                            <td><strong>abstraction_level</strong></td>
                            <td>latent-level (VFM-aligned) combining geometric (DINO) and semantic (SigLip/CLIP) representations</td>
                        </tr>
                        <tr>
                            <td><strong>feature_selection_mechanism</strong></td>
                            <td>learned cross-attention and decomposition networks that estimate clean latent components for geometry and semantics (interactive diffusion); attention weights implicitly highlight task-relevant latent features (no explicit masking or reward-weighted reconstruction)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>robotic manipulation (LIBERO-LONG, CALVIN D-D, real-world 7-DOF setup)</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_presence</strong></td>
                            <td>benchmarks include scene and object appearance variations and complex lighting (real-world experiments note complex lighting changes); no explicit synthetic distractor objects but natural visual variability across scenes</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>LIBERO-LONG Avg.SR 68.7% (10-demo setting, Table 1); full-data LIBERO Avg.SR 90.7% (Table 6); improvement over vanilla behavior cloning +27.9% (Avg.SR) in simulation (Table 4); real-world Avg.SR improvement over BC +20% (Tab.5); CALVIN D-D Avg.Len. 4.13 and Avg.SR 92.7%/Avg.Len 3.63 for full eval (Table 2 & 7 depending on split)</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_details</strong></td>
                            <td>World model: 8-layer transformer, hidden dim 384; trained with batch size 4 on an NVIDIA 4090 ~3 days. Policy: transformer encoder (6 layers) + decoder (4 layers), hidden dim 256; trained with batch size 24 on NVIDIA 4090 ~6 hours. Image input 128x128, patch size 14x14. No FLOPs or parameter counts reported.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td>Outperforms prior SOTA (Seer) by +15.1% Avg.SR in limited-data policy learning scenario (10 demos, reported in text); outperforms pixel-space diffusion world model by +14.7% Avg.SR (Ours 71.7% vs Pixel 54.0% in ablation, Table 10); full-data advantage over Seer +3.0% (90.7% vs Seer 87.7%, Table 6).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_learning_results</strong></td>
                            <td>Cross-scene transfer: world model trained on LIBERO directly transferred to policy learning on CALVIN achieved Avg.Len. = 3.05 (Table 7, 'Ours (cross-scene)'), showing cross-scene generalization. Real-world transfer: policy trained in sim then fine-tuned with 10 real demonstrations per task led to +20% Avg.SR over BC (Tab.5). World model is trained task-agnostically (on LIBERO-90) and used across tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>multi_task_performance</strong></td>
                            <td>Evaluated across multiple tasks (LIBERO-LONG: 10 long-horizon tasks reported in Table 1 with per-task Avg.SR; CALVIN D-D: multi-task sequences and Avg.Len reported). Uses shared, task-agnostic latent world model (trained on many short tasks) and a task-conditioned diffusion policy; shows consistent gains across tasks (e.g., Avg.SR 68.7% vs Seer 53.6% in same evaluation).</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Limited training sample scale constrains performance (authors list limited dataset scale as limitation). Longer-horizon prediction degrades due to compounding error; world model has difficulty producing accurate extended predictions. Iterative refinement saturates after ~2 iterations (no further gains beyond 2).</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td>Multiple ablations: (1) Removing diffusion or replacing interactive diffusion reduces performance (interactive diffusion > concatenation by +1.8%, Tab.3). (2) Adding semantic latents (SigLip) yields +3.4% Avg.SR improvement in ablation (Tab.3). (3) Pixel-space diffusion vs latent diffusion: pixel diffusion Avg.SR 54.0% vs LaDi-WM 71.7% (drop 14.7%, Tab.10). (4) Iterative refinement: 0->1->2 iterations yields substantial gains; 2-iter imagination gives +27.9% over BC (Table 4). (5) Imagined frames: performance peaks at 6 imagined frames (Table 11). (6) Denoising steps: convergent performance can be achieved with as few as 2 denoising steps (Table 11).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Noted strong gains with limited action-labeled training data: with only 10 demonstrations per task, LaDi-WM + diffusion policy achieves Avg.SR 68.7% vs vanilla BC 40.8% (improvement +27.9%, Table 9 & Table 1), indicating high sample efficiency relative to behavior cloning baselines in these experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_analysis</strong></td>
                            <td>Visualized attention weights show interactive diffusion aligns geometry and semantics to same objects (Fig.4). Cross-scene transfer experiments (LIBERO->CALVIN) produced Avg.Len 3.05 (Table 7), and real-world finetuning demonstrates good sim-to-real generalizability (+20% over BC). Authors report world-model MSE on test decreases with more training data (no absolute MSE numbers provided).</td>
                        </tr>
                        <tr>
                            <td><strong>reconstruction_quality</strong></td>
                            <td>World model predicts latent codes (no direct pixel reconstruction metric). Training objective uses MSE on latent predictions and noise terms; authors report test MSE decreases as world model training data increases but do not provide numeric MSE/PSNR/SSIM values for image reconstruction.</td>
                        </tr>
                        <tr>
                            <td><strong>task_relevance_analysis</strong></td>
                            <td>Authors analyze which latent features drive dynamics via transformer attention visualization: interactive diffusion causes latent connections to align to identical objects; adding semantic latents improves tasks that need semantics (Tasks 1,5,9). A 'copy' ablation (using current observation as future imagination) fails to improve BC, indicating naive or task-irrelevant future features are unhelpful (Table 9). Iterative refinement reduces entropy of action distribution (PCA + KDE analysis, Fig.7) implying concentration on task-relevant actions.</td>
                        </tr>
                        <tr>
                            <td><strong>dynamic_abstraction</strong></td>
                            <td>No explicit runtime switching of abstraction level, but the model fuses two abstraction levels (geometric DINO and semantic SigLip) via interactive diffusion; the cross-attention mechanism dynamically weights interactions between those latents during denoising.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_vs_exploitation</strong></td>
                            <td>Paper analyzes iterative refinement at inference (closed-loop refinement) which reduces stochasticity (entropy) of diffusion policy outputs and improves exploitation performance; does not explicitly evaluate abstraction choices during explicit exploration phases (RL exploration) because training focuses on imitation learning.</td>
                        </tr>
                        <tr>
                            <td><strong>information_theoretic_analysis</strong></td>
                            <td>No information-theoretic metrics (mutual information, compression bounds, rate-distortion) are reported.</td>
                        </tr>
                        <tr>
                            <td><strong>pixel_fidelity_benefits</strong></td>
                            <td>Paper argues pixel-level prediction is less generalizable and more computationally expensive; empirical ablation shows pixel diffusion world model underperforms latent diffusion by 14.7% Avg.SR (Table 10). The authors do not identify scenarios in their experiments where pixel-level fidelity provides practical benefits over the VFM-aligned latent abstractions.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2252.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2252.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models, representation learning methods, or predictive models that compare different levels of abstraction, discuss task-relevant vs task-irrelevant features, or evaluate transfer learning and multi-task performance.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Interactive Diffusion</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Interactive Latent Diffusion (geometry-semantic interaction)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A diffusion-process design that decomposes noisy DINO and SigLip latent codes into clean components and performs cross-conditioned denoising with cross-attention so geometric and semantic latents can interact during reverse diffusion.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Interactive latent diffusion (component decomposition + cross-attention)</td>
                        </tr>
                        <tr>
                            <td><strong>abstraction_level</strong></td>
                            <td>multi-level latent (explicit geometric vs semantic latent channels)</td>
                        </tr>
                        <tr>
                            <td><strong>feature_selection_mechanism</strong></td>
                            <td>decomposition networks estimate clean latent components (C_D, C_S) and cross-attention in denoising networks lets each latent type condition on the other to emphasize interacting features</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>robotic manipulation (used inside LaDi-WM)</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_presence</strong></td>
                            <td>operates under scene variations and lighting changes in benchmarks; no synthetic distractors reported</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Interactive diffusion outperforms concatenation baseline by +1.8% Avg.SR in ablation (Tab.3); visual attention alignment shown in Fig.4.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_details</strong></td>
                            <td>Implemented with diffusion transformer architecture; decomposition network is an 8-layer transformer (hidden dim 384); no FLOPs/params reported separately.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td>Compared with a concatenation-based latent approach and with architectures without diffusion; interactive diffusion gave better alignment and higher Avg.SR (+1.8% over concatenation, Tab.3) and improved over transformer-only world model by +7.8% (Tab.3 first row).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_learning_results</strong></td>
                            <td>Used within LaDi-WM which shows cross-scene transfer (LIBERO->CALVIN) and sim-to-real transfer; no separate cross-model transfer numbers reported specifically for interactive diffusion component alone.</td>
                        </tr>
                        <tr>
                            <td><strong>multi_task_performance</strong></td>
                            <td>Contributes to multi-task generalization of LaDi-WM; ablations show interactive diffusion helps across tasks, particularly aligning semantics and geometry for varied objects.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>No explicit failure modes isolated beyond overall model limitations (data scale, long-horizon compounding errors).</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td>Ablation comparing interactive diffusion vs concatenation (+1.8% Avg.SR) and transformer-only variants (+7.8% improvement reported for full design, Tab.3); attention visualization used to show improved alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Implicitly improves convergence speed and generalizability in small-data regimes when combined into LaDi-WM; no standalone sample counts reported.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_analysis</strong></td>
                            <td>Attention visualizations (Fig.4) and task performance ablations indicate better cross-scene and cross-task alignment when interactive diffusion is used.</td>
                        </tr>
                        <tr>
                            <td><strong>reconstruction_quality</strong></td>
                            <td>Operates on latent predictions; training uses MSE on latent reconstructions and noise variables; no standard image-reconstruction metrics provided.</td>
                        </tr>
                        <tr>
                            <td><strong>task_relevance_analysis</strong></td>
                            <td>Interactive diffusion yields attention maps that align both latents to same object regions indicating improved capture of task-relevant object features; ablations show semantic inclusion aided certain tasks (Tasks 1,5,9).</td>
                        </tr>
                        <tr>
                            <td><strong>dynamic_abstraction</strong></td>
                            <td>Enables dynamic interaction between two latent abstraction channels during denoising but does not perform explicit hierarchical switching.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_vs_exploitation</strong></td>
                            <td>Not explicitly analyzed for exploration/exploitation tradeoff, though interactive diffusion improves the imagined-state guidance used for action refinement (exploitation).</td>
                        </tr>
                        <tr>
                            <td><strong>information_theoretic_analysis</strong></td>
                            <td>None reported.</td>
                        </tr>
                        <tr>
                            <td><strong>pixel_fidelity_benefits</strong></td>
                            <td>Not applicable; intended to replace pixel-level diffusion with latent interaction to improve generalizability.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2252.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2252.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models, representation learning methods, or predictive models that compare different levels of abstraction, discuss task-relevant vs task-irrelevant features, or evaluate transfer learning and multi-task performance.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Pixel diffusion baseline</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Pixel-space Diffusion World Model (pixel diffusion)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A diffusion-based world model that predicts future in image (pixel) space used as an ablation baseline to compare against latent diffusion; same network architecture but without VFM latent extraction.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Pixel diffusion world model</td>
                        </tr>
                        <tr>
                            <td><strong>abstraction_level</strong></td>
                            <td>pixel-level reconstruction</td>
                        </tr>
                        <tr>
                            <td><strong>feature_selection_mechanism</strong></td>
                            <td>no explicit task-relevance mechanism; reconstructs pixels directly (implicit selection via learning objective)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>robotic manipulation (LIBERO-LONG benchmark ablation)</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_presence</strong></td>
                            <td>subject to same scene variations and lighting as latent experiments</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Avg.SR 54.0% in ablation (Table 10) compared to LaDi-WM 71.7% (drop of 14.7% Avg.SR).</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_details</strong></td>
                            <td>Authors note higher computational expense and overhead for pixel-level diffusion (image reconstruction + diffusion) which limits applicability, but no FLOPs/params/time numbers provided for pixel model specifically.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td>Underperforms LaDi-WM by 14.7% Avg.SR (Table 10). Authors argue latent diffusion (VFM-aligned) is more compact and generalizable.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_learning_results</strong></td>
                            <td>No specific transfer experiments reported for pixel diffusion; general claim that pixel models generalize worse is supported by lower Avg.SR.</td>
                        </tr>
                        <tr>
                            <td><strong>multi_task_performance</strong></td>
                            <td>Reported single multi-task aggregate (Avg.SR) in ablation; performs worse across tasks compared to latent model.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Lower generalization to unseen tasks/scenes and higher compute overhead; less robust for manipulation where geometry/semantics matter.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td>Direct ablation vs LaDi-WM (Table 10) shows substantial performance degradation when moving from latent to pixel diffusion.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Not explicitly reported; implied less sample-efficient due to poorer generalization in limited-data regime.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_analysis</strong></td>
                            <td>Authors conclude pixel diffusion is less generalizable from observed declines in Avg.SR in same evaluation settings.</td>
                        </tr>
                        <tr>
                            <td><strong>reconstruction_quality</strong></td>
                            <td>Pixel reconstructions are by construction exact targets of diffusion but no PSNR/SSIM/MSE to pixels are reported; metric comparisons are given at policy level (Avg.SR).</td>
                        </tr>
                        <tr>
                            <td><strong>task_relevance_analysis</strong></td>
                            <td>Paper argues pixel reconstructions include many task-irrelevant details and do not capture geometric/semantic latents needed for manipulation; empirical results support this claim via lower task success.</td>
                        </tr>
                        <tr>
                            <td><strong>dynamic_abstraction</strong></td>
                            <td>Fixed pixel-level abstraction (no dynamic adjustment).</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_vs_exploitation</strong></td>
                            <td>Not discussed.</td>
                        </tr>
                        <tr>
                            <td><strong>information_theoretic_analysis</strong></td>
                            <td>None reported.</td>
                        </tr>
                        <tr>
                            <td><strong>pixel_fidelity_benefits</strong></td>
                            <td>Paper does not present empirical scenarios where pixel fidelity yields better task performance; instead it reports worse manipulation success and higher compute cost.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2252.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2252.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models, representation learning methods, or predictive models that compare different levels of abstraction, discuss task-relevant vs task-irrelevant features, or evaluate transfer learning and multi-task performance.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DINO latents</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DINO-derived geometric latent representation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Pretrained self-supervised visual features (DINO) used to extract geometric latent codes that capture object geometry and structure; used by LaDi-WM as the geometric channel of latent state.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DINO (geometric latent encoder)</td>
                        </tr>
                        <tr>
                            <td><strong>abstraction_level</strong></td>
                            <td>geometric / mid-level visual features (object geometry, structure)</td>
                        </tr>
                        <tr>
                            <td><strong>feature_selection_mechanism</strong></td>
                            <td>pretrained self-supervised feature extractor; no task-specific masking—task relevance emerges via attention in world model and policy</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>robotic manipulation (as part of LaDi-WM latent state)</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_presence</strong></td>
                            <td>tested under scene appearance variations and lighting changes; DINO features are intended to be robust to such variations</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Not reported in isolation; ablation shows adding semantic latents on top of geometric improves Avg.SR by +3.4% (Tab.3), implying geometric latents are necessary but semantics add value for some tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_details</strong></td>
                            <td>Pretrained encoder used; no fine-tuning cost reported for DINO in this work.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td>DINO-based prior work (DINO-WM) is cited as effective for geometry; LaDi-WM extends this by adding semantic latents and latent diffusion.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_learning_results</strong></td>
                            <td>DINO features enable task-agnostic world-model training and cross-task generalization as part of LaDi-WM; no isolated DINO-transfer numbers provided.</td>
                        </tr>
                        <tr>
                            <td><strong>multi_task_performance</strong></td>
                            <td>Geometric features are helpful across manipulation tasks; inclusion in LaDi-WM supports multiple task types.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Alone, geometry-only features may miss semantic distinctions needed for tasks involving semantic object identity (authors show semantic latents help Tasks 1,5,9).</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td>Ablation removing semantic features indicates geometric-only representation performs worse by ~3.4% Avg.SR for the full model variant (Tab.3 second row).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Pretrained feature usage improves sample efficiency implicitly; exact numbers not separated by representation.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_analysis</strong></td>
                            <td>DINO features are leveraged to improve cross-scene and sim-to-real generalization as part of the combined latent world model.</td>
                        </tr>
                        <tr>
                            <td><strong>reconstruction_quality</strong></td>
                            <td>Not applicable (no pixel reconstruction by DINO in this setup).</td>
                        </tr>
                        <tr>
                            <td><strong>task_relevance_analysis</strong></td>
                            <td>Attention visualizations show geometric latents attend to object regions; when combined with semantics, attention aligns to task-relevant objects.</td>
                        </tr>
                        <tr>
                            <td><strong>dynamic_abstraction</strong></td>
                            <td>Serves as one static abstraction channel (geometry) that interacts with semantic channel during denoising.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_vs_exploitation</strong></td>
                            <td>Not discussed.</td>
                        </tr>
                        <tr>
                            <td><strong>information_theoretic_analysis</strong></td>
                            <td>None reported.</td>
                        </tr>
                        <tr>
                            <td><strong>pixel_fidelity_benefits</strong></td>
                            <td>Not applicable.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2252.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2252.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models, representation learning methods, or predictive models that compare different levels of abstraction, discuss task-relevant vs task-irrelevant features, or evaluate transfer learning and multi-task performance.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SigLip</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SigLip (semantic VFM latent representation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A VFM-derived semantic latent encoder used to extract semantic features (language/semantic-aligned) for predicting task-relevant semantic changes; integrated into LaDi-WM as the semantic channel.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>SigLip (semantic latent encoder)</td>
                        </tr>
                        <tr>
                            <td><strong>abstraction_level</strong></td>
                            <td>semantic / language-image aligned high-level features</td>
                        </tr>
                        <tr>
                            <td><strong>feature_selection_mechanism</strong></td>
                            <td>pretrained semantic encoder (SigLip) provides semantic latents; task relevance handled downstream by world model attention; no explicit task masking</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>robotic manipulation (semantic distinctions among objects and instructions)</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_presence</strong></td>
                            <td>semantic encoder helps disambiguate objects with similar geometry but different semantics across variable scenes</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Adding semantic latents to geometry-only model increases Avg.SR by +3.4% in ablation (Tab.3).</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_details</strong></td>
                            <td>Pretrained encoder used; fine-tuning or encoder cost not separately reported.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td>Compared implicitly to geometric-only models; semantic inclusion improves performance on tasks requiring semantic discrimination (Tasks 1,5,9).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_learning_results</strong></td>
                            <td>Semantic latents help task-agnostic world model generalize across tasks with semantic variations; no isolated numeric transfer metrics reported for SigLip alone.</td>
                        </tr>
                        <tr>
                            <td><strong>multi_task_performance</strong></td>
                            <td>Improves average performance across tasks with semantic requirements; part of shared representation used across tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Not sufficient alone; needs to be combined with geometric latents for detailed manipulation performance.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td>Semantic inclusion ablation shows +3.4% Avg.SR gain (Tab.3).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Pretrained semantics improve data efficiency when combined in LaDi-WM; no separate sample numbers.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_analysis</strong></td>
                            <td>Contributes to better handling of tasks requiring semantic distinctions; attention alignment shows semantics align to same objects as geometry after interactive diffusion.</td>
                        </tr>
                        <tr>
                            <td><strong>reconstruction_quality</strong></td>
                            <td>Not applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>task_relevance_analysis</strong></td>
                            <td>Authors report which tasks benefit from semantic signals (Tasks 1,5,9) — tasks with distinct semantic object identity.</td>
                        </tr>
                        <tr>
                            <td><strong>dynamic_abstraction</strong></td>
                            <td>Acts as a static semantic abstraction channel interacting with geometric channel during diffusion.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_vs_exploitation</strong></td>
                            <td>Not discussed.</td>
                        </tr>
                        <tr>
                            <td><strong>information_theoretic_analysis</strong></td>
                            <td>None reported.</td>
                        </tr>
                        <tr>
                            <td><strong>pixel_fidelity_benefits</strong></td>
                            <td>Not applicable.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2252.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e2252.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models, representation learning methods, or predictive models that compare different levels of abstraction, discuss task-relevant vs task-irrelevant features, or evaluate transfer learning and multi-task performance.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Seer</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Seer (previous state-of-the-art baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An inverse-dynamics approach that jointly predicts actions and subsequent frame images and was prior SOTA on LIBERO and CALVIN; used here as a main baseline comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Seer (inverse dynamics + image prediction)</td>
                        </tr>
                        <tr>
                            <td><strong>abstraction_level</strong></td>
                            <td>implicit image-frame-conditioned inverse dynamics (predicts images and actions)</td>
                        </tr>
                        <tr>
                            <td><strong>feature_selection_mechanism</strong></td>
                            <td>joint inverse dynamics and image prediction objective (no explicit geometric/semantic disentangling reported)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>robotic manipulation (LIBERO-LONG, CALVIN D-D comparisons)</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_presence</strong></td>
                            <td>evaluated on same benchmarks with scene variation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported Avg.SR 53.6% (Table 1, 10-demo setting) and full-data baseline Avg.SR 87.7% (Table 6) depending on setting.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_details</strong></td>
                            <td>Not detailed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td>LaDi-WM exceeds Seer by +15.1% Avg.SR in limited-data setting (10 demo, text) and +3.0% in full-data setting (90.7% vs 87.7%, Table 6).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_learning_results</strong></td>
                            <td>Seer was evaluated on the same benchmarks in original work; LaDi-WM demonstrates better generalization and usable imagination guidance for policy refinement in current experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>multi_task_performance</strong></td>
                            <td>Seer performance reported per-task in Table 1; LaDi-WM improves over Seer across many tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Authors argue Seer's implicit dynamics cannot directly provide imagined future states for policy iterative refinement; performance limitations in low-data regimes relative to LaDi-WM.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td>Not applicable here (Seer is baseline).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Seer performs worse than LaDi-WM in limited-data setting (10 demos) per reported numbers.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_analysis</strong></td>
                            <td>Seer lacks explicit task-agnostic latent alignment (geometry+semantics) that LaDi-WM leverages for cross-task generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>reconstruction_quality</strong></td>
                            <td>Seer predicts images as part of objective, but pixel reconstruction metrics not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>task_relevance_analysis</strong></td>
                            <td>Not detailed in this paper beyond performance comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>dynamic_abstraction</strong></td>
                            <td>No multi-level latent disentanglement reported; predicts images and actions jointly.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_vs_exploitation</strong></td>
                            <td>Not discussed here.</td>
                        </tr>
                        <tr>
                            <td><strong>information_theoretic_analysis</strong></td>
                            <td>None reported.</td>
                        </tr>
                        <tr>
                            <td><strong>pixel_fidelity_benefits</strong></td>
                            <td>Seer uses image prediction, but in authors' comparison LaDi-WM's latent predictions provide better guidance for policy learning.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2252.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e2252.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models, representation learning methods, or predictive models that compare different levels of abstraction, discuss task-relevant vs task-irrelevant features, or evaluate transfer learning and multi-task performance.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DreamerV3</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DreamerV3</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior latent-world-model method that learns latent dynamics for control; used as a baseline in evaluations and contrasted as optimizing representations for image reconstruction/reward rather than VFM-aligned geometry/semantics.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DreamerV3</td>
                        </tr>
                        <tr>
                            <td><strong>abstraction_level</strong></td>
                            <td>latent-space dynamics optimized for reconstruction/prediction (task reward oriented)</td>
                        </tr>
                        <tr>
                            <td><strong>feature_selection_mechanism</strong></td>
                            <td>learned latent representations via recurrent components and reconstruction objectives; no explicit semantic/geometric disentanglement</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>continuous control / robotic manipulation baselines (LIBERO, CALVIN comparisons)</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_presence</strong></td>
                            <td>subject to same benchmarks' visual variability</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported Avg.SR 33.5% on LIBERO-LONG (Table 1 in this paper's baselines listing) in the compared setting.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_details</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td>LaDi-WM significantly outperforms DreamerV3 in these manipulation tasks (e.g., DreamerV3 33.5% vs LaDi-WM 68.7% in Table 1 10-demo comparisons).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_learning_results</strong></td>
                            <td>Not evaluated here; authors note Dreamer-series methods require dense rewards and have limitations for manipulation tasks in this setup.</td>
                        </tr>
                        <tr>
                            <td><strong>multi_task_performance</strong></td>
                            <td>Baseline multi-task numbers reported in tables; DreamerV3 underperforms LaDi-WM across tasks reported.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Authors note representations optimized for reconstruction/reward can neglect geometric/semantic features critical for manipulation.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td>Not performed here (DreamerV3 used as baseline).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Per baselines, less sample-efficient for manipulation tasks in their experiments compared to LaDi-WM.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_analysis</strong></td>
                            <td>Not emphasized here.</td>
                        </tr>
                        <tr>
                            <td><strong>reconstruction_quality</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>task_relevance_analysis</strong></td>
                            <td>Authors critique Dreamer-style latent objectives for not explicitly capturing semantics/geometry required for manipulation.</td>
                        </tr>
                        <tr>
                            <td><strong>dynamic_abstraction</strong></td>
                            <td>No explicit multi-level abstraction in DreamerV3 beyond learned latent hierarchy.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_vs_exploitation</strong></td>
                            <td>Not discussed here.</td>
                        </tr>
                        <tr>
                            <td><strong>information_theoretic_analysis</strong></td>
                            <td>None reported.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2252.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e2252.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models, representation learning methods, or predictive models that compare different levels of abstraction, discuss task-relevant vs task-irrelevant features, or evaluate transfer learning and multi-task performance.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TDMPC2</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>TD-MPC2</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A scalable, robust world-model-and-planning method for continuous control used here as a baseline; noted to operate in latent space but optimize representations for reconstruction/reward.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>TDMPC2</td>
                        </tr>
                        <tr>
                            <td><strong>abstraction_level</strong></td>
                            <td>latent-space planning/control (reconstruction/reward-optimized representations)</td>
                        </tr>
                        <tr>
                            <td><strong>feature_selection_mechanism</strong></td>
                            <td>learned latent via predictive/reconstruction objectives and used for model predictive control; no explicit semantic/geometric disentangling</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>continuous control / robotic manipulation baseline</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_presence</strong></td>
                            <td>same benchmark visual variability as other baselines</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported Avg.SR 37.0% on LIBERO-LONG baseline (Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_details</strong></td>
                            <td>Not provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td>Underperforms LaDi-WM substantially in these manipulation evaluations (LaDi-WM 68.7% vs TDMPC2 37.0% in Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_learning_results</strong></td>
                            <td>Not directly reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>multi_task_performance</strong></td>
                            <td>Included as baseline multi-task comparisons; lower Avg.SR than LaDi-WM.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Requires dense reward shaping for manipulation in authors' setup; authors point out this limits application to complex manipulation tasks without dense rewards.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td>Not performed here.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Requires reward design and RL training; authors initialized policies with BC for baselines due to RL difficulty in manipulation.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_analysis</strong></td>
                            <td>Not emphasized.</td>
                        </tr>
                        <tr>
                            <td><strong>reconstruction_quality</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>task_relevance_analysis</strong></td>
                            <td>Authors claim these latent representations neglect critical geometric/semantic information needed for manipulation.</td>
                        </tr>
                        <tr>
                            <td><strong>dynamic_abstraction</strong></td>
                            <td>Not described.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_vs_exploitation</strong></td>
                            <td>Not discussed.</td>
                        </tr>
                        <tr>
                            <td><strong>information_theoretic_analysis</strong></td>
                            <td>None reported.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2252.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e2252.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models, representation learning methods, or predictive models that compare different levels of abstraction, discuss task-relevant vs task-irrelevant features, or evaluate transfer learning and multi-task performance.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ATM baseline</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ATM (instruction-conditioned trajectory prediction)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An instruction-conditioned prediction model that forecasts point trajectories and feeds them to a policy; cited as a baseline that is task-specific and point-based and thus limited in generalization for detailed manipulation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ATM</td>
                        </tr>
                        <tr>
                            <td><strong>abstraction_level</strong></td>
                            <td>point-trajectory (sparse trajectory-level) predictions</td>
                        </tr>
                        <tr>
                            <td><strong>feature_selection_mechanism</strong></td>
                            <td>task/instruction-conditioned trajectory predictions (task-specific inductive bias); no dense visual latent capture</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>language-conditioned manipulation (benchmarks used for comparison)</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_presence</strong></td>
                            <td>benchmarks' scene variability applies; ATM's sparse points ignore detailed observation distractors</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported Avg.SR 44.0% on LIBERO-LONG baseline (Table 1); CALVIN Avg.Len.&Avg.SR also reported in Table 2/7 baseline rows.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_details</strong></td>
                            <td>Not given in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td>LaDi-WM outperforms ATM in multi-task benchmarks (LaDi-WM Avg.SR 68.7% vs ATM 44.0% in Table 1). Authors note ATM's task-specific grounding limits generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_learning_results</strong></td>
                            <td>Not highlighted; ATM's instruction conditioning is argued to restrict zero-shot generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>multi_task_performance</strong></td>
                            <td>ATM performs worse across LIBERO tasks relative to LaDi-WM's imagined-latent-guided diffusion policy.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Sparse point representation ignores detailed visual information and limits generalization to unseen tasks or object-specific semantic distinctions.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td>Not performed here (ATM used as baseline).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Baseline numbers provided, but direct sample-efficiency claims not made.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_analysis</strong></td>
                            <td>Authors argue ATM is less generalizable due to instruction/task-specific predictions.</td>
                        </tr>
                        <tr>
                            <td><strong>reconstruction_quality</strong></td>
                            <td>Not applicable (point trajectories rather than images).</td>
                        </tr>
                        <tr>
                            <td><strong>task_relevance_analysis</strong></td>
                            <td>ATM is task-conditioned; not designed to discover task-agnostic semantics/geometry.</td>
                        </tr>
                        <tr>
                            <td><strong>dynamic_abstraction</strong></td>
                            <td>Fixed sparse trajectory abstraction.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_vs_exploitation</strong></td>
                            <td>Not discussed.</td>
                        </tr>
                        <tr>
                            <td><strong>information_theoretic_analysis</strong></td>
                            <td>None reported.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2252.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e2252.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models, representation learning methods, or predictive models that compare different levels of abstraction, discuss task-relevant vs task-irrelevant features, or evaluate transfer learning and multi-task performance.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Copy model</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Copy-imagination baseline</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An ablation baseline that supplies the policy with a copy of the current observation as the 'future imagination' to test whether naive or task-irrelevant future inputs improve policy learning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Copy model (current observation used as future imagination)</td>
                        </tr>
                        <tr>
                            <td><strong>abstraction_level</strong></td>
                            <td>current-frame pixel/latent copy (no real prediction)</td>
                        </tr>
                        <tr>
                            <td><strong>feature_selection_mechanism</strong></td>
                            <td>no feature selection; uses current observation as surrogate future</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>robotic manipulation (LIBERO-LONG ablation)</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_presence</strong></td>
                            <td>same dataset conditions</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Avg.SR 40.2% (Table 9) which is slightly worse than vanilla BC 40.8% indicating naive future-copy is unhelpful.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_details</strong></td>
                            <td>Trivial overhead compared to learned world model; no compute numbers given.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td>Copy model does not improve over BC (vanilla behavior cloning) and is far below LaDi-WM performance (LaDi-WM Avg.SR 68.7%, Table 9), showing that task-irrelevant or non-predictive future inputs do not provide benefit.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_learning_results</strong></td>
                            <td>Not applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>multi_task_performance</strong></td>
                            <td>Reported aggregated Avg.SR in Table 9; performs poorly across tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Using current observation as future imagination supplies inaccurate/task-irrelevant guidance and can harm policy learning.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td>Direct ablation showing copy model ~40.2% vs BC 40.8% and vs LaDi-WM 68.7% (Table 9).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>No evidence of improved sample efficiency; effectively no benefit.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_analysis</strong></td>
                            <td>Shows that non-predictive or task-irrelevant imagined states do not generalize to provide useful guidance.</td>
                        </tr>
                        <tr>
                            <td><strong>reconstruction_quality</strong></td>
                            <td>N/A.</td>
                        </tr>
                        <tr>
                            <td><strong>task_relevance_analysis</strong></td>
                            <td>Demonstrates necessity of task-relevant imagined futures; naive copies of current obs are insufficient.</td>
                        </tr>
                        <tr>
                            <td><strong>dynamic_abstraction</strong></td>
                            <td>N/A.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_vs_exploitation</strong></td>
                            <td>N/A.</td>
                        </tr>
                        <tr>
                            <td><strong>information_theoretic_analysis</strong></td>
                            <td>None reported.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2252.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e2252.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models, representation learning methods, or predictive models that compare different levels of abstraction, discuss task-relevant vs task-irrelevant features, or evaluate transfer learning and multi-task performance.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Diffusion policy</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Imagination-guided Diffusion Policy (transformer diffusion policy)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A diffusion-based policy model (transformer encoder-decoder) that predicts action sequences and iteratively refines them using imagined future latent states from LaDi-WM; benefits from multimodal conditioning and iterative inference-time refinement.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Diffusion policy (transformer-based)</td>
                        </tr>
                        <tr>
                            <td><strong>abstraction_level</strong></td>
                            <td>action-space diffusion conditioned on latent-state imaginations (policy-level)</td>
                        </tr>
                        <tr>
                            <td><strong>feature_selection_mechanism</strong></td>
                            <td>policy fuses modalities (task instruction, diffusion timestep, end-effector states, historical and imagined latents) via transformer attention to select task-relevant inputs; iterative refinement reduces action distribution entropy</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>robotic manipulation (LIBERO-LONG, CALVIN D-D, real-world finetuning)</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_presence</strong></td>
                            <td>operates with same visual variability as world model experiments</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Iterative refinement: 1-iter Avg.SR 60.7%; 2-iter Avg.SR 68.7% (Table 4); denoising steps: convergent performance with as few as 2 denoising steps (Table 11).</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_details</strong></td>
                            <td>Policy transformer: encoder 6 layers, decoder 4 layers, hidden dim 256; trained ~6 hours on an NVIDIA 4090 (batch size 24). Inference uses iterative refinement (authors find 2 iterations sufficient).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td>Outperforms vanilla BC when paired with LaDi-WM (BC Avg.SR 40.8% vs LaDi-WM+diffusion policy 68.7% in 10-demo setting, Table 9 and Table 1). Iterative refinement yields substantial gains over no-imagination or single-pass predictions (Table 4).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_learning_results</strong></td>
                            <td>Policy trained in sim and finetuned with 10 real demos achieves strong real-world performance (combined with LaDi-WM, +20% over BC); no isolated policy-only transfer numbers provided.</td>
                        </tr>
                        <tr>
                            <td><strong>multi_task_performance</strong></td>
                            <td>Policy conditioned on task instruction and imagined futures performs across multiple tasks (LIBERO and CALVIN benchmarks) with improved Avg.SR and Avg.Len values.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Initial action samples can be suboptimal; benefit relies on sufficiently accurate imagined states—poor long-horizon imaginations may limit refinement benefits; diminishing returns beyond ~2 refinement iterations.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td>Denoising steps ablation: convergent performance with 2 steps (Table 11). Iterative refinement ablation demonstrates performance improves substantially up to 2 iterations (Table 4).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>When combined with LaDi-WM, achieves strong performance with only 10 demonstrations per task (policy finetuning), indicating sample-efficient refinement; specific sample counts listed (10 demos) tied to reported Avg.SR improvements.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_analysis</strong></td>
                            <td>Iterative refinement reduces action entropy and variance across repeated evaluations (Fig.7 and Table 8), contributing to more consistent generalization across tasks and trials.</td>
                        </tr>
                        <tr>
                            <td><strong>reconstruction_quality</strong></td>
                            <td>Not applicable (policy predicts actions, not reconstructions).</td>
                        </tr>
                        <tr>
                            <td><strong>task_relevance_analysis</strong></td>
                            <td>By conditioning on imagined futures and instruction, the policy implicitly learns to rely on task-relevant imagined latent signals; ablations (copy model) show naive inputs do not help.</td>
                        </tr>
                        <tr>
                            <td><strong>dynamic_abstraction</strong></td>
                            <td>Policy operates at action-level abstraction but adjusts predictions iteratively based on higher-level latent imaginations; does not explicitly change representation abstraction.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_vs_exploitation</strong></td>
                            <td>Iterative refinement operates at inference to reduce stochasticity and improve exploitation; training focuses on imitation learning rather than exploration-driven RL.</td>
                        </tr>
                        <tr>
                            <td><strong>information_theoretic_analysis</strong></td>
                            <td>None reported.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Dino-wm: World models on pre-trained visual features enable zero-shot planning <em>(Rating: 2)</em></li>
                <li>Dream to control: Learning behaviors by latent imagination <em>(Rating: 2)</em></li>
                <li>Diffusion world model: Future modeling beyond step-by-step rollout for offline reinforcement learning <em>(Rating: 2)</em></li>
                <li>TD-MPC2: scalable, robust world models for continuous control <em>(Rating: 2)</em></li>
                <li>Diffusion for world modeling: Visual details matter in atari <em>(Rating: 2)</em></li>
                <li>Predictive inverse dynamics models are scalable learners for robotic manipulation <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2252",
    "paper_id": "paper-278740500",
    "extraction_schema_id": "extraction-schema-62",
    "extracted_data": [
        {
            "name_short": "LaDi-WM",
            "name_full": "Latent Diffusion-based World Model (LaDi-WM)",
            "brief_description": "A world model that predicts future states in a latent space aligned to pretrained Visual Foundation Models by using an interactive latent diffusion process over geometric (DINO) and semantic (SigLip) codes; used to provide imagined future latent states to a diffusion policy with iterative refinement for robotic manipulation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LaDi-WM",
            "abstraction_level": "latent-level (VFM-aligned) combining geometric (DINO) and semantic (SigLip/CLIP) representations",
            "feature_selection_mechanism": "learned cross-attention and decomposition networks that estimate clean latent components for geometry and semantics (interactive diffusion); attention weights implicitly highlight task-relevant latent features (no explicit masking or reward-weighted reconstruction)",
            "task_domain": "robotic manipulation (LIBERO-LONG, CALVIN D-D, real-world 7-DOF setup)",
            "distractor_presence": "benchmarks include scene and object appearance variations and complex lighting (real-world experiments note complex lighting changes); no explicit synthetic distractor objects but natural visual variability across scenes",
            "performance_metrics": "LIBERO-LONG Avg.SR 68.7% (10-demo setting, Table 1); full-data LIBERO Avg.SR 90.7% (Table 6); improvement over vanilla behavior cloning +27.9% (Avg.SR) in simulation (Table 4); real-world Avg.SR improvement over BC +20% (Tab.5); CALVIN D-D Avg.Len. 4.13 and Avg.SR 92.7%/Avg.Len 3.63 for full eval (Table 2 & 7 depending on split)",
            "computational_cost_details": "World model: 8-layer transformer, hidden dim 384; trained with batch size 4 on an NVIDIA 4090 ~3 days. Policy: transformer encoder (6 layers) + decoder (4 layers), hidden dim 256; trained with batch size 24 on NVIDIA 4090 ~6 hours. Image input 128x128, patch size 14x14. No FLOPs or parameter counts reported.",
            "comparison_to_baselines": "Outperforms prior SOTA (Seer) by +15.1% Avg.SR in limited-data policy learning scenario (10 demos, reported in text); outperforms pixel-space diffusion world model by +14.7% Avg.SR (Ours 71.7% vs Pixel 54.0% in ablation, Table 10); full-data advantage over Seer +3.0% (90.7% vs Seer 87.7%, Table 6).",
            "transfer_learning_results": "Cross-scene transfer: world model trained on LIBERO directly transferred to policy learning on CALVIN achieved Avg.Len. = 3.05 (Table 7, 'Ours (cross-scene)'), showing cross-scene generalization. Real-world transfer: policy trained in sim then fine-tuned with 10 real demonstrations per task led to +20% Avg.SR over BC (Tab.5). World model is trained task-agnostically (on LIBERO-90) and used across tasks.",
            "multi_task_performance": "Evaluated across multiple tasks (LIBERO-LONG: 10 long-horizon tasks reported in Table 1 with per-task Avg.SR; CALVIN D-D: multi-task sequences and Avg.Len reported). Uses shared, task-agnostic latent world model (trained on many short tasks) and a task-conditioned diffusion policy; shows consistent gains across tasks (e.g., Avg.SR 68.7% vs Seer 53.6% in same evaluation).",
            "failure_modes": "Limited training sample scale constrains performance (authors list limited dataset scale as limitation). Longer-horizon prediction degrades due to compounding error; world model has difficulty producing accurate extended predictions. Iterative refinement saturates after ~2 iterations (no further gains beyond 2).",
            "ablation_studies": "Multiple ablations: (1) Removing diffusion or replacing interactive diffusion reduces performance (interactive diffusion &gt; concatenation by +1.8%, Tab.3). (2) Adding semantic latents (SigLip) yields +3.4% Avg.SR improvement in ablation (Tab.3). (3) Pixel-space diffusion vs latent diffusion: pixel diffusion Avg.SR 54.0% vs LaDi-WM 71.7% (drop 14.7%, Tab.10). (4) Iterative refinement: 0-&gt;1-&gt;2 iterations yields substantial gains; 2-iter imagination gives +27.9% over BC (Table 4). (5) Imagined frames: performance peaks at 6 imagined frames (Table 11). (6) Denoising steps: convergent performance can be achieved with as few as 2 denoising steps (Table 11).",
            "sample_efficiency": "Noted strong gains with limited action-labeled training data: with only 10 demonstrations per task, LaDi-WM + diffusion policy achieves Avg.SR 68.7% vs vanilla BC 40.8% (improvement +27.9%, Table 9 & Table 1), indicating high sample efficiency relative to behavior cloning baselines in these experiments.",
            "generalization_analysis": "Visualized attention weights show interactive diffusion aligns geometry and semantics to same objects (Fig.4). Cross-scene transfer experiments (LIBERO-&gt;CALVIN) produced Avg.Len 3.05 (Table 7), and real-world finetuning demonstrates good sim-to-real generalizability (+20% over BC). Authors report world-model MSE on test decreases with more training data (no absolute MSE numbers provided).",
            "reconstruction_quality": "World model predicts latent codes (no direct pixel reconstruction metric). Training objective uses MSE on latent predictions and noise terms; authors report test MSE decreases as world model training data increases but do not provide numeric MSE/PSNR/SSIM values for image reconstruction.",
            "task_relevance_analysis": "Authors analyze which latent features drive dynamics via transformer attention visualization: interactive diffusion causes latent connections to align to identical objects; adding semantic latents improves tasks that need semantics (Tasks 1,5,9). A 'copy' ablation (using current observation as future imagination) fails to improve BC, indicating naive or task-irrelevant future features are unhelpful (Table 9). Iterative refinement reduces entropy of action distribution (PCA + KDE analysis, Fig.7) implying concentration on task-relevant actions.",
            "dynamic_abstraction": "No explicit runtime switching of abstraction level, but the model fuses two abstraction levels (geometric DINO and semantic SigLip) via interactive diffusion; the cross-attention mechanism dynamically weights interactions between those latents during denoising.",
            "exploration_vs_exploitation": "Paper analyzes iterative refinement at inference (closed-loop refinement) which reduces stochasticity (entropy) of diffusion policy outputs and improves exploitation performance; does not explicitly evaluate abstraction choices during explicit exploration phases (RL exploration) because training focuses on imitation learning.",
            "information_theoretic_analysis": "No information-theoretic metrics (mutual information, compression bounds, rate-distortion) are reported.",
            "pixel_fidelity_benefits": "Paper argues pixel-level prediction is less generalizable and more computationally expensive; empirical ablation shows pixel diffusion world model underperforms latent diffusion by 14.7% Avg.SR (Table 10). The authors do not identify scenarios in their experiments where pixel-level fidelity provides practical benefits over the VFM-aligned latent abstractions.",
            "uuid": "e2252.0"
        },
        {
            "name_short": "Interactive Diffusion",
            "name_full": "Interactive Latent Diffusion (geometry-semantic interaction)",
            "brief_description": "A diffusion-process design that decomposes noisy DINO and SigLip latent codes into clean components and performs cross-conditioned denoising with cross-attention so geometric and semantic latents can interact during reverse diffusion.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Interactive latent diffusion (component decomposition + cross-attention)",
            "abstraction_level": "multi-level latent (explicit geometric vs semantic latent channels)",
            "feature_selection_mechanism": "decomposition networks estimate clean latent components (C_D, C_S) and cross-attention in denoising networks lets each latent type condition on the other to emphasize interacting features",
            "task_domain": "robotic manipulation (used inside LaDi-WM)",
            "distractor_presence": "operates under scene variations and lighting changes in benchmarks; no synthetic distractors reported",
            "performance_metrics": "Interactive diffusion outperforms concatenation baseline by +1.8% Avg.SR in ablation (Tab.3); visual attention alignment shown in Fig.4.",
            "computational_cost_details": "Implemented with diffusion transformer architecture; decomposition network is an 8-layer transformer (hidden dim 384); no FLOPs/params reported separately.",
            "comparison_to_baselines": "Compared with a concatenation-based latent approach and with architectures without diffusion; interactive diffusion gave better alignment and higher Avg.SR (+1.8% over concatenation, Tab.3) and improved over transformer-only world model by +7.8% (Tab.3 first row).",
            "transfer_learning_results": "Used within LaDi-WM which shows cross-scene transfer (LIBERO-&gt;CALVIN) and sim-to-real transfer; no separate cross-model transfer numbers reported specifically for interactive diffusion component alone.",
            "multi_task_performance": "Contributes to multi-task generalization of LaDi-WM; ablations show interactive diffusion helps across tasks, particularly aligning semantics and geometry for varied objects.",
            "failure_modes": "No explicit failure modes isolated beyond overall model limitations (data scale, long-horizon compounding errors).",
            "ablation_studies": "Ablation comparing interactive diffusion vs concatenation (+1.8% Avg.SR) and transformer-only variants (+7.8% improvement reported for full design, Tab.3); attention visualization used to show improved alignment.",
            "sample_efficiency": "Implicitly improves convergence speed and generalizability in small-data regimes when combined into LaDi-WM; no standalone sample counts reported.",
            "generalization_analysis": "Attention visualizations (Fig.4) and task performance ablations indicate better cross-scene and cross-task alignment when interactive diffusion is used.",
            "reconstruction_quality": "Operates on latent predictions; training uses MSE on latent reconstructions and noise variables; no standard image-reconstruction metrics provided.",
            "task_relevance_analysis": "Interactive diffusion yields attention maps that align both latents to same object regions indicating improved capture of task-relevant object features; ablations show semantic inclusion aided certain tasks (Tasks 1,5,9).",
            "dynamic_abstraction": "Enables dynamic interaction between two latent abstraction channels during denoising but does not perform explicit hierarchical switching.",
            "exploration_vs_exploitation": "Not explicitly analyzed for exploration/exploitation tradeoff, though interactive diffusion improves the imagined-state guidance used for action refinement (exploitation).",
            "information_theoretic_analysis": "None reported.",
            "pixel_fidelity_benefits": "Not applicable; intended to replace pixel-level diffusion with latent interaction to improve generalizability.",
            "uuid": "e2252.1"
        },
        {
            "name_short": "Pixel diffusion baseline",
            "name_full": "Pixel-space Diffusion World Model (pixel diffusion)",
            "brief_description": "A diffusion-based world model that predicts future in image (pixel) space used as an ablation baseline to compare against latent diffusion; same network architecture but without VFM latent extraction.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Pixel diffusion world model",
            "abstraction_level": "pixel-level reconstruction",
            "feature_selection_mechanism": "no explicit task-relevance mechanism; reconstructs pixels directly (implicit selection via learning objective)",
            "task_domain": "robotic manipulation (LIBERO-LONG benchmark ablation)",
            "distractor_presence": "subject to same scene variations and lighting as latent experiments",
            "performance_metrics": "Avg.SR 54.0% in ablation (Table 10) compared to LaDi-WM 71.7% (drop of 14.7% Avg.SR).",
            "computational_cost_details": "Authors note higher computational expense and overhead for pixel-level diffusion (image reconstruction + diffusion) which limits applicability, but no FLOPs/params/time numbers provided for pixel model specifically.",
            "comparison_to_baselines": "Underperforms LaDi-WM by 14.7% Avg.SR (Table 10). Authors argue latent diffusion (VFM-aligned) is more compact and generalizable.",
            "transfer_learning_results": "No specific transfer experiments reported for pixel diffusion; general claim that pixel models generalize worse is supported by lower Avg.SR.",
            "multi_task_performance": "Reported single multi-task aggregate (Avg.SR) in ablation; performs worse across tasks compared to latent model.",
            "failure_modes": "Lower generalization to unseen tasks/scenes and higher compute overhead; less robust for manipulation where geometry/semantics matter.",
            "ablation_studies": "Direct ablation vs LaDi-WM (Table 10) shows substantial performance degradation when moving from latent to pixel diffusion.",
            "sample_efficiency": "Not explicitly reported; implied less sample-efficient due to poorer generalization in limited-data regime.",
            "generalization_analysis": "Authors conclude pixel diffusion is less generalizable from observed declines in Avg.SR in same evaluation settings.",
            "reconstruction_quality": "Pixel reconstructions are by construction exact targets of diffusion but no PSNR/SSIM/MSE to pixels are reported; metric comparisons are given at policy level (Avg.SR).",
            "task_relevance_analysis": "Paper argues pixel reconstructions include many task-irrelevant details and do not capture geometric/semantic latents needed for manipulation; empirical results support this claim via lower task success.",
            "dynamic_abstraction": "Fixed pixel-level abstraction (no dynamic adjustment).",
            "exploration_vs_exploitation": "Not discussed.",
            "information_theoretic_analysis": "None reported.",
            "pixel_fidelity_benefits": "Paper does not present empirical scenarios where pixel fidelity yields better task performance; instead it reports worse manipulation success and higher compute cost.",
            "uuid": "e2252.2"
        },
        {
            "name_short": "DINO latents",
            "name_full": "DINO-derived geometric latent representation",
            "brief_description": "Pretrained self-supervised visual features (DINO) used to extract geometric latent codes that capture object geometry and structure; used by LaDi-WM as the geometric channel of latent state.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "DINO (geometric latent encoder)",
            "abstraction_level": "geometric / mid-level visual features (object geometry, structure)",
            "feature_selection_mechanism": "pretrained self-supervised feature extractor; no task-specific masking—task relevance emerges via attention in world model and policy",
            "task_domain": "robotic manipulation (as part of LaDi-WM latent state)",
            "distractor_presence": "tested under scene appearance variations and lighting changes; DINO features are intended to be robust to such variations",
            "performance_metrics": "Not reported in isolation; ablation shows adding semantic latents on top of geometric improves Avg.SR by +3.4% (Tab.3), implying geometric latents are necessary but semantics add value for some tasks.",
            "computational_cost_details": "Pretrained encoder used; no fine-tuning cost reported for DINO in this work.",
            "comparison_to_baselines": "DINO-based prior work (DINO-WM) is cited as effective for geometry; LaDi-WM extends this by adding semantic latents and latent diffusion.",
            "transfer_learning_results": "DINO features enable task-agnostic world-model training and cross-task generalization as part of LaDi-WM; no isolated DINO-transfer numbers provided.",
            "multi_task_performance": "Geometric features are helpful across manipulation tasks; inclusion in LaDi-WM supports multiple task types.",
            "failure_modes": "Alone, geometry-only features may miss semantic distinctions needed for tasks involving semantic object identity (authors show semantic latents help Tasks 1,5,9).",
            "ablation_studies": "Ablation removing semantic features indicates geometric-only representation performs worse by ~3.4% Avg.SR for the full model variant (Tab.3 second row).",
            "sample_efficiency": "Pretrained feature usage improves sample efficiency implicitly; exact numbers not separated by representation.",
            "generalization_analysis": "DINO features are leveraged to improve cross-scene and sim-to-real generalization as part of the combined latent world model.",
            "reconstruction_quality": "Not applicable (no pixel reconstruction by DINO in this setup).",
            "task_relevance_analysis": "Attention visualizations show geometric latents attend to object regions; when combined with semantics, attention aligns to task-relevant objects.",
            "dynamic_abstraction": "Serves as one static abstraction channel (geometry) that interacts with semantic channel during denoising.",
            "exploration_vs_exploitation": "Not discussed.",
            "information_theoretic_analysis": "None reported.",
            "pixel_fidelity_benefits": "Not applicable.",
            "uuid": "e2252.3"
        },
        {
            "name_short": "SigLip",
            "name_full": "SigLip (semantic VFM latent representation)",
            "brief_description": "A VFM-derived semantic latent encoder used to extract semantic features (language/semantic-aligned) for predicting task-relevant semantic changes; integrated into LaDi-WM as the semantic channel.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "SigLip (semantic latent encoder)",
            "abstraction_level": "semantic / language-image aligned high-level features",
            "feature_selection_mechanism": "pretrained semantic encoder (SigLip) provides semantic latents; task relevance handled downstream by world model attention; no explicit task masking",
            "task_domain": "robotic manipulation (semantic distinctions among objects and instructions)",
            "distractor_presence": "semantic encoder helps disambiguate objects with similar geometry but different semantics across variable scenes",
            "performance_metrics": "Adding semantic latents to geometry-only model increases Avg.SR by +3.4% in ablation (Tab.3).",
            "computational_cost_details": "Pretrained encoder used; fine-tuning or encoder cost not separately reported.",
            "comparison_to_baselines": "Compared implicitly to geometric-only models; semantic inclusion improves performance on tasks requiring semantic discrimination (Tasks 1,5,9).",
            "transfer_learning_results": "Semantic latents help task-agnostic world model generalize across tasks with semantic variations; no isolated numeric transfer metrics reported for SigLip alone.",
            "multi_task_performance": "Improves average performance across tasks with semantic requirements; part of shared representation used across tasks.",
            "failure_modes": "Not sufficient alone; needs to be combined with geometric latents for detailed manipulation performance.",
            "ablation_studies": "Semantic inclusion ablation shows +3.4% Avg.SR gain (Tab.3).",
            "sample_efficiency": "Pretrained semantics improve data efficiency when combined in LaDi-WM; no separate sample numbers.",
            "generalization_analysis": "Contributes to better handling of tasks requiring semantic distinctions; attention alignment shows semantics align to same objects as geometry after interactive diffusion.",
            "reconstruction_quality": "Not applicable.",
            "task_relevance_analysis": "Authors report which tasks benefit from semantic signals (Tasks 1,5,9) — tasks with distinct semantic object identity.",
            "dynamic_abstraction": "Acts as a static semantic abstraction channel interacting with geometric channel during diffusion.",
            "exploration_vs_exploitation": "Not discussed.",
            "information_theoretic_analysis": "None reported.",
            "pixel_fidelity_benefits": "Not applicable.",
            "uuid": "e2252.4"
        },
        {
            "name_short": "Seer",
            "name_full": "Seer (previous state-of-the-art baseline)",
            "brief_description": "An inverse-dynamics approach that jointly predicts actions and subsequent frame images and was prior SOTA on LIBERO and CALVIN; used here as a main baseline comparison.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Seer (inverse dynamics + image prediction)",
            "abstraction_level": "implicit image-frame-conditioned inverse dynamics (predicts images and actions)",
            "feature_selection_mechanism": "joint inverse dynamics and image prediction objective (no explicit geometric/semantic disentangling reported)",
            "task_domain": "robotic manipulation (LIBERO-LONG, CALVIN D-D comparisons)",
            "distractor_presence": "evaluated on same benchmarks with scene variation",
            "performance_metrics": "Reported Avg.SR 53.6% (Table 1, 10-demo setting) and full-data baseline Avg.SR 87.7% (Table 6) depending on setting.",
            "computational_cost_details": "Not detailed in this paper.",
            "comparison_to_baselines": "LaDi-WM exceeds Seer by +15.1% Avg.SR in limited-data setting (10 demo, text) and +3.0% in full-data setting (90.7% vs 87.7%, Table 6).",
            "transfer_learning_results": "Seer was evaluated on the same benchmarks in original work; LaDi-WM demonstrates better generalization and usable imagination guidance for policy refinement in current experiments.",
            "multi_task_performance": "Seer performance reported per-task in Table 1; LaDi-WM improves over Seer across many tasks.",
            "failure_modes": "Authors argue Seer's implicit dynamics cannot directly provide imagined future states for policy iterative refinement; performance limitations in low-data regimes relative to LaDi-WM.",
            "ablation_studies": "Not applicable here (Seer is baseline).",
            "sample_efficiency": "Seer performs worse than LaDi-WM in limited-data setting (10 demos) per reported numbers.",
            "generalization_analysis": "Seer lacks explicit task-agnostic latent alignment (geometry+semantics) that LaDi-WM leverages for cross-task generalization.",
            "reconstruction_quality": "Seer predicts images as part of objective, but pixel reconstruction metrics not reported here.",
            "task_relevance_analysis": "Not detailed in this paper beyond performance comparisons.",
            "dynamic_abstraction": "No multi-level latent disentanglement reported; predicts images and actions jointly.",
            "exploration_vs_exploitation": "Not discussed here.",
            "information_theoretic_analysis": "None reported.",
            "pixel_fidelity_benefits": "Seer uses image prediction, but in authors' comparison LaDi-WM's latent predictions provide better guidance for policy learning.",
            "uuid": "e2252.5"
        },
        {
            "name_short": "DreamerV3",
            "name_full": "DreamerV3",
            "brief_description": "A prior latent-world-model method that learns latent dynamics for control; used as a baseline in evaluations and contrasted as optimizing representations for image reconstruction/reward rather than VFM-aligned geometry/semantics.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "DreamerV3",
            "abstraction_level": "latent-space dynamics optimized for reconstruction/prediction (task reward oriented)",
            "feature_selection_mechanism": "learned latent representations via recurrent components and reconstruction objectives; no explicit semantic/geometric disentanglement",
            "task_domain": "continuous control / robotic manipulation baselines (LIBERO, CALVIN comparisons)",
            "distractor_presence": "subject to same benchmarks' visual variability",
            "performance_metrics": "Reported Avg.SR 33.5% on LIBERO-LONG (Table 1 in this paper's baselines listing) in the compared setting.",
            "computational_cost_details": "Not reported here.",
            "comparison_to_baselines": "LaDi-WM significantly outperforms DreamerV3 in these manipulation tasks (e.g., DreamerV3 33.5% vs LaDi-WM 68.7% in Table 1 10-demo comparisons).",
            "transfer_learning_results": "Not evaluated here; authors note Dreamer-series methods require dense rewards and have limitations for manipulation tasks in this setup.",
            "multi_task_performance": "Baseline multi-task numbers reported in tables; DreamerV3 underperforms LaDi-WM across tasks reported.",
            "failure_modes": "Authors note representations optimized for reconstruction/reward can neglect geometric/semantic features critical for manipulation.",
            "ablation_studies": "Not performed here (DreamerV3 used as baseline).",
            "sample_efficiency": "Per baselines, less sample-efficient for manipulation tasks in their experiments compared to LaDi-WM.",
            "generalization_analysis": "Not emphasized here.",
            "reconstruction_quality": "Not reported here.",
            "task_relevance_analysis": "Authors critique Dreamer-style latent objectives for not explicitly capturing semantics/geometry required for manipulation.",
            "dynamic_abstraction": "No explicit multi-level abstraction in DreamerV3 beyond learned latent hierarchy.",
            "exploration_vs_exploitation": "Not discussed here.",
            "information_theoretic_analysis": "None reported.",
            "uuid": "e2252.6"
        },
        {
            "name_short": "TDMPC2",
            "name_full": "TD-MPC2",
            "brief_description": "A scalable, robust world-model-and-planning method for continuous control used here as a baseline; noted to operate in latent space but optimize representations for reconstruction/reward.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "TDMPC2",
            "abstraction_level": "latent-space planning/control (reconstruction/reward-optimized representations)",
            "feature_selection_mechanism": "learned latent via predictive/reconstruction objectives and used for model predictive control; no explicit semantic/geometric disentangling",
            "task_domain": "continuous control / robotic manipulation baseline",
            "distractor_presence": "same benchmark visual variability as other baselines",
            "performance_metrics": "Reported Avg.SR 37.0% on LIBERO-LONG baseline (Table 1).",
            "computational_cost_details": "Not provided in this paper.",
            "comparison_to_baselines": "Underperforms LaDi-WM substantially in these manipulation evaluations (LaDi-WM 68.7% vs TDMPC2 37.0% in Table 1).",
            "transfer_learning_results": "Not directly reported here.",
            "multi_task_performance": "Included as baseline multi-task comparisons; lower Avg.SR than LaDi-WM.",
            "failure_modes": "Requires dense reward shaping for manipulation in authors' setup; authors point out this limits application to complex manipulation tasks without dense rewards.",
            "ablation_studies": "Not performed here.",
            "sample_efficiency": "Requires reward design and RL training; authors initialized policies with BC for baselines due to RL difficulty in manipulation.",
            "generalization_analysis": "Not emphasized.",
            "reconstruction_quality": "Not reported here.",
            "task_relevance_analysis": "Authors claim these latent representations neglect critical geometric/semantic information needed for manipulation.",
            "dynamic_abstraction": "Not described.",
            "exploration_vs_exploitation": "Not discussed.",
            "information_theoretic_analysis": "None reported.",
            "uuid": "e2252.7"
        },
        {
            "name_short": "ATM baseline",
            "name_full": "ATM (instruction-conditioned trajectory prediction)",
            "brief_description": "An instruction-conditioned prediction model that forecasts point trajectories and feeds them to a policy; cited as a baseline that is task-specific and point-based and thus limited in generalization for detailed manipulation.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "ATM",
            "abstraction_level": "point-trajectory (sparse trajectory-level) predictions",
            "feature_selection_mechanism": "task/instruction-conditioned trajectory predictions (task-specific inductive bias); no dense visual latent capture",
            "task_domain": "language-conditioned manipulation (benchmarks used for comparison)",
            "distractor_presence": "benchmarks' scene variability applies; ATM's sparse points ignore detailed observation distractors",
            "performance_metrics": "Reported Avg.SR 44.0% on LIBERO-LONG baseline (Table 1); CALVIN Avg.Len.&Avg.SR also reported in Table 2/7 baseline rows.",
            "computational_cost_details": "Not given in this paper.",
            "comparison_to_baselines": "LaDi-WM outperforms ATM in multi-task benchmarks (LaDi-WM Avg.SR 68.7% vs ATM 44.0% in Table 1). Authors note ATM's task-specific grounding limits generalization.",
            "transfer_learning_results": "Not highlighted; ATM's instruction conditioning is argued to restrict zero-shot generalization.",
            "multi_task_performance": "ATM performs worse across LIBERO tasks relative to LaDi-WM's imagined-latent-guided diffusion policy.",
            "failure_modes": "Sparse point representation ignores detailed visual information and limits generalization to unseen tasks or object-specific semantic distinctions.",
            "ablation_studies": "Not performed here (ATM used as baseline).",
            "sample_efficiency": "Baseline numbers provided, but direct sample-efficiency claims not made.",
            "generalization_analysis": "Authors argue ATM is less generalizable due to instruction/task-specific predictions.",
            "reconstruction_quality": "Not applicable (point trajectories rather than images).",
            "task_relevance_analysis": "ATM is task-conditioned; not designed to discover task-agnostic semantics/geometry.",
            "dynamic_abstraction": "Fixed sparse trajectory abstraction.",
            "exploration_vs_exploitation": "Not discussed.",
            "information_theoretic_analysis": "None reported.",
            "uuid": "e2252.8"
        },
        {
            "name_short": "Copy model",
            "name_full": "Copy-imagination baseline",
            "brief_description": "An ablation baseline that supplies the policy with a copy of the current observation as the 'future imagination' to test whether naive or task-irrelevant future inputs improve policy learning.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Copy model (current observation used as future imagination)",
            "abstraction_level": "current-frame pixel/latent copy (no real prediction)",
            "feature_selection_mechanism": "no feature selection; uses current observation as surrogate future",
            "task_domain": "robotic manipulation (LIBERO-LONG ablation)",
            "distractor_presence": "same dataset conditions",
            "performance_metrics": "Avg.SR 40.2% (Table 9) which is slightly worse than vanilla BC 40.8% indicating naive future-copy is unhelpful.",
            "computational_cost_details": "Trivial overhead compared to learned world model; no compute numbers given.",
            "comparison_to_baselines": "Copy model does not improve over BC (vanilla behavior cloning) and is far below LaDi-WM performance (LaDi-WM Avg.SR 68.7%, Table 9), showing that task-irrelevant or non-predictive future inputs do not provide benefit.",
            "transfer_learning_results": "Not applicable.",
            "multi_task_performance": "Reported aggregated Avg.SR in Table 9; performs poorly across tasks.",
            "failure_modes": "Using current observation as future imagination supplies inaccurate/task-irrelevant guidance and can harm policy learning.",
            "ablation_studies": "Direct ablation showing copy model ~40.2% vs BC 40.8% and vs LaDi-WM 68.7% (Table 9).",
            "sample_efficiency": "No evidence of improved sample efficiency; effectively no benefit.",
            "generalization_analysis": "Shows that non-predictive or task-irrelevant imagined states do not generalize to provide useful guidance.",
            "reconstruction_quality": "N/A.",
            "task_relevance_analysis": "Demonstrates necessity of task-relevant imagined futures; naive copies of current obs are insufficient.",
            "dynamic_abstraction": "N/A.",
            "exploration_vs_exploitation": "N/A.",
            "information_theoretic_analysis": "None reported.",
            "uuid": "e2252.9"
        },
        {
            "name_short": "Diffusion policy",
            "name_full": "Imagination-guided Diffusion Policy (transformer diffusion policy)",
            "brief_description": "A diffusion-based policy model (transformer encoder-decoder) that predicts action sequences and iteratively refines them using imagined future latent states from LaDi-WM; benefits from multimodal conditioning and iterative inference-time refinement.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Diffusion policy (transformer-based)",
            "abstraction_level": "action-space diffusion conditioned on latent-state imaginations (policy-level)",
            "feature_selection_mechanism": "policy fuses modalities (task instruction, diffusion timestep, end-effector states, historical and imagined latents) via transformer attention to select task-relevant inputs; iterative refinement reduces action distribution entropy",
            "task_domain": "robotic manipulation (LIBERO-LONG, CALVIN D-D, real-world finetuning)",
            "distractor_presence": "operates with same visual variability as world model experiments",
            "performance_metrics": "Iterative refinement: 1-iter Avg.SR 60.7%; 2-iter Avg.SR 68.7% (Table 4); denoising steps: convergent performance with as few as 2 denoising steps (Table 11).",
            "computational_cost_details": "Policy transformer: encoder 6 layers, decoder 4 layers, hidden dim 256; trained ~6 hours on an NVIDIA 4090 (batch size 24). Inference uses iterative refinement (authors find 2 iterations sufficient).",
            "comparison_to_baselines": "Outperforms vanilla BC when paired with LaDi-WM (BC Avg.SR 40.8% vs LaDi-WM+diffusion policy 68.7% in 10-demo setting, Table 9 and Table 1). Iterative refinement yields substantial gains over no-imagination or single-pass predictions (Table 4).",
            "transfer_learning_results": "Policy trained in sim and finetuned with 10 real demos achieves strong real-world performance (combined with LaDi-WM, +20% over BC); no isolated policy-only transfer numbers provided.",
            "multi_task_performance": "Policy conditioned on task instruction and imagined futures performs across multiple tasks (LIBERO and CALVIN benchmarks) with improved Avg.SR and Avg.Len values.",
            "failure_modes": "Initial action samples can be suboptimal; benefit relies on sufficiently accurate imagined states—poor long-horizon imaginations may limit refinement benefits; diminishing returns beyond ~2 refinement iterations.",
            "ablation_studies": "Denoising steps ablation: convergent performance with 2 steps (Table 11). Iterative refinement ablation demonstrates performance improves substantially up to 2 iterations (Table 4).",
            "sample_efficiency": "When combined with LaDi-WM, achieves strong performance with only 10 demonstrations per task (policy finetuning), indicating sample-efficient refinement; specific sample counts listed (10 demos) tied to reported Avg.SR improvements.",
            "generalization_analysis": "Iterative refinement reduces action entropy and variance across repeated evaluations (Fig.7 and Table 8), contributing to more consistent generalization across tasks and trials.",
            "reconstruction_quality": "Not applicable (policy predicts actions, not reconstructions).",
            "task_relevance_analysis": "By conditioning on imagined futures and instruction, the policy implicitly learns to rely on task-relevant imagined latent signals; ablations (copy model) show naive inputs do not help.",
            "dynamic_abstraction": "Policy operates at action-level abstraction but adjusts predictions iteratively based on higher-level latent imaginations; does not explicitly change representation abstraction.",
            "exploration_vs_exploitation": "Iterative refinement operates at inference to reduce stochasticity and improve exploitation; training focuses on imitation learning rather than exploration-driven RL.",
            "information_theoretic_analysis": "None reported.",
            "uuid": "e2252.10"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Dino-wm: World models on pre-trained visual features enable zero-shot planning",
            "rating": 2
        },
        {
            "paper_title": "Dream to control: Learning behaviors by latent imagination",
            "rating": 2
        },
        {
            "paper_title": "Diffusion world model: Future modeling beyond step-by-step rollout for offline reinforcement learning",
            "rating": 2
        },
        {
            "paper_title": "TD-MPC2: scalable, robust world models for continuous control",
            "rating": 2
        },
        {
            "paper_title": "Diffusion for world modeling: Visual details matter in atari",
            "rating": 2
        },
        {
            "paper_title": "Predictive inverse dynamics models are scalable learners for robotic manipulation",
            "rating": 2
        }
    ],
    "cost": 0.02592675,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>LaDi-WM: A Latent Diffusion-based World Model for Predictive Manipulation
12 Sep 2025</p>
<p>Yuhang Huang 
Shilong Zou 
Xinwang Liu 
Ruizhen Hu 
Kai Xu </p>
<p>National University of Defense Technology Jiazhao Zhang Peking University</p>
<p>National University of Defense Technology</p>
<p>National University of Defense Technology</p>
<p>Shenzhen University</p>
<p>National University of Defense Technology</p>
<p>LaDi-WM: A Latent Diffusion-based World Model for Predictive Manipulation
12 Sep 2025ED13A119312242F2FA495131E5832963arXiv:2505.11528v6[cs.RO]World modelsLatent diffusion modelRobotic manipulation
Predictive manipulation has recently gained considerable attention in the Embodied AI community due to its potential to improve robot policy performance by leveraging predicted states.However, generating accurate future visual states of robot-object interactions from world models remains a well-known challenge, particularly in achieving high-quality pixel-level representations.To this end, we propose LaDi-WM, a world model that predicts the latent space of future states using diffusion modeling.Specifically, LaDi-WM leverages the well-established latent space aligned with pre-trained Visual Foundation Models (VFMs), which comprises both geometric features (DINO-based) and semantic features (CLIP-based).We find that predicting the evolution of the latent space is easier to learn and more generalizable than directly predicting pixel-level images.Building on LaDi-WM, we design a diffusion policy that iteratively refines output actions by incorporating forecasted states, thereby generating more consistent and accurate results.Extensive experiments on both synthetic and real-world benchmarks demonstrate that LaDi-WM significantly enhances policy performance by 27.9% on the LIBERO-LONG benchmark and 20% on the real-world scenario.Furthermore, our world model and policies achieve impressive generalizability in real-world experiments.The source code will be public at: https://github.com/GuHuangAI/LaDiWM.</p>
<p>Introduction</p>
<p>Predictive manipulation [1,2,3] has recently gained considerable attention in Embodied AI due to its potential to enhance robot policy performance by leveraging predicted future states.These predictive states, which represent anticipated interactions between robots and objects, have been widely received as beneficial for action planning which is error-prone especially for long-term tasks [4,5].To obtain predictive states, most existing methods employ world models [3,6] to predict visual frames.However, pixel-level visual prediction often exhibits limited performance and generalizability.Another line of research advances world models by predicting latent representations [7,8,4], which simplifies model learning and demonstrates strong generalizability.However, the latent space in existing works is typically optimized for image reconstruction, thus failing to capture the geometric and semantic information that is crucial for successful manipulations.</p>
<p>Inspired by latent-space world models, we propose LaDi-WM, which is designed to efficiently model the correlation of geometric and semantic information in the latent space of Visual Foundation Models (VFMs).Our method incorporates both DINO-based [9,10] and CLIP-based [11,12,13] latent representations, capturing geometric and semantic information, respectively.To effectively ⃝ Given an initial action sequence generated by the policy, our world model produces imagined future states.2 ⃝ These imagined states provide efficient guidance for the policy model, 3</p>
<p>⃝ yielding refined actions for improved manipulation.(b): Benefiting from our world model, we achieve significant improvements in both simulation and real-world performance.</p>
<p>model their dynamic correlations, we introduce an interactive diffusion process that performs crossattention between clean latent components decomposed from noisy latent representations of geometry and semantics.We find that modeling the dynamic interaction between geometry and semantics with latent space diffusion leads to better alignment between the two, and hence to fast learning convergence and strong cross-scene and cross-task generalibility.</p>
<p>Based on LaDi-WM, we propose an imagination-guided diffusion policy that features iterative action refinement.As illustrated in Fig. 1 (a), we train a diffusion policy within the latent space of both DINO and CLIP.This policy is trained to output a sequence of actions at a time.We predict for each action an imagined future state using the pretrained LaDi-WM.These imagined states are then fed into the policy model to produce a refined sequence of actions.Such refinement is guided by multi-step imaginations, yielding higher quality actions.The refinement can be iterated for multiple times until convergence.We provide a discussion on the convergence of the iterative refinement and show that it progressively improves the accuracy and consistency of action predictions.</p>
<p>We have conducted extensive experiments on both synthetic benchmarks and real-world environments.Benefiting from the efficient future guidance by LaDi-WM, our method achieves the highest success rate among the state-of-the-art methods.In particular, our approach significantly improves over the behavior cloning baseline by 27.9% in the synthetic experiment and 20.0% in the real-world environment, demonstrating the superior potential of LaDi-WM in guiding policy learning.</p>
<p>Related Work</p>
<p>World models can be regarded as future prediction models that forecast future environmental states based on historical observations and action controls.Current studies on world models can be split into two types, which learn the dynamics in the raw image space and the latent space, respectively.Typical world models [14] employ recurrent neural networks [15,16] and convolution networks [17] to learn environmental dynamics as future image prediction.Advanced methods [3,18,19] explore the use of diffusion models to build world models and learn dynamics in pixel space; however, the computational expense associated with image reconstruction and the overhead of diffusion models limit their applicability.In contrast, the Dreamer series of methods [7,8,20] builds upon recurrent neural networks and explores learning imagination in the latent space.Similarly, IRIS [6] and TDMPC2 [5] also learn latent dynamics but utilize transformer and MLP architectures for world modeling.To enhance the representation ability, DINO-WM [4] leverages the pretrained DINO model to extract the latent representation containing rich geometric information.In this paper, we propose the adoption of latent diffusion to establish a more generalizable world model, involving both geometric and semantic information, powered by pretrained foundation models.World models for predictive policy learning.World modes can interact with the policy to generate future dynamics, which can be used to facilitate policy learning.Early world models are usually utilized to support reinforcement learning.IRIS [6] and the Dreamer series [7,8,20] propose learning policies in the imagined world to solve simple game tasks.TDMPC series methods [5] and DINO-WM [4] combine world models with model predictive control, optimizing action sequences based on a goal image.However, reinforcement learning [21,22] requires dense reward, limiting its application in relatively hard tasks such as manipulation.Apart from reinforcement learning, another line of work lets the policy predict the action according to the future imagined observations.AVDC [2] employs a language-driven video prediction model to generate future frames and estimate future optical flow to calculate corresponding robotic transformations.Its performance depends largely on the accuracy of the estimated flow, which is not robust for manipulation tasks.ATM [1] introduces an instruction-conditioned prediction model to predict future point trajectories and utilizes the prediction as the input to the policy network for behavior cloning.The task-specific prediction limits its generalization, and the point representation ignores the detailed information, which may constrain performance improvement.In this paper, our world model is designed to deal with task-agnostic prediction for powerful generalization and learns more detailed information containing both geometry and semantics, resulting in a significant improvement in manipulation success rate.</p>
<p>Learning latent diffusion world model</p>
<p>Method</p>
<p>As illustrated in Fig. 2, our method involves two components: a latent diffusion-based world model for imagining the future states, and an imagination-guided policy for action planning, with details provided in Sec.3.1 and Sec.3.2, respectively.</p>
<p>Latent Diffusion-based World Model</p>
<p>We propose to utilize the latent diffusion to construct the world model, aiming to capture the diverse generalizable dynamics of the environments.Benefiting from the powerful ability of diffusion models for modeling multimodal distribution, our world model can handle complex environmental dynamics.Moreover, the diffusion process is conducted in a generalizable latent space that is governed by the pretrained foundation models, containing both geometric and semantic information.</p>
<p>Specifically, we adopt DINO [10] and Siglip [13] to extract both geometric and semantic latent codes, which are crucial for executing robotic tasks.The latent states extracted via the foundation models can be expressed by:
z t = [z D t ; z S t ] = [f dino (I t ); f sigl (I t )],(1)
where f dino/sigl represents the DINO and Siglip networks, I t and z t are the image observation and latent state at time t.</p>
<p>After obtaining the latent states z t , the goal of the world model is to predict future latent states given historical latent states z t−l:t and action control a t:t+k : z t+1:t+k+1 ∼ p θ (z t+1:t+k+1 |z t−l:t , a t:t+k ), (2) where p is the transition probability distribution, and θ is the parameter of the world model.To simplify the following formulation, we replace the future latent states z t+1:t+k+1 with z t+1: .Interactive diffusion modeling.We propose to utilize the diffusion model to model the transition probability.However, the two types of codes follow different distributions, which implies that it is inappropriate to apply the one diffusion process to learn the two types of dynamics.On the other hand, a separate learning framework may overlook the relationships between high-level semantics and low-level geometry, resulting in inefficient dynamic modeling.To this end, we propose a novel interactive diffusion process that enables two latent codes to interact with each other, leading to more effective dynamic modeling.Moreover, inspired by Huang et al. [23], we decompose the clean latent component from the noisy codes and leverage the clean latent component to conduct interaction.</p>
<p>In more detail, the forward diffusing process consists of an attenuation process of the latent states and a noise-increasing process.To formulate the forward process, we introduce an additional superscript n to represent the diffusion step, i.e., we use z t+1:,0 = z t+1: and z t+1:,n to denote the clean latent states and noisy states.Considering a continuous-time diffusion equation with n ∈ [0, 1], the forward process for the DINO latent code can be represented by:
z D t+1:,n = z D t+1:,0 + n 0 C D n dn + √ nη D , η D ∼ N (0, I),(3)
where z D t+1:,0 + n 0 C D n dn represents the attenuation process of the latent states while √ nη D denotes the growth of the normal gaussian noise, and z D t+1:,n is the noisy latent code.Essentially, C D n is the gradient of the latent code attenuation, which can be solved by z D t+1:,n + 1 0 C D n dn = 0.According to the derivation in [23], the attenuation process is governed by C D n , thus we can use C D n as the clean component of z D t+1:,n .Similarly, we can denote the clean component of z S t+1:,n as C S n .Following the derivation in Huang et al. [23], the conditional probability used in the reversed process can be expressed by:
q(z D t+1:,n−∆n |z D t+1:,n , z D t+1:,0 ) = N (z D t+1:,n−∆n , µ, Σ), µ = z D t+1:,n + ∆tz D t+1:,0 − ∆n √ n η D , Σ = ∆n(n − ∆n) n I.(4)
In the reversed process, the clean latent code z D t+1:,0 and η are not accessible, hence we need to use the parameterized p θ (z D t+1:,n−∆n |z D t+1:,n ) to approximate q(z D t+1:,n−∆n |z D t+1:,n , z D t+1:,0 ).In this way, the denoising network outputs z D t+1:,θ and η D θ to parameterize z D t+1:,0 and η D simultaneously.Note that we perform a similar implementation to predict the future Siglip latent code z S t+1:,0 .To let the two types of latent codes interact with each other in the diffusion process, we first leverage the decomposition network f θ1 and f θ2 to estimate the clean components C D n and C S n .The outputs can be expressed via:
C D θ = f θ1 (z D t+1:,n , n, z D t−l:t , a t:t+k ), C S θ = f θ2 (z S t+1:,n , n, z S t−l:t , a t:t+k ).(5)
Here, z D t−l:t and z S t−l:t are the historical latent codes and a t:t+k is the given action control, which serves as the conditional inputs.Then, C D θ1 and C S θ2 are fed into the denoising network f θ3 and f θ4 , generating the parameterized z D t+1:,θ and η D for recovering future latent codes.
z D t+1:,θ , η D θ = f θ3 (z D t+1:,n , n, C S θ , a t:t+k ), z S t+1:,θ , η S θ = f θ4 (z S t+1:,n , n, C D θ , a t:t+k ).(6)
We adopt the diffusion transformer [24] architecture to construct our world model, and more details can be found in Appendix A.</p>
<p>Predictive Policy Learning</p>
<p>Based on the world model, we propose a predictive policy learning framework to efficiently solve manipulation tasks by imitation learning.Specifically, given an initial action generated by the policy model, the world model can imagine the future latent states based on historical states.The imagined future states indicate the possible outcomes caused by the initial action, which provides effective guidance for the policy model.Utilizing the imagined guidance as additional input, the policy model can predict the action precisely and refine itself iteratively during testing time:
a t:t+k = π θ (z t−l:t , e t−l:t , ẑt+1:t+k+1 , T ),(7)
where z t−l:t and e t−l:t are l-frame historical latent and end-effector states, ẑt+1:t+k+1 is the imagined k-frame future states, and T is the task instruction.</p>
<p>We employ a diffusion policy with the transformer architecture as our policy model.The diffusion policy has advantages in learning multimodal action distributions and addressing high-dimensional action spaces, which is beneficial for solving diverse manipulation tasks.We build the diffusion process following Huang et al. [23] to improve the efficiency of action prediction.We utilized the different modalities of historical observations as input, including task instruction, diffusion time step, robotic end-effector states, and historical and imagined latent states.</p>
<p>Concretely, given the task instruction T , the diffusion time step n, the historical l frames of latent representation z t−l:t and end-effector poses e t−l:t , we first utilize the policy model to predict an initial action sequence ât:t+k−1 .The inital action sequence is fed into the world model with z t−l:t , producing the imagined future states ẑt+1:t+k .Then, we utilize ẑt+1:t+k as additional inputs of the policy model, guiding the prediction of the denoising process.More specifically, we tokenize different modalities of inputs and concatenate them together, and utilize the transformer encoder to fuse the different modality information.The fused features are sent to the transformer decoder, accompanied by the noisy action, obtaining the denoised action.The detailed architecture of the policy model can be found in Appendix A.</p>
<p>Iterative refinement in inference time.During inference, our policy model allows for an iterative refinement process through multi-time exploration within the world model.When an initial action is generated by the policy model, the world model produces corresponding future state predictions.However, since the initial action may be suboptimal, the resulting imagined future states may lack sufficient precision to provide effective guidance.To address this limitation, we implement an iterative correction mechanism in which the policy model continuously refines its actions based on progressively more accurate imagined states generated by the world model.This closedloop interaction between policy refinement and world model prediction enables increasingly precise action optimization, ultimately leading to more robust and effective policy execution.The detailed inference procedure is provided in Appendix B. We also analyze the convergence of the iterative refinement, which can be found in Appendix E.</p>
<p>Experiments</p>
<p>We conduct simulated experiments on two public benchmarks, LIBERO-LONG [25] and CALVIN D-D [26].The detailed information about the two benchmarks and evaluation metrics can be found in Appendix C. The quantitative comparisons to baselines, ablation studies, and real-world experiments are provided in Sec.4.1, Sec.4.2, and Sec.4.3, respectively.</p>
<p>Quantitative Results</p>
<p>LIBERO-LONG benchmark results.The results on the Libero benchmark are shown in Tab. 8, where we report detailed performance for each task along with the average success rate, with all Scalability.Figure 3 demonstrates our method's scalability across three dimensions.For world model training, we evaluate performance using 500 to 4,500 demonstrations.Our results show that increasing training data progressively reduces test MSE while simultaneously improving policy learning performance.In policy learning, expanding the training set from 10 demonstrations to complete data yields corresponding improvements in success rate.Notably, our approach consistently outperforms the previous SOTA [27] across all data scales, ultimately achieving a 90.7% average success rate (Avg.SR), i.e., a 3% improvement over Seer's best performance.Furthermore, scaling up the policy model architecture produces measurable gains on both the LIBERO-LONG and CALVIN benchmarks, further validating our method's scalability.</p>
<p>Ablation studies</p>
<p>We conducted a series of ablation experiments in simulated environments to evaluate the impact of our design choices.Except for necessary explanations, the ablations are conducted on the LIBERO-LONG with 10 training demonstrations for each task.We plot the learned relationship between the two types of dynamics in Fig. 4. According to the attention weight of the transformer layer, the most related features are connected by red lines.The result indicates that our interactive diffusion facilitates the alignment of two latent distributions.</p>
<p>Iterative refinement.We conduct an ablation study on the LIBERO-LONG benchmark to investigate the impact of interaction frequency during iterative refinement.The results are presented in Tab. 4, revealing that increasing the number of iterations (from 0 to 2) yields significant performance gains (27.9% in Avg.SR).However, performance plateaus beyond 2 iterations (3 to 4), suggesting that two rounds of imagination suffice for our model.</p>
<p>Other experiments.We also ablate the effects of latent diffusion and the influence of different imagined future frames.More details can be found in Appendix F.</p>
<p>Real-world Experiments</p>
<p>We conduct the manipulation experiment in the real world with a 7-DOF CR10 robot arm, as shown in Fig. 5. Same as in the simulated environment, we adopt two cameras to collect images from two views.The action space is the delta of the OSC pose.We define 7 tasks (details can be found in Appendix G) and use the same metric as LIBERO-LONG to evaluate our method.We finetune the policy model that is trained on the simulated environment using 10 realistic demonstrations per task.</p>
<p>Tab. 5 reports the detailed performance comparison.Compared to the vanilla behavior cloning method, our policy model obtains efficient guidance via imagination of the world model, achieving an improvement of 20% in average success rate.Moreover, we also ablate the latent and the diffusion designs in real-world experiments.The latent design and the diffusion design enhance the Avg.SR by 8.6% and 10.7% respectively.</p>
<p>Conclusion</p>
<p>We introduce LaDi-WM, a novel Latent Diffusion World Model that harnesses pre-trained visual foundation models to predict robotic future states in latent space.Our framework establishes distinct latent representations through DINO (geometric features) and SigLip (semantic features), combined with an iterative diffusion process that effectively models complex dynamics to generate highfidelity future state predictions.The tight integration between LaDi-WM and diffusion policies enables enhanced policy learning through latent-space future state conditioning.Comprehensive evaluations across both synthetic and real-world benchmarks demonstrate LaDi-WM's superior performance in augmenting diffusion-based policy effectiveness.These results demonstrate the significant potential of leveraging established latent representations for developing robust world models, advancing toward robotic systems capable of reliable long-horizon prediction.</p>
<p>Limitations</p>
<p>We propose a latent diffusion world model to learn general environmental dynamics, which can provide future imaginations as efficient guidance for solving various manipulation tasks.One limitation of our method lies in the limited scale of training samples.We plan to incorporate more training data [28,29] from diverse environments and tasks.On the other hand, longer-term future predictions can intuitively provide superior guidance for policy learning of long-horizon tasks, yet current world models face fundamental limitations in generating accurate extended predictions due to compounding error accumulation.To address this challenge, we identify the integration of memory mechanisms for maintaining long-horizon temporal consistency as a promising direction for future research.</p>
<p>Appendix</p>
<p>A Architecture Details World model.As shown on the left of Fig. 6, there are a noising process and a denoising process.We separately add the Gaussian noise to the DINO and Siglip latent codes in the noising process.</p>
<p>For the denoising process, we adopt an 8-layer transformer with a hidden dimension of 384 as the decomposition network.For the denoising network, we utilize a cross-attention layer to let the two types of latent representations interact with each other.</p>
<p>We use the mean square error (MSE) loss to train the world model, and the training objective is formulated by: min θ1,θ2,θ3,θ4
E q(zt+1:,0) E q(η) [∥z D t+1:,θ − z D t+1:,0 ∥ 2 + ∥η D θ − η D ∥ 2 E q(η) [∥z S t+1:,θ − z S t+1:,0 ∥ 2 + ∥η S θ − η S ∥ 2 + ∥C D θ − C D n ∥ 2 + ∥C S θ − C S n ∥ 2 ] (8)
where θ = [θ 1 , θ 2 , θ 3 , θ 4 ] are the parameters of the diffusion world model.</p>
<p>Policy network.The policy model is depicted in the right of Fig. 6.There are two processes in the diffusion policy: the noising and denoising processes.In the noising stage, we gradually add the Gaussian noise to the ground truth action until it is pure noise; while we leverage the transformer layers to learn the denoising process for recovering the action.We first tokenize the different modalities of inputs, which are fed into the transformer encoder and decoder to obtain the final output.</p>
<p>The transformer encoder and decoder contain 6 layers and 4 layers, and the hidden dimension is 256.We utilize the MSE loss between the denoised action and the ground truth action to train the policy model.Implementation details.The length l of historical videos is fixed at 4 frames, while the length of the imagination is ablated in Sec.4.2.The world model is trained with a batch size of 4 on an NVIDIA 4090 GPU, which requires approximately three days.We utilize the same device to train the policy model with a batch size of 24, which takes approximately six hours.The raw image resolution is 128 × 128, and the patch size is 14×14.The action space is seven-dimensional, representing the delta of translation, rotation, and aperture of the end-effector.</p>
<p>B Inference procedure</p>
<p>In the inference stage, we first let the policy model generate an initial action, which is then fed into the world model to obtain the imagination guidance.Since the initial action may be suboptimal, the Algorithm 1 Pseudocode of inference procedure.</p>
<p>1: Initialize t = 0, environment env, policy network π θ , world model p θ , done = False, max step = 600, instruction T , refine iterations m. 2: Reset env: z 0 , e 0 = env.reset(),a 0:k = 0; 3: while not done | t &lt; max step do ẑt+1:t+k+1 = p θ (z t−l:t , a t:t+k );
t = t + 1;
15: end while corresponding imagination guidance may not be accurate enough to provide effective guidance.We adopt an iterative correction mechanism to refine the action based on progressively more accurate imagined states generated by the world model.This closed-loop interaction between policy refinement and world model prediction enables increasingly precise action optimization, leading to more robust and effective policy execution.The pseudocode is shown in Alg. 1.</p>
<p>C Experiment Setup</p>
<p>C.1 Benchmarks and Evaluation Metrics LIBERO-LONG is designed for language-conditioned long-horizon manipulation tasks and provides various suites with expert human demonstrations.We train the world model on the LIBERO-90 dataset, which includes demonstrations for 90 short-horizon tasks.Note that the world model does not access the tasks of policy learning.Following Tian et al. [27], we report the average success rate (Avg.SR) of the top-3 checkpoints over 20 rollouts for the evaluation.Here is the list of tasks: • Task1: Turn on the stove and put the moka pot on it.</p>
<p>• Task2: Put the black bowl in the bottom drawer of the cabinet and close it.</p>
<p>• Task3: Put the yellow and white mug in the microwave and close it.</p>
<p>• Task4: Put both moka pots on the stove.• Task5: Put both the alphabet soup and the cream cheese box in the basket.</p>
<p>• Task6: Put both the alphabet soup and the sauce in the basket.</p>
<p>• Task7: Put both the cream cheese box and the butter in the basket.• Task8: Put the white mug on the left plate and put the yellow and white mug on the right plate.</p>
<p>• Task9: Put the white mug on the plate and put the chocolate pudding to the right of the plate.</p>
<p>• Task10: Pick up the book and place it in the back compartment of the caddy.</p>
<p>CALVIN D-D is a benchmark focusing on language-conditioned visual robot manipulation, which contains 34 tasks varying in object and scene visual appearance.To make the training data of the world model different from that of the policy model, we selected half of the tasks, including 'push', 'move', 'open', and 'place', to train the world model.The remaining tasks, such as 'lift', 'rotate', and 'stack', are used to evaluate policy learning.Following Tian et al. [27], we report the average success rate and the average length (Avg.Len) of completed sequences.</p>
<p>C.2 Baseline methods</p>
<p>• DreamerV3 [20] and TDMPC2 [5] employ learned world models for reinforcement learning and planning.While these methods also operate in latent space, their representations are optimized for image reconstruction or reward prediction, neglecting critical geometric and semantic features essential for manipulation tasks.In contrast, our approach harnesses visual foundation models to derive latent representations that explicitly capture both geometric and semantic dynamics, enabling more effective learning for manipulation.In particular, since DreamerV3 and TDMPC2 require dense rewards for world model training, we follow Tang et al. [30] to design appropriate dense rewards for manipulation tasks.Additionally, given that these reinforcement learning-based methods struggle with manipulation tasks, we initialize their policies using behavior cloning.</p>
<p>• ATM [1].ATM employs a language-conditioned trajectory prediction model that provides point-based trajectories as policy inputs.However, this approach faces two key limitations: (1) the language grounding inherently restricts generalization to unseen tasks, and</p>
<p>(2) the sparse point representation fails to capture critical observation details.Differently, our world model offers two distinct advantages: (1) it learns from task-agnostic video clips, enabling broader task generalization; (2) it operates in a rich latent space preserving geometric and semantic information.</p>
<p>• Seer [27].Seer represents the previous state-of-the-art method on both the LIBERO-LONG and CALVIN benchmarks.The approach introduces an inverse dynamics model that jointly predicts actions and subsequent frame images.However, this implicit dynamics model cannot directly provide future state guidance for policy learning.In contrast, our world model allows the policy model to obtain future states through imagination, enabling iterative refinement of the action prediction.</p>
<p>D More Quantitative Comparisons</p>
<p>Comparison on LIBERO-LONG with full data.While the main paper demonstrates our method's superiority under limited action-labeled data conditions, we additionally report full-data performance to establish the upper bound of our approach.As shown in Tab.6, our method achieves the best average success rate of 90.7%, representing a 3% absolute improvement over the previous state-of-the-art.</p>
<p>Cross-scene generalization with world model.To assess the generalization capacity of our world model, we directly transfer the model trained on the LIBERO benchmark to policy learning on the CALVIN benchmark, despite their distinct scene configurations.As demonstrated in the fifth row of Tab. 7, our cross-scene implementation achieves an average sequence length (Avg.Len.) of 3.05,</p>
<p>E Analysis of Iterative Refinement</p>
<p>The proposed latent diffusion-based world model allows the policy model to refine itself iteratively, and the results in the main paper show it can significantly surpass the vanilla BC by 27.9%.Here we conduct a series of experiments to analyze the effect of iterative refinement.</p>
<p>Why does the iterative refinement improve the success rate?Generally speaking, the output of the diffusion policy is not a deterministic value but a stochastic distribution.We claim that the iterative refinement can gradually reduce the entropy of the predicted distribution, thereby aligning predictions more closely with ground truth at each execution step and consequently improving task success rates.To validate this claim, we perform 10 repeated policy evaluations under different iteration settings.We leverage PCA to reduce the action dimension and Kernel Density Estimation to plot 4 distributions.As shown in Fig. 7, as the number of iterations increases, the entropy of the predicted distribution gradually reduces.7: Predicted action distribution.</p>
<p>Convergence of iterative refinement.We conduct additional experiments with varying numbers of training demonstrations to further verify convergence properties.As shown in Tab. 8, our method consistently achieves convergence within 2 refinement iterations across all experimental settings.Furthermore, the observed variance continuously decreases with increasing iterations, demonstrating that iterative refinement enhances performance through increasingly precise (low-entropy) predictions.</p>
<p>F Ablation Studies</p>
<p>Effect of the world model.We employ an imagination-guided policy based on behavior cloning (BC).To intuitively demonstrate the effect of world model guidance, we compare our method with a vanilla behavior cloning policy that solely relies on historical observations as inputs.Besides, we implement a comparison model that utilizes a copy of the current observation as the future imagination, and we call it the copy model.Our world model, which serves as a simulated environment, can provide future imagined states as efficient guidance.As reported in Tab. 9, the copy model can not bring improvement and is even slightly worse than vanilla behavior cloning.In contrast, our approach improves the vanilla behavior cloning method by 27.9%, highlighting the significant potential of imagination-guided policy learning.</p>
<p>Latent diffusion vs. image pixel diffusion.Unisim [3] proposes a pixel diffusion world model; however, it is not open-source.To ensure a fair comparison, we also employ our diffusion world model in the pixel space, where we retain the network architecture but remove the latent representation extraction stage.Tab. 10 presents a detailed performance comparison.Our method learns the diffusion process in a compact and more generalizable latent space, leveraging the features of pretrained foundation models, which results in robust dynamic predictions for unseen tasks.In contrast, the pixel diffusion world model exhibits inferior performance, with a 14.7% drop in the average success rate.Effect of the imagined future frames.The imagination-guided policy network takes the future latent states generated by the proposed world model as inputs.We conduct a series of experiments to investigate the impact of varying numbers of imagined latent state frames, and the results are presented in Tab.11.As the number of frames increases, the average success rate gradually improves, peaking at 6 frames.We hypothesize that excessively long future imagination may become unreliable due to compounding errors.</p>
<p>Effect of different denoising steps.For the policy network, we utilize ADM [23] as our diffusion algorithm, which introduces an analytical attenuation process to speed up the denoising process.We report the results of different denoising steps in Tab.11.Benefiting from the fast diffusion algorithm, our policy network can achieve a convergent performance with only 2 denoising steps.</p>
<p>Effect of different robotic states.There are two common options for the robotic states as inputs to the policy network: joint states and end-effector states (EE states).As shown in Tab. 12, the use of joint states outperforms EE states by 9.0%, suggesting that joint states are more suitable for solving manipulation tasks.More visual results of manipulation trajectory.We depict more visual trajectories of the realworld manipulation results in Fig. 8 and Fig. 9. Benefiting from the powerful dynamic modeling ability of the world model, our method can deal with complex lighting changes.</p>
<p>2 Figure 1 :
21
Figure 1: Our method consists of a latent diffusion-based world model and a corresponding predictive manipulation policy.(a): 1 ⃝ Given an initial action sequence generated by the policy, our world model produces imagined future states.2 ⃝ These imagined states provide efficient guidance for the policy model, 3 ⃝ yielding refined actions for improved manipulation.(b): Benefiting from our world model, we achieve significant improvements in both simulation and real-world performance.</p>
<p>Figure 2 :
2
Figure2: Overall framework of the proposed method.We first train LaDi-WM on task-agnostic clips, which benefits cross-task generalization.The trained world model is used to generate future imagined states, which serve as effective guidance and input to the policy network, improving the manipulation performance significantly.</p>
<p>Figure 3 :
3
Figure 3: (a) scaling up the training data of the world model; (b) scaling up the training data of the policy model; (c) scaling up the model size of the policy model.</p>
<p>Table 3 :
3
Ablation study of different world model architecture, evaluated with 1-time refinement.Method Task1 Task2 Task3 Task4 Task5 Task6 Task7 Task8 Task9 Task10 Avg.SR without diffusion 78.3 75.0 45.0 40.0 51.7 50.0 45.0 66.7</p>
<p>Figure 4 :
4
Figure 4: Visual analysis of interactive diffusion.Without the interactive diffusion, the latent codes exhibit connections to distinct regions (left).In contrast, interactive diffusion consistently aligns connections to identical objects (right).Ablation study of world model architecture.As shown in the first row of Tab. 3, our method outperforms the transformer-only architecture by 7.8%.As shown in the second row of Tab. 3, incorporating semantic information leads to a 3.4% improvement in average success rate, particularly for Task 1, Task 5, and Task 9, which involve objects with distinct semantic features.As shown in the third row, the interactive diffusion process outperforms the concatenation method by 1.8%, demonstrating its ability to learn more accurate dynamic predictions.</p>
<p>Table 5 :
5
Real-world performance of variants.</p>
<p>Figure 5 :
5
Figure 5: Real-world experiments: left shows the setup and right shows several execution examples.</p>
<p>Figure 6 :
6
Figure 6: Detailed architecture of the world model and policy model.</p>
<p>Table 12 : 7 G
127
Ablation on different robotic states as the inputs of the policy network.The results are obtained with 1-iter refinement.More Details of Real-world experiments Task definition.We define 7 manipulation tasks in the real scene, in order: 'stack the middle bowl on the back bowl', 'put the bowl inside the middle drawer of the cabinet and close it', 'open the top drawer and put the bowl inside', 'pick up the milk and place it in the basket', 'pick up the cans and place it in the basket', 'pick up the orange juice and place it in the basket', 'pick up the ketchup and place it in the basket'.</p>
<p>Table 1 :
1
Performance on LIBERO-LONG benchmark.
MethodTask1 Task2 Task3 Task4 Task5 Task6 Task7 Task8 Task9 Task10 Avg.SR↑DreamerV3 [20] 38.3 21.7 33.3 26.7 30.0 28.3 41.7 31.7 33.3 50.033.5TDMPC2 [5]45.0 36.7 35.0 31.7 31.7 23.3 36.7 40.0 38.3 51.737.0ATM [1]46.7 58.3 60.0 31.7 33.3 20.0 43.3 51.7 41.7 53.344.0Seer [27]71.7 50.0 48.3 51.7 66.7 53.3 51.7 45.0 48.3 50.053.6Ours88.3 68.3 63.3 45.0 83.3 65.0 78.3 63.3 60.0 71.768.7</p>
<p>Table 2 :
2
Performance on CALVIN D-D benchmark.
Method12Task completed in a row 3 4 5Avg.Len.↑Vanilla BC81.460.845.932.124.22.44DreamerV3 [20]82.063.146.634.125.12.51ATM [1]83.370.958.546.339.12.98Seer [27]92.282.671.560.653.53.60Ours92.783.172.161.254.13.63
[27]ls trained using only 10 demonstrations per task.Notably, our proposed world model achieves significant improvements in policy learning, despite being trained on dynamics unseen during policy training.Compared to the previous SOTA[27], our approach improves the average success rate by 15.1%.This advancement demonstrates the effectiveness of our framework.We provide more results of LIBERO-LONG benchmark in Appendix D.CALVIN D-D benchmark results.Tab. 2 reports the performance comparison on the CALVIN D-D benchmark.Firstly, compared to vanilla behavior cloning (BC) without the world model, our method significantly improves the Avg.Len.by 1.19.Moreover, our method surpasses other methods in terms of Avg.Len.and success rate, demonstrating the superiority of the predictive policy learning.</p>
<p>Table 4 :
4
Ablation study of iteration in refinement.
MethodTask1 Task2 Task3 Task4 Task5 Task6 Task7 Task8 Task9 Task10 Avg.SR↑without imagination 66.7 63.3 31.7 40.0 35.0 41.7 35.0 43.3 25.0 26.740.81-iter imagination90.0 83.3 50.0 45.0 66.7 55.0 55.0 71.7 50.0 40.0 60.7 (+19.9)2-iter imagination88.3 68.3 63.3 45.0 83.3 65.0 78.3 63.3 60.0 71.7 68.7 (+27.9)3-iter imagination90.0 76.7 58.3 48.3 78.3 63.3 71.7 66.7 58.3 68.3 67.9 (+27.1)4-iter imagination90.0 81.7 60.0 46.7 75.0 61.7 71.7 70.0 58.3 66.7 68.2 (+27.4)</p>
<p>t:t+k = π θ (z t−l:t , e t−l:t , t , e t , done = env.step(at );
8:# get action:9:10:ẑt+1:t+k+1 , T );11:end for12:# environment step:13:14:
a z</p>
<p>Table 6 :
6
Performance comparisons on LIBERO-LONG benchmark with all training data.
MethodAvg.SR↑Put bowl in drawer and close itPut mug in microwave and close itPut both pots on stovePut soup and box in basketDreamerV339.358.325.030.035.0TDMPC246.861.733.340.036.7OpenVLA [31]54.045.055.020.035.0ATM59.683.345.043.360.0Seer87.710071.761.791.7Ours90.793.395.091.795.0Put soupPut boxPut mugs onPut mug onPick bookTurn onand sauceand bufferleft andplate and putand placestove andin basketin basketright platespudding to rightit in backput pot43.333.345.036.730.056.745.046.758.345.040.061.745.095.040.060.080.065.053.355.063.360.045.088.388.390.091.785.093.398.388.390.085.085.088.395.0</p>
<p>Table 7 :
7
Comparisons against state-of-the-art methods on CALVIN D-D benchmark.
Method12Task completed in a row 3 4 5Avg.Len.↑Vanilla BC81.460.845.932.124.22.44DreamerV3 [20]82.063.146.634.125.12.51ATM [1]83.370.958.546.339.12.98Seer [27]92.282.671.560.653.53.60Ours (cross-scene)84.172.260.348.540.33.05Ours92.783.172.161.254.13.63surpassing both vanilla behavior cloning (BC) by 0.61 and ATM by 0.07. These results substantiateour world model's ability to generalize across different environmental scenarios.</p>
<p>Table 8 :
8
Performances of iterative refinement trained with different data scales.The results are obtained under 10 repeated evaluations.
Data scale 10 demo 20 demo 30 demo1-iter59.9±7.23 75.1±6.65 80.4±4.682-iter68.1±4.38 77.3±4.63 81.5±3.823-iter68.3±3.25 77.5±4.01 81.5±2.634-iter68.6±2.51 77.5±3.12 81.7±1.93Figure</p>
<p>Table 9 :
9
Ablation on the effect of the world model.
MethodTask1 Task2 Task3 Task4 Task5 Task6 Task7 Task8 Task9 Task10 Avg.SRBC66.7 63.3 31.7 40.0 35.0 41.7 35.0 43.3 25.026.740.8Copy model 68.3 40.0 51.7 36.7 41.7 26.7 50.0 20.0 30.036.740.2Ours88.3 68.3 63.3 45.0 83.3 65.0 78.3 63.3 60.071.768.7</p>
<p>Table 10 :
10
Ablation on different diffusion spaces.
MethodTask1 Task2 Task3 Task4 Task5 Task6 Task7 Task8 Task9 Task10 Avg.SRPixel diffusion 83.3 78.3 50.0 35.0 61.7 50.0 45.0 65.0 36.735.054.0Ours88.3 68.3 63.3 45.0 83.3 65.0 78.3 63.3 60.071.768.7</p>
<p>Table 11 :
11
Ablation on imagination frames and denoising steps.The results are obtained with 1-iter refinement.
Frames12468Denoising step1251030Avg.SR 44.9 49.6 56.3 60.6 60.0 Avg.SR60.0 60.6 60.5 60.3 60.6
AcknowledgementThis work is supported in part by the NSFC (62325211, 62132021, 62372457, 62322207), the Major Program of Xiangjiang Laboratory (23XJ01009).
Any-point trajectory modeling for policy learning. C Wen, X Lin, J I R So, K Chen, Q Dou, Y Gao, P Abbeel, Robotics: Science and Systems. 2024</p>
<p>Learning to act from actionless videos through dense correspondences. P Ko, J Mao, Y Du, S Sun, J B Tenenbaum, International Conference on Learning Representations, ICLR. 2024</p>
<p>Learning interactive real-world simulators. S Yang, Y Du, S K S Ghasemipour, J Tompson, L P Kaelbling, D Schuurmans, P Abbeel, International Conference on Learning Representations, ICLR. 2024</p>
<p>Dino-wm: World models on pre-trained visual features enable zero-shot planning. G Zhou, H Pan, Y Lecun, L Pinto, arXiv:2411.049832024arXiv preprint</p>
<p>TD-MPC2: scalable, robust world models for continuous control. N Hansen, H Su, X Wang, International Conference on Learning Representations, ICLR. 2024</p>
<p>Transformers are sample-efficient world models. V Micheli, E Alonso, F Fleuret, The Eleventh International Conference on Learning Representations, ICLR. 2023</p>
<p>Dream to control: Learning behaviors by latent imagination. D Hafner, T P Lillicrap, J Ba, M Norouzi, International Conference on Learning Representations. 20202020</p>
<p>Mastering atari with discrete world models. D Hafner, T P Lillicrap, M Norouzi, J Ba, International Conference on Learning Representations, ICLR. 2021</p>
<p>Grounding dino: Marrying dino with grounded pre-training for open-set object detection. S Liu, Z Zeng, T Ren, F Li, H Zhang, J Yang, Q Jiang, C Li, J Yang, H Su, European Conference on Computer Vision. Springer2024</p>
<p>Learning robust visual features without supervision. M Oquab, T Darcet, T Moutakanni, H Vo, M Szafraniec, V Khalidov, P Fernandez, D Haziza, F Massa, A El-Nouby, Trans. Mach. Learn. Res. 20242024</p>
<p>Cyclip: Cyclic contrastive language-image pretraining. S Goel, H Bansal, S Bhatia, R Rossi, V Vinay, A Grover, Advances in Neural Information Processing Systems. 202235</p>
<p>Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation. J Li, D Li, C Xiong, S Hoi, International conference on machine learning. PMLR2022</p>
<p>Sigmoid loss for language image pretraining. X Zhai, B Mustafa, A Kolesnikov, L Beyer, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer Vision2023</p>
<p>. D Ha, J Schmidhuber, arXiv:1803.101222018World models. arXiv preprint</p>
<p>An on-line algorithm for dynamic reinforcement learning and planning in reactive environments. J Schmidhuber, IJCNN international joint conference on neural networks. IEEE1990</p>
<p>Reinforcement learning in markovian and non-markovian environments. Advances in neural information processing systems. J Schmidhuber, 19903</p>
<p>Imagination-augmented agents for deep reinforcement learning. S Racanière, T Weber, D Reichert, L Buesing, A Guez, D Jimenez Rezende, A Puigdomènech, O Badia, N Vinyals, Y Heess, Li, Advances in neural information processing systems. 2017</p>
<p>Diffusion world model: Future modeling beyond step-by-step rollout for offline reinforcement learning. Z Ding, A Zhang, Y Tian, Q Zheng, arXiv:2402.035702024arXiv preprint</p>
<p>Diffusion for world modeling: Visual details matter in atari. E Alonso, A Jelley, V Micheli, A Kanervisto, A J Storkey, T Pearce, F Fleuret, Advances in Neural Information Processing Systems. 202437</p>
<p>Mastering diverse domains through world models. D Hafner, J Pasukonis, J Ba, T Lillicrap, arXiv:2301.041042023arXiv preprint</p>
<p>J Schulman, F Wolski, P Dhariwal, A Radford, O Klimov, arXiv:1707.06347Proximal policy optimization algorithms. 2017arXiv preprint</p>
<p>Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor. T Haarnoja, A Zhou, P Abbeel, S Levine, International conference on machine learning. Pmlr2018</p>
<p>Simultaneous image-to-zero and zero-to-noise: Diffusion models with analytical image attenuation. Y Huang, Z Qin, X Liu, K Xu, arXiv:2306.137202023arXiv preprint</p>
<p>Scalable diffusion models with transformers. W Peebles, S Xie, Proceedings of the IEEE/CVF international conference on computer vision. the IEEE/CVF international conference on computer vision2023</p>
<p>Libero: Benchmarking knowledge transfer for lifelong robot learning. B Liu, Y Zhu, C Gao, Y Feng, Q Liu, Y Zhu, P Stone, Advances in Neural Information Processing Systems. 202436</p>
<p>Calvin: A benchmark for languageconditioned policy learning for long-horizon robot manipulation tasks. O Mees, L Hermann, E Rosete-Beas, W Burgard, IEEE Robotics and Automation Letters (RA-L). 732022</p>
<p>Predictive inverse dynamics models are scalable learners for robotic manipulation. Y Tian, S Yang, J Zeng, P Wang, D Lin, H Dong, J Pang, International Conference on Learning Representations. 2025</p>
<p>Open x-embodiment: Robotic learning datasets and rt-x models: Open x-embodiment collaboration 0. A O'neill, A Rehman, A Maddukuri, A Gupta, A Padalkar, A Lee, A Pooley, A Gupta, A Mandlekar, A Jain, 2024 IEEE International Conference on Robotics and Automation (ICRA). IEEE2024</p>
<p>A Khazatsky, K Pertsch, S Nair, A Balakrishna, S Dasari, S Karamcheti, S Nasiriany, M K Srirama, L Y Chen, K Ellis, arXiv:2403.12945A large-scale in-the-wild robot manipulation dataset. 2024arXiv preprint</p>
<p>Automate: Specialist and generalist assembly policies over diverse geometries. B Tang, I Akinola, J Xu, B Wen, A Handa, K V Wyk, D Fox, G S Sukhatme, F Ramos, Y S Narang, Robotics: Science and Systems. D Kulic, G Venture, K E Bekris, E Coronado, 2024</p>
<p>Openvla: An open-source vision-language-action model. M J Kim, K Pertsch, S Karamcheti, T Xiao, A Balakrishna, S Nair, R Rafailov, E P Foster, P R Sanketi, Q Vuong, T Kollar, B Burchfiel, R Tedrake, D Sadigh, S Levine, P Liang, C Finn, Conference on Robot Learning. 2024</p>            </div>
        </div>

    </div>
</body>
</html>
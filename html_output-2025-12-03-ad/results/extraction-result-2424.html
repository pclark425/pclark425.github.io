<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2424 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2424</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2424</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-66.html">extraction-schema-66</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated research systems, AI scientists, or automated idea generation/implementation systems, including details about what types of research problems they were applied to, the characteristics of those problems, and the success or failure rates of the automated system.</div>
                <p><strong>Paper ID:</strong> paper-273695165</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2410.23166v1.pdf" target="_blank">SciPIP: An LLM-based Scientific Paper Idea Proposer</a></p>
                <p><strong>Paper Abstract:</strong> The exponential growth of knowledge and the increasing complexity of interdisciplinary research pose significant challenges for researchers, including information overload and difficulties in exploring novel ideas. The advancements in large language models (LLMs), such as GPT-4, have shown great potential in enhancing idea proposals, but how to effectively utilize large models for reasonable idea proposal has not been thoroughly explored. This paper proposes a scientific paper idea proposer (SciPIP). Based on a user-provided research background, SciPIP retrieves helpful papers from a literature database while leveraging the capabilities of LLMs to generate more novel and feasible ideas. To this end, 1) we construct a literature retrieval database, extracting lots of papers' multi-dimension information for fast access. Then, a literature retrieval method based on semantics, entity, and citation co-occurrences is proposed to search relevant literature from multiple aspects based on the user-provided background. 2) After literature retrieval, we introduce dual-path idea proposal strategies, where one path infers solutions from the retrieved literature and the other path generates original ideas through model brainstorming. We then combine the two to achieve a good balance between feasibility and originality. Through extensive experiments on the natural language processing (NLP) field, we demonstrate that SciPIP can retrieve citations similar to those of existing top conference papers and generate many ideas consistent with them. Additionally, we evaluate the originality of other ideas generated by SciPIP using large language models, further validating the effectiveness of our proposed method. The code and the database are released at https://github.com/cheerss/SciPIP.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2424.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2424.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated research systems, AI scientists, or automated idea generation/implementation systems, including details about what types of research problems they were applied to, the characteristics of those problems, and the success or failure rates of the automated system.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SciPIP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Scientific Paper Idea Proposer (SciPIP)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LLM-based system that, given a user-provided research background, retrieves relevant literature from a prebuilt database and generates novel and feasible scientific paper ideas via single- and dual-path generation with filtering and refinement.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>SciPIP</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>SciPIP builds a literature database (48,895 NLP papers from major venues), extracts multi-dimensional paper information with an LLM (entities, background, summary, main ideas, detailed ideas, core references), encodes text with Sentence-BERT, and performs SEC-based retrieval (Semantics, Entities, Citation co-occurrence). Retrieved papers are clustered to reduce redundancy. Idea generation uses three pipelines: direct proposal (SciPIP-A) and two dual-path methods (SciPIP-B and SciPIP-C) that combine literature-inspired idea generation and LLM brainstorming; generated ideas are filtered and refined by LLM prompts. GLM-4 is used for paper parsing/summarization; GPT-4o is used for idea generation and evaluation in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>Automated Idea Generation System / Hypothesis Generation System</td>
                        </tr>
                        <tr>
                            <td><strong>problem_domain</strong></td>
                            <td>Machine learning / Natural Language Processing (LLM-assisted scientific ideation and literature retrieval)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Given a textual research background, produce novel and feasible scientific paper ideas and retrieve the most relevant prior literature to (a) inspire feasible ideas and (b) avoid redundant proposals; evaluate idea novelty and match to existing conference papers (ACL 2024) and retrieval of core citations.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>High: multi-modal semantic matching across ~48.9k papers, large discrete search space of potential ideas (combinatorial creativity), balancing novelty vs. feasibility (multi-objective), and avoiding redundant/retrieval-noise; experimental evaluation covers thousands of candidate ideas (e.g., SciPIP-C produced 7,638 ideas in retrospective tests).</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Large pre-collected literature database (48,895 papers from ICLR, NeurIPS, ICML, ACL, NAACL, EMNLP over ten years). Data quality: parsed PDF sections (title, abstract, intro, method, references); some PDFs failed parsing (small number excluded). No additional labeled idea data required; evaluation uses ACL 2024 papers as ground truth.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_requirements</strong></td>
                            <td>LLM inference at scale (GLM-4 for parsing/summarization, Sentence-BERT for embeddings, GPT-4o for idea generation/evaluation), database storage and embedding search (Top-K ~55 per query), clustering of retrieved sets (hundreds of papers), and repeated LLM-based filtering/refinement; the paper does not state exact compute hours or costs.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_structure</strong></td>
                            <td>Semi-structured and partly open-ended: retrieval and idea-matching are well-defined (semantic embeddings, cosine similarity, clustering), idea generation is open-ended and creative (stochastic LLM output). Evaluation metrics (similarity to published ideas, novelty scores, retrieval recall) are defined; domain knowledge encoded via literature database and entity extraction.</td>
                        </tr>
                        <tr>
                            <td><strong>success_metric</strong></td>
                            <td>Multiple metrics: (1) idea matching to ACL 2024 (LLM-assigned similarity scores 0-5, focusing on high matches e.g., score=4), (2) novelty scores assigned by GPT-4o after Semantic Scholar comparisons (0-10), and (3) literature retrieval recall@K (Recall@10,20,30,40,50).</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Retrieval: SciPIP recall@10 = 0.419, recall@50 = 0.684 (better than SCIMON-like and ResearchAgent-like baselines: recall@10 0.381 and 0.377 respectively). Idea-matching: across methods, SciPIP on average generates ~4–5 highly-matching ideas (high-similarity matches) per 100 input backgrounds (i.e., ~4–5% per 100). Novelty: SciPIP produced many high-novelty ideas; e.g., SciPIP-A generated 92 ideas with novelty score 9 (GPT-4o evaluation) in a 100-background test vs. AI Scientist's 12.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Generated ideas can be overly literature-dependent (close paraphrases of retrieved papers) or, conversely, brainstorming-only ideas may rely on LLM prior knowledge and not increase novelty; some LLM prompts/filter stages still admit low-relevance ideas which are filtered out; parsing failures for some PDFs reduce database coverage; the paper notes unexplained phenomena (e.g., ideas with lower similarity to existing literature sometimes rated more novel) requiring further study.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Key contributors: (1) a large prebuilt literature database enabling fast, multi-granularity retrieval; (2) SEC retrieval combining semantics, entity matching and citation co‑occurrence; (3) clustering-based redundancy reduction; (4) dual-path idea generation that balances feasibility (literature-driven path) and originality (brainstorming path); (5) iterative LLM-based filtering and refinement; and (6) use of strong LLMs (GLM-4 for parsing, GPT-4o for generation/evaluation).</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td>Compared to SCIMON-like and ResearchAgent-like retrieval, SciPIP achieves higher retrieval recall across K: Recall@10 (SciPIP 0.419 vs SCIMON-like 0.381 vs ResearchAgent-like 0.377) and Recall@50 (SciPIP 0.684 vs SCIMON-like 0.616 vs ResearchAgent-like 0.622). For idea novelty, SciPIP variants produced substantially more very-high novelty (score 9) ideas in 100-background tests (SciPIP-A 92, B 63, C 67) than AI Scientist (12). For idea matching to ACL 2024, SciPIP methods yielded ~4–5 high-similarity matches per 100 backgrounds while AI Scientist's generated ideas had maximum similarity score typically ≤3.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline</strong></td>
                            <td>Human baseline is represented by the ACL 2024 papers themselves (the corpus of human-authored ideas). SciPIP's retrospective experiment demonstrates that it can reproduce a small fraction of those human ideas: on average ~4–5 highly-matching ideas per 100 input backgrounds (i.e., a modest fraction compared to the full set of human-published ideas).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SciPIP: An LLM-based Scientific Paper Idea Proposer', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2424.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2424.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated research systems, AI scientists, or automated idea generation/implementation systems, including details about what types of research problems they were applied to, the characteristics of those problems, and the success or failure rates of the automated system.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SCIMON</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SCIMON (Scientific Inspiration Machines Optimized for Novelty)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A retrieval-augmented LLM system that retrieves 'inspiration' papers and iteratively optimizes generated ideas for novelty by comparing to prior literature and updating until sufficient novelty is achieved.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Scimon: Scientific inspiration machines optimized for novelty</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>SCIMON (as baseline / related work)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>SCIMON retrieves relevant past papers as inspiration and explicitly optimizes LLM-generated ideas for novelty via iterative comparison against retrieved literature, attempting to maximize novelty while maintaining feasibility. In this paper, the authors implemented a SCIMON-like retrieval method for baseline comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>Hypothesis Generation System / Idea Proposal System</td>
                        </tr>
                        <tr>
                            <td><strong>problem_domain</strong></td>
                            <td>Machine learning / NLP idea generation and novelty optimization</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Generate novel scientific hypotheses/ideas by retrieving influential prior works and pushing generated ideas away from existing literature until novelty thresholds are met.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Medium-to-high: requires retrieval over large literature sets and iterative LLM comparison loops; novelty optimization implies a non-convex, discrete creative search with human-meaningful novelty constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Operates over literature databases; performance depends on coverage of prior works. In this paper it was evaluated using the same or comparable literature corpus as SciPIP (implemented as SCIMON-like baseline).</td>
                        </tr>
                        <tr>
                            <td><strong>computational_requirements</strong></td>
                            <td>Iterative retrieval + LLM inference loops for novelty checking; no exact compute costs reported in this paper's baseline implementation.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_structure</strong></td>
                            <td>Open-ended creative generation constrained by novelty metrics derived from literature similarity; relies on clear retrieval and similarity measures but open creative output.</td>
                        </tr>
                        <tr>
                            <td><strong>success_metric</strong></td>
                            <td>Novelty of generated ideas (distance from retrieved literature), and indirect metrics like whether generated ideas match human-published ideas in retrospective tests.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Implemented as SCIMON-like baseline in this paper: retrieval recall@10 = 0.381 and recall@50 = 0.616 (lower than SciPIP's retrieval). The paper did not report SCIMON's idea-matching or novelty counts directly beyond using it as a retrieval baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Lower retrieval recall compared to SciPIP when run as a SCIMON-like baseline; potential to over-optimize for novelty at the expense of feasibility (general SCIMON observation elsewhere), sensitivity to retrieval quality.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Effectiveness depends on retrieval of diverse 'inspiration' papers and iterative novelty comparison mechanisms.</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td>When implemented as a baseline, SCIMON-like retrieval trails SciPIP's SEC+clustering approach (recall@10: 0.381 vs 0.419). The paper uses SCIMON primarily as a conceptual and baseline comparator rather than a full direct head-to-head in idea-generation metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline</strong></td>
                            <td>Not directly compared to human performance in this paper beyond being a baseline retrieval/generation approach; implemented only for retrieval comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SciPIP: An LLM-based Scientific Paper Idea Proposer', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2424.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2424.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated research systems, AI scientists, or automated idea generation/implementation systems, including details about what types of research problems they were applied to, the characteristics of those problems, and the success or failure rates of the automated system.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ResearchAgent</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Research Agent (ResearchAgent)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An agentic LLM-based research system that expands knowledge from a core paper via an academic graph and uses an entity-centric knowledge store and multiple reviewing agents to iteratively generate and refine research ideas.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Researchagent: Iterative research idea generation over scientific literature with large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>ResearchAgent</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>ResearchAgent begins from a core paper and expands context by traversing an academic graph, retrieves entities from an entity-centric store, and employs multiple reviewer agents to iteratively review and refine ideas; designed for iterative, agent-driven research ideation. In this paper the authors implemented a ResearchAgent-like retrieval baseline for comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>Agentic Research Idea Generation System</td>
                        </tr>
                        <tr>
                            <td><strong>problem_domain</strong></td>
                            <td>Machine learning / automated literature-based idea expansion and review</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Automatically expand a starting paper into broader directions and generate iterative research ideas using graph expansions, entity retrieval, and reviewer-agent feedback loops.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>High: graph traversal over large academic graphs, multi-agent iterative refinement, and combining entity-level retrieval with higher-level concept linking.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Requires an academic graph and entity-centric knowledge store; in the paper a ResearchAgent-like method was simulated/implemented for retrieval baselines using the available literature database.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_requirements</strong></td>
                            <td>Graph traversal, multiple agent LLM inferences, and retrieval—no explicit compute numbers provided in this paper's baseline implementation.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_structure</strong></td>
                            <td>Semi-structured: starts from a discrete core seed paper but expands into open-ended idea generation via graph and agent interactions; evaluation relies on retrieval metrics and idea quality judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>success_metric</strong></td>
                            <td>Retrieval recall and idea quality/novelty when compared to baselines; the paper used retrieval recall@K for baseline comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>As a ResearchAgent-like baseline, retrieval recall@10 = 0.377 and recall@50 = 0.622 in this paper (both lower than SciPIP's reported recall). Idea-generation metrics for ResearchAgent were not directly reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>In the baseline implementation, lower retrieval recall than SciPIP; possible sensitivity to graph coverage and entity extraction quality; complexity and reliance on graph completeness can limit performance.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Quality and coverage of the academic graph and entity store and the efficacy of reviewer/critic agents for refinement.</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td>ResearchAgent-like retrieval underperforms SciPIP's SEC+clustering approach on recall@K metrics (recall@10: 0.377 vs 0.419). The paper uses ResearchAgent mainly as a conceptual comparator and implemented a 'ResearchAgent-like' retrieval baseline rather than running the original system end-to-end.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline</strong></td>
                            <td>Not directly provided; ResearchAgent aimed to automate aspects of human literature exploration and iterative refinement but in this paper serves as a baseline comparator.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SciPIP: An LLM-based Scientific Paper Idea Proposer', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2424.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2424.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated research systems, AI scientists, or automated idea generation/implementation systems, including details about what types of research problems they were applied to, the characteristics of those problems, and the success or failure rates of the automated system.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AI Scientist</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>The AI Scientist (Lu et al., 2024)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A recent system that aims for fully automated open-ended scientific discovery by autonomously generating ideas, implementing and executing experiments, searching related work, and producing full research papers in machine learning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>The AI scientist: Towards fully automated open-ended scientific discovery</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>AI Scientist</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>AI Scientist is designed to automate the full research loop: ideation, experimental implementation and execution, literature search, iterative refinement, and paper writing. In this paper, SciPIP is compared against AI Scientist for idea-proposal quality; the authors adapted AI Scientist's idea-refinement process for a limited comparison (using an initial idea drawn from retrieved literature and refining it).</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>AI Scientist / Automated Discovery System</td>
                        </tr>
                        <tr>
                            <td><strong>problem_domain</strong></td>
                            <td>Machine learning research automation (idea generation, experiment execution, and paper drafting)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Automate open-ended scientific discovery in machine learning: propose ideas, implement experiments, iterate based on results, and write up findings.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Very high: full automation requires experimental design and execution, environment setup, resource-intensive experiments, multi-step reasoning and iterative planning, and handling stochastic experimental outcomes.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Depends on the experimental domain; AI Scientist requires access to code/data/resources to implement and run experiments. In this paper AI Scientist's ideation/refinement component was used as a comparative baseline; full experimental automation was not reproduced here.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_requirements</strong></td>
                            <td>Potentially very large (automated experiments and repeated LLM runs); this paper did not provide quantitative compute costs for AI Scientist, only comparative ideation metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_structure</strong></td>
                            <td>Open-ended, high-dimensional, and stochastic (experiments can be non-deterministic); success requires clearly measurable evaluation metrics for experiments but ideation itself is open-ended.</td>
                        </tr>
                        <tr>
                            <td><strong>success_metric</strong></td>
                            <td>In the paper's ideation comparison: similarity of generated ideas to ACL 2024 ideas (LLM similarity scores) and novelty; AI Scientist's generated ideas were evaluated and compared to SciPIP's outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>In retrospective ideation comparisons reported here, AI Scientist's highest similarity score to ACL-2024 ideas was only 3 (lower than SciPIP methods which achieved higher similarity matches); in novelty evaluations AI Scientist produced far fewer top-scoring novel ideas (e.g., 12 ideas with novelty score 9 in a 100-background test) compared to SciPIP-A (92).</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>When applied to the retrospective ideation task in this paper, AI Scientist's idea outputs were less similar to ACL-2024 published ideas and produced fewer top-novelty ideas compared to SciPIP. Possible broader failure modes (from cited description) include the high cost/complexity of automating experiment execution and domain transferability limitations.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Strengths stem from end-to-end automation capability (ideation through experimentation) and iterative refinement loops, but success depends heavily on domain-specific experimental infrastructure and reliable automated evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td>For ideation on the ACL-2024 retrospective task, AI Scientist generated fewer high-novelty ideas and achieved lower similarity-to-published-ideas scores than SciPIP variants (e.g., novelty-score-9 counts: AI Scientist 12 vs SciPIP-A 92 in a 100-background evaluation). AI Scientist did not perform literature retrieval in the same way; the paper adapted AI Scientist's refinement process for limited comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline</strong></td>
                            <td>Human researchers (authors of ACL-2024 papers) remain the target baseline for original idea generation. AI Scientist did not match human-produced ideas as often as SciPIP in the retrospective ideation task reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SciPIP: An LLM-based Scientific Paper Idea Proposer', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Scimon: Scientific inspiration machines optimized for novelty <em>(Rating: 2)</em></li>
                <li>Researchagent: Iterative research idea generation over scientific literature with large language models <em>(Rating: 2)</em></li>
                <li>The AI scientist: Towards fully automated open-ended scientific discovery <em>(Rating: 2)</em></li>
                <li>Scientific inspiration machines optimized for novelty <em>(Rating: 1)</em></li>
                <li>Training language models to follow instructions with human feedback <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2424",
    "paper_id": "paper-273695165",
    "extraction_schema_id": "extraction-schema-66",
    "extracted_data": [
        {
            "name_short": "SciPIP",
            "name_full": "Scientific Paper Idea Proposer (SciPIP)",
            "brief_description": "An LLM-based system that, given a user-provided research background, retrieves relevant literature from a prebuilt database and generates novel and feasible scientific paper ideas via single- and dual-path generation with filtering and refinement.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "SciPIP",
            "system_description": "SciPIP builds a literature database (48,895 NLP papers from major venues), extracts multi-dimensional paper information with an LLM (entities, background, summary, main ideas, detailed ideas, core references), encodes text with Sentence-BERT, and performs SEC-based retrieval (Semantics, Entities, Citation co-occurrence). Retrieved papers are clustered to reduce redundancy. Idea generation uses three pipelines: direct proposal (SciPIP-A) and two dual-path methods (SciPIP-B and SciPIP-C) that combine literature-inspired idea generation and LLM brainstorming; generated ideas are filtered and refined by LLM prompts. GLM-4 is used for paper parsing/summarization; GPT-4o is used for idea generation and evaluation in experiments.",
            "system_type": "Automated Idea Generation System / Hypothesis Generation System",
            "problem_domain": "Machine learning / Natural Language Processing (LLM-assisted scientific ideation and literature retrieval)",
            "problem_description": "Given a textual research background, produce novel and feasible scientific paper ideas and retrieve the most relevant prior literature to (a) inspire feasible ideas and (b) avoid redundant proposals; evaluate idea novelty and match to existing conference papers (ACL 2024) and retrieval of core citations.",
            "problem_complexity": "High: multi-modal semantic matching across ~48.9k papers, large discrete search space of potential ideas (combinatorial creativity), balancing novelty vs. feasibility (multi-objective), and avoiding redundant/retrieval-noise; experimental evaluation covers thousands of candidate ideas (e.g., SciPIP-C produced 7,638 ideas in retrospective tests).",
            "data_availability": "Large pre-collected literature database (48,895 papers from ICLR, NeurIPS, ICML, ACL, NAACL, EMNLP over ten years). Data quality: parsed PDF sections (title, abstract, intro, method, references); some PDFs failed parsing (small number excluded). No additional labeled idea data required; evaluation uses ACL 2024 papers as ground truth.",
            "computational_requirements": "LLM inference at scale (GLM-4 for parsing/summarization, Sentence-BERT for embeddings, GPT-4o for idea generation/evaluation), database storage and embedding search (Top-K ~55 per query), clustering of retrieved sets (hundreds of papers), and repeated LLM-based filtering/refinement; the paper does not state exact compute hours or costs.",
            "problem_structure": "Semi-structured and partly open-ended: retrieval and idea-matching are well-defined (semantic embeddings, cosine similarity, clustering), idea generation is open-ended and creative (stochastic LLM output). Evaluation metrics (similarity to published ideas, novelty scores, retrieval recall) are defined; domain knowledge encoded via literature database and entity extraction.",
            "success_metric": "Multiple metrics: (1) idea matching to ACL 2024 (LLM-assigned similarity scores 0-5, focusing on high matches e.g., score=4), (2) novelty scores assigned by GPT-4o after Semantic Scholar comparisons (0-10), and (3) literature retrieval recall@K (Recall@10,20,30,40,50).",
            "success_rate": "Retrieval: SciPIP recall@10 = 0.419, recall@50 = 0.684 (better than SCIMON-like and ResearchAgent-like baselines: recall@10 0.381 and 0.377 respectively). Idea-matching: across methods, SciPIP on average generates ~4–5 highly-matching ideas (high-similarity matches) per 100 input backgrounds (i.e., ~4–5% per 100). Novelty: SciPIP produced many high-novelty ideas; e.g., SciPIP-A generated 92 ideas with novelty score 9 (GPT-4o evaluation) in a 100-background test vs. AI Scientist's 12.",
            "failure_modes": "Generated ideas can be overly literature-dependent (close paraphrases of retrieved papers) or, conversely, brainstorming-only ideas may rely on LLM prior knowledge and not increase novelty; some LLM prompts/filter stages still admit low-relevance ideas which are filtered out; parsing failures for some PDFs reduce database coverage; the paper notes unexplained phenomena (e.g., ideas with lower similarity to existing literature sometimes rated more novel) requiring further study.",
            "success_factors": "Key contributors: (1) a large prebuilt literature database enabling fast, multi-granularity retrieval; (2) SEC retrieval combining semantics, entity matching and citation co‑occurrence; (3) clustering-based redundancy reduction; (4) dual-path idea generation that balances feasibility (literature-driven path) and originality (brainstorming path); (5) iterative LLM-based filtering and refinement; and (6) use of strong LLMs (GLM-4 for parsing, GPT-4o for generation/evaluation).",
            "comparative_results": "Compared to SCIMON-like and ResearchAgent-like retrieval, SciPIP achieves higher retrieval recall across K: Recall@10 (SciPIP 0.419 vs SCIMON-like 0.381 vs ResearchAgent-like 0.377) and Recall@50 (SciPIP 0.684 vs SCIMON-like 0.616 vs ResearchAgent-like 0.622). For idea novelty, SciPIP variants produced substantially more very-high novelty (score 9) ideas in 100-background tests (SciPIP-A 92, B 63, C 67) than AI Scientist (12). For idea matching to ACL 2024, SciPIP methods yielded ~4–5 high-similarity matches per 100 backgrounds while AI Scientist's generated ideas had maximum similarity score typically ≤3.",
            "human_baseline": "Human baseline is represented by the ACL 2024 papers themselves (the corpus of human-authored ideas). SciPIP's retrospective experiment demonstrates that it can reproduce a small fraction of those human ideas: on average ~4–5 highly-matching ideas per 100 input backgrounds (i.e., a modest fraction compared to the full set of human-published ideas).",
            "uuid": "e2424.0",
            "source_info": {
                "paper_title": "SciPIP: An LLM-based Scientific Paper Idea Proposer",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "SCIMON",
            "name_full": "SCIMON (Scientific Inspiration Machines Optimized for Novelty)",
            "brief_description": "A retrieval-augmented LLM system that retrieves 'inspiration' papers and iteratively optimizes generated ideas for novelty by comparing to prior literature and updating until sufficient novelty is achieved.",
            "citation_title": "Scimon: Scientific inspiration machines optimized for novelty",
            "mention_or_use": "mention",
            "system_name": "SCIMON (as baseline / related work)",
            "system_description": "SCIMON retrieves relevant past papers as inspiration and explicitly optimizes LLM-generated ideas for novelty via iterative comparison against retrieved literature, attempting to maximize novelty while maintaining feasibility. In this paper, the authors implemented a SCIMON-like retrieval method for baseline comparisons.",
            "system_type": "Hypothesis Generation System / Idea Proposal System",
            "problem_domain": "Machine learning / NLP idea generation and novelty optimization",
            "problem_description": "Generate novel scientific hypotheses/ideas by retrieving influential prior works and pushing generated ideas away from existing literature until novelty thresholds are met.",
            "problem_complexity": "Medium-to-high: requires retrieval over large literature sets and iterative LLM comparison loops; novelty optimization implies a non-convex, discrete creative search with human-meaningful novelty constraints.",
            "data_availability": "Operates over literature databases; performance depends on coverage of prior works. In this paper it was evaluated using the same or comparable literature corpus as SciPIP (implemented as SCIMON-like baseline).",
            "computational_requirements": "Iterative retrieval + LLM inference loops for novelty checking; no exact compute costs reported in this paper's baseline implementation.",
            "problem_structure": "Open-ended creative generation constrained by novelty metrics derived from literature similarity; relies on clear retrieval and similarity measures but open creative output.",
            "success_metric": "Novelty of generated ideas (distance from retrieved literature), and indirect metrics like whether generated ideas match human-published ideas in retrospective tests.",
            "success_rate": "Implemented as SCIMON-like baseline in this paper: retrieval recall@10 = 0.381 and recall@50 = 0.616 (lower than SciPIP's retrieval). The paper did not report SCIMON's idea-matching or novelty counts directly beyond using it as a retrieval baseline.",
            "failure_modes": "Lower retrieval recall compared to SciPIP when run as a SCIMON-like baseline; potential to over-optimize for novelty at the expense of feasibility (general SCIMON observation elsewhere), sensitivity to retrieval quality.",
            "success_factors": "Effectiveness depends on retrieval of diverse 'inspiration' papers and iterative novelty comparison mechanisms.",
            "comparative_results": "When implemented as a baseline, SCIMON-like retrieval trails SciPIP's SEC+clustering approach (recall@10: 0.381 vs 0.419). The paper uses SCIMON primarily as a conceptual and baseline comparator rather than a full direct head-to-head in idea-generation metrics.",
            "human_baseline": "Not directly compared to human performance in this paper beyond being a baseline retrieval/generation approach; implemented only for retrieval comparisons.",
            "uuid": "e2424.1",
            "source_info": {
                "paper_title": "SciPIP: An LLM-based Scientific Paper Idea Proposer",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "ResearchAgent",
            "name_full": "Research Agent (ResearchAgent)",
            "brief_description": "An agentic LLM-based research system that expands knowledge from a core paper via an academic graph and uses an entity-centric knowledge store and multiple reviewing agents to iteratively generate and refine research ideas.",
            "citation_title": "Researchagent: Iterative research idea generation over scientific literature with large language models",
            "mention_or_use": "mention",
            "system_name": "ResearchAgent",
            "system_description": "ResearchAgent begins from a core paper and expands context by traversing an academic graph, retrieves entities from an entity-centric store, and employs multiple reviewer agents to iteratively review and refine ideas; designed for iterative, agent-driven research ideation. In this paper the authors implemented a ResearchAgent-like retrieval baseline for comparison.",
            "system_type": "Agentic Research Idea Generation System",
            "problem_domain": "Machine learning / automated literature-based idea expansion and review",
            "problem_description": "Automatically expand a starting paper into broader directions and generate iterative research ideas using graph expansions, entity retrieval, and reviewer-agent feedback loops.",
            "problem_complexity": "High: graph traversal over large academic graphs, multi-agent iterative refinement, and combining entity-level retrieval with higher-level concept linking.",
            "data_availability": "Requires an academic graph and entity-centric knowledge store; in the paper a ResearchAgent-like method was simulated/implemented for retrieval baselines using the available literature database.",
            "computational_requirements": "Graph traversal, multiple agent LLM inferences, and retrieval—no explicit compute numbers provided in this paper's baseline implementation.",
            "problem_structure": "Semi-structured: starts from a discrete core seed paper but expands into open-ended idea generation via graph and agent interactions; evaluation relies on retrieval metrics and idea quality judgments.",
            "success_metric": "Retrieval recall and idea quality/novelty when compared to baselines; the paper used retrieval recall@K for baseline comparisons.",
            "success_rate": "As a ResearchAgent-like baseline, retrieval recall@10 = 0.377 and recall@50 = 0.622 in this paper (both lower than SciPIP's reported recall). Idea-generation metrics for ResearchAgent were not directly reported here.",
            "failure_modes": "In the baseline implementation, lower retrieval recall than SciPIP; possible sensitivity to graph coverage and entity extraction quality; complexity and reliance on graph completeness can limit performance.",
            "success_factors": "Quality and coverage of the academic graph and entity store and the efficacy of reviewer/critic agents for refinement.",
            "comparative_results": "ResearchAgent-like retrieval underperforms SciPIP's SEC+clustering approach on recall@K metrics (recall@10: 0.377 vs 0.419). The paper uses ResearchAgent mainly as a conceptual comparator and implemented a 'ResearchAgent-like' retrieval baseline rather than running the original system end-to-end.",
            "human_baseline": "Not directly provided; ResearchAgent aimed to automate aspects of human literature exploration and iterative refinement but in this paper serves as a baseline comparator.",
            "uuid": "e2424.2",
            "source_info": {
                "paper_title": "SciPIP: An LLM-based Scientific Paper Idea Proposer",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "AI Scientist",
            "name_full": "The AI Scientist (Lu et al., 2024)",
            "brief_description": "A recent system that aims for fully automated open-ended scientific discovery by autonomously generating ideas, implementing and executing experiments, searching related work, and producing full research papers in machine learning.",
            "citation_title": "The AI scientist: Towards fully automated open-ended scientific discovery",
            "mention_or_use": "mention",
            "system_name": "AI Scientist",
            "system_description": "AI Scientist is designed to automate the full research loop: ideation, experimental implementation and execution, literature search, iterative refinement, and paper writing. In this paper, SciPIP is compared against AI Scientist for idea-proposal quality; the authors adapted AI Scientist's idea-refinement process for a limited comparison (using an initial idea drawn from retrieved literature and refining it).",
            "system_type": "AI Scientist / Automated Discovery System",
            "problem_domain": "Machine learning research automation (idea generation, experiment execution, and paper drafting)",
            "problem_description": "Automate open-ended scientific discovery in machine learning: propose ideas, implement experiments, iterate based on results, and write up findings.",
            "problem_complexity": "Very high: full automation requires experimental design and execution, environment setup, resource-intensive experiments, multi-step reasoning and iterative planning, and handling stochastic experimental outcomes.",
            "data_availability": "Depends on the experimental domain; AI Scientist requires access to code/data/resources to implement and run experiments. In this paper AI Scientist's ideation/refinement component was used as a comparative baseline; full experimental automation was not reproduced here.",
            "computational_requirements": "Potentially very large (automated experiments and repeated LLM runs); this paper did not provide quantitative compute costs for AI Scientist, only comparative ideation metrics.",
            "problem_structure": "Open-ended, high-dimensional, and stochastic (experiments can be non-deterministic); success requires clearly measurable evaluation metrics for experiments but ideation itself is open-ended.",
            "success_metric": "In the paper's ideation comparison: similarity of generated ideas to ACL 2024 ideas (LLM similarity scores) and novelty; AI Scientist's generated ideas were evaluated and compared to SciPIP's outputs.",
            "success_rate": "In retrospective ideation comparisons reported here, AI Scientist's highest similarity score to ACL-2024 ideas was only 3 (lower than SciPIP methods which achieved higher similarity matches); in novelty evaluations AI Scientist produced far fewer top-scoring novel ideas (e.g., 12 ideas with novelty score 9 in a 100-background test) compared to SciPIP-A (92).",
            "failure_modes": "When applied to the retrospective ideation task in this paper, AI Scientist's idea outputs were less similar to ACL-2024 published ideas and produced fewer top-novelty ideas compared to SciPIP. Possible broader failure modes (from cited description) include the high cost/complexity of automating experiment execution and domain transferability limitations.",
            "success_factors": "Strengths stem from end-to-end automation capability (ideation through experimentation) and iterative refinement loops, but success depends heavily on domain-specific experimental infrastructure and reliable automated evaluation.",
            "comparative_results": "For ideation on the ACL-2024 retrospective task, AI Scientist generated fewer high-novelty ideas and achieved lower similarity-to-published-ideas scores than SciPIP variants (e.g., novelty-score-9 counts: AI Scientist 12 vs SciPIP-A 92 in a 100-background evaluation). AI Scientist did not perform literature retrieval in the same way; the paper adapted AI Scientist's refinement process for limited comparison.",
            "human_baseline": "Human researchers (authors of ACL-2024 papers) remain the target baseline for original idea generation. AI Scientist did not match human-produced ideas as often as SciPIP in the retrospective ideation task reported here.",
            "uuid": "e2424.3",
            "source_info": {
                "paper_title": "SciPIP: An LLM-based Scientific Paper Idea Proposer",
                "publication_date_yy_mm": "2024-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Scimon: Scientific inspiration machines optimized for novelty",
            "rating": 2,
            "sanitized_title": "scimon_scientific_inspiration_machines_optimized_for_novelty"
        },
        {
            "paper_title": "Researchagent: Iterative research idea generation over scientific literature with large language models",
            "rating": 2,
            "sanitized_title": "researchagent_iterative_research_idea_generation_over_scientific_literature_with_large_language_models"
        },
        {
            "paper_title": "The AI scientist: Towards fully automated open-ended scientific discovery",
            "rating": 2,
            "sanitized_title": "the_ai_scientist_towards_fully_automated_openended_scientific_discovery"
        },
        {
            "paper_title": "Scientific inspiration machines optimized for novelty",
            "rating": 1,
            "sanitized_title": "scientific_inspiration_machines_optimized_for_novelty"
        },
        {
            "paper_title": "Training language models to follow instructions with human feedback",
            "rating": 1,
            "sanitized_title": "training_language_models_to_follow_instructions_with_human_feedback"
        }
    ],
    "cost": 0.0157395,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>SciPIP: An LLM-based Scientific Paper Idea Proposer SCIPIP: AN LLM-BASED SCIENTIFIC PAPER IDEA PROPOSER
30 Oct 2024</p>
<p>Wenxiao Wang 
Zhejiang University
2 Alibaba Cloud</p>
<p>Lihui Gu 
Zhejiang University
2 Alibaba Cloud</p>
<p>Liye Zhang 
Zhejiang University
2 Alibaba Cloud</p>
<p>Yunxiang Luo 
Zhejiang University
2 Alibaba Cloud</p>
<p>Yi Dai 
Zhejiang University
2 Alibaba Cloud</p>
<p>Chen Shen 
Liang Xie 
Zhejiang University of Technology</p>
<p>Binbin Lin 
Zhejiang University
2 Alibaba Cloud</p>
<p>Xiaofei He 
Zhejiang University
2 Alibaba Cloud</p>
<p>Jieping Ye 
SciPIP: An LLM-based Scientific Paper Idea Proposer SCIPIP: AN LLM-BASED SCIENTIFIC PAPER IDEA PROPOSER
30 Oct 202454684C70AD4C603A0F81980DF1D674D2arXiv:2410.23166v1[cs.CL]
The exponential growth of knowledge and the increasing complexity of interdisciplinary research pose significant challenges for researchers, including information overload and difficulties in exploring novel ideas.The advancements in large language models (LLMs), such as GPT-4, have shown great potential in enhancing idea proposals, but how to effectively utilize large models for reasonable idea proposal has not been thoroughly explored.This paper proposes a scientific paper idea proposer (SciPIP).Based on a user-provided research background, SciPIP retrieves helpful papers from a literature database while leveraging the capabilities of LLMs to generate more novel and feasible ideas.To this end, 1) we construct a literature retrieval database, extracting lots of papers' multi-dimension information for fast access.Then, a literature retrieval method based on semantics, entity, and citation co-occurrences is proposed to search relevant literature from multiple aspects based on the user-provided background.2) After literature retrieval, we introduce dual-path idea proposal strategies, where one path infers solutions from the retrieved literature and the other path generates original ideas through model brainstorming.We then combine the two to achieve a good balance between feasibility and originality.Through extensive experiments on the natural language processing (NLP) field, we demonstrate that SciPIP can retrieve citations similar to those of existing top conference papers and generate many ideas consistent with them.Additionally, we evaluate the originality of other ideas generated by SciPIP using large language models, further validating the effectiveness of our proposed method 1 .</p>
<p>INTRODUCTION</p>
<p>With the exponential growth of knowledge and the increasing complexity of interdisciplinary research, machine learning researchers face significant challenges, including information overload and difficulties in exploring novel ideas.Against this backdrop, generating new ideas and innovative concepts efficiently has become a pressing need.Recent advancements in large language models (e.g., GPT-4 (Ouyang et al., 2022), LLaMA (Touvron et al., 2023a;b), Qwen (Bai et al., 2023;Yang et al., 2024), GLM-4 (Zeng et al., 2024), and etc), have demonstrated immense potential in enhancing innovation generation.These models are not only capable of understanding and generating complex academic content but also excel in aligning multimodal information, constructing implicit chains of thought, and uncovering non-obvious connections.Leveraging LLMs to assist researchers in generating new ideas holds significant implications for improving research productivity and offers a theoretical foundation and practical guidance for the design of future intelligent research assistants.</p>
<p>Large language model (LLM)-based idea proposers should have the ability to understand the userprovided research background, autonomously retrieve relevant literature, and generate novel and feasible ideas aimed at addressing problems within the given background.Some previous works have proposed their methods (Wang et al., 2024;Baek et al., 2024;Lu et al., 2024).However, existing LLM-based idea proposers still face two challenges: 1) Similar to human researchers, literature retrieval is essential to inspire new ideas and avoid repetitive ideas.Nevertheless, online literature searches are limited to simple keyword matching and cannot fully leverage the user-provided information or the existing literature, leading to incomplete and inaccurate retrieval results.2) Scientific paper ideas require both novelty and feasibility.However, it is still under-explored about how to enable LLMs to generate entirely new ideas while ensuring their feasibility.</p>
<p>To address the above challenges, we propose our Scientific Paper Idea Proposer (SciPIP).In terms of challenge 1), SciPIP first constructs a literature retrieval database.Specifically, we collect a large body of literature from the natural language processing (NLP) field and extract multiple dimensions of information for each paper using techniques such as entity extraction, semantic encoding, summarization, and citation analysis.The information is stored in the database, enabling rapid access to various aspects of the literature during retrieval.Building on this database, we propose a literature retrieval method based on semantics, entities, and citation co-occurrence (SEC-based retrieval).In this framework, "semantics" captures the global information of a paper, "entities" focus on local details, and "citation co-occurrence" reflects the hidden relationships uncovered by previous researchers.By matching at these three different levels of granularity, SciPIP offers more comprehensive literature retrieval.</p>
<p>To address the challenge 2), SciPIP introduces a new method for idea proposal.It first organizes the retrieved literature and generates ideas inspired by the retrieved works.Subsequently, SciPIP uses a brainstorming approach to generate new ideas without reference to the literature.Depending on the combination of literature-based and brainstorming-based idea generation, we derive three variants of SciPIP.The ideas generated by our method are further filtered and refined to enhance both their novelty and feasibility.</p>
<p>Extensive experiments are conducted to evaluate both idea proposal and literature retrieval on the NLP field.In the retrospective experiments, we use the backgrounds of ACL 2024 papers as inputs to test whether the models could generate the same ideas as those in the published papers, or whether SciPIP could retrieve the same references as the actual citations.Additionally, we conduct innovation experiments, in which the models are prompted to freely propose ideas based on a given background, and the quality of the proposed ideas are assessed by an LLM in terms of novelty, feasibility, and etc.The experimental results demonstrate that, compared to existing methods, SciPIP can match more existing ideas and generate ideas with significantly greater novelty and potential.</p>
<p>RELATED WORKS</p>
<p>Around 60 years ago, scientists began exploring scientific discoveries based on literature retrieval, known as Literature-Based Discovery (LBD) (Swanson, 1986).This approach concentrated on a specific, narrow type of hypothesis: the connections between pairs of concepts, often involving drugs and diseases.LBD introduced the "ABC" model, positing that two concepts A and C are hypothesized to be linked if they appear in conjunction with an intermediate concept B in the literature.</p>
<p>The advent of large language models (LLMs) has revolutionized various fields, and one of the most intriguing applications is their ability to generate scientific hypotheses (Wang et al., 2024;Baek et al., 2024;Lu et al., 2024).LLMs, trained on extensive datasets encompassing a vast array of scientific literature, possess an impressive capacity to recognize patterns and synthesize information across disciplines.By leveraging their advanced natural language processing (NLP) capabilities, these models can propose novel hypotheses that might not be immediately apparent to researchers.The process begins with the model receiving a prompt, typically related to a specific scientific domain, which guides it to generate hypotheses grounded in existing knowledge while also incorporating innovative perspectives.For example, SCIMON (Wang et al., 2024) uses retrieval of "inspirations" from past scientific papers to generate ideas.It explicitly optimizes for novelty by iteratively comparing generated ideas with prior papers and updating them until sufficient novelty is achieved.In contrast, Research Agent (Baek et al., 2024) starts with a core paper as the primary focus and expands its knowledge by connecting information over an academic graph and retrieving entities from an entity-centric knowledge store based on their underlying concepts.It also leverages multiple Reviewing Agents to provide iterative reviews and feedback for refining the generated ideas.AI Scientist leverages large language models (LLMs) to autonomously generate research ideas, implement and execute experiments, search for related works, and produce comprehensive research papers in machine learning.The AI Scientist is designed to automate the entire scientific process, from ideation to experimentation and iterative refinement.</p>
<p>METHODS</p>
<p>We propose a Scientific Paper Idea Proposer (SciPIP) that takes the user-provided background of a specific research field as input, retrieves relevant literature from the database, and generates novel and feasible ideas.To achieve this, we will first construct a literature database in Section 3.1 for literature retrieval during the idea proposal process.Then, in Section 3.2, we detail how to retrieve literature related to the user-provided background.Finally, in Section 3.3, we outline the process of idea proposal.</p>
<p>LITERATURE DATABASE CONSTRUCTION</p>
<p>Just like human researchers, reading other literature and drawing inspirations from them is an important process for LLMs to generate valuable ideas.However, online literature reading is a very time-consuming process, so we collect a literature database in advance for the following literature retrieval and idea proposal process.</p>
<p>To be specific, we collect papers published in ICLR, NeurIPS, ICML, ACL, NAACL, and EMNLP in past ten years, yielding a database with 48,895 papers.For each paper, we parse the PDF file and extract its title, abstract, introduction, method, and references sections.Then, as shown in Figure 1 , given an LLM f , we prompt it to read and summarize the paper as:
E (p) = f (τ 1 , T (p) a ), (T (p) b , T (p) s , T (p) i ) = f (τ 2 , T (p) t , T (p) a , T (p) n ), T (p) d = f (τ 3 , T (p) m , T (p) i ),(1)
where
T (p) t , T (p) a , T (p) n , T(p)
m are the paper p's title, abstract, introduction, and method sections.
E (p) , T (p) b , T (p) s , T (p) i , T (p) d , T (p) r
are extracted entities, background, summary, main ideas, detailed ideas, and core references, as shwon in Figure 1.τ i , i ∈ {1, 2, 3} represent our designed prompt templates, and specific prompts are shown in the Appendix A.1.In practice, we use GLM-42 (Zeng et al., 2024) as f .Besides, "Core References" in Figure 1 means extracting papers referenced in introduction and method sections, because we believe these references have the greatest impact on paper p among all references.</p>
<p>User-provided Background</p>
<p>The need to reduce the memory footprint and training time in finetuning large language models, as existing methods still require considerable memory and do not simultaneously address all three contributors to the memory demand: model weights, optimizer states, and intermediate activations.Additionally, the background, summary, and main ideas are also encoded with Sentence-BERT (Reimers &amp; Gurevych, 2019) for their embeddings e</p>
<p>Semantics</p>
<p>s and e (p) i , respectively.All extracted information are recorded into our literature database.</p>
<p>To retrieve literature faster, we also construct a paper-entity graph in the database.we also store all occurrence relationships of papers and entities in the database.As shown in Figure 1, if an entity T e1 appears in the paper p1, there will be an edge between the two paper nodes.</p>
<p>LITERATURE RETRIEVAL AND FILTERING</p>
<p>Literature retrieval is an essential process for idea proposal.It should follow the rule of comprehensiveness and low-redundancy.On the one hand, a comprehensive retrieval can provide researchers with instructive inspirations and avoid repetitive idea proposal.On the other hand, more retrieved papers are not necessarily better because redundant papers may also introduce noise and disperse a researcher's attention.To this end, we first propose a SEC-based (Semantics, Entities, and Citation co-occurrence) literature retrieval.Then, we propose a clustering-based literature filtering to pick out the most helpful papers.The process is shown in Figure 2.</p>
<p>SEC-BASED LITERATURE RETRIEVAL</p>
<p>Semantics-based retrieval.As shown in Figure 2, given a user-provided background T (u) b , we encode it as an embedding with Sentence-BERT (Reimers &amp; Gurevych, 2019), marked as e is compared with e b of all papers' backgrounds in the literature database to identify a subset of papers with the maximum cosine similarity as the semantic-based retrieval results.Assume the retrieved papers as N 1 ,
N 1 = {p|e (p) b ∈ TopK(cosine(e (u) b , e (i) b )) for i ∈ D},(2)
where p or i represents a paper in the literature database.In practice, we take K = 55 for the TopK operation.</p>
<p>Entity-based retrieval.As we can see in Figure 2, after semantic literature retrieval, we take the user-provided background T as input and prompt GLM-4 to extract all entities in the background.Then, the abstract section of semantics-based retrieved papers (i.e., p ∈ N 1 ) are also given to the GLM-4 to extract their entities.The exact prompt we use is provided in the Appendix A.1.After entity extraction, we also expand the entity set by giving these entities back to GLM-4 and let it generate some synonyms.The motivation behind entity expansion is that the same concept may express in different ways, and entity expansion can help us retrieve papers that use synonyms in the following process.We notate the entity set after synonym expansion as E 1 .</p>
<p>Additionally, we further expand the entity set through an entity-neighborhood-based approach.In simple terms, for an entity T e in the current entity set E 1 , any paper p that includes entity T e should also have its other entities included in the candidate entity set.However, we find that this will induce many redundant or even noisy entities, and the reasons are twofold:</p>
<ol>
<li>
<p>Two entities with low relevance may appear together in a paper due to the specific content requirements of that paper.</p>
</li>
<li>
<p>High-frequency words do not effectively characterize a paper or its background.For instance, the user-provided background might include the term "Transformer", but this does not imply that all entities co-occurring with "Transformer" in other papers are significant to us.This is because "Transformer" is a high-frequency term that may appear in many recent publications.</p>
</li>
</ol>
<p>To this end, we propose two filtering mechanisms for neighborhood-based entity expansion:</p>
<p>1.An entity will only be supplemented if it has appeared together with another entity in at least m papers.In practice, we take m = 2.</p>
<ol>
<li>Inspired by the TF-IDF (Jones, 2004) algorithm, we believe that if an entity appears frequently across the entire paper database, it indicates that the entity is less representative.Therefore, we only select the n entities that appear the least in all literature as the final entity set.In practice, we take n = 5.</li>
</ol>
<p>The entity set after a second expansion is represented as E (u) .Entities are key words that are most relevant with a paper's topic.A paper is likely to be helpful to us if it contains entities that match those in our entity set E (u) .Thus, for any entity T e in set E (u) , we search for papers that also contain T e in our database.Marking all searched papers as a set N 2 ,
N 2 = {p|∃T e ∈ E (u) ∧ T e ∈ T (p) b , p ∈ D}.(3)
Co-occurrence-based retrieval.In the above, we retrieve literature relevant to the user-provided background through entities and semantics.Wherein, entities represent specific details of a paper, while semantics represent the broader, overall meaning within the background.However, in actual research, we often encounter two papers, p 1 and p 2 , which are neither similar in details nor in semantics, yet are cited together.This indicates that researchers have discovered a latent relationship between p 1 and p 2 in past studies.To capture and fully utilize these insights, we propose a literature retrieval method based on citation co-occurrence.Specifically, as shown in Figure 2, for any paper p 1 we have already retrieved, if p 2 is frequently cited alongside p 1 in other papers, we will include p 2 in our literature retrieval set:
N 3 = {p 2 |p 1 ∈ (N 1 ∪ N 2 ) ∧ co-cite(p 1 , p 2 )},(4)
where co-cite means p 1 and p 2 are often simultaneously cited by other papers.In practice, we select the 2 papers that are most frequently co-cited with each paper.</p>
<p>Finally, the whole retrieved papers can be represented as
N = N 1 ∪ N 2 ∪ N 3 .</p>
<p>LITERATURE CLUSTERING</p>
<p>After SEC-based literature retrieval, we may get over 500 papers, so further filtering is essential to pick out the most significant ones.Since we have observed that the retrieved papers often present similar ideas, we hope to retain only one paper among those with similar content during the generation of new ideas.To achieve this, we propose clustering the papers based on cosine similarity measures.Specifically, we first define the embedding of a retrieved paper as:
e (p) = w s e (p) s + w i e (p) i ,(5) where e (p)
s and e (p)</p>
<p>i are embeddings for summary and main ideas of an idea, as illustrated in Figure 1.We choose w s = w i = 0.5 in practice.Then, we apply clustering to group papers according to their cosine similarity.In practice, since the semantic embeddings of all papers are pre-recorded in a database, we only need to perform the similarity comparison and clustering processes.Finally, we select one paper from each cluster, respectively, and make up the retrieved papers.</p>
<p>IDEA PROPOSAL</p>
<p>Upon completion of the literature retrieval, we propose three approaches for generating research paper ideas.In essence, the idea generation process can leverage two types of information: the first is derived from the content of the retrieved papers, which inspires the LLM to generate ideas; the second involves the LLM freely brainstorming to produce new ideas.Based on this principle, we delineate three methods of idea generation that vary in their application of brainstorming.</p>
<p>As illustrated in Figure 3(a), the direct proposal method (SciPIP-A), does not use brainstorm.While the first dual-path proposal method (SciPIP-B), as Figure 3(b) shows, utilizes the user-provided background into two branches.The first branch employs this background for literature retrieval, problem summarization, and idea generation based on the retrieved literature, while the second branch engages in brainstorming solutions directly from the user-provided background.Following the independent generation of ideas in both branches, the outputs are merged and subsequently filtered and refined to yield the final ideas.Similarly, as shown in Figure 3(c), the second dual-path proposal method (SciPIP-C) follows a process analogous to SciPIP-B, with the key distinction being that the content generated through the LLM's brainstorming is utilized not only for idea generation but also integrated with the user-provided background for entity extraction and other literature retrieval processes.We will provide a detailed exposition of these three methods of idea proposal in the following sections.We use GPT-4o3 by default in this section.</p>
<p>DIRECT IDEA PROPOSAL METHOD</p>
<p>As depicted in Figure 3(a), in the direct proposal method, we first retrieve papers following the pipeline described in Section 3.2.Then, the user-provided background along with the retrieved papers are utilized to prompt the LLM to summarize the core problem we aim to address and provide justifications.The specific prompts can be found in the Appendix A.1.</p>
<p>With the summarized problem and justifications, the LLM is prompted to generate around 10 initial ideas.In the prompt, both the problem, the justification and the retrieved papers are provided.The LLM is encouraged to generate clear, innovative, valid, and comprehensive ideas.The specific prompts for this step can be also found in the Appendix A.1.</p>
<p>Though the prompt has declared, the initially generated ideas may still have shortcomings in terms of novelty or relevance to the problem.To address this, we filter the initial ideas using prompt engineering (prompts are illustrated in the Appendix A.1), with the primary criterion being that the ideas are generated in response to the given problem.Additionally, the ideas must exhibit a high degree of novelty and feasibility.During this process, each generated idea is evaluated independently, and about half of them will be filtered.</p>
<p>Then, the LLM is encouraged to further improve the filtered ideas by considering their interrelationships.That is, the LLM is tasked with considering the compatibility of the ideas, ensuring that it does not generate conflicting or repetitive ideas.Moreover, the LLM is required to gen- erate formulas or algorithms to better elaborate the ideas if needed.The prompt is shown in the Appendix A.1.Finally, about 3 to 4 refined ideas will be proposed.</p>
<p>DUAL-PATH IDEA PROPOSAL METHODS</p>
<p>We find that the directly generated ideas often rely heavily on the retrieved literature, sometimes closely resembling the methods presented in those papers.They frequently involve transferring approaches from other fields or making minor improvements to existing methods within the same field, resulting in relatively ordinary novelty and rarely yielding breakthrough thinking.</p>
<p>Therefore, we further propose idea proposers that incorporates brainstorming, encouraging the LLM to produce more novel thoughts.Specifically, brainstorming can play a role in both processes of idea generation.As shown in Figure 3(b), the SciPIP-B has two paths, where one path follows the direct proposal approach, while the other path uses the LLM to brainstorm possible solutions based on the user-input background, outputting these as ideas.Ultimately, these ideas will be merged with those generated based on the retrieved papers, filtered and refined to produce the final ideas.In this model, the results of brainstorming are independent of the generation based on retrieved papers.</p>
<p>In another approach, as shown in Figure 3(c), brainstorming generates ideas independently while also being utilized in literature retrieval.Specifically, we extract entities from the brainstorming results and incorporate them as part of the entity set in the literature retrieval process.With this method, some keywords arising from the brainstorming will also help enhance the effectiveness of literature retrieval.The ideas generated through brainstorming will also be merged with those produced after literature retrieval.</p>
<p>EXPERIMENTS</p>
<p>EVALUATION DATASET</p>
<p>We collect all papers accepted by ACL 2024, including long papers, short papers, findings, and workshop papers.After excluding a few PDFs that could not be correctly parsed, 1,968 papers are remained for analysis.The remaining papers are processed similarly to those in the literature database in Section 3.1, with their entities, backgrounds, summaries, main ideas, detailed ideas, and references extracted in advance.</p>
<p>The experiments in this study are divided into two parts: retrospective experiments and innovation experiments.Retrospective experiments refer to testing whether different algorithms can generate the same ideas and literature retrieval results as the original papers on the evaluation dataset (i.e., ACL 2024 papers) with providing the background of the papers as input.In contrast, innovation experiments allow the models to freely propose new ideas, which are then evaluated from multiple perspectives, including novelty and feasibility.</p>
<p>RETROSPECTIVE EXPERIMENTS FOR IDEA PROPOSAL.</p>
<p>Compared algorithms.AI Scientist (Lu et al., 2024), when given an existing idea, iteratively refines the idea through multiple rounds of LLM inference.Afterward, the AI Scientist will expand the Idea into a full paper.Since our algorithm only focuses on proposing ideas, we only compare  b , we first retrieve a paper from the literature database with a similar background.The idea from this paper serves as the initial idea for refinement by the AI Scientist.In contrast, our algorithm directly uses the user-provided background T (u) b as input for idea proposal.We then compare the similarity of generated ideas by two algorithms to the ideas from ACL 2024 papers.</p>
<p>Evaluation Protocol.To evaluate the matching rate between the generated ideas and those from ACL 2024, we first preprocess all ACL papers following the method in Section 3.1 and store them in a database.The generated ideas are then compared based on cosine distance to retrieve the 10 most similar ideas from the database.Next, using prompt engineering, GPT-4o selects the most similar idea and assigns a similarity score between 0 and 5, where a higher score indicates greater similarity.From our observations, a score of 4 indicates that the two ideas are almost identical, differing only in minor details, while a score of 3 or lower suggests more significant differences.Wherein, SciPIP-C is tested on all ACL 2024 papers, while other methods are tested with 100 backgrounds randomly sampled from the whole test set.</p>
<p>However, we believe that low-scoring ideas in the retrospective experiments do not necessarily lack value.On the contrary, some of these ideas exhibit strong novelty and feasibility, though they do not ideas published at ACL 2024.To further assess the novelty and feasibility of all ideas generated by SciPIP, we employ the LLMs for evaluation.For each round of comparison, we sample one idea from each of 5 similarity scores and ask the LLM to rank them based on their novelty and feasibility.We then record the win rate (i.e., the probability of ranking first) of ideas across different similarity scores in all rounds.</p>
<p>Results and analyses.As we can see in Table 1, our proposed three idea proposal strategies can, on average, generate 4 to 5 ideas that highly match ACL 2024 conference papers out of every 100 input backgrounds.This indicates that SciPIP is capable of generating ideas consistent with human thought, whereas the highest similarity score for all ideas generated by the AI Scientist is only 3. Additionally, the three methods we propose exhibit similar performance.</p>
<p>Moreover, the results in Table 2 illustrates that ideas with lower similarities to published ideas even show higher novelty, while the reasons still need more explorations.Further, ideas do not show much difference in terms of their feasibility.</p>
<p>Besides, we also provide two examples of SciPIP proposed ideas in Figure 4.The two examples both get a similarity score of 4 to an existing paper in ACL 2024, and the generated idea is indeed very similar to the matched idea.For example, in the second example (with the yellow background), the background points out the drawback of existing code generation algorithms.Both our generated and 1.The limitations of existing methods in leveraging nonverbal information for discerning complex semantics in unsupervised scenarios.2. The recognition that non-verbal modalities (video and audio) play a critical role in performing unsupervised clustering and can provide useful cues for semantics discovery."</p>
<p>Background</p>
<p>SciPIP generated idea</p>
<p>Contrastive Multimodal Clustering (CMC) -The Contrastive Multimodal Clustering (CMC) model adapts multimodal contrastive learning for clustering by aligning embeddings of video, audio, and text data in a joint embedding space.It minimizes a contrastive loss function to bring similar instances from different modalities closer together, optimizing cosine similarity for positive pairs and minimizing it for negative pairs.This approach captures semantically rich representations without label supervision, effectively incorporating nonverbal cues into the clustering task.</p>
<p>Matched groundtruth idea</p>
<p>An unsupervised multimodal clustering method constructs augmentation views for multimodal data to perform pre-training by initializing representations for clustering with positive augmentation views, retaining text modality as core and masking either video or audio for data augmentation, utilizing a multimodal unsupervised contrastive learning loss for learning implicit similarities in shared modalities.</p>
<p>LLM-based similarity score: 4</p>
<p>1.The current challenge of large language models (LLMs) in code generation, where the correct solution is not always generated in a single attempt.2. The need to move beyond traditional verification properties from software engineering that are assumed to be superior to generated code solutions, but are often produced by the same model.</p>
<p>Background</p>
<p>SciPIP generated idea</p>
<p>Persistent Error-Guided Code Refinement Loop: Introduce a continuous feedback mechanism where the LLM-generated code undergoes iterative cycles of execution and refinement.Each cycle utilizes error messages and runtime exceptions to make corrections.A dynamic reinforcement learning (RL) model is incorporated to reward sequences of effective corrective actions that lead to successful code execution.Specifically, a reward function R(s, a) evaluates the efficacy of a correction action a in a state s, based on error resolution and code performance improvement metrics.This loop ensures persistent reduction of errors and refinement of generated code.</p>
<p>Matched groundtruth idea</p>
<p>Adoption of a reward model that acts as a critic to provide feedback for the fine-tuned language model's actions, using reinforcement learning to optimize the model's repair policies.The reward model serves as a virtual tool that assesses the quality of the program outputs.It is trained using pairwise ranking based on the severity of program errors, providing feedback to the language model.This approach uses reinforcement learning, specifically the Proximal Policy Optimization (PPO) algorithm, to fine-tune the language model.The model iteratively refines programs based on the feedback from the reward model, aiming to maximize the rewards received.The process continues until no further improvement is detected or a predefined maximum number of iterations is reached.</p>
<p>LLM-based similarity score: 4 the matched idea propose to iteratively refine the generated code, and reinforcement learning based reward model should be used to evaluate the generated code.The reward should be decided by the error resolution, the severity of errors, and so on.More examples can be seen in the Appendix A.2.</p>
<p>NOVELTY EXPERIMENTS FOR IDEA PROPOSAL</p>
<p>Compared algorithms and evaluation protocol.We also compare with AI Scientist (Lu et al., 2024) for novelty verification.The verification way is drawn from the official source code of AI Scientist with some modifications.To be specific, a proposed idea will give some key words that being used to search similar papers in Semantic Scholar4 .Through comparison with several similar papers drawn from Semantic Scholar, GPT-4o judges the novelty of the generated idea.The novelty score is from 0 to 10, higher score means smaller similarity with existing papers or higher novelty.</p>
<p>Results and analyses.The results are in Table 3.It can be seen that both SciPIP and AI Scientist can generate very novel ideas with score 9.While our proposed ideas with 9 score are much more than AI Scientist (92 vs. 12).Unexpectedly, SciPIP with brainstorm perform worse than the direct proposal.It may be because brainstorm utilizes the knowledge from the GPT-4o itself in essence.Therefore, it is hard for the model to generate brand new ideas that are totally different with existing literature.However, we believe brainstorming will be a significant supplement to retrieval-based generation, so we still preserve the results of SciPIP-B/C, hoping attract the community's attention.</p>
<p>At least, all versions of SciPIP generate over 270 high-scoring (score &gt; 7) ideas even though they only match 4 to 5 ideas in ACL 2024.The results indicate that non-matching ideas may be more valuable because SciPIP generate novel ideas that do not appear (or even do not put forward by human).</p>
<p>RETROSPECTIVE EXPERIMENTS FOR PAPER RETRIEVAL</p>
<p>Compared algorithms.Since AI Scientist does not perform a literature retrieval when generating ideas, the results primarily on SCIMON (Wang et al., 2024) and ResearchAgent (Baek et al., 2024).However, the experimental setups and literature database of SCIMON and ResearchAgent for generating scientific paper ideas differ from those in this study.Additionally, ResearchAgent is not open source, making it challenging to fully replicate the exact algorithm.Therefore, based on the descriptions in the original papers, we implement similar literature search algorithms, namely SCIMON-like and ResearchAgent-like in Table 4.</p>
<p>Evaluation protocol.Only a few reference papers are crucial for generating a paper's idea; using all citations as ground truth may introduce significant noise.Among contemporaneous papers, there may be similar ideas, and researchers might only cite one of them.To address this, we propose two strategies: We believe that the most important citations for a paper typically appear in the introduction and method sections; thus, we extract only these sections' citations as ground truth during PDF parsing.Additionally, as mentioned earlier, our method clusters the retrieved literature after searching, treating all papers in the same cluster as similar.In the retrospective experiment, we evaluate the distance between ground truth citations and cluster centers.If a ground truth citation falls within a cluster retrieved by SciPIP, we consider the retrieval result correct.</p>
<p>Results and analyses.The results are shown in Table 4, where Recall 10 represents the proportion of correctly retrieved papers when the algorithm is restricted to returning only 10 papers.For example, if the ground truth for a paper's literature search includes 20 references, a recall rate of 0.684 indicates that approximately 13 relevant papers were correctly retrieved.From the data in the table, it can be observed that our algorithm successfully retrieves more relevant papers compared to SCIMON and ResearchAgent.We also provide some ablation studies about literature retrieval in Table 5.As we can see, SE performs better than using only semantics or entities for retrieval.Moreover, citation co-occurrence and clustering also help improve the retrieval results.</p>
<p>CONCLUSIONS AND LIMITATIONS</p>
<p>In this paper, we propose a method for generating scientific paper ideas and demonstrate its effectiveness on natural language processing datasets.The experimental results show that SciPIP is capable of proposing numerous novel ideas through the capabilities of LLMs.These ideas not only match papers published at recent academic conferences but also exhibit significant potential in terms of novelty, feasibility, and other key aspects.Despite these positive results, we gain more questions than conclusions in this work.For example, why do the ideas with lower similarity score looks more novel (refereed as to Table 2).We need more explorations to answer these questions.</p>
<p>A APPENDIX A.1 PROMPTS USED IN THIS PAPER</p>
<p>We employ prompt engineering accomplishing our task in this paper, and the used prompts are summarized in Table 6.</p>
<p>A.2 EXAMPLES OF OUR GENERATED IDEAS</p>
<p>More examples of SciPIP proposed ideas are given in Figure 5.</p>
<p>Table 6: Summarization of our used prompts.</p>
<p>Prompts Place</p>
<p>The prompt for entity extraction, namely τ 1 .Table 7 The prompt for summary, background, and main ideas extraction, namely τ 2 .</p>
<p>Table 8 The prompt for detailed ideas extraction, namely τ 3 .</p>
<p>Table 9 The prompt for problem/rational generation.</p>
<p>Table 10 The prompt for initial idea generation.</p>
<p>Table 11 The prompt for idea filtering.</p>
<p>Table 12 The prompt for idea improvement.</p>
<p>Table 13 The prompt for brainstorming.</p>
<p>Table 14 The prompt for picking out the most similar idea from several ideas.</p>
<p>Table 15 The prompt for evaluating the similarity score between two ideas.</p>
<p>Table 16 The prompt for scoring the novelty of an idea.</p>
<p>Table 17 The prompt for comparing two ideas for their clarity, novelty, feasibility, and generalizability.Table 18 The prompt for comparing five ideas for their clarity, novelty, feasibility, and generalizability.Table 19 Table 7: The prompt for entity extraction, namely τ 1 .</p>
<p>System Message</p>
<p>Now you are an expert in extracting key entities from research contents.You are good at identifying the most important keywords or phrases that summarize the main topics or concepts discussed in the content.</p>
<p>User Message</p>
<p>Task Description: I will provide you with a content from a research paper.Your task is to extract the key entities from this content.These entities are the most important keywords or phrases that summarize the main topics or concepts discussed in the content.</p>
<p>Instruction:</p>
<p>Content: The content is your key focus, and the extracted entities should be based on the content.In other words, the entities you extract should be concrete manifestations of the main themes and topics discussed in the content.</p>
<p>Your approach should be systematic: -Start by thoroughly reading the content to understand its main themes and topics.</p>
<p>-Identify and list the key entities that are central to the content.</p>
<p>-Ensure that the entities are relevant, meaningful, and representative of the content.</p>
<p>-Each entity in entities should be no longer than 5 words.</p>
<p>-Each entity in entities should contain at least 2 words.</p>
<p>-The number of entities should be less than or equal to 5.</p>
<p>-Each entity in entities should be nouns or noun phrases.</p>
<p>examples: {examples}</p>
<p>Your turn: Given the following content: {content}</p>
<p>Your answer should follow this format: entity1, entity2, entity3, ......</p>
<p>System Message</p>
<p>Now you are an expert in extracting key entities from research contents.You are good at identifying the most important keywords or phrases that summarize the main topics or concepts discussed in the content.</p>
<p>User Message For Summary</p>
<p>User Message For Background And Main Ideas</p>
<p>Please read the title, abstract, and introduction of the paper again, as well as the summary you provided.Complete the following two tasks: 1.Briefly provide the two most critical motivations behind proposing these methods to address the problems.2.Briefly provide the three most critical or innovative details of the paper that were not mentioned in your summary (It's best if these details are the new methods or techniques adopted in this paper).</p>
<p>Output: Motivations
:1.[motivation1]. 2.[motivation2]. Details:1.[detail1]. 2.[de- tail2]. 3.[detail3].
Table 9: The prompt for detailed ideas extraction, namely τ 3 .</p>
<p>System Message</p>
<p>Now you are an expert in extracting key entities from research contents.You are good at identifying the most important keywords or phrases that summarize the main topics or concepts discussed in the content.</p>
<p>User Message</p>
<h3>Task Description: You will be provided with the abstract and a text extracted from a paper and three contributions of the paper.Your task is to filter, refine, and revise the content of the contributions through the text provided to you.</h3>
<p>System Message</p>
<p>Now you are a researcher in the field of AI with innovative and pioneering abilities.You are good at using innovative and original methods to solve cutting-edge problems in the field of AI.</p>
<p>User Message</p>
<h3>Task Description: You will be provided with a research problem along with its rationales.Your task is to brainstorm some ideas that are clear, innovative, valid, and comprehensive to address the problem.Additionally, some cue words along with summaries, backgrounds, and contributions (methods) of related papers will be provided as sources of inspiration for generating novel ideas.### Information Provided: 1. <strong>Research Problem &amp; Rationales</strong>: The key issues or aspects of the problem that need to be addressed.These will form the foundation for generating your ideas.2. <strong>Related Papers</strong>: Draw inspiration from the abstracts, backgrounds, and methods of these papers.Delve deeply into these methods, understand the motivations behind them, and think critically about how they might inform your approach.Avoid merely stacking existing methods; instead, integrate relevant aspects with your own insights to create original solutions.### Approach: Your approach should be systematic:</h3>
<p>System Message</p>
<p>Now you are a researcher in the field of AI.You are good at selecting the ideas that meet the requirements.</p>
<p>User Message</p>
<h3>Task Description: You will be provided with some ideas you previously generated, and a research background.Your task is to select 5-6 ideas that best address the problems described in the research background (priority) and ideas that are relatively novel and feasible (secondary).</h3>
<p>System Message</p>
<p>Now you are a researcher in the field of AI with innovative and pioneering abilities.You are good at using innovative and original methods to solve cutting-edge problems in the field of AI.</p>
<p>User Message</p>
<h3>Task Description: You will be provided with the research background and the original ideas you previously generated.Your task is to refine these original ideas by filtering out those with low feasibility and insufficient novelty while enhancing the most critical and relevant ideas to make them more novel, feasible, targeted, and specific.If applicable, you may include formulas or algorithms to support the ideas.Additionally, please adhere to the following requirements: 1. Do not generate ideas that are repetitive or contradictory.2. Ensure that the generated ideas are coherent and form a cohesive whole.### Information Provided: 1. <strong>Research background</strong>: This is the starting point of the original idea and the basis for analyzing whether the idea should be filtered.-<strong>0</strong>: The generated idea and reference idea are completely unrelated with no discernible similarities.</h3>
<p>-<strong>1</strong>: The generated idea and reference idea have a vague connection, but differ significantly in their main concepts or approach.</p>
<p>-<strong>2</strong>: The generated idea and reference idea share a general concept but differ in most key aspects such as methodology or application.</p>
<p>-<strong>3</strong>: The generated idea and reference idea are similar in several areas, including general concept and some aspects of methodology, but differ in details or specific approaches.</p>
<p>-<strong>4</strong>: The generated idea and reference idea are largely similar in concept, methodology, and approach, with only minor differences in specifics.</p>
<p>-<strong>5</strong>: The generated idea and reference idea are nearly identical in all key aspects, including concept, methodology, and approach.</p>
<h3>Specific Information: I will provide you with specific information now, please use them according to the instructions above: Table 17: The prompt for scoring the novelty of an idea.</h3>
<p>System Message</p>
<p>You are an ambitious AI PhD student who is looking to publish a paper that will contribute significantly to the field.</p>
<p>You have an idea and you want to check if it is novel or not.I.e., not overlapping significantly with existing literature or already well explored.Be a harsh critic for novelty, ensure there is a sufficient contribution in the idea for a new conference or workshop paper.</p>
<p>You will be given access to the Semantic Scholar API, which you may use to survey the literature and find relevant papers to help you make your decision.</p>
<p>The top 10 results for any search query will be presented to you with the abstracts.</p>
<p>You will be given num rounds rounds to decide on the paper.At any round, compare the provided idea with the information found in the article and provide a novelty score from 0 to 10.In each search round, you should give a query and a novelty score based on the information in the relevant papers.</p>
<p>If there are no relevant papers, give a novelty score based on your own feelings.</p>
<p>User Message</p>
<p>Round current round/num rounds.You have this idea:</p>
<p>"idea"</p>
<p>The results of the last query are (empty on first round): "last query results"</p>
<p>Respond in the following format:</p>
<p>THOUGHT: <THOUGHT></p>
<p>RESPONSE:</p>
<p>′′′ json <JSON></p>
<p>′′′</p>
<p>In <THOUGHT>, first briefly reason over the idea and identify any query that could help you suggest a score based on its novelty.Then give your perceived novelty score.</p>
<p>In <JSON>, respond in JSON format with ONLY the following field:</p>
<p>-"Query": An optional search query to search the literature (e.g.attention is all you need).You must make a query if you have not decided this round.</p>
<p>-"Novelty Score": A novelty score from 0 to 10.</p>
<p>A query will work best if you are able to recall the exact name of the paper you are looking for, or the authors.This JSON will be automatically parsed, so ensure the format is precise.</p>
<p>(the JSON MUST contain the "Query" and the "Novelty Score") In the last round, you should assign a "" value to the "Query" even if you don't need to generate it.-"Clarity": Choose between 1 and 2 (If idea1 is better, fill in 1; otherwise, fill in 2. The same applies below.)-"Novelty": Choose between 1 and 2 -"Feasibility": Choose between 1 and 2 -"Generalizability": Choose between 1 and 2 -"summary": Choose between 1 and 2 This JSON will be automatically parsed, so ensure the format is precise.</p>
<p>Table 19: The prompt for comparing five ideas for their clarity, novelty, feasibility, and generalizability.</p>
<p>System Message</p>
<p>You are an artificial intelligence researcher with extensive knowledge in this field, and now you need to make a comprehensive comparison among five ideas.</p>
<p>You will obtain a comparison standard, compare every point on the standard, and make a overall ranking at the end.In <THOUGHT>, You can record your reasoning process to make your comparison more organized..</p>
<p>In <JSON>, respond in JSON format with ONLY the following field:</p>
<p>-"Clarity": Provide an array consisting of 1-5, representing each idea separately, with the better idea placed at the beginning (e.g.[4, 5, 3, 2, 1]) -"Novelty": Same as above.</p>
<p>-"Feasibility": Same as above.</p>
<p>-"Generalizability": Same as above.</p>
<p>-"Overall Ranking": Same as above.</p>
<p>This JSON will be automatically parsed, so ensure the format is precise.</p>
<p>1.The need to reduce hallucinated content in responses from LLM-based chatbots, which limits their reliability in sensitive domains like healthcare and education.2. The requirement for a training-free and easy-to-use method that can improve the credibility of chatbot responses without additional data annotation or extensive retraining.</p>
<p>Background</p>
<p>SciPIP generated idea</p>
<p>Idea 1: LIME-Based Post-Processing Filter with Reinforced Logical Verification Details: Enhance the basic LIME-based post-processing filter by integrating a reinforcement mechanism that goes beyond mere flagging of suspect segments.After LIME evaluates which parts of the response are grounded in factual data and which may be hallucinated, an additional logical verification step is applied using a lightweight, training-free logical consistency checker.This checker could employ principles from SATNet to ensure the logical coherence of the flagged segments.If logical inconsistencies are detected, the response can either be discarded, revised, or accompanied by a disclaimer highlighting the possibly unreliable segments.</p>
<p>Matched groundtruth idea</p>
<p>Step-by-step verification protocol for reasoning chains.This protocol formalizes the process of verifying the correctness of each step in a reasoning chain, including the relevance of each step, the attribution of each step to external sources, and the logical correctness of each step, enabling finegrained evaluation of reasoning verifiers..</p>
<p>LLM-based similarity score: 4</p>
<p>1.The need to improve the effectiveness of large language models in utilizing retrieved information for enhanced text generation in retrieval-augmented generation frameworks.2. The absence of a clear training mechanism that teaches LLMs to refine and integrate knowledge from retrieved texts of varying quality.</p>
<p>Background</p>
<p>SciPIP generated idea</p>
<p>Contrastive Learning for Enhanced Knowledge Integration: Employ contrastive learning techniques to train the LLM using pairs of relevant and irrelevant retrieval instances, with a contrastive loss function \( L_{\text{contrastive}} = d_{\text{pos}} -d_{\text{neg}} \).This approach helps the model distinguish between high-quality and low-quality information, improving the integration of the most informative and pertinent knowledge for text generation.</p>
<p>Matched groundtruth idea</p>
<p>A method for generating training data using Large Language Models (LLMs) that incorporates both positive and negative examples and employs a contrastive loss objective.This method leverages LLMs to generate a diverse set of training examples, which includes both positive utterances related to an intent and negative utterances that are unrelated or express a negation of the intent.By using a contrastive loss term, the model is encouraged to encode similar semantic meanings closer together in the embedding space, while pushing apart embeddings of differing or opposite meanings.This approach aims to enhance the model's semantic encoding capabilities, particularly for understanding negations and implicatures.</p>
<p>LLM-based similarity score: 4</p>
<p>1.The need to bridge the gap between the capabilities of current benchmarks and the real-world visually grounded tasks that require agents to process both visual and textual information.2. The desire to advance the development of autonomous agents by providing a rigorous assessment that simulates human interaction with modern computing interfaces.</p>
<p>Background</p>
<p>SciPIP generated idea</p>
<p>Interactive Contextual Scenario Emulation -Details: Design advanced contextual scenarios that require agents to interact dynamically with both text and visual content.These scenarios should emulate real-world tasks such as web browsing, multimedia interpretation, and virtual assistants navigating through user instructions.Implement a simulation environment where these tasks evolve based on the agent's interactions, measuring adaptability and decision-making in context-rich environments.The evaluation metrics should include task completion accuracy, adaptability to new information, and responsiveness to real-time changes, simulating human-computer interaction dynamics more rigorously.</p>
<p>Matched groundtruth idea</p>
<p>Contribution 1: Introduction of VisualWebArena, a Multimodal Benchmark for Web-based Tasks Details: This method introduces a novel benchmark suite called VisualWebArena, which is specifically designed to evaluate the performance of autonomous multimodal agents on visually grounded web tasks.It comprises a diverse set of web environments including Classifieds, Shopping, and Reddit.These environments contain realistic tasks that demand agents to process and understand image-text inputs, interpret natural language instructions, and execute actions on websites to achieve predefined objectives.</p>
<p>LLM-based similarity score: 4</p>
<p>Figure 5: Some more randomly picked samples of SciPIP proposed ideas.Matched groundtruth idea means ideas proposed in some paper of ACL 2024.</p>
<p>Figure 1 :
1
Figure 1: The pipeline of constructing the literature database.</p>
<p>Figure 2 :
2
Figure 2: The pipeline of SEC-based literature retrieval and literature clustering.Red words in the user-provided background are entity examples.</p>
<p>search in the literature database D for its semantic neighbors.Specifically, e (u) b</p>
<p>The direct proposal method (SciPIP-A).(b) A dual-path proposal method (SciPIP-B).(c)A dual-path proposal method (SciPIP-C).</p>
<p>Figure 3 :
3
Figure 3: Three pipelines for idea proposal.</p>
<p>Figure 4 :
4
Figure 4: Randomly picked samples of SciPIP proposed ideas.Matched groundtruth idea means ideas proposed in some paper of ACL 2024.</p>
<p>with the title, abstract, and introduction of a research paper.Your task is to generate a concise summary of what kind of problem does this paper aim to solve and what methods are proposed to address it.The summary should follow this format: The problem of [problem] can be addressed by [main idea/approach].Instructions: Title: Read the title to understand the general topic of the paper.Abstract: Read the abstract to get a concise summary of the research, including the problem addressed, the methods used, and the main findings.Introduction: Read the introduction to gain a deeper understanding of the background, significance, and specific problem the paper addresses, as well as the proposed approach or solution.Based on the provided information, generate a single sentence that captures the essence of the paper, following the format specified above.Your Turn: Given the following paper information: Title: title Abstract: abstract Introduction: introduction Output: The problem of [problem] can be addressed by [main idea/approach].</p>
<h3>Information Provided: 1. <strong>Abstract</strong>: It's the abstract directly extracted from the paper.2. <strong>Contributions</strong>: These are the contributions (methods) we have summarized based on the abstract and introduction of the paper.3. <strong>Text</strong>: It's the text directly extracted from the paper, containing the methodology of the paper.### Approach: Your approach should be systematic: -<strong>Step 1</strong>: Start by reading the abstract and contributions, to understand the main work of this paper.-<strong>Step 2</strong>: Then, read the text, to find information related to the contributions and ignore other information.If you think there is missing content in the contributions section, you can add one.On the contrary, if you think there is content duplication, merge or delete one.Please ensure that the final contributions have 2 to 4 entries.-<strong>Step 3</strong>: Finally, provide specific details for each contribution as detailed and comprehensive as possible based on the content in the text.If applicable, you may include formulas or algorithms to support the ideas.### Specific Information: I will provide you with specific information now, please use them according to the instructions above: 1. <strong>Abstract</strong>: {abstract} 2. <strong>Contribution</strong>: {contribution} 3. <strong>Text</strong>: {text} ### Format for Your Response: Your output should follow the format, and please note that your subject should not be 'the paper' but 'this method' or the specific method name: <strong>Idea 1</strong>: [The first method idea] -<strong>Details</strong>: [Details of the first idea] <strong>Idea 2</strong>: [The second method idea] -<strong>Details</strong>: [Details of the second idea] ...</h3>
<p>-<strong>Step 1</strong>: Thoroughly read the research problem to understand your primary focus.-<strong>Step 2</strong>: Review the summaries, backgrounds, and contributions (methods) of the related papers to gain a broader perspective and insights relevant to the problem.-<strong>Step 3</strong>: Based on the provided information, propose some ideas that are clear, innovative, valid, and comprehensive.### Specific Information: I will provide you with specific information now, please use them according to the instructions above: 1. <strong>Research Problem &amp; Rationales</strong>: {problem} 2. <strong>Related Papers</strong>: {related papers information} ### Format for Your Response: Please ensure that your final ideas include about 10 entries, presented in the following format: <strong>Idea 1</strong>: [The first method idea] <strong>Idea 2</strong>: [The second method idea] <strong>Idea 3</strong>: [The third method idea] ...</p>
<h3>Information Provided: 1. <strong>Ideas</strong>: These are the ideas you previously generated based on the research background and several related papers.2. <strong>Research Background</strong>: This document describes specific problems and challenges that need to be addressed.### Approach: Your approach should be systematic: -<strong>Step 1</strong>: Analyze the research background to understand the specific problems that need solutions.-<strong>Step 2</strong>: Critically review the ideas, selecting 5-6 ideas that are most effective in solving the problems in the research background (priority) and that are also relatively novel and feasible (secondary).### Specific Information: I will provide you with specific information now; please use them according to the instructions above: 1. <strong>Ideas</strong>: {idea} 2. <strong>Research Background</strong>: {background} ### Format for Your Response: Please ensure that your final ideas include 5-6 entries, whose content has not been modified.Don't generate any explanation and just present the filtered ideas as well as their content in the following format: <strong>Idea 1</strong>: [The first method idea] <strong>Idea 2</strong>: [The second method idea] <strong>Idea 3</strong>: [The third method idea] ...</h3>
<ol>
<li>
<p><strong>Original ideas</strong>: These are the ideas you previously generated based on research background and several related papers.### Approach: Your approach should be systematic: -<strong>Step 1</strong>: Thoroughly review the research background to understand the context and objectives.-<strong>Step 2</strong>: Analyze the original ideas critically, identifying aspects with low feasibility or insufficient novelty, and then filter out them.-<strong>Step 3</strong>: Enhance the most critical and relevant ideas by making them more novel, feasible, targeted, and specific.Incorporate formulas or algorithms if they strengthen the ideas.### Specific Information: I will provide you with specific information now, please use them according to the instructions above: 1. <strong>Research background</strong>: {background} 2. <strong>Original idea</strong>: {idea} ### Format for Your Response: Please ensure that your response only includes the final ideas, which include 2 to 4 entries, presented in the following format: <strong>Idea 1</strong>: [The first method idea] -<strong>Details</strong>: [Details of the first idea] <strong>Idea 2</strong>: [The second method idea] -<strong>Details</strong>: [Details of the second idea] ...</p>
</li>
<li>
<p><strong>Generated Idea</strong>: {idea} 2. <strong>Reference Idea</strong>: {reference idea} ### Format for Your Response: Your answer can only have one number (from 0 to 5), indicating the similarity score, and cannot contain any other content.</p>
</li>
</ol>
<p>Table 1 :
1
The number of proposed ideas that successfully matched ACL 2024 ideas.More highscoring ideas are better."#" means "the number of".The results with † are averaged over 1968 input backgrounds.
Proposal Methods Variants#Backgrounds/ #Proposed Ideas#Ideas of Similarity Score 4 3 2 10AI Scientist-100 / 400058 211 1238SciPIP-A100 / 3855 115 192712SciPIP-B100 / 3794 139 157754SciPIPSciPIP-C  †100 / 3885 117 177854SciPIP-C1968 / 763891 2305 3492 168169</p>
<p>Table 2 :
2
The win rate of proposed ideas in terms of novelty and feasibility.The ideas are classified in terms of their similarity scores with their most similar existing ideas.The experiments are done on SciPIP-C proposed 7638 ideas.
Similarity Score43210Novelty10.2%13.1%16.4%20.1%40.2%Feasibility19.1%11.5%16.7%25.5%23.2%</p>
<p>Table 3 :
3
The novelty scores of proposed ideas.The scores are evaluated by GPT-4o after comparing with similar papers in Semantic Scholar.
Proposal Methods#Backgrounds/ #Proposed Ideas 109#Ideas of Novelty Score 8 7 6 5 43 2 1 0AI Scientist100 / 4000 12 131 98 55 30 44 26 4 0 0SciPIP-A100 / 3850 92 145 73 37 16 148 0 0 0SciPIP-B100 / 3790 63 161 55 37 19 26 14 4 0 0SciPIP-C100 / 3730 67 155 64 40 15 20 10 2 0 0
the idea proposal part with AI Scientist.For this purpose, we make slight adjustments to the AI Scientist's process.Specifically, for the user-provided background T (u)</p>
<p>Table 4 :
4
The literature retrieval results.The groundtruth are the real citations of the tested papers.Recall 10 means the recall rate of the top 10 ranked papers among the retrieved literature compared to the ground truth citations.
Retrieval MethodsRecall 10Recall 20Recall 30Recall 40Recall 50AI ScientistNot ApplicableSCIMON-like0.3810.4810.5480.5870.616ResearchAgent-like0.3770.4840.5500.5980.622SciPIP (Ours)0.4190.5440.6150.6570.684</p>
<p>Table 5 :
5
Ablation studies for literature retrieval.SE means our proposed semantic-entity based retrieval, CC means citation co-occurrence, and CL means clustering.Semantics Entity SE CC CL Recall 10 Recall 20 Recall 30 Recall 40 Recall 50
✓0.3770.4840.5500.5980.622✓0.3160.3830.4210.4620.487✓✓0.3480.4280.4680.5060.529✓0.3830.4750.5480.6020.633✓✓0.3910.4970.5760.6240.668✓✓0.3950.5060.5740.6160.643✓✓✓0.4190.5440.6150.6570.684</p>
<p>Table 8 :
8
The prompt for summary, background, and main ideas extraction, namely τ 2 .</p>
<p>Table 11 :
11
The prompt for initial idea generation.</p>
<p>Table 12 :
12
The prompt for idea filtering.</p>
<p>Table 13 :
13
The prompt for idea improvement.</p>
<p>Table 14 :
14
The prompt for brainstorming.Message Now you are a researcher in the field of AI with innovative and pioneering abilities.You are good at generating creative and original ideas.
System User Message### Task Description: You are an AI researcher tasked with brainstorming initial, innovative ideas to address a given research problem in AI. Focus on generating diverse and creative approaches rather than finalized methods. The ideas can be rough and in their infancy but should cover a range of possible directions that could be explored further. ### Information Provided: -<strong>Research Background</strong>: {background} ### Approach:Your brainstorming should be systematic:-<strong>Step 1</strong>: Thoroughly understand the research background.-<strong>Step 2</strong>: Generate a list of 4 to 6 high-level ideas or directions thatcould potentially solve problems in the given background. Be creative,think outside the box, and avoid merely rephrasing existing methods.### Format for Your Response:Please present 4 to 6 ideas in the following format:<strong>Idea 1</strong>: [Brief description of the first idea]<strong>Idea 2</strong>: [Brief description of the second idea]...</p>
<p>Table 15 :
15
The prompt for picking out the most similar idea from several ideas.
System Message -### Task Description:You will be provided with an idea you previously generated, and somereference ideas. Your task is to select the idea that is most similar to theone you generated from the reference ideas.### Information Provided:1. <strong>Generated Idea</strong>: This is the idea you previously generated based onresearch background and several related papers.2. <strong>Reference Ideas</strong>: These are the ideas that you should select from.### Approach:Your approach should be systematic:User Message-<strong>Step 1</strong>: Analyze the generated idea to understand the methods it describes.-<strong>Step 2</strong>: Critically review the reference ideas, selecting the idea thatis most similar to the methods in the generated idea.### Specific Information:I will provide you with specific information now, please use them accordingto the instructions above:1. <strong>Idea</strong>: {idea}2. <strong>Reference Ideas</strong>: {reference ideas}### Format for Your Response:Your answer can only have one number (strating from 1), indicating thenumber of the most similar idea, and cannot contain any other content.</p>
<p>Table 16 :
16
The prompt for evaluating the similarity score between two ideas.
System Message -### Task Description:You will be provided with an idea you previously generated, and a refer-ence idea. Your task is to determine the similarity between the generatedidea and the reference idea and give a score from 0 to 5.### Information Provided:1. <strong>Generated Idea</strong>: This is the idea you previously generated based onresearch background and several related papers.2. <strong>Reference Idea</strong>: This is the idea we provide you with that you needto compare with the generated idea.### Approach:You should follow the following scoring criteria:User Message</p>
<p>Table 18 :
18
The prompt for comparing two ideas for their clarity, novelty, feasibility, and generalizability.
You are an artificial intelligence researcher with extensive knowledge inthis field, and now you need to make a comprehensive comparison betweenSystem Messagetwo ideas.You will obtain a comparison standard, compare every point on the stan-dard, and make a summary comparison at the end.
The code and the database are released at https://github.com/cheerss/SciPIP.
We use the GLM-4 released in May 20th, 2024 (glm4-20240520).
We use theGPT-4o released in May 13th, 2024 (gpt-4o-2024-05-13), which has an October 2023 knowledge cutoff.
https://www.semanticscholar.org/
Table10: The prompt for problem/rational generation.System MessageNow you are a researcher in the field of AI with innovative and pioneering abilities.You are good at proposing novel and valuable questions based on research background.User Message### Task Description: You will receive a research background along with summaries, backgrounds, and contributions (methods) of several related papers.Your task is to carefully analyze this information and propose a research problem that is original, clear, feasible, relevant, and significant to its field.Additionally, provide the rationales behind the proposed problem.
Researchagent: Iterative research idea generation over scientific literature with large language models. Jinheon Baek, Sujay Kumar Jauhar, Silviu Cucerzan, Sung Ju Hwang, 10.48550/arXiv.2404.077382024</p>
<p>. Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, 2023Xiaohuan Zhou, and Tianhang Zhu. Qwen technical report. CoRR, abs/2309.16609</p>
<p>A statistical interpretation of term specificity and its application in retrieval. Karen Spärck, Jones , J. Documentation. 6052004</p>
<p>The AI scientist: Towards fully automated open-ended scientific discovery. Chris Lu, Cong Lu, Robert Tjarko Lange, Jakob Foerster, Jeff Clune, David Ha, 10.48550/arXiv.2408.062922024</p>
<p>Training language models to follow instructions with human feedback. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll L Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul F Christiano, Jan Leike, Ryan Lowe, Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems. S Sanmi Koyejo, A Mohamed, Danielle Agarwal, K Belgrave, A Cho, Oh, NeurIPS; New Orleans, LA, USA2022. 2022. November 28 -December 9, 2022, 2022</p>
<p>Sentence-bert: Sentence embeddings using siamese bertnetworks. Nils Reimers, Iryna Gurevych, 10.18653/v1/D19-1410Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019. Kentaro Inui, Jing Jiang, Vincent Ng, Xiaojun Wan, the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019Hong Kong, ChinaAssociation for Computational LinguisticsNovember 3-7, 2019. 2019</p>
<p>Llama: Open and efficient foundation language models. Don R Swanson, CoRR, abs/2302.13971Undiscovered public knowledge. The Library Quarterly. Thibaut Hugo Touvron, Gautier Lavril, Xavier Izacard, Marie-Anne Martinet, Timothée Lachaux, Baptiste Lacroix, Naman Rozière, Eric Goyal, Faisal Hambro, Aurélien Azhar, Armand Rodriguez, Edouard Joulin, Guillaume Grave, Lample, 1986. 2023a56</p>
<p>. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton-Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing , Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, 2023bAurélien RodriguezAngela Fan, Melanie Kambadur; Robert Stojnic, Sergey Edunovand Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models. CoRR, abs/2307.09288</p>
<p>Scimon: Scientific inspiration machines optimized for novelty. Qingyun Wang, Doug Downey, Heng Ji, Tom Hope, 10.18653/v1/2024.acl-long.18Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. Long Papers. Lun-Wei Ku, Andre Martins, Vivek Srikumar, the 62nd Annual Meeting of the Association for Computational LinguisticsBangkok, ThailandAugust 11-16, 2024. 2024ACL 2024</p>
<p>. An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, Guanting Dong, Haoran Wei, Huan Lin, Jialong Tang, Jialin Wang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Ma, Jianxin Yang, Jin Xu, Jingren Zhou, Jinze Bai, Jinzheng He, Junyang Lin, Kai Dang, Keming Lu, Keqin Chen, Kexin Yang, Mei Li, Mingfeng Xue, Na Ni, Pei Zhang, Peng Wang, Ru Peng, Rui Men, Ruize Gao, Runji Lin, Shijie Wang, Shuai Bai, Sinan Tan, Tianhang Zhu, Tianhao Li, Tianyu Liu, Wenbin Ge, Xiaodong Deng, Xiaohuan Zhou, Xingzhang Ren, Xinyu Zhang, Xipin Wei, Xuancheng Ren, Xuejing Liu, Yang Fan, Yang Yao, Yichang Zhang, Yu Wan, Yunfei Chu, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, 2024Zhifang Guo, and Zhihao Fan. Qwen2 technical report. CoRR, abs/2407.10671</p>
<p>. Aohan Zeng, Bin Xu, Bowen Wang, Chenhui Zhang, Da Yin, Diego Rojas, Guanyu Feng, Hanlin Zhao, Hanyu Lai, Hao Yu, Hongning Wang, Jiadai Sun, Jiajie Zhang, Jiale Cheng, Jiayi Gui, Jie Tang, Jing Zhang, Juanzi Li, Lei Zhao, Lindong Wu, Lucen Zhong, Mingdao Liu, Minlie Huang, Peng Zhang, Qinkai Zheng, Rui Lu, Shuaiqi Duan, Shudan Zhang, Shulin Cao, Shuxun Yang, Weng Lam Tam, Wenyi Zhao, Xiao Liu, Xiao Xia, Xiaohan Zhang, Xiaotao Gu, Xin Lv, Xinghan Liu, Xinyi Liu, Xinyue Yang, Xixuan Song, Xunkai Zhang, Yifan An, Yifan Xu, Yilin Niu, Yuantao Yang, Yueyan Li, Yushi Bai, Yuxiao Dong, Zehan Qi, Zhaoyu Wang, Zhen Yang, Zhengxiao Du, Zhenyu Hou, Zihan Wang, 2024Chatglm: A family of large language models from GLM-130B to GLM-4 all tools. CoRR, abs/2406.12793</p>            </div>
        </div>

    </div>
</body>
</html>
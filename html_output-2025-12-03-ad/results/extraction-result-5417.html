<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5417 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5417</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5417</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-110.html">extraction-schema-110</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <p><strong>Paper ID:</strong> paper-abcb4bbf57100ac69a9e8d845893c8b7a8130b30</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/abcb4bbf57100ac69a9e8d845893c8b7a8130b30" target="_blank">Guiding Large Language Models to Post-Edit Machine Translation with Error Annotations</a></p>
                <p><strong>Paper Venue:</strong> NAACL-HLT</p>
                <p><strong>Paper TL;DR:</strong> This work exploits the complementary strengths of LLMs and supervised MT by guiding LLMs to automatically post-edit MT with external feedback on its quality, derived from Multidimensional Quality Metric (MQM) annotations.</p>
                <p><strong>Paper Abstract:</strong> Machine Translation (MT) remains one of the last NLP tasks where large language models (LLMs) have not yet replaced dedicated supervised systems. This work exploits the complementary strengths of LLMs and supervised MT by guiding LLMs to automatically post-edit MT with external feedback on its quality, derived from Multidimensional Quality Metric (MQM) annotations. Working with LLaMA-2 models, we consider prompting strategies varying the nature of feedback provided and then fine-tune the LLM to improve its ability to exploit the provided guidance. Through experiments on Chinese-English, English-German, and English-Russian MQM data, we demonstrate that prompting LLMs to post-edit MT improves TER, BLEU and COMET scores, although the benefits of fine-grained feedback are not clear. Fine-tuning helps integrate fine-grained feedback more effectively and further improves translation quality based on both automatic and human evaluation.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5417.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5417.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Chen2023</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Iterative translation refinement with large language models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Reports using a large closed LLM (GPT-3.5) to iteratively refine machine translation outputs by generating feedback and using it to improve the translation, evaluated across systems and languages.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Iterative translation refinement with large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI GPT-3.5 family (closed proprietary LLM); specific parameter count not provided in this paper's citation.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>self-generated iterative refinement</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>The LLM generates feedback about its own translation and then refines the translation based on that self-generated feedback (generate → critique/feedback → revise). Described as zero-shot refinement of MT outputs from a variety of systems and languages.</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Machine Translation post-editing (multilingual)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Refinement/post-editing of machine translation outputs produced by various MT systems across languages.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Reported a decrease in string-based quality metrics and comparable if not improved neural metrics; human evaluation indicated a reduction in 'translationese'. (No precise numeric scores are reported in this paper's discussion.)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Not numerically specified in this paper's mention; baseline refers to original MT outputs prior to LLM refinement.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_improvement</strong></td>
                            <td>Authors observed that iterative self-refinement reduced translationese in human evaluation and produced comparable or improved neural (referenced) metrics despite decreases in string-based metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Reported decreases in string-based metrics (suggesting possible harms on BLEU-like measures); results were obtained with large closed LLMs (GPT-3.5) which the current paper notes are different from the smaller open models it explores.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Guiding Large Language Models to Post-Edit Machine Translation with Error Annotations', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5417.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5417.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Raunak2023</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Leveraging GPT4 for automatic translation post-editing</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Uses GPT-4 to perform automatic post-editing of MT outputs by framing the process as an iterative reasoning/refinement (chain-of-thought) procedure, reporting improvements in neural MT evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Leveraging GPT4 for automatic translation post-editing</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI GPT-4 (closed proprietary large language model); exact architecture and parameter count not specified here.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>chain-of-thought / self-refinement</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Post-editing framed as chain-of-thought: the model generates intermediate reasoning/self-feedback and uses it to iteratively refine the translation output, improving quality measures.</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Automatic translation post-editing (MS Translator outputs across language pairs)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Post-edit MT outputs from a commercial MT system (Microsoft Translator), evaluated across multiple language pairs, with particular emphasis on outputs into English.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Reported improvements in COMET scores for MS Translator outputs across language pairs, particularly into English (no numeric values provided in this paper's mention).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Not numerically specified in this paper's mention; baseline is the original MS Translator outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_improvement</strong></td>
                            <td>Improved COMET (neural) metric scores reported for post-edited outputs when using GPT-4 with chain-of-thought style refinement.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Mentioned implicitly: work relied on a very large closed LLM (GPT-4); improvements reported on neural metrics (COMET) rather than necessarily on string-based metrics; results vary by language direction.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Guiding Large Language Models to Post-Edit Machine Translation with Error Annotations', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5417.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5417.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Xu2024-LLMRefine</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Llmrefine: Pinpointing and refining large language models via fine-grained actionable feedback</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Presents an iterative refinement/search procedure that takes an MT hypothesis and automatically generated MQM-style error information to improve LLM translations (PaLM2 used in the cited work).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Llmrefine: Pinpointing and refining large language models via fine-grained actionable feedback</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PaLM2</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>PaLM 2 family (Anil et al., 2023) — a large closed LLM referenced as the model used in the contemporaneous pre-print.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>iterative refinement cast as search</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>An iterative search-based refinement: the method uses automatically generated MQM-style actionable error information to guide a search/iterative refinement process that improves the LLM's original translations.</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>WMT translation tasks (Chinese→English and German→English)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Standard WMT translation evaluation tasks (WMT datasets) where LLM-generated translations are iteratively refined using automatically detected MQM-style errors.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Reported that the search-based iterative refinement improved the quality of the LLM's original translations on Chinese→English and German→English WMT tasks (no numeric values provided in this paper's mention).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Not numerically specified in this paper's mention; baseline refers to the LLM's initial translations prior to refinement.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_improvement</strong></td>
                            <td>Authors report improvements on WMT tasks when applying iterative/search refinement guided by MQM-style feedback; cited as evidence that refinement can enhance LLM translations.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Described work used large closed models (PaLM2); detailed numeric comparisons and failure modes are not recapitulated in this paper's mention.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Guiding Large Language Models to Post-Edit Machine Translation with Error Annotations', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5417.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e5417.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SelfRefine-Madaan2023</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-refine: Iterative refinement with self-feedback</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Introduces Self-Refine, an iterative generate-then-reflect approach where an LLM produces self-feedback on its outputs and then uses that feedback to refine them, applied across multiple tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Self-refine: Iterative refinement with self-feedback</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Various large LLMs (Self-Refine method)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Self-Refine is a method applied to large autoregressive LLMs to enable iterative self-correction; specific model variants used vary by experiment in the original work (not detailed here).</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>self-refine / generate-then-reflect</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>The model generates an initial output, then produces self-generated feedback (critique/explanation), and finally uses that feedback to produce a refined output (one or more generate→critique→revise cycles).</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Diverse tasks including generation, math reasoning, code optimization, and toxicity control</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>A broad set of tasks where initial generations are iteratively refined using self-generated feedback (e.g., program synthesis, reasoning, toxicity reduction).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Reported improvements on a range of tasks when using iterative self-refinement (no task-level numeric values reported in this paper's mention).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Not numerically specified in this paper's mention; baseline is model outputs without self-refinement.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_improvement</strong></td>
                            <td>Cited as demonstrating that LLMs can 'self-correct' an initial draft by iteratively refining based on self-provided feedback across multiple generation and reasoning tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Quality of improvement depends on the quality of self-generated feedback; not all tasks or metrics necessarily improve uniformly (details not provided in this paper's mention).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Guiding Large Language Models to Post-Edit Machine Translation with Error Annotations', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Iterative translation refinement with large language models <em>(Rating: 2)</em></li>
                <li>Leveraging GPT4 for automatic translation post-editing <em>(Rating: 2)</em></li>
                <li>Llmrefine: Pinpointing and refining large language models via fine-grained actionable feedback <em>(Rating: 2)</em></li>
                <li>Self-refine: Iterative refinement with self-feedback <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5417",
    "paper_id": "paper-abcb4bbf57100ac69a9e8d845893c8b7a8130b30",
    "extraction_schema_id": "extraction-schema-110",
    "extracted_data": [
        {
            "name_short": "Chen2023",
            "name_full": "Iterative translation refinement with large language models",
            "brief_description": "Reports using a large closed LLM (GPT-3.5) to iteratively refine machine translation outputs by generating feedback and using it to improve the translation, evaluated across systems and languages.",
            "citation_title": "Iterative translation refinement with large language models",
            "mention_or_use": "mention",
            "model_name": "GPT-3.5",
            "model_description": "OpenAI GPT-3.5 family (closed proprietary LLM); specific parameter count not provided in this paper's citation.",
            "reflection_method_name": "self-generated iterative refinement",
            "reflection_method_description": "The LLM generates feedback about its own translation and then refines the translation based on that self-generated feedback (generate → critique/feedback → revise). Described as zero-shot refinement of MT outputs from a variety of systems and languages.",
            "num_iterations": null,
            "task_name": "Machine Translation post-editing (multilingual)",
            "task_description": "Refinement/post-editing of machine translation outputs produced by various MT systems across languages.",
            "performance_with_reflection": "Reported a decrease in string-based quality metrics and comparable if not improved neural metrics; human evaluation indicated a reduction in 'translationese'. (No precise numeric scores are reported in this paper's discussion.)",
            "performance_without_reflection": "Not numerically specified in this paper's mention; baseline refers to original MT outputs prior to LLM refinement.",
            "has_performance_comparison": true,
            "evidence_of_improvement": "Authors observed that iterative self-refinement reduced translationese in human evaluation and produced comparable or improved neural (referenced) metrics despite decreases in string-based metrics.",
            "limitations_or_failure_cases": "Reported decreases in string-based metrics (suggesting possible harms on BLEU-like measures); results were obtained with large closed LLMs (GPT-3.5) which the current paper notes are different from the smaller open models it explores.",
            "uuid": "e5417.0",
            "source_info": {
                "paper_title": "Guiding Large Language Models to Post-Edit Machine Translation with Error Annotations",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "Raunak2023",
            "name_full": "Leveraging GPT4 for automatic translation post-editing",
            "brief_description": "Uses GPT-4 to perform automatic post-editing of MT outputs by framing the process as an iterative reasoning/refinement (chain-of-thought) procedure, reporting improvements in neural MT evaluation.",
            "citation_title": "Leveraging GPT4 for automatic translation post-editing",
            "mention_or_use": "mention",
            "model_name": "GPT-4",
            "model_description": "OpenAI GPT-4 (closed proprietary large language model); exact architecture and parameter count not specified here.",
            "reflection_method_name": "chain-of-thought / self-refinement",
            "reflection_method_description": "Post-editing framed as chain-of-thought: the model generates intermediate reasoning/self-feedback and uses it to iteratively refine the translation output, improving quality measures.",
            "num_iterations": null,
            "task_name": "Automatic translation post-editing (MS Translator outputs across language pairs)",
            "task_description": "Post-edit MT outputs from a commercial MT system (Microsoft Translator), evaluated across multiple language pairs, with particular emphasis on outputs into English.",
            "performance_with_reflection": "Reported improvements in COMET scores for MS Translator outputs across language pairs, particularly into English (no numeric values provided in this paper's mention).",
            "performance_without_reflection": "Not numerically specified in this paper's mention; baseline is the original MS Translator outputs.",
            "has_performance_comparison": true,
            "evidence_of_improvement": "Improved COMET (neural) metric scores reported for post-edited outputs when using GPT-4 with chain-of-thought style refinement.",
            "limitations_or_failure_cases": "Mentioned implicitly: work relied on a very large closed LLM (GPT-4); improvements reported on neural metrics (COMET) rather than necessarily on string-based metrics; results vary by language direction.",
            "uuid": "e5417.1",
            "source_info": {
                "paper_title": "Guiding Large Language Models to Post-Edit Machine Translation with Error Annotations",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "Xu2024-LLMRefine",
            "name_full": "Llmrefine: Pinpointing and refining large language models via fine-grained actionable feedback",
            "brief_description": "Presents an iterative refinement/search procedure that takes an MT hypothesis and automatically generated MQM-style error information to improve LLM translations (PaLM2 used in the cited work).",
            "citation_title": "Llmrefine: Pinpointing and refining large language models via fine-grained actionable feedback",
            "mention_or_use": "mention",
            "model_name": "PaLM2",
            "model_description": "PaLM 2 family (Anil et al., 2023) — a large closed LLM referenced as the model used in the contemporaneous pre-print.",
            "reflection_method_name": "iterative refinement cast as search",
            "reflection_method_description": "An iterative search-based refinement: the method uses automatically generated MQM-style actionable error information to guide a search/iterative refinement process that improves the LLM's original translations.",
            "num_iterations": null,
            "task_name": "WMT translation tasks (Chinese→English and German→English)",
            "task_description": "Standard WMT translation evaluation tasks (WMT datasets) where LLM-generated translations are iteratively refined using automatically detected MQM-style errors.",
            "performance_with_reflection": "Reported that the search-based iterative refinement improved the quality of the LLM's original translations on Chinese→English and German→English WMT tasks (no numeric values provided in this paper's mention).",
            "performance_without_reflection": "Not numerically specified in this paper's mention; baseline refers to the LLM's initial translations prior to refinement.",
            "has_performance_comparison": true,
            "evidence_of_improvement": "Authors report improvements on WMT tasks when applying iterative/search refinement guided by MQM-style feedback; cited as evidence that refinement can enhance LLM translations.",
            "limitations_or_failure_cases": "Described work used large closed models (PaLM2); detailed numeric comparisons and failure modes are not recapitulated in this paper's mention.",
            "uuid": "e5417.2",
            "source_info": {
                "paper_title": "Guiding Large Language Models to Post-Edit Machine Translation with Error Annotations",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "SelfRefine-Madaan2023",
            "name_full": "Self-refine: Iterative refinement with self-feedback",
            "brief_description": "Introduces Self-Refine, an iterative generate-then-reflect approach where an LLM produces self-feedback on its outputs and then uses that feedback to refine them, applied across multiple tasks.",
            "citation_title": "Self-refine: Iterative refinement with self-feedback",
            "mention_or_use": "mention",
            "model_name": "Various large LLMs (Self-Refine method)",
            "model_description": "Self-Refine is a method applied to large autoregressive LLMs to enable iterative self-correction; specific model variants used vary by experiment in the original work (not detailed here).",
            "reflection_method_name": "self-refine / generate-then-reflect",
            "reflection_method_description": "The model generates an initial output, then produces self-generated feedback (critique/explanation), and finally uses that feedback to produce a refined output (one or more generate→critique→revise cycles).",
            "num_iterations": null,
            "task_name": "Diverse tasks including generation, math reasoning, code optimization, and toxicity control",
            "task_description": "A broad set of tasks where initial generations are iteratively refined using self-generated feedback (e.g., program synthesis, reasoning, toxicity reduction).",
            "performance_with_reflection": "Reported improvements on a range of tasks when using iterative self-refinement (no task-level numeric values reported in this paper's mention).",
            "performance_without_reflection": "Not numerically specified in this paper's mention; baseline is model outputs without self-refinement.",
            "has_performance_comparison": true,
            "evidence_of_improvement": "Cited as demonstrating that LLMs can 'self-correct' an initial draft by iteratively refining based on self-provided feedback across multiple generation and reasoning tasks.",
            "limitations_or_failure_cases": "Quality of improvement depends on the quality of self-generated feedback; not all tasks or metrics necessarily improve uniformly (details not provided in this paper's mention).",
            "uuid": "e5417.3",
            "source_info": {
                "paper_title": "Guiding Large Language Models to Post-Edit Machine Translation with Error Annotations",
                "publication_date_yy_mm": "2024-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Iterative translation refinement with large language models",
            "rating": 2
        },
        {
            "paper_title": "Leveraging GPT4 for automatic translation post-editing",
            "rating": 2
        },
        {
            "paper_title": "Llmrefine: Pinpointing and refining large language models via fine-grained actionable feedback",
            "rating": 2
        },
        {
            "paper_title": "Self-refine: Iterative refinement with self-feedback",
            "rating": 2
        }
    ],
    "cost": 0.01659325,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Guiding Large Language Models to Post-Edit Machine Translation with Error Annotations</h1>
<p>Dayeon Ki<br>Computer Science<br>University of Maryland<br>dayeonki@cs.umd.edu</p>
<h2>Marine Carpuat</h2>
<p>Computer Science, UMIACS
University of Maryland
marine@cs.umd.edu</p>
<h2>Abstract</h2>
<p>Machine Translation (MT) remains one of the last NLP tasks where large language models (LLMs) have not yet replaced dedicated supervised systems. This work exploits the complementary strengths of LLMs and supervised MT by guiding LLMs to automatically postedit MT with external feedback on its quality, derived from Multidimensional Quality Metric (MQM) annotations. Working with LLaMA-2 models, we consider prompting strategies varying the nature of feedback provided and then fine-tune the LLM to improve its ability to exploit the provided guidance. Through experiments on Chinese-English, English-German, and English-Russian MQM data, we demonstrate that prompting LLMs to post-edit MT improves TER, BLEU and COMET scores, although the benefits of fine-grained feedback are not clear. Fine-tuning helps integrate finegrained feedback more effectively and further improves translation quality based on both automatic and human evaluation. ${ }^{1}$</p>
<h2>1 Introduction</h2>
<p>Machine Translation (MT) remains one of the last NLP tasks where large language models (LLMs) have not yet replaced dedicated supervised systems. LLMs such as ChatGPT (Ouyang et al., 2022) started outperforming commercial MT systems very recently (Vilar et al., 2023; Hendy et al., 2023; Jiao et al., 2023). However, supervised models continue to outperform LLMs in numerous language pairs (Zhu et al., 2023; Kocmi et al., 2023), and the performance of LLMs remains uneven, exhibiting significant variation across models, languages, and translation directions (Bawden and Yvon, 2023; Zhu et al., 2023). This suggests that LLMs and supervised systems possess complementary strengths, and that combining them should offer some benefits.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" />   Figure 1: Guiding LLMs with external feedback enhances MT post-editing capabilities. We categorize feedback into different granularity: Generic, Score-based, and Fine-grained. Fine-grained feedback is annotated either by humans or automatic evaluation tools.</p>
<div class="codehilite"><pre><span></span><code><span class="nv">In</span><span class="w"> </span><span class="nv">this</span><span class="w"> </span><span class="nv">work</span>,<span class="w"> </span><span class="nv">we</span><span class="w"> </span><span class="nv">propose</span><span class="w"> </span><span class="nv">to</span><span class="w"> </span><span class="nv">leverage</span><span class="w"> </span><span class="nv">LLM</span><span class="s1">&#39;s text rewriting abilities (Brown et al., 2020; Reif et al., 2022; Raheja et al., 2023; Alves et al., 2024) to improve MT outputs given error annotations. If we provide an LLM with a source sentence, a MT translation of arbitrary origin, and some feedback on the quality of the MT (Figure 1), can we reliably improve the quality of the MT? This approach can be seen as revisiting the task of MT post-editing (Knight and Chander, 1994; Simard et al., 2007) in the light of recent work highlighting LLMs&#39;</span><span class="w"> </span><span class="nv">ability</span><span class="w"> </span><span class="nv">to</span><span class="w"> </span><span class="nv">refine</span><span class="w"> </span><span class="nv">its</span><span class="w"> </span><span class="nv">own</span><span class="w"> </span><span class="nv">outputs</span><span class="w"> </span><span class="ss">(</span><span class="nv">Madaan</span><span class="w"> </span><span class="nv">et</span><span class="w"> </span><span class="nv">al</span>.,<span class="w"> </span><span class="mi">2023</span><span class="c1">; Zeng et al., 2023; Chen et al., 2023). Indeed Chen et al. (2023); Raunak et al. (2023); Xu et al. (2024) recently show the promise of using LLMs for improving MT via refinement. We depart from these three papers by guiding the refinement abilities of LLMs with external feedback rather than self-generated feedback, and by post-editing outputs from arbitrary models rather than improve the LLM&#39;s own outputs only. Perhaps most importantly, while they relied exclusively on the largest closed LLMs - GPT3.5 (Brown et al., 2020), GPT4</span>
</code></pre></div>

<p>(OpenAI, 2023), PaLM-2 (Anil et al., 2023) - we argue that it is also worth exploring to what extent LLMs of more moderate size (e.g., 7B, 13B) can perform post-editing, as such models are less costly to train, run, and deploy in actual applications. This leads us to explore a different set of strategies. We further work with open models facilitating reproducibility of our results and hopefully encourages others to build on this work.</p>
<p>We explore a range of techniques to guide LLaMA-2 models (Touvron et al., 2023) to improve MT outputs using fine-grained feedback derived from Multidimensional Quality Metric (MQM) annotations (Freitag et al., 2021), as shown in Figure 1. Following prior work on refinement, we start with evaluating the impact of such feedback when prompting LLMs in zero-shot and few-shot settings (§5). Different from prior work, we then explore fine-tuning the LLM to advance its ability to improve translations based on the feedback provided in the prompt, in an instruction following style (Taori et al., 2023) (§6).</p>
<p>Through extensive experiments with three language pairs (Chinese-English, English-German, and English-Russian), we show that prompting LLMs to edit MT with feedback reliably improves translation quality as measured by automatic metrics, particularly in the few shot settings where the LLaMA-2 7B model achieves close peformance to the 13B version (§5). However, the models are unable to make the most of the fine-grained feedback which performs roughly on par with generic prompts for improvement. Instruction fine-tuning shows stronger improvements on translation quality based on both automatic and human evaluation (§6). Our analysis reveals that prompting the finetuned LLMs with fine-grained feedback not only helps fix the errors highlighted in the prompt (§7), but also leads to more natural outputs.</p>
<h2>2 Related Work</h2>
<p>MT Error Annotation. An increasing body of work seeks to evaluate MT by providing actionable feedback rather than a single score aggregating diverse dimensions of quality. Freitag et al. (2021) introduce an evaluation methodology based on the multi-dimensional human evaluation (MQM) framework (Lommel et al., 2014) to guide human annotators in identifying spans of translated text that are errors, labeling their types and severity level using a rich taxonomy. Their work inspired
automatic approaches to error annotation, building on existing work on automatic evaluation of text generation (Sellam et al., 2020; Fu et al., 2023). These include generating a scalar score to represent MT quality as a whole (Xu et al., 2024; Fu et al., 2023; Fernandes et al., 2023), and more nuanced methods that detail error severity (Kocmi and Federmann, 2023b), error span, and type (Kocmi and Federmann, 2023a), aligning closely with human judgements (Liu et al., 2023). Additionally, learned evaluation metrics have also emerged, pinpointing fine-grained aspects (error span, type, severity level) of MT errors (Guerreiro et al., 2023; Xu et al., 2024) and providing detailed error explanations (Xu et al., 2023). We build on this work by comparing them using human annotated vs. machine annotated errors as feedback to refine MT outputs.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Paper</th>
<th style="text-align: left;">Model</th>
<th style="text-align: left;">Feedback</th>
<th style="text-align: left;">Prompting</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Chen et al. (2023)</td>
<td style="text-align: left;">ChatGPT</td>
<td style="text-align: left;">Self-generated</td>
<td style="text-align: left;">Zero-shot</td>
</tr>
<tr>
<td style="text-align: left;">Raunak et al. (2023)</td>
<td style="text-align: left;">GPT-4</td>
<td style="text-align: left;">Self-generated</td>
<td style="text-align: left;">Zero-shot</td>
</tr>
<tr>
<td style="text-align: left;">Xu et al. (2024)</td>
<td style="text-align: left;">PaLM</td>
<td style="text-align: left;">Self-generated</td>
<td style="text-align: left;">Zero-shot</td>
</tr>
<tr>
<td style="text-align: left;">Ours</td>
<td style="text-align: left;">LLaMA-2</td>
<td style="text-align: left;">External</td>
<td style="text-align: left;">Zero-, Few-shot, Fine-tune</td>
</tr>
</tbody>
</table>
<p>Table 1: Smaller models lead us to explore a wider range of settings for post-editing with LLMs.</p>
<p>MT Post-Editing. Recognizing that translation is an iterative process, automatic post-editing originally aimed to improve an original MT provided as input together with the source text (Knight and Chander, 1994; Simard et al., 2007; Chatterjee et al., 2018). Approaches have mirrored progress in MT, starting with statistical phrase-based models (Simard et al., 2007), multi-source neural encoderdecoder models (Junczys-Dowmunt and Grundkiewicz, 2016) and non-autoregressive Transformers (Gu et al., 2019; Wan et al., 2020). Most recent work relies on LLMs, relaxing the requirement for supervised examples of post-editing. Chen et al. (2023) perform refine MT outputs from a wide range of systems and languages using GPT3.5 (Brown et al., 2020), leading to a decrease of string-based quality metrics and comparable if not improved neural metrics. Human evaluation showed that this approach primarily reduces "translationese" in MT outputs. Raunak et al. (2023) frame post-editing as chain-of-thought (Kojima et al., 2023) and show that GPT-4 (OpenAI, 2023) improves COMET scores for MS Translator outputs across language pairs, particularly into English. Finally, in a contemporaneous pre-print, Xu et al. (2024) cast iterative refinement as a search</p>
<table>
<thead>
<tr>
<th>Category</th>
<th>Prompt</th>
</tr>
</thead>
<tbody>
<tr>
<td>Generic</td>
<td>Improve the translation from English to German without any explanation.</td>
</tr>
<tr>
<td></td>
<td>English: The newer items are bagged only.</td>
</tr>
<tr>
<td></td>
<td>German: Neue Gegenstände werden nur mit Gepäck versehen.</td>
</tr>
<tr>
<td></td>
<td>Improved German:</td>
</tr>
<tr>
<td>Score</td>
<td>Improve the translation from English to German without any explanation. This translation is scored 85 out of 100.</td>
</tr>
<tr>
<td></td>
<td>English: The newer items are bagged only.</td>
</tr>
<tr>
<td></td>
<td>German: Neue Gegenstände werden nur mit Gepäck versehen.</td>
</tr>
<tr>
<td></td>
<td>Improved German:</td>
</tr>
<tr>
<td>Fine-grained</td>
<td>Improve the translation from English to German based on the identified errors without any explanation.</td>
</tr>
<tr>
<td></td>
<td>(1) There is a major mistranslation error at "mit Gepäck versehen".</td>
</tr>
<tr>
<td></td>
<td>English: The newer items are bagged only.</td>
</tr>
<tr>
<td></td>
<td>German: Neue Gegenstände werden nur mit Gepäck versehen.</td>
</tr>
<tr>
<td></td>
<td>Improved German:</td>
</tr>
</tbody>
</table>
<p>Table 2: Exemplar prompt template of English-German language pair used for prompting experiments. The part highlighted in orange is the added component from the Generic prompt accordingly to each feedback category.
process that takes as input a current MT and automatically generated MQM style error information. Using the PaLM2 LLM (Anil et al., 2023), they show that this search improves the quality of the LLM's original translations on ChineseEnglish and German-English WMT tasks. Building on these encouraging results obtained with large closed models, we investigate whether smaller open LLMs can also achieve strong post-editing capabilities, which leads to explore a wider range of settings as summarized in Table 1.</p>
<p>LLM Self-Refinement. LLMs have been reported to "self-correct" an initial draft by iteratively refining it based on self-provided feedback for many tasks Pan et al. (2023). Briefly, past work has focused on generation tasks including mathematical program synthesis, lexically-constrained generation, and toxicity control (Welleck et al., 2023), reasoning tasks (Paul et al., 2024), and a range of generation, math reasoning, and code optimization tasks (Madaan et al., 2023), among others. Many works focus on incorporating self-refinement to MT (Chen et al., 2023; Raunak et al., 2023; Xu et al., 2024) where given source and MT translation, LLMs generate feedback and improve upon it. In the same vein, we study MT refinement with an LLM, but incorporate error annotations from various source as feedback to refine MT outputs.</p>
<h2>3 Method</h2>
<p>We consider two strategies for guiding language models to edit MT error annotations: prompting and fine-tuning with instructions.</p>
<h3>3.1 Prompting</h3>
<p>We consider zero-shot and few-shot prompting. The specific prompt templates used for each feedback level are outlined in Table 2, and provide a source text, a MT output and depending on the condition some feedback on the quality of the MT. We opt to construct our prompt templates in English, rather than the target language, as they have shown better performance (Lin et al., 2022), likely due to the greater prevalence of English in the pre-training data (Ahuja et al., 2023).</p>
<p>Our study encompasses the following forms of feedback for each model, as illustated in Table 2:</p>
<ul>
<li>Generic: The model is prompted to improve the initial translation without any specific external feedback.</li>
<li>Score: A single scalar MQM score ${ }^{2}$, reflecting the initial translation's overall quality, is provided to the model. We normalize the scores on a range from 0 to 100 .</li>
<li>Fine-grained: The model is provided with finegrained feedback (error span, type, severity level) in the MQM style.</li>
</ul>
<p>For the Fine-grained condition, we consider three distinct sources of error annotation:</p>
<ul>
<li>MQM: human annotation from the MQM WMT22 dataset (Kocmi et al., 2022).</li>
<li>InstructScore: automatic annotation by InstructScore (Xu et al., 2023), an explainable text</li>
</ul>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th>Language pair</th>
<th># of train</th>
<th># of dev</th>
<th># of test</th>
</tr>
</thead>
<tbody>
<tr>
<td>Zh-En</td>
<td>22,373 / 3,200</td>
<td>200</td>
<td>1,000</td>
</tr>
<tr>
<td>En-De</td>
<td>13,215 / 3,200</td>
<td>200</td>
<td>1,000</td>
</tr>
<tr>
<td>En-Ru</td>
<td>19,450 / 3,200</td>
<td>200</td>
<td>1,000</td>
</tr>
</tbody>
</table>
<p>generation evaluation metric, which fine-tunes LLaMA <em>Touvron et al. (2023)</em> to predict MQM style fine-grained error annotations. This metric only supports Chinese-English.</p>
<ul>
<li>xCOMET: automatic annotation by xCOMET <em>Guerreiro et al. (2023)</em>, an automatic evaluation and quality estimation tool, which fine-tunes XLM-RoBERTa <em>Conneau et al. (2020)</em> to predict both MQM and Direct Assessment <em>Graham et al. (2013)</em> annotations of MT quality.</li>
</ul>
<p>The three methods use different severity level ranges, and xCOMET does not provide error type information. See Appendix A for further details.</p>
<h3>3.2 Fine-tuning</h3>
<p>In the fine-tuning case, we focus on two types of feedback: generic and fine-grained feedback, to establish the ability of fine-tuning to guide LLMs for post-editing. First, generic and fine-grained feedback consistently shows better performance compared to the score-based baseline. Second, fine-grained feedback uses human annotation thus disentangling error annotation errors from post-editing errors. We leave the exploration of automatically generated feedback to future work.</p>
<p>For fine-grained feedback, we explore two finetuning settings: (1) Bilingual, where we individually fine-tune for each language pair and (2) Multilingual, where we combine three language pairs to fine-tune a single model. We construct finetuning datasets from two sources of MT human-annotated with errors: MQM <em>Freitag et al. (2021)</em> and DEMETR <em>Karpinska et al. (2022)</em>. DEMETR provides MT error annotations in 10 source languages into English direction. Therefore, we use De-En from DEMETR as En-De pair and Ru-En as En-Ru. We reformulate all annotations in an instruction-following style (see Appendix Table 10 for examples). The fine-tuning data statistics are summarized in Table 3. We automatically filter out instances that share identical source or target sentences with those in the test set to ensure a clean train/test separation.</p>
<h2>4 Experimental Setup</h2>
<h3>4.1 Datasets</h3>
<p>Data. We experiment with WMT-22 General machine translation task submissions <em>Kocmi et al. (2022)</em> annotated with MQM dimensions. We focus on three language pairs: Chinese-English (zhen), English-German (en-de), and English-Russian (en-ru). We evaluate on 1,000 WMT data instances for each language pair. Each sample contains one error span of average length ranging from 9 for En-Ru to 13 for Zh-En. Adequacy errors and minor errors dominate across languages. See Appendix C.1 for further details.</p>
<p>Table 3: Dataset statistics for fine-tuning instruction datasets. We use DEMETR as train set and split the MQM dataset into train, validation, and test sets. For # of train column, we represent as # of train (MQM) / # of train (DEMETR).</p>
<p>Error Annotations. In addition to the manual error annotations described above, we obtain automatic annotations of the same data using InstructScore and xCOMET.</p>
<p>To assess how much these different annotations agree with each other, we compute the overlap frequency for each pair of annotation method on a random sample of 200 test cases per language pairs. The overlap frequency measures how often error spans match across two sources of annotations. We observe that the overlap frequency between MQM and xCOMET is 33/200 for En-De and 42/200 for En-Ru. Notably, for Zh-En pair, xCOMET and InstructScore show the highest concordance (51/200), while overlaps with MQM are lower (24/200 with xCOMET and 25/200 with InstructScore). This discrepancy underscores that the automatic annotations are far from perfect. We will test whether they can nevertheless be useful.</p>
<h3>4.2 Metrics</h3>
<p>We report the traditional BLEU metric <em>Papineni et al. (2002)</em> with exponential smoothing as implemented in the sacrebleu toolkit <em>Post (2018)</em>, the Translation Edit Rate (TER) <em>Snover et al. (2006)</em> which is the minimum number of edits</p>
<h3>4.2.1.2 Data</h3>
<p>In this section, we present a table of data from the</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Zero- and 10-shot prompting results for LLaMA-2 7B. <em>Top</em>: BLEU scores for Chinese-English (Zh-En), English-German (En-De), English-Russian (En-Ru) pairs; <em>Middle</em>: Translation Edit Rate (TER) where zero-shot results show the amount increased compared to that of 10-shot; <em>Bottom</em>: COMETDA scores. Note that we only report the supporting language pair (zh-en) results for InstructScore. Numerical results for both 7B and 13B are in Appendix E.1.</p>
<p>needed to change a hypothesis so that it exactly matches one of the references, normalized by the average length of the references, and a modern neural metric, the reference-based COMETDA score (Rei et al., 2020). Scores for all these metrics are reported in the 0-1 range.</p>
<h3>4.3 Models</h3>
<p>We employ the widely-used open-source LLM LLaMA-2 (Touvron et al., 2023), experimenting with the 7B and 13B variants.<sup>5</sup></p>
<p><strong>Prompting settings.</strong> We set the temperature to 0 for greedy decoding throughout all experiments (Xu et al., 2023). Through this, we ensure to reduce sampling variations of getting inconsistent generations. For 10-shot prompting, the in-context examples are chosen randomly.</p>
<p><strong>Fine-tuning settings.</strong> We adopt QLoRA (Dettmers et al., 2023), quantized version of LoRA (Hu et al., 2022), for parameter-efficient fine-tuning. For LoRA configs, we set the LoRA rank to 16, scaling parameter to 32, and dropout probability for layers at 0.05. We fine-tune all of the available training parameters, which is approximately 0.16B (4.4%) of the total parameters. We use the Adam optimizer with an initial learning rate to 2e-4, batch size of 2, gradient accumulation over 4 steps, with a warmup phase of 20 steps. We train over 5 epochs, evaluating the model's performance on 200 MQM validation set instances at the end of each epoch. We implement early stopping to halt the fine-tuning process if there is no improvement in the model performance for 16 consecutive steps.</p>
<h3>5 Prompting Results</h3>
<p>Figure 2 shows the zero- and 10-shot prompting performance of LLaMA-2 7B across three language pairs. The complete results in table form for both LLaMA-2 7B and 13B can be found in Appendix E.</p>
<p><strong>Zero-Shot.</strong> For all language pairs, we observe a <em>marginal improvement when post-editing with any form of feedback in zero-shot settings</em>, with small increases in BLEU COMETDA scores, along with reduced TER. Although the score differences between the original and post-edited MT can be small, they are statistically significant (<em>p</em> ≤ 0.05).</p>
<p><sup>5</sup>As a sanity check, we prompted the LLaMA models to translate our WMT-22 test set. The resulting translation quality (Appendix E.4) suggests that WMT-22 was not included in pre-training, and is therefore a valid test set.</p>
<p>for all cases. One exception is Zh-En pair, for which BLEU drops by 0.01 to 0.02 points after integrating feedback other than MQM.</p>
<p>Few-Shot. The improvements from zero to 10shot prompting are shown by hashed lines in Figure 2. The performance gap between the original and post-edited MT widens with few-shot learning. We examine a consistent gain in both BLEU and $\operatorname{COMET}<em _mathrm_DA="\mathrm{DA">{\mathrm{DA}}$ scores, which represent the overall MT quality. The average gain across language pairs is +0.04 BLEU (on a 0-1 scale) and +0.03 for $\operatorname{COMET}</em>$. TER, which measures the remaining amount of edits to be made also shows -0.03 point improvement for Zh-En, -0.06 point for En-De, and -0.04 point for En-Ru.}</p>
<p>7B vs 13B. The 13B model unsurprisingly achieve higher BLEU and $\operatorname{COMET}_{\mathrm{DA}}$ and lower TER compared to the 7B model in zero-shot settings. However, this performance gap narrows down with the increase in number of few-shot examples. This trend suggests that few-shot learning helps bridge the performance gap between model sizes for MT post-editing. We report comprehensive results on LLaMA-2 13B in Appendix E.</p>
<p>Feedback Granularity. We categorize external feedback into three granularity levels: generic, score-based, and fine-grained error annotation. Fine-grained feedback is further divided into human-annotated (MQM) and automatically detected by metrics (xCOMET, InstructScore). We observe that differences in the automatic metrics across different types of feedback are small. Providing fine-grained feedback on errors has limited benefits over a generic feedback while score-based feedback shows to have the least improvement in the MT output. Overall, the performance difference between various granularity of feedback is more evident for zero-shot setting while increasing to 10-shot prompting paints a different picture.</p>
<p>For 10-shot prompting, most forms of our tested feedback, regardless of granularity, converge to a similar performance. However, while the two MT quality metrics, BLEU and $\operatorname{COMET}_{\mathrm{DA}}$ remains similar for different forms of feedback, there is a clear difference for TER. When providing generic feedback or automatic annotations from xCOMET, TER decreases by approximately 0.15 points for Zh-En and 0.3 points for En-De and En-Ru compared to the original baseline. Score-based feedback remains to show the least increase in perfor-
mance, but they also decrease 0.1 points for Zh-En and 0.2 points for En-De and En-Ru, which are statistically significant. Nevertheless, prompting does not reveal a marked advantage for using certain type of feedback for post-editing.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Language</th>
<th style="text-align: center;">Type</th>
<th style="text-align: center;">BLEU ( $\uparrow$ )</th>
<th style="text-align: center;">TER ( $\downarrow$ )</th>
<th style="text-align: center;">COMET ( $\uparrow$ )</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Zh-En</td>
<td style="text-align: center;">Original</td>
<td style="text-align: center;">0.47</td>
<td style="text-align: center;">0.75</td>
<td style="text-align: center;">0.70</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">prompt $(k=0)$</td>
<td style="text-align: center;">0.48</td>
<td style="text-align: center;">0.72</td>
<td style="text-align: center;">0.70</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">prompt $(k=10)$</td>
<td style="text-align: center;">0.51</td>
<td style="text-align: center;">0.65</td>
<td style="text-align: center;">0.72</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">FT (Generic)</td>
<td style="text-align: center;">0.47</td>
<td style="text-align: center;">0.71</td>
<td style="text-align: center;">0.72</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">FT (Bi)</td>
<td style="text-align: center;">$\mathbf{0 . 5 3}$</td>
<td style="text-align: center;">$0.63^{2}$</td>
<td style="text-align: center;">$\mathbf{0 . 7 6}$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">FT (Multi)</td>
<td style="text-align: center;">$\mathbf{0 . 5 3}$</td>
<td style="text-align: center;">$\mathbf{0 . 6 1}$</td>
<td style="text-align: center;">$\mathbf{0 . 7 6}$</td>
</tr>
<tr>
<td style="text-align: center;">En-De</td>
<td style="text-align: center;">Original</td>
<td style="text-align: center;">0.45</td>
<td style="text-align: center;">0.86</td>
<td style="text-align: center;">0.69</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">prompt $(k=0)$</td>
<td style="text-align: center;">0.51</td>
<td style="text-align: center;">0.68</td>
<td style="text-align: center;">0.75</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">prompt $(k=10)$</td>
<td style="text-align: center;">0.58</td>
<td style="text-align: center;">0.56</td>
<td style="text-align: center;">0.79</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">FT (Generic)</td>
<td style="text-align: center;">0.52</td>
<td style="text-align: center;">0.62</td>
<td style="text-align: center;">0.74</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">FT (Bi)</td>
<td style="text-align: center;">$0.56^{2}$</td>
<td style="text-align: center;">$0.58^{2}$</td>
<td style="text-align: center;">$0.79^{2}$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">FT (Multi)</td>
<td style="text-align: center;">$\mathbf{0 . 5 9}$</td>
<td style="text-align: center;">$\mathbf{0 . 5 5}$</td>
<td style="text-align: center;">$0.79^{2}$</td>
</tr>
<tr>
<td style="text-align: center;">En-Ru</td>
<td style="text-align: center;">Original</td>
<td style="text-align: center;">0.43</td>
<td style="text-align: center;">0.82</td>
<td style="text-align: center;">0.73</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">prompt $(k=0)$</td>
<td style="text-align: center;">0.48</td>
<td style="text-align: center;">0.69</td>
<td style="text-align: center;">0.74</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">prompt $(k=10)$</td>
<td style="text-align: center;">0.49</td>
<td style="text-align: center;">0.67</td>
<td style="text-align: center;">0.79</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">FT (Generic)</td>
<td style="text-align: center;">0.46</td>
<td style="text-align: center;">0.67</td>
<td style="text-align: center;">0.74</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">FT (Bi)</td>
<td style="text-align: center;">$0.51^{2}$</td>
<td style="text-align: center;">$0.65^{2}$</td>
<td style="text-align: center;">$\mathbf{0 . 8 0}$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">FT (Multi)</td>
<td style="text-align: center;">$\mathbf{0 . 5 2}$</td>
<td style="text-align: center;">$\mathbf{0 . 6 3}$</td>
<td style="text-align: center;">$\mathbf{0 . 8 0}$</td>
</tr>
</tbody>
</table>
<p>Table 4: Fine-tuning (FT) results for LLaMA-2 7B. prompt $(k=0)$ and prompt $(k=10)$ indicate the zeroand 10-shot prompting results of LLaMA-2 7B respectively. FT (Generic): Fine-tuning with generic feedback; FT (Bi): Fine-tuning with fine-grained feedback in bilingual setting, where models are individually finetuned for each language pair; FT (Multi): Fine-tuning with fine-grained feedback in multilingual setting, combine 3 language pairs to fine-tune a single model. We test the statistically significance of improvements over the best prompting baseline and $\dagger$ marks results with $p$-value $\leq 0.05$.</p>
<h2>6 Fine-Tuning Results</h2>
<h3>6.1 Automatic Evaluation</h3>
<p>We examine the effectiveness of fine-tuning errorannotated translations for MT post-editing. Table 4 shows that fine-tuning with error annotated translations gives an extra boost in the performance across all metrics.</p>
<p>Original vs Fine-tuning. We compare the finetuning results of each language pair against the original translation quality (indicated as 'Original' in Table 4). Across language pairs, metrics of MT quality all increase for fine-tuning. Translation quality increases steeply by approximately +0.07 BLEU, +0.08 $\operatorname{COMET}_{\mathrm{DA}}$ and -0.21 TER on average for all language pairs. The multilingual approach mostly outperforms the bilingual one, suggesting that the advantages gleaned from fine-</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Error analysis for LLaMA-2 7B. We observe how much each error type is resolved by integrating external feedback during post-editing. We classify an error as 'No match' if the output translation does not contain the specific error span. Across all language pairs, fine-tuning best addresses the errors present in the initial translation. <strong>FT (Bi)</strong>: fine-tuning in bilingual setting; <strong>FT (Multi)</strong>: fine-tuning in multilingual setting. We do not include InstructScore or xCOMET as InstructScore annotates more than 1 error spans, making it difficult for fair comparison and xCOMET does not output error type information.</p>
<p>tuning with diverse language pairs outweigh the benefits of matching the fine-tuning data language consistent with the test language pair. We observe the same trend with LLaMA-2 13B in Appendix Table 7: fine-tuning results improve upon the original baseline results by +0.1 BLEU, +0.08 COMETDA and -0.25 TER points on average.</p>
<p><strong>Prompting vs Fine-tuning.</strong> Next, we examine fine-tuning evaluation compared to the zero- and 10-shot prompting results, collected from either LLaMA-2 7B or 13B. Compared to zero-shot prompting, fine-tuning with error annotations always outperform across all metrics and the multilingual approach outperforms 10-shot prompting results for most of the cases.</p>
<p><strong>Feedback granularity.</strong> We compare the two distinct types of feedback used for fine-tuning: generic and fine-grained feedback, denoted as 'FT (Generic)' and 'FT (Multi)' respectively in Table 4. While prompting experiments demonstrate no clear preference between levels of feedback granularity, fine-tuning using fine-grained feedback consistently yields superior translation quality compared to fine-tuning with generic feedback with a gap of 4 to 6 BLEU points, 3 to 8 TER, and 4 to 6 COMET. This shows that fine-tuning allows the models to take advantage of the fine-grained feedback more effectively.</p>
<p>As there are few error tokens overall, we first expected to see small edits from our fine-tuned model, thus small score difference. However, surprisingly, fine-tuning results overall show greater improvements, especially for TER, considering that the original MQM dataset only has one error span per sentence. Examining outputs (see Appendix E.5 for examples) suggests that fine-tuning not only edits the targeted error spans but also improve the overall naturalness in the target language, consistent with prior evidence that post-editing with LLMs reduces translationese effects (chen2023llm2). To further validate this hypothesis, we turn to human evaluation.</p>
<h3>6.2 Human Evaluation</h3>
<p>We ask bilingual human annotators to assess the post-edited outputs obtained by fine-tuning in the bilingual setting as it is the stronger approach based on automatic scores. We randomly select 50 instances for each language pair for annotation. Each instance is examined by 3 human annotators. For each instance of source text, original MT with MQM annotation, post-edited MT, the annotator is asked to rate on a 5-point Likert scale (1 strongly disagree to 5 strongly agree) whether the translation quality has improved, and to what extent the annotated errors are actually resolved through post-editing. Ordinal Kripendorff's alpha [krippendorff2011], which measure the inter-annotator agreement is moderate for the Overall quality: 0.527, 0.479, 0.421 for Zh-En, En-De, and En-Ru. Annotators are also given the option to provide free form comments. Refer to Appendix F for further details on the annotation set-up.</p>
<p><sup>6</sup>Kripendorff's alpha ranges from 0 to 1, where 0 means no agreement and 1 means perfect agreement.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Human evaluation results for 3 language pairs. We collect a total of 150 annotations for each language pair. <em>Overall Quality</em>: Output translation from the fine-tuned model is better than the initial translation; <em>Resolve Errors</em>: Output translation resolves errors in the initial translation.</p>
<p>As illustrated in Figure 4, our human evaluation results confirm that fine-tuning with error annotations enhances overall translation quality (<em>Overall Quality</em>) and effectively resolves errors in the initial translation (<em>Resolve Errors</em>). While this improvement is notably evident in Zh-En and En-De pair, for the En-Ru pair, approximately 40/150 annotations lean towards the <em>Disagree</em> category. Some of the feedback from En-Ru annotators who choose to <em>Disagree</em> state that there are cases when the output translation from the fine-tuned model is more precise in the target language, but loses some of the nuance in the source text.</p>
<p>Further, feedback from the annotators support our own observation that the post-editing via fine-tuning does not only fix targeted errors in the original translation but rewrites for naturalness in the target language. They comment that the fine-tuning translation "<em>better explains the context</em>" and "<em>flows better in the target language</em>" compared to the original translation which seems to be directly translated without consideration of the context. We list further comments in Appendix Table 20.</p>
<h2>7 Analysis by MT Error Categories</h2>
<p>Our error analysis aims to pinpoint the types of errors that are most effectively resolved through the integration of external feedback. We evaluate 200 output translations generated by prompting LLaMA-2 7B with each generic, score-based, and MQM feedback. We do not include InstructScore or xCOMET as InstructScore annotates more than 1 error spans making it difficult for fair comparison and xCOMET does not output error type information. We also compare the outputs from our custom fine-tuned models, both bilingual and multilingual version. All of the feedback is based on MQM, thus we categorize the error type as per "Error Category" from MQM detailed in Appendix Table 8.</p>
<p>In Figure 3, we illustrate the extent to which each error type has been resolved by incorporating external feedback. First, we check whether a span annotated as an error in the original translation matches the output after post-editing with feedback. A match increments the count for the error type associated with the span. If there is no match found, the count for the "No match" category is incremented. We observe that using any form of feedback (generic, score, or MQM) increases the portion of "No match" category compared to the original translation. However, there is no distinct trend for any specific error type; all of the errors are addressed in a balanced manner.</p>
<p>Further, by incorporating the output translations from our fine-tuned model, we see a sudden leap in the "No match" category. This suggests that fine-tuning best fixes the targeted error span. This finding is also consistent with the conclusions from Section 6, where we noted that fine-tuning help align LLM behavior with the provided feedback.</p>
<h2>8 Post-Editing Correct Outputs</h2>
<p>The experiments we have presented so far are focused on post-editing MT hypotheses that are known to leave room for improvement. For completeness, we present in Appendix Table 14 decoding results when zero-shot prompting the LLaMA-2 models to post-edit approaches to 200 WMT hypotheses labeled as "No error" by the WMT human annotators.</p>
<p>As expected, the resulting edits lead to a small drop in automatic metrics, confirming the observation that the nature of edits goes beyond correcting errors to address more stylistic issues such as translationese. Interestingly, the larger LLaMA-2 model and the fine-grained feedback are the least prone to over-editing. We anticipate that different prompts and fine-tuning data are needed for models to jointly consider the task of editing or not, and of</p>
<p>what edits to perform.</p>
<h2>9 Conclusion</h2>
<p>We explore a range of strategies to guide LLaMA-2 models to improve MT outputs using external feedback, varying in different granularity. We demonstrate that prompting LLM to edit MT with feedback reliably enhances the overall translation quality and post-editing efforts. We further explore instruction fine-tuning LLMs with fine-grained feedback. Through automatic and human evaluation, we demonstrate that fine-tuning shows stronger improvements on enhancing the translation quality, resolving errors in the initial translation, and most notably, generating translations that are more natural (less translationese) in the target language.</p>
<p>Taken together, these results clearly show that post-editing MT output does not require the largest proprietary LLM models and can be done with smaller open-source models. This opens many questions for future work to further explore how to do this well in more diverse settings, while minimizing the reliance on human annotated MT outputs which are expensive to obtain at scale. Building on LLMs fine-tuned for many translation related tasks (Alves et al., 2024) is a promising direction for encouraging transfer learning from limited amounts of annotation.</p>
<h2>10 Limitations</h2>
<p>We evaluate the impact of post-editing separately on MT outputs that contain one or more errors (§5) and on MT outputs that do not contain any errors (§8). This leaves open the question of how to design a workflow that takes in any MT input and automatically determines whether and how it should be post-edited, possibly selecting among different potential feedback mechanisms, which we leave to future work.</p>
<p>Furthermore, the fine-tuning data is in the same domain as the test data which will not always be the case in practice. While we test on diverse languages and on out-of-English and into-English directions, it remains to be seen how our findings generalize to a wider variety of languages, particularly in lowresource settings.</p>
<p>Finally, our work highlights the effectiveness of using external feedback to resolve errors in translations. Although integrating external feedback is an attractive approach, the scarcity of high-quality feedback remains a significant challenge. This
scarcity underscores the demand for the development of automated systems capable of generating high-quality error annotations. In regard to constraints in the currently available external feedback for MT post-editing, our study is constrained in terms of forms of feedback (generic, score, finegrained) and language pairs. Future works can focus on incorporating automatic systems that can generate consistent, high quality feedback.</p>
<h2>11 Acknowledgement</h2>
<p>We thank the anonymous reviewers, Shramay Palta, Nishant Balepur, Calvin Bao, Yu Hou and the members of the CLIP lab at University of Maryland for their valuable suggestions and constructive feedback.</p>
<p>This work was supported in part by NSF Fairness in AI Grant 2147292, by the Institute for Trustworthy AI in Law and Society (TRAILS), which is supported by the National Science Foundation under Award No. 2229885, and by the Office of the Director of National Intelligence (ODNI), Intelligence Advanced Research Projects Activity (IARPA), via the HIATUS Program contract #202222072200006, by NSF grant 2147292. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies, either expressed or implied, of ODNI, IARPA, NSF or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for governmental purposes notwithstanding any copyright annotation therein.</p>
<h2>References</h2>
<p>Kabir Ahuja, Harshita Diddee, Rishav Hada, Millicent Ochieng, Krithika Ramesh, Prachi Jain, Akshay Nambi, Tanuja Ganu, Sameer Segal, Mohamed Ahmed, Kalika Bali, and Sunayana Sitaram. 2023. MEGA: Multilingual evaluation of generative AI. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 4232-4267, Singapore. Association for Computational Linguistics.</p>
<p>Duarte M. Alves, José Pombal, Nuno M. Guerreiro, Pedro H. Martins, João Alves, Amin Farajian, Ben Peters, Ricardo Rei, Patrick Fernandes, Sweta Agrawal, Pierre Colombo, José G. C. de Souza, and André F. T. Martins. 2024. Tower: An open multilingual large language model for translation-related tasks.</p>
<p>Rohan Anil, Andrew M. Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak</p>
<p>Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, Eric Chu, Jonathan H. Clark, Laurent El Shafey, Yanping Huang, Kathy Meier-Hellstern, Gaurav Mishra, Erica Moreira, Mark Omernick, Kevin Robinson, Sebastian Ruder, Yi Tay, Kefan Xiao, Yuanzhong Xu, Yujing Zhang, Gustavo Hernandez Abrego, Junwhan Ahn, Jacob Austin, Paul Barham, Jan Botha, James Bradbury, Siddhartha Brahma, Kevin Brooks, Michele Catasta, Yong Cheng, Colin Cherry, Christopher A. Choquette-Choo, Aakanksha Chowdhery, Clément Crepy, Shachi Dave, Mostafa Dehghani, Sunipa Dev, Jacob Devlin, Mark Díaz, Nan Du, Ethan Dyer, Vlad Feinberg, Fangxiaoyu Feng, Vlad Fienber, Markus Freitag, Xavier Garcia, Sebastian Gehrmann, Lucas Gonzalez, Guy GurAri, Steven Hand, Hadi Hashemi, Le Hou, Joshua Howland, Andrea Hu, Jeffrey Hui, Jeremy Hurwitz, Michael Isard, Abe Iltycheriah, Matthew Jagielski, Wenhao Jia, Kathleen Kenealy, Maxim Krikun, Sneha Kudugunta, Chang Lan, Katherine Lee, Benjamin Lee, Eric Li, Music Li, Wei Li, YaGuang Li, Jian Li, Hyeontaek Lim, Hanzhao Lin, Zhongtao Liu, Frederick Liu, Marcello Maggioni, Aroma Mahendru, Joshua Maynez, Vedant Misra, Maysam Moussalem, Zachary Nado, John Nham, Eric Ni, Andrew Nystrom, Alicia Parrish, Marie Pellat, Martin Polacek, Alex Polozov, Reiner Pope, Siyuan Qiao, Emily Reif, Bryan Richter, Parker Riley, Alex Castro Ros, Aurko Roy, Brennan Saeta, Rajkumar Samuel, Renee Shelby, Ambrose Slone, Daniel Smilkov, David R. So, Daniel Sohn, Simon Tokumine, Dasha Valter, Vijay Vasudevan, Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, Tao Wang, John Wieting, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Qiao Zhang, Steven Zheng, Ce Zheng, Weikang Zhou, Denny Zhou, Slav Petrov, and Yonghui Wu. 2023. Palm 2 technical report.</p>
<p>Rachel Bawden and François Yvon. 2023. Investigating the Translation Performance of a Large Multilingual Language Model: The Case of BLOOM.</p>
<p>Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901.</p>
<p>Rajen Chatterjee, Matteo Negri, Raphael Rubino, and Marco Turchi. 2018. Findings of the WMT 2018 Shared Task on Automatic Post-Editing. In Proceedings of the Third Conference on Machine Translation: Shared Task Papers, pages 710-725, Belgium, Brussels. Association for Computational Linguistics.</p>
<p>Pinzhen Chen, Zhicheng Guo, Barry Haddow, and Kenneth Heafield. 2023. Iterative translation refinement with large language models.</p>
<p>Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzmán, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. 2020. Unsupervised
cross-lingual representation learning at scale. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 84408451, Online. Association for Computational Linguistics.</p>
<p>Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. 2023. Qlora: Efficient finetuning of quantized llms.</p>
<p>Patrick Fernandes, Daniel Deutsch, Mara Finkelstein, Parker Riley, André Martins, Graham Neubig, Ankush Garg, Jonathan Clark, Markus Freitag, and Orhan Firat. 2023. The devil is in the errors: Leveraging large language models for fine-grained machine translation evaluation. In Proceedings of the Eighth Conference on Machine Translation, pages 10661083, Singapore. Association for Computational Linguistics.</p>
<p>Markus Freitag, George Foster, David Grangier, Viresh Ratnakar, Qijun Tan, and Wolfgang Macherey. 2021. Experts, errors, and context: A large-scale study of human evaluation for machine translation. Transactions of the Association for Computational Linguistics, 9:1460-1474.</p>
<p>Jinlan Fu, See-Kiong Ng, Zhengbao Jiang, and Pengfei Liu. 2023. Gptscore: Evaluate as you desire.</p>
<p>Yvette Graham, Timothy Baldwin, Alistair Moffat, and Justin Zobel. 2013. Continuous measurement scales in human evaluation of machine translation. In Proceedings of the 7th Linguistic Annotation Workshop and Interoperability with Discourse, pages 33-41, Sofia, Bulgaria. Association for Computational Linguistics.</p>
<p>Jiatao Gu, Changhan Wang, and Jake Zhao Junbo. 2019. Levenshtein transformer. In Proceedings of the 33rd International Conference on Neural Information Processing Systems, 1003, pages 11181-11191. Curran Associates Inc., Red Hook, NY, USA.</p>
<p>Nuno M. Guerreiro, Ricardo Rei, Daan van Stigt, Luisa Coheur, Pierre Colombo, and André F. T. Martins. 2023. xcomet: Transparent machine translation evaluation through fine-grained error detection.</p>
<p>Amr Hendy, Mohamed Abdelrehim, Amr Sharaf, Vikas Raunak, Mohamed Gabr, Hitokazu Matsushita, Young Jin Kim, Mohamed Afify, and Hany Hassan Awadalla. 2023. How good are gpt models at machine translation? a comprehensive evaluation.</p>
<p>Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2022. LoRA: Low-rank adaptation of large language models. In International Conference on Learning Representations.</p>
<p>Wenxiang Jiao, Wenxuan Wang, Jen-tse Huang, Xing Wang, and Zhaopeng Tu. 2023. Is ChatGPT A Good Translator? Yes With GPT-4 As The Engine.</p>
<p>Marcin Junczys-Dowmunt and Roman Grundkiewicz. 2016. Log-linear Combinations of Monolingual and Bilingual Neural Machine Translation Models for Automatic Post-Editing. In Proceedings of the First Conference on Machine Translation: Volume 2, Shared Task Papers, pages 751-758, Berlin, Germany. Association for Computational Linguistics.</p>
<p>Marzena Karpinska, Nishant Raj, Katherine Thai, Yixiao Song, Ankita Gupta, and Mohit Iyyer. 2022. DEMETR: Diagnosing evaluation metrics for translation. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 9540-9561, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.</p>
<p>Kevin Knight and Ishwar Chander. 1994. Automated postediting of documents. In Proceedings of the Twelfth AAAI National Conference on Artificial Intelligence, AAAI'94, pages 779-784, Seattle, Washington. AAAI Press.</p>
<p>Tom Kocmi, Eleftherios Avramidis, Rachel Bawden, Ondřej Bojar, Anton Dvorkovich, Christian Federmann, Mark Fishel, Markus Freitag, Thamme Gowda, Roman Grundkiewicz, Barry Haddow, Philipp Koehn, Benjamin Marie, Christof Monz, Makoto Morishita, Kenton Murray, Makoto Nagata, Toshiaki Nakazawa, Martin Popel, Maja Popović, and Mariya Shmatova. 2023. Findings of the 2023 Conference on Machine Translation (WMT23): LLMs Are Here but Not Quite There Yet. In Proceedings of the Eighth Conference on Machine Translation, pages 1-42, Singapore. Association for Computational Linguistics.</p>
<p>Tom Kocmi, Rachel Bawden, Ondřej Bojar, Anton Dvorkovich, Christian Federmann, Mark Fishel, Thamme Gowda, Yvette Graham, Roman Grundkiewicz, Barry Haddow, Rebecca Knowles, Philipp Koehn, Christof Monz, Makoto Morishita, Masaaki Nagata, Toshiaki Nakazawa, Michal Novák, Martin Popel, and Maja Popović. 2022. Findings of the 2022 conference on machine translation (WMT22). In Proceedings of the Seventh Conference on Machine Translation (WMT), pages 1-45, Abu Dhabi, United Arab Emirates (Hybrid). Association for Computational Linguistics.</p>
<p>Tom Kocmi and Christian Federmann. 2023a. GEMBAMQM: Detecting translation quality error spans with GPT-4. In Proceedings of the Eighth Conference on Machine Translation, pages 768-775, Singapore. Association for Computational Linguistics.</p>
<p>Tom Kocmi and Christian Federmann. 2023b. Large language models are state-of-the-art evaluators of translation quality. In Proceedings of the 24th Annual Conference of the European Association for Machine Translation, pages 193-203, Tampere, Finland. European Association for Machine Translation.</p>
<p>Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. 2023. Large Language Models are Zero-Shot Reasoners.</p>
<p>Klaus Krippendorff. 2011. Computing krippendorff's alpha-reliability.</p>
<p>Xi Victoria Lin, Todor Mihaylov, Mikel Artetxe, Tianlu Wang, Shuohui Chen, Daniel Simig, Myle Ott, Naman Goyal, Shruti Bhosale, Jingfei Du, Ramakanth Pasunuru, Sam Shleifer, Punit Singh Koura, Vishrav Chaudhary, Brian O'Horo, Jeff Wang, Luke Zettlemoyer, Zornitsa Kozareva, Mona Diab, Veselin Stoyanov, and Xian Li. 2022. Few-shot learning with multilingual generative language models. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 9019-9052, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.</p>
<p>Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu. 2023. G-eval: NLG evaluation using gpt-4 with better human alignment. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 2511-2522, Singapore. Association for Computational Linguistics.</p>
<p>Arle Lommel, Hans Uszkoreit, and Aljoscha Burchardt. 2014. Multidimensional Quality Metrics (MQM): A Framework for Declaring and Describing Translation Quality Metrics. Revista tradumàtica: traducció i tecnologies de la informació i la comunicació, pages $455-463$.</p>
<p>Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Shashank Gupta, Bodhisattwa Prasad Majumder, Katherine Hermann, Sean Welleck, Amir Yazdanbakhsh, and Peter Clark. 2023. Self-refine: Iterative refinement with self-feedback. In Thirty-seventh Conference on Neural Information Processing Systems.</p>
<p>OpenAI. 2023. Gpt-4 technical report.
Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:27730-27744.</p>
<p>Liangming Pan, Michael Saxon, Wenda Xu, Deepak Nathani, Xinyi Wang, and William Yang Wang. 2023. Automatically correcting large language models: Surveying the landscape of diverse self-correction strategies.</p>
<p>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: A method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, ACL '02, page 311-318, USA. Association for Computational Linguistics.</p>
<p>Debjit Paul, Mete Ismayilzada, Maxime Peyrard, Beatriz Borges, Antoine Bosselut, Robert West, and Boi Faltings. 2024. REFINER: Reasoning feedback on</p>
<p>intermediate representations. In Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1100-1126, St. Julian's, Malta. Association for Computational Linguistics.</p>
<p>Matt Post. 2018. A call for clarity in reporting BLEU scores. In Proceedings of the Third Conference on Machine Translation: Research Papers, pages 186191, Brussels, Belgium. Association for Computational Linguistics.</p>
<p>Vipul Raheja, Dhruv Kumar, Ryan Koo, and Dongyeop Kang. 2023. CoEdIT: Text Editing by Task-Specific Instruction Tuning.</p>
<p>Vikas Raunak, Amr Sharaf, Yiren Wang, Hany Awadalla, and Arul Menezes. 2023. Leveraging GPT4 for automatic translation post-editing. In Findings of the Association for Computational Linguistics: EMNLP 2023, pages 12009-12024, Singapore. Association for Computational Linguistics.</p>
<p>Ricardo Rei, Craig Stewart, Ana C Farinha, and Alon Lavie. 2020. COMET: A neural framework for MT evaluation. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2685-2702, Online. Association for Computational Linguistics.</p>
<p>Emily Reif, Daphne Ippolito, Ann Yuan, Andy Coenen, Chris Callison-Burch, and Jason Wei. 2022. A Recipe for Arbitrary Text Style Transfer with Large Language Models. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 837848, Dublin, Ireland. Association for Computational Linguistics.</p>
<p>Thibault Sellam, Dipanjan Das, and Ankur Parikh. 2020. BLEURT: Learning Robust Metrics for Text Generation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7881-7892, Online. Association for Computational Linguistics.</p>
<p>Michel Simard, Cyril Goutte, and Pierre Isabelle. 2007. Statistical Phrase-Based Post-Editing. In Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics; Proceedings of the Main Conference, pages 508-515, Rochester, New York. Association for Computational Linguistics.</p>
<p>Matthew Snover, Bonnie Dorr, Rich Schwartz, Linnea Micciulla, and John Makhoul. 2006. A study of translation edit rate with targeted human annotation. In Proceedings of the 7th Conference of the Association for Machine Translation in the Americas: Technical Papers, pages 223-231, Cambridge, Massachusetts, USA. Association for Machine Translation in the Americas.</p>
<p>Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy</p>
<p>Liang, and Tatsunori B. Hashimoto. 2023. Stanford alpaca: An instruction-following llama model. https://github.com/tatsu-lab/ stanford_alpaca.</p>
<p>Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. 2023. Llama: Open and efficient foundation language models.</p>
<p>David Vilar, Markus Freitag, Colin Cherry, Jiaming Luo, Viresh Ratnakar, and George Foster. 2023. Prompting PaLM for Translation: Assessing Strategies and Performance.</p>
<p>David Wan, Chris Kedzie, Faisal Ladhak, Marine Carpuat, and Kathleen McKeown. 2020. Incorporating Terminology Constraints in Automatic PostEditing. In Proceedings of the Fifth Conference on Machine Translation, pages 1193-1204, Online. Association for Computational Linguistics.</p>
<p>Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H. Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, and William Fedus. 2022. Emergent abilities of large language models. Transactions on Machine Learning Research. Survey Certification.</p>
<p>Sean Welleck, Ximing Lu, Peter West, Faeze Brahman, Tianxiao Shen, Daniel Khashabi, and Yejin Choi. 2023. Generating sequences by learning to self-correct. In The Eleventh International Conference on Learning Representations.</p>
<p>Wenda Xu, Daniel Deutsch, Mara Finkelstein, Juraj Juraska, Biao Zhang, Zhongtao Liu, William Yang Wang, Lei Li, and Markus Freitag. 2024. Llmrefine: Pinpointing and refining large language models via fine-grained actionable feedback.</p>
<p>Wenda Xu, Danqing Wang, Liangming Pan, Zhenqiao Song, Markus Freitag, William Wang, and Lei Li. 2023. INSTRUCTSCORE: Towards explainable text generation evaluation with automatic feedback. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 5967-5994, Singapore. Association for Computational Linguistics.</p>
<p>Jiali Zeng, Fandong Meng, Yongjing Yin, and Jie Zhou. 2023. Improving machine translation with large language models: A preliminary study with cooperative decoding.</p>
<p>Wenhao Zhu, Hongyi Liu, Qingxiu Dong, Jingjing Xu, Shujian Huang, Lingpeng Kong, Jiajun Chen, and Lei Li. 2023. Multilingual Machine Translation with Large Language Models: Empirical Results and Analysis.</p>
<table>
<thead>
<tr>
<th>A</th>
<th>Fine-grained Feedback format</th>
</tr>
</thead>
<tbody>
<tr>
<td>In this section, we discuss the details on the format of fine-grained feedback both human-annotated and automatically annotated by InstructScore or xCOMET. We refer to fine-grained feedback in three components: error span position, error type, and error severity level. MQM annotations and InstructScore use the same MQM hierarchy to define error type as shown in Table 8, with InstructScore omitting categories such as "Source error", "Non-translation", and "Other". Unlike these, xCOMET does not provide error type information in their annotation.</td>
<td></td>
</tr>
<tr>
<td>The levels of error severity are summarized in Table 5. In our prompting experiments, we eliminate instances annotated as "No-error" in the MQM dataset, as our focus is on understanding the role of external feedback in post-editing erroneous translations. However, for fine-tuning, we include all instances, regardless of their error severity level.</td>
<td></td>
</tr>
<tr>
<td>Metric</td>
<td>Severity level</td>
</tr>
<tr>
<td>MQM</td>
<td>Major, Minor, No-error</td>
</tr>
<tr>
<td>InstructScore</td>
<td>Major, Minor</td>
</tr>
<tr>
<td>xCOMET</td>
<td>Critical, Major, Minor</td>
</tr>
</tbody>
</table>
<p>Table 5: Error severity levels supported by each metric.</p>
<table>
<thead>
<tr>
<th>Severity</th>
<th>Category</th>
<th>Weight</th>
</tr>
</thead>
<tbody>
<tr>
<td>Major</td>
<td>Non-translation</td>
<td>25</td>
</tr>
<tr>
<td></td>
<td>All others</td>
<td>5</td>
</tr>
<tr>
<td>Minor</td>
<td>Fluency/Punctuation</td>
<td>0.1</td>
</tr>
<tr>
<td></td>
<td>All others</td>
<td>1</td>
</tr>
<tr>
<td>Neutral</td>
<td>All</td>
<td>0</td>
</tr>
</tbody>
</table>
<p>Table 6: MQM error weighting. Each score can range from 0 (perfect) to 25 (worst). The final score is the average over scores from all annotators.</p>
<h2>Appendix B Error Annotation Examples</h2>
<p>In Table 9, we present error annotation examples from three sources: MQM, xCOMET, and InstructScore. We obtain automatic annotations of the same evaluation dataset using InstructScore and xCOMET. The consistency of these error annotations across different tools is further discussed in Section 4.1.</p>
<h2>Appendix C Dataset Details</h2>
<h3>C.1 MQM Dataset</h3>
<p>We analyze 1,000 MQM data instances used for evaluation. We note that the average number of error spans per sentence is 1 as from the original MQM dataset. The average error span length is 13.5 for Zh-En, 11.3 for En-De, and 9.3 for En-Ru. Further, we observe the error type and severity level distribution in Figure 5 and 6. Across all language pairs, "Accuracy" errors are the majority (524 for Zh-En, 362 for En-De, 592 for En-Ru), followed by "Fluency" (274 for Zh-En, 324 for En-De, 257 for En-Ru). For the severity level, Zh-En has the most "major" errors (512/1000), then En-Ru (388/1000) and En-De (202/1000).</p>
<h2>Appendix D Fine-tuning Details</h2>
<h3>D.1 Fine-tuning Dataset format</h3>
<p>We illustrate the instruction template used for constructing fine-tuning dataset in Table 10. We explicitly include the fine-grained errors in the instruction to guide LLMs on how to leverage them as hints and align in their improved translation outputs. We employ the same instruction format during inference time.</p>
<h3>D.2 LLaMA-2 13B Results</h3>
<p>We also extend the fine-tuning of LLaMA-2 13B for two settings, mirroring the 7B setup: bilingual and multilingual. We follow the identical experimental setup as in the 7B experiment. We show that similar trend is observed; fine-tuning with error annotations show better performance than the original baseline and the zero- and 10-shot prompting results across all metrics.</p>
<h2>Appendix E Detailed Results</h2>
<h3>E.1 LLaMA-2 Original</h3>
<p>We show the detailed numerical results for the LLaMA-2 7B and 13B experiments in Table 11 and 12 respectively. In zero-shot prompting, the 13B model outperforms 7B, which can be attributable by larger LLMs having stronger instruction-following capabilities than their smaller counterparts ( [Wei et al., 2022]). Further, 13B shows similar trend observed for 7B, where the use of external feedback in 10-shot prompting significantly improves performance.</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Error type distribution for 3 language pairs. Note that En-Ru dataset from WMT 22 General MT submissions use different names for each error type, thus conduct manual mapping.</p>
<table>
<thead>
<tr>
<th>Language</th>
<th>Type</th>
<th>BLEU (↑)</th>
<th>TER (↓)</th>
<th>COMET(↓, (↑)</th>
</tr>
</thead>
<tbody>
<tr>
<td>Zh-En</td>
<td>Original</td>
<td>0.47</td>
<td>0.75</td>
<td>0.70</td>
</tr>
<tr>
<td></td>
<td>prompt (k=0)</td>
<td>0.50</td>
<td>0.74</td>
<td>0.71</td>
</tr>
<tr>
<td></td>
<td>prompt (k=10)</td>
<td>0.50</td>
<td>0.61</td>
<td>0.73</td>
</tr>
<tr>
<td></td>
<td>FT (Bi)</td>
<td>0.51</td>
<td>0.61</td>
<td>0.77</td>
</tr>
<tr>
<td></td>
<td>FT (Multi)</td>
<td>0.54</td>
<td>0.58</td>
<td>0.76</td>
</tr>
<tr>
<td>En-De</td>
<td>Original</td>
<td>0.45</td>
<td>0.86</td>
<td>0.69</td>
</tr>
<tr>
<td></td>
<td>prompt (k=0)</td>
<td>0.51</td>
<td>0.68</td>
<td>0.75</td>
</tr>
<tr>
<td></td>
<td>prompt (k=10)</td>
<td>0.58</td>
<td>0.56</td>
<td>0.79</td>
</tr>
<tr>
<td></td>
<td>FT (Bi)</td>
<td>0.57</td>
<td>0.55</td>
<td>0.80</td>
</tr>
<tr>
<td></td>
<td>FT (Multi)</td>
<td>0.60</td>
<td>0.53</td>
<td>0.80</td>
</tr>
<tr>
<td>En-Ru</td>
<td>Original</td>
<td>0.43</td>
<td>0.82</td>
<td>0.73</td>
</tr>
<tr>
<td></td>
<td>prompt (k=0)</td>
<td>0.44</td>
<td>0.80</td>
<td>0.73</td>
</tr>
<tr>
<td></td>
<td>prompt (k=10)</td>
<td>0.53</td>
<td>0.56</td>
<td>0.80</td>
</tr>
<tr>
<td></td>
<td>FT (Bi)</td>
<td>0.53</td>
<td>0.57</td>
<td>0.80</td>
</tr>
<tr>
<td></td>
<td>FT (Multi)</td>
<td>0.54</td>
<td>0.56</td>
<td>0.81</td>
</tr>
</tbody>
</table>
<p>Table 7: Fine-tuning (FT) results for LLaMA-2 13B. <strong>prompt</strong> (k=0) and <strong>prompt</strong> (k=10) indicate the zero- and 10-shot prompting results of LLaMA-2 13B respectively. <strong>FT (Bi)</strong>: Fine-tuning in bilingual setting, where models are individually fine-tuned for each language pair; <strong>FT (Multi)</strong>: Fine-tuning in multilingual setting, combine 3 language pairs to fine-tune a single model. We test the statistically significance of improvements over the best prompting baseline and † marks results with p-value ≤ 0.05.</p>
<h3>E.2 LLaMA-2 Chat</h3>
<p>We expand our experiments with LLaMA-2-chat, an instruction fine-tuned version of LLaMA-2. Although they are optimized to better follow the instructions that users specify, we show that LLaMA-2 models consistently outperform the chat counterparts in Table 13. Our findings indicate that instruction-following ability of LLMs might not be the only determining factor for successful MT post-editing.</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: Severity level distribution for 3 language pairs. We note that the En-Ru dataset from WMT 22 General MT submissions use additional severity level category: "<em>critical</em>".</p>
<h3>E.3 Fine-grained Components</h3>
<p>In Table 15, we observe the impact of each component of the fine-grained feedback: error span position, error type, and severity level with 200 randomly sampled test cases. We examine that while the individual contribution of each error component is trivial, interestingly, providing only the severity level information consistently yields similar or superior results compared to providing all three components simultaneously. This shows that there could be other forms of feedback effective when prompted to LLMs, which we leave for future work.</p>
<h3>E.4 Translate from Scratch</h3>
<p>We present zero-shot LLaMA-2 translation results in Table 16. We report the scores for 1,000 WMT test instances used in our main evaluation, as translated with LLaMA-2 7B and 13B models with the prompt template as: "<em>Translate from {source language} to {target language} without any explanation.\n{source language}: {source sentence}\n{target language}:</em>". Results show that LLaMA-2 7B is not powerful at translating compared to the baseline hypothesis translations provided by the MQM. LLaMA-2 13B shows comparable results to the original performance except for En-De where it slightly outperforms the original. We would expect much higher scores if the test set had been memorized as part of the LLaMA-2 pre-training data. Further, we notice that translating from scratch with LLaMA-2 7B consistently shows lower performance than post-editing regard-</p>
<p>less of feedback types. For 13B, again translating shows lower performance compared to post-editing with generic or xCOMET feedback but similar to score-based or MQM feedback.</p>
<h3>E. 5 Qualitative Analysis</h3>
<p>In this section, we demonstrate how fine-tuning enhances the alignment of LLM behavior with the external feedback. Tables 17, 18, and 19 illustrate output translations generated by LLaMA-2 7B incorporating different types of feedback. While relatively coarse feedback (generic and score-based) are not able to accurately pinpoint and correct the targeted error spans, fine-grained feedback (MQM, InstructScore, and xCOMET) resolves this issue. Further, even in instances where fine-grained feedback falls short, fine-tuning enables the model to generate translations that more effectively narrow the gap. We also demonstrate that translations from the fine-tuned model not only resolves the errors but also makes it more natural (less translationese) in the target language.</p>
<h2>F Human Evaluation</h2>
<h2>F. 1 Evaluation Details</h2>
<p>For human evaluation, we employ Qualtrics ${ }^{7}$ to design our survey and Prolific ${ }^{8}$ to recruit human annotators. We randomly sample 50 instances for each language pair and further divide them into two separate sessions. Each session consists of 25 examples and is estimated to take approximately 30 minutes to complete. For every session and language pair, we engage 3 annotators who are fluent in both source and target languages. For example, when evaluating the Chinese-English pair, we choose annotators fluent in both Chinese and English. Consequently, for each language pair, we recruit a total of 6 annotators, amounting to 18 annotators overall. We offer a compensation of $\$ 7$ per session, totaling $\$ 126$ for the entire human evaluation process.</p>
<h2>F. 2 Annotator Instructions</h2>
<p>In Figure 7 and 8, we present the instructions and survey content provided to our annotators. Each annotator reviews 25 set of examples, each consisting of the source text, Translation 1 (the initial translation), and Translation 2 (the output translation from our fine-tuned model). They are tasked with</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup>comparing these two translation on a Likert scale ranging from 0 (Strongly disagree) to 5 (Strongly agree). This comparison is based on two criteria: (1) "Translation 2 is better than Translation 1" evaluates whether the output translation more effectively conveys the meaning of the source text and exhibits improved fluency in the target language; (2) "Translation 2 fixes errors that were present in Translation 1" examines whether the output translation resolves errors present in the original translation. Additionally, we provide a free-form text box alongside each example for any additional feedback or suggestions.</p>
<h2>F. 3 Feedback from Annotators</h2>
<p>In our survey, we provide a text box for each example to collect additional feedback or suggestions from the annotators. We present the feedback per language pair in Table 20. Main reasons for preferring the initial translation over the output translation from our fine-tuned model are that although the output translation may be grammatically and syntactically more precise, the initial translation often better preserves the meaning of the source sentence. Additionally, some annotators note that the initial version is more specific and understandable in some cases.</p>
<p>In contrast, annotators comment that they prefer the output translation from the fine-tuned model over the initial translation because (1) It fixes all the errors that were present in the initial translation; (2) It explains in the context of the target language; (3) It fits well to the actual use of the target language and flows better. All of the comments indicate that fine-tuning error annotations help make the translation more natural.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Error Category</th>
<th style="text-align: left;">Sub Category</th>
<th style="text-align: left;">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Accuracy</td>
<td style="text-align: left;">Addition</td>
<td style="text-align: left;">Translation includes information not present in the source.</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Omission</td>
<td style="text-align: left;">Translation has missing content from the source.</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Mistranslation</td>
<td style="text-align: left;">Target content does not accurately represent the source content.</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Untranslated text</td>
<td style="text-align: left;">Source text has been left untranslated.</td>
</tr>
<tr>
<td style="text-align: left;">Fluency</td>
<td style="text-align: left;">Character Encoding</td>
<td style="text-align: left;">Characters are garbled due to incorrect application of an encoding.</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Grammar</td>
<td style="text-align: left;">Problems with grammar or syntax of text, other than orthography.</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Inconsistency</td>
<td style="text-align: left;">Internal inconsistency (not related to terminology).</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Punctuation</td>
<td style="text-align: left;">Incorrect punctuation (for locale or style).</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Register</td>
<td style="text-align: left;">Wrong grammatical register (eg. informal pronouns or verb forms)</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Spelling</td>
<td style="text-align: left;">Incorrect spelling or capitalization.</td>
</tr>
<tr>
<td style="text-align: left;">Local convention</td>
<td style="text-align: left;">Address format</td>
<td style="text-align: left;">Wrong format for addresses.</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Currency format</td>
<td style="text-align: left;">Wrong format for currency.</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Date format</td>
<td style="text-align: left;">Wrong format for dates.</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Name format</td>
<td style="text-align: left;">Wrong format for names.</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Telephone format</td>
<td style="text-align: left;">Wrong format for telephone numbers.</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Time format</td>
<td style="text-align: left;">Wrong format for time expressions.</td>
</tr>
<tr>
<td style="text-align: left;">Terminology</td>
<td style="text-align: left;">Inappropriate for context</td>
<td style="text-align: left;">Terminology is non-standard or does not fit context.</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Inconsistent use</td>
<td style="text-align: left;">Terminology is used inconsistently.</td>
</tr>
<tr>
<td style="text-align: left;">Style</td>
<td style="text-align: left;">Awkward</td>
<td style="text-align: left;">Translation has stylistic problems.</td>
</tr>
<tr>
<td style="text-align: left;">Source error</td>
<td style="text-align: left;">Any error in the source.</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Non-translation</td>
<td style="text-align: left;">Impossible to reliably characterize distinct errors.</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Other</td>
<td style="text-align: left;">Any other issues.</td>
<td style="text-align: left;"></td>
</tr>
</tbody>
</table>
<p>Table 8: MQM hierarchy (Freitag et al., 2021).</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Source</th>
<th style="text-align: left;">現如今绝大多数遇难者的老父母均已避世，遗孤们也已长大成家就业。</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Candidate</td>
<td style="text-align: left;">Nowadays, the vast majority of the victims' elderly parents have died, and the orphans have grown into family employment.</td>
</tr>
<tr>
<td style="text-align: left;">Reference</td>
<td style="text-align: left;">Now the old parents of the vast majority of the victims have passed away, and the orphans have also grown up, started working and got married.</td>
</tr>
<tr>
<td style="text-align: left;">MQM</td>
<td style="text-align: left;">Error span: Nowadays <br> Error type: Accuracy/Mistranslation <br> Severity: Major</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Error span: [family employment, Nowadays]</td>
</tr>
<tr>
<td style="text-align: left;">InstructScore</td>
<td style="text-align: left;">Error type: [Incorrect translation is missing content from the correct translation, Incorrect translation has stylistic problems] <br> Severity: [Major, Major]</td>
</tr>
<tr>
<td style="text-align: left;">xCOMET</td>
<td style="text-align: left;">Error span: [die, have grown into family employment] <br> Severity: [Major, Major]</td>
</tr>
</tbody>
</table>
<p>Table 9: Chinese-English error annotation examples from three sources: MQM dataset, xCOMET, and InstructScore. We input source, candidate translation, and reference sentence to xCOMET and InstructScore. xCOMET does not output error type in their annotation. Both xCOMET and InstructScore returns a list if they detect more than one errors in the candidate translation.</p>
<h1>Instruction</h1>
<p>### English: Memorial meetings were organised at the residence of Sam Stafford, one of the agitators who died, and a playground in Guwahati, with attendees resolving to once again to intensify the stir against the Citizenship (Amendment) Act. 10
### German: Gedenkmülerversammlungen wurden in der Residenz von Sam Stafford, einem der gestorbenen Agitatoren, und einem Spielplatz in Guwahati organisiert, wobei die Teilnehmer sich entschlossen hatten, den Aufruhr gegen das Gesetz über die Staatsbürgerschaft (Änderung) erneut zu intensivieren. 10
### Errors: There is a minor accuracy/mistranslation error at "Gedenkmülerversammlungen". 10 h
### Improved German: Gedenkveranstaltungen fanden am Wohnsitz von Sam Stafford, einem der getöteten Aktivisten, sowie auf einem Schulhof in Guwahati statt, und die Teilnehmer beschlossen, noch einmal den Protest gegen den CAA zu verstärken.</p>
<p>Table 10: Example of fine-tuning instructions dataset reformulated from English-German error annotated translations. Texts in bold represent the placeholders for source and target language. We give fine-grained feedback after ### Errors:</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Language</th>
<th style="text-align: center;">Shots</th>
<th style="text-align: center;">BLEU ( $\uparrow$ ) / TER ( $\downarrow$ ) / COMET $_{\text {DA }}(\uparrow)$</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Original</td>
<td style="text-align: center;">Generic</td>
<td style="text-align: center;">Score</td>
<td style="text-align: center;">MQM</td>
<td style="text-align: center;">InstructScore</td>
<td style="text-align: center;">xCOMET</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Zh-En</td>
<td style="text-align: center;">$\begin{gathered} 0 \ 10 \end{gathered}$</td>
<td style="text-align: center;">$0.47 / 0.75 / 0.70$</td>
<td style="text-align: center;">$\begin{gathered} 0.45^{\prime} / 0.63^{1} / 0.71^{1} \ 0.51^{\prime} / 0.63^{1} / 0.73^{1} \end{gathered}$</td>
<td style="text-align: center;">$\begin{gathered} 0.46^{\prime} / 0.69^{2} / 0.7^{1} \ 0.51^{\prime} / 0.64^{2} / 0.73^{1} \end{gathered}$</td>
<td style="text-align: center;">$\begin{gathered} 0.48^{1} / 0.72^{2} / 0.70^{1} \ 0.51^{\prime} / 0.65^{2} / 0.72^{1} \end{gathered}$</td>
<td style="text-align: center;">$\begin{gathered} 0.45^{\prime} / 0.63^{1} / 0.72^{1} \ 0.50^{1} / 0.61^{1} / 0.74^{1} \end{gathered}$</td>
<td style="text-align: center;">$\begin{gathered} 0.45^{\prime} / 0.62^{1} / 0.72^{1} \ 0.51^{\prime} / 0.60^{1} / 0.75 \end{gathered}$</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">En-De</td>
<td style="text-align: center;">$\begin{gathered} 0 \ 10 \end{gathered}$</td>
<td style="text-align: center;">$0.45 / 0.86 / 0.69$</td>
<td style="text-align: center;">$\begin{gathered} 0.55^{1} / 0.57^{1} / 0.78^{1} \ 0.58^{1} / 0.55^{1} / 0.79^{1} \end{gathered}$</td>
<td style="text-align: center;">$\begin{gathered} 0.51^{1} / 0.68^{2} / 0.75^{1} \ 0.55^{1} / 0.59^{2} / 0.80^{1} \end{gathered}$</td>
<td style="text-align: center;">$\begin{gathered} 0.51^{1} / 0.68^{2} / 0.75^{1} \ 0.56^{1} / 0.58^{2} / 0.79^{1} \end{gathered}$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$\begin{gathered} 0.57^{\prime} / 0.54^{\prime} / 0.80^{1} \ 0.53^{1} / 0.57^{1} / 0.81^{\prime} \end{gathered}$</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">En-Ru</td>
<td style="text-align: center;">$\begin{gathered} 0 \ 10 \end{gathered}$</td>
<td style="text-align: center;">$0.43 / 0.82 / 0.73$</td>
<td style="text-align: center;">$\begin{gathered} 0.51^{1} / 0.60^{1} / 0.77^{1} \ 0.53^{1} / 0.56^{1} / 0.79^{1} \end{gathered}$</td>
<td style="text-align: center;">$\begin{gathered} 0.48^{1} / 0.70^{2} / 0.74^{1} \ 0.53^{1} / 0.60^{2} / 0.79^{1} \end{gathered}$</td>
<td style="text-align: center;">$\begin{gathered} 0.48^{1} / 0.69^{2} / 0.74^{1} \ 0.49^{1} / 0.70 \downarrow / 0.79^{1} \end{gathered}$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$\begin{gathered} 0.51^{1} / 0.58^{1} / 0.80^{1} \ 0.53^{1} / 0.56^{1} / 0.82 \end{gathered}$</td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Table 11: Zero- and 10-shot prompting performance of LLaMA-2 7B model. Original column measures between the source and target sentences from the original MQM dataset. Other columns represents the model performance for different types of feedback: Generic, Score, Fine-grained (MQM, xCOMET, InstructScore). Green best performance per language pair; Red worse performance than the original baseline. We test the statistically significance of improvements over the original and $\dagger$ marks results with $p$-value $\leq 0.05$ and $\ddagger$ marks results with $p$-value $\leq 0.1$.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Language</th>
<th style="text-align: center;">Shots</th>
<th style="text-align: center;">BLEU ( $\uparrow$ ) / TER ( $\downarrow$ ) / COMET $_{\text {DA }}(\uparrow)$</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Original</td>
<td style="text-align: center;">Generic</td>
<td style="text-align: center;">Score</td>
<td style="text-align: center;">MQM</td>
<td style="text-align: center;">InstructScore</td>
<td style="text-align: center;">xCOMET</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Zh-En</td>
<td style="text-align: center;">$\begin{gathered} 0 \ 10 \end{gathered}$</td>
<td style="text-align: center;">$0.47 / 0.75 / 0.70$</td>
<td style="text-align: center;">$\begin{gathered} 0.50^{1} / 0.66^{2} / 0.73^{1} \ 0.51^{1} / 0.61^{1} / 0.74^{1} \end{gathered}$</td>
<td style="text-align: center;">$\begin{gathered} 0.50^{1} / 0.72^{2} / 0.72 \ 0.51^{1} / 0.61^{1} / 0.74^{1} \end{gathered}$</td>
<td style="text-align: center;">$\begin{gathered} 0.50^{1} / 0.74^{2} / 0.71^{1} \ 0.50^{1} / 0.61^{1} / 0.73^{1} \end{gathered}$</td>
<td style="text-align: center;">$\begin{gathered} 0.50^{1} / 0.61^{1} / 0.75^{1} \ 0.50^{1} / 0.61^{1} / 0.75^{1} \end{gathered}$</td>
<td style="text-align: center;">$\begin{gathered} 0.53^{1} / 0.59^{1} / 0.76 \ 0.54^{\prime} / 0.58^{1} / 0.76 \end{gathered}$</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">En-De</td>
<td style="text-align: center;">$\begin{gathered} 0 \ 10 \end{gathered}$</td>
<td style="text-align: center;">$0.45 / 0.86 / 0.69$</td>
<td style="text-align: center;">$\begin{gathered} 0.58^{1} / 0.55^{1} / 0.80^{1} \ 0.58^{1} / 0.54^{1} / 0.80^{1} \end{gathered}$</td>
<td style="text-align: center;">$\begin{gathered} 0.49^{1} / 0.73^{1} / 0.73^{1} \ 0.58^{1} / 0.55^{1} / 0.80^{1} \end{gathered}$</td>
<td style="text-align: center;">$\begin{gathered} 0.48^{1} / 0.76^{2} / 0.72 \ 0.58^{1} / 0.54^{1} / 0.80^{1} \end{gathered}$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$\begin{gathered} 0.60^{1} / 0.52^{1} / 0.81^{1} \ 0.62^{1} / 0.51^{1} / 0.82 \end{gathered}$</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">En-Ru</td>
<td style="text-align: center;">$\begin{gathered} 0 \ 10 \end{gathered}$</td>
<td style="text-align: center;">$0.43 / 0.82 / 0.73$</td>
<td style="text-align: center;">$\begin{gathered} 0.53^{1} / 0.56^{1} / 0.79^{1} \ 0.54^{1} / 0.55^{1} / 0.80^{1} \end{gathered}$</td>
<td style="text-align: center;">$\begin{gathered} 0.46^{1} / 0.74^{1} / 0.74^{1} \ 0.54^{1} / 0.56^{1} / 0.80^{1} \end{gathered}$</td>
<td style="text-align: center;">$\begin{gathered} 0.44^{1} / 0.80 / 0.73 \ 0.53^{1} / 0.56^{1} / 0.80^{1} \end{gathered}$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$\begin{gathered} 0.55^{1} / 0.54^{1} / 0.83^{1} \ 0.57^{1} / 0.52^{1} / 0.85 \end{gathered}$</td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Table 12: Zero- and 10-shot prompting performance of LLaMA-2 13B model. Green best performance per language pair; Red worse performance than the original baseline. We test the statistically significance of improvements over the original and $\dagger$ marks results with $p$-value $\leq 0.05$.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Language</th>
<th style="text-align: center;">Shots</th>
<th style="text-align: center;">BLEU ( $\uparrow$ ) / TER ( $\downarrow$ ) / COMET $_{\text {DA }}(\uparrow)$</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Original</td>
<td style="text-align: center;">Generic</td>
<td style="text-align: center;">Score</td>
<td style="text-align: center;">MQM</td>
<td style="text-align: center;">InstructScore</td>
<td style="text-align: center;">xCOMET</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">LLaMA-2 chat 7B</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Zh-En</td>
<td style="text-align: center;">$\begin{gathered} 0 \ 10 \end{gathered}$</td>
<td style="text-align: center;">$0.47 / 0.75 / 0.70$</td>
<td style="text-align: center;">$\begin{gathered} 0.43^{1} / 0.69^{1} / 0.73^{1} \ 0.48 / 0.65^{1} / 0.74^{1} \end{gathered}$</td>
<td style="text-align: center;">$\begin{gathered} 0.45^{1} / 0.66^{1} / 0.74^{1} \ 0.48 / 0.64^{1} / 0.74^{1} \end{gathered}$</td>
<td style="text-align: center;">$\begin{gathered} 0.40^{1} / 0.70^{2} / 0.70 \ 0.48 / 0.64^{1} / 0.74^{1} \end{gathered}$</td>
<td style="text-align: center;">$\begin{gathered} 0.42^{1} / 0.68^{2} / 0.73^{1} \ 0.48 / 0.64^{1} / 0.73^{1} \end{gathered}$</td>
<td style="text-align: center;">$\begin{gathered} 0.41^{1} / 0.67^{1} / 0.73^{1} \ 0.48^{1} / 0.63 / 0.75^{1} \end{gathered}$</td>
<td style="text-align: center;">$\begin{gathered} 0.41^{1} / 0.67^{1} / 0.73^{1} \ 0.50 / 0.63^{1} / 0.76^{1} \end{gathered}$</td>
</tr>
<tr>
<td style="text-align: center;">En-De</td>
<td style="text-align: center;">$\begin{gathered} 0 \ 10 \end{gathered}$</td>
<td style="text-align: center;">$0.45 / 0.86 / 0.69$</td>
<td style="text-align: center;">$\begin{gathered} 0.42 / 0.68^{1} / 0.73^{1} \ 0.54^{1} / 0.60^{1} / 0.78^{1} \end{gathered}$</td>
<td style="text-align: center;">$\begin{gathered} 0.51^{1} / 0.63^{1} / 0.76^{1} \ 0.54^{1} / 0.60^{1} / 0.78^{1} \end{gathered}$</td>
<td style="text-align: center;">$\begin{gathered} 0.54^{1} / 0.62^{1} / 0.76^{1} \ 0.54^{1} / 0.59^{1} / 0.78^{1} \end{gathered}$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$\begin{gathered} 0.50^{1} / 0.6^{1} / 0.77^{1} \ 0.56^{1} / 0.57^{1} / 0.79^{1} \end{gathered}$</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">En-Ru</td>
<td style="text-align: center;">$\begin{gathered} 0 \ 10 \end{gathered}$</td>
<td style="text-align: center;">$0.43 / 0.82 / 0.73$</td>
<td style="text-align: center;">$\begin{gathered} 0.40^{1} / 0.72^{1} / 0.72^{1} \ 0.50^{1} / 0.61^{1} / 0.78^{1} \end{gathered}$</td>
<td style="text-align: center;">$\begin{gathered} 0.45^{1} / 0.66^{1} / 0.73 \ 0.49^{1} / 0.62^{1} / 0.77^{1} \end{gathered}$</td>
<td style="text-align: center;">$\begin{gathered} 0.48^{1} / 0.64^{1} / 0.74^{1} \ 0.50^{1} / 0.61^{1} / 0.77^{1} \end{gathered}$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$\begin{gathered} 0.45^{1} / 0.63^{1} / 0.76^{1} \ 0.52^{1} / 0.59^{1} / 0.81^{1} \end{gathered}$</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">LLaMA-2 chat 13B</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Zh-En</td>
<td style="text-align: center;">$\begin{gathered} 0 \ 10 \end{gathered}$</td>
<td style="text-align: center;">$0.47 / 0.75 / 0.70$</td>
<td style="text-align: center;">$\begin{gathered} 0.40^{1} / 0.71^{1} / 0.73^{1} \ 0.48 / 0.65 / 0.74^{1} \end{gathered}$</td>
<td style="text-align: center;">$\begin{gathered} 0.45^{1} / 0.69^{1} / 0.72^{1} \ 0.48^{1} / 0.64 / 0.74^{1} \end{gathered}$</td>
<td style="text-align: center;">$\begin{gathered} 0.47^{1} / 0.72^{1} / 0.70 \ 0.49^{1} / 0.62 / 0.74^{1} \end{gathered}$</td>
<td style="text-align: center;">$\begin{gathered} 0.44 / 0.65^{1} / 0.75^{1} \ 0.48^{1} / 0.64 / 0.75^{1} \end{gathered}$</td>
<td style="text-align: center;">$\begin{gathered} 0.47 / 0.64^{1} / 0.76^{1} \ 0.51 / 0.62 / 0.76^{1} \end{gathered}$</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">En-De</td>
<td style="text-align: center;">$\begin{gathered} 0 \ 10 \end{gathered}$</td>
<td style="text-align: center;">$0.45 / 0.86 / 0.69$</td>
<td style="text-align: center;">$\begin{gathered} 0.53^{1} / 0.59^{1} / 0.78^{1} \ 0.53^{1} / 0.60^{1} / 0.78^{1} \end{gathered}$</td>
<td style="text-align: center;">$\begin{gathered} 0.50^{1} / 0.71^{1} / 0.74^{1} \ 0.55^{1} / 0.58^{1} / 0.70^{1} \end{gathered}$</td>
<td style="text-align: center;">$\begin{gathered} 0.48^{1} / 0.74^{1} / 0.72 \ 0.55^{1} / 0.58^{1} / 0.79^{1} \end{gathered}$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$\begin{gathered} 0.57^{1} / 0.54^{1} / 0.80^{1} \ 0.55^{1} / 0.58^{1} / 0.80^{1} \end{gathered}$</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">En-Ru</td>
<td style="text-align: center;">$\begin{gathered} 0 \ 10 \end{gathered}$</td>
<td style="text-align: center;">$0.43 / 0.82 / 0.73$</td>
<td style="text-align: center;">$\begin{gathered} 0.47^{1} / 0.61^{1} / 0.77^{1} \ 0.47^{1} / 0.62^{1} / 0.77^{1} \end{gathered}$</td>
<td style="text-align: center;">$\begin{gathered} 0.47^{1} / 0.72^{1} / 0.74^{1} \ 0.49^{1} / 0.60^{1} / 0.79^{1} \end{gathered}$</td>
<td style="text-align: center;">$\begin{gathered} 0.47^{1} / 0.72^{1} / 0.74^{1} \ 0.49^{1} / 0.60^{1} / 0.78^{1} \end{gathered}$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$\begin{gathered} 0.51^{1} / 0.58^{1} / 0.81^{1} \ 0.49^{1} / 0.62^{1} / 0.81^{1} \end{gathered}$</td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Table 13: Top rows: Prompting performance of LLaMA-2 chat 7B model. Bottom rows: LLaMA-2 chat 13B model. Bold denotes best performance for each language pair in 7B and 13B. We test the statistically significance of improvements over the original and $\dagger$ marks results with $p$-value $\leq 0.05$ and $\ddagger$ marks results with $p$-value $\leq 0.1$.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Language</th>
<th style="text-align: center;">Size</th>
<th style="text-align: center;">BLEU $(\uparrow)$ / TER ( $\downarrow$ ) / COMET $_{\text {DA }}(\uparrow)$</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Original</td>
<td style="text-align: center;">Generic</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Score</td>
<td style="text-align: center;">Fine-grained</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Zh-En</td>
<td style="text-align: center;">7B</td>
<td style="text-align: center;">$0.66 / 0.53 / 0.85$</td>
<td style="text-align: center;">$0.61 / 0.56 / 0.82$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$0.62 / 0.55 / 0.82$</td>
<td style="text-align: center;">$0.61 / 0.56 / 0.82$</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">13B</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$0.62 / 0.56 / 0.82$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$0.62 / 0.56 / 0.82$</td>
<td style="text-align: center;">$0.62 / 0.56 / 0.82$</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">En-De</td>
<td style="text-align: center;">7B</td>
<td style="text-align: center;">$0.65 / 0.56 / 0.88$</td>
<td style="text-align: center;">$0.57 / 0.61 / 0.84$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$0.52 / 0.65 / 0.81$</td>
<td style="text-align: center;">$0.58 / 0.61 / 0.84$</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">13B</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$0.64 / 0.56 / 0.88$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$0.65 / 0.56 / 0.87$</td>
<td style="text-align: center;">$0.64 / 0.56 / 0.87$</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">En-Ru</td>
<td style="text-align: center;">7B</td>
<td style="text-align: center;">$0.62 / 0.58 / 0.92$</td>
<td style="text-align: center;">$0.51 / 0.68 / 0.85$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$0.51 / 0.66 / 0.84$</td>
<td style="text-align: center;">$0.56 / 0.64 / 0.87$</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">13B</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$0.61 / 0.60 / 0.91$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$0.62 / 0.58 / 0.91$</td>
<td style="text-align: center;">$0.61 / 0.59 / 0.91$</td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Table 14: Zero-shot prompting performance for instances with no error in their hypothesis translations. Original MT hypothesis: Translation quality from original MQM dataset. Resulting edits lead to small drop in the metrics but they correct stylistic issues such as translationese.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Language</th>
<th style="text-align: center;">Component</th>
<th style="text-align: center;">BLEU $(\uparrow)$</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">TER ( $\downarrow$ )</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">COMET $_{\text {DA }}(\uparrow)$</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">MQM</td>
<td style="text-align: center;">InstructScore</td>
<td style="text-align: center;">xCOMET</td>
<td style="text-align: center;">MQM</td>
<td style="text-align: center;">InstructScore</td>
<td style="text-align: center;">xCOMET</td>
<td style="text-align: center;">MQM</td>
<td style="text-align: center;">InstructScore</td>
<td style="text-align: center;">xCOMET</td>
</tr>
<tr>
<td style="text-align: center;">Zh-En</td>
<td style="text-align: center;">All</td>
<td style="text-align: center;">0.47</td>
<td style="text-align: center;">0.43</td>
<td style="text-align: center;">0.41</td>
<td style="text-align: center;">0.72</td>
<td style="text-align: center;">0.66</td>
<td style="text-align: center;">0.64</td>
<td style="text-align: center;">0.70</td>
<td style="text-align: center;">0.73</td>
<td style="text-align: center;">0.72</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Span</td>
<td style="text-align: center;">0.47</td>
<td style="text-align: center;">0.41</td>
<td style="text-align: center;">0.41</td>
<td style="text-align: center;">0.71</td>
<td style="text-align: center;">0.67</td>
<td style="text-align: center;">0.66</td>
<td style="text-align: center;">0.71</td>
<td style="text-align: center;">0.72</td>
<td style="text-align: center;">0.72</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Type</td>
<td style="text-align: center;">0.47</td>
<td style="text-align: center;">0.43</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.70</td>
<td style="text-align: center;">0.62</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.72</td>
<td style="text-align: center;">0.74</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Severity</td>
<td style="text-align: center;">0.48</td>
<td style="text-align: center;">0.44</td>
<td style="text-align: center;">0.44</td>
<td style="text-align: center;">0.66</td>
<td style="text-align: center;">0.65</td>
<td style="text-align: center;">0.64</td>
<td style="text-align: center;">0.70</td>
<td style="text-align: center;">0.74</td>
<td style="text-align: center;">0.74</td>
</tr>
<tr>
<td style="text-align: center;">En-De</td>
<td style="text-align: center;">All</td>
<td style="text-align: center;">0.47</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">0.54</td>
<td style="text-align: center;">0.75</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">0.60</td>
<td style="text-align: center;">0.71</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">0.75</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Span</td>
<td style="text-align: center;">0.49</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">0.56</td>
<td style="text-align: center;">0.71</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">0.58</td>
<td style="text-align: center;">0.72</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">0.75</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Type</td>
<td style="text-align: center;">0.49</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">0.73</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">0.71</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Severity</td>
<td style="text-align: center;">0.50</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">0.56</td>
<td style="text-align: center;">0.71</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">0.57</td>
<td style="text-align: center;">0.71</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">0.76</td>
</tr>
<tr>
<td style="text-align: center;">En-Ru</td>
<td style="text-align: center;">All</td>
<td style="text-align: center;">0.43</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">0.48</td>
<td style="text-align: center;">0.77</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">0.62</td>
<td style="text-align: center;">0.74</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">0.76</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Span</td>
<td style="text-align: center;">0.45</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">0.48</td>
<td style="text-align: center;">0.75</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">0.62</td>
<td style="text-align: center;">0.73</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">0.77</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Type</td>
<td style="text-align: center;">0.44</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">0.76</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">0.75</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Severity</td>
<td style="text-align: center;">0.45</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">0.50</td>
<td style="text-align: center;">0.76</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">0.61</td>
<td style="text-align: center;">0.76</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">0.78</td>
</tr>
</tbody>
</table>
<p>Table 15: Zero-shot prompting performance of LLaMA-2 7B when breaking down fine-grained feedback into three components. Note that results are missing as InstructScore only supports Zh-En MT pair and xCOMET does not output error type. While the individual contribution of each component is trivial, providing solely the severity level information outperforms the case of giving all components simultaneously. Bold indicates the best performance for each annotation.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Language</th>
<th style="text-align: center;">Type</th>
<th style="text-align: center;">BLEU $(\uparrow)$</th>
<th style="text-align: center;">TER ( $\downarrow$ )</th>
<th style="text-align: center;">COMET $_{\text {DA }}(\uparrow)$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Zh-En</td>
<td style="text-align: center;">Original MQM translation</td>
<td style="text-align: center;">0.47</td>
<td style="text-align: center;">0.75</td>
<td style="text-align: center;">0.70</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">LLaMA-2 (7B)</td>
<td style="text-align: center;">0.24</td>
<td style="text-align: center;">1.54</td>
<td style="text-align: center;">0.71</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">LLaMA-2 (13B)</td>
<td style="text-align: center;">0.47</td>
<td style="text-align: center;">0.72</td>
<td style="text-align: center;">0.73</td>
</tr>
<tr>
<td style="text-align: center;">En-De</td>
<td style="text-align: center;">Original MQM translation</td>
<td style="text-align: center;">0.45</td>
<td style="text-align: center;">0.86</td>
<td style="text-align: center;">0.69</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">LLaMA-2 (7B)</td>
<td style="text-align: center;">0.32</td>
<td style="text-align: center;">1.15</td>
<td style="text-align: center;">0.70</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">LLaMA-2 (13B)</td>
<td style="text-align: center;">0.50</td>
<td style="text-align: center;">0.68</td>
<td style="text-align: center;">0.73</td>
</tr>
<tr>
<td style="text-align: center;">En-Ru</td>
<td style="text-align: center;">Original MQM translation</td>
<td style="text-align: center;">0.43</td>
<td style="text-align: center;">0.82</td>
<td style="text-align: center;">0.73</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">LLaMA-2 (7B)</td>
<td style="text-align: center;">0.23</td>
<td style="text-align: center;">1.25</td>
<td style="text-align: center;">0.66</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">LLaMA-2 (13B)</td>
<td style="text-align: center;">0.44</td>
<td style="text-align: center;">0.73</td>
<td style="text-align: center;">0.75</td>
</tr>
</tbody>
</table>
<p>Table 16: Zero-shot translation from scratch performance of LLaMA-2 7B and 13B. Results do not support the hypothesis of train/test data contamination.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Type</th>
<th style="text-align: left;">Example</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Source</td>
<td style="text-align: left;">可以先豁免，到你们调查完再收取我的月费，这样行吗？</td>
</tr>
<tr>
<td style="text-align: left;">Translation</td>
<td style="text-align: left;">You can first exempt, and then charge my monthly fee until you investigate, okay?</td>
</tr>
<tr>
<td style="text-align: left;">Reference</td>
<td style="text-align: left;">It can be exempted, and you can charge me with the monthly fee after the investigation, is that Okay?</td>
</tr>
<tr>
<td style="text-align: left;">Generic</td>
<td style="text-align: left;">Can you first exempt me, and then charge my monthly fee until you investigate, okay?</td>
</tr>
<tr>
<td style="text-align: left;">Score</td>
<td style="text-align: left;">You can first exempt, and then charge my monthly fee until you investigate, okay?</td>
</tr>
<tr>
<td style="text-align: left;">MQM</td>
<td style="text-align: left;">You can first waive the fee, and then charge my monthly fee after you investigate, okay?</td>
</tr>
<tr>
<td style="text-align: left;">InstructScore</td>
<td style="text-align: left;">You can first exempt, and then charge my monthly fee until you investigate, okay?</td>
</tr>
<tr>
<td style="text-align: left;">xCOMET</td>
<td style="text-align: left;">Can you first exempt, and then charge my monthly fee after you investigate, okay?</td>
</tr>
<tr>
<td style="text-align: left;">FT</td>
<td style="text-align: left;">It can be waived, and you can charge me the monthly fee after you finish the investigation, can that work?</td>
</tr>
</tbody>
</table>
<p>Table 17: Example of error in Chinese-English translation pair. Underlined part represents the error span in the candidate translation. Each row from Generic to FT (fine-tuning) presents the generations by incorporating each type of feedback. For prompting results, we use 10-shot prompting of LLaMA-2 7B.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Type</th>
<th style="text-align: left;">Example</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Source</td>
<td style="text-align: left;">Robust Character Creation Tools - Make the character YOU want to play.</td>
</tr>
<tr>
<td style="text-align: left;">Translation</td>
<td style="text-align: left;">Robust Character Creation Tools - Machen Sie den Charakter, den Sie spielen möchten.</td>
</tr>
<tr>
<td style="text-align: left;">Reference</td>
<td style="text-align: left;">Robuste Charaktererstellungs-Tools - baue dir die Charaktere, die DU spielen möchtest.</td>
</tr>
<tr>
<td style="text-align: left;">Generic</td>
<td style="text-align: left;">Robuste Charaktererstellungswerkzeuge - Machen Sie den Charakter, den Sie spielen möchten.</td>
</tr>
<tr>
<td style="text-align: left;">Score</td>
<td style="text-align: left;">Robuste Charakter-Erstellungswerkzeuge - Erstellt den Charakter, den Sie spielen möchten.</td>
</tr>
<tr>
<td style="text-align: left;">MQM</td>
<td style="text-align: left;">Robuste Charakter-Erstellungswerkzeuge - Machen Sie den Charakter, den Sie spielen möchten.</td>
</tr>
<tr>
<td style="text-align: left;">xCOMET</td>
<td style="text-align: left;">Robuste Charaktererstellung - Machen Sie den Charakter, den Sie spielen möchten.</td>
</tr>
<tr>
<td style="text-align: left;">FT</td>
<td style="text-align: left;">Robuste Charaktererstellungstools - Machen Sie den Charakter, den Sie spielen möchten.</td>
</tr>
</tbody>
</table>
<p>Table 18: Example of major accuracy (mistranslation) error in English-German translation pair. Underlined part represents the error span in the candidate translation.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Type</th>
<th style="text-align: left;">Example</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Source</td>
<td style="text-align: left;">Have you check on your doorstep or reception area?</td>
</tr>
<tr>
<td style="text-align: left;">Translation</td>
<td style="text-align: left;">Вы проверили порог своей двери или приемную?</td>
</tr>
<tr>
<td style="text-align: left;">Reference</td>
<td style="text-align: left;">Вы проверили свой порог или стойку регистрации?</td>
</tr>
<tr>
<td style="text-align: left;">Generic</td>
<td style="text-align: left;">Вы проверили, если вы находитесь на пороге или в регистрационной лавке?</td>
</tr>
<tr>
<td style="text-align: left;">Score</td>
<td style="text-align: left;">Вы проверили свой порог или стойку регистрации?</td>
</tr>
<tr>
<td style="text-align: left;">MQM</td>
<td style="text-align: left;">Вы проверили ваш порог или регистрационную стойку?</td>
</tr>
<tr>
<td style="text-align: left;">xCOMET</td>
<td style="text-align: left;">Вы проверили ли свой порог или стойку регистрации?</td>
</tr>
<tr>
<td style="text-align: left;">FT</td>
<td style="text-align: left;">Проверьте на своем входе или зоне приемных залов?</td>
</tr>
</tbody>
</table>
<p>Table 19: Example of error in English-Russian translation pair. Underlined part represents the error span in the candidate translation.</p>
<h1>0\% Survey Completion</h1>
<p>This project aims to evaluate the ability of AI to fix translation errors.
In this survey, you will be presented with 25 examples of a short text written in English, followed by two translations in German.</p>
<p>For each example, you will be asked to assess which of the two translations is better, both in form and content, and whether the second translation fixes any errors that were present in the first.</p>
<p>We expect that the survey will take about 30 minutes to complete.</p>
<p>Figure 7: Instructions for human evaluation. This is shown as the first page of our survey to all annotators.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Language</th>
<th style="text-align: center;">Feedback</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Zh-En</td>
<td style="text-align: center;">(1) Translation 1 is better because it explains more, but Translation 2 corrects the errors that 1 has. <br> (2) Translation 1 is better than Translation 2, is more specific and understandable. <br> (3) Translation 2 is better because it is easy to understand and explains the context. <br> (4) Translation 2 fixes all errors in Translation 1. <br> (5) Translation 2 is better because there are no errors and it is more concrete. <br> (6) Translation 2 is better, but it can improve more.</td>
</tr>
<tr>
<td style="text-align: center;">En-De</td>
<td style="text-align: center;">(1) Although Translation 1 is more faithful to the original source sentence, it looks like it was directly translated from it. <br> (2) Translation 2 is more fitting to the actual use of the German language syntax and flow.</td>
</tr>
<tr>
<td style="text-align: center;">En-Ru</td>
<td style="text-align: center;">(1) Translation 2 is better because it avoids the major error of Translation 1. <br> (2) Translation 2 is more accurate and flows better in the target language. <br> (3) Translation 2 correctly uses the phrase in the source sentence while Translation 1 has a small error, which is not contextually correct. <br> (4) Translation 1 does not have a Russian translation of the English text. <br> (5) Translation 2 was more emotive than the original text. <br> (6) Translation 1 is misleading whereas Translation 2 speaks on the actual events.</td>
</tr>
</tbody>
</table>
<p>Table 20: Feedback from the human annotators. We refer to Translation 1 as initial translation and Translation 2 as output translation from our fine-tuned model.</p>
<p>1\% Survey Completion
Please read the following English text and two translations into German:</p>
<h1>Source:</h1>
<p>Organisations on high alert as techs race to fix software flaw</p>
<h2>Translation1:</h2>
<p>Organisationen in höchster Alarmbereitschaft, während die Techniker um die Behebung von Softwarefehlern rennen</p>
<h2>Translation2:</h2>
<p>Rüstung in Bereitschaft, während die Techniker das Fehlerhervorrufen verhindern müssen</p>
<p>A good translation should accurately convey the meaning of the original text and should flow well in the target language. Please indicate to what extent you agree with the following statement:</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Strongly Disagree</th>
<th style="text-align: center;">Disagree</th>
<th style="text-align: center;">Not sure</th>
<th style="text-align: center;">Agree</th>
<th style="text-align: center;">Strongly Agree</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Translation 2 is better than <br> Translation 1.</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: left;">Translation 2 fixes errors <br> that were present in <br> Translation 1</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
</tr>
</tbody>
</table>
<p>If you would like to further explain your ratings, please do so here:
$\square$</p>
<p>Figure 8: Survey content for human evaluation. Given Source, Translation 1 (original translation), and Translation 2 (output translation from the bilingual fine-tuned model), annotators are asked to answer 2 questions on a scale from 0 to 5 . Extra text box is given for each example for further suggestions.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{7}$ https://www.qualtrics.com/
${ }^{8}$ https://www.prolific.com/&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7542 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7542</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7542</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-138.html">extraction-schema-138</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of the use of large language models as text-based simulators for scientific subdomains, including details about the model, domain, simulation task, prompting strategy, evaluation metrics, reported accuracy, baselines, and any factors identified as influencing accuracy.</div>
                <p><strong>Paper ID:</strong> paper-a52dd1e900200e0733eea927edc7d6c27aeba187</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/a52dd1e900200e0733eea927edc7d6c27aeba187" target="_blank">TheoremQA: A Theorem-driven Question Answering dataset</a></p>
                <p><strong>Paper Venue:</strong> Conference on Empirical Methods in Natural Language Processing</p>
                <p><strong>Paper TL;DR:</strong> This paper introduces TheoremQA, the first theorem-driven question-answering dataset designed to evaluate AI models' capabilities to apply theorems to solve challenging science problems and finds that GPT-4's capabilities to solve these problems are unparalleled, achieving an accuracy of 51% with Program-of-Thoughts Prompting.</p>
                <p><strong>Paper Abstract:</strong> The recent LLMs like GPT-4 and PaLM-2 have made tremendous progress in solving fundamental math problems like GSM8K by achieving over 90% accuracy. However, their capabilities to solve more challenging math problems which require domain-specific knowledge (i.e. theorem) have yet to be investigated. In this paper, we introduce TheoremQA, the first theorem-driven question-answering dataset designed to evaluate AI models' capabilities to apply theorems to solve challenging science problems. TheoremQA is curated by domain experts containing 800 high-quality questions covering 350 theorems (e.g. Taylor's theorem, Lagrange's theorem, Huffman coding, Quantum Theorem, Elasticity Theorem, etc) from Math, Physics, EE&CS, and Finance. We evaluate a wide spectrum of 16 large language and code models with different prompting strategies like Chain-of-Thoughts and Program-of-Thoughts. We found that GPT-4's capabilities to solve these problems are unparalleled, achieving an accuracy of 51% with Program-of-Thoughts Prompting. All the existing open-sourced models are below 15%, barely surpassing the random-guess baseline. Given the diversity and broad coverage of TheoremQA, we believe it can be used as a better benchmark to evaluate LLMs' capabilities to solve challenging science problems. The data and code are released in https://github.com/wenhuchen/TheoremQA.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7542.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7542.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of the use of large language models as text-based simulators for scientific subdomains, including details about the model, domain, simulation task, prompting strategy, evaluation metrics, reported accuracy, baselines, and any factors identified as influencing accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generative Pre-trained Transformer 4</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A closed-source instruction-tuned large language model from OpenAI evaluated as a text-based simulator to apply university-level theorems to solve numerical and symbolic STEM problems across Math, Physics, EE&CS, and Finance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>instruction-tuned (closed-source)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>STEM (Mathematics, Physics, Electrical Engineering & Computer Science, Finance)</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task_description</strong></td>
                            <td>Text-based simulation of theorem application: solve theorem-driven questions (compute integrals, probabilities, projections, coding-based numeric solvers, Riemann sums, vector calculus, etc.) by producing step-by-step reasoning or executable programs whose outputs are executed to produce final numeric answers.</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_strategy</strong></td>
                            <td>Chain-of-Thought (CoT) prompting and Program-of-Thoughts (PoT) prompting (generate Python program then execute externally); also evaluated with theorem-augmentation (concatenated theorem text) and multimodal caption augmentation for image questions.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Accuracy (exact-match of normalized numeric/boolean/list/multiple-choice answers after normalization via WolframAlpha); additional diagnostics: % of generated programs that are executable, error-type breakdown (E1/E2/E3).</td>
                        </tr>
                        <tr>
                            <td><strong>reported_accuracy</strong></td>
                            <td>CoT: 43.8% overall accuracy; PoT: 52.4% overall accuracy (table values; text sometimes reports ~51% with PoT).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_accuracy</strong></td>
                            <td>Random-guess baseline: ~10.5% overall; other closed-source models: ChatGPT 30.2% (CoT) / 35.6% (PoT) as comparative baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>factors_reported</strong></td>
                            <td>['Prompting strategy: PoT consistently improves accuracy vs. CoT for GPT models (≈+5–8%).', 'Program executability: higher % of runnable programs correlates with higher PoT gains (GPT-4 ~92% runnable).', 'Instruction tuning / pretraining / private data: closed-source GPT variants show large advantage over open-source instruction-tuned models.', 'Error types: many GPT-4 errors are minor calculation/rounding mistakes (E3) suggesting accuracy sensitive to numeric fidelity and execution correctness.', 'Theorem augmentation (simple concatenation) provided negligible improvement (~≤1%).', 'Multimodal captioning/visual encoding quality: information loss in captions harms performance on image-based theorem questions.']</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_conditions</strong></td>
                            <td>Models evaluated as instruction-tuned chat endpoints; PoT outputs Python programs which were executed externally (an external executor) to obtain numeric answers; answer extraction used ChatGPT to find answer spans and WolframAlpha to normalize outputs; multimodal images were converted to captions (BLIP/Chameleon) and appended to prompts; temperatures or sampling hyperparameters not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_modes</strong></td>
                            <td>Non-critical errors often due to calculation/rounding; multimodal questions extremely challenging (accuracy <10%) due to caption information loss; simple theorem concatenation does not help; dataset avoids outputs that are hard to auto-evaluate (e.g., symbolic/matrix answers), so observed accuracy is a conservative estimate; some instability in CoT numeric steps.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'TheoremQA: A Theorem-driven Question Answering dataset', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7542.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7542.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of the use of large language models as text-based simulators for scientific subdomains, including details about the model, domain, simulation task, prompting strategy, evaluation metrics, reported accuracy, baselines, and any factors identified as influencing accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChatGPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChatGPT (GPT-3.5-based conversational model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An instruction-following chat model (OpenAI) evaluated on theorem-driven STEM QA, producing natural-language reasoning or programs for numeric answer extraction.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGPT</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>instruction-tuned conversational</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>STEM (Mathematics, Physics, CS & EE, Finance)</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task_description</strong></td>
                            <td>Apply university-level theorems to solve numerical STEM questions; generate stepwise reasoning or Python programs to compute final numeric answers.</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_strategy</strong></td>
                            <td>Chain-of-Thought (CoT) and Program-of-Thoughts (PoT); also tested with theorem-augmentation (concatenated theorem descriptions).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Accuracy (exact-match normalized numeric answers); comparison of CoT vs PoT performance.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_accuracy</strong></td>
                            <td>CoT: 30.2% overall accuracy; PoT: 35.6% overall accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_accuracy</strong></td>
                            <td>Random-guess baseline ~10.5%; ChatGPT used as mid-tier closed-source baseline vs GPT-4 (higher) and open-source models (lower).</td>
                        </tr>
                        <tr>
                            <td><strong>factors_reported</strong></td>
                            <td>['Prompting strategy: PoT improved performance relative to CoT (consistent gain).', 'Executable program ratio: ChatGPT typically generated ~80% runnable programs (higher than many open-source models) improving PoT effectiveness.', 'Instruction-tuning and conversational format likely contribute to numeric reasoning abilities.']</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_conditions</strong></td>
                            <td>Same pipeline as other GPT models: PoT program execution externally; answer extraction normalized with WolframAlpha; theorem concatenation tested (+/-) with negligible gains.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_modes</strong></td>
                            <td>Makes numeric/calculation errors sometimes; PoT helps but does not reach GPT-4 levels; theorem-augmentation provided minimal improvement; multimodal questions perform poorly due to captioning loss.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'TheoremQA: A Theorem-driven Question Answering dataset', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7542.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7542.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of the use of large language models as text-based simulators for scientific subdomains, including details about the model, domain, simulation task, prompting strategy, evaluation metrics, reported accuracy, baselines, and any factors identified as influencing accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5 / GPT-3</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3.5 / GPT-3 (text-davinci-002)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Earlier OpenAI instruction-tuned LMs evaluated on theorem-driven problems; produce CoT or programmatic outputs with lower accuracy than later GPT variants.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5 (and GPT-3 text-davinci-002)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>instruction-tuned (closed-source)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>STEM (Mathematics, Physics, CS & EE, Finance)</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task_description</strong></td>
                            <td>Solve theorem-based numerical problems via CoT or program generation; used as lower-performing closed-source baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_strategy</strong></td>
                            <td>CoT and PoT; for non-chat models (text-davinci-002) one-shot example provided to control output format.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Accuracy (exact-match normalized numeric answers).</td>
                        </tr>
                        <tr>
                            <td><strong>reported_accuracy</strong></td>
                            <td>GPT-3 (text-davinci-002) CoT: 16.6%; GPT-3.5 CoT: 22.8%; PoT improves both (GPT-3 PoT 20.6%, GPT-3.5 PoT 27.8%).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_accuracy</strong></td>
                            <td>Random-guess baseline ~10.5%; these models sit between open-source models (~10–14%) and ChatGPT/GPT-4.</td>
                        </tr>
                        <tr>
                            <td><strong>factors_reported</strong></td>
                            <td>['Prompting strategy: PoT yields measurable gains vs CoT.', 'Model architecture and instruction-tuning quality affect performance; non-chat models require format-control examples.', 'Executable program ratio increases with model capability and correlates with PoT gains.']</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_conditions</strong></td>
                            <td>One-shot exemplar used for non-chat models to force format; programs executed externally when generated; answer normalization via WolframAlpha.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_modes</strong></td>
                            <td>Lower program-run reliability and lower numeric reasoning accuracy; still susceptible to calculation mistakes; limited capability to apply domain theorems compared to GPT-4.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'TheoremQA: A Theorem-driven Question Answering dataset', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7542.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7542.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of the use of large language models as text-based simulators for scientific subdomains, including details about the model, domain, simulation task, prompting strategy, evaluation metrics, reported accuracy, baselines, and any factors identified as influencing accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PaLM-2</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PaLM 2 (Google)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Google's instruction-tuned large language model evaluated as a text-based theorem solver across STEM QA in TheoremQA.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PaLM-2</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>instruction-tuned (closed-source)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>STEM (Math, Physics, CS & EE, Finance)</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task_description</strong></td>
                            <td>Apply theorems to answer numeric STEM questions using CoT prompting (and compared against PoT where reported).</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_strategy</strong></td>
                            <td>Chain-of-Thought (CoT) primarily reported.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Accuracy (exact-match normalized numeric answers).</td>
                        </tr>
                        <tr>
                            <td><strong>reported_accuracy</strong></td>
                            <td>CoT: 31.8% overall accuracy (PoT not reported for PaLM-2 in table).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_accuracy</strong></td>
                            <td>Random-guess baseline ~10.5%; lower than GPT-4 but comparable to ChatGPT on some subfields.</td>
                        </tr>
                        <tr>
                            <td><strong>factors_reported</strong></td>
                            <td>['Instruction-tuned model capabilities influence performance; CoT helps but less than GPT-4.', 'Domain coverage of theorems and numeric precision impact results.']</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_conditions</strong></td>
                            <td>Evaluated with CoT prompting; same answer extraction and normalization pipeline as other models.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_modes</strong></td>
                            <td>Does not match GPT-4; still susceptible to numerical reasoning errors and theorem application mistakes.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'TheoremQA: A Theorem-driven Question Answering dataset', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7542.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e7542.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of the use of large language models as text-based simulators for scientific subdomains, including details about the model, domain, simulation task, prompting strategy, evaluation metrics, reported accuracy, baselines, and any factors identified as influencing accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Claude-v1</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Claude v1 (Anthropic)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Anthropic's instruction-tuned model evaluated on the TheoremQA benchmark for theorem-driven numeric reasoning using CoT and PoT prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Claude-v1</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>instruction-tuned (closed-source)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>STEM (Mathematics, Physics, CS & EE, Finance)</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task_description</strong></td>
                            <td>Text-based simulation of theorem application to produce numeric answers; compared across CoT and PoT prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_strategy</strong></td>
                            <td>Chain-of-Thought (CoT) and Program-of-Thoughts (PoT).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Accuracy (exact-match normalized numeric answers); program-run validity for PoT.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_accuracy</strong></td>
                            <td>CoT: 24.9% overall; PoT: 25.9% overall (small PoT gains compared to GPT models).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_accuracy</strong></td>
                            <td>Random-guess baseline ~10.5%; sits below ChatGPT and PaLM-2, above many open-source models.</td>
                        </tr>
                        <tr>
                            <td><strong>factors_reported</strong></td>
                            <td>['Prompting strategy: PoT gave limited improvement over CoT compared to GPT models.', 'Program generation validity: Claude produced a higher fraction of valid programs (~60%) than many open-source models but lower than GPT-* models.']</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_conditions</strong></td>
                            <td>Used CoT and PoT; program executability measured; same answer normalization pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_modes</strong></td>
                            <td>Smaller PoT benefit due to lower program executability; still makes domain/formula errors and calculation mistakes.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'TheoremQA: A Theorem-driven Question Answering dataset', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7542.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e7542.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of the use of large language models as text-based simulators for scientific subdomains, including details about the model, domain, simulation task, prompting strategy, evaluation metrics, reported accuracy, baselines, and any factors identified as influencing accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Alpaca-13B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Alpaca (13B, instruction-tuned LLaMA derivative)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source instruction-tuned 13B model based on LLaMA evaluated on TheoremQA; performs near random baseline and struggles to apply theorems.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Alpaca-13B</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>13B</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>instruction-tuned (open-source, fine-tuned on GPT-4-generated data)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>STEM (Mathematics, Physics, CS & EE, Finance)</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task_description</strong></td>
                            <td>Attempt to apply university-level theorems to answer numeric STEM questions via CoT and PoT prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_strategy</strong></td>
                            <td>Chain-of-Thought (CoT) and Program-of-Thoughts (PoT); also evaluated with theorem-augmentation concatenation.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Accuracy (exact-match normalized numeric answers); error-type analysis (E1/E2/E3).</td>
                        </tr>
                        <tr>
                            <td><strong>reported_accuracy</strong></td>
                            <td>CoT: 13.5% overall; with theorem-augmentation CoT + theorem: 14.2% (Table 3); PoT results ~11–12% in PoT table entries for similar open-source models.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_accuracy</strong></td>
                            <td>Random-guess baseline ~10.5%; Alpaca only slightly above random and well below closed-source models.</td>
                        </tr>
                        <tr>
                            <td><strong>factors_reported</strong></td>
                            <td>['Lack of theorem knowledge: human eval shows ~90% of Alpaca errors are E1 (does not know the theorem).', 'Instruction-tuning data source (synthetic GPT-generated) insufficient to imbue deep theorem knowledge.', 'Program generation/executability poor (~40% runnable for many open-source code-capable models), limiting PoT benefits.']</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_conditions</strong></td>
                            <td>Standard CoT and PoT prompts used; theorem concatenation tested (small gains); answer extraction and normalization pipeline used.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_modes</strong></td>
                            <td>Dominant failure mode is ignorance of theorem (E1) producing unfixable errors; little benefit from PoT; overall performance near random baseline; dataset intentionally excludes answers hard to auto-evaluate (may bias results).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'TheoremQA: A Theorem-driven Question Answering dataset', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7542.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e7542.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of the use of large language models as text-based simulators for scientific subdomains, including details about the model, domain, simulation task, prompting strategy, evaluation metrics, reported accuracy, baselines, and any factors identified as influencing accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>StarChat-16B / StarCoder-based</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>StarChat (16B) / StarCoder-based instruction-tuned model</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source 16B model (StarCoder family) instruction-tuned for conversational tasks and evaluated on theorem-driven numerical QA; struggles with runnable program generation and low accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>StarChat-16B</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>16B</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>instruction-tuned (open-source, code-capable)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>STEM (Mathematics, Physics, CS & EE, Finance)</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task_description</strong></td>
                            <td>Generate reasoning or Python programs to solve theorem-based numeric questions; PoT prompting evaluated including program executability measurement.</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_strategy</strong></td>
                            <td>CoT and PoT prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Accuracy (exact-match normalized numeric answers); program executability ratio.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_accuracy</strong></td>
                            <td>CoT: 11.6% overall; PoT: 11.3% overall (Table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_accuracy</strong></td>
                            <td>Random-guess baseline ~10.5%; performance only marginally above random.</td>
                        </tr>
                        <tr>
                            <td><strong>factors_reported</strong></td>
                            <td>['Low program executability (~40% runnable) limiting PoT effectiveness.', 'Instruction-tuning corpus and model scale insufficient to capture theorem knowledge.', 'Diagram/image captioning information loss affects multimodal subset.']</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_conditions</strong></td>
                            <td>PoT generated programs were tested for executability; same answer normalization pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_modes</strong></td>
                            <td>Poor program quality and low runnable program fraction; accuracy close to random; multimodal performance poor.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'TheoremQA: A Theorem-driven Question Answering dataset', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7542.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e7542.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of the use of large language models as text-based simulators for scientific subdomains, including details about the model, domain, simulation task, prompting strategy, evaluation metrics, reported accuracy, baselines, and any factors identified as influencing accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>InstructCodeT5+ (CodeT5+)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>InstructCodeT5+ (instruction-tuned CodeT5+)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A code-oriented open-source model (CodeT5+ based, instruction-tuned) evaluated with PoT/CoT on theorem-driven numeric problems; low overall accuracy and program executability.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>InstructCodeT5+</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>16B</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>instruction-tuned code model (open-source)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>STEM (Mathematics, Physics, CS & EE, Finance)</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task_description</strong></td>
                            <td>Use program-generation (PoT) and CoT to solve theorem-driven numerical tasks; intended to leverage code capabilities for numeric execution.</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_strategy</strong></td>
                            <td>PoT and CoT prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Accuracy (exact-match normalized numeric answers); program executability measured.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_accuracy</strong></td>
                            <td>CoT: ~11.6% overall (in table PoT: 11.6% for InstructCodeT5+), performance near random.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_accuracy</strong></td>
                            <td>Random-guess baseline ~10.5%; similar open-source models performed in 10–14% range.</td>
                        </tr>
                        <tr>
                            <td><strong>factors_reported</strong></td>
                            <td>['Program generation quality low; only a minority of generated programs runnable.', 'Open-source pretraining and instruction-tuning insufficient to embed theorem knowledge.', 'PoT provides limited benefit when generated programs are not executable.']</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_conditions</strong></td>
                            <td>Programs generated under PoT were executed externally to obtain numeric outputs; same answer extraction and normalization.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_modes</strong></td>
                            <td>Low runnable program fraction, low accuracy close to random, errors mainly due to lack of theorem knowledge and poor program generation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'TheoremQA: A Theorem-driven Question Answering dataset', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks <em>(Rating: 2)</em></li>
                <li>Chain-of-thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>PAL: Program-aided Language Models <em>(Rating: 2)</em></li>
                <li>GPT-4 technical report <em>(Rating: 2)</em></li>
                <li>Bootstrapping Language-Image Pre-training (BLIP) <em>(Rating: 1)</em></li>
                <li>Visual instruction tuning <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7542",
    "paper_id": "paper-a52dd1e900200e0733eea927edc7d6c27aeba187",
    "extraction_schema_id": "extraction-schema-138",
    "extracted_data": [
        {
            "name_short": "GPT-4",
            "name_full": "Generative Pre-trained Transformer 4",
            "brief_description": "A closed-source instruction-tuned large language model from OpenAI evaluated as a text-based simulator to apply university-level theorems to solve numerical and symbolic STEM problems across Math, Physics, EE&CS, and Finance.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_size": null,
            "model_type": "instruction-tuned (closed-source)",
            "scientific_domain": "STEM (Mathematics, Physics, Electrical Engineering & Computer Science, Finance)",
            "simulation_task_description": "Text-based simulation of theorem application: solve theorem-driven questions (compute integrals, probabilities, projections, coding-based numeric solvers, Riemann sums, vector calculus, etc.) by producing step-by-step reasoning or executable programs whose outputs are executed to produce final numeric answers.",
            "prompting_strategy": "Chain-of-Thought (CoT) prompting and Program-of-Thoughts (PoT) prompting (generate Python program then execute externally); also evaluated with theorem-augmentation (concatenated theorem text) and multimodal caption augmentation for image questions.",
            "evaluation_metric": "Accuracy (exact-match of normalized numeric/boolean/list/multiple-choice answers after normalization via WolframAlpha); additional diagnostics: % of generated programs that are executable, error-type breakdown (E1/E2/E3).",
            "reported_accuracy": "CoT: 43.8% overall accuracy; PoT: 52.4% overall accuracy (table values; text sometimes reports ~51% with PoT).",
            "baseline_accuracy": "Random-guess baseline: ~10.5% overall; other closed-source models: ChatGPT 30.2% (CoT) / 35.6% (PoT) as comparative baselines.",
            "factors_reported": [
                "Prompting strategy: PoT consistently improves accuracy vs. CoT for GPT models (≈+5–8%).",
                "Program executability: higher % of runnable programs correlates with higher PoT gains (GPT-4 ~92% runnable).",
                "Instruction tuning / pretraining / private data: closed-source GPT variants show large advantage over open-source instruction-tuned models.",
                "Error types: many GPT-4 errors are minor calculation/rounding mistakes (E3) suggesting accuracy sensitive to numeric fidelity and execution correctness.",
                "Theorem augmentation (simple concatenation) provided negligible improvement (~≤1%).",
                "Multimodal captioning/visual encoding quality: information loss in captions harms performance on image-based theorem questions."
            ],
            "experimental_conditions": "Models evaluated as instruction-tuned chat endpoints; PoT outputs Python programs which were executed externally (an external executor) to obtain numeric answers; answer extraction used ChatGPT to find answer spans and WolframAlpha to normalize outputs; multimodal images were converted to captions (BLIP/Chameleon) and appended to prompts; temperatures or sampling hyperparameters not reported.",
            "limitations_or_failure_modes": "Non-critical errors often due to calculation/rounding; multimodal questions extremely challenging (accuracy &lt;10%) due to caption information loss; simple theorem concatenation does not help; dataset avoids outputs that are hard to auto-evaluate (e.g., symbolic/matrix answers), so observed accuracy is a conservative estimate; some instability in CoT numeric steps.",
            "uuid": "e7542.0",
            "source_info": {
                "paper_title": "TheoremQA: A Theorem-driven Question Answering dataset",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "ChatGPT",
            "name_full": "ChatGPT (GPT-3.5-based conversational model)",
            "brief_description": "An instruction-following chat model (OpenAI) evaluated on theorem-driven STEM QA, producing natural-language reasoning or programs for numeric answer extraction.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "ChatGPT",
            "model_size": null,
            "model_type": "instruction-tuned conversational",
            "scientific_domain": "STEM (Mathematics, Physics, CS & EE, Finance)",
            "simulation_task_description": "Apply university-level theorems to solve numerical STEM questions; generate stepwise reasoning or Python programs to compute final numeric answers.",
            "prompting_strategy": "Chain-of-Thought (CoT) and Program-of-Thoughts (PoT); also tested with theorem-augmentation (concatenated theorem descriptions).",
            "evaluation_metric": "Accuracy (exact-match normalized numeric answers); comparison of CoT vs PoT performance.",
            "reported_accuracy": "CoT: 30.2% overall accuracy; PoT: 35.6% overall accuracy.",
            "baseline_accuracy": "Random-guess baseline ~10.5%; ChatGPT used as mid-tier closed-source baseline vs GPT-4 (higher) and open-source models (lower).",
            "factors_reported": [
                "Prompting strategy: PoT improved performance relative to CoT (consistent gain).",
                "Executable program ratio: ChatGPT typically generated ~80% runnable programs (higher than many open-source models) improving PoT effectiveness.",
                "Instruction-tuning and conversational format likely contribute to numeric reasoning abilities."
            ],
            "experimental_conditions": "Same pipeline as other GPT models: PoT program execution externally; answer extraction normalized with WolframAlpha; theorem concatenation tested (+/-) with negligible gains.",
            "limitations_or_failure_modes": "Makes numeric/calculation errors sometimes; PoT helps but does not reach GPT-4 levels; theorem-augmentation provided minimal improvement; multimodal questions perform poorly due to captioning loss.",
            "uuid": "e7542.1",
            "source_info": {
                "paper_title": "TheoremQA: A Theorem-driven Question Answering dataset",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "GPT-3.5 / GPT-3",
            "name_full": "GPT-3.5 / GPT-3 (text-davinci-002)",
            "brief_description": "Earlier OpenAI instruction-tuned LMs evaluated on theorem-driven problems; produce CoT or programmatic outputs with lower accuracy than later GPT variants.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-3.5 (and GPT-3 text-davinci-002)",
            "model_size": null,
            "model_type": "instruction-tuned (closed-source)",
            "scientific_domain": "STEM (Mathematics, Physics, CS & EE, Finance)",
            "simulation_task_description": "Solve theorem-based numerical problems via CoT or program generation; used as lower-performing closed-source baselines.",
            "prompting_strategy": "CoT and PoT; for non-chat models (text-davinci-002) one-shot example provided to control output format.",
            "evaluation_metric": "Accuracy (exact-match normalized numeric answers).",
            "reported_accuracy": "GPT-3 (text-davinci-002) CoT: 16.6%; GPT-3.5 CoT: 22.8%; PoT improves both (GPT-3 PoT 20.6%, GPT-3.5 PoT 27.8%).",
            "baseline_accuracy": "Random-guess baseline ~10.5%; these models sit between open-source models (~10–14%) and ChatGPT/GPT-4.",
            "factors_reported": [
                "Prompting strategy: PoT yields measurable gains vs CoT.",
                "Model architecture and instruction-tuning quality affect performance; non-chat models require format-control examples.",
                "Executable program ratio increases with model capability and correlates with PoT gains."
            ],
            "experimental_conditions": "One-shot exemplar used for non-chat models to force format; programs executed externally when generated; answer normalization via WolframAlpha.",
            "limitations_or_failure_modes": "Lower program-run reliability and lower numeric reasoning accuracy; still susceptible to calculation mistakes; limited capability to apply domain theorems compared to GPT-4.",
            "uuid": "e7542.2",
            "source_info": {
                "paper_title": "TheoremQA: A Theorem-driven Question Answering dataset",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "PaLM-2",
            "name_full": "PaLM 2 (Google)",
            "brief_description": "Google's instruction-tuned large language model evaluated as a text-based theorem solver across STEM QA in TheoremQA.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "PaLM-2",
            "model_size": null,
            "model_type": "instruction-tuned (closed-source)",
            "scientific_domain": "STEM (Math, Physics, CS & EE, Finance)",
            "simulation_task_description": "Apply theorems to answer numeric STEM questions using CoT prompting (and compared against PoT where reported).",
            "prompting_strategy": "Chain-of-Thought (CoT) primarily reported.",
            "evaluation_metric": "Accuracy (exact-match normalized numeric answers).",
            "reported_accuracy": "CoT: 31.8% overall accuracy (PoT not reported for PaLM-2 in table).",
            "baseline_accuracy": "Random-guess baseline ~10.5%; lower than GPT-4 but comparable to ChatGPT on some subfields.",
            "factors_reported": [
                "Instruction-tuned model capabilities influence performance; CoT helps but less than GPT-4.",
                "Domain coverage of theorems and numeric precision impact results."
            ],
            "experimental_conditions": "Evaluated with CoT prompting; same answer extraction and normalization pipeline as other models.",
            "limitations_or_failure_modes": "Does not match GPT-4; still susceptible to numerical reasoning errors and theorem application mistakes.",
            "uuid": "e7542.3",
            "source_info": {
                "paper_title": "TheoremQA: A Theorem-driven Question Answering dataset",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "Claude-v1",
            "name_full": "Claude v1 (Anthropic)",
            "brief_description": "Anthropic's instruction-tuned model evaluated on the TheoremQA benchmark for theorem-driven numeric reasoning using CoT and PoT prompting.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Claude-v1",
            "model_size": null,
            "model_type": "instruction-tuned (closed-source)",
            "scientific_domain": "STEM (Mathematics, Physics, CS & EE, Finance)",
            "simulation_task_description": "Text-based simulation of theorem application to produce numeric answers; compared across CoT and PoT prompting.",
            "prompting_strategy": "Chain-of-Thought (CoT) and Program-of-Thoughts (PoT).",
            "evaluation_metric": "Accuracy (exact-match normalized numeric answers); program-run validity for PoT.",
            "reported_accuracy": "CoT: 24.9% overall; PoT: 25.9% overall (small PoT gains compared to GPT models).",
            "baseline_accuracy": "Random-guess baseline ~10.5%; sits below ChatGPT and PaLM-2, above many open-source models.",
            "factors_reported": [
                "Prompting strategy: PoT gave limited improvement over CoT compared to GPT models.",
                "Program generation validity: Claude produced a higher fraction of valid programs (~60%) than many open-source models but lower than GPT-* models."
            ],
            "experimental_conditions": "Used CoT and PoT; program executability measured; same answer normalization pipeline.",
            "limitations_or_failure_modes": "Smaller PoT benefit due to lower program executability; still makes domain/formula errors and calculation mistakes.",
            "uuid": "e7542.4",
            "source_info": {
                "paper_title": "TheoremQA: A Theorem-driven Question Answering dataset",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "Alpaca-13B",
            "name_full": "Alpaca (13B, instruction-tuned LLaMA derivative)",
            "brief_description": "An open-source instruction-tuned 13B model based on LLaMA evaluated on TheoremQA; performs near random baseline and struggles to apply theorems.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Alpaca-13B",
            "model_size": "13B",
            "model_type": "instruction-tuned (open-source, fine-tuned on GPT-4-generated data)",
            "scientific_domain": "STEM (Mathematics, Physics, CS & EE, Finance)",
            "simulation_task_description": "Attempt to apply university-level theorems to answer numeric STEM questions via CoT and PoT prompting.",
            "prompting_strategy": "Chain-of-Thought (CoT) and Program-of-Thoughts (PoT); also evaluated with theorem-augmentation concatenation.",
            "evaluation_metric": "Accuracy (exact-match normalized numeric answers); error-type analysis (E1/E2/E3).",
            "reported_accuracy": "CoT: 13.5% overall; with theorem-augmentation CoT + theorem: 14.2% (Table 3); PoT results ~11–12% in PoT table entries for similar open-source models.",
            "baseline_accuracy": "Random-guess baseline ~10.5%; Alpaca only slightly above random and well below closed-source models.",
            "factors_reported": [
                "Lack of theorem knowledge: human eval shows ~90% of Alpaca errors are E1 (does not know the theorem).",
                "Instruction-tuning data source (synthetic GPT-generated) insufficient to imbue deep theorem knowledge.",
                "Program generation/executability poor (~40% runnable for many open-source code-capable models), limiting PoT benefits."
            ],
            "experimental_conditions": "Standard CoT and PoT prompts used; theorem concatenation tested (small gains); answer extraction and normalization pipeline used.",
            "limitations_or_failure_modes": "Dominant failure mode is ignorance of theorem (E1) producing unfixable errors; little benefit from PoT; overall performance near random baseline; dataset intentionally excludes answers hard to auto-evaluate (may bias results).",
            "uuid": "e7542.5",
            "source_info": {
                "paper_title": "TheoremQA: A Theorem-driven Question Answering dataset",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "StarChat-16B / StarCoder-based",
            "name_full": "StarChat (16B) / StarCoder-based instruction-tuned model",
            "brief_description": "An open-source 16B model (StarCoder family) instruction-tuned for conversational tasks and evaluated on theorem-driven numerical QA; struggles with runnable program generation and low accuracy.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "StarChat-16B",
            "model_size": "16B",
            "model_type": "instruction-tuned (open-source, code-capable)",
            "scientific_domain": "STEM (Mathematics, Physics, CS & EE, Finance)",
            "simulation_task_description": "Generate reasoning or Python programs to solve theorem-based numeric questions; PoT prompting evaluated including program executability measurement.",
            "prompting_strategy": "CoT and PoT prompting.",
            "evaluation_metric": "Accuracy (exact-match normalized numeric answers); program executability ratio.",
            "reported_accuracy": "CoT: 11.6% overall; PoT: 11.3% overall (Table 2).",
            "baseline_accuracy": "Random-guess baseline ~10.5%; performance only marginally above random.",
            "factors_reported": [
                "Low program executability (~40% runnable) limiting PoT effectiveness.",
                "Instruction-tuning corpus and model scale insufficient to capture theorem knowledge.",
                "Diagram/image captioning information loss affects multimodal subset."
            ],
            "experimental_conditions": "PoT generated programs were tested for executability; same answer normalization pipeline.",
            "limitations_or_failure_modes": "Poor program quality and low runnable program fraction; accuracy close to random; multimodal performance poor.",
            "uuid": "e7542.6",
            "source_info": {
                "paper_title": "TheoremQA: A Theorem-driven Question Answering dataset",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "InstructCodeT5+ (CodeT5+)",
            "name_full": "InstructCodeT5+ (instruction-tuned CodeT5+)",
            "brief_description": "A code-oriented open-source model (CodeT5+ based, instruction-tuned) evaluated with PoT/CoT on theorem-driven numeric problems; low overall accuracy and program executability.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "InstructCodeT5+",
            "model_size": "16B",
            "model_type": "instruction-tuned code model (open-source)",
            "scientific_domain": "STEM (Mathematics, Physics, CS & EE, Finance)",
            "simulation_task_description": "Use program-generation (PoT) and CoT to solve theorem-driven numerical tasks; intended to leverage code capabilities for numeric execution.",
            "prompting_strategy": "PoT and CoT prompting.",
            "evaluation_metric": "Accuracy (exact-match normalized numeric answers); program executability measured.",
            "reported_accuracy": "CoT: ~11.6% overall (in table PoT: 11.6% for InstructCodeT5+), performance near random.",
            "baseline_accuracy": "Random-guess baseline ~10.5%; similar open-source models performed in 10–14% range.",
            "factors_reported": [
                "Program generation quality low; only a minority of generated programs runnable.",
                "Open-source pretraining and instruction-tuning insufficient to embed theorem knowledge.",
                "PoT provides limited benefit when generated programs are not executable."
            ],
            "experimental_conditions": "Programs generated under PoT were executed externally to obtain numeric outputs; same answer extraction and normalization.",
            "limitations_or_failure_modes": "Low runnable program fraction, low accuracy close to random, errors mainly due to lack of theorem knowledge and poor program generation.",
            "uuid": "e7542.7",
            "source_info": {
                "paper_title": "TheoremQA: A Theorem-driven Question Answering dataset",
                "publication_date_yy_mm": "2023-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks",
            "rating": 2
        },
        {
            "paper_title": "Chain-of-thought prompting elicits reasoning in large language models",
            "rating": 2
        },
        {
            "paper_title": "PAL: Program-aided Language Models",
            "rating": 2
        },
        {
            "paper_title": "GPT-4 technical report",
            "rating": 2
        },
        {
            "paper_title": "Bootstrapping Language-Image Pre-training (BLIP)",
            "rating": 1
        },
        {
            "paper_title": "Visual instruction tuning",
            "rating": 1
        }
    ],
    "cost": 0.015877,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>TheoremQA: A Theorem-driven Question Answering Dataset</h1>
<p>${ }^{\bullet}$ Wenhu Chen, ${ }^{*}$ Ming Yin, ${ }^{\bullet}$ Max Ku, ${ }^{\bullet}$ Pan Lu, ${ }^{\bullet}$ Yixin Wan,<br>${ }^{\bullet}$ Xueguang Ma, ${ }^{\text {P }}$,Jianyu Xu, ${ }^{\text {P }}$ Xinyi Wang, ${ }^{\text {T }}$ Tony Xia<br>University of Waterloo, Canada ${ }^{\bullet}$<br>University of California, Santa Barbara, United States ${ }^{\bullet}$<br>University of California, Los Angeles, United States ${ }^{\bullet}$</p>
<h4>Abstract</h4>
<p>The recent LLMs like GPT-4 and PaLM-2 have made tremendous progress in solving fundamental math problems like GSM8K by achieving over $90 \%$ accuracy. However, their capabilities to solve more challenging math problems which require domain-specific knowledge (i.e. theorem) have yet to be investigated. In this paper, we introduce TheoremQA, the first theorem-driven question-answering dataset designed to evaluate AI models' capabilities to apply theorems to solve challenging science problems. TheoremQA is curated by domain experts containing 800 high-quality questions covering 350 theorems ${ }^{1}$ from Math, Physics, EE\&amp;CS, and Finance. We evaluate a wide spectrum of 16 large language and code models with different prompting strategies like Chain-ofThoughts and Program-of-Thoughts. We found that GPT-4's capabilities to solve these problems are unparalleled, achieving an accuracy of $51 \%$ with Program-of-Thoughts Prompting. All the existing open-sourced models are below $15 \%$, barely surpassing the random-guess baseline. Given the diversity and broad coverage of TheoremQA, we believe it can be used as a better benchmark to evaluate LLMs' capabilities to solve challenging science problems.</p>
<h2>1 Introduction</h2>
<p>A long-standing goal of AI systems is to help human beings solve challenging problems, especially more domain-specific problems. To benchmark the progress towards this goal, researchers propose to evaluate AI systems' performance on different math word problem (WMP) datasets. In recent years, there has been a plethora of WMP datasets (Lu et al., 2023c), which we include in Table 1. Most of these datasets are meant for fundamental questions aimed at Grade 1-12 students on</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>a narrow subject. On the other hand, these datasets do not involve much domain-specific knowledge, aka theorem. Due to these two deficiencies, we believe that these datasets are not ideal to benchmark the existing powerful LLMs (Brown et al., 2020; Tamkin et al., 2022; Chen et al., 2021b; Chowdhery et al., 2022; Hoffmann et al., 2022; Taylor et al., 2022) due to their simplicity. In fact, on the popular GSM8K dataset (Cobbe et al., 2021), GPT4 (OpenAI, 2023) and PaLM-2 (Google, 2023) both already achieved $92 \%$ accuracy. Similarly, we tested GPT-4 (OpenAI, 2023) on the subsets of several other listed datasets in Table 1 and observed $90+\%$ accuracy in most cases. The only exception is MATH (Hendrycks et al., 2021) containing highschool math competition problems with SoTA performance around 50\% (Zheng et al., 2023). However, MATH (Hendrycks et al., 2021) is focused on math skills rather than theorem.</p>
<p>In this paper, we propose the first theoremdriven QA dataset built on university-level theorems across Math, Physics, EE\&amp;CS, and Finance. The whole collection process takes two steps: (1) we first enumerate roughly 400 theorems in different subfields like algebra, number theory, graph theory, information theory, etc, (2) we ask domain experts to search for questions regarding these theorems from different sources like Internet and Textbooks. The domain experts will adjust these questions to ensure the answers follow the desired format for the ease of automatic evaluation. Through the careful construction process, we collected 800 high-quality question-theorem-answer triples as our final release version.</p>
<p>We evaluate a wide spectrum of instructionfinetuned language and code models including GPT (Brown et al., 2020), Claude (Bai et al., 2022), LLaMA (Touvron et al., 2023), Pythia (Biderman et al., 2023), CodeGen (Nijkamp et al., 2022), GLM (Zeng et al., 2022), StarCoder (Li et al., 2023), and CodeT5+ (Wang et al., 2023)</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: The overview of TheoremQA and the prompting strategies adopted.</p>
<table>
<thead>
<tr>
<th>Dataset</th>
<th>Domain</th>
<th>Level</th>
<th>Source</th>
<th>Theorem</th>
</tr>
</thead>
<tbody>
<tr>
<td>DRAW (Upadhyay and Chang, 2015)</td>
<td>Algebra</td>
<td>Elementary School</td>
<td>Generated</td>
<td>-</td>
</tr>
<tr>
<td>MAWPS (Koncel-Kedziorski et al., 2016)</td>
<td>Arithmetic</td>
<td>Elementary School</td>
<td>Generated</td>
<td>-</td>
</tr>
<tr>
<td>DRAW1K (Upadhyay and Chang, 2017)</td>
<td>Algebra</td>
<td>Elementary School</td>
<td>Generated</td>
<td>-</td>
</tr>
<tr>
<td>ASDiv (Miao et al., 2020)</td>
<td>Arithm/Algebra</td>
<td>Elementary School</td>
<td>Internet</td>
<td>-</td>
</tr>
<tr>
<td>SVAMP (Patel et al., 2021a)</td>
<td>Arithm/Algebra</td>
<td>Elementary School</td>
<td>ASDiv</td>
<td>-</td>
</tr>
<tr>
<td>Math23K (Wang et al., 2017)</td>
<td>Algebra</td>
<td>Elementary School</td>
<td>Internet</td>
<td>-</td>
</tr>
<tr>
<td>TabMWP (Lu et al., 2023b)</td>
<td>Arithm/Algebra</td>
<td>Elem./Middle School</td>
<td>Textbooks</td>
<td>NO</td>
</tr>
<tr>
<td>GSM8K (Cobbe et al., 2021)</td>
<td>Arithm/Algebra</td>
<td>Middle School</td>
<td>Annotated</td>
<td>NO</td>
</tr>
<tr>
<td>GEOS (Seo et al., 2015)</td>
<td>Geometry</td>
<td>Middle School</td>
<td>SAT</td>
<td>NO</td>
</tr>
<tr>
<td>Geometry3K (Lu et al., 2021a)</td>
<td>Geometry</td>
<td>Middle/High School</td>
<td>Textbooks</td>
<td>NO</td>
</tr>
<tr>
<td>GeoQA (Chen et al., 2021a)</td>
<td>Geometry</td>
<td>Middle/High School</td>
<td>Exam</td>
<td>NO</td>
</tr>
<tr>
<td>UniGeo (Chen et al., 2022a)</td>
<td>Geometry</td>
<td>Middle/High School</td>
<td>Textbooks</td>
<td>NO</td>
</tr>
<tr>
<td>ScienceQA (Lu et al., 2022)</td>
<td>Science</td>
<td>Middle/High School</td>
<td>Textbooks</td>
<td>NO</td>
</tr>
<tr>
<td>MATH (Hendrycks et al., 2021)</td>
<td>Math</td>
<td>High School</td>
<td>Competition</td>
<td>YES</td>
</tr>
<tr>
<td>AQuA (Ling et al., 2017)</td>
<td>Arithm/Algebra</td>
<td>University</td>
<td>GMAT/GRE</td>
<td>NO</td>
</tr>
<tr>
<td>MathQA (Amini et al., 2019)</td>
<td>Arithm/Algebra</td>
<td>University</td>
<td>AQuA</td>
<td>NO</td>
</tr>
<tr>
<td>MathQA-Python (Austin et al., 2021)</td>
<td>Arithm/Algebra</td>
<td>University</td>
<td>AQuA</td>
<td>NO</td>
</tr>
<tr>
<td>FinQA (Chen et al., 2021c)</td>
<td>Finance</td>
<td>University</td>
<td>CrowdSource</td>
<td>NO</td>
</tr>
<tr>
<td>TAT-QA (Zhu et al., 2021)</td>
<td>Finance</td>
<td>University</td>
<td>CrowdSource</td>
<td>NO</td>
</tr>
<tr>
<td>TheoremQA (Ours)</td>
<td>STEM</td>
<td>University</td>
<td>Internet+Expert</td>
<td>350+</td>
</tr>
</tbody>
</table>
<p>Table 1: List of existing Math and STEM QA datasets.</p>
<p>on our dataset. We adopt two prompting methods: Chain-of-Thoughts (CoT) (Wei et al., 2022b) and Program-of-Thoughts (PoT) (Chen et al., 2022b) to prompt the large language models. We also investigate how to infuse the theorem into the thought process of LLMs and how to present the multimodal inputs to the LLMs.</p>
<p>In the course of our experiments, several notable observations were made. First, GPT-4 (OpenAI, 2023) significantly outperformed all existing models, reaching an accuracy level of 51% when combined with Program-of-Thoughts prompting. Trailing behind GPT-4, the second most effective model was ChatGPT, achieving an accuracy of 35% through the same prompting method. Additionally, our human evaluation determined that half of GPT-4's errors are caused by minor mistakes like calculation errors, rounding errors, etc. We believe these errors could be easily rectified with a more deliberate prompting strategy or human intervention. This suggests that there is still significant headroom for GPT-4 to achieve with more deliberate prompting strategies. Secondly, we found that all open-source, instruction-tuned language and code models scored below 15% in accuracy, barely exceeding the random guess baseline of 10%. Our human evaluation reveals that open-source models like Alpaca are making errors mainly due to their ignorance of the theorem, where 90% of the errors are not rectifiable. This stark gap between GPT and open-source models suggests that further enhancement strategies, such as science-focused pre-training or fine-tuning, should be considered to narrow the performance disparity. Thirdly, we explored the potential to do theorem-augmented generation. However, the simple strategy of concatenation did not yield a significant improvement. We conjecture that a more complex integration strategy may be needed to achieve more gains. Lastly, we examined the performance of various multi-modal instruction-tuned models on the multimodal subset of the TheoremQA dataset. Surprisingly, these models did not demonstrate significant performance gains over their text-only counterparts. This is mainly due to the unnaturalness of the image, which consists of lots of diagrams and text. Such images are not well</p>
<p>captured by existing visual encoder models.
To sum up, our contributions are three folds:</p>
<ul>
<li>We propose the first theorem-driven questionanswering dataset to understand LLMs’ capabilities to apply science theorems.</li>
<li>We comprehensively evaluate a wide spectrum of 16 LLMs on TheoremQA.</li>
<li>We perform different analyses in the theorem integration and multimodal understanding aspects to provide detailed insights.</li>
</ul>
<h2>2 Related Work</h2>
<h3>2.1 Math Word Problems</h3>
<p>Mathematical reasoning skills are crucial for general-purpose intelligent systems, garnering significant interest from the research community. In the past, studies have explored the ability of NLP models to solve arithmetic and algebraic problems (Hosseini et al., 2014; Koncel-Kedziorski et al., 2015; Roy and Roth, 2015; Ling et al., 2017). More recently, researchers have introduced increasingly challenging datasets (Saxton et al., 2019; Miao et al., 2020; Amini et al., 2019; Hendrycks et al., 2021; Lu et al., 2021b; Patel et al., 2021b) aimed at enhancing difficulty, diversity, and adversarial robustness. LiLA (Mishra et al., 2022) proposes to assemble a vast collection of mathematical datasets into a single, unified dataset. LiLA also annotates Python programs as target outputs for solving mathematical problems. However, the existing datasets were mostly focused on grade school simple mathematics. To further investigate the LLMs’ capabilities to assist humans to solve challenging math problems, we propose TheoremQA as the first benchmark to enable research in this direction.</p>
<h3>2.2 Large Language Models</h3>
<p>In recent years, there has been a surge of research and development in the area of large language models (LLMs) that have significantly advanced the field of natural language processing. GPT3 (Brown et al., 2020) demonstrated a strong capability to perform few-shot predictions, where the model is given a description of the task in natural language with few examples. By using human-feedback reinforcement learning, InstructGPT (Ouyang et al., 2022) has shown its unprecedented capabilities to follow human instructions.</p>
<p>Scaling model size, data, and computing are crucial to enable this learning ability. Later, Rae et al. (2021); Chowdhery et al. (2022); Zhang et al. (2022); Touvron et al. (2023); Chen et al. (2021b) have proposed to train different types of LLMs with different training recipes. The capability to follow few-shot exemplars to solve unseen tasks is not existent on smaller LMs, but only emerges as the model scales up (Wei et al., 2022a). More recently, GPT-4 (OpenAI, 2023) shows tremendous progress on lots of complex reasoning tasks spanning mathematics, coding, vision, medicine, law, psychology, and more. Bubeck et al. (2023) shows that GPT-4 is already demonstrating more general intelligence than previous AI models. To further validate GPT4's capability to solve challenging reasoning tasks, we propose TheoremQA as the new benchmark to further understand LLMs' upper limit.</p>
<h3>2.3 Reasoning with Large Language Model</h3>
<p>To better unleash large language models' capabilities to solve complex reasoning tasks. Chain-ofThought Prompting (Wei et al., 2022b; Kojima et al., 2022; Wang et al., 2022) was proposed, which aims at prompting the large language models to generate the 'thought process' before outputting the answer. Later on, several other works (Drozdov et al., 2022; Zhou et al., 2022; Nye et al., 2021) also propose different approaches to utilize LLMs to solve reasoning tasks by allowing intermediate steps. Our method can be seen as an extension to CoT by leveraging an extra step of symbolic execution. Another line of work (Gao et al., 2022; Chen et al., 2022b) was proposed to adopt Python programs as the demonstration for the 'thought process' to solve different reasoning tasks.</p>
<h2>3 Dataset</h2>
<p>Our dataset collection pipeline contains two steps:
Theorem Enumeration Our aim was to encompass a wide range of theorems. To this end, we began by prompting Large Language Models (LLMs), specifically GPT-4 (OpenAI, 2023), to enumerate popular subfields in Mathematics, Physics, Finance, and Electrical Engineering \&amp; Computer Science. The covered subfields are listed in Figure 4. Subsequently, we prompted GPT-4 to propose plausible university-level theorems relevant to these subfields. For instance, within the 'Calculus' subfield, GPT-4 might suggest the 'Intermediate Value Theorem', 'Rolle's Theorem', and so on. After gath-</p>
<p>ering an extensive list of theorems, we assembled a team of domain experts (holders of Masters and PhDs in Statistics, Electrical Engineering, Computer Science, and Finance) to refine the theorem inventory and supplement any omitted theorems. Ultimately, we collected approximately 400 theorems, encapsulating a diverse range of topics within these fields. We then delegated these theorems to nine domain experts, instructing them to locate question/answer pairs from varied sources. During the annotation process, a small number of theorems were discarded due to their evaluation complexity.</p>
<p>Question Annotation Our problems were sourced from websites, books, or devised by the experts themselves. One challenge we encountered was the potential for questions found online to have been included in the training data. To mitigate this 'data contamination' issue, we encouraged domain experts to modify these questions. Another challenge arose from questions with answers in symbolic form, matrix form, figure form, etc. These presented significant obstacles for our automatic evaluation. To overcome this, we instructed domain experts to alter the question so the answer would be limited to the following forms: (1) integer, (2) float, (3) list of integers/flots, (4) boolean, and (5) multiple-choice options. For instance, if the original question concerned a matrix, we would revise it to ask about the trace of the answer matrix. This modification significantly streamlined the evaluation process. An example of this can be found in Figure 2.</p>
<p>Dataset Statistics Finally, we collected a total of 800 questions over 354 theorems. Specifically, there are 199 Math theorems, 52 Physics theorems, 55 Finance theorems, and 48 CS&amp;EE theorems. There are 442 Math questions, 146 CS&amp;EE questions, 131 physics questions, and 81 Finance questions. We show the answer-type distribution in Figure 3. To further enhance the multimodality aspect of TheoremQA, we also include 51 questions with image input (diagrams), where the model needs to understand the visual input to answer questions.</p>
<p>The majority of the questions in TheoremQA have float and integer as the answers, which is more realistic than the existing multi-choice datasets like ScienceQA (Lu et al., 2022) or AQuA QA (Ling et al., 2017). Therefore, the models are unlikely to take shortcuts to achieve high accuracy.</p>
<p>Question: Please use the Stoke's theorem to evaluate $\iint_{S} \operatorname{curl} \vec{F} \cdot d\vec{r}$ where $\vec{F}=z^{2} \vec{i}-3 x y \vec{j}+x^{2} y^{2} \vec{k}$ and $S$ is the part of $z=5-x^{2}-y^{2}$ above the plane $z=1$. S is oriented upwards.</p>
<p>Stoke's Theorem: Let S be an oriented smooth surface that is bounded by a simple, closed, smooth boundary curve C with positive orientation. Also let $\vec{F}$ be a vector. We can compute the integral as follows:</p>
<p>$$
\int_{C} \vec{F} \cdot d \vec{r}=\iint_{S} \operatorname{curl} \vec{F} \cdot d \vec{S}
$$</p>
<p>Answer: 0, Type: Float
Quesiton: Let $\mathrm{W}(\mathrm{t})$ be the standard Brownian motion. Find the probability of $P(W(1)+W(2)&gt;2)$.</p>
<p>Winer's Process The Wiener process $W_{t}$ is characterised by the following properties: $W$ has independent increment. For every $t&gt;0$, the future increment $W_{t+n}-W_{t}$ are independent from the past $W_{t} \quad \mathrm{~W}$ has Gaussian increments, $W_{t+n}-W_{t}$ has Gaussian distribution $\mathcal{N}(0, u)$.</p>
<p>Answer: 0.186, Type: Float
Figure 2: Examples from TheoremQA. The first question requires the usage of Stoke's theorem to transform the double integral into a line integral. The second question requires knowing the properties of Wiener's process.</p>
<p>Human-Level Performance To provide a rough but informative estimate of human-level performance. We randomly select 20 questions and assign these questions to the 4 Math\&amp;CS undergraduate students (average GPA) who have taken the required courses regarding these questions. The participants are given 24 hours with internet access to solve these questions. The four undergraduate students achieve 12/20, 15/20, 18/20, and 19/20 scores on these randomly sampled questions. From this experiment, we are more confident that an expert-level performance should be $100 \%$.</p>
<h2>4 Method</h2>
<p>Our method for addressing these demanding questions in the TheoremQA is comprised of several distinct modules, as outlined in Figure 1:</p>
<p>Prompting We utilize two established prompting strategies:</p>
<ul>
<li>Chain-of-Thought Prompting (Wei et al.,</li>
</ul>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 3: Answer type distribution in TheoremQA.</p>
<p>2022b): This strategy prompts the language model to initially generate a step-by-step thought process, eventually leading to the final answer.</p>
<ul>
<li>Program-of-Thought Prompting (Chen et al., 2022b; Gao et al., 2022): This strategy prompts the language model to progressively generate a program. The final answer is then derived by executing this program.</li>
</ul>
<p>By delegating computational tasks to an external executor, the problem-solving process is considerably enhanced in its reliability. This improvement results in remarkable advancements in existing math datasets being reported in (Chen et al., 2022b).</p>
<p>Answer Extraction We observed that parsing the output from Large Language Models (LLMs) can be challenging due to two main issues: (1) The answer is often embedded within a sentence, making it difficult to extract using regular expressions, and (2) The answer may not be normalized, such as 'pi / 3' or ' $2 * 10-\mathrm{e}$ ', which complicates comparison with the ground truth. To tackle these problems, we initially employ ChatGPT to identify the answer span within the model's output, then forward this string to WolframAlpha (Inc.) for normalization into a float, integer, or list.</p>
<p>Theorem Augmentation We explored the potential of enhancing large language models with retrieved theorem descriptions to assess their effect on performance. One approach is to retrieve descriptions of the given theorems from the Internet to supplement the LLMs' output. Another experiment involved prompting GPT-4 to generate text descriptions of the theorem, which are then used as an additional augmentation signal.</p>
<p>Multimodal Input A small portion of our data (50 instances) includes images, such as diagrams, as supplemental input, particularly in geometry questions. Since current LLMs don't support such multimodal inputs, we propose a solution: to employ captions like Chameleon (Lu et al., 2023a). These captions describe the image and are then appended to the LLMs' output as an additional signal.</p>
<h2>5 Experiments</h2>
<h3>5.1 Model Descriptions</h3>
<p>In our experiments, we mainly investigate the following models:</p>
<ul>
<li>GPT3/3.5/ChatGPT/GPT4: These are instruction-tuned models from OpenAI ${ }^{2}$.</li>
<li>PaLM-2: This is the instruction-tuned model from Google (Google, 2023).</li>
<li>Claude-v1/Claude-instant: These are instruction-tuned models from AnthropicAI ${ }^{3}$.</li>
<li>Alpaca-13B: This model is based on the LLaMA (Touvron et al., 2023). Alapaca is instruction-tuned by the 52 K data generated from GPT-4.</li>
<li>Vicuna-13B: This model is based on the LLaMA (Touvron et al., 2023). Vicuna is instruction-tuned by the 100K ShareGPT data generated by different GPT-based models.</li>
<li>OpenAssistant-12B: This model is based on Pythia (Biderman et al., 2023). The model is instruction-tuned by OpenAssistant data ${ }^{4}$.</li>
<li>MOSS-instruct-16B: This model is based on CodeGen (Nijkamp et al., 2022), which is further instruction-tuned with instruction following dataset distilled from GPT. ${ }^{5}$.</li>
<li>StarChat-16B: This model is based on StarCoder (Li et al., 2023). StartChat is being instruction-tuned on OpenAssistant data ${ }^{6}$ and ShareGPT data.</li>
</ul>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: center;">Mathematics</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Physics</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Finance</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Calculus</td>
<td style="text-align: center;">103</td>
<td style="text-align: center;">Kinetics</td>
<td style="text-align: center;">30</td>
<td style="text-align: center;">Economics</td>
<td style="text-align: center;">22</td>
</tr>
<tr>
<td style="text-align: center;">Combinatorics</td>
<td style="text-align: center;">57</td>
<td style="text-align: center;">Electromagnetism</td>
<td style="text-align: center;">21</td>
<td style="text-align: center;">Quantitive methods</td>
<td style="text-align: center;">14</td>
</tr>
<tr>
<td style="text-align: center;">Algebra</td>
<td style="text-align: center;">52</td>
<td style="text-align: center;">Atomic physics</td>
<td style="text-align: center;">11</td>
<td style="text-align: center;">Derivatives</td>
<td style="text-align: center;">14</td>
</tr>
<tr>
<td style="text-align: center;">Mathematical analysis</td>
<td style="text-align: center;">42</td>
<td style="text-align: center;">Wave</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">Fixed income</td>
<td style="text-align: center;">11</td>
</tr>
<tr>
<td style="text-align: center;">Number theory</td>
<td style="text-align: center;">29</td>
<td style="text-align: center;">Optics</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">management</td>
<td style="text-align: center;">10</td>
</tr>
<tr>
<td style="text-align: center;">Geometry</td>
<td style="text-align: center;">27</td>
<td style="text-align: center;">Condensed matter</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">Investments</td>
<td style="text-align: center;">10</td>
</tr>
<tr>
<td style="text-align: center;">Numerical analysis</td>
<td style="text-align: center;">24</td>
<td style="text-align: center;">Particle</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">Total</td>
<td style="text-align: center;">81</td>
</tr>
<tr>
<td style="text-align: center;">Statistics</td>
<td style="text-align: center;">24</td>
<td style="text-align: center;">Statistical physics</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Complex analysis</td>
<td style="text-align: center;">21</td>
<td style="text-align: center;">Relativity</td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">CS \&amp; EE</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Probability theory</td>
<td style="text-align: center;">19</td>
<td style="text-align: center;">Celestial mechanics</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">Signal processing</td>
<td style="text-align: center;">47</td>
</tr>
<tr>
<td style="text-align: center;">Stochastic process</td>
<td style="text-align: center;">16</td>
<td style="text-align: center;">Thermodynamics</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">Graph theory</td>
<td style="text-align: center;">34</td>
</tr>
<tr>
<td style="text-align: center;">Group theory</td>
<td style="text-align: center;">11</td>
<td style="text-align: center;">Quantum</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">Information theory</td>
<td style="text-align: center;">29</td>
</tr>
<tr>
<td style="text-align: center;">Functional analysis</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">Classic mechanics</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">Computer networking</td>
<td style="text-align: center;">23</td>
</tr>
<tr>
<td style="text-align: center;">Real analysis</td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">Fluid mechanics</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">Machine learning</td>
<td style="text-align: center;">13</td>
</tr>
<tr>
<td style="text-align: center;">Total</td>
<td style="text-align: center;">442</td>
<td style="text-align: center;">Total</td>
<td style="text-align: center;">131</td>
<td style="text-align: center;">Total</td>
<td style="text-align: center;">146</td>
</tr>
</tbody>
</table>
<p>Figure 4: Subfields of TheoremQA under Math, Physics, Engineering, and Finance.</p>
<ul>
<li>InstructCodeT5+: This model is based on CodeT5+ (Wang et al., 2023). InstructCodeT5+ is further instruction-tuned on Code Alpaca data ${ }^{7}$ to follow instructions.</li>
</ul>
<h3>5.2 Main Results</h3>
<p>We demonstrate our main results on Table 2. We will summarize different findings in the following:</p>
<p>Closed-source Models For GPT-3 (text-davinci002) and GPT-3.5 model, since these two models are not Chat-based models, we need to demonstrate one example ensure to help them generate outputs of the desired format. With CoT prompting, GPT-3 (text-davinci-002) and GPT-3.5 models are only achieving $16.6 \%$ and $22.8 \%$ accuracy. By adopting the program as the intermediate reasoning form, both models can gain reasonable improvements. For Claude-v1, we found that it is matching the performance of GPT-3.5. ChatGPT outperforms GPT-3.5 and Claude-v1 significantly by $8 \%$, which indicates ChatGPT's capabilities to perform complex numerical reasoning. GPT-4 is the strongest model being evaluated, which beats all the rest models by a huge margin. With Chain-of-Thoughts prompting, GPT-4 can outperform ChatGPT by $13 \%$. With Program-of-Thoughts prompting, GPT4 can outperform ChatGPT by $16 \%$. Though some other models have shown to match GPT-4 on simple tasks, GPT-4's capability to solve challenging tasks seems unparalleled.</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Open-source Models For the open-source models, we found that their performance is much behind. To better understand their accuracy, we also provide the random-guess baseline of $10 \%$. We test both prompting strategies, however, their results consistently lie in the range of 10-14\%. The results indicate that these open-source LMs are still struggling with more complex mathematical reasoning tasks in TheoremQA. Given that ChatGPT of a similar size is able to achieve much higher performance, we believe the parameter size is not the only cause. There is still a significant amount of effort during pre-training or supervised fine-tuning to instill enough scientific knowledge into the models' parameters to close the gap.</p>
<p>Program of Thoughts Analysis From Table 2, we observe that PoT brings consistent improvement over CoT on GPT-<em> models. Different GPT-</em> models can normally yield a gain of 5-8\% accuracy. In contrast, Claude-v1 and StarChat are almost obtaining the same accuracy. To better analyze where the gains are coming from, we plot Figure 5 to understand how many of generated Python programs are actually 'executable'. As can be seen, both StarChat and CodeT5+ are having trouble generating 'runnable' programs with only $40 \%$ programs being executable. Claude-v1 is able to increase the validity of the generated programs to $60 \%$. In contrast, GPT3.5 and ChatGPT can further increase the ratio to around $80 \%$. GPT-4 is extremely accurate in generating programs, where $92 \%$ of the generated programs are runnable. Such a high executable ratio explains why the gain brought to GPT-* model</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 5: Ratio of Executable Python Program of different models with PoT prompting.</p>
<p>is much higher than Claude-v1 and StarChat.</p>
<h3>5.3 Additional Result</h3>
<p><strong>Theorem Augmentation</strong> We also investigate whether feeding theorem as an additional text condition would help the model better solve the problem. Specifically, we ask GPT-4 to generate a paragraph to describe the theorem, which we post-processed to ensure correctness. We feed the theorem in the prompt to different language models to see the performance change and plot our findings in Table 3. For all the evaluated scenarios, we found that the improvement is limited to within 1%. Unlike the Text or KB knowledge, theorem knowledge is more abstract and symbolic, simply concatenating the theorem definition is not enough. We believe a more sophisticated augmentation scheme is needed to truly help the model understand and apply the theorems to solve problems.</p>
<p><strong>Multimodal Questions</strong> Our aim was to assess how effectively the current method could tackle multimodal questions (those with image inputs) in the TheoremQA dataset. An example is illustrated in Figure 6, where an image is converted into 'captions' by BLIP (Li et al., 2022). We graphed the results from over 50 multimodal question subsets in Figure 7. Notably, this subset posed substantial challenges; none of the models were able to achieve an accuracy rate of 10%. This is primarily due to information loss during the captioning process.</p>
<p>In light of this, we conducted further evaluations on two multimodal instruction-tuned models, LLaVA-13B (Liu et al., 2023) and VisualGLM-6B (Zeng et al., 2022) [8]. These models utilize a visual encoder (either CLIP (Radford et al., 2021) or BLIP (Li et al., 2022)) to encode image input, which is then integrated with language models for multimodal conversation. However, these models demonstrated performance similar to their text-only</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Question: Are the circuits shown in Fig. Q1a and Fig. Q1b are identical in terms of the Tranfer functions. BLIP Caption: a diagram of a block diagram with a block diagram and a block diagram.</p>
<p>Figure 6: An example of Multimodal question.</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 7: Accuracy on the Multimodal Question Subset</p>
<p>equivalent, Alpaca, with the addition of a visual encoder not significantly enhancing the results. We hypothesize that the current visual encoding modules may not be suited for representing these diagrammatic images, resulting in these less than ideal outcomes. We believe these multimodal questions remain a challenge for the research community, and we eagerly anticipate further advancements in addressing these multimodal scientific questions.</p>
<p><strong>Error Analysis</strong> We conduct detailed error analysis on 200 erroneous cases from different models to analyze their error distribution. Specifically, we pick GPT4, ChatGPT and Alpaca to understand their error sources. We include the following error types: (E1) the model does not even know this theorem, (E2) the model does know the theorem, but uses the wrong formula or algorithm, (E3) the model knows the theorem and the formula, the error is only caused by minor calculation mistakes. The severity of the error decrease as the error number increases. We plot our findings in Figure 8, where the bar indicate the percentage of specific error types. We can observe that almost half of the errors made by GPT4 are non-critical with caused by minor calculation mistakes. This error analysis suggests that there is a still a significant headroom for GPT4 to improve with more deliberate prompting strategies or human intervention to mitigate these minor errors. In contrast, Alpaca's errors are mainly caused by not knowing the theorem at all.</p>
<p><sup>8</sup>https://github.com/THUDM/VisualGLM-6B</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>Integer</th>
<th>Float</th>
<th>Option</th>
<th>List</th>
<th>Bool</th>
<th>Math</th>
<th>CS&amp;EE</th>
<th>Physics</th>
<th>Finance</th>
<th>All</th>
</tr>
</thead>
<tbody>
<tr>
<td>Random Guess</td>
<td>0</td>
<td>0</td>
<td>38.9</td>
<td>0</td>
<td>65.5</td>
<td>10.0</td>
<td>24.7</td>
<td>0</td>
<td>4.9</td>
<td>10.5</td>
</tr>
<tr>
<td>Chain of Thoughts (CoT)</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>GPT-3</td>
<td>11.6</td>
<td>11.7</td>
<td>27.8</td>
<td>6.8</td>
<td>46.6</td>
<td>15.8</td>
<td>34.2</td>
<td>2.3</td>
<td>12.3</td>
<td>16.6</td>
</tr>
<tr>
<td>GPT-3.5</td>
<td>13.0</td>
<td>14.3</td>
<td>50.0</td>
<td>13.7</td>
<td>69.8</td>
<td>22.6</td>
<td>36.3</td>
<td>7.6</td>
<td>23.5</td>
<td>22.8</td>
</tr>
<tr>
<td>ChatGPT</td>
<td>32.4</td>
<td>22.3</td>
<td>50.0</td>
<td>20.5</td>
<td>55.2</td>
<td>31.0</td>
<td>41.1</td>
<td>16.8</td>
<td>28.4</td>
<td>30.2</td>
</tr>
<tr>
<td>GPT-4</td>
<td>40.3</td>
<td>36.7</td>
<td>66.7</td>
<td>37.0</td>
<td>74.6</td>
<td>43.9</td>
<td>50.6</td>
<td>30.5</td>
<td>51.4</td>
<td>43.8</td>
</tr>
<tr>
<td>PaLM-2</td>
<td>26.4</td>
<td>22.8</td>
<td>61.1</td>
<td>23.3</td>
<td>71.6</td>
<td>31.0</td>
<td>47.3</td>
<td>19.8</td>
<td>27.2</td>
<td>31.8</td>
</tr>
<tr>
<td>Claude-v1</td>
<td>18.1</td>
<td>19.4</td>
<td>27.8</td>
<td>15.1</td>
<td>61.2</td>
<td>21.7</td>
<td>42.5</td>
<td>13.7</td>
<td>28.4</td>
<td>24.9</td>
</tr>
<tr>
<td>Claude-instant</td>
<td>19.9</td>
<td>16.7</td>
<td>44.4</td>
<td>17.8</td>
<td>53.4</td>
<td>21.5</td>
<td>36.3</td>
<td>14.5</td>
<td>27.2</td>
<td>23.6</td>
</tr>
<tr>
<td>Alpaca (13B)</td>
<td>11.1</td>
<td>6.9</td>
<td>27.8</td>
<td>2.7</td>
<td>45.7</td>
<td>12.9</td>
<td>27.4</td>
<td>3.8</td>
<td>9.9</td>
<td>13.5</td>
</tr>
<tr>
<td>Vicuna (13B)</td>
<td>8.8</td>
<td>6.9</td>
<td>16.7</td>
<td>2.7</td>
<td>45.7</td>
<td>12.2</td>
<td>24.0</td>
<td>3.1</td>
<td>12.3</td>
<td>12.9</td>
</tr>
<tr>
<td>OpenAssistant (12B)</td>
<td>8.3</td>
<td>5.0</td>
<td>22.2</td>
<td>1.4</td>
<td>37.9</td>
<td>10.2</td>
<td>25.0</td>
<td>0</td>
<td>4.9</td>
<td>10.7</td>
</tr>
<tr>
<td>MOSS (16B)</td>
<td>8.8</td>
<td>5.4</td>
<td>24.2</td>
<td>2.4</td>
<td>44.2</td>
<td>11.3</td>
<td>28.4</td>
<td>1.6</td>
<td>8.9</td>
<td>12.2</td>
</tr>
<tr>
<td>StarChat (16B)</td>
<td>7.9</td>
<td>4.9</td>
<td>22.3</td>
<td>1.9</td>
<td>44.1</td>
<td>10.7</td>
<td>23.5</td>
<td>0.6</td>
<td>6.8</td>
<td>11.6</td>
</tr>
<tr>
<td>Program of Thoughts (PoT)</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>GPT-3</td>
<td>17.1</td>
<td>15.9</td>
<td>22.2</td>
<td>9.6</td>
<td>49.1</td>
<td>23.3</td>
<td>25.4</td>
<td>8.4</td>
<td>17.3</td>
<td>20.6</td>
</tr>
<tr>
<td>GPT-3.5</td>
<td>23.6</td>
<td>19.9</td>
<td>50.0</td>
<td>21.9</td>
<td>61.2</td>
<td>26.7</td>
<td>41.1</td>
<td>14.5</td>
<td>30.9</td>
<td>27.8</td>
</tr>
<tr>
<td>ChatGPT</td>
<td>31.0</td>
<td>35.0</td>
<td>38.9</td>
<td>21.9</td>
<td>54.3</td>
<td>35.7</td>
<td>35.6</td>
<td>26.7</td>
<td>49.4</td>
<td>35.6</td>
</tr>
<tr>
<td>GPT-4</td>
<td>44.4</td>
<td>50.7</td>
<td>66.7</td>
<td>39.7</td>
<td>78.4</td>
<td>52.0</td>
<td>51.4</td>
<td>45.8</td>
<td>66.7</td>
<td>52.4</td>
</tr>
<tr>
<td>Claude-v1</td>
<td>17.1</td>
<td>21.8</td>
<td>33.3</td>
<td>6.9</td>
<td>62.5</td>
<td>23.1</td>
<td>37.5</td>
<td>17.1</td>
<td>28.4</td>
<td>25.9</td>
</tr>
<tr>
<td>StarChat (16B)</td>
<td>7.7</td>
<td>6.1</td>
<td>0.0</td>
<td>3.0</td>
<td>43.5</td>
<td>13.6</td>
<td>17.6</td>
<td>5.1</td>
<td>5.1</td>
<td>11.3</td>
</tr>
<tr>
<td>InstructCodeT5+ (16B)</td>
<td>8.9</td>
<td>6.3</td>
<td>0.0</td>
<td>6.9</td>
<td>45.2</td>
<td>13.8</td>
<td>17.9</td>
<td>4.2</td>
<td>5.1</td>
<td>11.6</td>
</tr>
</tbody>
</table>
<p>Table 2: Results for CoT and PoT prompting on TheoremQA. We report the accuracy over different fine-grained question types and scientific fields.</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>Method</th>
<th>Theorem</th>
<th>All</th>
</tr>
</thead>
<tbody>
<tr>
<td>ChatGPT</td>
<td>CoT</td>
<td>-</td>
<td>30.2</td>
</tr>
<tr>
<td>ChatGPT</td>
<td>CoT</td>
<td>+</td>
<td>30.8</td>
</tr>
<tr>
<td>Claude-v1</td>
<td>CoT</td>
<td>-</td>
<td>24.9</td>
</tr>
<tr>
<td>Claude-v1</td>
<td>CoT</td>
<td>+</td>
<td>25.4</td>
</tr>
<tr>
<td>ChatGPT</td>
<td>PoT</td>
<td>-</td>
<td>35.6</td>
</tr>
<tr>
<td>ChatGPT</td>
<td>PoT</td>
<td>+</td>
<td>35.8</td>
</tr>
<tr>
<td>Alpaca-13B</td>
<td>CoT</td>
<td>-</td>
<td>13.5</td>
</tr>
<tr>
<td>Alpaca-13B</td>
<td>CoT</td>
<td>+</td>
<td>14.2</td>
</tr>
</tbody>
</table>
<p>Table 3: Results for CoT and PoT prompting with additional theorem conditions.</p>
<table>
<thead>
<tr>
<th>园GPT4 园 ChatGPT 园 Alpaca</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>E1 E2 E3</td>
<td></td>
</tr>
</tbody>
</table>
<p>Case Study We list a few successful and failed examples generated by GPT-4 in Figure 9 to do a side-by-side comparison between chain-of-thoughts prompting and program-of-thoughts prompting. In the first example, the question is regarding 'orthogonal projection theorem'. As can be seen, Chain-of-Thoughts prompting requires a very long paragraph to generate the results. We prompted GPT-4 a few times with the same input and the results seem unstable. Sometimes the model will make tiny computation mistakes in the middle to derive the wrong answer. In contrast, the program solution is brief and concise, which leads to rather stable outputs. For the second ex-
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 8: Error Analysis for GPT4, ChatGPT and Alpaca. Error severity: E1 &gt; E2 &gt; E3
ample, the computation requires 'for loop' to iteratively compute delta values for Riemann Sum. We found that such problems are also more natural for programs to solve. Through these examples, we can see GPT-4's unprecedented capabilities to solve these difficult math problems even without any demonstration or hints.</p>
<p>We also show some examples in Figure 9 to compare the results of CoT and PoT prompting. We can see that the PoT can significantly shorten the output sequence length. By leveraging the additional tool, PoT is able to significantly lower the task difficulty.</p>
<p>Question: Find the orthogonal projection of $[9,0,0,0]$ onto the subspace of R^4 spanned by [2, $2,1,0]$ and $[-2,2,0,1]$.</p>
<p>The orthogonal projection of a vector onto a subspace $W$ spanned by a set of vectors $(v 1, v 2, \ldots, v n)$ is given by the sum of the projections onto each of these vectors.
In your case, the vector is $v=9 e_{-} 1=[9,0,0,0]$ and the subspace $W$ is spanned by $u=[2,2,1,0]$ and $w=[-2,2,0,1]$.
The projection of $v$ onto $u$ is given by:
$\operatorname{proj}<em -="-">{-} u(v)=\left((v-u) / | u |^{\wedge} 2\right)^{<em>} u$
where " $\leftarrow$ " denotes the dot product, and $| u |$ is the norm of $u$.
And the projection of $v$ onto $w$ is given by:
$\operatorname{proj}_{-} w(v)=\left((v-w) / | w |^{\wedge} 2\right)^{</em>} w$
The total projection onto the subspace $W$ is then given by the sum of these individual projections:
$\operatorname{proj}</em>} W(v)=\operatorname{proj<em -="-">{-} u(v)+\operatorname{proj}</em> w(v)$
Now let's compute these values:
The dot product $v \cdot u=\left(9^{<em>} 2\right)+\left(0^{</em>} 2\right)+\left(0^{<em>} 1\right)+\left(0^{</em>} 0\right)=18$
The norm $| u |^{\wedge} 2=\left(2^{\wedge} 2+2^{\wedge} 2+1^{\wedge} 2+0^{\wedge} 2\right)=9$
So, proj_u(v) $=((18) / 9) <em>[2,2,1,0]=[4,4,2,0]$
Similarly, the dot product $v \cdot w=\left(9^{</em>}-2\right)+\left(0^{<em>} 2\right)+\left(0^{</em>} 0\right)+\left(0^{<em>} 1\right)=-18$
The norm $| w |^{\wedge} 2=\left(-2^{\wedge} 2+2^{\wedge} 2+0^{\wedge} 2+1^{\wedge} 2\right)=9$
So, proj_w(v) $=((-18) / 9) </em>[-2,2,0,1]=[4,-4,0,-2]$
The total projection onto the subspace $W$ is then given by the sum of these individual projections:
$\operatorname{proj}<em -="-">{-} W(v)=\operatorname{proj}</em>} u(v)+\operatorname{proj<em -="-">{-} w(v)=\left[4,4,2,0\right]+\left[4,-4,0,-2\right]=[8,0,2,-2]$
So, the orthogonal projection of $9 \mathrm{e}</em> 4$ spanned by $[2,2,1,0]$ and $[-2,2,0,1]$ is $[8,0,2,-2]$.} 1$ onto the subspace of $\mathrm{R}^{\wedge</p>
<p>Chain of Thoughts
$\square$
def solve():
import numpy as np
# Define the given vectors
e1 = np.array([1, 0, 0, 0])
v1 = np.array([2, 2, 1, 0])
v2 = np.array([-2, 2, 0, 1])
# Calculate the orthogonal projection
$A=$ np.column_stack((v1, v2))
proj_matrix = A @ np.linalg.inv(A.T @ A) @ A.T proj_e1 = proj_matrix @ (9 * e1)
return proj_e1.tolist()
print(solve())
Program of Thoughts</p>
<p>Question: Let rectangle $R=[1,2.5] *[1,2]$. Calculate the Riemann Sum $S _(3,2)$ for $\operatorname{Vint} \operatorname{Vint}_{-}(R)$ xy dA for the integral, using the lower-left vertex of rectangles as sample points.</p>
<p>The given region $R$ is a rectangle from $[1,2.5]$ in $x$ and $[1,2]$ in y. We are asked to find the Riemann sum $S_{-}(3,2)$ which means we will partition the interval in $x$ into 3 equal subintervals and the interval in y into 2 equal subintervals.</p>
<p>The width of each subinterval in $x, \Delta x$, is $(2.5-1) / 3=0.5$. The height of each subinterval in $y, \Delta y$, is $(2-1) / 2=0.5$.
Now, we are sampling from the lower left corner of each subrectangle. This means we'll be evaluating our function $f(x, y)$ $=x y$ at the points $(x, y)$ where $x$ ranges over $(1,1.5,2)$, and $y$ ranges over $(1,1.5)$.</p>
<p>The Riemann sum is given by:
$S_{-}(3,2)=\Delta x^{<em>} \Delta y^{</em>} \Sigma f\left(x_{-} i, y_{-}\right)$
where the sum $\Sigma$ is over all pairs $\left(x_{-} i, y_{-}\right)$from the lower left vertices of the subrectangles.
$=0.5^{<em>} 0.5^{</em>}\left[\left(11+11.5+1.51+1.51 .5\right)+\left(1.51+1.51 .5+21+\right.\right.$ 21.5)]
$=0.25^{<em>}[3.5+5]$
$=0.25^{</em>} 8.5=2.125$
So, the Riemann sum $S_{-}(3,2)$ for the double integral over $R$ of $f(x, y)=x y$ with respect to area, using the lower-left vertex of each subrectangle as the sample point, is 2.125 .</p>
<p>Chain of Thoughts
def solve():
# Define the limits of integration
$x_{-} \min , x_{-} \max =1,2.5$
$y_{-} \min , y_{-} \max =1,2$
# Define the number of rectangles in each direction
num_rectangles_x = 3
num_rectangles_y = 2
# Calculate the width and height of each rectangle delta_x $=\left(x_{-} \max -x_{-} \min\right) /$ num_rectangles_x delta_y $=\left(y_{-} \max -y_{-} \min\right) /$ num_rectangles_y
# Initialize the Riemann Sum
riemann_sum $=0$
# Iterate through the rectangles
for $i$ in range(num_rectangles_x):
for j in range(num_rectangles_y):
# Calculate the lower-left vertex of the rectangle
$x=x_{-} \min +{ }^{\wedge}$ delta_ $x$
$y=y_{-} \min +{ }^{\wedge}$ delta_y
# Calculate the value of the function at
$f _x y=x^{*} y$
# Add the contribution to the Riemann Sum
riemann_sum $+=$ f_xy * delta_x * delta_y
return riemann_sum
Program of Thoughts</p>
<p>Figure 9: Case Study of GPT-4 generation with both prompting strategies.</p>
<h1>6 Conclusion</h1>
<p>In this paper, we propose the first theorem-driven science question-answering dataset and evaluate different LLMs on it. Though GPT-4 can achieve strong performance on our new dataset, the existing open-source LLMs are still struggling to achieve reasonable performance. We conjecture it is essen-
tial to leverage more science-related pre-training or fine-tuning to close the gap. On the hand, we found that the multimodal science questions are still extremely challenging for the existing visual LLMs. We believe more specialized visual encoding models are needed to better represent diagrams in these science questions.</p>
<h2>Limitations</h2>
<p>In this work, we explore the possibilities to utilize different large language models to solve challenging theorem-driven questions. There are still some limitations: (1) our answer extraction is still not perfect. There are some cases where our answer extractor is not able to locate the answer. Thus the final accuracy is still an approximate lower bound. (2) in our dataset collection, we specifically avoid the hard-to-evaluate cases where the answer is a formula, figure, or a matrix. Our choice of the questions can be biased in terms of evaluating the overall ability. (3) in the multimodal questions in TheoremQA, we have investigated different existing models but none of them succeed in achieving reasonable performance.</p>
<h2>References</h2>
<p>Aida Amini, Saadia Gabriel, Shanchuan Lin, Rik Koncel-Kedziorski, Yejin Choi, and Hannaneh Hajishirzi. 2019. Mathqa: Towards interpretable math word problem solving with operation-based formalisms. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages $2357-2367$.</p>
<p>Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. 2021. Program synthesis with large language models. arXiv preprint arXiv:2108.07732.</p>
<p>Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et al. 2022. Constitutional ai: Harmlessness from ai feedback. arXiv preprint arXiv:2212.08073.</p>
<p>Stella Biderman, Hailey Schoelkopf, Quentin Anthony, Herbie Bradley, Kyle O’Brien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, et al. 2023. Pythia: A suite for analyzing large language models across training and scaling. arXiv preprint arXiv:2304.01373.</p>
<p>Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901.</p>
<p>Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar,</p>
<p>Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. 2023. Sparks of artificial general intelligence: Early experiments with gpt-4. arXiv preprint arXiv:2303.12712.</p>
<p>Jiaqi Chen, Tong Li, Jinghui Qin, Pan Lu, Liang Lin, Chongyu Chen, and Xiaodan Liang. 2022a. UniGeo: Unifying geometry logical reasoning via reformulating mathematical expression. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 3313-3323, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.</p>
<p>Jiaqi Chen, Jianheng Tang, Jinghui Qin, Xiaodan Liang, Lingbo Liu, Eric Xing, and Liang Lin. 2021a. Geoqa: A geometric question answering benchmark towards multimodal numerical reasoning. In Findings of the Association for Computational Linguistics: ACLIJCNLP 2021, pages 513-523.</p>
<p>Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. 2021b. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374.</p>
<p>Wenhu Chen, Xueguang Ma, Xinyi Wang, and William W Cohen. 2022b. Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks. arXiv preprint arXiv:2211.12588.</p>
<p>Zhiyu Chen, Wenhu Chen, Charese Smiley, Sameena Shah, Iana Borova, Dylan Langdon, Reema Moussa, Matt Beane, Ting-Hao Huang, Bryan R Routledge, et al. 2021c. Finqa: A dataset of numerical reasoning over financial data. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 3697-3711.</p>
<p>Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. 2022. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311.</p>
<p>Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. 2021. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168.</p>
<p>Andrew Drozdov, Nathanael Schärli, Ekin Akyürek, Nathan Scales, Xinying Song, Xinyun Chen, Olivier Bousquet, and Denny Zhou. 2022. Compositional semantic parsing with large language models. arXiv preprint arXiv:2209.15003.</p>
<p>Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, and Graham Neubig. 2022. Pal: Program-aided language models. arXiv preprint arXiv:2211.10435.</p>
<p>Google. 2023. Palm 2 technical report. https://ai.google/static/documents/palm2techreport.pdf.</p>
<p>Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. 2021. Measuring mathematical problem solving with the math dataset. Conference on Neural Information Processing Systems.</p>
<p>Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. 2022. Training compute-optimal large language models. arXiv preprint arXiv:2203.15556.</p>
<p>Mohammad Javad Hosseini, Hannaneh Hajishirzi, Oren Etzioni, and Nate Kushman. 2014. Learning to solve arithmetic word problems with verb categorization. In EMNLP, pages 523-533.</p>
<p>Wolfram Research, Inc. Mathematica, Version 13.2. Champaign, IL, 2022.</p>
<p>Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. 2022. Large language models are zero-shot reasoners. In Advances in Neural Information Processing Systems.</p>
<p>Rik Koncel-Kedziorski, Hannaneh Hajishirzi, Ashish Sabharwal, Oren Etzioni, and Siena Dumas Ang. 2015. Parsing algebraic word problems into equations. Transactions of the Association for Computational Linguistics, 3:585-597.</p>
<p>Rik Koncel-Kedziorski, Subhro Roy, Aida Amini, Nate Kushman, and Hannaneh Hajishirzi. 2016. Mawps: A math word problem repository. In Proceedings of the 2016 conference of the north american chapter of the association for computational linguistics: human language technologies, pages 1152-1157.</p>
<p>Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. 2022. Blip: Bootstrapping language-image pretraining for unified vision-language understanding and generation. In International Conference on Machine Learning, pages 12888-12900. PMLR.</p>
<p>Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, et al. 2023. Starcoder: may the source be with you! arXiv preprint arXiv:2305.06161.</p>
<p>Wang Ling, Dani Yogatama, Chris Dyer, and Phil Blunsom. 2017. Program induction by rationale generation: Learning to solve and explain algebraic word problems. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 158-167.</p>
<p>Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. 2023. Visual instruction tuning. arXiv preprint arXiv:2304.08485.</p>
<p>Pan Lu, Ran Gong, Shibiao Jiang, Liang Qiu, Siyuan Huang, Xiaodan Liang, and Song-Chun Zhu. 2021a. Inter-GPS: Interpretable geometry problem solving with formal language and symbolic reasoning. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 67746786, Online. Association for Computational Linguistics.</p>
<p>Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, KaiWei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan. 2022. Learn to explain: Multimodal reasoning via thought chains for science question answering. Advances in Neural Information Processing Systems, 35:2507-2521.</p>
<p>Pan Lu, Baolin Peng, Hao Cheng, Michel Galley, KaiWei Chang, Ying Nian Wu, Song-Chun Zhu, and Jianfeng Gao. 2023a. Chameleon: Plug-and-play compositional reasoning with large language models. arXiv preprint arXiv:2304.09842.</p>
<p>Pan Lu, Liang Qiu, Kai-Wei Chang, Ying Nian Wu, Song-Chun Zhu, Tanmay Rajpurohit, Peter Clark, and Ashwin Kalyan. 2023b. Dynamic prompt learning via policy gradient for semi-structured mathematical reasoning. In International Conference on Learning Representations (ICLR).</p>
<p>Pan Lu, Liang Qiu, Jiaqi Chen, Tony Xia, Yizhou Zhao, Wei Zhang, Zhou Yu, Xiaodan Liang, and Song-Chun Zhu. 2021b. Iconqa: A new benchmark for abstract diagram understanding and visual language reasoning. In The 35th Conference on Neural Information Processing Systems (NeurIPS) Track on Datasets and Benchmarks.</p>
<p>Pan Lu, Liang Qiu, Wenhao Yu, Sean Welleck, and KaiWei Chang. 2023c. A survey of deep learning for mathematical reasoning. In The 61st Annual Meeting of the Association for Computational Linguistics (ACL).</p>
<p>Shen-Yun Miao, Chao-Chun Liang, and Keh-Yih Su. 2020. A diverse corpus for evaluating and developing english math word problem solvers. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 975-984.</p>
<p>Swaroop Mishra, Matthew Finlayson, Pan Lu, Leonard Tang, Sean Welleck, Chitta Baral, Tanmay Rajpurohit, Oyvind Tafjord, Ashish Sabharwal, Peter Clark, and Ashwin Kalyan. 2022. Lila: A unified benchmark for mathematical reasoning. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing (EMNLP).</p>
<p>Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese, and Caiming Xiong. 2022. Codegen: An open large language model for code with multi-turn program synthesis. arXiv preprint arXiv:2203.13474.</p>
<p>Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, et al. 2021. Show your work: Scratchpads for intermediate computation with language models. In Deep Learning for Code Workshop.</p>
<p>OpenAI. 2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774.</p>
<p>Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35:27730-27744.</p>
<p>Arkil Patel, Satwik Bhattamishra, and Navin Goyal. 2021a. Are nlp models really able to solve simple math word problems? In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 2080-2094.</p>
<p>Arkil Patel, Satwik Bhattamishra, and Navin Goyal. 2021b. Are NLP models really able to solve simple math word problems? In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 2080-2094, Online. Association for Computational Linguistics.</p>
<p>Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. 2021. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 8748-8763. PMLR.</p>
<p>Jack W Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, et al. 2021. Scaling language models: Methods, analysis \&amp; insights from training gopher. arXiv preprint arXiv:2112.11446.</p>
<p>Subhro Roy and Dan Roth. 2015. Solving general arithmetic word problems. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1743-1752.</p>
<p>David Saxton, Edward Grefenstette, Felix Hill, and Pushmeet Kohli. 2019. Analysing mathematical reasoning abilities of neural models. arXiv preprint arXiv:1904.01557.</p>
<p>Minjoon Seo, Hannaneh Hajishirzi, Ali Farhadi, Oren Etzioni, and Clint Malcolm. 2015. Solving geometry problems: Combining text and diagram interpretation. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1466-1476, Lisbon, Portugal. Association for Computational Linguistics.</p>
<p>Alex Tamkin, Kunal Handa, Avash Shrestha, and Noah Goodman. 2022. Task ambiguity in humans and language models. arXiv preprint arXiv:2212.10711.</p>
<p>Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony Hartshorn, Elvis Saravia, Andrew Poulton, Viktor Kerkez, and Robert Stojnic. 2022. Galactica: A large language model for science. arXiv preprint arXiv:2211.09085.</p>
<p>Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971.</p>
<p>Shyam Upadhyay and Ming-Wei Chang. 2015. Draw: A challenging and diverse algebra word problem set. Technical report, Citeseer.</p>
<p>Shyam Upadhyay and Ming-Wei Chang. 2017. Annotating derivations: A new evaluation strategy and dataset for algebra word problems. In Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers, pages 494-504, Valencia, Spain. Association for Computational Linguistics.</p>
<p>Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, and Denny Zhou. 2022. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171.</p>
<p>Yan Wang, Xiaojiang Liu, and Shuming Shi. 2017. Deep neural solver for math word problems. In Proceedings of the 2017 conference on empirical methods in natural language processing, pages 845-854.</p>
<p>Yue Wang, Hung Le, Akhilesh Deepak Gotmare, Nghi DQ Bui, Junnan Li, and Steven CH Hoi. 2023. Codet5+: Open code large language models for code understanding and generation. arXiv preprint arXiv:2305.07922.</p>
<p>Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al. 2022a. Emergent abilities of large language models. Transactions on Machine Learning Research.</p>
<p>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed H Chi, Quoc V Le, Denny Zhou, et al. 2022b. Chain-of-thought prompting elicits reasoning in large language models. In Advances in Neural Information Processing Systems.</p>
<p>Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao Xia, et al. 2022. Glm-130b: An open bilingual pre-trained model. arXiv preprint arXiv:2210.02414.</p>
<p>Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. 2022. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068.</p>
<p>Chuanyang Zheng, Zhengying Liu, Enze Xie, Zhenguo Li, and Yu Li. 2023. Progressive-hint prompting improves reasoning in large language models. arXiv preprint arXiv:2304.09797.</p>
<p>Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Olivier Bousquet, Quoc Le, and Ed Chi. 2022. Least-to-most prompting enables complex reasoning in large language models. arXiv preprint arXiv:2205.10625.</p>
<p>Fengbin Zhu, Wenqiang Lei, Youcheng Huang, Chao Wang, Shuo Zhang, Jiancheng Lv, Fuli Feng, and Tat-Seng Chua. 2021. Tat-qa: A question answering benchmark on a hybrid of tabular and textual content in finance. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 3277-3287.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{7}$ https://github.com/sahil280114/codealpaca&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
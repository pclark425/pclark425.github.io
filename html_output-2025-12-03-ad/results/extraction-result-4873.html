<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4873 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4873</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4873</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-106.html">extraction-schema-106</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs or neural language models) being used to solve puzzle games that require spatial knowledge (such as Sudoku or other spatial reasoning tasks). Include details about the models, the puzzles, the mechanisms or strategies used, performance metrics, evidence of spatial reasoning, limitations, and comparisons.</div>
                <p><strong>Paper ID:</strong> paper-235765540</p>
                <p><strong>Paper Title:</strong> <a href="https://arxiv.org/pdf/2107.03438v3.pdf" target="_blank">LanguageRefer: Spatial-Language Model for 3D Visual Grounding</a></p>
                <p><strong>Paper Abstract:</strong> For robots to understand human instructions and perform meaningful tasks in the near future, it is important to develop learned models that comprehend referential language to identify common objects in real-world 3D scenes. In this paper, we introduce a spatial-language model for a 3D visual grounding problem. Specifically, given a reconstructed 3D scene in the form of point clouds with 3D bounding boxes of potential object candidates, and a language utterance referring to a target object in the scene, our model successfully identifies the target object from a set of potential candidates. Specifically, LanguageRefer uses a transformer-based architecture that combines spatial embedding from bounding boxes with fine-tuned language embeddings from DistilBert to predict the target object. We show that it performs competitively on visio-linguistic datasets proposed by ReferIt3D. Further, we analyze its spatial reasoning task performance decoupled from perception noise, the accuracy of view-dependent utterances, and viewpoint annotations for potential robotics applications.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4873.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4873.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs or neural language models) being used to solve puzzle games that require spatial knowledge (such as Sudoku or other spatial reasoning tasks). Include details about the models, the puzzles, the mechanisms or strategies used, performance metrics, evidence of spatial reasoning, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LanguageRefer</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LanguageRefer: Spatial-Language Model for 3D Visual Grounding</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A transformer-based spatial-language model that combines pre-trained DistilBert token embeddings with sinusoidal positional encodings of 3D bounding boxes to identify the target object referred to by natural language in reconstructed 3D scenes.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LanguageRefer (transformer fine-tuned from DistilBert)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Transformer-based reference model fine-tuned from a pre-trained cased DistilBert base; token embeddings are 768-dimensional. Spatial information (6D bounding-box center+size) is mapped via sinusoidal positional encoding to 768-d vectors and added to object-token embeddings. The pipeline is modular: a PointNet++ semantic classifier provides object class labels, which are concatenated into the token sequence; the reference model is trained with a reference loss, a binary target-class classification loss, and a masked-token recovery loss. Optimization used AdamW with lr=1e-4 and HuggingFace DistilBert initialization. (No parameter counts provided in paper.)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>3D visual grounding (ReferIt3D: Nr3D and Sr3D)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_description</strong></td>
                            <td>Given a reconstructed 3D scene (point clouds and candidate 3D bounding boxes) and a natural-language utterance referring to an object, select the referenced target object among multiple candidates; tasks require spatial reasoning about relations (left/right, in-between, facing) and sometimes viewpoint inference, with many same-class distractors.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_strategy</strong></td>
                            <td>Concatenate predicted object class labels to the utterance, tokenize and embed with DistilBert, add sinusoidal positional encodings of each object's 3D bounding box to the corresponding object-token embeddings, then fine-tune the transformer to (1) pick one object (reference classification), (2) binary classify whether each object matches the target class, and (3) predict randomly masked noun tokens (mask loss). At inference, filter candidates by semantic-class predictions (top-k) and use a DistilBert-based target-class predictor. The pipeline decouples perception (PointNet++ semantic classifier) from spatial-language reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Quantitative ablations and dataset manipulations: (a) Large performance gains when replacing predicted class labels with ground-truth class labels (e.g., Sr3D from 56.0% to 91.1%), indicating the spatial-language module learns spatial relations when perception noise is removed; (b) viewpoint-correction experiments: annotating and correcting scene orientations for view-dependent utterances improved model accuracy significantly on view-dependent subset (+12.7%), demonstrating sensitivity to viewpoint/spatial relations; (c) transfer experiments (train on Sr3D, eval on Nr3D and vice versa) show model's spatial reasoning generalizes across datasets; (d) ablations show binary target-class loss improves performance while mask loss had little effect.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>With predicted semantic labels: Nr3D overall accuracy 43.9%, Sr3D overall accuracy 56.0%. Compared to ReferIt3D baseline and other non-extra-data methods: +8.3% over ReferIt3D baseline on Nr3D and +15.2% on Sr3D; comparable to SAT (which used extra 2D training data) within 1.9%. With ground-truth class labels: Nr3D 54.3%, Sr3D 91.1%. PointNet++ semantic classifier used in pipeline achieved ~69% instance classification accuracy; an extra DistilBert-based utterance->target-class predictor reported 94% accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Performance degraded by perception noise from the semantic classifier (large gaps between predicted vs. ground-truth label experiments). View-dependent utterances often lack explicit viewpoint/orientation information making correct interpretation impossible in some cases; the dataset's noisy 3D reconstructions and diverse natural language expressions (Nr3D) also reduce performance. Masking loss produced little benefit; adding a text prediction loss degraded performance in ablation. The model does not attempt end-to-end 3D perception + reasoning integration (it relies on a separate semantic classifier).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared against multiple prior 3D visual grounding methods on ReferIt3D: outperformed other models that were not trained with extra data (e.g., improved +2.2% over prior best on Nr3D); on Sr3D the improvement was larger. Compared to SAT (which used auxiliary 2D images during training), LanguageRefer achieved comparable performance (within 1.9%) despite not using extra 2D data. Transfer experiments produced accuracies comparable to other non-SAT methods (e.g., InstanceRefer).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LanguageRefer: Spatial-Language Model for 3D Visual Grounding', 'publication_date_yy_mm': '2021-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4873.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4873.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs or neural language models) being used to solve puzzle games that require spatial knowledge (such as Sudoku or other spatial reasoning tasks). Include details about the models, the puzzles, the mechanisms or strategies used, performance metrics, evidence of spatial reasoning, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DistilBert</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DistilBert (a distilled version of BERT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A smaller, faster Transformer-based pre-trained language model used to provide token embeddings and as the base model for fine-tuning the spatial-language transformer in LanguageRefer.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DistilBert (cased DistilBert base)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Pre-trained transformer (DistilBert) used for token embeddings (768-dim) and fine-tuned as the core of the spatial-language model; initialized from HuggingFace DistilBert cased base. Fine-tuning included adding positional spatial encodings and training with multi-task losses (reference, binary classification, mask prediction). Specific parameter counts are not provided in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>3D visual grounding (ReferIt3D: Nr3D and Sr3D)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_description</strong></td>
                            <td>Serves to encode natural-language utterances and concatenated predicted object class labels so the transformer can attend across language tokens and object tokens that include spatial encodings.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_strategy</strong></td>
                            <td>Used as a pre-trained language encoder; utterance tokens and object class label tokens are embedded by DistilBert; object-token embeddings are augmented with sinusoidal encoding of 3D bounding-box values and then the transformer layers (fine-tuned DistilBert) produce contextualized representations used for object selection and auxiliary tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Indirect: overall system performance and ablations indicate that fine-tuning DistilBert with added spatial encodings enables learning spatial relations (improvements with ground-truth labels and viewpoint corrections). A DistilBert-based target-class predictor component achieved 94% accuracy on predicting target class from utterance.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Used within LanguageRefer; the DistilBert-based target-class predictor reported 94% accuracy. The paper does not report standalone DistilBert parameter counts or intrinsic spatial-reasoning benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Mask loss (masked-noun recovery) added during training did not significantly improve accuracy. Concatenation of many object class label 'sentences' into DistilBert's sequence violates typical sentence-count assumptions, which the authors note but handle by token concatenation; no parameter count scaling experiments reported.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>LanguageRefer leverages a pre-trained DistilBert rather than training BERT-like models from scratch (as in some compared work), reducing design complexity and compute; performance comparable or superior to other methods that used heavier multi-modal training.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LanguageRefer: Spatial-Language Model for 3D Visual Grounding', 'publication_date_yy_mm': '2021-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4873.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4873.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs or neural language models) being used to solve puzzle games that require spatial knowledge (such as Sudoku or other spatial reasoning tasks). Include details about the models, the puzzles, the mechanisms or strategies used, performance metrics, evidence of spatial reasoning, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ReferIt3D (dataset)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ReferIt3D: Neural listeners for fine-grained 3D object identification in real-world scenes</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 3D visual grounding benchmark built on ScanNet with two subsets: Nr3D (natural utterances) and Sr3D (template spatial utterances), used to evaluate models that map language references to objects in 3D scenes.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Referit3d: Neural listeners for fine-grained 3d object identification in real-world scenes.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ReferIt3D (benchmark: Nr3D and Sr3D)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Nr3D: 41,503 natural-language utterances; Sr3D: 83,572 template-based spatial utterances; both use ScanNet reconstructed 3D scenes with ground-truth bounding boxes and 76 target classes and are designed to include multiple same-class distractors; utterances are labeled as view-independent or view-dependent.</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>3D referring-expression comprehension / 3D visual grounding</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_description</strong></td>
                            <td>Given a 3D scene and a referring utterance, select the correct object bounding box; requires spatial reasoning about object relations and sometimes viewpoint inference, and is made more difficult by noisy reconstructions and multiple same-class distractors.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_strategy</strong></td>
                            <td>Benchmark used for training and evaluation; authors additionally annotated view-dependent utterances with explicit viewpoint orientations (VD-explicit vs VD-implicit) to study viewpoint effects and rotated scenes accordingly for experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Authors used ReferIt3D's Sr3D (spatial-templated utterances) to show LanguageRefer's strong spatial reasoning (91.1% with ground-truth labels), and used viewpoint-annotation experiments on Nr3D to show improved handling of view-dependent language (+12.7% improvement when corrected orientations applied).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Dataset counts provided; used as evaluation benchmark with reported model accuracies: LanguageRefer 43.9% (Nr3D predicted labels), 56.0% (Sr3D predicted labels), and with ground-truth labels 54.3% (Nr3D) and 91.1% (Sr3D).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Many view-dependent utterances in ReferIt3D lack explicit orientation/origin viewpoint information, making inference ambiguous; ScanNet reconstructions are noisy and lack fine detail, complicating perception and grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Used as the common benchmark for comparing LanguageRefer to prior methods (InstanceRefer, SAT, original ReferIt3D baseline, etc.); LanguageRefer outperformed other non-extra-data methods on both Nr3D and Sr3D splits.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LanguageRefer: Spatial-Language Model for 3D Visual Grounding', 'publication_date_yy_mm': '2021-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4873.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4873.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs or neural language models) being used to solve puzzle games that require spatial knowledge (such as Sudoku or other spatial reasoning tasks). Include details about the models, the puzzles, the mechanisms or strategies used, performance metrics, evidence of spatial reasoning, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SAT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SAT: 2d semantics assisted training for 3d visual grounding</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A recently proposed transformer-based method for 3D visual grounding that learns fused multi-modal embeddings and leverages auxiliary 2D image data during training to improve grounding performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>SAT: 2d semantics assisted training for 3d visual grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>SAT (transformer-based multi-modal model)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A transformer-based approach that fuses multi-modal inputs (3D and 2D) and uses auxiliary 2D image datasets during training to assist 3D grounding; learns fused embeddings potentially requiring training additional BERT-like modules from scratch (per paper's related-work discussion).</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>3D visual grounding (ReferIt3D)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_description</strong></td>
                            <td>Same ReferIt3D 3D referring-expression task; SAT augments training with 2D semantic data to improve grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_strategy</strong></td>
                            <td>Learns fused multi-modal embeddings (3D point cloud + 2D images + language) using a transformer architecture and auxiliary 2D image supervision to boost performance.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>In the paper SAT is presented as a strong baseline that benefits from 2D semantics; LanguageRefer reports being comparable to SAT despite not using extra 2D data (reported gap ~1.9%), implying both methods learn spatial relations, but SAT's improvement is attributed to the extra 2D training signal.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Not numerically listed in this paper beyond comparison statements: LanguageRefer is 'comparable to SAT (with a 1.9% difference)'; precise SAT numbers are reported in SAT's own paper (not reproduced here).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Requires additional 2D image training data to reach top performance (per discussion); increased training data and modality requirements compared to LanguageRefer's approach.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Used as a comparison in results: LanguageRefer outperforms other non-extra-data methods and is comparable to SAT which used extra 2D images.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LanguageRefer: Spatial-Language Model for 3D Visual Grounding', 'publication_date_yy_mm': '2021-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4873.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e4873.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs or neural language models) being used to solve puzzle games that require spatial knowledge (such as Sudoku or other spatial reasoning tasks). Include details about the models, the puzzles, the mechanisms or strategies used, performance metrics, evidence of spatial reasoning, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CLEVR / CLEVRER</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CLEVR; CLEVRER</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Synthetic diagnostic benchmarks for compositional visual reasoning (CLEVR) and spatio-temporal reasoning in videos (CLEVRER) cited as prior successful domains for spatial reasoning models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Clevr: A diagnostic dataset for compositional language and elementary visual reasoning.; CLEVRER: collision events for video representation and reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>CLEVR / CLEVRER (benchmarks)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>CLEVR: synthetic 2D rendered scenes with compositional queries designed to probe spatial and logical reasoning; CLEVRER: an extension for video-based spatio-temporal reasoning (collision events). These are datasets/benchmarks, not language models.</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Diagnostic visual/spatio-temporal reasoning tasks (synthetic)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_description</strong></td>
                            <td>Tasks focus on compositional spatial relations, counting, and causal/spatio-temporal inference in synthetic scenes/videos; commonly used to validate models' spatial reasoning capabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_strategy</strong></td>
                            <td>Mentioned as inspiration: prior methods that succeeded on CLEVR/CLEVRER motivated decoupling spatial reasoning from perception noise in 3D grounding; no experiments with these datasets are run in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Cited prior work showing success on these benchmarks as motivation for authors' approach; no direct new evidence presented in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Only referenced in related work as prior successful domains; the paper highlights that synthetic benchmarks differ from noisy real-world 3D reconstructions like ScanNet (used in ReferIt3D).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Used as motivating examples of spatial-reasoning benchmarks; authors contrast their real-world 3D noisy setting with the cleaner synthetic CLEVR/CLEVRER domains.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LanguageRefer: Spatial-Language Model for 3D Visual Grounding', 'publication_date_yy_mm': '2021-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Clevr: A diagnostic dataset for compositional language and elementary visual reasoning. <em>(Rating: 2)</em></li>
                <li>CLEVRER: collision events for video representation and reasoning. <em>(Rating: 2)</em></li>
                <li>Referit3d: Neural listeners for fine-grained 3d object identification in real-world scenes. <em>(Rating: 2)</em></li>
                <li>SAT: 2d semantics assisted training for 3d visual grounding. <em>(Rating: 2)</em></li>
                <li>Instancerefer: Cooperative holistic understanding for visual grounding on point clouds through instance multi-level contextual referring <em>(Rating: 2)</em></li>
                <li>Scanrefer: 3d object localization in rgb-d scans using natural language. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4873",
    "paper_id": "paper-235765540",
    "extraction_schema_id": "extraction-schema-106",
    "extracted_data": [
        {
            "name_short": "LanguageRefer",
            "name_full": "LanguageRefer: Spatial-Language Model for 3D Visual Grounding",
            "brief_description": "A transformer-based spatial-language model that combines pre-trained DistilBert token embeddings with sinusoidal positional encodings of 3D bounding boxes to identify the target object referred to by natural language in reconstructed 3D scenes.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LanguageRefer (transformer fine-tuned from DistilBert)",
            "model_description": "Transformer-based reference model fine-tuned from a pre-trained cased DistilBert base; token embeddings are 768-dimensional. Spatial information (6D bounding-box center+size) is mapped via sinusoidal positional encoding to 768-d vectors and added to object-token embeddings. The pipeline is modular: a PointNet++ semantic classifier provides object class labels, which are concatenated into the token sequence; the reference model is trained with a reference loss, a binary target-class classification loss, and a masked-token recovery loss. Optimization used AdamW with lr=1e-4 and HuggingFace DistilBert initialization. (No parameter counts provided in paper.)",
            "puzzle_name": "3D visual grounding (ReferIt3D: Nr3D and Sr3D)",
            "puzzle_description": "Given a reconstructed 3D scene (point clouds and candidate 3D bounding boxes) and a natural-language utterance referring to an object, select the referenced target object among multiple candidates; tasks require spatial reasoning about relations (left/right, in-between, facing) and sometimes viewpoint inference, with many same-class distractors.",
            "mechanism_or_strategy": "Concatenate predicted object class labels to the utterance, tokenize and embed with DistilBert, add sinusoidal positional encodings of each object's 3D bounding box to the corresponding object-token embeddings, then fine-tune the transformer to (1) pick one object (reference classification), (2) binary classify whether each object matches the target class, and (3) predict randomly masked noun tokens (mask loss). At inference, filter candidates by semantic-class predictions (top-k) and use a DistilBert-based target-class predictor. The pipeline decouples perception (PointNet++ semantic classifier) from spatial-language reasoning.",
            "evidence_of_spatial_reasoning": "Quantitative ablations and dataset manipulations: (a) Large performance gains when replacing predicted class labels with ground-truth class labels (e.g., Sr3D from 56.0% to 91.1%), indicating the spatial-language module learns spatial relations when perception noise is removed; (b) viewpoint-correction experiments: annotating and correcting scene orientations for view-dependent utterances improved model accuracy significantly on view-dependent subset (+12.7%), demonstrating sensitivity to viewpoint/spatial relations; (c) transfer experiments (train on Sr3D, eval on Nr3D and vice versa) show model's spatial reasoning generalizes across datasets; (d) ablations show binary target-class loss improves performance while mask loss had little effect.",
            "performance_metrics": "With predicted semantic labels: Nr3D overall accuracy 43.9%, Sr3D overall accuracy 56.0%. Compared to ReferIt3D baseline and other non-extra-data methods: +8.3% over ReferIt3D baseline on Nr3D and +15.2% on Sr3D; comparable to SAT (which used extra 2D training data) within 1.9%. With ground-truth class labels: Nr3D 54.3%, Sr3D 91.1%. PointNet++ semantic classifier used in pipeline achieved ~69% instance classification accuracy; an extra DistilBert-based utterance-&gt;target-class predictor reported 94% accuracy.",
            "limitations_or_failure_cases": "Performance degraded by perception noise from the semantic classifier (large gaps between predicted vs. ground-truth label experiments). View-dependent utterances often lack explicit viewpoint/orientation information making correct interpretation impossible in some cases; the dataset's noisy 3D reconstructions and diverse natural language expressions (Nr3D) also reduce performance. Masking loss produced little benefit; adding a text prediction loss degraded performance in ablation. The model does not attempt end-to-end 3D perception + reasoning integration (it relies on a separate semantic classifier).",
            "comparison_baseline": "Compared against multiple prior 3D visual grounding methods on ReferIt3D: outperformed other models that were not trained with extra data (e.g., improved +2.2% over prior best on Nr3D); on Sr3D the improvement was larger. Compared to SAT (which used auxiliary 2D images during training), LanguageRefer achieved comparable performance (within 1.9%) despite not using extra 2D data. Transfer experiments produced accuracies comparable to other non-SAT methods (e.g., InstanceRefer).",
            "uuid": "e4873.0",
            "source_info": {
                "paper_title": "LanguageRefer: Spatial-Language Model for 3D Visual Grounding",
                "publication_date_yy_mm": "2021-07"
            }
        },
        {
            "name_short": "DistilBert",
            "name_full": "DistilBert (a distilled version of BERT)",
            "brief_description": "A smaller, faster Transformer-based pre-trained language model used to provide token embeddings and as the base model for fine-tuning the spatial-language transformer in LanguageRefer.",
            "citation_title": "Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter.",
            "mention_or_use": "use",
            "model_name": "DistilBert (cased DistilBert base)",
            "model_description": "Pre-trained transformer (DistilBert) used for token embeddings (768-dim) and fine-tuned as the core of the spatial-language model; initialized from HuggingFace DistilBert cased base. Fine-tuning included adding positional spatial encodings and training with multi-task losses (reference, binary classification, mask prediction). Specific parameter counts are not provided in the paper.",
            "puzzle_name": "3D visual grounding (ReferIt3D: Nr3D and Sr3D)",
            "puzzle_description": "Serves to encode natural-language utterances and concatenated predicted object class labels so the transformer can attend across language tokens and object tokens that include spatial encodings.",
            "mechanism_or_strategy": "Used as a pre-trained language encoder; utterance tokens and object class label tokens are embedded by DistilBert; object-token embeddings are augmented with sinusoidal encoding of 3D bounding-box values and then the transformer layers (fine-tuned DistilBert) produce contextualized representations used for object selection and auxiliary tasks.",
            "evidence_of_spatial_reasoning": "Indirect: overall system performance and ablations indicate that fine-tuning DistilBert with added spatial encodings enables learning spatial relations (improvements with ground-truth labels and viewpoint corrections). A DistilBert-based target-class predictor component achieved 94% accuracy on predicting target class from utterance.",
            "performance_metrics": "Used within LanguageRefer; the DistilBert-based target-class predictor reported 94% accuracy. The paper does not report standalone DistilBert parameter counts or intrinsic spatial-reasoning benchmarks.",
            "limitations_or_failure_cases": "Mask loss (masked-noun recovery) added during training did not significantly improve accuracy. Concatenation of many object class label 'sentences' into DistilBert's sequence violates typical sentence-count assumptions, which the authors note but handle by token concatenation; no parameter count scaling experiments reported.",
            "comparison_baseline": "LanguageRefer leverages a pre-trained DistilBert rather than training BERT-like models from scratch (as in some compared work), reducing design complexity and compute; performance comparable or superior to other methods that used heavier multi-modal training.",
            "uuid": "e4873.1",
            "source_info": {
                "paper_title": "LanguageRefer: Spatial-Language Model for 3D Visual Grounding",
                "publication_date_yy_mm": "2021-07"
            }
        },
        {
            "name_short": "ReferIt3D (dataset)",
            "name_full": "ReferIt3D: Neural listeners for fine-grained 3D object identification in real-world scenes",
            "brief_description": "A 3D visual grounding benchmark built on ScanNet with two subsets: Nr3D (natural utterances) and Sr3D (template spatial utterances), used to evaluate models that map language references to objects in 3D scenes.",
            "citation_title": "Referit3d: Neural listeners for fine-grained 3d object identification in real-world scenes.",
            "mention_or_use": "use",
            "model_name": "ReferIt3D (benchmark: Nr3D and Sr3D)",
            "model_description": "Nr3D: 41,503 natural-language utterances; Sr3D: 83,572 template-based spatial utterances; both use ScanNet reconstructed 3D scenes with ground-truth bounding boxes and 76 target classes and are designed to include multiple same-class distractors; utterances are labeled as view-independent or view-dependent.",
            "puzzle_name": "3D referring-expression comprehension / 3D visual grounding",
            "puzzle_description": "Given a 3D scene and a referring utterance, select the correct object bounding box; requires spatial reasoning about object relations and sometimes viewpoint inference, and is made more difficult by noisy reconstructions and multiple same-class distractors.",
            "mechanism_or_strategy": "Benchmark used for training and evaluation; authors additionally annotated view-dependent utterances with explicit viewpoint orientations (VD-explicit vs VD-implicit) to study viewpoint effects and rotated scenes accordingly for experiments.",
            "evidence_of_spatial_reasoning": "Authors used ReferIt3D's Sr3D (spatial-templated utterances) to show LanguageRefer's strong spatial reasoning (91.1% with ground-truth labels), and used viewpoint-annotation experiments on Nr3D to show improved handling of view-dependent language (+12.7% improvement when corrected orientations applied).",
            "performance_metrics": "Dataset counts provided; used as evaluation benchmark with reported model accuracies: LanguageRefer 43.9% (Nr3D predicted labels), 56.0% (Sr3D predicted labels), and with ground-truth labels 54.3% (Nr3D) and 91.1% (Sr3D).",
            "limitations_or_failure_cases": "Many view-dependent utterances in ReferIt3D lack explicit orientation/origin viewpoint information, making inference ambiguous; ScanNet reconstructions are noisy and lack fine detail, complicating perception and grounding.",
            "comparison_baseline": "Used as the common benchmark for comparing LanguageRefer to prior methods (InstanceRefer, SAT, original ReferIt3D baseline, etc.); LanguageRefer outperformed other non-extra-data methods on both Nr3D and Sr3D splits.",
            "uuid": "e4873.2",
            "source_info": {
                "paper_title": "LanguageRefer: Spatial-Language Model for 3D Visual Grounding",
                "publication_date_yy_mm": "2021-07"
            }
        },
        {
            "name_short": "SAT",
            "name_full": "SAT: 2d semantics assisted training for 3d visual grounding",
            "brief_description": "A recently proposed transformer-based method for 3D visual grounding that learns fused multi-modal embeddings and leverages auxiliary 2D image data during training to improve grounding performance.",
            "citation_title": "SAT: 2d semantics assisted training for 3d visual grounding.",
            "mention_or_use": "mention",
            "model_name": "SAT (transformer-based multi-modal model)",
            "model_description": "A transformer-based approach that fuses multi-modal inputs (3D and 2D) and uses auxiliary 2D image datasets during training to assist 3D grounding; learns fused embeddings potentially requiring training additional BERT-like modules from scratch (per paper's related-work discussion).",
            "puzzle_name": "3D visual grounding (ReferIt3D)",
            "puzzle_description": "Same ReferIt3D 3D referring-expression task; SAT augments training with 2D semantic data to improve grounding.",
            "mechanism_or_strategy": "Learns fused multi-modal embeddings (3D point cloud + 2D images + language) using a transformer architecture and auxiliary 2D image supervision to boost performance.",
            "evidence_of_spatial_reasoning": "In the paper SAT is presented as a strong baseline that benefits from 2D semantics; LanguageRefer reports being comparable to SAT despite not using extra 2D data (reported gap ~1.9%), implying both methods learn spatial relations, but SAT's improvement is attributed to the extra 2D training signal.",
            "performance_metrics": "Not numerically listed in this paper beyond comparison statements: LanguageRefer is 'comparable to SAT (with a 1.9% difference)'; precise SAT numbers are reported in SAT's own paper (not reproduced here).",
            "limitations_or_failure_cases": "Requires additional 2D image training data to reach top performance (per discussion); increased training data and modality requirements compared to LanguageRefer's approach.",
            "comparison_baseline": "Used as a comparison in results: LanguageRefer outperforms other non-extra-data methods and is comparable to SAT which used extra 2D images.",
            "uuid": "e4873.3",
            "source_info": {
                "paper_title": "LanguageRefer: Spatial-Language Model for 3D Visual Grounding",
                "publication_date_yy_mm": "2021-07"
            }
        },
        {
            "name_short": "CLEVR / CLEVRER",
            "name_full": "CLEVR; CLEVRER",
            "brief_description": "Synthetic diagnostic benchmarks for compositional visual reasoning (CLEVR) and spatio-temporal reasoning in videos (CLEVRER) cited as prior successful domains for spatial reasoning models.",
            "citation_title": "Clevr: A diagnostic dataset for compositional language and elementary visual reasoning.; CLEVRER: collision events for video representation and reasoning.",
            "mention_or_use": "mention",
            "model_name": "CLEVR / CLEVRER (benchmarks)",
            "model_description": "CLEVR: synthetic 2D rendered scenes with compositional queries designed to probe spatial and logical reasoning; CLEVRER: an extension for video-based spatio-temporal reasoning (collision events). These are datasets/benchmarks, not language models.",
            "puzzle_name": "Diagnostic visual/spatio-temporal reasoning tasks (synthetic)",
            "puzzle_description": "Tasks focus on compositional spatial relations, counting, and causal/spatio-temporal inference in synthetic scenes/videos; commonly used to validate models' spatial reasoning capabilities.",
            "mechanism_or_strategy": "Mentioned as inspiration: prior methods that succeeded on CLEVR/CLEVRER motivated decoupling spatial reasoning from perception noise in 3D grounding; no experiments with these datasets are run in this paper.",
            "evidence_of_spatial_reasoning": "Cited prior work showing success on these benchmarks as motivation for authors' approach; no direct new evidence presented in this paper.",
            "performance_metrics": null,
            "limitations_or_failure_cases": "Only referenced in related work as prior successful domains; the paper highlights that synthetic benchmarks differ from noisy real-world 3D reconstructions like ScanNet (used in ReferIt3D).",
            "comparison_baseline": "Used as motivating examples of spatial-reasoning benchmarks; authors contrast their real-world 3D noisy setting with the cleaner synthetic CLEVR/CLEVRER domains.",
            "uuid": "e4873.4",
            "source_info": {
                "paper_title": "LanguageRefer: Spatial-Language Model for 3D Visual Grounding",
                "publication_date_yy_mm": "2021-07"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Clevr: A diagnostic dataset for compositional language and elementary visual reasoning.",
            "rating": 2,
            "sanitized_title": "clevr_a_diagnostic_dataset_for_compositional_language_and_elementary_visual_reasoning"
        },
        {
            "paper_title": "CLEVRER: collision events for video representation and reasoning.",
            "rating": 2,
            "sanitized_title": "clevrer_collision_events_for_video_representation_and_reasoning"
        },
        {
            "paper_title": "Referit3d: Neural listeners for fine-grained 3d object identification in real-world scenes.",
            "rating": 2,
            "sanitized_title": "referit3d_neural_listeners_for_finegrained_3d_object_identification_in_realworld_scenes"
        },
        {
            "paper_title": "SAT: 2d semantics assisted training for 3d visual grounding.",
            "rating": 2,
            "sanitized_title": "sat_2d_semantics_assisted_training_for_3d_visual_grounding"
        },
        {
            "paper_title": "Instancerefer: Cooperative holistic understanding for visual grounding on point clouds through instance multi-level contextual referring",
            "rating": 2,
            "sanitized_title": "instancerefer_cooperative_holistic_understanding_for_visual_grounding_on_point_clouds_through_instance_multilevel_contextual_referring"
        },
        {
            "paper_title": "Scanrefer: 3d object localization in rgb-d scans using natural language.",
            "rating": 1,
            "sanitized_title": "scanrefer_3d_object_localization_in_rgbd_scans_using_natural_language"
        }
    ],
    "cost": 0.016184,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>LanguageRefer: Spatial-Language Model for 3D Visual Grounding</p>
<p>Junha Roh rohjunha@cs.washington.edu 
University of Washington
United States</p>
<p>Karthik Desingh kdesingh@cs.washington.edu 
University of Washington
United States</p>
<p>Ali Farhadi 
University of Washington
United States</p>
<p>Dieter Fox fox@cs.washington.edu 
University of Washington
United States</p>
<p>Paul G Allen School 
University of Washington
United States</p>
<p>LanguageRefer: Spatial-Language Model for 3D Visual Grounding
Referring taskLanguage model3D visual grounding3D Navigation
For robots to understand human instructions and perform meaningful tasks in the near future, it is important to develop learned models that comprehend referential language to identify common objects in real-world 3D scenes. In this paper, we introduce a spatial-language model for a 3D visual grounding problem. Specifically, given a reconstructed 3D scene in the form of point clouds with 3D bounding boxes of potential object candidates, and a language utterance referring to a target object in the scene, our model successfully identifies the target object from a set of potential candidates. Specifically, LanguageRefer uses a transformer-based architecture that combines spatial embedding from bounding boxes with fine-tuned language embeddings from DistilBert [1] to predict the target object. We show that it performs competitively on visio-linguistic datasets proposed by ReferIt3D [2]. Further, we analyze its spatial reasoning task performance decoupled from perception noise, the accuracy of view-dependent utterances, and viewpoint annotations for potential robotics applications. Project website: https://sites.google.com/view/language-refer. Figure 1: Simplified overview of LanguageRefer. The LanguageRefer model takes as input a groundinglanguage description of a single object in the scene, a 3D point cloud of a scene, and bounding boxes of objects in the scene and predicts the target object. Its four modules include: a classifier, a spatial embedder, a language embedder, and a spatial-language model.arXiv:2107.03438v3 [cs.RO] 4 Nov 2021Our long-term goal is to enable robots to visually navigate indoor environments based on referential instructions. In this paper we take a step towards this goal by leveraging the ReferIt3D dataset to build a model that can identify the 3D object referred to in a language utterance. Referential language to identify an object in real-world 3D scenes poses a challenging problem. Consider the sample utterance"Facing the foot of the bed, the bed on the right" along with a 3D scene as shown inFigure 1. Humans can easily follow the language clues, infer the point of view of the speaker, locate all the referenced elements, and spatially reason to locate the bed in the scene despite two instances of beds. However, viewpoint prediction, object identification, and spatial reasoning remain open-ended research problems in robotics and vision. The 3D reference task proposed by ReferIt3D is difficult because: (1) reconstructed 3D scenes of the real world in the ScanNet dataset are noisy and lack fine details compared to 2D images or rendered 3D scenes, (2) fine-grained class labels and expressions in natural language utterances are diverse and not exactly matched, (3) view-dependent utterances often require guessing the original viewpoints, which deters the model from properly learning spatial concepts, and (4) the combined complexity of multiple challenges complicates efforts to analyze what the model learns or understands.</p>
<p>Introduction</p>
<p>For robots to communicate seamlessly with humans to perform meaningful tasks in indoor environments, they must understand natural language utterances and ground them to real-world elements. Several recent advances have combined language and visual elements, producing methods for tasks such as visual question and answering (VQA) involving spatio-temporal reasoning tasks [3,4,5], embodied QA [6], and pre-training for visual recognition tasks with language descriptions [7]. Further, embodied agents can follow visually grounded language instructions to perform embodied tasks [8,9]. However, for real robots to intelligently perform these tasks, we need 3D representations from raw sensor data; ReferIt3D [2] proposes a benchmarking dataset of language utterances referring to objects in 3D scenes from the ScanNet [10] dataset.</p>
<p>Inspired by the success of the methods in [11,12] on CLEVR and CLEVRER domains for the spatial reasoning task, we hypothesized that for the 3D reference task, decoupling the spatial-reasoning from the perceptual task of identifying the objects in the 3D scene would improve performance and clearly track the role of perception noise in performance. More specifically, instead of developing an integrated multi-modal perception system, we assumed a pre-trained instance classification model or ground-truth classes for the objects that informed the spatial reasoning task. We focused on how the language model with spatial information could handle the reference task.</p>
<p>The ReferIt3D dataset includes two sub-datasets containing natural and synthetic language utterances, namely Nr3D and Sr3D, respectively. In our experiments, our model achieved comparable scores on both with predicted instance class labels. We observed high accuracy with ground-truth labels in Sr3D, which indicates that our model better understood template-based language data. Since our pipeline is modular and features multiple models (perceptual, spatial embedding, pre-trained language embedding, and spatial-language), our approach is flexible and adaptable to different environments and object entities.</p>
<p>Another aspect of the 3D reference task is viewpoint prediction. The ReferIt3D dataset contains utterances that can be grouped into view-independent (VI) and view-dependent (VD) categories. An example of a VI utterance is "The lamp closer to the white armchair" and a VD utterance is "The lamp on the right in-between the beds". The VD utterance requires viewpoint prediction. This distinction is crucial for robotics applications where the agent must infer the viewpoint to which the speaker is referring. In the ReferIt3D dataset, some VD utterances lack information to guess the valid orientation of the agent (who utters the language description), which prevents a model from understanding the significance of spatial relationships such as 'left of.' This is due to the annotation process of ReferIt3D datasets, where both a speaker and a listener can freely rotate the scene to infer an ill-defined orientation in the utterance. Human annotators who are aware of spatial concepts tend to validate otherwise arbitrary orientations. However, a data-driven model will suffer from degraded learning when it cannot verify orientations as well as human annotators. Viewpoint-free annotations may suit most robotics applications; nonetheless, predicting or verifying whether a given statement and the viewpoint match remains important. To better train and investigate the agent in view dependencies, we provide an extra collection of orientation annotations for VD utterances and compare the models with and without viewpoint correction from them.</p>
<p>To summarize, the paper contributes: (1) a novel transformer-based spatial-language model in a modular pipeline that better understands spatial relationships in a 3D visual grounding task. We show that our model achieves comparable performance with state-of-the-art methods on the Nr3D and Sr3D datasets. (2) analysis of the ReferIt3D dataset with viewpoint orientation annotations to remove potential artifacts from implicit orientations. (3) ablation and additional experiments with ground-truth classes that decouple the impact of perception noise in the spatial reasoning task.</p>
<p>Related Work</p>
<p>Vision-and-Language Navigation and Robot Navigation</p>
<p>Vision-and-language navigation (VLN) has been extensively studied and made remarkable progress over the last few years ( [13,14,15,16,17,18,19,20,21,22,23]). Given a language instruction in the simulation environment, the goal is for an agent to reach the desired node on the pre-defined traversal graph using images as input. The literature offers several extensions. ALFRED [8] proposes an extended VLN task that grounds a sequence of sub-tasks to achieve a higher level task in the AI2Thor environment [9]. ALFRED navigation sequences have (implicit) goals that are often close to objects of interest in the subsequent sub-tasks, e.g., when an agent is asked to move in front of the sink because it is going to clean a cup there. With high-level semantic tasks, object-centric spatial understanding is of even greater importance.</p>
<p>In another direction, recent approaches have relaxed the constraint of discrete traversal in VLN into continuous space ( [24,25,26,27,28]). Here, an agent encounters more complex tasks involving time and space. Thus, expanding the space representation to 3D can be an effective solution. In the context of VLN, we consider the 3D visual grounding task as a proxy for 3D indoor navigation that includes the full observability assumption and goal-oriented language descriptions. In particular, our approach focuses on understanding spatial relationships among objects, which plays a key role in VLN and robot navigation.</p>
<p>2D and 3D Visual Grounding</p>
<p>The 2D visual grounding task localizes an object or region in an image given a language description about the object or region ( [29,30,31]). Most methods use two-stage approaches: they first generate proposals and then compare the proposals to the language description to choose the grounded proposal ( [32,33,34,35,36]). The 3D visual grounding task localizes a 3D bounding box from the point cloud of a scene given a language description. Recently, ReferIt3D [2] and ScanRefer [37] were proposed as datasets for 3D visual grounding tasks, with language annotation on the ScanNet [10] dataset. Most 3D grounding approaches ( [2,37,38,39,40]) follow a two-stage schemes similar to many 2D visual grounding tasks. First, multiple bounding boxes are proposed or the ground-truth bounding boxes are used, and then features from the proposals are combined or compared with features from the language description. InstanceRefer [38] extracts attribute features both from point clouds and utterances and compares them to select the object that best matches. FFL-3DOG [39] matches features from language and point clouds with guidance from language and a visual scene graph. These methods rely on specific designs, e.g., bird-eye-view mappings with different types of operations or intensive language pre-processing for graph generation. In contrast, our approach leverages the language embedding space from the pre-trained language model. Following the pipeline and general architecture of the language model therefore requires minimal manual design compared to previous works. SAT [40], like our approach, relies on transformer [41] models; it learns a fused embedding of multi-modal inputs and uses auxiliary 2D images. In contrast, our approach uses a semantic classifier to predict object class labels and takes these labels as input. It has a marginal cost of learning fused embeddings compared to training multiple BERT models [42] from scratch in SAT [40]. Even given only semantic information from point clouds, our model still achieved comparable performance with state-of-the-art methods on Nr3D and Sr3D datasets. In addition, the decoupled perception module makes our approach modular and thus transferable to different data.</p>
<p>Problem Statement and Methodology</p>
<p>Given a 3D scene S with a list of objects (O 1 ,    , O M ) and a language utterance U , the problem is to predict the target object O T , T  I M = {1,    , M } referred to in the language. A single object O i consists of a bounding box B i  B = R 6 and corresponding point cloud P i  P = R Ni6 (xyz positions and RGB values) in the bounding box with N i number of points.</p>
<p>We propose an approach based on language models, called LanguageRefer, to solve a 3D visual grounding task. Our model focuses on understanding spatial relationships between objects from language descriptions and 3D bounding box information. We chose this approach due to (1) the high Figure 2: Detailed overview of LanguageRefer. A semantic classifier predicts class labels from a 3D point cloud in each bounding box (using color and xyz positions). The language description or utterance (e.g., "Facing the foot of the bed, the bed on the right") is transformed into a sequence of tokens. The input token embedding in DistilBert [1] converts the tokens into embedded feature vectors (green squares). Bounding box position and size information are positional-encoded to form encoded vectors using techniques from [41] (orange squares); they are added to the corresponding embedded feature vectors (green squares). After the addition, our reference model processes the modified features and feeds them to multiple tasks. The main task is a reference task, i.e., it chooses the referred object from the object features. The instance classification task is a binary classification, i.e., it determines whether the given object feature belongs to the target class. Finally, the masking task, commonly used in language modeling, recovers the original token from a randomly replaced token in the utterance. dependency on spatial relationship descriptions in language, and (2) the holistic nature of spatial relationship information, which differs from unary attribute information such as color and shape. As shown in Figure. 2, we use a two-stage approach. First, we determine the class labels of objects in the scene. Second, we use spatial-language embedding to identify the referenced object. The following subsections describe these steps in detail.</p>
<p>Semantic Classification Model and Tokenization. For semantic classification of the point cloud in a bounding box, we employed PointNet++ [43], which achieved 69% accuracy on average in the test dataset. In training and inference, we use a sampled point cloud P i  R 10246 and PointNet++ predicts the semantic labell  L, where L is a set of class labels in text. Each scene has pairs of predicted class labels and bounding box values ((l i , B i ) :l i  L, B i  B) from objects. Predicted labels are concatenated to the utterance U with a separator [SEP] and then split by a tokenizer into a list of indices of tokens: U becomes (u 1 ,    , u t ), and each predicted class labell i becomes
(o i 1 ,    , o i n1 ),
where each token index is in I D and D is the size of the dictionary. Language Model and Token Embedding Generation. Our model uses a pre-trained language model, DistilBert, for the reference task. Transformers consider relationships among all pairs of elements through attention, and they can be effectively leveraged to explain spatial relationships between objects as discrete entities. In our formulation, predicted class labels are considered to be sentences, so they are concatenated to the utterance with separation by [SEP]. Therefore, the final sequence of token indices would be V = ([CLS], u 1 ,    , u t ,
[SEP], o 1 1 ,    , o 1 n1 , [SEP],    , [SEP], o M 1 ,    , o M n M , [SEP]
). Though the token index sequence complies with the specification of Distil-Bert, it violates the number of sentences. Then, we transform V into the token embedding sequence W = (w 1 ,    , w T ), w i  R 768 using DistilBert's word embeddings. For concise notation, we define the indices of utterance tokens in V or W as a mask M U = (2,    , t+1) and the indices of first tokens from objects (o 1 1 ,    , o M 1 ) as a mask M O . We also define a mask operator [] to manipulate specific elements in the sequence; for instance, W [M U ] = 0 empties all utterance embeddings in W .</p>
<p>Spatial-Language Model and Spatial Embeddings. To combine spatial information from raw bounding box values into the token embedding W , we employ sinusoidal positional encoding PE() from [41] to transform the bounding box vector (center position and size) B i  B  R 6 to b i = PE(B i )  R 768 , which is then added to W [M O ]. 1 The token embedding W is then combined with spatial information and finally transformed into the output embedding X = (x 1 ,    , x T ), x i  R 768 by the reference model, which is fine-tuned from the pre-trained DistilBert. The final reference task is performed by the reference classifier from X[M O ], as explained in the following subsection.</p>
<p>Loss Functions. We use three tasks for training and corresponding loss values. First, we use the reference loss, L ref , following the original proposal in [2]. We ask the model to choose one object as the target instance from M candidates. We collect scalar values from X[M O ] by a linear layer and take the argmax on those values to choose the target instance.</p>
<p>Second, we add a binary target classification loss L clf on X[M O ] to determine whether a given object belongs to the target class.</p>
<p>Last, we employ mask loss from language model pre-training, L mask . We randomly replace the tokens from nouns in the utterance with a probability of 15 %. The noun token is replaced by [MASK] with an 80 % chance, by a random token with a 10 % chance, or it remains the same with a probability of 10 %. Then, the model is asked to recover the original token index. We expect the model to fill in the replaced tokens in the utterance by understanding the relationship between objects. We use cross entropy loss for all tasks and compute the final loss as
L = L ref + 0.5L clf + 0.5L mask .(1)
At inference, we followed the approach of InstanceRefer [38] to filter out objects that do not belong to the predicted target class. We used an extra DistilBert-based target classification model of 94 % accuracy that takes the language utterance as input to predict the target class. To reduce the chance of removing the true target instance in the filtering process, the top-k class predictions (from the semantic classifier) for each object are compared to the predicted target class. We use k = 4 throughout the experiments. For masking loss computation, we extracted nouns in the utterance. We used flair [44] for part-of-speech (POS) tagging.</p>
<p>Experiments</p>
<p>Datasets</p>
<p>We evaluated our model on the reference task from ReferIt3D [2] with two datasets, Nr3D (Natural reference in 3D) and Sr3D (Spatial reference in 3D, which contains spatial descriptions only). Both datasets augment ScanNet [10], a reconstructed 3D indoor scene dataset with language descriptions. Nr3D has 41,503 natural language utterances, and Sr3D contains 83,572 template-based utterances, on 707 scenes following the official splits of ScanNet [10]. The datasets have 76 target classes and are designed to have multiple same-class distractors in the scene.</p>
<p>Experiment Settings</p>
<p>ReferIt3D [2] provides the ground-truth bounding boxes of objects in the scene, point clouds of the scene, utterances and corresponding target objects. We measured the accuracy of the model by comparing the object selected from M candidates to the ground-truth target object. When the number of same-class distractors exceeded two, we classified the instance as "hard" according to [2]. The other cases were classified as "easy."</p>
<p>We trained models with a learning rate of 0.0001 using AdamW optimization, warm-up and linear scheduling. Our model was initialized with a pre-trained model of the cased Distilbert base [1] from the Hugging Face implementation [45].</p>
<p>We compared the performance of our model with state-of-the-art methods on ReferIt3D ( [2,40,37,38,39]) based on the reported numbers on the challenge website [46] and corresponding papers.</p>
<p>Since SAT [40] uses an extra 2D image dataset in their training, we separated it from non-SAT, their baseline model that is not trained with the extra dataset.  Table 1: Accuracy on ReferIt3D [2]. Our model outperformed state-of-the-art models on both Nr3D and Sr3D except for models with additional training data (SAT with 2D images). The average performance gap between ours and other models on Sr3D (10.6%) is larger than that on Nr3D (6.3%) since our model uses only spatial reasoning for the reference task. Table 1 shows the accuracy on Nr3D and Sr3D. Our model outperformed other models (without extra training datasets) on both Nr3D and Sr3D. It achieved 43.9% on Nr3D with an +8.3% improvement over the baseline method of ReferIt3D [2] and a 2.2% increase over the best accuracy from other models in Nr3D. For Sr3D, our model achieved 56.0%, which is a 15.2% and 8.0% increase over the baseline and the best accuracy in the Sr3D section, respectively. It is also comparable to the accuracy of SAT (with a 1.9% difference), which was trained with additional 2D image training data. These results prove that our model can accurately reason about spatial relationships from spatial-language embeddings and outperforms other models on both datasets despite the loss of information in appearance. In addition, less diverse language expressions and utterances only about spatial reasoning can explain our model's strong performance on Sr3D.</p>
<p>Evaluation on ReferIt3D [2]</p>
<p>Evaluation with Ground-Truth Class Labels</p>
<p>We further investigated the model's spatial reasoning ability by removing noise in class labels. We replaced predicted class labels with ground-truth class labels, which was possible due to the explicit usage of class labels in our model. Table 2 shows the result with and without ground-truth class labels. The first and second columns demonstrate the type of dataset in training and evaluation, respectively. For instance, the second row shows the evaluation result of the model trained with the Nr3D dataset with ground-truth class labels and evaluated on the Nr3D dataset with predicted class labels. When we trained and evaluated our model with ground-truth labels on Nr3D and Sr3D (row 4 and 8), we achieved 54.3% and 91.1% accuracy, respectively. Compared to the accuracy of models trained and evaluated with noisy labels, we realized an improvement is 10.4% and 35.1%, respectively, for each dataset. When we evaluated with ground-truth labels the models trained with noisy labels, their performance increase was 9.7% and 24.2%, respectively. Multiple reasons account for the difference in performance on Nr3D and Sr3D: diversity in the natural language dataset, a higher portion of view-dependent utterances or view-dependent utterances with no mention of orientation, and descriptions other than spatial relationships, such as color or appearance.</p>
<p>In addition, we examined the accuracy given switched datasets, namely, when we train a model on Nr3D and evaluate it using Sr3D, and vice versa. The last two rows in Table 2 show two switched evaluation results: 40.0% and 37.6% on Sr3D and Nr3D, respectively. The 37.6% accuracy on Nr3D from the model trained on Sr3D is comparable to the accuracy of non-SAT [40] and InstanceRefer [38]. It shows our model's generalizability of spatial reasoning and potential to transfer to different perception modules and datasets.  Table 2: Ablation with ground-truth class labels. First and second columns show types of data used in training and evaluation. The high overall accuracy (91.1% at row 8) of the model both trained and evaluated with ground-truth class labels on Sr3D shows its spatial reasoning ability besides the perception noise. Accuracy gaps between ground-truth and predicted class labels on Nr3D and Sr3D (9.7%, 24.2%, respectively) indirectly tell us about language complexity and information loss due to classification. Transferring the model trained with Sr3D to Nr3D evaluation shows an overall number (37.6% at row 10) comparable to those from other methods. Our model can easily accommodate different classification models or datasets.</p>
<p>Ablation on Loss Terms</p>
<p>We trained models with different combinations of loss terms, and Table 3 shows the results. In addition to three tasks, we examined the effect of a text classification task that predicts the target class label l  L from the tokens from utterance X[M U ].</p>
<p>We found that only the binary classification loss shows its effect clearly (+3.5% from Ref.  Table 3: Ablation of loss terms on Nr3D. The classification loss was effective, the mask loss did not significantly affect accuracy, and the text loss degraded accuracy. We chose the model without text losses (fifth row, in blue).</p>
<p>Viewpoint Annotation</p>
<p>View-dependent utterances without information about original viewpoint make the reference task in ReferIt3D [2] more challenging. For instance, utterances such as "The door is wood with the handle on the left side." assume specific orientations of the agent, and it is impossible to recover the true orientation without knowing the referred object; this differs from view-dependent utterances with explicit viewpoint information, such as "Facing the foot of the bed." However, the original dataset   We assume that the robot is always inside the room except for cases specified by utterances.</p>
<p>of ReferIt3D [2] does not distinguish the utterances without orientation information from those with it. Therefore, we split the view-dependent (VD) utterance category into two subcategories, VDexplicit and VD-implicit, where VD-explicit has explicit viewpoint information in the utterance. We then collected orientations from human annotators that validated the utterances. We set four standard orientations assuming the agent is in the room (around the center of the scene) and asked annotators to select all orientations that could be considered valid from the utterance. Figure 3 shows examples of the four orientations. We found that four orientations were sufficient to recover the original viewpoints of the speakers. In total, 12,680 view-dependent utterances of the Nr3D dataset were annotated; from these, 5,942 utterances were classified as VD-explicit. For train and test split, 10,206 and 2,474 utterances were annotated, respectively.</p>
<p>From the orientation for view-dependent utterances, we revised the dataset with the corrected orientation; we rotated the scene with respect to the annotation so all scenes remained valid at the canonical orientation. For view-independent utterances, we randomly rotated the scenes since they were valid in any direction. Table 4 shows the accuracy values for models trained with and without corrected orientations. At inference, we evaluated each model with and without corrected orientations, as well. The second row shows the accuracy of the model that was trained and evaluated with corrected orientations. Its overall accuracy was improved by +5.1% from the final model without the correction that was used in Table 1 (the last row in Table 4). Note that the improvement on view-independent utterances was marginal (+1.4%), but the improvement on view-dependent ones was significant (+12.7%). The first row shows the accuracy of a model trained with corrected orientations and evaluated on the test data without correction. This model achieved accuracy comparable to the final model (0.4%). This implies the correction helped the model to accurately interpret the view-dependent scene when the orientation was consistently aligned, introducing no unwanted bias.</p>
<p>Conclusion</p>
<p>We proposed LanguageRefer, a spatial-language model for 3D visual grounding in a reference task. LanguageRefer combines language embeddings from utterances and class labels with positionalencoded spatial information for efficient learning of the spatial-language embedding space without different modules for individual modality. Experimental results show that LanguageRefer outperformed state-of-the-art models on ReferIt3D with no additional training data. Analysis and ablations we performed demonstrate the effects of 1) noisy class labels, 2) arbitrary viewpoints in view-dependent utterances, 3) and the transfer of our model to different datasets for future robotics applications.</p>
<p>Figure 3 :
3Examples of standard orientations for viewpoint annotation on Nr3D (a-d).</p>
<p>Table 4 :
4Comparison of accuracy with and without corrected orientations on Nr3D.Correction </p>
<p>Overall 
Easy 
Hard 
View-dep. View-indep. 
Training Evaluation </p>
<p>-
43.5 % 50.7 % 36.0 % 
37.0 % 
46.6 % 
49.0 % 56.0 % 41.8 % 
54.4 % 
46.4 % 
-
43.1 % 50.3 % 35.6 % 
40.4 % 
44.4 % 
-
-
43.9 % 51.0 % 36.6 % 
41.7 % 
45.0 % </p>
<p>Each value in bi is extended to 128-dimensions by PE and then concatenated to form a 768-dimensional vector.
AcknowledgmentsWe would like to thank to Xiangyun Meng and Mohit Shridhar for discussion and feedback. This work is in part supported by NSF IIS 1652052, IIS 17303166, DARPA N66001-19-2-4031, DARPA W911NF-15-1-0543, Honda Research Institute as part of the Curious Minded Machine initiative, and gifts from Allen Institute for Artificial Intelligence.
V Sanh, L Debut, J Chaumond, T Wolf, arXiv:1910.01108Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter. arXiv preprintV. Sanh, L. Debut, J. Chaumond, and T. Wolf. Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter. arXiv preprint arXiv:1910.01108, 2019.</p>
<p>Referit3d: Neural listeners for fine-grained 3d object identification in real-world scenes. P Achlioptas, A Abdelreheem, F Xia, M Elhoseiny, L Guibas, 16th European Conference on Computer Vision (ECCV). 2020P. Achlioptas, A. Abdelreheem, F. Xia, M. Elhoseiny, and L. Guibas. Referit3d: Neural listen- ers for fine-grained 3d object identification in real-world scenes. 16th European Conference on Computer Vision (ECCV), 2020.</p>
<p>Clevr: A diagnostic dataset for compositional language and elementary visual reasoning. J Johnson, B Hariharan, L Van Der Maaten, L Fei-Fei, C L Zitnick, R Girshick, CVPR. J. Johnson, B. Hariharan, L. van der Maaten, L. Fei-Fei, C. L. Zitnick, and R. Girshick. Clevr: A diagnostic dataset for compositional language and elementary visual reasoning. In CVPR, 2017.</p>
<p>CLEVRER: collision events for video representation and reasoning. K Yi, C Gan, Y Li, P Kohli, J Wu, A Torralba, J B Tenenbaum, ICLR. K. Yi, C. Gan, Y. Li, P. Kohli, J. Wu, A. Torralba, and J. B. Tenenbaum. CLEVRER: collision events for video representation and reasoning. In ICLR, 2020.</p>
<p>CATER: A diagnostic dataset for Compositional Actions and TEmporal Reasoning. R Girdhar, D Ramanan, ICLR. R. Girdhar and D. Ramanan. CATER: A diagnostic dataset for Compositional Actions and TEmporal Reasoning. In ICLR, 2020.</p>
<p>Embodied question answering. A Das, S Datta, G Gkioxari, S Lee, D Parikh, D Batra, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionA. Das, S. Datta, G. Gkioxari, S. Lee, D. Parikh, and D. Batra. Embodied question answering. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 1-10, 2018.</p>
<p>Learning transferable visual models from natural language supervision. A Radford, J W Kim, C Hallacy, A Ramesh, G Goh, S Agarwal, G Sastry, A Askell, P Mishkin, J Clark, arXiv:2103.00020arXiv preprintA. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, et al. Learning transferable visual models from natural language supervi- sion. arXiv preprint arXiv:2103.00020, 2021.</p>
<p>Alfred: A benchmark for interpreting grounded instructions for everyday tasks. M Shridhar, J Thomason, D Gordon, Y Bisk, W Han, R Mottaghi, L Zettlemoyer, D Fox, M. Shridhar, J. Thomason, D. Gordon, Y. Bisk, W. Han, R. Mottaghi, L. Zettlemoyer, and D. Fox. Alfred: A benchmark for interpreting grounded instructions for everyday tasks. 2020</p>
<p>IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 10737- 10746, 2020.</p>
<p>E Kolve, R Mottaghi, W Han, E Vanderbilt, L Weihs, A Herrasti, D Gordon, Y Zhu, A Gupta, A Farhadi, AI2-THOR: An Interactive 3D Environment for Visual AI. arXiv. E. Kolve, R. Mottaghi, W. Han, E. VanderBilt, L. Weihs, A. Herrasti, D. Gordon, Y. Zhu, A. Gupta, and A. Farhadi. AI2-THOR: An Interactive 3D Environment for Visual AI. arXiv, 2017.</p>
<p>Scannet: Richlyannotated 3d reconstructions of indoor scenes. A Dai, A X Chang, M Savva, M Halber, T Funkhouser, M Niener, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionA. Dai, A. X. Chang, M. Savva, M. Halber, T. Funkhouser, and M. Niener. Scannet: Richly- annotated 3d reconstructions of indoor scenes. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 5828-5839, 2017.</p>
<p>Object-based attention for spatio-temporal reasoning: Outperforming neuro-symbolic models with flexible distributed architectures. D Ding, F Hill, A Santoro, M Botvinick, arXiv:2012.08508arXiv preprintD. Ding, F. Hill, A. Santoro, and M. Botvinick. Object-based attention for spatio-temporal rea- soning: Outperforming neuro-symbolic models with flexible distributed architectures. arXiv preprint arXiv:2012.08508, 2020.</p>
<p>Mdetr-modulated detection for end-to-end multi-modal understanding. A Kamath, M Singh, Y Lecun, I Misra, G Synnaeve, N Carion, arXiv:2104.12763arXiv preprintA. Kamath, M. Singh, Y. LeCun, I. Misra, G. Synnaeve, and N. Carion. Mdetr-modulated detection for end-to-end multi-modal understanding. arXiv preprint arXiv:2104.12763, 2021.</p>
<p>Vision-and-language navigation: Interpreting visually-grounded navigation instructions in real environments. P Anderson, Q Wu, D Teney, J Bruce, M Johnson, N Snderhauf, I Reid, S Gould, A V Hengel, IEEE/CVF Conference on Computer Vision and Pattern Recognition. P. Anderson, Q. Wu, D. Teney, J. Bruce, M. Johnson, N. Snderhauf, I. Reid, S. Gould, and A. V. Hengel. Vision-and-language navigation: Interpreting visually-grounded navigation in- structions in real environments. 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3674-3683, 2018.</p>
<p>Room-across-room: Multilingual visionand-language navigation with dense spatiotemporal grounding. A Ku, P Anderson, R Patel, E Ie, J Baldridge, EMNLP. A. Ku, P. Anderson, R. Patel, E. Ie, and J. Baldridge. Room-across-room: Multilingual vision- and-language navigation with dense spatiotemporal grounding. In EMNLP, 2020.</p>
<p>Reverie: Remote embodied visual referring expression in real indoor environments. Y Qi, Q Wu, P Anderson, X Wang, W Wang, C Shen, A V Hengel, IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Y. Qi, Q. Wu, P. Anderson, X. Wang, W. Wang, C. Shen, and A. V. Hengel. Reverie: Remote embodied visual referring expression in real indoor environments. 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 9979-9988, 2020.</p>
<p>Topological planning with transformers for vision-and-language navigation. K Chen, J K Chen, J Chuang, M , S Savarese, abs/2012.05292ArXiv. K. Chen, J. K. Chen, J. Chuang, M. V'azquez, and S. Savarese. Topological planning with transformers for vision-and-language navigation. ArXiv, abs/2012.05292, 2020.</p>
<p>Habitat: A platform for embodied ai research. M Savva, A Kadian, O Maksymets, Y Zhao, E Wijmans, B Jain, J Straub, J Liu, V Koltun, J Malik, D Parikh, D Batra, M. Savva, A. Kadian, O. Maksymets, Y. Zhao, E. Wijmans, B. Jain, J. Straub, J. Liu, V. Koltun, J. Malik, D. Parikh, and D. Batra. Habitat: A platform for embodied ai research. 2019</p>
<p>IEEE/CVF International Conference on Computer Vision (ICCV). IEEE/CVF International Conference on Computer Vision (ICCV), pages 9338-9346, 2019.</p>
<p>Self-monitoring navigation agent via auxiliary progress estimation. C.-Y Ma, J Lu, Z Wu, G Al-Regib, Z Kira, R Socher, C Xiong, abs/1901.03035ArXiv. C.-Y. Ma, J. Lu, Z. Wu, G. Al-Regib, Z. Kira, R. Socher, and C. Xiong. Self-monitoring navigation agent via auxiliary progress estimation. ArXiv, abs/1901.03035, 2019.</p>
<p>Tactical rewind: Self-correction via backtracking in vision-and-language navigation. L Ke, X Li, Y Bisk, A Holtzman, Z Gan, J Liu, J Gao, Y Choi, S Srinivasa, IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). L. Ke, X. Li, Y. Bisk, A. Holtzman, Z. Gan, J. Liu, J. Gao, Y. Choi, and S. Srinivasa. Tactical rewind: Self-correction via backtracking in vision-and-language navigation. 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 6734-6742, 2019.</p>
<p>On evaluation of embodied navigation agents. P Anderson, A X Chang, D S Chaplot, A Dosovitskiy, S Gupta, V Koltun, J Kosecka, J Malik, R Mottaghi, M Savva, A Zamir, abs/1807.06757ArXiv. P. Anderson, A. X. Chang, D. S. Chaplot, A. Dosovitskiy, S. Gupta, V. Koltun, J. Kosecka, J. Malik, R. Mottaghi, M. Savva, and A. Zamir. On evaluation of embodied navigation agents. ArXiv, abs/1807.06757, 2018.</p>
<p>Speaker-follower models for vision-and-language navigation. D Fried, R Hu, V Cirik, A Rohrbach, J Andreas, L.-P Morency, T Berg-Kirkpatrick, K Saenko, D Klein, T Darrell, NeurIPS. D. Fried, R. Hu, V. Cirik, A. Rohrbach, J. Andreas, L.-P. Morency, T. Berg-Kirkpatrick, K. Saenko, D. Klein, and T. Darrell. Speaker-follower models for vision-and-language navi- gation. In NeurIPS, 2018.</p>
<p>A recurrent vision-and-language bert for navigation. ArXiv, abs. Y Hong, Q Wu, Y Qi, C Rodriguez-Opazo, S Gould, Y. Hong, Q. Wu, Y. Qi, C. Rodriguez-Opazo, and S. Gould. A recurrent vision-and-language bert for navigation. ArXiv, abs/2011.13922, 2020.</p>
<p>A Shrivastava, K Gopalakrishnan, Y Liu, R Piramuthu, G Tur, D Parikh, D Hakkani-Tur, Visitron: Visual semantics-aligned interactively trained object-navigator. ArXiv, abs/2105.11589. A. Shrivastava, K. Gopalakrishnan, Y. Liu, R. Piramuthu, G. Tur, D. Parikh, and D. Hakkani- Tur. Visitron: Visual semantics-aligned interactively trained object-navigator. ArXiv, abs/2105.11589, 2021.</p>
<p>Simto-real transfer for vision-and-language navigation. P Anderson, A Shrivastava, J Truong, A Majumdar, D Parikh, D Batra, S Lee, arXiv:2011.03807arXiv preprintP. Anderson, A. Shrivastava, J. Truong, A. Majumdar, D. Parikh, D. Batra, and S. Lee. Sim- to-real transfer for vision-and-language navigation. arXiv preprint arXiv:2011.03807, 2020.</p>
<p>Beyond the nav-graph: Visionand-language navigation in continuous environments. J Krantz, E Wijmans, A Majumdar, D Batra, S Lee, European Conference on Computer Vision. SpringerJ. Krantz, E. Wijmans, A. Majumdar, D. Batra, and S. Lee. Beyond the nav-graph: Vision- and-language navigation in continuous environments. In European Conference on Computer Vision, pages 104-120. Springer, 2020.</p>
<p>Few-shot object grounding and mapping for natural language robot instruction following. ArXiv, abs. V Blukis, R A Knepper, Y Artzi, V. Blukis, R. A. Knepper, and Y. Artzi. Few-shot object grounding and mapping for natural language robot instruction following. ArXiv, abs/2011.07384, 2020.</p>
<p>J Roh, C Paxton, A Pronobis, A Farhadi, D Fox, abs/1910.07615Conditional driving from natural language instructions. J. Roh, C. Paxton, A. Pronobis, A. Farhadi, and D. Fox. Conditional driving from natural language instructions. ArXiv, abs/1910.07615, 2019.</p>
<p>Hierarchical cross-modal agent for robotics vision-andlanguage navigation. M Z Irshad, C.-Y. Ma, Z Kira, abs/2104.10674ArXiv. M. Z. Irshad, C.-Y. Ma, and Z. Kira. Hierarchical cross-modal agent for robotics vision-and- language navigation. ArXiv, abs/2104.10674, 2021.</p>
<p>Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence models. B A Plummer, L Wang, C M Cervantes, J C Caicedo, J Hockenmaier, S Lazebnik, International Journal of Computer Vision. 123B. A. Plummer, L. Wang, C. M. Cervantes, J. C. Caicedo, J. Hockenmaier, and S. Lazebnik. Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence models. International Journal of Computer Vision, 123:74-93, 2015.</p>
<p>Referitgame: Referring to objects in photographs of natural scenes. S Kazemzadeh, V Ordonez, M Matten, T L Berg, EMNLP. S. Kazemzadeh, V. Ordonez, M. Matten, and T. L. Berg. Referitgame: Referring to objects in photographs of natural scenes. In EMNLP, 2014.</p>
<p>Generation and comprehension of unambiguous object descriptions. J Mao, J Huang, A Toshev, O.-M Camburu, A Yuille, K Murphy, IEEE Conference on Computer Vision and Pattern Recognition (CVPR). J. Mao, J. Huang, A. Toshev, O.-M. Camburu, A. Yuille, and K. Murphy. Generation and comprehension of unambiguous object descriptions. 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 11-20, 2016.</p>
<p>Mattnet: Modular attention network for referring expression comprehension. L Yu, Z L Lin, X Shen, J Yang, X Lu, M Bansal, T L Berg, IEEE/CVF Conference on Computer Vision and Pattern Recognition. L. Yu, Z. L. Lin, X. Shen, J. Yang, X. Lu, M. Bansal, and T. L. Berg. Mattnet: Modular attention network for referring expression comprehension. 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1307-1315, 2018.</p>
<p>Dynamic graph attention for referring expression comprehension. S Yang, G Li, Y Yu, IEEE/CVF International Conference on Computer Vision (ICCV). S. Yang, G. Li, and Y. Yu. Dynamic graph attention for referring expression comprehen- sion. 2019 IEEE/CVF International Conference on Computer Vision (ICCV), pages 4643- 4652, 2019.</p>
<p>Modeling context in referring expressions. L Yu, P Poirson, S Yang, A Berg, T L Berg, abs/1608.00272ArXiv. L. Yu, P. Poirson, S. Yang, A. Berg, and T. L. Berg. Modeling context in referring expressions. ArXiv, abs/1608.00272, 2016.</p>
<p>Learning cross-modal context graph for visual grounding. Y Liu, B Wan, X.-D Zhu, X He, abs/1911.09042ArXiv. Y. Liu, B. Wan, X.-D. Zhu, and X. He. Learning cross-modal context graph for visual ground- ing. ArXiv, abs/1911.09042, 2020.</p>
<p>Cross-modal self-attention network for referring image segmentation. L Ye, M Rochan, Z Liu, Y Wang, L. Ye, M. Rochan, Z. Liu, and Y. Wang. Cross-modal self-attention network for referring image segmentation, 2019.</p>
<p>D Z Chen, A X Chang, M Niener, Scanrefer: 3d object localization in rgb-d scans using natural language. 16th European Conference on Computer Vision (ECCV). 2020D. Z. Chen, A. X. Chang, and M. Niener. Scanrefer: 3d object localization in rgb-d scans using natural language. 16th European Conference on Computer Vision (ECCV), 2020.</p>
<p>Instancerefer: Cooperative holistic understanding for visual grounding on point clouds through instance multi-level contextual referring. Z Yuan, X Yan, Y Liao, R Zhang, Z Li, S Cui, Z. Yuan, X. Yan, Y. Liao, R. Zhang, Z. Li, and S. Cui. Instancerefer: Cooperative holistic understanding for visual grounding on point clouds through instance multi-level contextual referring, 2021.</p>
<p>Freeform description guided 3d visual graph network for object grounding in point cloud. M Feng, Z Li, Q Li, L Zhang, X Zhang, G Zhu, H Zhang, Y Wang, A Mian, M. Feng, Z. Li, Q. Li, L. Zhang, X. Zhang, G. Zhu, H. Zhang, Y. Wang, and A. Mian. Free- form description guided 3d visual graph network for object grounding in point cloud, 2021.</p>
<p>SAT: 2d semantics assisted training for 3d visual grounding. CoRR, abs/2105.11450. Z Yang, S Zhang, L Wang, J Luo, Z. Yang, S. Zhang, L. Wang, and J. Luo. SAT: 2d semantics assisted training for 3d visual grounding. CoRR, abs/2105.11450, 2021. URL https://arxiv.org/abs/2105.11450.</p>
<p>A Vaswani, N Shazeer, N Parmar, J Uszkoreit, L Jones, A N Gomez, L Kaiser, I Polosukhin, arXiv:1706.03762Attention is all you need. arXiv preprintA. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polo- sukhin. Attention is all you need. arXiv preprint arXiv:1706.03762, 2017.</p>
<p>J Devlin, M.-W Chang, K Lee, K Toutanova, Bert, arXiv:1810.04805Pre-training of deep bidirectional transformers for language understanding. arXiv preprintJ. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.</p>
<p>Pointnet++: Deep hierarchical feature learning on point sets in a metric space. C R Qi, L Yi, H Su, L J Guibas, NIPS. C. R. Qi, L. Yi, H. Su, and L. J. Guibas. Pointnet++: Deep hierarchical feature learning on point sets in a metric space. In NIPS, 2017.</p>
<p>Flair: An easy-touse framework for state-of-the-art nlp. A Akbik, T Bergmann, D Blythe, K Rasul, S Schweter, R Vollgraf, NAACL 2019, 2019 Annual Conference of the North American Chapter of the Association for Computational Linguistics (Demonstrations). A. Akbik, T. Bergmann, D. Blythe, K. Rasul, S. Schweter, and R. Vollgraf. Flair: An easy-to- use framework for state-of-the-art nlp. In NAACL 2019, 2019 Annual Conference of the North American Chapter of the Association for Computational Linguistics (Demonstrations), pages 54-59, 2019.</p>
<p>. T Wolf, L Debut, V Sanh, J Chaumond, C Delangue, A Moi, P Cistac, T Rault, R Louf, M Funtowicz, J Davison, S Shleifer, P Von Platen, C Ma, Y Jernite, J Plu, C Xu, T L Scao, S Gugger, M Drame, Q Lhoest, A M Rush, Huggingface's transformers: Stateof-the-art natural language processingT. Wolf, L. Debut, V. Sanh, J. Chaumond, C. Delangue, A. Moi, P. Cistac, T. Rault, R. Louf, M. Funtowicz, J. Davison, S. Shleifer, P. von Platen, C. Ma, Y. Jernite, J. Plu, C. Xu, T. L. Scao, S. Gugger, M. Drame, Q. Lhoest, and A. M. Rush. Huggingface's transformers: State- of-the-art natural language processing, 2020.</p>
<p>. P Achlioptas, Referit3d benchmarksP. Achlioptas. Referit3d benchmarks. URL https://referit3d.github.io/ benchmarks.html.</p>            </div>
        </div>

    </div>
</body>
</html>
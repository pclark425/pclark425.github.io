<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7159 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7159</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7159</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-134.html">extraction-schema-134</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform self‑reflection or self‑critique, including the specific reflection method, number of generate‑then‑reflect iterations, tasks or benchmarks evaluated, performance before and after reflection, evaluation metrics, and any reported limitations or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-3fa70115248377c3d1517c9f978791a296fbc1dd</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/3fa70115248377c3d1517c9f978791a296fbc1dd" target="_blank">Large Language Models Can Self-Improve</a></p>
                <p><strong>Paper Venue:</strong> Conference on Empirical Methods in Natural Language Processing</p>
                <p><strong>Paper TL;DR:</strong> This work uses a pre-trained LLM to generate "high-confidence"rationale-augmented answers for unlabeled questions using Chain-of-Thought prompting and self-consistency, and fine-tune the LLM using those self-generated solutions as target outputs to improve the general reasoning ability.</p>
                <p><strong>Paper Abstract:</strong> Large Language Models (LLMs) have achieved excellent performances in various tasks. However, fine-tuning an LLM requires extensive supervision. Human, on the other hand, may improve their reasoning abilities by self-thinking without external inputs. In this work, we demonstrate that an LLM is also capable of self-improving with only unlabeled datasets. We use a pre-trained LLM to generate"high-confidence"rationale-augmented answers for unlabeled questions using Chain-of-Thought prompting and self-consistency, and fine-tune the LLM using those self-generated solutions as target outputs. We show that our approach improves the general reasoning ability of a 540B-parameter LLM (74.4%->82.1% on GSM8K, 78.2%->83.0% on DROP, 90.0%->94.4% on OpenBookQA, and 63.4%->67.9% on ANLI-A3) and achieves state-of-the-art-level performance, without any ground truth label. We conduct ablation studies and show that fine-tuning on reasoning is critical for self-improvement.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7159.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7159.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform self‑reflection or self‑critique, including the specific reflection method, number of generate‑then‑reflect iterations, tasks or benchmarks evaluated, performance before and after reflection, evaluation metrics, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LMSI-GSM8K</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Language Model Self-Improved (LMSI) on GSM8K via Self-Consistency + CoT self-training</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper's self-improvement procedure: sample many Chain-of-Thought (CoT) reasoning paths, select the majority (self-consistent) answer and associated rationales, augment into mixed formats, and fine-tune the model on these self-generated rationale-augmented outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PaLM-540B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Autoregressive Transformer-based large language model (pretrained), used with few-shot CoT prompting and self-consistency sampling; fine-tuned for LMSI.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>540B</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Self-Consistency majority-voting + CoT self-training (LMSI)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Generate m diverse Chain-of-Thought reasoning paths per question (sampled decoding, T>0); pick the most-consistent answer by majority vote (self-consistency); keep all reasoning paths that lead to the majority answer; augment those paths into mixed prompt/answer formats and fine-tune the model on this self-generated rationale-augmented dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>iteration_type</strong></td>
                            <td>voting over multiple samples followed by fine-tuning (generate-then-train)</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td>1</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>GSM8K</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Grade-school math word problem reasoning (arithmetic reasoning).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_before_reflection</strong></td>
                            <td>74.4% (Self-Consistency, PaLM-540B pre-LMSI)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_after_reflection</strong></td>
                            <td>82.1% (Self-Consistency, PaLM-540B after LMSI)</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Majority-voted (self-consistent) wrong answers can still occur; authors report that such incorrect majority answers tend to have lower confidence (supported by fewer paths) so they introduce limited noise. Method relies on strong base model capacity (larger models benefit more); requires compute to generate many paths and to fine-tune (they use m=32 and fine-tune 10k steps).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models Can Self-Improve', 'publication_date_yy_mm': '2022-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7159.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7159.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform self‑reflection or self‑critique, including the specific reflection method, number of generate‑then‑reflect iterations, tasks or benchmarks evaluated, performance before and after reflection, evaluation metrics, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LMSI-DROP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LMSI on DROP via Self-Consistency + CoT self-training</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Same LMSI procedure applied to DROP (numerical reading comprehension) yielding improved accuracy after fine-tuning on self-generated CoT solutions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PaLM-540B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Autoregressive Transformer-based large language model (pretrained), used with few-shot CoT prompting and self-consistency; fine-tuned for LMSI.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>540B</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Self-Consistency majority-voting + CoT self-training (LMSI)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Generate m CoT paths (m=32), majority-vote for the most consistent answer, keep paths that support that answer, augment formats and fine-tune.</td>
                        </tr>
                        <tr>
                            <td><strong>iteration_type</strong></td>
                            <td>voting over multiple samples followed by fine-tuning (generate-then-train)</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td>1</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>DROP (dev subset)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Reading comprehension benchmark requiring discrete and numerical reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_before_reflection</strong></td>
                            <td>78.2% (Self-Consistency, PaLM-540B pre-LMSI)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_after_reflection</strong></td>
                            <td>83.0% (Self-Consistency, PaLM-540B after LMSI)</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Authors note LMSI is competitive with supervised methods but cannot fully match methods using ground-truth labels (e.g., OPERA). Requires reliable CoT generations; risk of noise from incorrect self-labeled examples though confidence filtering mitigates this.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models Can Self-Improve', 'publication_date_yy_mm': '2022-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7159.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7159.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform self‑reflection or self‑critique, including the specific reflection method, number of generate‑then‑reflect iterations, tasks or benchmarks evaluated, performance before and after reflection, evaluation metrics, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LMSI-OpenBookQA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LMSI on OpenBookQA via Self-Consistency + CoT self-training</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>LMSI applied to multiple-choice commonsense reasoning (OpenBookQA) using generated CoT paths filtered by majority vote and used for fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PaLM-540B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Autoregressive Transformer-based large language model (pretrained), used with few-shot CoT prompting and self-consistency; fine-tuned for LMSI.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>540B</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Self-Consistency majority-voting + CoT self-training (LMSI)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Sample CoT outputs (m=32), majority-vote to select the high-confidence answer, keep rationales supporting it, augment and fine-tune.</td>
                        </tr>
                        <tr>
                            <td><strong>iteration_type</strong></td>
                            <td>voting over multiple samples followed by fine-tuning (generate-then-train)</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td>1</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>OpenBookQA</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Multiple-choice commonsense reasoning question set requiring domain knowledge and reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_before_reflection</strong></td>
                            <td>90.0% (Self-Consistency, PaLM-540B pre-LMSI)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_after_reflection</strong></td>
                            <td>94.4% (Self-Consistency, PaLM-540B after LMSI)</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Method achieves SOTA-level performance without ground-truth labels; paper notes dependence on quality of generated CoT and that larger models benefit more from LMSI.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models Can Self-Improve', 'publication_date_yy_mm': '2022-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7159.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7159.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform self‑reflection or self‑critique, including the specific reflection method, number of generate‑then‑reflect iterations, tasks or benchmarks evaluated, performance before and after reflection, evaluation metrics, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LMSI-ANLI-A3</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LMSI on ANLI-A3 (Adversarial NLI) via Self-Consistency + CoT self-training</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Application of LMSI to adversarial natural language inference (ANLI-A3) using self-generated CoT rationales filtered with majority voting and fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PaLM-540B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Autoregressive Transformer-based large language model (pretrained), used with few-shot CoT prompting and self-consistency; fine-tuned for LMSI.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>540B</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Self-Consistency majority-voting + CoT self-training (LMSI)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Generate multiple CoT sequences per question, majority-vote, keep rationales aligned with majority answer, augment and fine-tune.</td>
                        </tr>
                        <tr>
                            <td><strong>iteration_type</strong></td>
                            <td>voting over multiple samples followed by fine-tuning (generate-then-train)</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td>1</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>ANLI-A3</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Adversarial natural language inference (hard subset), three-way classification (entailment/neutral/contradiction).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_before_reflection</strong></td>
                            <td>63.4% (Self-Consistency, PaLM-540B pre-LMSI)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_after_reflection</strong></td>
                            <td>67.9% (Self-Consistency, PaLM-540B after LMSI)</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Improvement is modest but consistent; authors caution that self-labeled training may introduce noise when the model's majority is wrong, but they find confidence correlates with correctness so high-confidence examples dominate.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models Can Self-Improve', 'publication_date_yy_mm': '2022-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7159.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e7159.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform self‑reflection or self‑critique, including the specific reflection method, number of generate‑then‑reflect iterations, tasks or benchmarks evaluated, performance before and after reflection, evaluation metrics, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LMSI-OOD-multi</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LMSI multi-task self-training generalization to Out-of-Domain tasks</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Train one LMSI checkpoint on a mixture of in-domain questions and evaluate generalization on unseen OOD benchmarks; shows coherent accuracy gains across arithmetic, commonsense, and NLI OOD tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PaLM-540B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Autoregressive Transformer-based large language model (pretrained), fine-tuned on a mixture of self-generated CoT outputs from multiple in-domain datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>540B</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Self-Consistency majority-voting + CoT self-training (multi-task LMSI)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Combine self-generated CoT training data from multiple in-domain datasets (GSM8K, DROP, ARC-c, OpenBookQA, ANLI-A2/A3), fine-tune a single checkpoint; evaluate on OOD benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>iteration_type</strong></td>
                            <td>voting over multiple samples followed by multi-task fine-tuning</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td>1</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>OOD set: AQUA, SVAMP, StrategyQA, ANLI-A1, RTE, MNLI</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Out-of-domain arithmetic, commonsense, and NLI benchmarks used to evaluate generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_before_reflection</strong></td>
                            <td>AQUA 35.8%, SVAMP 79.0%, StrategyQA 75.3%, ANLI-A1 68.8%, RTE 79.1%, MNLI-M/MM 72.0%/74.0% (CoT-prompting, pre-LMSI)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_after_reflection</strong></td>
                            <td>AQUA 39.0%, SVAMP 82.8%, StrategyQA 77.8%, ANLI-A1 79.2%, RTE 80.1%, MNLI-M/MM 81.8%/82.2% (CoT-prompting, post-LMSI)</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Some OOD tasks differ significantly from in-domain data and still require specific prompts; authors note improvements but not uniform elimination of domain gaps. Effective generalization depends on diversity of self-generated training examples.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models Can Self-Improve', 'publication_date_yy_mm': '2022-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7159.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e7159.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform self‑reflection or self‑critique, including the specific reflection method, number of generate‑then‑reflect iterations, tasks or benchmarks evaluated, performance before and after reflection, evaluation metrics, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-Generated-Questions</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-generation of training questions + LMSI fine-tuning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>LLM generates additional training questions by prompting with concatenated seed questions and keeping only generated questions whose answers are high-confidence under self-consistency; these generated questions are then used with LMSI.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PaLM-540B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Autoregressive Transformer used to self-generate new questions and then to generate CoT answers for filtering and fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>540B</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Self-generation of questions + Self-Consistency filtering + CoT self-training</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Concatenate seed questions to prompt model to auto-generate new questions (no ground-truth); apply multi-path CoT sampling and self-consistency to keep only high-confidence generated questions; generate CoT rationales for those and fine-tune.</td>
                        </tr>
                        <tr>
                            <td><strong>iteration_type</strong></td>
                            <td>generate training data (questions) -> generate CoT answers -> filter by self-consistency -> fine-tune</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td>1</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>GSM8K (few-shot setting)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Arithmetic reasoning; tested in low-resource/few-shot setting where only 10 real questions provided and many generated questions used for LMSI.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_before_reflection</strong></td>
                            <td>56.5% (CoT-Prompting, PaLM-540B pre-LMSI)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_after_reflection</strong></td>
                            <td>66.2% (CoT-Prompting, LMSI trained on self-generated questions); using full training-set questions gives 73.5%</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Self-generated questions help but do not match the gains from using real training-set questions; quality of generated questions matters and requires additional filtering.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models Can Self-Improve', 'publication_date_yy_mm': '2022-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7159.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e7159.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform self‑reflection or self‑critique, including the specific reflection method, number of generate‑then‑reflect iterations, tasks or benchmarks evaluated, performance before and after reflection, evaluation metrics, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-Generated-CoT-Prompts</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-generated few-shot Chain-of-Thought prompts (Step-by-Step) used at inference</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Generate few-shot CoT exemplars via the model itself (using 'A: Let's think step by step.'), then use those exemplars as few-shot CoT prompts for multi-path decoding; improves zero-shot performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PaLM-540B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Autoregressive Transformer; used to generate CoT exemplars (Step-by-Step) that are then used as few-shot exemplars for other questions.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>540B</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Self-generated CoT exemplars (Step-by-Step) + multi-path decoding</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Greedy 'Step-by-Step' prompting to generate CoT exemplars; collect many diverse generated exemplars, use them as few-shot CoT prompts and perform multi-path sampling + self-consistency at test time.</td>
                        </tr>
                        <tr>
                            <td><strong>iteration_type</strong></td>
                            <td>generate exemplars -> use exemplars for multi-path generation and self-consistency (no fine-tune required)</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td>1</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>GSM8K (zero-shot / few-shot)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Grade-school math problems; used to evaluate zero-shot/few-shot CoT prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_before_reflection</strong></td>
                            <td>Step-by-Step (Kojima) baseline: 53.8% (10 paths) and 70.1% (40 paths) [baseline numbers referenced in paper]</td>
                        </tr>
                        <tr>
                            <td><strong>performance_after_reflection</strong></td>
                            <td>Few-Shot with self-generated Step-by-Step exemplars: 66.2% (10 paths) and 74.2% (40 paths)</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Greedy-generated CoT exemplars alone have limited accuracy (e.g., greedy Step-by-Step ~43%), but diversity (many generated exemplars) and multi-path decoding mitigate this; method may require generating many exemplars (computational cost).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models Can Self-Improve', 'publication_date_yy_mm': '2022-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7159.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e7159.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform self‑reflection or self‑critique, including the specific reflection method, number of generate‑then‑reflect iterations, tasks or benchmarks evaluated, performance before and after reflection, evaluation metrics, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>UL2-LMSI</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Application of LMSI to UL2 model</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Demonstration that the LMSI self-training pipeline (generate multiple CoT paths, majority-vote, fine-tune on self-generated rationales) also yields improvements on a public UL2 checkpoint, albeit smaller than for a very large model.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>UL2 (pretrained checkpoint at step 2,650,000)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>UL2 family model (public research model); authors use a specific UL2 checkpoint and apply the same LMSI pipeline with m=40 generation.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Self-Consistency majority-voting + CoT self-training (LMSI applied to UL2)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Generate m=40 CoT reasoning paths per question for majority voting (self-consistency), keep paths supporting majority answer, augment and fine-tune UL2 on these self-generated outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>iteration_type</strong></td>
                            <td>voting over multiple samples followed by fine-tuning (generate-then-train)</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td>1</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>GSM8K, DROP, ARC-c, OpenBookQA, ANLI-A2, ANLI-A3 (same set as main experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same reasoning and NLI benchmarks as main experiments but using UL2 base model.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>accuracy (and equation-corrected accuracy for arithmetic)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_before_reflection</strong></td>
                            <td>Example (Self-Consistency pre-LMSI): GSM8K 6.4% / 9.9% (exact / eq-corrected); DROP 16.8% / 26.5%; ARC-c 54.9%; OpenBookQA 54.0%; ANLI-A2 37.4%; ANLI-A3 36.8%</td>
                        </tr>
                        <tr>
                            <td><strong>performance_after_reflection</strong></td>
                            <td>Example (Self-Consistency post-LMSI): GSM8K 7.9% / 10.2% (exact / eq-corrected); DROP 18.1% / 28.1%; ARC-c 54.7%; OpenBookQA 55.2%; ANLI-A2 38.1%; ANLI-A3 37.4%</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Improvements on UL2 are modest compared to PaLM-540B, suggesting LMSI benefits scale with base model capacity; authors attribute smaller gains to model size/capacity limits.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models Can Self-Improve', 'publication_date_yy_mm': '2022-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Self-consistency improves chain of thought reasoning in language models <em>(Rating: 2)</em></li>
                <li>Chain of thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Large language models are zero-shot reasoners <em>(Rating: 2)</em></li>
                <li>Star: Bootstrapping reasoning with reasoning <em>(Rating: 1)</em></li>
                <li>On the advance of making language models better reasoners <em>(Rating: 1)</em></li>
                <li>Self-training: A survey <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7159",
    "paper_id": "paper-3fa70115248377c3d1517c9f978791a296fbc1dd",
    "extraction_schema_id": "extraction-schema-134",
    "extracted_data": [
        {
            "name_short": "LMSI-GSM8K",
            "name_full": "Language Model Self-Improved (LMSI) on GSM8K via Self-Consistency + CoT self-training",
            "brief_description": "The paper's self-improvement procedure: sample many Chain-of-Thought (CoT) reasoning paths, select the majority (self-consistent) answer and associated rationales, augment into mixed formats, and fine-tune the model on these self-generated rationale-augmented outputs.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "PaLM-540B",
            "model_description": "Autoregressive Transformer-based large language model (pretrained), used with few-shot CoT prompting and self-consistency sampling; fine-tuned for LMSI.",
            "model_size": "540B",
            "reflection_method_name": "Self-Consistency majority-voting + CoT self-training (LMSI)",
            "reflection_method_description": "Generate m diverse Chain-of-Thought reasoning paths per question (sampled decoding, T&gt;0); pick the most-consistent answer by majority vote (self-consistency); keep all reasoning paths that lead to the majority answer; augment those paths into mixed prompt/answer formats and fine-tune the model on this self-generated rationale-augmented dataset.",
            "iteration_type": "voting over multiple samples followed by fine-tuning (generate-then-train)",
            "num_iterations": 1,
            "task_name": "GSM8K",
            "task_description": "Grade-school math word problem reasoning (arithmetic reasoning).",
            "evaluation_metric": "accuracy",
            "performance_before_reflection": "74.4% (Self-Consistency, PaLM-540B pre-LMSI)",
            "performance_after_reflection": "82.1% (Self-Consistency, PaLM-540B after LMSI)",
            "improvement_observed": true,
            "limitations_or_failure_cases": "Majority-voted (self-consistent) wrong answers can still occur; authors report that such incorrect majority answers tend to have lower confidence (supported by fewer paths) so they introduce limited noise. Method relies on strong base model capacity (larger models benefit more); requires compute to generate many paths and to fine-tune (they use m=32 and fine-tune 10k steps).",
            "uuid": "e7159.0",
            "source_info": {
                "paper_title": "Large Language Models Can Self-Improve",
                "publication_date_yy_mm": "2022-10"
            }
        },
        {
            "name_short": "LMSI-DROP",
            "name_full": "LMSI on DROP via Self-Consistency + CoT self-training",
            "brief_description": "Same LMSI procedure applied to DROP (numerical reading comprehension) yielding improved accuracy after fine-tuning on self-generated CoT solutions.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "PaLM-540B",
            "model_description": "Autoregressive Transformer-based large language model (pretrained), used with few-shot CoT prompting and self-consistency; fine-tuned for LMSI.",
            "model_size": "540B",
            "reflection_method_name": "Self-Consistency majority-voting + CoT self-training (LMSI)",
            "reflection_method_description": "Generate m CoT paths (m=32), majority-vote for the most consistent answer, keep paths that support that answer, augment formats and fine-tune.",
            "iteration_type": "voting over multiple samples followed by fine-tuning (generate-then-train)",
            "num_iterations": 1,
            "task_name": "DROP (dev subset)",
            "task_description": "Reading comprehension benchmark requiring discrete and numerical reasoning.",
            "evaluation_metric": "accuracy",
            "performance_before_reflection": "78.2% (Self-Consistency, PaLM-540B pre-LMSI)",
            "performance_after_reflection": "83.0% (Self-Consistency, PaLM-540B after LMSI)",
            "improvement_observed": true,
            "limitations_or_failure_cases": "Authors note LMSI is competitive with supervised methods but cannot fully match methods using ground-truth labels (e.g., OPERA). Requires reliable CoT generations; risk of noise from incorrect self-labeled examples though confidence filtering mitigates this.",
            "uuid": "e7159.1",
            "source_info": {
                "paper_title": "Large Language Models Can Self-Improve",
                "publication_date_yy_mm": "2022-10"
            }
        },
        {
            "name_short": "LMSI-OpenBookQA",
            "name_full": "LMSI on OpenBookQA via Self-Consistency + CoT self-training",
            "brief_description": "LMSI applied to multiple-choice commonsense reasoning (OpenBookQA) using generated CoT paths filtered by majority vote and used for fine-tuning.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "PaLM-540B",
            "model_description": "Autoregressive Transformer-based large language model (pretrained), used with few-shot CoT prompting and self-consistency; fine-tuned for LMSI.",
            "model_size": "540B",
            "reflection_method_name": "Self-Consistency majority-voting + CoT self-training (LMSI)",
            "reflection_method_description": "Sample CoT outputs (m=32), majority-vote to select the high-confidence answer, keep rationales supporting it, augment and fine-tune.",
            "iteration_type": "voting over multiple samples followed by fine-tuning (generate-then-train)",
            "num_iterations": 1,
            "task_name": "OpenBookQA",
            "task_description": "Multiple-choice commonsense reasoning question set requiring domain knowledge and reasoning.",
            "evaluation_metric": "accuracy",
            "performance_before_reflection": "90.0% (Self-Consistency, PaLM-540B pre-LMSI)",
            "performance_after_reflection": "94.4% (Self-Consistency, PaLM-540B after LMSI)",
            "improvement_observed": true,
            "limitations_or_failure_cases": "Method achieves SOTA-level performance without ground-truth labels; paper notes dependence on quality of generated CoT and that larger models benefit more from LMSI.",
            "uuid": "e7159.2",
            "source_info": {
                "paper_title": "Large Language Models Can Self-Improve",
                "publication_date_yy_mm": "2022-10"
            }
        },
        {
            "name_short": "LMSI-ANLI-A3",
            "name_full": "LMSI on ANLI-A3 (Adversarial NLI) via Self-Consistency + CoT self-training",
            "brief_description": "Application of LMSI to adversarial natural language inference (ANLI-A3) using self-generated CoT rationales filtered with majority voting and fine-tuning.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "PaLM-540B",
            "model_description": "Autoregressive Transformer-based large language model (pretrained), used with few-shot CoT prompting and self-consistency; fine-tuned for LMSI.",
            "model_size": "540B",
            "reflection_method_name": "Self-Consistency majority-voting + CoT self-training (LMSI)",
            "reflection_method_description": "Generate multiple CoT sequences per question, majority-vote, keep rationales aligned with majority answer, augment and fine-tune.",
            "iteration_type": "voting over multiple samples followed by fine-tuning (generate-then-train)",
            "num_iterations": 1,
            "task_name": "ANLI-A3",
            "task_description": "Adversarial natural language inference (hard subset), three-way classification (entailment/neutral/contradiction).",
            "evaluation_metric": "accuracy",
            "performance_before_reflection": "63.4% (Self-Consistency, PaLM-540B pre-LMSI)",
            "performance_after_reflection": "67.9% (Self-Consistency, PaLM-540B after LMSI)",
            "improvement_observed": true,
            "limitations_or_failure_cases": "Improvement is modest but consistent; authors caution that self-labeled training may introduce noise when the model's majority is wrong, but they find confidence correlates with correctness so high-confidence examples dominate.",
            "uuid": "e7159.3",
            "source_info": {
                "paper_title": "Large Language Models Can Self-Improve",
                "publication_date_yy_mm": "2022-10"
            }
        },
        {
            "name_short": "LMSI-OOD-multi",
            "name_full": "LMSI multi-task self-training generalization to Out-of-Domain tasks",
            "brief_description": "Train one LMSI checkpoint on a mixture of in-domain questions and evaluate generalization on unseen OOD benchmarks; shows coherent accuracy gains across arithmetic, commonsense, and NLI OOD tasks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "PaLM-540B",
            "model_description": "Autoregressive Transformer-based large language model (pretrained), fine-tuned on a mixture of self-generated CoT outputs from multiple in-domain datasets.",
            "model_size": "540B",
            "reflection_method_name": "Self-Consistency majority-voting + CoT self-training (multi-task LMSI)",
            "reflection_method_description": "Combine self-generated CoT training data from multiple in-domain datasets (GSM8K, DROP, ARC-c, OpenBookQA, ANLI-A2/A3), fine-tune a single checkpoint; evaluate on OOD benchmarks.",
            "iteration_type": "voting over multiple samples followed by multi-task fine-tuning",
            "num_iterations": 1,
            "task_name": "OOD set: AQUA, SVAMP, StrategyQA, ANLI-A1, RTE, MNLI",
            "task_description": "Out-of-domain arithmetic, commonsense, and NLI benchmarks used to evaluate generalization.",
            "evaluation_metric": "accuracy",
            "performance_before_reflection": "AQUA 35.8%, SVAMP 79.0%, StrategyQA 75.3%, ANLI-A1 68.8%, RTE 79.1%, MNLI-M/MM 72.0%/74.0% (CoT-prompting, pre-LMSI)",
            "performance_after_reflection": "AQUA 39.0%, SVAMP 82.8%, StrategyQA 77.8%, ANLI-A1 79.2%, RTE 80.1%, MNLI-M/MM 81.8%/82.2% (CoT-prompting, post-LMSI)",
            "improvement_observed": true,
            "limitations_or_failure_cases": "Some OOD tasks differ significantly from in-domain data and still require specific prompts; authors note improvements but not uniform elimination of domain gaps. Effective generalization depends on diversity of self-generated training examples.",
            "uuid": "e7159.4",
            "source_info": {
                "paper_title": "Large Language Models Can Self-Improve",
                "publication_date_yy_mm": "2022-10"
            }
        },
        {
            "name_short": "Self-Generated-Questions",
            "name_full": "Self-generation of training questions + LMSI fine-tuning",
            "brief_description": "LLM generates additional training questions by prompting with concatenated seed questions and keeping only generated questions whose answers are high-confidence under self-consistency; these generated questions are then used with LMSI.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "PaLM-540B",
            "model_description": "Autoregressive Transformer used to self-generate new questions and then to generate CoT answers for filtering and fine-tuning.",
            "model_size": "540B",
            "reflection_method_name": "Self-generation of questions + Self-Consistency filtering + CoT self-training",
            "reflection_method_description": "Concatenate seed questions to prompt model to auto-generate new questions (no ground-truth); apply multi-path CoT sampling and self-consistency to keep only high-confidence generated questions; generate CoT rationales for those and fine-tune.",
            "iteration_type": "generate training data (questions) -&gt; generate CoT answers -&gt; filter by self-consistency -&gt; fine-tune",
            "num_iterations": 1,
            "task_name": "GSM8K (few-shot setting)",
            "task_description": "Arithmetic reasoning; tested in low-resource/few-shot setting where only 10 real questions provided and many generated questions used for LMSI.",
            "evaluation_metric": "accuracy",
            "performance_before_reflection": "56.5% (CoT-Prompting, PaLM-540B pre-LMSI)",
            "performance_after_reflection": "66.2% (CoT-Prompting, LMSI trained on self-generated questions); using full training-set questions gives 73.5%",
            "improvement_observed": true,
            "limitations_or_failure_cases": "Self-generated questions help but do not match the gains from using real training-set questions; quality of generated questions matters and requires additional filtering.",
            "uuid": "e7159.5",
            "source_info": {
                "paper_title": "Large Language Models Can Self-Improve",
                "publication_date_yy_mm": "2022-10"
            }
        },
        {
            "name_short": "Self-Generated-CoT-Prompts",
            "name_full": "Self-generated few-shot Chain-of-Thought prompts (Step-by-Step) used at inference",
            "brief_description": "Generate few-shot CoT exemplars via the model itself (using 'A: Let's think step by step.'), then use those exemplars as few-shot CoT prompts for multi-path decoding; improves zero-shot performance.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "PaLM-540B",
            "model_description": "Autoregressive Transformer; used to generate CoT exemplars (Step-by-Step) that are then used as few-shot exemplars for other questions.",
            "model_size": "540B",
            "reflection_method_name": "Self-generated CoT exemplars (Step-by-Step) + multi-path decoding",
            "reflection_method_description": "Greedy 'Step-by-Step' prompting to generate CoT exemplars; collect many diverse generated exemplars, use them as few-shot CoT prompts and perform multi-path sampling + self-consistency at test time.",
            "iteration_type": "generate exemplars -&gt; use exemplars for multi-path generation and self-consistency (no fine-tune required)",
            "num_iterations": 1,
            "task_name": "GSM8K (zero-shot / few-shot)",
            "task_description": "Grade-school math problems; used to evaluate zero-shot/few-shot CoT prompting.",
            "evaluation_metric": "accuracy",
            "performance_before_reflection": "Step-by-Step (Kojima) baseline: 53.8% (10 paths) and 70.1% (40 paths) [baseline numbers referenced in paper]",
            "performance_after_reflection": "Few-Shot with self-generated Step-by-Step exemplars: 66.2% (10 paths) and 74.2% (40 paths)",
            "improvement_observed": true,
            "limitations_or_failure_cases": "Greedy-generated CoT exemplars alone have limited accuracy (e.g., greedy Step-by-Step ~43%), but diversity (many generated exemplars) and multi-path decoding mitigate this; method may require generating many exemplars (computational cost).",
            "uuid": "e7159.6",
            "source_info": {
                "paper_title": "Large Language Models Can Self-Improve",
                "publication_date_yy_mm": "2022-10"
            }
        },
        {
            "name_short": "UL2-LMSI",
            "name_full": "Application of LMSI to UL2 model",
            "brief_description": "Demonstration that the LMSI self-training pipeline (generate multiple CoT paths, majority-vote, fine-tune on self-generated rationales) also yields improvements on a public UL2 checkpoint, albeit smaller than for a very large model.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "UL2 (pretrained checkpoint at step 2,650,000)",
            "model_description": "UL2 family model (public research model); authors use a specific UL2 checkpoint and apply the same LMSI pipeline with m=40 generation.",
            "model_size": null,
            "reflection_method_name": "Self-Consistency majority-voting + CoT self-training (LMSI applied to UL2)",
            "reflection_method_description": "Generate m=40 CoT reasoning paths per question for majority voting (self-consistency), keep paths supporting majority answer, augment and fine-tune UL2 on these self-generated outputs.",
            "iteration_type": "voting over multiple samples followed by fine-tuning (generate-then-train)",
            "num_iterations": 1,
            "task_name": "GSM8K, DROP, ARC-c, OpenBookQA, ANLI-A2, ANLI-A3 (same set as main experiments)",
            "task_description": "Same reasoning and NLI benchmarks as main experiments but using UL2 base model.",
            "evaluation_metric": "accuracy (and equation-corrected accuracy for arithmetic)",
            "performance_before_reflection": "Example (Self-Consistency pre-LMSI): GSM8K 6.4% / 9.9% (exact / eq-corrected); DROP 16.8% / 26.5%; ARC-c 54.9%; OpenBookQA 54.0%; ANLI-A2 37.4%; ANLI-A3 36.8%",
            "performance_after_reflection": "Example (Self-Consistency post-LMSI): GSM8K 7.9% / 10.2% (exact / eq-corrected); DROP 18.1% / 28.1%; ARC-c 54.7%; OpenBookQA 55.2%; ANLI-A2 38.1%; ANLI-A3 37.4%",
            "improvement_observed": true,
            "limitations_or_failure_cases": "Improvements on UL2 are modest compared to PaLM-540B, suggesting LMSI benefits scale with base model capacity; authors attribute smaller gains to model size/capacity limits.",
            "uuid": "e7159.7",
            "source_info": {
                "paper_title": "Large Language Models Can Self-Improve",
                "publication_date_yy_mm": "2022-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Self-consistency improves chain of thought reasoning in language models",
            "rating": 2
        },
        {
            "paper_title": "Chain of thought prompting elicits reasoning in large language models",
            "rating": 2
        },
        {
            "paper_title": "Large language models are zero-shot reasoners",
            "rating": 2
        },
        {
            "paper_title": "Star: Bootstrapping reasoning with reasoning",
            "rating": 1
        },
        {
            "paper_title": "On the advance of making language models better reasoners",
            "rating": 1
        },
        {
            "paper_title": "Self-training: A survey",
            "rating": 1
        }
    ],
    "cost": 0.019098749999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Large Language Models Can Self-Improve</h1>
<p>Jiaxin Huang ${ }^{1 *}$ Shixiang Shane Gu ${ }^{2}$ Le Hou ${ }^{2 \dagger}$ Yuexin Wu ${ }^{2}$ Xuezhi Wang ${ }^{2}$ Hongkun Yu ${ }^{2}$ Jiawei Han ${ }^{1}$<br>${ }^{1}$ University of Illinois at Urbana-Champaign ${ }^{2}$ Google<br>${ }^{1}{$ jiaxinh3, hanj}@illinois.edu ${ }^{2}$ {shanegu, lehou, crickwu, xuezhiw, hongkuny}@google.com</p>
<h4>Abstract</h4>
<p>Large Language Models (LLMs) have achieved excellent performances in various tasks. However, fine-tuning an LLM requires extensive supervision. Human, on the other hand, may improve their reasoning abilities by self-thinking without external inputs. In this work, we demonstrate that an LLM is also capable of self-improving with only unlabeled datasets. We use a pre-trained LLM to generate "high-confidence" rationale-augmented answers for unlabeled questions using Chain-of-Thought prompting and self-consistency, and fine-tune the LLM using those self-generated solutions as target outputs. We show that our approach improves the general reasoning ability of a 540B-parameter LLM ( $74.4 \% \rightarrow 82.1 \%$ on GSM8K, $78.2 \% \rightarrow 83.0 \%$ on DROP, $90.0 \% \rightarrow 94.4 \%$ on OpenBookQA, and $63.4 \% \rightarrow 67.9 \%$ on ANLI-A3) and achieves state-of-the-art-level performance, without any ground truth label. We conduct ablation studies and show that finetuning on reasoning is critical for self-improvement.</p>
<h2>1 INTRODUCTION</h2>
<p>Scaling has enabled Large Language Models (LLMs) to achieve state-of-the-art performance on a range of Natural Language Processing (NLP) tasks (Wang et al., 2018; 2019; Rajpurkar et al., 2016). More importantly, new capabilities have emerged from LLMs as they are scaled to hundreds of billions of parameters (Wei et al., 2022a): in-context few-shot learning (Brown et al., 2020) makes it possible for an LLM to perform well on a task it never trained on with only a handful of examples; Chain-of-Thought (CoT) prompting (Wei et al., 2022b; Kojima et al., 2022) demonstrates strong reasoning ability of LLMs across diverse tasks with or without few-shot examples; self-consistency (Wang et al., 2022b) further improves the performance via self-evaluating multiple reasoning paths.
Despite these incredible capabilities of models trained on large text corpus (Brown et al., 2020; Chowdhery et al., 2022), fundamentally improving the model performances beyond few-shot baselines still requires finetuning on an extensive amount of high-quality supervised datasets. FLAN (Wei et al., 2021; Chung et al., 2022) and T0 (Sanh et al., 2022) curated tens of benchmark NLP datasets to boost zero-shot task performances on unseen tasks; InstructGPT (Ouyang et al., 2022) crowd-sourced many human answers for diverse sets of text instructions to better align their model to human instructions. While significant efforts were committed on collecting high-quality supervised datasets, human brain, on the contrary, is capable of the metacognition process (Dunlosky \&amp; Metcalfe, 2008), where we can refine our own reasoning ability without external inputs.
In this paper, we study how an LLM is able to self-improve its reasoning ability without supervised data. We show that using only input sequences (without ground truth output sequences) from multiple NLP task datasets, a pre-trained LLM is able to improve performances for both in-domain and out-of-domain tasks. Our method is shown in Figure 1: we first sample multiple predictions using few-shot Chain-of-Thought (CoT) (Wei et al., 2022b) as prompts, filter "high-confidence" predictions using majority voting (Wang et al., 2022b), and finally finetune the LLM on these highconfidence predictions. The resulting model shows improved reasoning in both greedy and multipath evaluations. We call the model fine-tuned in this way as Language Model Self-Improved</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Overview of our method. With Chain-of-Thought (CoT) examples as demonstration (Wei et al., 2022b), the language model generates multiple CoT reasoning paths and answers (temperature $T&gt;0$ ) for each question. The most consistent answer is selected by majority voting (Wang et al., 2022b). The "high-confidence" CoT reasoning paths that lead to the majority answer are augmented by mixed formats as the final training samples to be fed back to the model for fine-tuning.
(LMSI). This is similar to how a human brain sometimes learns: given a question, think multiple times to derive different possible results, conclude on how the question should be solved, and then learn from or memorize its own solution. We empirically verify our method using a pre-trained PaLM-540B LLM, where our method not only improves training task performances ( $74.4 \% \rightarrow 82.1 \%$ on GSM8K, $78.2 \% \rightarrow 83.0 \%$ on DROP, $90.0 \% \rightarrow 94.4 \%$ on OpenBookQA, and $63.4 \% \rightarrow 67.9 \%$ on ANLI-A3), but also enhances out-of-domain (OOD) test tasks (AQUA, StrategyQA, MNLI), achieving state-of-the-art performances in many tasks without relying on supervised ground truth answers. Lastly, we conduct preliminary studies on self-generating additional input questions and few-shot CoT prompts, which could further reduce the amount of human effort required for model selfimproving, and ablation studies on important hyperparameters of our approach. We hope our simple approach and strong empirical results could encourage more future work by the community to investigate optimal performances of pretrained LLMs without additional human supervision.
Our contributions are summarized as follows:</p>
<ul>
<li>We demonstrate that a large language model can self-improve by taking datasets without ground truth outputs, by leveraging CoT reasoning (Wei et al., 2022b) and selfconsistency (Wang et al., 2022b), achieving competitive in-domain multi-task performances as well as out-of-domain generalization. We achieve state-of-the-art-level results on ARC, OpenBookQA, and ANLI datasets.</li>
<li>We provide detailed ablation studies on training sample formatting and sampling temperature after fine-tuning, and identify critical design choices for most successful selfimprovement by LLMs.</li>
<li>We study two other approaches for self-improvements, where the model generates additional questions from finite input questions and generates few-shot CoT prompt templates itself. The latter achieves $74.2 \%$ on GSM8K, which is the state-of-the-art zero-shot performance, against $43.0 \%$ by Kojima et al. (2022) or $70.1 \%$ through its naive extension with Wang et al. (2022b).</li>
</ul>
<h1>2 Related Work</h1>
<p>Learning from explanations. Augmenting a machine learning model with explanations has been studied in existing literature extensively. For example, in the supervised learning setting, a model can be fine-tuned using human-annotated rationales (Zaidan et al., 2007; Ling et al., 2017b; Narang et al., 2020; Camburu et al., 2018; Cobbe et al., 2021; Chung et al., 2022). A few works have also looked at how explanations can help the models in various settings, e.g., in-context learning</p>
<p>(Lampinen et al., 2022) and in distillation (Pruthi et al., 2022). In this paper, we focus more on the unsupervised learning setting, where we do not assume we have a rationale-augmented training dataset available, since human-annotated rationales can be expensive.</p>
<p>Few-shot explanations improves reasoning in LLMs. Recently, a lot of progress has been made towards improving LLMs’ reasoning abilities via prompting or in-context learning. Wei et al. (2022b) propose Chain-of-Thought prompting, which prompts the language model to generate a series of natural-language-based intermediate steps, and show it can help language models better solve complex and multi-step reasoning tasks. Wang et al. (2022b) improve Chain-of-Thought prompting by sampling multiple diverse reasoning paths and finding the most consistent answers via majority voting. Kojima et al. (2022) propose to prompt the language model with "Let’s think step by step" to generate reasoning in a zero-shot fashion. Zhou et al. (2022a) further decompose the questions into multiple sub-questions, and ask the language model to solve each sub-question sequentially.</p>
<p>Refining explanations. More recent work proposes to further refine the generated reasoning paths as some of them could be unreliable. For example, Ye &amp; Durrett (2022) calibrate model predictions based on the reliability of the explanations, Jung et al. (2022) show that inducing a tree of explanations and inferring the satisfiability of each explanation can further help judge the correctness of explanations. Li et al. (2022b) show that sampling a diverse set of prompts from the training data, and a voting verifier can be used to improve model’s reasoning performance. Zelikman et al. (2022) proposes better rationale generation by augmenting ground truth answers as hints when predicted answers are incorrect. Our work is orthogonal to these lines of work, as we utilize refined explanations from Wang et al. (2022b) for fine-tuning the model for self-improvement, and could readily incorporate these other refinement techniques for generating higher-quality self-training data. Our work is similar to Zelikman et al. (2022) where we both propose to fine-tune a model on self-generated CoT data, but our method does not require ground truth labels and shows stronger empirical results with multi-task generalization.</p>
<p>Self-training models. One related line of work is self-training (see a survey from Amini et al. (2022)). The key idea is to assign pseudo labels from a learned classifier to unlabeled data, and use these pseudo-labeled examples to further improve the original model training, e.g., (RoyChowdhury et al., 2019; Xie et al., 2020; He et al., 2020; Chen et al., 2021). Different from such prior work, our proposed self-improvement framework uses CoT prompting plus self-consistency to obtain high-confidence solutions on a large set of unlabeled data to augment the fine-tuning process.</p>
<p>Distillation and dark knowledge. Our method also tangentially relates to rich literature on distillation (Ba &amp; Caruana, 2014; Hinton et al., 2015), where a student network imitates a teacher network’s classifier predictions on input examples. A key detail is to learn from soft targets instead of hard predicted labels, as softmax outputs with a high temperature reveal more detailed relative class likelihoods, colloquially known as dark knowledge (Hinton et al., 2015; Korattikara Balan et al., 2015). Recent studies (Zelikman et al., 2022; Snell et al., 2022; Eisenstein et al., 2022) show that dark knowledge within LLMs can be retrieved with more computation at inference time, such as adding informative instructions into the input sequence, and output CoT generation (Wei et al., 2022b; Kojima et al., 2022). In our work, we explicitly show that imperfect CoT reasoning (which may lead to incorrect answer) can be used directly for self-improving language models as evidenced in our experiments in Sections 5.2 and 5.3.</p>
<h2>3 Method</h2>
<p>The overview of our method is illustrated in Fig. 1: We are given a pre-trained Large Language Model (LLM) $M$ and a question-only training dataset $\mathcal{D}^{\text{train}}=\left{x_{i}\right}<em i__1="i_{1">{i=1}^{D}$ with few-shot Chain-of-Thought (CoT) examples (Wei et al., 2022b). We apply multiple path decoding with a sampling temperature $T&gt;0$ for generating $m$ reasoning paths and answers $\left{r</em>$, and use majority voting (self-consistency) to select the most consistent, highest confidence answer (Wang et al., 2022b). We then keep all reasoning paths that lead to the most consistent answer, apply mixed formats of prompts and answers for augmentation, and fine-tune the model on these self-generated reasoning-answer data. We consider our approach as making the}},r_{i_{2}},...,r_{i_{m}}\right}$ for each question $x_{i}$ in $\mathcal{D}^{\text{train}</p>
<p>model self-improve. In the following sections, we detail important designs within our method, along with additional approaches for the model to self-improve without supervised data.</p>
<h1>3.1 Generating and Filtering Multiple Reasoning Paths</h1>
<p>Self-consistency (Wang et al., 2022b) brings large improvements on reasoning tasks (e.g., $56.5 \% \rightarrow$ $74.4 \%$ on GSM8K test set), and the gap between greedy decoding and diverse decoding shows there is a potential for further improving the reasoning ability of $M$, using the self-selected highconfidence reasoning paths as training data.</p>
<p>Question: Stefan goes to a restaurant with his family. They order an appetizer that costs $\$ 10$ and 4 entrees that are $\$ 20$ each. If they tip $20 \%$ of the total, what is the total amount of money that they spend?</p>
<h2>Multiple Path Decoding:</h2>
<p>Output 1: The appetizer costs $\$ 10$. The entrees cost $\$ 20$ each so in total $4 * \$ 20=\$ 80$. This means the total cost is $\$ 10+\$ 80=\$ 90$. They tip $20 \%$ of it, so the total amount they spend is $\$ 90 * 1.2$ $=\$ 108$. The answer is 108 .</p>
<p>Output 2: The appetizer costs $\$ 10$ and the entrees are $\$ 20$ each. There are 4 entrees so the sum is $\$ 20 * 4=\$ 80$. The waiter gets $20 \%$ of the total. $20 \%$ of $\$ 80$ is $\$ 80 * .2=\$ 16$. The answer is $\$ 80$ $+\$ 16=\$ 96$. (Incorrect reasoning path)</p>
<p>Output 3: The appetizer costs $\$ 10$. The entrees cost $4 * \$ 20=\$ 80$. The tip is $20 \%$ of the total, so it is $20 \%$ of the $\$ 90$ they have spent. The tip is $0.2 * 90=\$ 18$. The total they spent is $\$ 90+\$ 18$ $=\$ 108$. The answer is 108 .</p>
<p>Table 1: Examples of 3 self-generated CoT reasoning paths given a question. Output 1 and 3 are the most consistent reasoning paths based on majority voting and kept as self-training data.</p>
<p>For each training question $x_{i}$, we sample $m$ CoT reasoning paths, denoted as $\left{r_{i_{1}}, r_{i_{2}}, \ldots, r_{i_{m}}\right}$ (see Table 1 for examples). Since $M$ is prompted with the CoT examples from Wei et al. (2022b), we apply the same output parsing with "The answer is" to generate their predicted answers $\left{y_{i_{1}}, y_{i_{2}}, \ldots, y_{i_{m}}\right}$. The most consistent answer, which is not necessarily a correct answer, is selected by majority voting, denoted as $\tilde{y}<em y__i__j="y_{i_{j">{i}=\arg \max </em>}}} \sum_{k=1}^{m} \mathbb{I}\left(y_{i_{j}}=y_{i_{k}}\right)$. For all the training questions, we filter the CoT reasoning paths that reach $\tilde{y}$ as the final answer to be put into the self-training data, denoted as $\mathcal{D}^{\text {self-consistent }}=\left{x_{i}, \hat{\boldsymbol{r}<em i="i">{i}\right}$, where $\hat{\boldsymbol{r}}</em>\right}$.}=\left{r_{i_{j}} \mid 1 \leq j \leq m, y_{i_{j}}=\tilde{y}_{i</p>
<p>Since we do not use any ground truth labels to filter out cases where $\tilde{y}<em i="i">{i} \neq y</em>$ is wrong, it is likely to be supported by fewer CoT paths, and brings little noise to the training samples.
}$, it is important that the self-generated CoT reasoning paths are mostly reliable and incorrect answers do not hurt the self-improvement of the model. We plot the relation between the accuracy and confidence of self-generated CoT paths for each question in GSM8K training set in Fig. 2. The confidence is the number of CoT paths leading to $\tilde{y}$ divided by the total path number $m$. The y-axis shows the accuracy of $\tilde{y}$ under a certain confidence. The circle area and the color darkness shows the number of questions under a certain confidence. We can observe that confident answers are more likely to be correct, which means that when a question has many consistent CoT paths, then the corresponding $\tilde{y}$ is more likely to be correct. On the other hand, when $\tilde{y<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: The relation of accuracy and confidence of the majorityvoted answer after multiple path decoding on GSM8K training-set questions. Predicted confidence from selfconsistency (Wang et al., 2022b) is well calibrated (Guo et al., 2017).</p>
<h1>3.2 Training with Mixed Formats</h1>
<p>To prevent the language model from overfitting to specific prompts or answer styles, we create four different formats for each reasoning path to be mixed in the self-training data, shown in Table 2. In the first format, a few Chain-of-Thought examples (questions followed by reasoning paths leading to the correct final answers) are prepended to the new question, while the language model output is trained to be the same with the filtered CoT reasoning paths. In the second format, we use examples of questions and their direct answers as standard prompting, and the language model output is supposed to also only contain the direct answer. The third and fourth format are similar to the first and second format, except that no example of question-answer pairs are given, so that the model will learn to think on its own in an in-context zero-shot manner. In the third format, where we want the model to output CoT reasoning without prepending examples containing CoT reasonings, we append "Let's think step by step." at the end of the input sequence, to guide the language model to generate step-by-step CoT reasoning paths (Kojima et al., 2022). The mixed formats of training samples are then used to fine-tune the pre-trained language model $M$.</p>
<p>Question: Amy is 10 years old. Jake is 8 years old. Alex's age is right in the middle. How old is Alex?
Selected Chain-of-Thought: Amy is 10 years old. Jake is 8 years old. Alex's age is in the middle of Amy and Jake, so Alex is $(8+10) / 2=9$ years old. The answer is 9 .</p>
<h2>Mixed-formats of training data:</h2>
<p>Format 1: Input: [CoT prompting examples] $+{ }^{\prime} \backslash \mathrm{n}^{\prime}+$ [Question] $+{ }^{\prime} \backslash \mathrm{n}^{\prime}+$ 'A:'
Output: Amy is 10 years old. Jake is 8 years old. Alex's age is in the middle of Amy and Jake, so Alex is $(8+10) / 2=9$ years old. The answer is 9 .</p>
<p>Format 2: Input: [Standard prompting examples] $+{ }^{\prime} \backslash \mathrm{n}^{\prime}+$ [Question] $+{ }^{\prime} \backslash \mathrm{n}^{\prime}+$ 'A:'
Output: The answer is 9 .
Format 3: Input: [Question] $+{ }^{\prime} \backslash \mathrm{n}^{\prime}+$ 'A: Let's think step by step.'
Output: Amy is 10 years old. Jake is 8 years old. Alex's age is in the middle of Amy and Jake, so Alex is $(8+10) / 2=9$ years old. The answer is 9 .</p>
<p>Format 4: Input: [Question] $+{ }^{\prime} \backslash \mathrm{n}^{\prime}+$ 'A:'
Output: The answer is 9 .
Table 2: An example of how a reasoning path is augmented into four formats of training data with different prompts (in input) and answer styles (in output). Specifically, the CoT prompting examples used for each tasks are listed in Appendix A.2. The Standard prompting examples are the same question-answer pairs with CoT prompting examples, except that reasoning is removed.</p>
<h3>3.3 Generating Questions and Prompts</h3>
<p>Given a set of training questions and a few human-written Chain-of-Thought (CoT) examples as prompts, our proposed approach enables model self-improvement. However, when the amount of training questions or CoT examples is limited, our method may not generate sufficient training samples for language model self-training. Collecting questions from the web requires human engineering. To further reduce human effort, we investigate how to self-generate more training questions as well as example prompts.</p>
<p>Question Generation Previous work (Yoo et al., 2021; Meng et al., 2022) discuss few-shot data augmentation by generating diverse training samples using LLMs. However, those methods are designed for classification tasks and require ground truth label for each few-shot example. We use a simple yet effective approach to generate diverse questions (without ground truth answers) for in-domain questions. Specifically, we randomly select several existing questions, concatenate them in a random order as input prompt, and let the language model generate consecutive sequences as new questions. We repeat the process to obtain a large set of new questions, then use selfconsistency (Wang et al., 2022b) to only keep the questions that have a highly confident answer. Those questions are then used as self-generated training questions.</p>
<p>Prompt Generation Given a set of questions, humans can write CoT examples as reasoning paths leading to the final answer. In zero-shot setting without manual prompts, we can generate these CoT paths using the model itself. Following Kojima et al. (2022), we start the answer with "A: Let's think step by step." and let the language model generate the consecutive reasoning paths. We then use those generated reasoning paths as examples for few-shot CoT prompting.</p>
<h1>4 EXPERIMENTAL SETUP</h1>
<p>Tasks and Datasets. We demonstrate the effectiveness of our method on three types of tasks ${ }^{1}$ :</p>
<ul>
<li>Arithmetic reasoning: We use the math problem set GSM8K (Cobbe et al., 2021), and a reading comprehension benchmark DROP (Dua et al., 2019) which requires numerical reasoning. We follow Zhou et al. (2022a) to partition the DROP dataset into football related and non-football related subsets for training.</li>
<li>Commonsense reasoning: We use the OpenBookQA (Mihaylov et al., 2018) dataset, and the AI2 Reasoning Challenge (ARC) (Clark et al., 2018) dataset. Note that for ARC, we only use the Challenge sub-set (ARC-c) in our experiments. Both datasets contain multiple-choice questions.</li>
<li>Natural Language Inference: We use the Adversarial NLI (ANLI) (Mihaylov et al., 2018) subsets, ANLI-A2 and ANLI-A3, which are the more challenging subsets compared to ANLI-A1. These datasets contain pairs of sentences with relations of entailment, neutral, or contradiction.</li>
</ul>
<p>Models, Training settings and Hyperparameters We follow previous studies (Wei et al., 2022b; Wang et al., 2022b) and conduct our experiments on an autoregressive Transformer-based language model with 540 billion parameters. The CoT examples for each dataset are listed in Appendix A.2. We generate $m=32$ reasoning paths for each question in a training set. Since each reasoning path is augmented into four formats in Sec. 3.2, the final training samples are up to the size of $128 \times\left|\mathcal{D}^{\text {train }}\right|$, with $\left|\mathcal{D}^{\text {train }}\right|$ being the size of the corresponding training set. For all datasets except DROP, we use the whole training set; To reduce the training burden, we sample 5 k examples from the non-football and football partition of the DROP dataset, and sample 5k examples from ANLI-A2 and ANLI-A3. For each dataset, we fine-tune the model for 10k steps with a learning rate of $5 \mathrm{e}-5$ and a batch size of 32 . For multiple path decoding, we use a sampling temperature of $T=0.7$ with the pre-trained model as suggested by Wang et al. (2022b). We use $T=1.2$ for the language model after self-improvement (LMSI). We set the maximum number of decoded steps to 256 for all experiments.</p>
<h2>5 ReSULTS</h2>
<p>We conduct a series of experiments to demonstrate the effectiveness of our proposed self-improving method. First, we apply our method on each individual dataset (task) and report the results. We then merge the generated data from all datasets and train one model to study the generalization ability of the model on unseen datasets as in (Wei et al., 2021). In addition to the results of using generated CoT reasoning paths, we show studies on generating input questions and few-shot prompts. We end with ablation studies on model sizes and hyperparameters.</p>
<h3>5.1 MAIN ReSULTS</h3>
<p>We list the results of using the PaLM-540B model before and after LMSI in Table 3. For each model, during test time, we apply three separate prompting methods on all six datasets: standardprompting, CoT-Prompting, and Self-Consistency. We observe that after LMSI, the performance of all three prompting methods increase by a large margin. We observe significant improvement, comparing self-consistency versus LMSI with self-consistency: $+7.7 \%$ on GSM8K, $+4.8 \%$ on DROP, $+4.4 \%$ on OpenBookQA, and $+4.5 \%$ on ANLI-A3. This shows that our proposed method is quite effective. Furthermore, the single path CoT-Prompting performance of LMSI is close to or even better than the multiple path Self-Consistency performance of the model without LMSI,</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Prompting Method</th>
<th style="text-align: center;">GSM8K</th>
<th style="text-align: center;">DROP</th>
<th style="text-align: center;">ARC-c</th>
<th style="text-align: center;">OpenBookQA</th>
<th style="text-align: center;">ANLI-A2</th>
<th style="text-align: center;">ANLI-A3</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Previous SOTA</td>
<td style="text-align: center;">$82.3^{a}$</td>
<td style="text-align: center;">$84.9^{b}$</td>
<td style="text-align: center;">$88.7^{c}$</td>
<td style="text-align: center;">$91.0^{d}$</td>
<td style="text-align: center;">$64.9^{d}$</td>
<td style="text-align: center;">$66.0^{d}$</td>
</tr>
<tr>
<td style="text-align: center;">w/o LMSI</td>
<td style="text-align: center;">Standard-Prompting</td>
<td style="text-align: center;">17.9</td>
<td style="text-align: center;">60.0</td>
<td style="text-align: center;">87.1</td>
<td style="text-align: center;">84.4</td>
<td style="text-align: center;">55.8</td>
<td style="text-align: center;">55.8</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">CoT-Prompting</td>
<td style="text-align: center;">56.5</td>
<td style="text-align: center;">70.6</td>
<td style="text-align: center;">85.2</td>
<td style="text-align: center;">86.4</td>
<td style="text-align: center;">58.9</td>
<td style="text-align: center;">60.6</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Self-Consistency</td>
<td style="text-align: center;">74.4</td>
<td style="text-align: center;">78.2</td>
<td style="text-align: center;">88.7</td>
<td style="text-align: center;">90.0</td>
<td style="text-align: center;">64.5</td>
<td style="text-align: center;">63.4</td>
</tr>
<tr>
<td style="text-align: center;">LMSI</td>
<td style="text-align: center;">Standard-Prompting</td>
<td style="text-align: center;">32.2</td>
<td style="text-align: center;">71.7</td>
<td style="text-align: center;">87.2</td>
<td style="text-align: center;">92.0</td>
<td style="text-align: center;">64.8</td>
<td style="text-align: center;">66.9</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">CoT-Prompting</td>
<td style="text-align: center;">73.5</td>
<td style="text-align: center;">76.2</td>
<td style="text-align: center;">88.3</td>
<td style="text-align: center;">93.0</td>
<td style="text-align: center;">65.3</td>
<td style="text-align: center;">67.3</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Self-Consistency</td>
<td style="text-align: center;">82.1</td>
<td style="text-align: center;">83.0</td>
<td style="text-align: center;">$\mathbf{8 9 . 8}$</td>
<td style="text-align: center;">$\mathbf{9 4 . 4}$</td>
<td style="text-align: center;">$\mathbf{6 6 . 5}$</td>
<td style="text-align: center;">$\mathbf{6 7 . 9}$</td>
</tr>
</tbody>
</table>
<p>Table 3: Accuracy results on six reasoning benchmarks. The previous SOTA results are from: (a) Li et al. (2022a), (b) Zhou et al. (2022b), (c) Wang et al. (2022b), (d) Wang et al. (2022a).
showing that LMSI truly helps the language model learn from the multiple consistent reasoning paths. We also compare our results with previous SOTA, achieved by different methods on different datasets, listed in Table 3. On ARC-c, OpenBookQA, ANLI-A2 and ANLI-A3, LMSI outperforms previous SOTA. On GSM8K dataset, LMSI is close to the DiVeRSe approach (Li et al., 2022a) which uses diverse prompts and a voting verifier to ensemble 100 output paths. On the contrary, we only use 32 output paths for self-generating training samples and for self-consistency with LMSI. On the DROP dataset, LMSI is close to the OPERA approach (Zhou et al., 2022b) which uses ground truth labels for training. On the other hand, our method only leverages the questions in the training set, without using ground truth labels.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Self-training data</th>
<th style="text-align: center;">AQUA</th>
<th style="text-align: center;">SVAMP</th>
<th style="text-align: center;">StrategyQA</th>
<th style="text-align: center;">ANLI-A1</th>
<th style="text-align: center;">RTE</th>
<th style="text-align: center;">MNLI-M/MM</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">w/o LMSI</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">35.8</td>
<td style="text-align: center;">79.0</td>
<td style="text-align: center;">75.3</td>
<td style="text-align: center;">68.8</td>
<td style="text-align: center;">79.1</td>
<td style="text-align: center;">$72.0 / 74.0$</td>
</tr>
<tr>
<td style="text-align: left;">LMSI</td>
<td style="text-align: center;">GSM8K + DROP + $\ldots$</td>
<td style="text-align: center;">39.0</td>
<td style="text-align: center;">82.8</td>
<td style="text-align: center;">77.8</td>
<td style="text-align: center;">79.2</td>
<td style="text-align: center;">80.1</td>
<td style="text-align: center;">$81.8 / 82.2$</td>
</tr>
</tbody>
</table>
<p>Table 4: Comparison of CoT-prompting accuracy results on six Out-Of-Domain benchmarks with or without training on six In-Domain (GSM8K, DROP, ARC-c, OpenBookQA, ANLI-A2, ANLI-A3) training-set questions.</p>
<p>Multi-task self-training for unseen tasks To demonstrate the generalization ability of LMSI, we conduct experiments of self-training on a mixture of the training-set questions from the above six datasets (denoted as In-Domain tasks), then use the same model checkpoint for the evaluation on six Out-Of-Domain (OOD) tasks, as shown in Table 4. Of all the OOD tasks: (1) AQUA (Ling et al., 2017a) and SVAMP (Patel et al., 2021) are arithmetic reasoning tasks; (2) StrategyQA (Geva et al., 2021) is a commonsense reasoning task; (3) ANLI-A1 (Mihaylov et al., 2018), RTE (Dagan et al., 2005) and MNLI-M/MM (Williams et al., 2018) are natural language inference tasks. ${ }^{2}$ Among these tasks, AQUA, StrategyQA, and RTE are significantly different from any In-Domain task. These three tasks have their own few-shot prompts. From Table 4, we can observe that LMSI achieves higher accuracy results on all OOD tasks, showing that the overall reasoning ability of the language model is improved.</p>
<p>Importance of training with Chain-of-Thought formats We demonstrate the importance of training language models with Chain-of-Thoughts compared to training with only direct answers. In Table 5, we list the results of LMSI with all four formats, and the results of LMSI with only direct answer formats. The results clearly show that without the CoT formats, the language model can still self-improve, but the performance gain drops by a large amount compared to using all four formats.</p>
<table>
<thead>
<tr>
<th></th>
<th>Results on GSM8K</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>Standard Prompting</td>
<td>CoT Prompting</td>
<td></td>
</tr>
<tr>
<td>w/o LMSI</td>
<td>17.9</td>
<td>56.5</td>
</tr>
<tr>
<td>LMSI w/o CoT formats</td>
<td>23.6</td>
<td>61.6</td>
</tr>
<tr>
<td>LMSI</td>
<td>32.2</td>
<td>73.5</td>
</tr>
</tbody>
</table>
<p>Table 5: Ablation study: w/ or w/o CoT reasoning paths as training format on GSM8K dataset.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Questions used <br> for Self-Training</th>
<th style="text-align: center;">Results on GSM8K <br> CoT-Prompting</th>
<th style="text-align: center;">Self-Consistency</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">w/o LMSI</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">56.5</td>
<td style="text-align: center;">74.4</td>
</tr>
<tr>
<td style="text-align: left;">LMSI</td>
<td style="text-align: center;">Generated Questions</td>
<td style="text-align: center;">66.2</td>
<td style="text-align: center;">78.1</td>
</tr>
<tr>
<td style="text-align: left;">LMSI</td>
<td style="text-align: center;">Training-set Questions</td>
<td style="text-align: center;">$\mathbf{7 3 . 5}$</td>
<td style="text-align: center;">$\mathbf{8 2 . 1}$</td>
</tr>
</tbody>
</table>
<p>Table 6: Accuracy on GSM8K test set after self-training on self-generated or training set questions.</p>
<h1>5.2 PUSHING THE LIMIT OF SELF-IMPROVEMENTS</h1>
<p>Self-Generating Questions We further explore the few-shot setting where there are only limited training questions in the target domain. On GSM8K, we sample 10 real questions as few-shot samples, and use the language model to generate more training questions using the method in Section 3.3. We then self-train the language model with these generated questions and list the results in Table 6. The results show that using self-generated questions still improves the reasoning ability of language models, but using the real training-set questions leads to better results.
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Accuracy results on GSM8K test set using PaLM-540B model with multi-path sampling and self-consistency (Wang et al., 2022b). "Step-by-Step" is the baseline performance of Kojima et al. (2022) plus self-consistency (Wang et al., 2022b), while our "Few-Shot w/ Step-by-Step" uses exemplers self-generated from Step-by-Step (greedy decoding) for few-shot prompting the LLM.</p>
<p>Self-Generating Few-Shot CoT Prompts We explore the situation where no in-domain CoT examples are provided for a task. We apply the Step-by-Step method (Kojima et al., 2022) to generate CoT examples using the language model as described in Section 3.3, and show the results in Figure 3. We observe that few-shot prompting with self-generated Step-by-Step CoT examples substantially outperforms the Step-by-Step (Kojima et al., 2022) baseline ( $66.2 \%$ vs $53.8 \%$ at 10 paths, $74.2 \%$ vs $70.1 \%$ at 40 paths), and nearly matches the performance of human-written few-shot CoT (Wei et al., 2021) ( $74.4 \%$ at 40 paths (Wang et al., 2022b)). The strong performance of "Few-Shot w/ Step-</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>by-Step" despite the limited accuracy of prompt examples (43.0% for greedy Step-by-Step) likely comes from leveraging more diverse CoT prompts for multi-path decoding (Li et al., 2022a), where at 40 paths it uses 20 generate prompt-templates, each with 4-shot CoT examples, i.e. a total of 80 generated CoT examples compared to 8 human-written examples use in Wei et al. (2022b). Since we did not use training questions or few-shot CoT examples, 74.2% also marks the new state-of-the-art zero-shot performance on GSM8K.</p>
<h3>5.3 Distillation to SMALLER MODELS</h3>
<table>
<thead>
<tr>
<th></th>
<th>Results on GSM8K</th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>8 billion</td>
<td>62 billion</td>
<td>540 billion</td>
</tr>
<tr>
<td>w/o LMSI</td>
<td>5.0</td>
<td>29.7</td>
<td>56.5</td>
</tr>
<tr>
<td>Distilled from LMSI 540 billion</td>
<td>33.4</td>
<td>57.4</td>
<td>-</td>
</tr>
</tbody>
</table>
<p>Table 7: Distillation from PaLM-540B model to small models. We see that distilled smaller models outperform models that are one-tier larger.</p>
<p>We also explore whether the knowledge can be distilled to smaller models, such as in distillation (Hinton et al., 2015) and in Zelikman et al. (2022). We use the same set of training samples generated by the PaLM-540B model, but fine-tune on models with smaller sizes (PaLM-8B and PaLM-62B respectively), and show the results of CoT-prompting in Table 7. It is interesting to point out that after distillation from LMSI, the 62 billion model can outperform the pre-trained 540 billion model, and the 8 billion model can outperform the pre-trained 62 billion model. This implies that for downstream applications with limited computing resources, the reasoning knowledge from large models can be used to largely enhance small models to achieve competitive performance.</p>
<h3>5.4 Hyperparameter Study</h3>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Hyperparameter study results.</p>
<p>Sampling Temperature after Self-Improvement We study the effect of varying the temperature $T$ for multiple path decoding after LMSI is applied. Specifically, we vary $T$ between $[0.7,1.0,1.2,1.5]$ and show the results on GSM8K and DROP dataset respectively in Fig. 4(a). As shown in the figure, $T=1.2$ benefits both datasets the most, and is used in the Self-Consistency method for LMSI on all datasets. We notice that the optimal $T$ after model self-improvement is larger than the optimal $T=0.7$ (Wang et al., 2022b) before self-improvement. We believe the reason is that after training the model, the entropy of the output distribution is reduced.</p>
<p>Number of Sampled Reasoning Paths We study whether the number of sampled reasoning paths $m$ for Self-Consistency largely affects the accuracy after LMSI is applied. We show the accuracy on GSM8K test set for models both with or without LMSI in Fig. 4(b). For both cases, setting</p>
<p>$m=15$ already achieves a reasonably good accuracy, and using a larger $m$ only brings marginal improvements. We also notice that after Self-Improvement, using 5 paths for Self-Consistency can already surpass the performance of using 32 paths for model without Self-Improvement. Thus, with a well-improved model, huge computing resources can be saved when applied to real applications.</p>
<h1>6 CONCLUSIONS</h1>
<p>We demonstrated that a Large Language Model (LLM) is capable of improving its performance on reasoning datasets by training on its own generated labels, given input questions only. Experiments using an LLM with 540 billion parameters show that our approach improves the accuracy scores on the six datasets by $1.1 \%$ to $7.7 \%$, achieving new state-of-the-art results on ARC, OpenBookQA, and ANLI, without training on ground truth labels. Furthermore, we show that it is possible for the LLM to self-improve even on its own generated questions and few-shot Chain-of-Thought prompts. As part of our future work, we plan to combine large-scale generated data from our approach and existing supervised data, to further improve the performance of LLMs.</p>
<h2>ACKNOWLEDGEMENTS</h2>
<p>We thank Jason Wei (Google research), Hyung Won Chung (Google research), and Denny Zhou (Google research) for their advice and feedback on our work. We thank Yi Tay (Google research) for the guidance on UL2 fine-tuning. We also thank Jingjin Li (Cornell University) for the discussion about metacognition.</p>
<h2>REFERENCES</h2>
<p>Massih-Reza Amini, Vasilii Feofanov, Loic Pauletto, Emilie Devijver, and Yury Maximov. Selftraining: A survey, 2022. URL https://arxiv.org/abs/2202.12040.</p>
<p>Jimmy Ba and Rich Caruana. Do deep nets really need to be deep? Advances in neural information processing systems, 27, 2014.</p>
<p>BIG bench collaboration. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. ArXiv, abs/2206.04615, 2022.</p>
<p>Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, T. J. Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeff Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In Neurips, 2020.</p>
<p>Oana-Maria Camburu, Tim Rocktäschel, Thomas Lukasiewicz, and Phil Blunsom. esnli: Natural language inference with natural language explanations. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett (eds.), Advances in Neural Information Processing Systems 31, pp. 9539-9549. Curran Associates, Inc., 2018. URL http://papers.nips.cc/paper/ 8163-e-snli-natural-language-inference-with-natural-language-explanations. pdf.</p>
<p>Xiaokang Chen, Yuhui Yuan, Gang Zeng, and Jingdong Wang. Semi-supervised semantic segmentation with cross pseudo supervision. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2021.</p>
<p>Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek B Rao, Parker Barnes, Yi Tay, Noam M. Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Benton C. Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier García, Vedant</p>
<p>Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Oliveira Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Díaz, Orhan Firat, Michele Catasta, Jason Wei, Kathleen S. MeierHellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. Palm: Scaling language modeling with pathways. ArXiv, abs/2204.02311, 2022.</p>
<p>Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Adams Yu, Albert Webson, Xinyun Chen, Gaurav Mishra, Zhuyun Dai, Shixiang Shane Gu, Mirac Suzgun, Vincent Zhao, Aakanksha Chowdhery, Sharan Narang, Yanping Huang, Andrew Dai, Hongkun Yu, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, and Jason Wei. Scaling instruction-finetuned language models. In arxiv, 2022.</p>
<p>Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. ArXiv, abs/1803.05457, 2018.</p>
<p>Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems. ArXiv, abs/2110.14168, 2021.</p>
<p>Ido Dagan, Oren Glickman, and Bernardo Magnini. The pascal recognising textual entailment challenge. In MLCW, 2005.</p>
<p>Dheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh, and Matt Gardner. Drop: A reading comprehension benchmark requiring discrete reasoning over paragraphs. In NAACL, 2019.</p>
<p>John Dunlosky and Janet Metcalfe. Metacognition. Sage Publications, 2008.
Jacob Eisenstein, Daniel Andor, Bernd Bohnet, Michael Collins, and David Mimno. Honest students from untrusted teachers: Learning an interpretable question-answering pipeline from a pretrained language model. arXiv preprint arXiv:2210.02498, 2022.</p>
<p>Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, Dan Roth, and Jonathan Berant. Did aristotle use a laptop? a question answering benchmark with implicit reasoning strategies. Transactions of the Association for Computational Linguistics, 9:346-361, 2021.</p>
<p>Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q Weinberger. On calibration of modern neural networks. In International conference on machine learning, pp. 1321-1330. PMLR, 2017.</p>
<p>Junxian He, Jiatao Gu, Jiajun Shen, and Marc'Aurelio Ranzato. Revisiting self-training for neural sequence generation. In International Conference on Learning Representations, 2020. URL https://openreview.net/forum?id=SJgdnAVKDH.</p>
<p>Geoffrey Hinton, Oriol Vinyals, Jeff Dean, et al. Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531, 2(7), 2015.</p>
<p>Jaehun Jung, Lianhui Qin, Sean Welleck, Faeze Brahman, Chandra Bhagavatula, Ronan Le Bras, and Yejin Choi. Maieutic prompting: Logically consistent reasoning with recursive explanations, 2022. URL https://arxiv.org/abs/2205.11822.</p>
<p>Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. Neural Information Processing Systems (NeurIPS), 2022.</p>
<p>Anoop Korattikara Balan, Vivek Rathod, Kevin P Murphy, and Max Welling. Bayesian dark knowledge. Advances in neural information processing systems, 28, 2015.</p>
<p>Andrew K. Lampinen, Ishita Dasgupta, Stephanie C. Y. Chan, Kory Matthewson, Michael Henry Tessler, Antonia Creswell, James L. McClelland, Jane X. Wang, and Felix Hill. Can language models learn from explanations in context?, 2022. URL https://arxiv.org/abs/2204. 02329 .</p>
<p>Yifei Li, Zeqi Lin, Shizhuo Zhang, Qiang Fu, Bei Chen, Jian-Guang Lou, and Weizhu Chen. On the advance of making language models better reasoners. ArXiv, abs/2206.02336, 2022a.</p>
<p>Yifei Li, Zeqi Lin, Shizhuo Zhang, Qiang Fu, Bei Chen, Jian-Guang Lou, and Weizhu Chen. On the advance of making language models better reasoners, 2022b. URL https://arxiv.org/ abs/2206.02336.</p>
<p>Wang Ling, Dani Yogatama, Chris Dyer, and Phil Blunsom. Program induction by rationale generation: Learning to solve and explain algebraic word problems. In ACL, 2017a.</p>
<p>Wang Ling, Dani Yogatama, Chris Dyer, and Phil Blunsom. Program induction by rationale generation: Learning to solve and explain algebraic word problems. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 2017b. doi: 10.18653/v1/P17-1015. URL https://aclanthology.org/P17-1015.</p>
<p>Yu Meng, Jiaxin Huang, Yu Zhang, and Jiawei Han. Generating training data with language models: Towards zero-shot language understanding. ArXiv, abs/2202.04538, 2022.</p>
<p>Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a suit of armor conduct electricity? a new dataset for open book question answering. In EMNLP, 2018.</p>
<p>Sharan Narang, Colin Raffel, Katherine Lee, Adam Roberts, Noah Fiedel, and Karishma Malkan. Wt5?! training text-to-text models to explain their predictions, 2020. URL https://arxiv. org/abs/2004.14546.</p>
<p>Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. arXiv preprint arXiv:2203.02155, 2022.</p>
<p>Arkil Patel, S. Bhattamishra, and Navin Goyal. Are nlp models really able to solve simple math word problems? In NAACL, 2021.</p>
<p>Danish Pruthi, Rachit Bansal, Bhuwan Dhingra, Livio Baldini Soares, Michael Collins, Zachary C. Lipton, Graham Neubig, and William W. Cohen. Evaluating Explanations: How Much Do Explanations from the Teacher Aid Students? Transactions of the Association for Computational Linguistics, 10:359-375, 04 2022. ISSN 2307-387X. doi: 10.1162/tacl_a_00465. URL https://doi.org/10.1162/tacl_a_00465.</p>
<p>Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. Squad: 100,000+ questions for machine comprehension of text. In EMNLP, 2016.</p>
<p>Aruni RoyChowdhury, Prithvijit Chakrabarty, Ashish Singh, SouYoung Jin, Huaizu Jiang, Liangliang Cao, and Erik G. Learned-Miller. Automatic adaptation of object detectors to new domains using self-training. In CVPR, pp. 780-790, 2019. URL http: //openaccess.thecvf.com/content_CVPR_2019/html/RoyChowdhury_ Automatic_Adaptation_of_Object_Detectors_to_New_Domains_Using_ Self-Training_CVPR_2019_paper.html.</p>
<p>Victor Sanh, Albert Webson, Colin Raffel, Stephen H Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja, et al. Multitask prompted training enables zero-shot task generalization. In $I C L R, 2022$.</p>
<p>Charlie Snell, Dan Klein, and Ruiqi Zhong. Learning by distilling context. arXiv preprint arXiv:2209.15189, 2022.</p>
<p>Yi Tay, Mostafa Dehghani, Vinh Quang Tran, Xavier García, Jason Wei, Xuezhi Wang, Hyung Won Chung, Dara Bahri, Tal Schuster, Huaixiu Zheng, Denny Zhou, Neil Houlsby, and Donald Metzler. Ul2: Unifying language learning paradigms. 2022.</p>
<p>Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. Glue: A multi-task benchmark and analysis platform for natural language understanding. In BlackboxNLP@EMNLP, 2018.</p>
<p>Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. Superglue: A stickier benchmark for general-purpose language understanding systems. ArXiv, abs/1905.00537, 2019.</p>
<p>Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, and Denny Zhou. Rationaleaugmented ensembles in language models. ArXiv, abs/2207.00747, 2022a.</p>
<p>Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. ArXiv, abs/2203.11171, 2022b.</p>
<p>Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, and Quoc V Le. Finetuned language models are zero-shot learners. arXiv preprint arXiv:2109.01652, 2021.</p>
<p>Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al. Emergent abilities of large language models. arXiv preprint arXiv:2206.07682, 2022a.</p>
<p>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Brian Ichter, Fei Xia, Quoc Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems, 35, 2022b.</p>
<p>Adina Williams, Nikita Nangia, and Samuel R. Bowman. A broad-coverage challenge corpus for sentence understanding through inference. In NAACL, 2018.</p>
<p>Qizhe Xie, Minh-Thang Luong, Eduard Hovy, and Quoc V. Le. Self-training with noisy student improves imagenet classification. In 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 10684-10695, 2020. doi: 10.1109/CVPR42600.2020.01070.</p>
<p>Xi Ye and Greg Durrett. The unreliability of explanations in few-shot in-context learning, 2022.
Kang Min Yoo, Dongju Park, Jaewook Kang, Sang-Woo Lee, and Woomyeong Park. Gpt3mix: Leveraging large-scale language models for text augmentation. In EMNLP Findings, 2021.</p>
<p>Omar Zaidan, Jason Eisner, and Christine Piatko. Using "annotator rationales" to improve machine learning for text categorization. NAACL, 2007. URL https://aclanthology.org/ NU7-1033.</p>
<p>Eric Zelikman, Yuhuai Wu, Jesse Mu, and Noah D. Goodman. Star: Bootstrapping reasoning with reasoning, 2022. URL https://arxiv.org/abs/2203.14465.</p>
<p>Denny Zhou, Nathanael Scharli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Olivier Bousquet, Quoc Le, and Ed Chi. Least-to-most prompting enables complex reasoning in large language models. ArXiv, abs/2205.10625, 2022a.</p>
<p>Yongwei Zhou, Junwei Bao, Chaoqun Duan, Haipeng Sun, Jiahui Liang, Yifan Wang, Jing Zhao, Youzheng Wu, Xiaodong He, and Tiejun Zhao. Opera: Operation-pivoted discrete reasoning over text. In NAACL, 2022b.</p>
<h1>A APPENDIX</h1>
<h2>A. 1 RESULTS ON UL2 MODEL</h2>
<p>We also apply LMSI on a recently proposed public language model, UL2 (Tay et al., 2022), using the pre-trained model at step 2,650,000 ${ }^{3}$. We use a fixed set of hyperparameters for fine-tuning on each dataset. Specifically, we generate $m=40$ reasoning paths for each question in a training set for majority voting. We fine-tune the model for 10 k steps with a learning rate of $5 \mathrm{e}-5$ and a batch size of 32 . For multiple path decoding, we use a sampling temperature of $T=0.5$ with the pre-trained UL2 model following Tay et al. (2022), and set $T=0.7$ for the language model after LMSI. We set the maximum number of decode steps to 256 for all experiments.</p>
<p>The results are shown in Table 8. For arithmetic reasoning datasets, we follow (Tay et al., 2022) to provide both exact matching accuracy scores as well as accuracy scores after an equation-correction postprocessing step. We observe that for most datasets, LMSI still improves the reasoning accuracy, but the improvement on UL2 is not as large as that on PaLM-540B. We think the reason is that, since LMSI exploits the implicit rationale of language models, and the capacity of a language model is determined by its size, larger models can capture more high-order semantics and are more likely to benefit from LMSI.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Prompting Method</th>
<th style="text-align: center;">GSM8K</th>
<th style="text-align: center;">DROP</th>
<th style="text-align: center;">ARC-c</th>
<th style="text-align: center;">OpenBookQA</th>
<th style="text-align: center;">ANLI-A2</th>
<th style="text-align: center;">ANLI-A3</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">w/o LMSI</td>
<td style="text-align: center;">CoT-Prompting</td>
<td style="text-align: center;">$5.4 / 7.1$</td>
<td style="text-align: center;">$11.1 / 16.8$</td>
<td style="text-align: center;">49.9</td>
<td style="text-align: center;">53.6</td>
<td style="text-align: center;">35.9</td>
<td style="text-align: center;">33.8</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Self-Consistency</td>
<td style="text-align: center;">$6.4 / 9.9$</td>
<td style="text-align: center;">$16.8 / 26.5$</td>
<td style="text-align: center;">54.9</td>
<td style="text-align: center;">54.0</td>
<td style="text-align: center;">37.4</td>
<td style="text-align: center;">36.8</td>
</tr>
<tr>
<td style="text-align: center;">LMSI</td>
<td style="text-align: center;">CoT-Prompting</td>
<td style="text-align: center;">$6.1 / 8.6$</td>
<td style="text-align: center;">$11.4 / 17.1$</td>
<td style="text-align: center;">50.9</td>
<td style="text-align: center;">53.8</td>
<td style="text-align: center;">35.4</td>
<td style="text-align: center;">34.4</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Self-Consistency</td>
<td style="text-align: center;">$7.9 / 10.2$</td>
<td style="text-align: center;">$18.1 / 28.1$</td>
<td style="text-align: center;">54.7</td>
<td style="text-align: center;">55.2</td>
<td style="text-align: center;">38.1</td>
<td style="text-align: center;">37.4</td>
</tr>
</tbody>
</table>
<p>Table 8: Accuracy results on six reasoning benchmarks with LMSI on UL2. On GSM8K and DROP, we also include accuracy scores after an equation-correction postprocessing step.</p>
<h2>A. 2 Chain-of-Thought Prompts for Each Dataset</h2>
<p>We list the Chain-of-Thought Prompts for each dataset for "CoT-Prompting" experiments and selfgenerated training samples.</p>
<p>Q: Poison causes harm to which of the following? (a) a Tree (b) a robot (c) a house (d) a car
A: Poison will harm living things, only a tree is a living thing. The answer is (a).
Q: As you look deeper into a Marbel you can see (a) the future (b) minut defects (c) colors (d) the other side
A: Marbel is not transparent, so you can not see the other side. Marbel does not necessarily have multiple colors. You will see minut defects. The answer is (b).</p>
<p>Q: When food is reduced in the stomach (a) the mind needs time to digest (b) take a second to digest what I said (c) nutrients are being deconstructed (d) reader's digest is a body of works
A: The food is being deconstructed in the stomach during digestion. The answer is (c).
Q: The sun is responsible for (a) puppies learning new tricks (b) children growing up and getting old (c) flowers wilting in a vase (d) plants sprouting, blooming and wilting
A: The sun can affect the growing of living things, like plants. The answer is (d).
Table 9: Few-shot CoT prompts for OpenBookQA, from Wang et al. (2022a).</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Q: There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be 21 trees. How many trees did the grove workers plant today?
A: We start with 15 trees. Later we have 21 trees. The difference must be the number of trees they planted. So, they must have planted $21-15=6$ trees. The answer is 6 .</p>
<p>Q: If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?
A: There are 3 cars in the parking lot already. 2 more arrive. Now there are $3+2=5$ cars. The answer is 5 .
Q: Leah had 32 chocolates and her sister had 42. If they ate 35, how many pieces do they have left in total?
A: Leah had 32 chocolates and Leah's sister had 42. That means there were originally $32+42=74$ chocolates. 35 have been eaten. So in total they still have $74-35=39$ chocolates. The answer is 39 .</p>
<p>Q: Jason had 20 lollipops. He gave Denny some lollipops. Now Jason has 12 lollipops. How many lollipops did Jason give to Denny?
A: Jason had 20 lollipops. Since he only has 12 now, he must have given the rest to Denny. The number of lollipops he has given to Denny must have been $20-12=8$ lollipops. The answer is 8 .</p>
<p>Q: Shawn has five toys. For Christmas, he got two toys each from his mom and dad. How many toys does he have now?
A: He has 5 toys. He got 2 from mom, so after that he has $5+2=7$ toys. Then he got 2 more from dad, so in total he has $7+2=9$ toys. The answer is 9 .</p>
<p>Q: There were nine computers in the server room. Five more computers were installed each day, from monday to thursday. How many computers are now in the server room?
A: There are 4 days from monday to thursday. 5 computers were added each day. That means in total 4 * $5=20$ computers were added. There were 9 computers in the beginning, so now there are $9+20=29$ computers. The answer is 29 .</p>
<p>Q: Michael had 58 golf balls. On tuesday, he lost 23 golf balls. On wednesday, he lost 2 more. How many golf balls did he have at the end of wednesday?
A: Michael initially had 58 balls. He lost 23 on Tuesday, so after that he has $58-23=35$ balls. On Wednesday he lost 2 more so now he has $35-2=33$ balls. The answer is 33 .</p>
<p>Q: Olivia has $\$ 23$. She bought five bagels for $\$ 3$ each. How much money does she have left?
A: She bought 5 bagels for $\$ 3$ each. This means she spent $5 * \$ 3=\$ 15$ on the bagels. She had $\$ 23$ in beginning, so now she has $\$ 23-\$ 15=\$ 8$. The answer is 8 .</p>
<p>Table 10: Few-shot CoT prompts for GSM8K and SVAMP, from Wei et al. (2022b).</p>
<p>Q: Since the 1970s, U.S. governments have negotiated managed-trade agreements, such as the North American Free Trade Agreement in the 1990s, the Dominican Republic-Central America Free Trade Agreement in 2006, and a number of bilateral agreements. In Europe, six countries formed the European Coal and Steel Community in 1951 which became the European Economic Community in 1958. Two core objectives of the EEC were the development of a common market, subsequently renamed the single market, and establishing a customs union between its member states. How many years did the European Coal and Steel Community exist?
A: According to the passage, the European Coal and Steel Community was established in 1951 and became the EEC in 1958. 1958 - $1951=7$. So the answer is 7 .</p>
<p>Q: In the county, the population was spread out with $23.50 \%$ under the age of $18,8.70 \%$ from 18 to 24 , $29.70 \%$ from 25 to $44,24.70 \%$ from 45 to 64 , and $13.30 \%$ who were 65 years of age or older. How many more percent are under the age of 18 compared to the 18 to 24 group?
A: According to the passage, $23.5 \%$ are under the age of 18 , and $8.7 \%$ are from ages 18 to $24.23 .5 \%$ $8.7 \%=14.8 \%$. So the answer is 14.8 .</p>
<p>Q: Playing in their second straight Thanksgiving game, the Eagles struggled especially on defense, where they were unable to stop the much-hyped Lions offense. The worst of it all was how unproven rookie Eric Rowe was tasked with covering wide receiver Calvin Johnson, leading to Johnson catching 3 touchdowns. Stafford's five passing touchdowns, including three of them to Johnson was too much for the Eagles to overcome and for the second consecutive time this season, the Eagles gave up 45 points in a game. With the loss, the Eagles drop to 4-7 on the season and 6-1 when playing on Thanksgiving. How many TD passes did Stafford throw other than to Johnson?
A: According to the passage, Stafford threw 5 TD passes, 3 of which were to Johnson. 5 - $3=2$. So the answer is 2 .</p>
<p>Table 11: Few-shot CoT prompts for DROP (nonfootball), from Zhou et al. (2022a).</p>
<p>Q: The Seahawks played the San Francisco 49ers. In the first quarter, the Hawks RB Julius Jones got a 27-yard TD run, along with DT Craig Terrill returning a fumble 9 yards for a touchdown. In the third quarter, the 49ers almost rallied as RB H. J. Torres made a 12-yard TD pass to Lucas Nelly, along with Mare kicking a 32-yard field goal. In the final quarter, Julius Jones got another 11-yard TD. How many yards do the shortest touchdown run and the longest touchdown pass combine for?
A: All the touchdown runs are: a 27 -yard touchdown run, a 9 -yard touchdown run, a 11 -yard touchdown run. The smallest number among $27,9,11$ is 9 . So the shortest touchdown run was 9 yards. All the touchdown passes are: a 12 -yard touchdown pass. So the longest touchdown pass was 12 yards. So the shortest touchdown run and the longest touchdown pass combine for $9+12=21$ yards. So the answer is 21 yards.</p>
<p>Q: The Steelers went home for a duel with the Baltimore Ravens. Pittsburgh would deliver the opening punch in the first quarter with a 1-yard touchdown from running back Rashard Mendenhall. The Ravens would make it even as running back Willis McGahee got a 9-yard TD. The Ravens kicker Billy Cundiff got a 45 -yard field goal in the second quarter, concluding the first half with a 10-7 lead. The Steelers brought the game into overtime with a 38 -yard field goal by Andrew Foster. The Ravens Billy Cundiff pulled off a winning 33 -yard field goal in overtime. How many points did the Ravens have at halftime?
A: The Ravens kicker Billy Cundiff got a 45 -yard field goal in the second quarter, concluding the first half with a 10-7 lead. So the Ravens had 10 points at halftime. So the answer is 10 points.</p>
<p>Q: The Vikings flew to Bank of America Stadium to face the Carolina Panthers. After a scoreless first quarter, Carolina got on the board with quarterback Matt Moore finding fullback Brad Hoover on a 1-yard TD pass. After yet another scoreless quarter, Carolina sealed the game as Matt Moore completed a 42-yard touchdown pass to wide receiver Steve Smith. How many scoreless quarters were there?
A: The first and third quarters were the scoreless quarters. So there are 2 scoreless quarters. So the answer is 2 .</p>
<p>Table 12: Few-shot CoT prompts for DROP (football), from Zhou et al. (2022a).</p>
<h1>Premise:</h1>
<p>"Conceptually cream skimming has two basic dimensions - product and geography."
Based on this premise, can we conclude the hypothesis "Product and geography are what make cream skimming work." is true?
OPTIONS:</p>
<ul>
<li>yes</li>
<li>no</li>
<li>it is not possible to tell</li>
</ul>
<p>A: Based on "cream skimming has two basic dimensions" we can't infer that these two dimensions are what make cream skimming work. The answer is it is not possible to tell.</p>
<h2>Premise:</h2>
<p>"One of our member will carry out your instructions minutely."
Based on this premise, can we conclude the hypothesis "A member of my team will execute your orders with immense precision." is true?
OPTIONS:</p>
<ul>
<li>yes</li>
<li>no</li>
<li>it is not possible to tell</li>
</ul>
<p>A: "one of" means the same as "a member of", "carry out" means the same as "execute", and "minutely" means the same as "immense precision". The answer is yes.</p>
<h2>Premise:</h2>
<p>"Fun for adults and children."
Based on this premise, can we conclude the hypothesis "Fun for only children." is true?
OPTIONS:</p>
<ul>
<li>yes</li>
<li>no</li>
<li>it is not possible to tell</li>
</ul>
<p>A: "adults and children" contradicts "only children". The answer is no.
Premise:
"He turned and smiled at Vrenna."
Based on this premise, can we conclude the hypothesis "He smiled at Vrenna who was walking slowly behind him with her mother." is true?
OPTIONS:</p>
<ul>
<li>yes</li>
<li>no</li>
<li>it is not possible to tell</li>
</ul>
<p>A: the premise does not say anything about "Vrenna was walking". The answer is it is not possible to tell.
Premise:
"well you see that on television also"
Based on this premise, can we conclude the hypothesis "You can see that on television, as well." is true?
OPTIONS:</p>
<ul>
<li>yes</li>
<li>no</li>
<li>it is not possible to tell</li>
</ul>
<p>A: "also" and "as well" mean the same thing. The answer is yes.
Premise:
"Vrenna and I both fought him and he nearly took us."
Based on this premise, can we conclude the hypothesis "Neither Vrenna nor myself have ever fought him." is true?
OPTIONS:</p>
<ul>
<li>yes</li>
<li>no</li>
<li>it is not possible to tell</li>
</ul>
<p>A: "Vrenna and I both" contradicts "neither Vrenna nor myself". The answer is no.
Table 13: Few-shot CoT prompts for NLI tasks, including ANLI and MNLI, from Wang et al. (2022a).</p>
<p>Q: George wants to warm his hands quickly by rubbing them. Which skin surface will produce the most heat? (a) dry palms. (b) wet palms. (c) palms covered with oil. (d) palms covered with lotion.
A: Dry surfaces will more likely cause more friction via rubbing than other smoother surfaces, hence dry palms will produce the most heat. The answer is (a).
Q: Which factor will most likely cause a person to develop a fever? (a) a leg muscle relaxing after exercise. (b) a bacterial population in the bloodstream. (c) several viral particles on the skin. (d) carbohydrates being digested in the stomach.
A: Option (b), bacterial population is the most likely cause for a person developing fever. The answer is (b).</p>
<p>Q: Which change in the state of water particles causes the particles to become arranged in a fixed position? (a) boiling. (b) melting. (c) freezing. (d) evaporating.</p>
<p>A: When water is freezed, the particles are arranged in a fixed position; the particles are still moving for all other options. The answer is (c).
$\mathbf{Q}$ : When a switch is used in an electrical circuit, the switch can (a) cause the charge to build. (b) increase and decrease the voltage. (c) cause the current to change direction. (d) stop and start the flow of current.
A: The function of a switch is to start and stop the flow of a current. The answer is (d).
Table 14: Few-shot CoT prompts for ARC-challenge, from Wang et al. (2022b).</p>
<p>Q: John found that the average of 15 numbers is 40 . If 10 is added to each number then the mean of the numbers is? Answer Choices: (a) 50 (b) 45 (c) 65 (d) 78 (e) 64
A: If 10 is added to each number, then the mean of the numbers also increases by 10 . So the new mean would be 50 . The answer is (a).
Q: If a / b = 3/4 and $8 \mathrm{a}+5 \mathrm{~b}=22$, then find the value of a. Answer Choices: (a) $1 / 2$ (b) $3 / 2$ (c) $5 / 2$ (d) $4 / 2$ (e) $7 / 2$</p>
<p>A: If a / b = 3/4, then $\mathrm{b}=4 \mathrm{a} / 3$. So $8 \mathrm{a}+5(4 \mathrm{a} / 3)=22$. This simplifies to $8 \mathrm{a}+20 \mathrm{a} / 3=22$, which means $44 \mathrm{a} / 3=22$. So a is equal to $3 / 2$. The answer is (b).
Q: A person is traveling at $20 \mathrm{~km} / \mathrm{hr}$ and reached his destiny in 2.5 hr then find the distance? Answer Choices: (a) 53 km (b) 55 km (c) 52 km (d) 60 km (e) 50 km
A: The distance that the person traveled would have been $20 \mathrm{~km} / \mathrm{hr} * 2.5 \mathrm{hrs}=50 \mathrm{~km}$. The answer is (e).
Q: How many keystrokes are needed to type the numbers from 1 to 500? Answer Choices: (a) 1156 (b) 1392 (c) 1480 (d) 1562 (e) 1788
A: There are 9 one-digit numbers from 1 to 9 . There are 90 two-digit numbers from 10 to 99 . There are 401 three-digit numbers from 100 to $500.9+90(2)+401(3)=1392$. The answer is (b).</p>
<p>Table 15: Few-shot CoT prompts for AQUA, from Wang et al. (2022b).</p>
<p>Q: Do hamsters provide food for any animals?
A: Hamsters are prey animals. Prey are food for predators. Thus, hamsters provide food for some animals. The answer is yes.</p>
<p>Q: Could Brooke Shields succeed at University of Pennsylvania?
A: Brooke Shields went to Princeton University. Princeton University is about as academically rigorous as the University of Pennsylvania. Thus, Brooke Shields could also succeed at the University of Pennsylvania. The answer is yes.</p>
<p>Q: Yes or no: Hydrogen's atomic number squared exceeds number of Spice Girls?
A: Hydrogen has an atomic number of 1.1 squared is 1 . There are 5 Spice Girls. Thus, Hydrogen's atomic number squared is less than 5 . The answer is no.</p>
<p>Q: Yes or no: Is it common to see frost during some college commencements?
A: College commencement ceremonies can happen in December, May, and June. December is in the winter, so there can be frost. Thus, there could be frost at some commencements. The answer is yes.</p>
<p>Q: Yes or no: Could a llama birth twice during War in Vietnam (1945-46)?
A: The War in Vietnam was 6 months. The gestation period for a llama is 11 months, which is more than 6 months. Thus, a llama could not give birth twice during the War in Vietnam. The answer is no.</p>
<p>Q: Yes or no: Would a pear sink in water?
A: The density of a pear is about $0.6 \mathrm{~g} / \mathrm{cm}^{3}$, which is less than water. Objects less dense than water float. Thus, a pear would float. The answer is no.</p>
<p>Table 16: Few-shot CoT prompts for StrategyQA, from Wei et al. (2022b).</p>
<h1>Premise:</h1>
<p>"No Weapons of Mass Destruction Found in Iraq Yet."
Based on this premise, can we conclude the hypothesis "Weapons of Mass Destruction Found in Iraq." is true?
A: "No Weapons of Mass Destruction Found" contradicts "Weapons of Mass Destruction Found". The answer is no.</p>
<h2>Premise:</h2>
<p>"A place of sorrow, after Pope John Paul II died, became a place of celebration, as Roman Catholic faithful gathered in downtown Chicago to mark the installation of new Pope Benedict XVI."
Based on this premise, can we conclude the hypothesis "Pope Benedict XVI is the new leader of the Roman Catholic Church." is true?"
A: "installation of new Pope Benedict XVI." means "Pope Benedict XVI is the new leader". The answer is yes.
Premise:
"A man is due in court later charged with the murder 26 years ago of a teenager whose case was the first to be featured on BBC One's Crimewatch. Colette Aram, 16, was walking to her boyfriend's house in Keyworth, Nottinghamshire, on 30 October 1983 when she disappeared. Her body was later found in a field close to her home. Paul Stewart Hutchinson, 50, has been charged with murder and is due before Nottingham magistrates later."
Based on this premise, can we conclude the hypothesis "Paul Stewart Hutchinson is accused of having stabbed a girl." is true?
A: The premise does not say Paul Stewart Hutchinson "stabbed" this girl. The answer is no.
Premise:
"Herceptin was already approved to treat the sickest breast cancer patients, and the company said, Monday, it will discuss with federal regulators the possibility of prescribing the drug for more breast cancer patients." Based on this premise, can we conclude the hypothesis "Herceptin can be used to treat breast cancer." is true?
A: "Herceptin was approved to treat breast cancer" implies that "Herceptin can be used to treat breast cancer". The answer is yes.</p>
<p>Table 17: Few-shot CoT prompts for RTE, from Wang et al. (2022a).</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{3}$ UL2: https://github.com/google-research/google-research/tree/master/ul2&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6555 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6555</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6555</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-130.html">extraction-schema-130</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models solving spatial puzzle games, including details about the model, the puzzle, the reasoning or prompting method, performance metrics, internal representations, use of external tools, and any analysis or limitations reported.</div>
                <p><strong>Paper ID:</strong> paper-272689782</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2409.10502v1.pdf" target="_blank">Causal Language Modeling Can Elicit Search and Reasoning Capabilities on Logic Puzzles</a></p>
                <p><strong>Paper Abstract:</strong> Causal language modeling using the Transformer architecture has yielded remarkable capabilities in Large Language Models (LLMs) over the last few years. However, the extent to which fundamental search and reasoning capabilities emerged within LLMs remains a topic of ongoing debate. In this work, we study if causal language modeling can learn a complex task such as solving Sudoku puzzles. To solve a Sudoku, the model is first required to search over all empty cells of the puzzle to decide on a cell to fill and then apply an appropriate strategy to fill the decided cell. Sometimes, the application of a strategy only results in thinning down the possible values in a cell rather than concluding the exact value of the cell. In such cases, multiple strategies are applied one after the other to fill a single cell. We observe that Transformer models trained on this synthetic task can indeed learn to solve Sudokus (our model solves $94.21\%$ of the puzzles fully correctly) when trained on a logical sequence of steps taken by a solver. We find that training Transformers with the logical sequence of steps is necessary and without such training, they fail to learn Sudoku. We also extend our analysis to Zebra puzzles (known as Einstein puzzles) and show that the model solves $92.04 \%$ of the puzzles fully correctly. In addition, we study the internal representations of the trained Transformer and find that through linear probing, we can decode information about the set of possible values in any given cell from them, pointing to the presence of a strong reasoning engine implicit in the Transformer weights.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6555.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6555.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models solving spatial puzzle games, including details about the model, the puzzle, the reasoning or prompting method, performance metrics, internal representations, use of external tools, and any analysis or limitations reported.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Solver-decomposed Transformer (this work)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>42M-parameter decoder-only Transformer trained with solver-decomposed reasoning order</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A GPT-2-style 8-layer, 8-head causal Transformer (42M parameters) trained from scratch with next-token prediction on synthetic Sudoku and Zebra puzzles using a solver-provided adaptive order of cell/triplet outputs; demonstrates strong emergent search and reasoning and an implicit candidate-set representation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Transformer (GPT-2 style, 8-layer, 42M)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Decoder-only causal Transformer (GPT-2 architecture) with 8 layers, 8 attention heads, model dim 576, MLP dim 3456, trained with next-token prediction on synthetic puzzle sequences.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>42M</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Sudoku (9x9) and Zebra (symbolic Einstein puzzles, sizes 3–6)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>constraint satisfaction / combinatorial logic (spatial for Sudoku; logic-grid for Zebra)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>Sud20-derived Sudoku set (Radcliffe 2020 filtered to 1.9M solvable-by-7-strategies examples) and synthetic Zebra dataset (320k puzzles, sizes 3–6)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>trained end-to-end with solver-decomposed reasoning order (Chain-of-Thought-like training signals); experiments also used hinted-position inference and beam-search decoding</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_technique</strong></td>
                            <td>learned application of human-like Sudoku strategies (7 strategies: Lone single, Hidden single, Naked pair/triplet, Locked candidate, XY-wing, Unique rectangle) and implicit candidate-set constraint propagation; model learns to (1) search for easy-to-decode cells and (2) apply strategies to produce values</td>
                        </tr>
                        <tr>
                            <td><strong>internal_representation</strong></td>
                            <td>Symbolic sequence encoding: Sudoku and Zebra inputs/outputs serialized as triplets (row, column, value) tokens for Sudoku and (entity, attribute, value) triplets for Zebra; causal token sequence with no explicit block token (block implied via row/col)</td>
                        </tr>
                        <tr>
                            <td><strong>use_of_external_tool</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td>A handcrafted Sudoku/Zebra solver (implements 7 Sudoku strategies and k-sized clue-subset deductions for Zebra) was used offline to filter datasets, to generate solver-decomposed reasoning orders for training, and to provide position-hints during some inference experiments; the learned Transformer itself does not call external tools at decode time except when experiments explicitly condition on solver hints.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Cell accuracy (percentage of unfilled cells predicted correctly) and Complete puzzle accuracy (percentage of puzzles with all cells correct)</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Sudoku (solver-decomposed training, greedy k=1): 94.23% cell accuracy, 87.18% complete-puzzle accuracy; beam-search k=3: 96.07% cell, 91.36% full; beam-search k=5: 98.03% cell, 94.21% full. Zebra (solver-decomposed training): 95.63% cell accuracy, 91.17% complete-puzzle accuracy; beam k=5: 96.26% complete-puzzle accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>analysis_findings</strong></td>
                            <td>1) Training with solver-decomposed (adaptive) output order is crucial — it decomposes search and strategy application and yields large gains. 2) The model implicitly represents candidate sets: extracting top-k logits (k = size of solver candidate set) yields >93% overlap with solver candidate set across validation states, improving up to ~99% as more prefix is given. 3) The main failure mode is searching for the next easy-to-decode cell (step (1)), not applying the strategy once position is given: hinted-position accuracy = 99.02%. 4) Model misses more 'block'-based easy cells, likely due to no explicit block token in input encoding. 5) Mistakes early in a puzzle cascade (an invalid prefix increases later errors).</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_comparison</strong></td>
                            <td>Order ablation: fixed-order training → 58.64% cell, 7.2% full; random-order training → ~52% cell, ~1% full; solver-decomposed training → 94.23% cell, 87.18% full. Hinted-position (inference) increases accuracy marginally for randomly trained model (~+3% cell). Hinted-position on solver-decomposed trained model: hinted cell accuracy 99.02% (shows strategy application is near-perfect when position is given). Beam-search ablation: k=3 and k=5 improve complete-puzzle accuracy from 87.18% (k=1) to 91.36% (k=3) and 94.21% (k=5).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Experiments restricted to puzzles solvable by a fixed set of 7 human-like, polynomial-time strategies (dataset filtered), so results do not extend to NP-hard Sudoku instances requiring backtracking; synthetic setting (symbolic puzzles) may not generalize to open-ended, creative, or probabilistic reasoning tasks; model training relies on an external solver to produce decomposed reasoning sequences (data-generation dependency); input encoding lacks explicit block token, correlating with failures on block-based easy cells; model size and training compute are modest relative to frontier LLMs, so scaling effects are not explored.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Causal Language Modeling Can Elicit Search and Reasoning Capabilities on Logic Puzzles', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6555.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6555.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models solving spatial puzzle games, including details about the model, the puzzle, the reasoning or prompting method, performance metrics, internal representations, use of external tools, and any analysis or limitations reported.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4o (4-shot CoT)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4o (evaluated with 4-shot Chain-of-Thought prompting)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A frontier large language model evaluated in a 4-shot Chain-of-Thought (CoT) prompting setup on serialized 9x9 Sudoku puzzles; achieved near-random per-cell accuracy and failed to solve any full puzzles in this experiment.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Frontier large language model (proprietary); evaluated in few-shot CoT prompting (4-shot) with serialized (row,column,value) triplet format</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Sudoku (9x9)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>constraint satisfaction / combinatorial logic (spatial)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>3000 Sudoku test examples (serialized triplet input), 4-shot CoT prompts constructed by authors</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>4-shot Chain-of-Thought prompting (CoT), prompts contained 4 puzzle-solution pairs before test puzzle)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_technique</strong></td>
                            <td>none beyond CoT prompting; model not provided with external solver or search trace</td>
                        </tr>
                        <tr>
                            <td><strong>internal_representation</strong></td>
                            <td>Serialized as sequences of (row, column, value) triplets</td>
                        </tr>
                        <tr>
                            <td><strong>use_of_external_tool</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Percent of puzzles solved fully and percent of cells answered correctly (per-cell accuracy)</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>0% of puzzles solved fully; 9.5% of cells answered correctly (on the evaluated 3k examples)</td>
                        </tr>
                        <tr>
                            <td><strong>analysis_findings</strong></td>
                            <td>Frontier LLM GPT-4o, when given few-shot CoT examples serialized as triplets, failed to solve any 9x9 Sudoku fully and exhibited per-cell accuracy near random guessing; suggests that few-shot CoT prompting alone is insufficient for complex CSPs like full 9x9 Sudoku in this format.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_comparison</strong></td>
                            <td>Compared directly in the same 4-shot CoT setup to Gemini-1.5Pro which had similar failure (0% full, ~10.2% cell), demonstrating that both frontier LLMs struggled under this prompting format.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Results are for a specific 4-shot CoT setup and tokenization/serialization format; does not rule out other prompting, fine-tuning, tool-augmented, or search-based methods, but demonstrates brittleness of simple few-shot CoT for these spatial CSPs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Causal Language Modeling Can Elicit Search and Reasoning Capabilities on Logic Puzzles', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6555.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6555.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models solving spatial puzzle games, including details about the model, the puzzle, the reasoning or prompting method, performance metrics, internal representations, use of external tools, and any analysis or limitations reported.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Gemini-1.5Pro (4-shot CoT)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Gemini-1.5Pro (evaluated with 4-shot Chain-of-Thought prompting)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A frontier LLM evaluated in the same 4-shot CoT setup on serialized 9x9 Sudoku puzzles; like GPT-4o, it failed to fully solve any puzzles and produced ~10% per-cell accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Gemini-1.5Pro</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Frontier large language model (proprietary Google model), evaluated in 4-shot CoT prompting on serialized triplet Sudoku format</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Sudoku (9x9)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>constraint satisfaction / combinatorial logic (spatial)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>3000 Sudoku test examples (serialized triplet input), 4-shot CoT prompts constructed by authors</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>4-shot Chain-of-Thought prompting (CoT)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_technique</strong></td>
                            <td>none beyond prompting</td>
                        </tr>
                        <tr>
                            <td><strong>internal_representation</strong></td>
                            <td>Serialized (row, column, value) triplet token sequences</td>
                        </tr>
                        <tr>
                            <td><strong>use_of_external_tool</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Percent puzzles solved fully; percent cells answered correctly</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>0% puzzles solved fully; 10.2% of cells answered correctly (on the evaluated 3k examples)</td>
                        </tr>
                        <tr>
                            <td><strong>analysis_findings</strong></td>
                            <td>Gemini-1.5Pro performed at near-random per-cell levels under the 4-shot CoT protocol in this serialization, demonstrating that few-shot CoT does not suffice for solving standard 9x9 Sudoku in this setup.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_comparison</strong></td>
                            <td>Directly compared to GPT-4o under identical 4-shot CoT prompting; both models showed near-identical failure modes and low per-cell accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Same as GPT-4o: result limited to this prompting/serialization; does not explore other interventions like fine-tuning, external search, or solver augmentation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Causal Language Modeling Can Elicit Search and Reasoning Capabilities on Logic Puzzles', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6555.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6555.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models solving spatial puzzle games, including details about the model, the puzzle, the reasoning or prompting method, performance metrics, internal representations, use of external tools, and any analysis or limitations reported.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Tree-of-Thoughts (prior work mention)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Tree-of-Thoughts prompting (Lon23 evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A tree-based search prompting technique applied to LLMs to explore multiple reasoning branches (Tree-of-Thoughts); prior evaluation (cited) achieved results only on smaller 5x5 Sudoku puzzles and is noted to be computationally expensive.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Tree-of-Thoughts (applied to various LLMs)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Prompting framework that steers LLMs to explore tree-structured chains of thought (expensive search over multiple thought branches); used with large LLMs like GPT-4 in cited prior work</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Sudoku (reported prior result on 5x5 Sudoku)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>constraint satisfaction / combinatorial logic (spatial)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>prior work evaluation (5x5 Sudoku reported by Lon23) — exact dataset not detailed in this paper</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Tree-of-Thoughts prompting (search over multiple CoT branches)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_technique</strong></td>
                            <td>explicit tree search over generated intermediate reasoning states (explores multiple candidate reasoning paths)</td>
                        </tr>
                        <tr>
                            <td><strong>internal_representation</strong></td>
                            <td>Not specified here; applied to LLM-generated reasoning traces</td>
                        </tr>
                        <tr>
                            <td><strong>use_of_external_tool</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Not numerically detailed in this paper; qualitative statement that results were achieved only on 5x5 Sudoku and technique is expensive</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Reported in prior work to succeed only on smaller (5x5) Sudoku instances; no numeric metrics provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>analysis_findings</strong></td>
                            <td>Authors note Tree-of-Thoughts has been applied to Sudoku in prior work but only succeeded at small-scale 5x5 puzzles and is computationally expensive; contrasted with their approach which trains on decomposed sequences rather than running expensive tree search at inference.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_comparison</strong></td>
                            <td>Compared conceptually: tree-of-thoughts performs expensive search at inference; the present paper shows training with decomposed order can yield strong performance without expensive inference-time tree search.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>As cited, Tree-of-Thoughts is computation-heavy and in prior reported evaluation only scaled to small Sudoku variants (5x5).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Causal Language Modeling Can Elicit Search and Reasoning Capabilities on Logic Puzzles', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Recurrent Relational Network <em>(Rating: 2)</em></li>
                <li>Sudoku solver with step-by-step guidance <em>(Rating: 1)</em></li>
                <li>Solving sudoku puzzles with recurrent neural networks <em>(Rating: 1)</em></li>
                <li>Tree of Thoughts <em>(Rating: 2)</em></li>
                <li>Puzzle solving without search or human knowledge: An unnatural language approach <em>(Rating: 1)</em></li>
                <li>GPT-2 models trained on natural language perform on puzzle tasks (NB21) <em>(Rating: 2)</em></li>
                <li>Learning to solve constraint satisfaction problems with recurrent transformer <em>(Rating: 2)</em></li>
                <li>3 million sudoku puzzles with ratings <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6555",
    "paper_id": "paper-272689782",
    "extraction_schema_id": "extraction-schema-130",
    "extracted_data": [
        {
            "name_short": "Solver-decomposed Transformer (this work)",
            "name_full": "42M-parameter decoder-only Transformer trained with solver-decomposed reasoning order",
            "brief_description": "A GPT-2-style 8-layer, 8-head causal Transformer (42M parameters) trained from scratch with next-token prediction on synthetic Sudoku and Zebra puzzles using a solver-provided adaptive order of cell/triplet outputs; demonstrates strong emergent search and reasoning and an implicit candidate-set representation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Transformer (GPT-2 style, 8-layer, 42M)",
            "model_description": "Decoder-only causal Transformer (GPT-2 architecture) with 8 layers, 8 attention heads, model dim 576, MLP dim 3456, trained with next-token prediction on synthetic puzzle sequences.",
            "model_size": "42M",
            "puzzle_name": "Sudoku (9x9) and Zebra (symbolic Einstein puzzles, sizes 3–6)",
            "puzzle_type": "constraint satisfaction / combinatorial logic (spatial for Sudoku; logic-grid for Zebra)",
            "dataset_name": "Sud20-derived Sudoku set (Radcliffe 2020 filtered to 1.9M solvable-by-7-strategies examples) and synthetic Zebra dataset (320k puzzles, sizes 3–6)",
            "prompting_method": "trained end-to-end with solver-decomposed reasoning order (Chain-of-Thought-like training signals); experiments also used hinted-position inference and beam-search decoding",
            "reasoning_technique": "learned application of human-like Sudoku strategies (7 strategies: Lone single, Hidden single, Naked pair/triplet, Locked candidate, XY-wing, Unique rectangle) and implicit candidate-set constraint propagation; model learns to (1) search for easy-to-decode cells and (2) apply strategies to produce values",
            "internal_representation": "Symbolic sequence encoding: Sudoku and Zebra inputs/outputs serialized as triplets (row, column, value) tokens for Sudoku and (entity, attribute, value) triplets for Zebra; causal token sequence with no explicit block token (block implied via row/col)",
            "use_of_external_tool": true,
            "external_tool_description": "A handcrafted Sudoku/Zebra solver (implements 7 Sudoku strategies and k-sized clue-subset deductions for Zebra) was used offline to filter datasets, to generate solver-decomposed reasoning orders for training, and to provide position-hints during some inference experiments; the learned Transformer itself does not call external tools at decode time except when experiments explicitly condition on solver hints.",
            "evaluation_metric": "Cell accuracy (percentage of unfilled cells predicted correctly) and Complete puzzle accuracy (percentage of puzzles with all cells correct)",
            "performance": "Sudoku (solver-decomposed training, greedy k=1): 94.23% cell accuracy, 87.18% complete-puzzle accuracy; beam-search k=3: 96.07% cell, 91.36% full; beam-search k=5: 98.03% cell, 94.21% full. Zebra (solver-decomposed training): 95.63% cell accuracy, 91.17% complete-puzzle accuracy; beam k=5: 96.26% complete-puzzle accuracy.",
            "analysis_findings": "1) Training with solver-decomposed (adaptive) output order is crucial — it decomposes search and strategy application and yields large gains. 2) The model implicitly represents candidate sets: extracting top-k logits (k = size of solver candidate set) yields &gt;93% overlap with solver candidate set across validation states, improving up to ~99% as more prefix is given. 3) The main failure mode is searching for the next easy-to-decode cell (step (1)), not applying the strategy once position is given: hinted-position accuracy = 99.02%. 4) Model misses more 'block'-based easy cells, likely due to no explicit block token in input encoding. 5) Mistakes early in a puzzle cascade (an invalid prefix increases later errors).",
            "ablation_comparison": "Order ablation: fixed-order training → 58.64% cell, 7.2% full; random-order training → ~52% cell, ~1% full; solver-decomposed training → 94.23% cell, 87.18% full. Hinted-position (inference) increases accuracy marginally for randomly trained model (~+3% cell). Hinted-position on solver-decomposed trained model: hinted cell accuracy 99.02% (shows strategy application is near-perfect when position is given). Beam-search ablation: k=3 and k=5 improve complete-puzzle accuracy from 87.18% (k=1) to 91.36% (k=3) and 94.21% (k=5).",
            "limitations": "Experiments restricted to puzzles solvable by a fixed set of 7 human-like, polynomial-time strategies (dataset filtered), so results do not extend to NP-hard Sudoku instances requiring backtracking; synthetic setting (symbolic puzzles) may not generalize to open-ended, creative, or probabilistic reasoning tasks; model training relies on an external solver to produce decomposed reasoning sequences (data-generation dependency); input encoding lacks explicit block token, correlating with failures on block-based easy cells; model size and training compute are modest relative to frontier LLMs, so scaling effects are not explored.",
            "uuid": "e6555.0",
            "source_info": {
                "paper_title": "Causal Language Modeling Can Elicit Search and Reasoning Capabilities on Logic Puzzles",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "GPT-4o (4-shot CoT)",
            "name_full": "GPT-4o (evaluated with 4-shot Chain-of-Thought prompting)",
            "brief_description": "A frontier large language model evaluated in a 4-shot Chain-of-Thought (CoT) prompting setup on serialized 9x9 Sudoku puzzles; achieved near-random per-cell accuracy and failed to solve any full puzzles in this experiment.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "GPT-4o",
            "model_description": "Frontier large language model (proprietary); evaluated in few-shot CoT prompting (4-shot) with serialized (row,column,value) triplet format",
            "model_size": null,
            "puzzle_name": "Sudoku (9x9)",
            "puzzle_type": "constraint satisfaction / combinatorial logic (spatial)",
            "dataset_name": "3000 Sudoku test examples (serialized triplet input), 4-shot CoT prompts constructed by authors",
            "prompting_method": "4-shot Chain-of-Thought prompting (CoT), prompts contained 4 puzzle-solution pairs before test puzzle)",
            "reasoning_technique": "none beyond CoT prompting; model not provided with external solver or search trace",
            "internal_representation": "Serialized as sequences of (row, column, value) triplets",
            "use_of_external_tool": false,
            "external_tool_description": null,
            "evaluation_metric": "Percent of puzzles solved fully and percent of cells answered correctly (per-cell accuracy)",
            "performance": "0% of puzzles solved fully; 9.5% of cells answered correctly (on the evaluated 3k examples)",
            "analysis_findings": "Frontier LLM GPT-4o, when given few-shot CoT examples serialized as triplets, failed to solve any 9x9 Sudoku fully and exhibited per-cell accuracy near random guessing; suggests that few-shot CoT prompting alone is insufficient for complex CSPs like full 9x9 Sudoku in this format.",
            "ablation_comparison": "Compared directly in the same 4-shot CoT setup to Gemini-1.5Pro which had similar failure (0% full, ~10.2% cell), demonstrating that both frontier LLMs struggled under this prompting format.",
            "limitations": "Results are for a specific 4-shot CoT setup and tokenization/serialization format; does not rule out other prompting, fine-tuning, tool-augmented, or search-based methods, but demonstrates brittleness of simple few-shot CoT for these spatial CSPs.",
            "uuid": "e6555.1",
            "source_info": {
                "paper_title": "Causal Language Modeling Can Elicit Search and Reasoning Capabilities on Logic Puzzles",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "Gemini-1.5Pro (4-shot CoT)",
            "name_full": "Gemini-1.5Pro (evaluated with 4-shot Chain-of-Thought prompting)",
            "brief_description": "A frontier LLM evaluated in the same 4-shot CoT setup on serialized 9x9 Sudoku puzzles; like GPT-4o, it failed to fully solve any puzzles and produced ~10% per-cell accuracy.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "Gemini-1.5Pro",
            "model_description": "Frontier large language model (proprietary Google model), evaluated in 4-shot CoT prompting on serialized triplet Sudoku format",
            "model_size": null,
            "puzzle_name": "Sudoku (9x9)",
            "puzzle_type": "constraint satisfaction / combinatorial logic (spatial)",
            "dataset_name": "3000 Sudoku test examples (serialized triplet input), 4-shot CoT prompts constructed by authors",
            "prompting_method": "4-shot Chain-of-Thought prompting (CoT)",
            "reasoning_technique": "none beyond prompting",
            "internal_representation": "Serialized (row, column, value) triplet token sequences",
            "use_of_external_tool": false,
            "external_tool_description": null,
            "evaluation_metric": "Percent puzzles solved fully; percent cells answered correctly",
            "performance": "0% puzzles solved fully; 10.2% of cells answered correctly (on the evaluated 3k examples)",
            "analysis_findings": "Gemini-1.5Pro performed at near-random per-cell levels under the 4-shot CoT protocol in this serialization, demonstrating that few-shot CoT does not suffice for solving standard 9x9 Sudoku in this setup.",
            "ablation_comparison": "Directly compared to GPT-4o under identical 4-shot CoT prompting; both models showed near-identical failure modes and low per-cell accuracy.",
            "limitations": "Same as GPT-4o: result limited to this prompting/serialization; does not explore other interventions like fine-tuning, external search, or solver augmentation.",
            "uuid": "e6555.2",
            "source_info": {
                "paper_title": "Causal Language Modeling Can Elicit Search and Reasoning Capabilities on Logic Puzzles",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "Tree-of-Thoughts (prior work mention)",
            "name_full": "Tree-of-Thoughts prompting (Lon23 evaluation)",
            "brief_description": "A tree-based search prompting technique applied to LLMs to explore multiple reasoning branches (Tree-of-Thoughts); prior evaluation (cited) achieved results only on smaller 5x5 Sudoku puzzles and is noted to be computationally expensive.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "Tree-of-Thoughts (applied to various LLMs)",
            "model_description": "Prompting framework that steers LLMs to explore tree-structured chains of thought (expensive search over multiple thought branches); used with large LLMs like GPT-4 in cited prior work",
            "model_size": null,
            "puzzle_name": "Sudoku (reported prior result on 5x5 Sudoku)",
            "puzzle_type": "constraint satisfaction / combinatorial logic (spatial)",
            "dataset_name": "prior work evaluation (5x5 Sudoku reported by Lon23) — exact dataset not detailed in this paper",
            "prompting_method": "Tree-of-Thoughts prompting (search over multiple CoT branches)",
            "reasoning_technique": "explicit tree search over generated intermediate reasoning states (explores multiple candidate reasoning paths)",
            "internal_representation": "Not specified here; applied to LLM-generated reasoning traces",
            "use_of_external_tool": false,
            "external_tool_description": null,
            "evaluation_metric": "Not numerically detailed in this paper; qualitative statement that results were achieved only on 5x5 Sudoku and technique is expensive",
            "performance": "Reported in prior work to succeed only on smaller (5x5) Sudoku instances; no numeric metrics provided in this paper.",
            "analysis_findings": "Authors note Tree-of-Thoughts has been applied to Sudoku in prior work but only succeeded at small-scale 5x5 puzzles and is computationally expensive; contrasted with their approach which trains on decomposed sequences rather than running expensive tree search at inference.",
            "ablation_comparison": "Compared conceptually: tree-of-thoughts performs expensive search at inference; the present paper shows training with decomposed order can yield strong performance without expensive inference-time tree search.",
            "limitations": "As cited, Tree-of-Thoughts is computation-heavy and in prior reported evaluation only scaled to small Sudoku variants (5x5).",
            "uuid": "e6555.3",
            "source_info": {
                "paper_title": "Causal Language Modeling Can Elicit Search and Reasoning Capabilities on Logic Puzzles",
                "publication_date_yy_mm": "2024-09"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Recurrent Relational Network",
            "rating": 2,
            "sanitized_title": "recurrent_relational_network"
        },
        {
            "paper_title": "Sudoku solver with step-by-step guidance",
            "rating": 1,
            "sanitized_title": "sudoku_solver_with_stepbystep_guidance"
        },
        {
            "paper_title": "Solving sudoku puzzles with recurrent neural networks",
            "rating": 1,
            "sanitized_title": "solving_sudoku_puzzles_with_recurrent_neural_networks"
        },
        {
            "paper_title": "Tree of Thoughts",
            "rating": 2,
            "sanitized_title": "tree_of_thoughts"
        },
        {
            "paper_title": "Puzzle solving without search or human knowledge: An unnatural language approach",
            "rating": 1,
            "sanitized_title": "puzzle_solving_without_search_or_human_knowledge_an_unnatural_language_approach"
        },
        {
            "paper_title": "GPT-2 models trained on natural language perform on puzzle tasks (NB21)",
            "rating": 2,
            "sanitized_title": "gpt2_models_trained_on_natural_language_perform_on_puzzle_tasks_nb21"
        },
        {
            "paper_title": "Learning to solve constraint satisfaction problems with recurrent transformer",
            "rating": 2,
            "sanitized_title": "learning_to_solve_constraint_satisfaction_problems_with_recurrent_transformer"
        },
        {
            "paper_title": "3 million sudoku puzzles with ratings",
            "rating": 2,
            "sanitized_title": "3_million_sudoku_puzzles_with_ratings"
        }
    ],
    "cost": 0.01496175,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Causal Language Modeling Can Elicit Search and Reasoning Capabilities on Logic Puzzles
September 17, 2024</p>
<p>Kulin Shah kulinshah@utexas.edu 
Rina Panigrahy rinap@google.com </p>
<p>UT Austin</p>
<p>Nishanth Dikkala Google Research</p>
<p>Causal Language Modeling Can Elicit Search and Reasoning Capabilities on Logic Puzzles
September 17, 2024B145C229FBFD9F047C564A4589382C7BarXiv:2409.10502v1[cs.LG]
Causal language modeling using the Transformer architecture has yielded remarkable capabilities in Large Language Models (LLMs) over the last few years.However, the extent to which fundamental search and reasoning capabilities emerged within LLMs remains a topic of ongoing debate.In this work, we study if causal language modeling can learn a complex task such as solving Sudoku puzzles.To solve a Sudoku, the model is first required to search over all empty cells of the puzzle to decide on a cell to fill and then apply an appropriate strategy to fill the decided cell.Sometimes, the application of a strategy only results in thinning down the possible values in a cell rather than concluding the exact value of the cell.In such cases, multiple strategies are applied one after the other to fill a single cell.We observe that Transformer models trained on this synthetic task can indeed learn to solve Sudokus (our model solves 94.21% of the puzzles fully correctly) when trained on a logical sequence of steps taken by a solver.We find that training Transformers with the logical sequence of steps is necessary and without such training, they fail to learn Sudoku.We also extend our analysis to Zebra puzzles (known as Einstein puzzles) and show that the model solves 92.04% of the puzzles fully correctly.In addition, we study the internal representations of the trained Transformer and find that through linear probing, we can decode information about the set of possible values in any given cell from them, pointing to the presence of a strong reasoning engine implicit in the Transformer weights 1 .</p>
<p>Introduction</p>
<p>Language models using the Transformer architecture [VSP + 17] have displayed remarkable abilities on a variety of Machine Learning tasks over the last few years [BMR + 20, RWC + 19].Trained with simply the task of predicting the next token on huge amounts of text, these models display highly performant and deep language understanding skills.In order to make progress on achieving a human-like artificial intelligence, one of the most important ability is the ability to perform human-like reasoning and planning.Although LLMs have displayed a seemingly remarkable ability to excel at reasoning and planning tasks as well, it is a ongoing debate as to whether this ability comes from a true understanding and reasoning of the underlying problem or some other process which simulates reasoning but can be highly brittle.For instance, although LLMs show remarkable performance on benchmarks requiring non-trivial reasoning and planning skills such as MATH [HBK + 21], HumanEval [CTJ + 21] and others, there is research showing that these abilities can be extremely brittle or worse, the model is simply performing 'approximate retrieval' [VOSK22, DLS + 24].</p>
<p>In this work, we aim to understand how complex a reasoning task can Transformers trained with next-token prediction solve by focusing on a set of synthetic tasks: logic puzzles.In this work we focus our analysis on two types of logic puzzles: Sudoku puzzles and Zebra puzzles.</p>
<p>• Sudoku.In the classic variant of Sudoku, we are given a 9 × 9 grid where each cell is to be occupied by a number in the range {1, 2, . . ., 9}.The constraints are that the numbers along each row and column should be unique.In addition, the numbers within each 3 × 3 mini-grid should also be unique.Given a set of initially filled positions, the goal is to figure out the values that can occur in the unfilled cells.In standard Sudoku puzzles, there will always only exist a unique solution to the puzzle.</p>
<p>• Zebra Puzzles.There are a more verbal style of a puzzle (Figure 1) where we need to fill in values in a grid again but this time the type of possible constraints is much richer.These are also known as Einstein riddles.</p>
<p>Focusing on synthetic tasks like this gives a precise handle on what data the model has seen, and allows us to also control the difficulty of reasoning required for the task, see e.g.[LHB + 23, AZL23, LSL + 23, LAG + 22].</p>
<p>There are 3 people next to each other in a row.Everyone has a different name: Ali, Rose, Randy.Every one lives in a different colored house: gold, silver, indigo.Everyone likes a different drink: orange juice, beer, coffee.Match the people to the correct value for each of their characteristics using the clues.</p>
<p>1.The person who likes orange juice is immediately to the left of the person who likes coffee.</p>
<ol>
<li>
<p>The person who likes beer is somewhere to the left of the person who lives in the indigo house.</p>
</li>
<li>
<p>The person at the 1st position is Rose.</p>
</li>
<li>
<p>Randy is not the person who likes orange juice.</p>
</li>
<li>
<p>Randy is the person who lives in the gold house.Prior works have studied how causal language modeling with Transformers performs on synthetic tasks such as learning how to make valid moves in Othello, learning context-free grammars, learning deterministic finite automata and learning specific algorithmic tasks [LAG + 22, LHB + 23, NLW23, AZL23, YXLAZ24a, YXLAZ24b].Compared to these, Sudoku puzzles present a more challenging task.The Sudoku environment is a highly challenging Constraint Satisfaction Problem (CSP) and determining the value in even a single cell can require highly complex reasoning involving multiple steps.In general the extension of the puzzle to n × n grids is known to be NP-complete [YS03].Same is the case for Zebra puzzles.However, we will consider a class of logic puzzles which can be solved in polynomial time.This class still remains non-trivial to learn.For an idea of how challenging these can be, we performed a small experiment on how well some frontier LLM's of today can solve Sudoku puzzles.We prompted them in a 4-shot manner with 4 Sudoku puzzles (we serialize a puzzle by converting it into a sequence of (row, column, value) triplets) and their corresponding solutions given before asking for the solution for a 5th test Sudoku puzzle.We evaluated 3000 examples on Gemini-1.5Pro and GPT-4o.We observed that neither models are able to solve any of the puzzles fully correctly.The results are summarized in Table 1.</p>
</li>
</ol>
<p>Model</p>
<p>Percent of puzzles solved fully Percent of cells answered correctly GPT-4o 0% 9.5% Gemini-1.5Pro 0% 10.2%</p>
<p>Table 1: Results of 4-shot with CoT prompting on Sudoku solving by two of the frontier LLM models.They solved 0 % of the puzzles completely right and their accuracy on a per cell basis was around 9-10% (close to random guessing).</p>
<p>Our setup</p>
<p>We treat each Sudoku/Zebra puzzle as a sequence to sequence problem.In a Sudoku, given the sequence of filled cell positions and their values, the model needs to output the sequence of unfilled cell positions and their corresponding values.Similarly in a Zebra puzzle, we are given all the clues and the possible values for the characteristics in a sequential manner and we need to predict the values in the grid.To focus attention on a model's reasoning abilities, we abstract out symbolic versions of the Zebra puzzles.This means we refer to each person as an entity indexed by a number and their favorite color or car becomes an attribute number.For both types of puzzles, clearly, the order in which the model outputs the sequence of filled cells doesn't matter as long as the values are correct.However, we will see that the order in which the solutions are presented to the model during training makes a significant difference in the final performance of the model.</p>
<p>We consider a dataset of Sudoku puzzles of varying difficulty levels from [Rad20].In addition, we use a Sudoku solver which employs a set of 7 strategies that humans commonly use for solving Sudokus.Given these set of 7 strategies the solver iteratively scans through all unfilled cells and checks if progress can be made using one or more of the strategies.If it finds a cell where progress can be made if fills in its value and repeats the process of searching for the next cell to fill.Although some of the 7 strategies are simple and direct, some of them are highly non-trivial and non-local.From the dataset, we filter out those puzzles which cannot be solved by our solver and end up with 1.9M examples.This ensures that all our puzzles are solvable in polynomial time2 .</p>
<p>We can characterize the size of a Zebra puzzle by a tuple of two numbers: the number of entities and the number of attributes.Each clue in a puzzle is one of 7 different types.We generate around 320,000 Zebra puzzles of sizes varying between (3,3) to (6,6) in the following manner.We first design a human-like solver for these puzzles which tries to solve the puzzles in an iterative manner without backtracking.This solver runs in time cubic in the number of clues of the puzzle.When generating a puzzle of a certain size, we iteratively keep adding clues to a clue set until our solver is able to solve the puzzle.This way, we can ensure that, similar to our Sudoku puzzles, all our sampled Zebra puzzles are also solvable efficiently.Note that, even in the symbolic format, there are an exponential number of puzzles possible implying that our train set and test set won't overlap with a very high probability.</p>
<p>Our Results</p>
<p>In this section, we provide an overview of our results.We mainly focus on the Sudoku puzzles to explain our results and include a brief discussion on the Zebra puzzles.The more detailed results are provided in the respective sections.</p>
<p>Our first experiment studies whether a Transformer is capable of solving the Sudoku puzzle in a fixed cell order (from top-left to bottom-right).This would amount to the model knowing what values to fill in each unfilled cell in a single forward pass.We observe that although the model learns to predict the values for some cells in a puzzle (average cell accuracy 58.64% across all unfilled cells), in general, this leads to poor accuracy of solving the complete puzzle (7.2%).</p>
<p>Observe that solving a Sudoku puzzle can be thought of as finding easy-to-decode cells and then finding correct value at such cells.We combine this observation with insights from Chain-of-Thought prompting and use our solver to provide the order to fill cells for a given puzzle.In this setting, we use the cell positions provided by the solver during the decoding (i.e., position hints of easy to decode cells) and calculate how many cell values the model gets right.In other words, given a prefix of a partially solved puzzle, we query the solver to find out the "easiest" cell position to solve next and then, conditioning on this position, query the model for its value.The average cell accuracy only goes up marginally by about 3%.</p>
<p>To exploit the full value of the order given by our solver, we train the model from scratch using the solver order.This allows the model to learn what is a good strategic order in which to fill the cells.Importantly this order is adaptive based on the puzzle.To train it in this manner, we first feed each puzzle to our solver and collect the sequence of cells it fills in order.We use these sequences as our training data which acts as our Chain-of-Thought data for the model.We observe that this leads to a much stronger model which is able to solve full Sudoku puzzles to an accuracy of 87.18% (see Section 3.4).</p>
<p>Given this new model, we again try giving position hints during decoding as above and we see the average cell accuracy shoot up to 99.02%.This indicates the following.The iterative process of solving Sudokus can be broken down into two steps: (1) searching and finding a cell position where we can apply a subset of the strategies, (2) given a cell position, computing the value that needs to be filled in that position.</p>
<p>Step (1) is the harder task for a model to learn.We provide examples of Sudoku puzzles where the model makes a mistake in step (1) where step (2) is quite trivial (see Section 4.3 for more details).To make the model more proficient at solving the puzzle without the position hints, we perform a beam search of width 3 or 5 and notice that this suffices to get stronger full puzzle solving accuracies of 91.36% and 94.21% respectively (see Section 3.5).</p>
<p>In an environment where the model needs to search over a set of candidates to take as the next step, recent work by [BN24] demonstrated that next-token prediction might be a flawed objective.Another recent work [LSM + 24] posit including the entire search trace as part of the training Chain-of-Thought data to help a Transformer learn tasks involving search and planning dynamics.In contrast to these works, we observe that Transformers trained with the next-token prediction objective and without access to the entire search trace can learn complex reasoning tasks.</p>
<p>Finally, we further ask if we can see a similarity between the model's way of solving the puzzles to a humans/solvers way of solving the puzzle.We study this via probing which has been a technique to understand the latent conceptual content, see e.g.[LHB + 23, AZL23, PCV23, NLW23, JRR + 24].In particular, works such as [PCV23, NLW23, JRR + 24] argue that often simple functions of the model's activations or weights can extract useful latent information.We study the following via probing.Generally, humans and algorithmic solvers for Sudoku keep track of a possible set of values for each cell at a given state of the board to make progress on solving the Sudoku puzzle.We also see that the model also implicitly keeps track of a candidate set and this candidate set matches with the solver's candidate set (see Section 4.4 for more details).</p>
<p>We perform a similar set of experiments as above on Zebra puzzles and observe qualitatively similar trends giving evidence that our conclusions are not limited to the domain of Sudoku (See Section 5 for more details).In summary, our contributions are 1.We show that causal language modeling with the Transformer architecture is capable of learning to perform search and reasoning in highly non-trivial domains like Sudoku and Zebra puzzles.</p>
<ol>
<li>
<p>We present evidence that the right form of training data which decomposes the problem into smaller constituents is crucial.However this data is not required to be too descriptive.In particular, it need not contain search traces similar to those provided in [LSM + 24].</p>
</li>
<li>
<p>We perform a probing analysis to show that human-like abstract reasoning concepts such as candidate set of values emerge implicitly within the model's activations.</p>
</li>
</ol>
<p>Related work</p>
<p>There are many works which study the ability of language models to perform reasoning tasks which involve search and planning with mixed evidence as to whether they are actually learning to reason and plan.[BMR + 20] was a seminal work which showed that large language models (LLMs) are few-shot learners and [KGR + 22] argued that they can be zero-shot reasoners.In contrast to the above, [VOSK22] show that LLMs when acting alone or when combined with techniques such as Chain-of-Thought or Tree-of-thought cannot solve some standard planning and reasoning benchmarks when the questions are rephrased with a new terminology.This is even when we use techniques such as Chain-of-Thought, fine-tuning etc. [XZC + 24] show that even the biggest LLMs perform very poorly at real-world travel planning tasks with a multitude of soft and hard constraints.[DLS + 24] show that LLMs are limited and brittle in their ability to perform compositional tasks such as multi-digit multiplication, logic grid puzzles and dynamic programming.[MHF + 23] argue that LLMs have weak cognitive maps which are crucial for planning.[BN24] show that rather than the architecture, the training objective of next-token prediction might be crippling the planning and reasoning ability of a language model.</p>
<p>There are many works which use the help of synthetic tasks to gain insights into how Transformer language models work.We present a non-exhaustive list here.Comparison to Traditional Solvers and other ML approaches.Traditional constraint satisfaction libraries use very powerful combinatorial search algorithms to solve logic puzzles and are much more powerful than any deep model we learn here.In addition, many prior works study machine learning-based approaches for solving general combinatorial problems [BPL + 16, MSIB21, CFK + 23].In addition, there are several approaches that tend to handcraft the architecture or loss to the puzzle using human understanding of the puzzle structure [MKPZ11,PPW18,Zhu].Even though our goal is to understand the capabilities and limitations of causal language modeling and not to compete with such solvers, we discuss some of these works more in detail.</p>
<p>[MKPZ11] try to setup a Hopfield network to solve Sudoku puzzles.[PPW18] handcrafts the recurrent network to match the puzzle structure (and obey the constraints) and performs multiple rounds of message passing between cells of the sudoku puzzle to arrive at a solution.We evaluate our trained model (trained using causal language modeling) on the test dataset proposed in this work and we observe a comparable performance without handcrafting the network or loss function.[Zhu] achieves a 65% accuracy of RNN based solvers on 3x3 Sudoku puzzles.[NB21] study how well GPT-2 models trained on natural language perform on puzzle tasks such as Rubik's cube and Sudoku.[YIL23] study solving Sudokus using a recurrent form of Transformers by baking the knowledge of Sudoku's constraints into the model architecture and training pipeline.For chess, works like [RDM + 24] use a chess engine such as Stockfish to provide supervised labels for different board states and train a Transformer network to predict the value function of a board state.This can then be used to play expert level chess.</p>
<p>Preliminaries and setup</p>
<p>In this section, we provide a brief overview of the logic puzzles we consider and the input/output data format that is fed to the model.</p>
<p>Sudoku puzzle and solver.The goal of the sudoku puzzle is to fill out the whole board with numbers 1 to 9 without having duplicates in each row, column, and box (See appendix E for more details).Unless specified otherwise, we will use (r, c) to denote the position of a cell on the board and v(r, c) to denote the value at position (r, c) where r ∈ {1, 2, . . ., 9} denotes the row number of the cell and c ∈ {1, 2, . . ., 9} denotes the column number of the cell.Additionally, we use b(r, c) to denote the block number (among one of the nine 3 × 3 blocks) of the cell at position (r, c).To solve a Sudoku puzzle, a sudoku-solver (and humans up to an extent) keeps track of the candidate set for each of the empty cells.See more details about Candidate set in Appendix E.</p>
<p>As mentioned earlier, the generalized version of Sudoku with board size n × n is NP-complete [YS03].This implies that for some Sudoku puzzles, progress likely can not be made using any strategy that executes in polynomial time.We avoid such puzzles by restricting our focus to those Sudoku puzzles that can be solved using a set of 7 well-known and commonly used strategies which are executable efficiently 3 .Further details about each of the strategies is provided in Appendix A. An important point to note is that not all the strategies fill a value in a cell.In fact, only 2 out of 7 strategies that we use, fill a value in a cell and the other strategies are used to eliminate possible values of a cell and narrow down the candidate set at a particular cell.Additionally, some strategies (e.g., XY wing, Unique rectangle) involve reasoning on multiple cells in different rows/columns/blocks and these strategies don't fill a value at any cell and therefore, these strategies need to be applied in combination with other strategies to deduce a value at a cell.Additionally, we only provide the solution list of cell values to the puzzle during training, therefore the model is not getting any direct signal about the strategies that eliminate possible values of a cell and is only getting a signal in combinations of the strategies that deduce a value.</p>
<p>Zebra puzzle and solver.The Zebra puzzle is characterized by the number of entities m and the number of attributes n for each entity.e.g., in Figure 1 each person is an entity, and name, house color and drink are attributes associated with each entity.The relationships between entities and attribute values are given as clues in the puzzle and the task is to figure out values for all attributes and all the entities.See Figure 1.</p>
<p>Observe that each clue puts some constraints on the value of attributes and entities.e.g., "The person who likes beer is somewhere to the left of the person who lives in the indigo house."clue says that the house (entity) which has drink attribute = beer is somewhere left to the house whose color attribute = indigo.As we allow more and more complex relationships in clues, the puzzles become more and more complex.In addition, increasing size makes the puzzles more complex as well as more and more interconnected clues are required to uniquely pin down a solution.Larger puzzles also have a higher chance of deeper and trickier reasoning chains being utilized.Similar to Sudokus, a generalized version of m × n Zebra puzzles is also NP-hard.Unlike Sudoku puzzles, where the constraints are only the uniqueness constraints within each row, column and box, Zebra puzzles can have a much more diverse set of constraints which significantly increases the number of 'strategies' that can be used to make progress.Moreover, Zebra puzzles are a step closer to natural language than Sudoku puzzles.</p>
<p>In the Zebra puzzles, we have 7 different types of clues.Details about each of the clue types is provided in Appendix A. We generate our own dataset of Zebra puzzles as follows.We first create a Zebra puzzle solver.The solver for the Zebra puzzle takes in a clue set and iteratively tries to make progress by using k-sized subsets of the clues at a time (for k ranging from 1 to 3).If it is able to make a deduction, the solver marks that entry in the answer table and iterates over the clue subsets again.Given this solver, a new puzzle is generated by starting with an empty clue set and iteratively adding randomly generated clues until the solver is able to successfully solve the puzzle.While adding new clues we ensure we do not add duplicates.Nonetheless, some clues might still be rendered redundant due to the presence of 2 or more other clues.To keep the clue set lean, once we have a puzzle that the solver is able to solve, we filter out the clues unused by the solver.We generate puzzles of sizes m × n for m, n ranging in [3, 4, 5, 6].</p>
<p>Dataset, model architecture and training.Our training dataset for the Sudoku experiment contains 1.8M puzzles and the test dataset contains 0.1M puzzles.Each puzzle also comes with a difficulty rating calculated as follows.To rate a puzzle, a backtracking based solver (different from the one we use to generate our solver-order data) is employed.This solver tries to iteratively make progress on a puzzle using some elimination techniques.When it gets stuck, it makes guesses and tries to solve the puzzle.The difficulty rating is the maximum depth of the guess stack the solver had to use to solve the puzzle.Therefore, even a puzzle rated 0.5 can require complex strategies beyond simple scanning to solve them without guessing.We train a sequence-to-sequence model that takes in as input a representation of a Sudoku puzzle as a sequence and needs to output the solution of the puzzle as a sequence.During the training, we provide information about a single cell using three tokens (r, c, v(r, c)): the first two tokens (r, c) contain information about the position of the cell (row and column number) and the third token contains the number in that cell.Each training sequence is divided into two parts.The first part contains the information about cells whose values are given in the puzzle question and the second part contains information about unfilled cells in the solution.</p>
<p>Note that there can multiple valid orders for the solution.Also, note that the length of the first part depends on the number of cells filled in the puzzle.We train the model using the next-token prediction loss but we don't apply the loss corresponding to the prediction of the filled cells given in the question.</p>
<p>Our training dataset for the Zebra experiment contains 0.3M puzzles and the test dataset contains 15k puzzles.</p>
<p>The input to the model during the training is divided into two parts.The first part (given in the puzzle) contains the clues and the second part (solution) contains values for all attributes and all entities.Each clue contains two parts: 1) the relationship type between attributes and entities and 2) the specific attributes and entity values that are in this relationship whereas the solution part of the puzzle consists of multiple triplets of entity, attribute, and the solution for that entity and attribute.Similar to the sudoku puzzle, there can be different orders in which the solution triplets for each entity and attribute can be provided during the training.</p>
<p>We use a Transformer-based GPT-2 [RWC + 19] architecture with 8 layers for both puzzles.Each layer has 8 attention heads with a model dimension of 576 and an MLP of hidden dimension 3456 (6× model dimension) follows in each layer.The total number of parameters of our model is 42M.We use causal masking in the attention layers to avoid looking into the future.More details about the dataset, our architectures and hyperparameters can be found in Appendix B.</p>
<p>Evaluation metrics.To evaluate the performance of our model, we use the following two metrics primarily: 1) Cell accuracy: denotes the percentage of the unfilled cells whose values are correctly predicted by the model.2) Complete puzzle accuracy: denotes the percentages of the correctly solved puzzles in the evaluation dataset.A puzzle with even a single mistake is counted as incorrect.</p>
<p>Experiments on Sudoku puzzles</p>
<p>We study the performance of a Transformer model when the model is trained with the next-token prediction objective.We set up the model architecture and training of the model as discussed in Section 2 however, the question remains how to order cells in the input sequences of the Sudoku puzzle during the training of the model.Note that given the state of a Sudoku puzzle, some cells might be easier to solve than others so the order of the cells of input sequences provided during training could be important.We first try using a predefined fixed order or a random order of the cells during the training and inference in Section 3.1.However, this leads to poor performance.Thereafter, we turn our focus on using a solver to create a better order which we call solver-decomposed reasoning order (Section 3.2).Inspired by Chain-of-Thoughts literature [WWS + 22b], Section 3.3 uses solver-decomposed reasoning order only during the inference on the above trained models to provide position hints.Yet, conditioning on these position hints during decoding only provides a relatively small improvement in the performance showing that even if we tell the model to find the value in a particular cell, it has not learnt fully how to do so.</p>
<p>Therefore, in Section 3.4, we explore training the model using cells provided in solver-decomposed reasoning order.This provides a huge boost to the performance allowing the model to solve over 85% of the puzzles in the test set accurately.However, it still does not achieve near-perfect cell accuracy.Therefore, Section 3.5 uses beam search decoding to improve the performance.</p>
<p>Training using fixed or random order of the cells</p>
<p>A natural choice for the cell order in the input sequence would be to use a fixed order of the cells or a random order of the cells in the puzzle for the input sequence.Note that the order of the puzzle is only provided during the training and we do not penalize the model for wrong order during evaluation as long as it solves the given sudoku puzzle correctly.</p>
<p>Fixed order of the cells.In this ordering of the cells, we arrange the cells in a predefined fixed order of top-left to bottom-right of the board of the puzzle.To be more precise, for any two cells (r, c) and (r ′ , c ′ ) where r and r ′ denote the row numbers and c and c ′ denote the column number, we will order the first cell (r, c) before (r ′ , c ′ ) if r &lt; r ′ or r = r ′ and c &lt; c ′ .We order both parts of the puzzle (input sequence) -given cells in the puzzle and the remaining solution of the puzzle using the above-mentioned ordering.</p>
<p>Random order of the cells.Another way to arrange the cells that we consider is to randomly order cells in given cells of the puzzle and solution of the inputs.For any given prefix (state of the puzzle), we randomly pick a cell from the set of empty cells and append that cell and corresponding value to the prefix.</p>
<p>Results.We provide the experimental results for the fixed order in Figure 3.We see that the model trained with fixed order achieves 58.64% cell accuracy and only 7.2% full puzzle accuracy whereas the model trained with random order only achieves around 52% cell accuracy and only 1% complete puzzle accuracy.</p>
<p>In the above ordering of the cells, given a state of the puzzle, the model decides on a random cell or fixed cell to output value but at that state, only a few cells might be easier to solve than others and the model trained using random or fixed order of cells do not necessarily decode the easier cells at that state.Therefore, inspired by Chain-of-thought literature [WWS + 22b], we ask the following question: if we provide the model information during inference which cells are easier to fill then does the performance improve?Before we answer the above question, we define the solver-decomposed reasoning order which will be useful in finding cells that are easier to fill.</p>
<p>Solver-decomposed reasoning order</p>
<p>A natural way humans solve Sudoku is by iteratively trying to find cells that look easier to fill.The search process involves trying to see if any of a given set of strategies can be applied to fill in the value or otherwise make progress on a cell.Inspired by this analogy, we construct an order of filling cells using a solver.The solver uses 7 strategies as mentioned in Section 2. At any given state of the puzzle, it tries to apply to an easier strategy first and if it can not make progress with an easier strategy then it goes to a harder strategy.</p>
<p>To apply a strategy, the solver goes through all the cells and tries to apply the strategy for each cell to make progress toward solving the puzzle.Progress doesn't necessarily mean filling a value in a cell but simply eliminating possible values from the candidate set (set of possible values) of a cell also counts as progress.</p>
<p>We call the order given by the solver as solver-decomposed reasoning order or decomposed reasoning order for brevity when it is clear from the context.Note that the decomposed reasoning order arranges the cells based on how easy they are to fill in, as the solver initially employs simpler strategies to make progress.</p>
<p>Hinted cell accuracy</p>
<p>In recent years, chain-of-thought (CoT) prompting [WWS + 22b] has emerged as one of the effective techniques to extract complex reasoning abilities from a model.The main idea of CoT is to lead the model to the correct output by providing intermediate steps to help the model.Inspired by the CoT prompting, we ask if providing the model additional information about easier-to-decode cells during inference improves the performance?</p>
<p>Specifically, we use decomposed reasoning order to provide position hints during inference.Recall that to infer a value at position (r, c) at a state of the puzzle s, we provide (s, r, c) as input to the transformer model and the random-order baseline model is trained to predict value v as next token given positions in previous two-tokens (r, c), and because positions are chosen randomly during the training, the model is forced to use the positions in previous two-tokens (r, c) while predicting the value.(Note that this is not the case for the model trained with fixed order and therefore, we don't consider them in this experiment).</p>
<p>Now, to provide additional information to the model about the easier cells to predict, we use the decomposed reasoning order.Specifically, for any given state s, we provide the state s to the solver to obtain the position of the easiest cell.Suppose the solver picks (r ′ , c ′ ), then we provide (s, r ′ , c ′ ) to the trained model to predict a value at (r ′ , c ′ ).We reiterate this process for every non-empty cell of the puzzle.We measure the cell accuracy in this setting and we call this accuracy as hinted cell accuracy to denote the provided hints about easy-to-decode positions from the solver.</p>
<p>Results.We see that the model trained using random order achieves 54.57% hinted cell accuracy.This means that providing hints about easy-to-decode positions improves the accuracy by around 3% over without any hints.At first glance, it seems like the model is struggling significantly to implement the correct strategy even when we provide information about the positions of easy-to-fill cells as hints during the inference.However, this might not be the correct conclusion because of the following reasons: during the training, the model is trained to predict the value from random cells, and the model needs to learn and apply very hard strategies as well to improve its training loss.In the process of learning hard strategies, the model might fail to learn easier strategies as well because of various reasons (e.g., limited data corresponding to easier strategies, limited model size, etc.).</p>
<p>When we use decomposed reasoning order during the inference, it helps to improve the performance for the model trained using random-order of the cells but because the model needs to perform a hard search and reasoning task while decoding a value at a single cell, it not only seems to hurt the model in searching easy-to-decode cells but also affects its reasoning capabilities to decode a value at given cells even after we explicitly provide positions of easy to fill cells.This motivates us to use decomposed reasoning order during the training.</p>
<p>Using solver for CoT training</p>
<p>Solving the sudoku puzzle can be decomposed into two sub-tasks: 1) Search across the board to find the cells that are easy to fill and 2) After finding the easy-to-fill cell, apply the correct strategy on the cell to obtain a correct value in it.As mentioned earlier, the model trained for fixed-order and random-order does not have explicit incentives to perform a search for easy-to-fill cells.Therefore, the motivation for providing the solver-decomposed reasoning order during the training is to provide an order of cells such that training a model using the order helps the model to decompose the complex task of solving sudoku into smaller sub-tasks.</p>
<p>To provide decomposed reasoning order of cells during the training, we arrange the cells according to how easy to fill they are.Note that we can obtain this using Then, we use these sequences during the training with the next-token-prediction loss for all the tokens.Therefore, given a board state s, the loss corresponding to position tokens incentivizes the model to learn to find easy-to-decode cells, and the loss corresponding to value tokens incentivizes the model to learn the strategy.</p>
<p>Result.We provide the result for the decomposed reasoning order training in Figure 3.We see that using the decomposed reasoning order achieves the cell accuracy 94.23 % and complete puzzle accuracy 87.18% accuracy.Training the model on the decomposed reasoning order improves cell accuracy by around 36% over the fixed-order training and by around 43% over the random-order training.The most noticeable improvement comes in complete puzzle accuracy where decomposed reasoning order training achieves 87.18 % accuracy whereas the fixed-order training achieves around 8 % accuracy and the random-order training achieves around 1 %.</p>
<p>Hinted accuracy for training using solver-decomposed reasoning order.Even though solver-decomposed reasoning order training significantly improves performance over fixed-order training and random-order training, it does not achieve near-perfect accuracy.Therefore, to understand whether the model is struggling to perform a search for easy-to-decode training or to employ a strategy given a position, we perform the experiment of providing hints about easy-to-decode positions (presented in Section 3.3).Recall that to measure the hinted cell accuracy for a model, we provide information about easy-to-decode cells to the model during inference and measure cell accuracy in that setting.We see that the model with solver-decomposed reasoning order training achieves 99.02 % hinted cell accuracy.This means that the model can employ the correct strategy assuming it has access to information about easy-to-decode cells and that the performance gap in cell accuracy (94.23 % to near-perfect accuracy) is mainly due to searching for easy-to-decode cells.</p>
<p>Next, we try to bridge the gap between 94.23% cell accuracy and 99.02% hinted cell accuracy achieved by the model trained using decomposed reasoning order.To improve the accuracy, we first understand the number of cells that are filled for a puzzle when the model is making the first mistake for the puzzle.Note that when the model makes a mistake by filling in an incorrect value on the puzzle, then the probability of the model making a mistake on the remaining empty cells increases.We also see this happen in our experiments (See Appendix C).</p>
<p>A hypothesis about lower cell accuracy than hinted cell accuracy is that given a certain state of the puzzle, the model might be confused between several cells about which cells are easier to decode.However, if the model is allowed to explore multiple potential cells of the puzzle, it might figure out the true solution as the model will make a prediction confidently for the true solution of the puzzle compared to other wrong solutions.</p>
<p>Because of this reason, we try beam-search decoding for the model trained using solver-decomposed reasoning order.</p>
<p>Beam-search decoding</p>
<p>Beam-search decoding (used in many popular NLP systems, e.g.[WSC + 16]) in language modeling allows the model to explore multiple partial decoding of the sequences during the inference of a language model and output the most probable explored sequence.The beam width k of the beam search decoding denotes how many partial sequences (hypotheses) are kept at each step.At each step of the decoding, it expands all partial solutions of the puzzle by decoding one more token.Then, among this expanded partial solution set, the model selects the top k most probable partial solutions.This process is repeated for the decoding of every token.Note that beam search only maintains k possible output sequences throughout the decoding process.Compared to standard decoding, beam search incurs a computational overhead of a factor of k 2 .In the Sudoku puzzle, It is important to note that the beam search can not try out all possible outputs for the Sudoku puzzle.After all, the total number of outputs can be arbitrarily large because many of the empty cells will have on average 2 to 5 possible values and the total number of empty cells in the puzzle is at least 50.</p>
<p>Results.We present our results for beam-search decoding in Table 2.The beam search with k = 1 is equivalent to greedy decoding as it only keeps one partial sequence.We see that beam search with k = 3 improves the cell accuracy by around 2% and complete puzzle accuracy by around 4%.We see a similar improvement when we increase beam width from k = 3 to k = 5.Note that the cell accuracy with beam width k = 5 is able to bridge the gap from the hinted cell accuracy up to a large extent but does not need hints about easy-to-decode positions.</p>
<p>Analysis</p>
<p>In the above section, we showed that solver-decomposed reasoning order during the training can greatly help improve the model's performance.In this section, we analyze the trained model on several fronts.We compare the model's performance to a neural network-based method designed to solve the Sudoku puzzle [PPW18] in section 4.1.Section 4.2 contains the breakdown of the complete puzzle accuracy across various difficulties.Section 4.3 contains a discussion about failure cases of the model in searching easy-to-decode cells.In Section 4.4, we show that the candidate set information emerges in the model to explain how the learned model is solving the puzzles.</p>
<p>Comparison with neural network based Sudoku solver</p>
<p>The goal of our work is to understand the capabilities and limitations of causal language modeling (and not to propose a new approach to solve the Sudoku puzzle).To understand if there exists a performance gap between a model trained using causal language modelling and a model specifically designed to solve Sudoku puzzles, we compare the performance of our model with a neural network based Sudoku solver proposed in [PPW18].Palm et al. [PPW18] handcrafts the recurrent network to match the Sudoku puzzle structure (and obey the constraints) and perform multiple rounds of message passing between cells of the Sudoku puzzle to arrive at a solution.We evaluate our trained model on the test dataset proposed in [PPW18] of 18000 Sudoku puzzles (See Table 3 for the result).We observe that our trained model (trained using solver-decomposed reasoning order and causal language modeling) combined with the beam search obtains a comparable performance without handcrafting the network or loss function.</p>
<p>Eval accuracy Eval complete puzzle accuracy</p>
<p>Performance analysis of the model using the difficulty of the puzzles</p>
<p>We provide the breakdown of the complete puzzle accuracy across various difficulties in Figure 4 to better understand the performance of the trained model.We use the difficulty measure provided in the Kaggle dataset [Rad20].To obtain the difficulty of a puzzle, it considers a solver (different from the one we use to generate our solver-order data) which tries to iteratively make progress on a puzzle using some simple strategies.When the solver gets stuck, it makes guesses and tries to solve the puzzle.The difficulty rating is the average number of guesses the solver had to make to solve the puzzle.We wish to point out that even a puzzle rated 1.0 can require complex strategies beyond simple scanning to solve them without guessing and therefore, this is an imperfect measure of the difficulty.</p>
<p>In Figure 4, we observe that the model achieves almost perfect complete puzzle accuracy for lower difficulty accuracy and as the difficulty of the puzzle increases, the complete puzzle accuracy goes down.We want to note that even when the difficulty rating is between 3 to 3.5, the model can solve around 50 % of the puzzles completely.Additionally, the advantage of beam search increases for the higher difficulty puzzles as the model can explore multiple solutions when it has confusion and output a solution at the end.</p>
<p>Figure 4: Complete puzzle accuracy for different difficulty Sudoku puzzles.The difficulty rating is computed as the average number of guesses the rating-solver had to make to solve the puzzle therefore, the difficulty rating is an imperfect measure of the difficulty.</p>
<p>Failure in search for easy-to-decode cells</p>
<p>As discussed in section 3.4, the model trained using solver-decomposed reasoning order solves 94.23 % cells of the sudoku puzzles correctly.To understand the failure modes of the model, we measure the hinted cell accuracy by providing information about easy-to-decode cells to the model during inference.We see that the model achieves 99.02% accuracy.This shows that the model can find the correct value at the cell when it is provided the information about easy-to-decode cells and the performance gap in cell accuracy is mainly due to the inability to search for easy-to-decode cells.</p>
<p>We also provide some examples of the puzzle situations in Figure 5 and Figure 8 (in Appendix) when the model makes a mistake by trying to fill a cell but there is another cell for which it is easier to fill.This supports our finding by hinted cell accuracy that the performance gap of our trained model to the perfect accuracy is due to the inability to search for easy-to-decode cells.</p>
<p>Additionally, a cell can be easy-to-decode because of either row, column or block constraint of the Sudoku puzzle.We found that our trained model misses more cells which are easy-to-decode because of the block constraint.This might be due to the input format being not explicit for block and explicit for row and column of a cell (recall that a cell is in the format of (r, c, v(r, c)) as input to the model).A natural extension in this case could be to provide a block number also as an input to the model.We leave it for future work.</p>
<p>Emergence of candidate set information in the model</p>
<p>We saw in the previous section that a model trained with puzzles given in solver-decomposed reasoning order performs very well.Therefore, we focus on how the model is learning such a task that requires planning and reasoning.As mentioned earlier, the sudoku solver (and to an extent humans) keeps track of possible values that a cell can take for the given puzzle.Therefore, we ask the following question: does the model also keep track of possible values of a cell?Can we extract them from the model?</p>
<p>We answer both of these questions (perhaps surprisingly) positively by showing that for a given puzzle, the candidate set of the solver can be extracted from the logits of the model.The candidate set of an empty cell keeps track of possible values that the cell can take given a state of the puzzle.Note that given Calculating candidate set equivalence.For all puzzles in the validation dataset, we obtain the candidate set of all empty cells from the solver when the number of filled cells is in the set S = {35, 40, 45, 50, 55, 60, 65, 70, 75}.For a state s of a puzzle, we denote the candidate set of the solver at an empty cell (r, c) as f * (s, r, c).We use |f * (s, r, c)| to denote the number of possible values in the candidate set f * (s, r, c).To extract the candidate set from the model at the state s of the puzzle and at an empty cell (r, c), we feed (s, r, c) as the prefix to the model and values corresponding to top-k output logits where k = |f * (s, r, c)| becomes the candidate set of the model.We denote the model's candidate set as m(s, r, c).Importantly, note that we DO NOT only evaluate the top-k candidates on the cell the model chooses to predict.Although during its natural course of decoding the model might wish to decode cell location A, we force it (by conditioning) to decode at every other location and evaluate the top-k candidates.This ensures that we are looking at what the model thinks is the set of possible candidates of cell location (r 1 , c 1 ) even when it has decided to decode cell (r 2 , c 2 ) ̸ = (r 1 , c 1 ) next.We note that this style of probing differs from the more common way to perform a probing analysis which involves learning a linear/non-linear probe which takes in the embedding and outputs a label indicating a concept.However, we use probing in more general sense to refer to understand some of the inner workings of the model.Results.The results of candidate set equivalence accuracy are given in Table 4.We see that for all positions the average overlap between the solver's and the model's candidate set is above 93 %.This overlap improves to around 96.5 % when the prefix has information about 60 cells and to around 98.5 % when the prefix contains information about 70 cells.Note that to extract the candidate set of the model, we are just reading the logits and not even training a linear function.Additionally, the candidate set equivalence result is not only for cells that are easy to decode but for all empty cells.Moreover, during the training of the model, no direct information about the candidate set is provided and the model is only trained to predict the correct value for a cell and therefore is not directly incentivized to predict the correct candidate set for all the empty cells with such a high accuracy.</p>
<p>Experiments on Zebra puzzle</p>
<p>To extend our results beyond Sudoku, we also conduct our experiments on the Zebra puzzle (also known as Einstein's Puzzle).Like Sudoku puzzles, we consider providing the solution in either a fixed, random, or solver-decomposed reasoning order during the training.</p>
<p>Order of the solution The input provided for a Zebra puzzle during training consists of two parts: the clues and the solution.The solution part contains values assigned to each entity and attribute.Based on the given clues, certain values for specific entities and attributes are easier to determine than others (e.g., in Figure 1, the third clue immediately reveals that the first house has a person with the name attribute = Rose).Thus, as seen in the Sudoku puzzle, the order in which the solution is provided is important.</p>
<p>Similar to Sudoku puzzles, we consider providing the solution in either a fixed, random, or solver-decomposed reasoning order during the training.In all cases, the clues part of the input remains unchanged.In fixed-order training, the solution is given in a predetermined sequence (we use the order starting from the first house's first attribute to the last house's first attribute, followed by the next attribute).In random-order training, the solution is shuffled.In solver-decomposed reasoning order, the solution is arranged based on how the solver approaches the puzzle, progressively using smaller subsets of clues to make progress and therefore, dividing the reasoning to solve the puzzle into multiple stages.</p>
<p>Results</p>
<p>We plot the cell accuracy and complete puzzle accuracy on an evaluation set of 1k puzzles during the training in Figure 6.We see that the training using solver-decomposed reasoning order achieves 95.63 % cell accuracy and 91.17 % complete puzzle accuracy whereas the random order training achieves almost zero complete puzzle accuracy and the fixed order training achieves 79.36 % complete puzzle accuracy.We believe this is due to a larger number of small-sized Zebra puzzles in the evaluation set (e.g., puzzles with 3 or 4 attributes and entities) which are easier to solve than larger-sized Zebra puzzles.We also evaluate the model's performance by using beam search decoding and report the results in Table 5.We see that using beam search decoding with width=3 improves the performance by 0.7 % and increasing it to width=5 improves the performance by an additional 0.2 %.The training dataset contains Zebra puzzles with the no. of entities and the no. of attributes in {3, 4, 5, 6} set.For each combination of the no. of entries and attributes, we generate 20k puzzles therefore, the complete dataset contains 320k puzzles of varying sizes.From the complete dataset, we randomly choose 15k puzzles for evaluation and the rest of the puzzles for training the model.Evaluation accuracy: the percentage of correctly predicted attributes on the evaluation set and eval complete puzzle accuracy: the percentage of correctly and completely solved puzzles.</p>
<p>Conclusion</p>
<p>We have shown that even on complex logical reasoning tasks such as Sudoku and Zebra puzzles, simple next-token prediction provided with a high-level decomposition of the reasoning steps during training is able to learn to solve the task.This suggests that, given the right level of detail and breakdown of reasoning steps in the training data, a pre-trained model might already present as a strong reasoning engine (without the need for post-training techniques such as fine-tuning, prompt engineering, self-consistency, tree-of-thoughts etc).These techniques might help significantly boost the baseline performance of a model or potentially make up for deficiencies in the pre-training data however.To move towards more general reasoning systems, an interesting challenge to overcome would be to simulate the decomposed reasoning data in an efficient manner.These tasks capture many different types of constraint satisfaction problems and we believe the framework and results should generalize to other settings as well.</p>
<p>Finally, we conclude with some limitations of our study.Firstly, we note that we studied a synthetic setting on a toy task and real-world reasoning and planning tasks can be much more abstract and challenging.More specifically, Sudoku is a task which doesn't require the same degree of long-term planning as some harder benchmarks.That is, any cell we can make progress on is progress unlike constraint problems where one might need to backtrack.Moreover, we focused on a reasoning setting where creative thinking was not required.That is, the model did not need to invent new strategies to solve any test time puzzle.It is an interesting future direction to study to what extent causal language modeling can yield novel reasoning strategies.Moreover, there can be many different types of reasoning tasks which are not logic puzzles (for instance probabilistic puzzles or rule-less puzzles, see e.g.[GLFS24]) and our experiments do not explore those.4. Neighbour of: This relation type provides the information that an entity with a particular attribute value is the neighbor of another entity.This relation type generalizes the "immediate left" relationship to include the immediate neighbors of the left and right sides.An example this type of clue is the person with a dog is next to the person who drinks milk.</p>
<p>Ends in:</p>
<p>This relation type provides the information that an entity with the particular attribute value is on either end of the order.For example, the person with the Zebra is on either end of the order.</p>
<ol>
<li>Left of: This relation type provides the relative order of two entities with some particular values.Note that left-of relation does not mean the immediate left of an entity.For example, the person who drinks tea is left of the Japanese person.</li>
</ol>
<p>7.</p>
<p>In between: This relation type provides the relative order of three entities with some particular attribute values.For example, the Englishman lives in-between the person with the Horse and the person who drinks coffee.</p>
<p>B Details about hyperparameters of training</p>
<p>Dataset.We consider the Sudoku dataset from [Rad20] and then we adapt a Sudoku solver from [MP23] to filter out the puzzles that can not be solved by the 7 strategies listed above.After filtering, the dataset contains 1.9M puzzles.We randomly choose 0.1M puzzles from these puzzles and use them as a validation dataset for the evaluation of the model and the remaining 1.8M puzzles are part of our training dataset.We use the AdamW optimizer for our experiments.For all the experiments, learning rate is set to 1 × 10 −4 and models are trained for 4 million steps with a batch size of 64.We use the cosine learning rate schedule [LH16] with the first 4000 tokens as the warmup phase and an end learning rate factor of 0.2.</p>
<p>C Mistake position frequency experiment.</p>
<p>We present the results about the mistake frequency and first mistake frequency in Figure 7.In this section, we show that for a puzzle, the model makes more first mistakes for that puzzle at the start of the puzzle when there are more empty cells because for a model, it is harder to predict the correct value for that cell but the distribution of all mistakes is more towards the later mistakes.This shows that when a model makes a mistake on a sequence, it is likely that it will keep making a mistake because of the invalid prefix.the rows, columns, and boxes contain duplicates.In our experiments, we consider Sudoku of board size 9 × 9 which is further divided into nine 3 × 3 boxes.</p>
<p>E.2 Candidate set example</p>
<p>The candidate set for a position (r, c) keeps track of all possible values that the cell can take (See Section 4.4 for more details) and then uses the candidate sets to either deduce a value at a particular cell or narrow down the candidate set (set of possible values) at an empty cell.</p>
<p>Figure 1 :
1
Figure 1: An example Zebra puzzle with 3 entities, each having 3 attributes.</p>
<p>[LHB + 23] use the synthetic task of learning to predict valid next moves in an Othello game to study whether an internal model of the Othello board emerges in the model or not.[AZL23] use synthetic tasks such as learning context-free grammars to understand the mechanics of how large-scale learning in LLMs works.[GTLV22] use linear regression from in-context examples to study the in-context learning phenomenon.[NCL + 23] use the task of modular addition to understand the specific algorithm a shallow Transformer implements to solve the problem.[AG23] use the task of sorting a list of numbers to study length generalization.</p>
<p>Figure 2 :
2
Figure 2: Examples of complex strategies that involves reasoning about multiple cells.Left: XY-Wing, where a pivot cell (gray) has two candidate values (X and Y), the wing cells (green) share a column, row or box with the pivot and share one candidate value (X or Y) with pivot and another common candidate value (Z), then in any cell that shares a column, row or box with both wing cells (yellow), we can eliminate Z from the candidate set; Right: Unique Rectangle, where four cells form a rectangle, among which three cells (gray) share the exact same 2 candidate values, and the fourth cell (green) share at least one of the 2 values, then both values can be eliminated from the candidate set for the fourth cell.</p>
<p>Figure 3 :
3
Figure 3: Comparison of cell accuracy and full puzzle accuracy for fixed order training, random order training and solver-decomposed reasoning order training.</p>
<p>Figure 5 :
5
Figure 5: A failure case of the model in searching for easy-to-decode cells.The left figure shows the sudoku puzzle state when the model makes the first mistake and the right figure shows the puzzle's solution.Numbers given in the blue are provided in the puzzle.The puzzle makes a mistake by choosing to fill the red-colored cell whereas the green background cell can be easily filled.</p>
<p>Figure 6 :
6
Figure 6: Comparison of cell accuracy and full puzzle accuracy for fixed order training, random order training, and solver-decomposed order training</p>
<p>Figure 7 :
7
Figure 7: Left figure: Plots the number of mistakes made after how many number of cells were filled.Right figures: Plots the number of first mistakes that are made against number of cells that were filled when it made the first mistake.</p>
<p>Figure 9 :
9
Figure 9: An example of the candidate set for a puzzle</p>
<p>[LAD + 22]shows that by fine-tuning on the appropriate data LLMs can exhibit a high performance on the non-trivial MATH [HBK + 21] dataset.In addition, the reports of frontier models like GPT-4 [AAA + 23] and Gemini [TAB + 23] also contain support for the idea that LLMs can perform reasoning and planning.Building on these lines of work, [HXX + 22, SBM + 23, ABB + 22, SWW + 23] employ LLMs in planning tasks in the robotics domain.
A number of follow-up works study how we can improve the reasoning and planning capabilities of LLMsusing various techniques such as prompt engineering, tool use, using LLMs in combination with an ex-ternal deduction engine. Some of the prominent works in this bracket are Chain-of-Thought prompting[WWS + 22b], least-to-most prompting [ZSH + 22], self-consistency [WWS + 22a], tree-of-thought prompting[Lon23, YYZ + 24, XKZ + 24], program of thoughts [CMWC22], planning for code generation [ZCS + 23]. Ofthese, [Lon23] evaluate the efficact of Tree-of-Thought reasoning using LLMs like GPT-4 on solving Sudokupuzzles and only achieve results on 5x5 Sudoku puzzles, Moreover, the tree-of-thought prompting techniqueis known to quite expensive to run. [ZLH24] use LLMs to provide a commonsense world model and a policywhich can be fed to a Monte-Carlo tree search algorithm. The ReAct framework [YZY + 22] uses LLMs togenerate reasoning traces and task-specific actions in an interleaved manner.</p>
<p>Table 2 :
2
Performance (cell accuracy and complete puzzle accuracy) change as we increase beam-width in beam-search.
Cell accuracy Complete puzzle accuracyBeam width k = 194.23 %87.18 %Beam width k = 396.07 %91.36 %Beam width k = 598.03 %94.21 %</p>
<p>Table 3 :
3
Evaluating our model on the evaluation dataset of Sudoku given in Recurrent Relational Network (RRN) by Palm et al. [PPW18].Our trained model performs comparably to the RRN model but does not require handcrafting the network and training procedure for training on the Sudoku puzzles.
Beam search width=198.15 %94.76 %Beam search width=398.28 %95.12 %Beam search width=598.37 %95.43 %Recurrent Relational NetworkNA96.6 %</p>
<p>Table 4 :
4
The accuracy for the candidate set equivalence between the solver and the model at a state s of a puzzle and at an empty (r, c) is measured by |f * (s, r, c) ∩ m(s, r, c)|/|f * (s, r, c)|.The reported accuracy at position n ∈ S in Table4is the average over all empty cells when the number of filled cells is n for the puzzles which are correctly solved by the model.Intuitively, the candidate-set equivalence accuracy measures the average overlap between solver's and model's candidate set for the correctly solved puzzles.Candidate set equivalence accuracy when the number of filled cells is different in the given puzzle.The candidate-set equivalence accuracy measures the average overlap between the solver's and the model's candidate set for the correctly solved puzzles.
Number of filled cells354045505560657075Accuracy (%)93.19 93.23 94.14 94.81 94.70 96.60 97.71 98.54 99.37</p>
<p>Table 5 :
5
Zebra puzzle results.
Evaluation accuracy Eval complete puzzle accuracyBeam search width=195.63 %91.17 %Beam search width=396.03 %91.83 %Beam search width=596.26 %92.04 %
Each of the 7 strategies the solver uses can be generalized to a general n × n grid and they can be applied in poly(n) time
The list of the strategies we consider is Lone Single, Hidden Single, Naked Pair, Naked Triplet, Locked Candidate, XY Wing, Unique Rectangle,
AcknowledgmentsAuthors would like to thank Erik Vee for guiding them to use the hinted cell accuracy to understand the failure modes of the trained model.[DLS + 24] Nouha Dziri, Ximing Lu, Melanie Sclar, Xiang Lorraine Li, Liwei Jiang, Bill Yuchen Lin, Sean Welleck, Peter West, Chandra Bhagavatula, Ronan Le Bras, et al.Faith and fate: Limits of transformers on compositionality.Advances in Neural Information Processing Systems, 36, 2024.[GLFS24]Panagiotis Giadikiaroglou, Maria Lymperaiou, Giorgos Filandrianos, and Giorgos Stamou.Puzzle solving using reasoning of large language models: A survey.arXiv preprint arXiv:2402.11291,2024.[GTLV22] Shivam Garg, Dimitris Tsipras, Percy S Liang, and Gregory Valiant.A Details about our list of strategiesAs mentioned earlier, we consider puzzles with 7 strategies for both Sudoku and Zebra puzzles.A.1 Strategies for Sudoku puzzlesWe first list all the strategies used in Sudoku puzzles with an explanation.1. Lone single: This strategy is applied to a cell where only one candidate number is possible based on the rules.2. Hidden single: This strategy is applied the situation where a number can only be placed in one specfic cell within a row, column or box.3. Naked pair: This strategy is applied when two cells in a row, column or box contain the exact same two admissible numbers.This strategy is used to eliminate the number of possible values.4. Naked triplet": This strategy is applied when three cells in a row, column or box contain the exact same three admissible numbers.Similar to the "naked pair" strategy, this strategy is used to eliminate the number of possible values.Locked candidate:This strategy is applied when all the possible positions for a specific number within a box are on the same row or column.6. XY wing: This is a complicated strategy that involves three cells and multiple deduction steps.First, identify a vacant cell (called a pivot) that has two admissible numbers (denoted by X and Y ); second, identify two other cells (called wing cells) such that each of them shares a column, row or box with the pivot, and one cell has two admissible numbers X and Z, and the other cell has two admissible numbers Y and Z. third, for every other cell that share a column, row or box with both wing cells, Z can be eliminated from their admissible numbers.7.Unique rectangle: This is another complicated strategy that involves four cells.First, identify four cells that forms a rectangle such that three of these cells have only two admissible numbers and the numbers are the same, and the fourth cell share at least of the numbers as an admissible number; second, both numbers can be eliminated from the admissible numbers for the fourth cell.A.2 Relationtypes for Zebra puzzlesWe now list all the relationship types for the Zebra puzzles.The examples of the following relation types are given for the original Zebra puzzles.1. Is equal to: This relation type provides the value for an attribute.An example of this type of clue can be the Norwegian lives in the first house.Is not equal to:This relation type provides the information that an attribute can not have a particular value.An example of this type of clue can be the Englishman does not live in the first house.Immediate left:This relation type provides the relation order for the values either between attribute values or entities in the solution.An example of this type of clue can be the person with the dog is immediately left of the person who drinks the coffee.E An example of candidate set for the puzzleD Additional examples of failure mode of the modelE.1 Sudoku puzzleA generalized version of Sudoku has a board of size n × n with n boxes of size √ n × √ n, and the goal of the game is to fill a partially filled board so that each row, column and √ n × √ n boxes contains a full set of numbers from 1 to n. Implicitly, this means that the goal is to fill out the complete board such that none of
Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, arXiv:2303.08774arXiv:2204.01691Chuyuan Fu, Keerthana Gopalakrishnan, Karol Hausman, et al. Do as i can, not as i say: Grounding language in robotic affordances. Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, Chelsea Finn2023. 2022arXiv preprintABB + 22</p>
<p>Improving length-generalization in transformers via task hinting. Pranjal Awasthi, Anupam Gupta, arXiv:2310.007262023arXiv preprint</p>
<p>Language models are few-shot learners. Zeyuan Allen, -Zhu , Yuanzhi Li, ; Tom, B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, arXiv:2305.13673Physics of language models: Part 1, context-free grammar. Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam Mc-Candlish, Alec Radford2023. 2020arXiv preprintNeurIPS</p>
<p>Optimizing solution-samplers for combinatorial problems: The landscape of policygradient method. Gregor Bachmann, Vaishnavh Nagarajan ; Irwan, Hieu Bello, Pham, Mohammad Quoc V Le, Samy Norouzi, Bengio, arXiv:2403.06963arXiv:1611.09940Neural combinatorial optimization with reinforcement learning. A Oh, T Naumann, A Globerson, K Saenko, M Hardt, S Levine, Curran Associates, Inc2024. 2016. 202336arXiv preprintAdvances in Neural Information Processing Systems</p>
<p>Evaluating cognitive maps and planning in large language models with cogeval. Wenhu Chen, Xueguang Ma, Xinyi Wang, William W Cohen, Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, arXiv:2211.12588arXiv:2107.03374Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks. Ida Momennejad, Hosein Hasanbeig, Felipe Vieira Frujeri, Hiteshi Sharma, Nebojsa Jojic, Hamid Palangi, Robert Ness, Jonathan Larson, 2022. 2021. 2023arXiv preprintThirty-seventh Conference on Neural Information Processing Systems</p>
<p>Solving sudoku puzzles by using hopfield neural networks. Valeri Mladenov, Karampelas, Pavlatos, Zirintsis, Proc. of ICACM. of ICACM201111</p>
<p>Sudoku solver with step-by-step guidance. Aleksei Maslakov, Basil Papadimas, 2023</p>
<p>Reinforcement learning for combinatorial optimization: A survey. Nina Mazyavkina, Sergey Sviridov, Sergei Ivanov, Evgeny Burnaev, Computers &amp; Operations Research. 1341054002021</p>
<p>Puzzle solving without search or human knowledge: An unnatural language approach. David Noever, Ryerson Burdick, Neel Nanda, Lawrence Chan, Tom Lieberum, Jess Smith, Jacob Steinhardt, arXiv:2109.02797arXiv:2301.05217Progress measures for grokking via mechanistic interpretability. 2021. 2023arXiv preprintNCL + 23</p>
<p>Emergent linear representations in world models of self-supervised sequence models. Neel Nanda, Andrew Lee, Martin Wattenberg, arXiv:2309.009412023arXiv preprint</p>
<p>The linear representation hypothesis and the geometry of large language models. Kiho Park, Yo Joong Choe, Victor Veitch, arXiv:2311.036582023arXiv preprint</p>
<p>Recurrent relational networks. Rasmus Palm, Ulrich Paquet, Ole Winther, Advances in neural information processing systems. 201831</p>
<p>3 million sudoku puzzles with ratings. David G Radcliffe, 2020</p>
<p>Progprompt: Generating situated robot task plans using large language models. Anian Ruoss, Grégoire Delétang, Sourabh Medapati, Jordi Grau-Moya, Li , Kevin Wenliang, Elliot Catt, John Reid, Tim Genewein ; Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, arXiv:2402.044942023 IEEE International Conference on Robotics and Automation (ICRA). Arsalan Mousavian, Ankit Goyal, Danfei Xu, Jonathan Tremblay, Dieter Fox, Jesse Thomason, Animesh Garg, IEEE2024. 2019. 20231arXiv preprintLanguage models are unsupervised multitask learners</p>
<p>Llm-planner: Few-shot grounded planning for embodied agents with large language models. Hee Chan, Jiaman Song, Clayton Wu, Brian M Washington, Wei-Lun Sadler, Yu Chao, ; Gemini Su, Rohan Team, Sebastian Anil, Yonghui Borgeaud, Jean-Baptiste Wu, Jiahui Alayrac, Radu Yu, Johan Soricut, Andrew M Schalkwyk, Anja Dai, Hauth, arXiv:2312.11805Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer Vision2023. 2023arXiv preprintfamily of highly capable multimodal models</p>
<p>Large language models still can't plan (a benchmark for llms on planning and reasoning about change). Karthik Valmeekam, Alberto Olmo, Sarath Sreedharan, Subbarao Kambhampati, arXiv:2206.104982022arXiv preprint</p>
<p>Google's neural machine translation system: Bridging the gap between human and machine translation. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, Illia Polosukhin ; Yonghui Wu, Mike Schuster, Zhifeng Chen, Mohammad Quoc V Le, Wolfgang Norouzi, Maxim Macherey, Yuan Krikun, Qin Cao, Klaus Gao, Macherey, arXiv:1609.08144NIPS. 2017. 2016arXiv preprintAttention is all you need</p>
<p>Self-consistency improves chain of thought reasoning in language models. Wws + 22a] Xuezhi, Jason Wang, Dale Wei, Quoc Schuurmans, Ed Le, Sharan Chi, Aakanksha Narang, Denny Chowdhery, Zhou, arXiv:2203.111712022arXiv preprint</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Wws + 22b] Jason, Xuezhi Wei, Dale Wang, Maarten Schuurmans, Fei Bosma, Ed Xia, Chi, Denny Quoc V Le, Zhou, arXiv:2402.01622Travelplanner: A benchmark for real-world planning with language agents. 2022. 2024. 202435arXiv preprintAdvances in Neural Information Processing Systems</p>
<p>Learning to solve constraint satisfaction problems with recurrent transformer. Zhun Yang, Adam Ishay, Joohyung Lee, The Eleventh International Conference on Learning Representations. 2023</p>
<p>Complexity and completeness of finding another solution and its application to puzzles. Takayuki Yato, Takahiro Seta, IEICE transactions on fundamentals of electronics, communications and computer sciences. 8652003</p>
<p>Physics of language models: Part 2.1, grade-school math and the hidden reasoning process. Tian Ye, Zicheng Xu, Yuanzhi Li, Zeyuan Allen-Zhu, arXiv:2407.203112024arXiv preprint</p>
<p>Physics of language models: Part 2.2, how to learn from mistakes on grade-school math problems. Tian Ye, Zicheng Xu, Yuanzhi Li, Zeyuan Allen, -Zhu ; Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, Karthik Narasimhan ; Shunyu, Jeffrey Yao, Dian Zhao, Nan Yu, Izhak Du, Karthik Shafran, Yuan Narasimhan, Cao, arXiv:2210.03629arXiv:2303.05510Shun Zhang, Zhenfang Chen, Yikang Shen, Mingyu Ding, Joshua B Tenenbaum, and Chuang Gan. Planning with large language models for code generation. 2024. 2024. 2022. 202336arXiv preprintReact: Synergizing reasoning and acting in language models</p>
<p>Solving sudoku puzzles with recurrent neural networks. Richard Zhu, </p>
<p>Large language models as commonsense knowledge for large-scale task planning. Zirui Zhao, Wee Sun Lee, David Hsu, ; Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, arXiv:2205.10625-to-most prompting enables complex reasoning in large language models. Claire Cui, Olivier Bousquet, Quoc Le, Nathan Scales, Xuezhi Wang, Dale Schuurmans2024. 202236arXiv preprintAdvances in Neural Information Processing Systems</p>            </div>
        </div>

    </div>
</body>
</html>
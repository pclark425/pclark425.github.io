<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5086 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5086</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5086</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-107.html">extraction-schema-107</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <p><strong>Paper ID:</strong> paper-e7028cd7ea838ab8294ecf26d5a2c0dbb8cfa81a</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/e7028cd7ea838ab8294ecf26d5a2c0dbb8cfa81a" target="_blank">Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain-of-Thought</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Learning Representations</p>
                <p><strong>Paper TL;DR:</strong> This work presents a new synthetic question-answering dataset called PrOntoQA, where each example is generated from a synthetic world model represented in first-order logic, and shows that LLMs are quite capable of making correct individual deduction steps, and so are generally capable of reasoning, even in fictional contexts.</p>
                <p><strong>Paper Abstract:</strong> Large language models (LLMs) have shown remarkable reasoning capabilities given chain-of-thought prompts (examples with intermediate reasoning steps). Existing benchmarks measure reasoning ability indirectly, by evaluating accuracy on downstream tasks such as mathematical reasoning. However, it is unclear how these models obtain the answers and whether they rely on simple heuristics rather than the generated chain-of-thought. To enable systematic exploration of the reasoning ability of LLMs, we present a new synthetic question-answering dataset called PrOntoQA, where each example is generated from a synthetic world model represented in first-order logic. This allows us to parse the generated chain-of-thought into symbolic proofs for formal analysis. Our analysis on InstructGPT and GPT-3 shows that LLMs are quite capable of making correct individual deduction steps, and so are generally capable of reasoning, even in fictional contexts. However, they have difficulty with proof planning: When multiple valid deduction steps are available, they are not able to systematically explore the different options.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5086.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5086.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>InSTRUCTGPT (text-davinci-002)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>InSTRUCTGPT (OpenAI text-davinci-002; GPT-3 variant fine-tuned with RLHF)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The text-davinci-002 InSTRUCTGPT model (GPT-3 family) fine-tuned via reinforcement learning from human feedback, evaluated for stepwise logical reasoning on a synthetic first-order-logic QA benchmark (PrOntoQA) using chain-of-thought prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Training language models to follow instructions with human feedback</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>text-davinci-002 (InSTRUCTGPT)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A GPT-3 family model variant fine-tuned with RLHF (Instruct-style fine-tuning). Evaluated via few-shot (8-shot) chain-of-thought (CoT) prompting to elicit intermediate reasoning steps; greedy decoding used for main experiments. Also probed with self-consistency sampling and DFS-example prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>logical_reasoning_task</strong></td>
                            <td>PrOntoQA (Proof and Ontology-Generated Question-Answering)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>A synthetic QA benchmark where each example is generated from a first-order logic ontology and a unique proof composed only from modus ponens (and axioms). Examples are translated to simple natural language (context, query, CoT, label). Tasks control number of hops (1/3/5), ontology type (fictional / true / false), and traversal order (top-down / bottom-up). Proofs are parsed back into symbolic steps for formal per-step evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Evaluation using chain-of-thought prompting (8-shot in-context examples with explicit CoT and label), greedy decoding for main experiments. Additional prompting/decoding variants tested: self-consistency (40 samples, temperature 0.7; aggregate by highest-prob semantic parse) and in-context examples demonstrating depth-first-search-like traces.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>text-davinci-002 was the only model in the suite that performed reliably better than chance across tested settings. Baseline valid-proof accuracy reported for a challenging setting (5 hops, fictional ontology, top-down) was ≈0.545 (paper baseline used for comparison in A.7). In the 5-hop fictional experiments: predicted proof steps distribution was 93.2% strictly-valid, 2.4% broadly-valid, and 5.9% invalid; 93.2% of steps were canonical in that experiment. Self-consistency produced valid proof accuracy 0.56 vs 0.545 baseline (not a significant improvement); DFS-example prompting produced 0.55 vs 0.545 baseline (not significant). Authors ran 400-example runs per experimental condition (48 experiments total) and computed 95% CIs (binomial/Wilson).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Fails at proof planning when multiple valid deduction steps exist: frequently selects misleading but strictly-valid atomic steps that do not progress toward the gold proof and cannot reliably recover. Performance degrades with longer proofs (5 hops) and when context sentence order is top-down (reverse of gold proof): accuracy can fall to chance. Model performance heavily leverages real-world pretrained knowledge (true ontologies) and is worse on fictional/counterfactual ontologies, reducing generalizability. Evaluation limited to modus ponens, short/simple sentence templates, and proofs up to 5 hops; unable to conclude about other deduction rules or longer/more branching proofs.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison</strong></td>
                            <td>Compared across GPT-3 family variants in the paper: InSTRUCTGPT (text-davinci-002) outperformed smaller/open variants; smaller models produced more invalid or non-atomic steps. Self-consistency and DFS-style in-context examples did not meaningfully improve valid-proof accuracy in the tested settings. No human baseline provided; authors note reliance on pretraining knowledge (true ontologies) gives advantage versus fictional settings.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis_results</strong></td>
                            <td>Systematic analyses reported: (1) Model size correlates with proof accuracy (authors report increases when going from 350M → 1.3B → 6.7B, but only text-davinci-002 exceeded chance); (2) Ontology type: 'true' ontologies (real-world consistent) lead to higher accuracy and enable the model to 'skip' hops using background knowledge, while fictional/false ontologies reduce performance; (3) Number of hops: 1- and 3-hop tasks handled well, 5-hop (especially top-down traversal) causes severe performance drops; (4) Traversal/ordering: bottom-up (matches gold proof order) is easier than top-down; (5) Error analysis: majority of incorrect proofs start with strictly-valid atomic misleading steps (i.e., correct-local but unhelpful), while smaller models make invalid/non-atomic errors more often; (6) Prompt strategy ablations: self-consistency and DFS-example prompting produced no statistically significant improvement in valid-proof accuracy in tested settings; (7) Statistical regime: experiments used 400 examples per condition and report 95% Wilson CIs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain-of-Thought', 'publication_date_yy_mm': '2022-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5086.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5086.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3 family (ada / babbage / curie / davinci / text-davinci-001)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3 family variants evaluated (text-ada-001, text-babbage-001, text-curie-001, davinci, text-davinci-001)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Multiple GPT-3 family variants were evaluated on PrOntoQA with identical prompting (8-shot CoT) to study the effect of model scale and variant on formal logical reasoning performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Language models are few-shot learners</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3 family (text-ada-001, text-babbage-001, text-curie-001, davinci, text-davinci-001)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>GPT-3 family models (multiple variants made available by OpenAI) evaluated under the same CoT prompting and decoding regimes; greedy decoding used for main experiments. Paper groups experiments by increasing model capacity (authors report experiments spanning models corresponding to sizes reported elsewhere ranging from hundreds of millions to multiple billions of parameters).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>varied (authors report experiments spanning 350M → 1.3B → 6.7B scale points; exact per-variant sizes not explicitly enumerated in main text)</td>
                        </tr>
                        <tr>
                            <td><strong>logical_reasoning_task</strong></td>
                            <td>PrOntoQA (Proof and Ontology-Generated Question-Answering)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Synthetic first-order-logic-derived QA with unique proofs composed from modus ponens; examples translated to simplistic natural language to allow semantic parsing of predicted CoTs into symbolic proofs; variables controlled: number of hops (1/3/5), ontology type (fictional/true/false), traversal order.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Evaluation via 8-shot chain-of-thought prompting with greedy decoding (main), probing different model sizes/variants to study scaling effects; additional runs exploring self-consistency sampling and DFS-style in-context examples for the best-performing variant.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Smaller variants performed poorly relative to the largest InSTRUCTGPT variant; authors report proof accuracy rises substantially with model scale (notably across the reported scale points 350M → 1.3B → 6.7B), but only the top-performing variant (text-davinci-002) was reliably above chance. Smaller models exhibited higher rates of invalid and non-atomic steps. Exact numeric accuracies per small variant are provided in paper figures (per-condition bars computed over 400 examples); paper highlights qualitative scale trend rather than a single scalar summary across all conditions.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Smaller GPT-3 variants are more prone to invalid and non-atomic steps and fail more often at both local deduction correctness and global proof planning. All variants struggle with proof planning when multiple valid steps exist; performance degrades with longer proofs and when the context ordering is adversarial (top-down). Results limited to the restricted calculus (modus ponens) and short proofs.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison</strong></td>
                            <td>Authors compare across the GPT-3 variants and conclude that larger capacity correlates with fewer invalid steps and higher proof accuracy; still, even the largest non-Instruct variants (e.g., text-davinci-001 / davinci) performed worse than text-davinci-002, suggesting fine-tuning (RLHF/instruction tuning) and/or other differences matter. Prompting modifications (self-consistency, DFS exemplars) did not substantially alter final valid-proof accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis_results</strong></td>
                            <td>Scaling ablation: rising model capacity reduces frequency of invalid steps and increases proof accuracy (fig. 8). Other controlled variables analyzed: ontology type (true vs fictional vs false), traversal direction, and hop count; all modulate performance. The paper shows that for smaller models the first non-canonical error is often invalid/non-atomic, whereas for larger models the predominant first non-canonical error is a strictly-valid misleading step.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain-of-Thought', 'publication_date_yy_mm': '2022-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Proofwriter: Generating implications, proofs, and abductive statements over natural language <em>(Rating: 2)</em></li>
                <li>FOLIO: natural language reasoning with first-order logic <em>(Rating: 2)</em></li>
                <li>Chain of thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Self-consistency improves chain of thought reasoning in language models <em>(Rating: 2)</em></li>
                <li>Language models show human-like content effects on reasoning <em>(Rating: 2)</em></li>
                <li>Large language models still can't plan (A benchmark for llns on planning and reasoning about change) <em>(Rating: 1)</em></li>
                <li>On the paradox of learning to reason from data <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5086",
    "paper_id": "paper-e7028cd7ea838ab8294ecf26d5a2c0dbb8cfa81a",
    "extraction_schema_id": "extraction-schema-107",
    "extracted_data": [
        {
            "name_short": "InSTRUCTGPT (text-davinci-002)",
            "name_full": "InSTRUCTGPT (OpenAI text-davinci-002; GPT-3 variant fine-tuned with RLHF)",
            "brief_description": "The text-davinci-002 InSTRUCTGPT model (GPT-3 family) fine-tuned via reinforcement learning from human feedback, evaluated for stepwise logical reasoning on a synthetic first-order-logic QA benchmark (PrOntoQA) using chain-of-thought prompting.",
            "citation_title": "Training language models to follow instructions with human feedback",
            "mention_or_use": "use",
            "model_name": "text-davinci-002 (InSTRUCTGPT)",
            "model_description": "A GPT-3 family model variant fine-tuned with RLHF (Instruct-style fine-tuning). Evaluated via few-shot (8-shot) chain-of-thought (CoT) prompting to elicit intermediate reasoning steps; greedy decoding used for main experiments. Also probed with self-consistency sampling and DFS-example prompts.",
            "model_size": null,
            "logical_reasoning_task": "PrOntoQA (Proof and Ontology-Generated Question-Answering)",
            "task_description": "A synthetic QA benchmark where each example is generated from a first-order logic ontology and a unique proof composed only from modus ponens (and axioms). Examples are translated to simple natural language (context, query, CoT, label). Tasks control number of hops (1/3/5), ontology type (fictional / true / false), and traversal order (top-down / bottom-up). Proofs are parsed back into symbolic steps for formal per-step evaluation.",
            "method_or_approach": "Evaluation using chain-of-thought prompting (8-shot in-context examples with explicit CoT and label), greedy decoding for main experiments. Additional prompting/decoding variants tested: self-consistency (40 samples, temperature 0.7; aggregate by highest-prob semantic parse) and in-context examples demonstrating depth-first-search-like traces.",
            "performance": "text-davinci-002 was the only model in the suite that performed reliably better than chance across tested settings. Baseline valid-proof accuracy reported for a challenging setting (5 hops, fictional ontology, top-down) was ≈0.545 (paper baseline used for comparison in A.7). In the 5-hop fictional experiments: predicted proof steps distribution was 93.2% strictly-valid, 2.4% broadly-valid, and 5.9% invalid; 93.2% of steps were canonical in that experiment. Self-consistency produced valid proof accuracy 0.56 vs 0.545 baseline (not a significant improvement); DFS-example prompting produced 0.55 vs 0.545 baseline (not significant). Authors ran 400-example runs per experimental condition (48 experiments total) and computed 95% CIs (binomial/Wilson).",
            "limitations_or_failure_cases": "Fails at proof planning when multiple valid deduction steps exist: frequently selects misleading but strictly-valid atomic steps that do not progress toward the gold proof and cannot reliably recover. Performance degrades with longer proofs (5 hops) and when context sentence order is top-down (reverse of gold proof): accuracy can fall to chance. Model performance heavily leverages real-world pretrained knowledge (true ontologies) and is worse on fictional/counterfactual ontologies, reducing generalizability. Evaluation limited to modus ponens, short/simple sentence templates, and proofs up to 5 hops; unable to conclude about other deduction rules or longer/more branching proofs.",
            "comparison": "Compared across GPT-3 family variants in the paper: InSTRUCTGPT (text-davinci-002) outperformed smaller/open variants; smaller models produced more invalid or non-atomic steps. Self-consistency and DFS-style in-context examples did not meaningfully improve valid-proof accuracy in the tested settings. No human baseline provided; authors note reliance on pretraining knowledge (true ontologies) gives advantage versus fictional settings.",
            "ablation_or_analysis_results": "Systematic analyses reported: (1) Model size correlates with proof accuracy (authors report increases when going from 350M → 1.3B → 6.7B, but only text-davinci-002 exceeded chance); (2) Ontology type: 'true' ontologies (real-world consistent) lead to higher accuracy and enable the model to 'skip' hops using background knowledge, while fictional/false ontologies reduce performance; (3) Number of hops: 1- and 3-hop tasks handled well, 5-hop (especially top-down traversal) causes severe performance drops; (4) Traversal/ordering: bottom-up (matches gold proof order) is easier than top-down; (5) Error analysis: majority of incorrect proofs start with strictly-valid atomic misleading steps (i.e., correct-local but unhelpful), while smaller models make invalid/non-atomic errors more often; (6) Prompt strategy ablations: self-consistency and DFS-example prompting produced no statistically significant improvement in valid-proof accuracy in tested settings; (7) Statistical regime: experiments used 400 examples per condition and report 95% Wilson CIs.",
            "citation": "here",
            "uuid": "e5086.0",
            "source_info": {
                "paper_title": "Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain-of-Thought",
                "publication_date_yy_mm": "2022-10"
            }
        },
        {
            "name_short": "GPT-3 family (ada / babbage / curie / davinci / text-davinci-001)",
            "name_full": "GPT-3 family variants evaluated (text-ada-001, text-babbage-001, text-curie-001, davinci, text-davinci-001)",
            "brief_description": "Multiple GPT-3 family variants were evaluated on PrOntoQA with identical prompting (8-shot CoT) to study the effect of model scale and variant on formal logical reasoning performance.",
            "citation_title": "Language models are few-shot learners",
            "mention_or_use": "use",
            "model_name": "GPT-3 family (text-ada-001, text-babbage-001, text-curie-001, davinci, text-davinci-001)",
            "model_description": "GPT-3 family models (multiple variants made available by OpenAI) evaluated under the same CoT prompting and decoding regimes; greedy decoding used for main experiments. Paper groups experiments by increasing model capacity (authors report experiments spanning models corresponding to sizes reported elsewhere ranging from hundreds of millions to multiple billions of parameters).",
            "model_size": "varied (authors report experiments spanning 350M → 1.3B → 6.7B scale points; exact per-variant sizes not explicitly enumerated in main text)",
            "logical_reasoning_task": "PrOntoQA (Proof and Ontology-Generated Question-Answering)",
            "task_description": "Synthetic first-order-logic-derived QA with unique proofs composed from modus ponens; examples translated to simplistic natural language to allow semantic parsing of predicted CoTs into symbolic proofs; variables controlled: number of hops (1/3/5), ontology type (fictional/true/false), traversal order.",
            "method_or_approach": "Evaluation via 8-shot chain-of-thought prompting with greedy decoding (main), probing different model sizes/variants to study scaling effects; additional runs exploring self-consistency sampling and DFS-style in-context examples for the best-performing variant.",
            "performance": "Smaller variants performed poorly relative to the largest InSTRUCTGPT variant; authors report proof accuracy rises substantially with model scale (notably across the reported scale points 350M → 1.3B → 6.7B), but only the top-performing variant (text-davinci-002) was reliably above chance. Smaller models exhibited higher rates of invalid and non-atomic steps. Exact numeric accuracies per small variant are provided in paper figures (per-condition bars computed over 400 examples); paper highlights qualitative scale trend rather than a single scalar summary across all conditions.",
            "limitations_or_failure_cases": "Smaller GPT-3 variants are more prone to invalid and non-atomic steps and fail more often at both local deduction correctness and global proof planning. All variants struggle with proof planning when multiple valid steps exist; performance degrades with longer proofs and when the context ordering is adversarial (top-down). Results limited to the restricted calculus (modus ponens) and short proofs.",
            "comparison": "Authors compare across the GPT-3 variants and conclude that larger capacity correlates with fewer invalid steps and higher proof accuracy; still, even the largest non-Instruct variants (e.g., text-davinci-001 / davinci) performed worse than text-davinci-002, suggesting fine-tuning (RLHF/instruction tuning) and/or other differences matter. Prompting modifications (self-consistency, DFS exemplars) did not substantially alter final valid-proof accuracy.",
            "ablation_or_analysis_results": "Scaling ablation: rising model capacity reduces frequency of invalid steps and increases proof accuracy (fig. 8). Other controlled variables analyzed: ontology type (true vs fictional vs false), traversal direction, and hop count; all modulate performance. The paper shows that for smaller models the first non-canonical error is often invalid/non-atomic, whereas for larger models the predominant first non-canonical error is a strictly-valid misleading step.",
            "citation": "here",
            "uuid": "e5086.1",
            "source_info": {
                "paper_title": "Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain-of-Thought",
                "publication_date_yy_mm": "2022-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Proofwriter: Generating implications, proofs, and abductive statements over natural language",
            "rating": 2
        },
        {
            "paper_title": "FOLIO: natural language reasoning with first-order logic",
            "rating": 2
        },
        {
            "paper_title": "Chain of thought prompting elicits reasoning in large language models",
            "rating": 2
        },
        {
            "paper_title": "Self-consistency improves chain of thought reasoning in language models",
            "rating": 2
        },
        {
            "paper_title": "Language models show human-like content effects on reasoning",
            "rating": 2
        },
        {
            "paper_title": "Large language models still can't plan (A benchmark for llns on planning and reasoning about change)",
            "rating": 1
        },
        {
            "paper_title": "On the paradox of learning to reason from data",
            "rating": 1
        }
    ],
    "cost": 0.013666499999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain-ofThOUGHT</h1>
<p>Abulhair Saparov \&amp; He He<br>Center for Data Science, New York University, New York, NY 10011, USA<br>{as17582, hhe}@nyu.edu</p>
<h4>Abstract</h4>
<p>Large language models (LLMs) have shown remarkable reasoning capabilities given chain-of-thought prompts (examples with intermediate reasoning steps). Existing benchmarks measure reasoning ability indirectly, by evaluating accuracy on downstream tasks such as mathematical reasoning. However, it is unclear how these models obtain the answers and whether they rely on simple heuristics rather than the generated chain-of-thought. To enable systematic exploration of the reasoning ability of LLMs, we present a new synthetic question-answering dataset called PRONTOQA, where each example is generated from a synthetic world model represented in first-order logic. This allows us to parse the generated chain-ofthought into symbolic proofs for formal analysis. Our analysis on InSTRUCTGPT and GPT-3 shows that LLMs are quite capable of making correct individual deduction steps, and so are generally capable of reasoning, even in fictional contexts. However, they have difficulty with proof planning: When multiple valid deduction steps are available, they are not able to systematically explore the different options.</p>
<h2>1 INTRODUCTION</h2>
<p>The ability to reason-drawing new conclusions from provided facts-is a hallmark of human intelligence. Recently, chain-of-thought (CoT) prompting has enabled large language models (LLMs) to perform logical reasoning tasks with impressive accuracy (Wei et al., 2022; Chowdhery et al., 2022; Lewkowycz et al., 2022). In CoT prompting, each example consists of a question (e.g., " $\frac{6}{3}-1$ ?"), a short description of the reasoning required to answer the question called the "chain-of-thought" (e.g., " $\frac{6}{3}$ is $2.2-1$ is 1. ), and a label (e.g., " 1 "). When prompted with a few CoT examples, the elicited reasoning allows LLMs to predict the label with much higher accuracy than standard question-answer prompting. However, it is unclear to what extent these models can reason due to several confounding factors. First, existing studies primarily rely on question-answering (QA) tasks from real-world settings such as math word problems (Cobbe et al., 2021; Han et al., 2022; Weston et al., 2016). It is likely that LLMs have already acquired the knowledge through pretraining and simply retrieve the answer rather than reason over it. Second, the reasoning task may contain spurious correlations that allow the model to obtain the correct answer through shortcuts (Zhang et al., 2022b). In this work, we systematically investigate the reasoning capability of LLMs by directly evaluating their predicted chains-of-thought (the interpretable proof steps), rather than the predicted label.
To enable easy analysis of the CoT, we construct a new synthetic QA dataset called PRONTOQA, for Proof and Ontology-Generated Question-Answering. Inspired by the ProofWritER dataset (Tafjord et al., 2021), each example in PRONTOQA is generated from an ontology and has a unique proof (see figure 1 for an example). We convert the proofs into syntactically simple sentences using a grammar such that the inverse process is relatively easy: From the predicted CoT, we semantically parse each sentence into a formal language and reconstruct the underlying proof steps. We then directly analyze the model's reasoning by inspecting each step in the reconstructed proof and comparing them against the gold proof. ${ }^{1}$ We emphasize here that while the dataset is an important contribution of this paper, the main contribution is the analysis that is facilitated by the dataset.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Q: Each cat is a carnivore. Every carnivore is not herbivorous. Carnivores are mammals.
All mammals are warm-blooded. Mammals are vertebrates. Every vertebrate is an animal. $\qquad$ context Animals are multicellular. Fae is a cat. True or false: Fae is not herbivorous. query A: Fae is a cat. Cats are carnivores. Fae is a carnivore. Every carnivore is not herbivorous. $\qquad$ chain-of-thought Fae is not herbivorous. True label</p>
<p>Figure 1: A question-answering example from PrOntoQA, with each component highlighted and labeled.
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 2: Schematic of the generative process for each example in PrOntoQA. Step 1: We generate an ontology from a prior distribution, shown here as a tree. Each node denotes a concept (e.g., mammal), each with an optional property (e.g., warm_blooded), and each blue edge denotes a "subtype of" relation. Step 2: Generate proof from the ontology. Each horizontal black line indicates a proof step, with its premises written above the line and the conclusion written below. Step 3: Convert the ontology into a natural language context. Step 4: Convert the proof into a natural language query, chain-of-thought, and answer label. There is a one-to-one correspondence between the conclusion of each proof step and the sentences in the chain-of-thought.</p>
<p>We systematically evaluate InSTRUCTGPT ${ }^{2}$ (Ouyang et al., 2022) and the original GPT-3 (Brown et al., 2020) on PrOntoQA by controlling a number of variables that characterize the complexity of the reasoning task, such as the ontology type and the number of proof steps required. Our analysis shows that these models are quite good at producing valid individual proof steps, even on fictional and counterfactual ontologies. However, LLMs have difficulty with proof planning: when the models encounter a point in the proof where multiple valid proof steps are available, they sometimes select the wrong step, and this often leads to an incomplete proof and subsequently an incorrect answer. Interestingly, the models are much less likely to be misled with a true ontology, suggesting that the world knowledge acquired during pretraining plays an important role in LLM reasoning. We also find that our results generalize to more sophisticated/informative prompts, such as self-consistency prompting (Wang et al., 2022), and prompts with example traces of depth-first proof search instead of CoT.</p>
<h1>2 RELATED WORK</h1>
<p>Our proposed dataset is most closely related to ProofWriter (Tafjord et al., 2021) and FOLIO (Han et al., 2022) which are QA datasets designed to test reasoning ability. ProofWriter provides multi-hop proofs for each example. However, there are a number of key properties that led us to develop our own dataset (see table 1 for a summary). FOLIO does not provide easily-parseable proofs/CoTs in their examples, and evaluation is done by inspecting the predicted labels, which may not necessarily be a good measure of reasoning ability. In our analysis, we focus on more specific variables that may affect the reasoning of the model, such as: (1) Is the model's reasoning dependent on whether the example is consistent with pretraining ("true"), inconsistent ("false"), or neither ("fictional")? (2) Is the model's reasoning sensitive to whether the predicates in the examples are unary or binary? (3) Is the model's reasoning dependent on the rules of deduction in the examples? These variables are not controllable in existing datasets. Further, in some datasets, the code to generate examples is not available.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: left;">Dataset</th>
<th style="text-align: center;">Provides easily <br> semantically- <br> parseable <br> proofs</th>
<th style="text-align: center;">Controls for true <br> vs false vs <br> fictional contexts</th>
<th style="text-align: center;">Controls for <br> unary vs binary <br> predicates</th>
<th style="text-align: center;">Controls for <br> specific rules of <br> deduction</th>
<th style="text-align: center;">Tests reasoning <br> beyond the <br> domain of math <br> word problems</th>
<th style="text-align: center;">Generation code <br> available</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">G098K <br> Cobbe et al. (2021)</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">human- <br> annotated</td>
</tr>
<tr>
<td style="text-align: left;">ProofWriter <br> Tafjord et al. (2021)</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
</tr>
<tr>
<td style="text-align: left;">FOLIO <br> Han et al. (2022)</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">human- <br> annotated</td>
</tr>
<tr>
<td style="text-align: left;">SimpleLogic <br> Zhang et al. (2022b)</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: left;">PrONTOQA <br> (proposed dataset)</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
</tbody>
</table>
<p>Table 1: Comparison of existing datasets for the formal analysis of reasoning ability.
There are efforts to tweak or extend CoT prompting to elicit more sophisticated reasoning behavior (Creswell et al., 2022; Wang et al., 2022; Creswell \&amp; Shanahan, 2022; Anil et al., 2022; Dohan et al., 2022), and they have shown that these prompting extensions to CoT can improve the elicited reasoning behavior of LLMs, even with smaller models. Rather than presenting a new prompting approach, the goal of this work is to measure the reasoning ability elicited by CoT. There are other datasets that have been designed or used to measure the reasoning capabilities of transformer-based models and LLMs (Han et al., 2022; Weston et al., 2016). They show that LLMs are able to answer questions that require reasoning in the few-shot setting with reasonable accuracy. Similar to our approach, Betz (2020) converts logical forms into fairly simple natural language using templates. However, the examples in these datasets are consistent with the real-world, and so they may confound measuring reasoning ability with retrieval ability. Valmeekam et al. (2022) found that LLMs had difficulty with a fairly simple planning task, but it is not clear whether this was due to an inability to reason or other abilities instrumental in planning, such as world modeling, keeping track of state changes, and reasoning about events that occur sequentially in time. This is despite their controlling for other variables involved in planning, such as plan generation, robustness to goal formulation, among others. They experimented with examples in a "Blocksworld" environment, a significant portion of which the model can acquire from pretraining. Our work aims to address this gap. As in our approach, Dasgupta et al. (2022) specifically looked at whether LLMs can reason in fictional or counterfactual settings and found that reasoning ability is indeed negatively affected in these settings. However they did not analyze individual steps of reasoning to better understand the cause of the errors. Since we are able to formally evaluate the LLM's predicted CoT step-by-step, we are able to perform a more fine-grained analysis of their reasoning ability. Zhang et al. (2022b) showed that BERT is not able to learn to reason robustly, but they did not use CoT prompting and it is not obvious if their results generalize to LLMs, which we evaluate.
There are two broad research approaches for reasoning in NLP: (1) reasoning over a formal symbolic language, possibly with neuro-symbolic methods and/or semantic parsing (Saparov \&amp; Mitchell, 2022; Zhang et al., 2022a; Kapanipathi et al., 2021; Dong et al., 2019; Rocktäschel \&amp; Riedel, 2017), or (2) reasoning directly over natural language (Chen et al., 2021; Bostrom et al., 2022; 2021; Welleck et al., 2021; Bhagavatula et al., 2020; Angeli \&amp; Manning, 2014; MacCartney \&amp; Manning, 2009). While PrONTOQA is generated from symbolic ontologies, the examples themselves are in natural language, and so provides value to both research directions.
Recent work has examined in-context learning and found that performance on certain tasks is sensitive to the prompt (Razeghi et al., 2022; Lu et al., 2022). However, they focused on sentiment classification and simple arithmetic tasks, and it is not clear if their results generalize to reasoning. The LLM could feasibly use retrieval, rather than reasoning, to perform those tasks. Our experiments on the fictional ontology show that the model is able to reason even when there is nothing to retrieve from.</p>
<h1>3 PrONTOQA: A SYNTHETIC DATASET FOR LOGICAL REASONING</h1>
<p>We create a new dataset, called PrONTOQA for Proof and Ontology-Generated Question-Answering, where each question is generated from a symbolic ontology and proof to facilitate formal analysis of the predicted CoT. To focus the scope of our exploration, and to limit the complexity of the generated questions to those within reach of current LLMs, we only consider questions that are answerable using repeated applications of the modus ponens deduction rule. More formally, modus ponens is a simple deduction rule where given the premises $\forall x(f(x) \rightarrow g(x))$ and $f(a)$, we conclude $g(a)$ (e.g., given "All cats are carnivores" and "Fae is a cat," we conclude "Fae is a carnivore;" see figure 6</p>
<p>in the appendix). ${ }^{3}$ This rule can be easily chained together to construct proofs with controllable size. We generate CoT examples consisting of: the context, query, CoT, and label, where the context is a short paragraph containing information relevant to answer the query (see figure 1 for an example). Each example is translated from a proof and ontology such that the inverse process is simple: the sentences in an example can be easily and uniquely parsed into symbolic logical forms amenable to formal analysis. More specifically, as shown in figure 2, we: (1) first generate an ontology from a set of concepts, (2) generate a proof by traversing the ontology, (3) translate the ontology into the natural language context, and (4) translate the proof into the query, CoT, and label by mapping logical forms to natural language sentences. We describe each step in further detail below.
Ontology generation. The first step is to generate a small hierarchical ontology. The ontology is a set of concepts (e.g., mammal, cat, carnivore, etc) and subtype relations between them (e.g., $\forall x($ cat $(x) \rightarrow$ carnivore $(x))$ ). The ontology also describes properties of concepts (e.g., $\forall x($ mammal $(x) \rightarrow \neg$ cold_blooded $(x))$ ). To generate questions that are not overly complex, we restrict the ontologies to be linear (i.e., in the tree, every node has exactly 0 or 1 child nodes). Since ontologies are randomly generated, they vary in size from as few as 3 concepts to as many as 10 .
Proof generation. We generate proofs from the ontology by choosing a starting node uniformly at random, and generating the initial axiom indicating that an entity has a specific type (e.g., cat(fae)). Then, we walk up the tree, with each step corresponding to an application of a deduction rule (i.e., a proof step). Each proof step consists of zero or more premises and one conclusion. We stop when we reach a node (e.g., carnivore(fae)), or a node property (e.g., $\neg$ herbivorous(fae)), such that the number of generated proof steps matches the target number of steps.
Translation to natural language example. Given a generated ontology and proof, we now translate it into a natural language CoT example consisting of the question (context and query), CoT, and label. We describe how each component is generated below:
We use a simple grammar to convert the formal statements of the ontology into the natural language utterances that make up the context. Every edge in the ontology is converted into sentences such as "All cats are carnivores" or "Every cat is a carnivore." Properties of nodes are also converted into sentences of the form "All mammals are not cold-blooded," etc.
The query is generated by using the same grammar to convert the initial axiom in the proof into a natural language sentence (e.g., "Fae is a cat"). We then determine with probability 0.5 whether to ask if the conclusion of the proof is true or if its negation is false, and convert it into a natural language "true or false" query (e.g., "True or false: Fae is not herbivorous.") and label (e.g., "True").
We convert the ordered sequence of proof steps into the CoT by translating the conclusion of each proof step into a CoT sentence.
Avoiding shortcuts. In section A. 2 in the appendix, we describe how we add distractor sentences in order to remove shortcuts that would allow the model to "guess" the answer without reasoning.
A unique feature of PrOntoQA is that it is easily programmable, with a handful of tunable knobs which we use to generate examples with varying degrees of complexity and study different aspects of reasoning in LLMs. These variables are described in greater detail in section 5.1.</p>
<h1>4 FORMAL ANALYSIS OF PREDICTED PROOFS</h1>
<p>Instead of measuring the accuracy of the predicted answers (i.e., "true" or "false"), we would like to directly evaluate the predicted CoT to check if the model derives the right answer for the right reason. We endeavor to analyze whether the model is able to apply deduction rules correctly at each proof step (i.e., local correctness), but also whether the model can plan ahead and work toward proving the answer for the query (i.e., global correctness). To measure the local correctness of a given proof step, we compute whether the step follows from one or more applications of deduction rules, and whether it requires additional rules beyond those of the gold proofs. To measure the global correctness, we wish to identify proof steps that deviate from the gold proof.
To achieve this, we parse each sentence of the predicted CoT into logical form via recursive-descent parsing using the simple grammar from the generative process. We then compute whether that logical form is provable from previous logical forms via one or more applications of deduction rules. This logical form corresponds to the conclusion of a proof step. We then evaluate the correctness of each proof step by categorizing it according to three dimensions:</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: center;">Step type</th>
<th style="text-align: center;">Example (the conclusion of each step is highlighted green)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Strictly-valid atomic correct step, or canonical step</td>
<td style="text-align: center;">"Fae is a cat. Cats are carnivores. Fae is a carnivore. Every carnivore is not herbivorous. Fae is not herbivorous. True" <br> (this is the gold CoT for this example)</td>
</tr>
<tr>
<td style="text-align: center;">Strictly-valid atomic misleading step</td>
<td style="text-align: center;">"Fae is a cat. Cats are carnivores. Fae is a carnivore. Every carnivore is a mammal. Fae is a mammal..."</td>
</tr>
<tr>
<td style="text-align: center;">Strictly-valid non-atomic correct step</td>
<td style="text-align: center;">"Fae is a cat. Fae is a carnivore. Every carnivore is not herbivorous. Fae is not herbivorous. True"</td>
</tr>
<tr>
<td style="text-align: center;">Strictly-valid non-atomic misleading step</td>
<td style="text-align: center;">"Fae is a cat. Cats are carnivores. Fae is a carnivore. Fae is a mammal. Every mammal is a vertebrate..."</td>
</tr>
<tr>
<td style="text-align: center;">Broadly-valid correct step</td>
<td style="text-align: center;">"Fae is a cat. Every cat is not herbivorous. Fae is not herbivorous..."</td>
</tr>
<tr>
<td style="text-align: center;">Broadly-valid misleading step</td>
<td style="text-align: center;">"Fae is a cat. Every cat is a mammal. Fae is a mammal..."</td>
</tr>
<tr>
<td style="text-align: center;">Invalid step</td>
<td style="text-align: center;">"Fae is a cat. Cats are carnivores. Fae is a carnivore. Every carnivore is a cat. Fae is a cat..."</td>
</tr>
</tbody>
</table>
<p>TABLE 2: The types of proof steps (and examples thereof) into which we categorize each step in the predicted chain-of-thought from LLMs. Compare the given chain-of-thought examples with the gold example provided in the first row.</p>
<ol>
<li>Validity: Is the current proof step provable from previous steps? If it is provable using only the deduction rules that appear in the gold proofs, we say the step is strictly-valid. If it is provable with a more powerful proof calculus, like natural deduction, we say the step is broadly-valid. Otherwise, we say the step is invalid.
For example, given the premises, "Cats are carnivores" and "Carnivores are mammals," the step with conclusion "Cats are mammals" is broadly-valid since an additional deduction rule is required to prove it: given $\forall x(f(x) \rightarrow g(x))$ and $\forall x(g(x) \rightarrow h(x))$, conclude $\forall x(f(x) \rightarrow h(x))$. Notice that this is distinct from a strictly-valid non-atomic step since this conclusion is not provable via repeated applications of modus ponens. We note that this the only additional rule that we check, as we did not encounter any instances of other broadly-valid rules.</li>
<li>Atomicity: Is the current proof step provable from previous steps with exactly one application of a deduction rule? If so, we say the proof step is atomic. Otherwise, it is non-atomic. Note that since all broadly-valid steps are non-atomic, this distinction is only useful for strictly-valid steps.
For example, given the premises, "Fae is a cat," "Cats are carnivores," and "Carnivores are mammals," the step with conclusion "Fae is a mammal" is non-atomic since the step "Fae is a carnivore" was skipped.</li>
<li>Utility: If the current proof step's premises are part of the gold proof, but its conclusion is not, then we say the proof step is misleading. Otherwise, it is correct.
For example, given the premises "Fae is a carnivore," "All carnivores are not herbivorous," and "Carnivores are mammals," and the goal is to prove "Fae is not herbivorous," the step "Fae is a mammal" is misleading since although the step is strictly-valid, it does not help to prove the goal.
The types of proof steps are listed in table 2 along with examples. Unparseable proof steps are marked as incorrect. For brevity, we refer to strictly-valid atomic correct steps as canonical steps. Psuedocode of the procedure to evaluate proofs is given in algorithm 1 in the Appendix.
Metrics. Given the above categorization of proof steps, a proof is defined to be correct if and only if there exists a path of proof steps from the premises to the conclusion (note that under this definition, it is possible for a correct proof to contain invalid proof steps). We could require that all proof steps in the path be canonical. But it is not obvious that this metric, which we call strict proof accuracy, would accurately measure the reasoning ability of the model. As such, we also consider more relaxed metrics: (a) we allow proof steps in the path to be strictly-valid non-atomic correct, which we call "skip" proof accuracy, (b) we allow proof steps to be broadly-valid, which we call broad proof accuracy, or (c) we allow proof steps to be strictly- or broadly-valid, which we call valid proof accuracy.</li>
</ol>
<h1>5 ReSULTS</h1>
<h3>5.1 EXPERIMENTAL SETUP</h3>
<p>In each experiment, we generate QA examples, perform CoT prompting on the LLMs, and analyze the predicted CoTs. We run the experiments on InSTRUCTGPT and the original GPT-3 (OpenAI</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>FIGURE 3: Scatter plots of label accuracy vs proof accuracy of all GPT-3 experiments in this paper. The black line indicates perfect agreement between label accuracy and proof accuracy. We emphasize that "proof accuracy" indicates the fraction of proofs (not proof steps) that are considered correct according to our metrics. Label accuracy is not well-correlated with strict or broad proof accuracy, and is better correlated with "skip" and valid proof accuracy, suggesting that label accuracy is a good measure of reasoning ability.
models text-ada-001, text-babbage-001, text-curie-001, davinci, text-davinci-001, text-davinci-002), with greedy decoding (Ouyang et al., 2022; Brown et al., 2020). We use 8 -shot in-context learning, so each input to the LLM consists of 8 fully-labeled questions followed by a single test question with missing CoT and label. The model's task is to predict the CoT and label for the test question. Note that all examples across all inputs are independently and identically generated from PrONTOQA.
There are a number of variables that we control when generating examples in PrONTOQA: (1) the number of hops, (2) the ordering in which the sentences are generated from the ontology, and (3) the type of the ontology. The number of hops directly controls the difficulty of the generated example, and we experiment with 1,3 , and 5 hops.
We control the ontology traversal direction: We either traverse the tree top-down (i.e., preorder) or bottom-up (i.e., postorder), generating a sentence for each traversed edge/node. The ordering also affects the difficulty of the generated example: if the sentences are generated bottom-up, they will follow the same order as the steps in the gold proof. On the other hand, if they are generated top-down, the order is reversed, and the task may be more difficult.
To avoid any confounding effects from knowledge acquired during pretraining, PrONTOQA generates examples with fictional concept names (e.g., "wumpus" instead of "cat," etc). But we are also interested in measuring this confounding effect, and so in addition to fictional ontologies, we also generate "true" and "false" ontologies. True ontologies use real concept names and are consistent with the real-world (we randomly sample from a list of three hand-coded real ontologies). False ontologies use real concept names but the trees are generated using the random process described in section 3, and so it is very likely to generate a false statement, such as "All mammals are cats."
For each combination of variables, we run the model on 400 examples generated from the testbed, for a total of 48 experiments. We compute $95 \%$ confidence intervals for each experiment, as the number of correct proofs is distributed as Binomial $(400, p)$ with $p$ being the model's accuracy (Wilson, 1927).</p>
<h1>5.2 DO CORRECT ANSWERS IMPLY CORRECT REASONING?</h1>
<p>Label accuracy may not necessarily measure whether the model is performing reasoning correctly, since the model may find ways to guess the label via heuristics. To gauge whether label accuracy is a good metric and which proof accuracy metric is best to measure reasoning ability, we investigate how label accuracy is related to the various proof accuracy metrics. We plot proof accuracy vs label accuracy (i.e., simply checking whether the predicted label "True" or "False" is correct) of every experiment that we conducted in figure 3. Each point in the scatter plot corresponds to one of our 48 experiments described above. Observe that the label accuracy is poorly correlated with strict proof accuracy, and that strict proof accuracy may underestimate the model's reasoning ability. Rather, the most permissive accuracy metric has the highest correlation with label accuracy, suggesting that label accuracy is appropriate to measure reasoning accuracy. It also suggests that the most permissive proof accuracy metric is most appropriate for measuring the reasoning ability of the model.</p>
<h3>5.3 PROOF ANALYSIS RESULTS</h3>
<p>Only the largest model is able to reason. We investigated how reasoning ability is affected by model size. In figure 8 in the Appendix, proof accuracy increases considerably when increasing the model size from 350 M to 1.3 B and 6.7 B . However, only text-davinci-002 is able to perform</p>
<p>better than chance. We were not able to conclusively discern the cause of the significant difference in performance between version 001 and 002. One possible factor is the maximum token limit of version 002 is roughly twice that of version 001. In fact, the model davinci seems to perform as well as, if not slightly better than, text-davinci-001. In addition, we notice that the frequency of invalid steps decreases as the model size increases, and so larger models seem to be better at making valid steps, whether or not those steps are actually useful.
For the remainder of the paper, our results focus on text-davinci-002. Our main results are in figure 4 where we show the proof accuracy and distribution of proof step types for all experiments.
Real-world knowledge helps reasoning. We investigate the extent to which reasoning ability is affected by whether the ontology is fictional, "true," or "false." Evidently from figure 4, the LLM seems to perform comparably in the fictional and "false" ontology settings (accuracy is slightly worse with a "false" ontology). But when using the "true" ontology, the model performs much better, and its performance does not drop when increasing the number of hops from 3 to 5 . The model is able to utilize its background knowledge from pretraining to "skip" hops, and is thus not as negatively affected by the increased hops. This is consistent with the findings of Dasgupta et al. (2022).
Evidently, the model's reasoning is heavily reliant on real-world knowledge, and this may be a problem for generalizability, such as when applying LLMs to novel scenarios or to settings that are not well-represented in the training data.
Longer proofs are still challenging. We investigate the extent to which reasoning ability is affected by the number of hops in the proof. We see from figure 4 that the model handles 1- and 3-hop examples quite well but struggles with 5-hop top-down examples, with accuracy falling to chance. So while it is able to perform reasoning to an extent, it is more limited as the number of hops increases.
Traversal direction affects reasoning. We also tested how reasoning ability is affected by the traversal direction of the ontology. We notice in figure 4 that as the number of hops increases, the model becomes sensitive to the traversal direction of the ontology (top-down vs bottom-up). This may be due to the fact that the order of the gold proof steps mirrors the bottom-up traversal, and is the reverse of the top-down traversal. Thus, the task may be made more difficult for language models if the context sentences are ordered top-down.
How do LLMs reason step-by-step? We investigate the fraction of correct and incorrect proofs that contain various types of proof steps, and whether the correctness of the proof is correlated with the presence of specific types of proof steps. Figure 4 breaks down the bars further (in darker red and blue) to indicate the fraction of proofs that contain proof steps other than canonical steps, since most predicted proof steps were canonical (in the 5-hop experiments with fictional ontology, they constitute $93.2 \%$ of proof steps). We make the following observations:</p>
<ol>
<li>Most predicted proof steps are strictly-valid (in the 5-hop experiments with fictional ontology, $93.2 \%$ of proof steps are strictly-valid, $2.4 \%$ are broadly-valid, and $5.9 \%$ are invalid).</li>
<li>LLMs tend to skip steps by producing non-atomic steps, just as humans do when they verbalize their reasoning (in the 5-hop experiments with fictional ontology, $2.4 \%$ of proof steps are nonatomic, even though all steps in the few-shot examples are atomic).</li>
<li>Most incorrect proofs contain misleading steps and invalid steps. This suggests that the source of the incorrect reasoning is either a due to a misleading step or an invalid step that causes the model to produce steps that do not belong to the gold proof.
Intriguingly, some correct proofs also contain misleading steps and invalid steps, which implies that the model is sometimes able to recover from these "mistakes" and return to the gold proof. We analyze this behavior in greater detail in section 5.4.</li>
</ol>
<h1>5.4 What LEAdS TO A MISTAKE?</h1>
<p>We investigate whether specific types of proof steps are causing InSTRUCTGPT to produce reasoning errors. To do so, we identify the first step in each incorrect proof that is not a canonical step. We observe in figure 5, among incorrect proofs, strictly-valid atomic misleading steps appear in the proof first far more often than other non-canonical step types, including invalid steps. See figure 7 in the appendix for an example prediction where a misleading step causes the model to fail to prove the goal and produce an invalid step. This indicates that for the best-performing models, the main source of reasoning error is from misleading steps, since most predicted steps are strictly-valid and atomic. That is, imagining the space of proof steps as a graph where each edge represents a single valid step, InSTRUCTGPT almost always performs a walk in this graph. Once InSTRUCTGPT encounters a branch where one path at the fork follows the correct proof and the other paths do not,</p>
<p>Figure 4: Proof accuracy versus ontology type, number of hops, and ontology traversal direction. Each bar is subdivided into six darker bars according to the types of proof steps that appear in the predicted chains-of-thought. For example, the dark red bar corresponding to "invalid steps" indicates the proportion of incorrect proofs that contain an invalid step. The dark blue bar corresponding to "invalid steps" indicates the proportion of correct proofs that contain an invalid step. The proof step types are detailed in figure 2.
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>InSTructGPT will select the incorrect direction with some frequency and is then not able to return to the correct path. Therefore, it seems that while LLMs are able to produce valid proof steps with high probability, they have difficulty with proof planning/strategizing.
We were curious if this relationship held in smaller models. We see in figure 5 that smaller models are more prone to make invalid or non-atomic steps as their first non-canonical step. But as model size increases, these types of steps become rarer, and is instead superseded by misleading steps.
Looking again at figure 4, we note that many correct proofs also contain misleading steps, and so it must be the case that InSTRUCTGPT sometimes returns to the correct proof path at some point after making a misleading step. To investigate this behavior more closely, we count the number of steps that the model takes after making a misleading step until it produces a step in the gold proof and plot the histogram in figure 10 in the appendix. We observe that, in general, the more time the model spends outside the correct proof path, the less likely it becomes to return to the correct proof.
We demonstrate in section A. 7 in the appendix that our findings generalize to more sophisticated prompting strategies via an experiment using self-consistency prompting (Wang et al., 2022) and an experiment using a prompt containing example traces of depth-first proof search (i.e. containing examples of the search recovering from misleading steps).</p>
<p>Figure 5: Proportion of incorrect proofs versus the type of the first error (i.e., noncanonical proof step) and model size. The proof step types are detailed in figure 2. We note that in the 3-hop experiments with fictional ontology, four of the 400 examples surpassed the 2049 token limit for all models (except text-davinci-002). These examples were ignored (so the effective number of examples is 396). We omit the results for the 1-hop experiments here since there were too few incorrect proofs.</p>
<p>Fictional ontology, 3 hops, top-down traversal direction
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<h1>6 CONCLUSION AND FUTURE WORK</h1>
<p>In this work, we introduced a synthetic fictional QA dataset called PrOntoQA designed to evaluate the reasoning ability of LLMs. We evaluated InSTructGPT and GPT-3 on PrOntoQA and found that while the largest model was generally able to perform reasoning, it had difficulty with proof planning and selecting the correct proof step when there are multiple available.
PrOntoQA, and our high-level approach more broadly, could be used to compare LLM reasoning with that of humans, and to explore which aspects of human reasoning were acquired by LLMs from their pretraining. As our work has shown that LLMs are able to reason to a degree, it is yet unclear where the model acquired this ability. Are there portions of the pretraining data that teach the model to reason? Our work shows that CoT prompting is not sufficient for more complex reasoning, such as in mathematical domains, since the reasoning tested in this work is a strict subset of that of general mathematical reasoning. Mathematical proofs contain steps with much higher branching factor, where robust proof planning is instrumental. Rather, our results suggest that reasoning systems may benefit from more sophisticated proof planning/search strategies, such as neurosymbolic approaches where part of the reasoning is done over interpretable symbolic structures. PrOntoQA can be used to train new reasoning systems, or to pretrain/fine-tune LLMs to improve their reasoning capability.
The inability of LLMs to plan ahead in their reasoning might be related to recent work illuminating the theoretical computational limitations of such models (Merrill et al., 2022).
Since our analysis was limited to modus ponens, proof lengths of at most 5, and semantically simple sentences, it remains to be seen whether LLMs are able to produce longer proofs, or reason with other deduction rules, or over more semantically complex sentences/logical forms.</p>
<h1>REPRODUCIBILITY STATEMENT</h1>
<p>All our experiments in the main text were run using the OpenAI API on September $9^{\text {th }}, 10^{\text {th }}$, and $11^{\text {th }}, 2022$. The self-consistency experiment was run on October $29^{\text {th }}$ and $30^{\text {th }}$, and the DFS experiment was run on November $16^{\text {th }}$ (see section A.7). For the sake of reproducibility of the analysis, all model outputs, the code for data generation, and the analysis code are freely available with a permissive open-source license at github.com/asaparov/prontoqa. The command python analyze_results.py produces all figures used in this paper.</p>
<h2>ACKNOWLEDGMENTS</h2>
<p>We thank Vishakh Padmakumar, Richard Yuanzhe Pang, Nitish Joshi, Daniel Khashabi, Nicholas Lourie, and Will Merrill for their helpful and insightful discussion. This research was supported by Open Philanthropy, Samsung Advanced Institute of Technology (Next Generation Deep Learning: From Pattern Recognition to AI), AWS AI, and Cisco Research.</p>
<h2>REFERENCES</h2>
<p>Gabor Angeli and Christopher D. Manning. Naturalli: Natural logic inference for common sense reasoning. In Alessandro Moschitti, Bo Pang, and Walter Daelemans (eds.), Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, EMNLP 2014, October 25-29, 2014, Doha, Qatar, A meeting of SIGDAT, a Special Interest Group of the ACL, pp. 534-545. ACL, 2014. doi: 10.3115/v1/d14-1059. URL https://doi.org/10.3115/v1/d14-1059.</p>
<p>Cem Anil, Yuhuai Wu, Anders Andreassen, Aitor Lewkowycz, Vedant Misra, Vinay V. Ramasesh, Ambrose Slone, Guy Gur-Ari, Ethan Dyer, and Behnam Neyshabur. Exploring length generalization in large language models. CoRR, abs/2207.04901, 2022. doi: 10.48550/arXiv.2207.04901. URL https://doi.org/10.48550/arXiv.2207.04901.</p>
<p>Gregor Betz. Critical thinking for language models. CoRR, abs/2009.07185, 2020. URL https: //arxiv.org/abs/2009.07185.</p>
<p>Chandra Bhagavatula, Ronan Le Bras, Chaitanya Malaviya, Keisuke Sakaguchi, Ari Holtzman, Hannah Rashkin, Doug Downey, Wen-tau Yih, and Yejin Choi. Abductive commonsense reasoning. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020. URL https://openreview.net/forum? id=Byg1v1HKDB.</p>
<p>Kaj Bostrom, Xinyu Zhao, Swarat Chaudhuri, and Greg Durrett. Flexible generation of natural language deductions. In Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wentau Yih (eds.), Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic, 7-11 November, 2021, pp. 6266-6278. Association for Computational Linguistics, 2021. doi: 10.18653/v1/2021. emnlp-main.506. URL https://doi.org/10.18653/v1/2021.emnlp-main.506.</p>
<p>Kaj Bostrom, Zayne Sprague, Swarat Chaudhuri, and Greg Durrett. Natural language deduction through search over statement compositions. CoRR, abs/2201.06028, 2022. URL https:// arxiv.org/abs/2201.06028.</p>
<p>Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In Hugo Larochelle, Marc'Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin (eds.), Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020. URL https://proceedings.neurips.cc/paper/2020/hash/ 1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html.</p>
<p>Jifan Chen, Eunsol Choi, and Greg Durrett. Can NLI models verify QA systems' predictions? In Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih (eds.), Findings</p>
<p>of the Association for Computational Linguistics: EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic, 16-20 November, 2021, pp. 3841-3854. Association for Computational Linguistics, 2021. doi: 10.18653/v1/2021.findings-emnlp.324. URL https://doi.org/10. 18653/v1/2021.findings-emnlp. 324.</p>
<p>Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. Palm: Scaling language modeling with pathways. CoRR, abs/2204.02311, 2022. doi: 10.48550/arXiv.2204.02311. URL https://doi.org/10.48550/arXiv.2204.02311.</p>
<p>Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems. CoRR, abs/2110.14168, 2021. URL https://arxiv.org/abs/2110.14168.</p>
<p>Antonia Creswell and Murray Shanahan. Faithful reasoning using large language models. CoRR, abs/2208.14271, 2022. doi: 10.48550/arXiv.2208.14271. URL https://doi.org/10.48550/ arXiv. 2208.14271.</p>
<p>Antonia Creswell, Murray Shanahan, and Irina Higgins. Selection-inference: Exploiting large language models for interpretable logical reasoning. CoRR, abs/2205.09712, 2022. doi: 10.48550/ arXiv.2205.09712. URL https://doi.org/10.48550/arXiv.2205.09712.</p>
<p>Ishita Dasgupta, Andrew K. Lampinen, Stephanie C. Y. Chan, Antonia Creswell, Dharshan Kumaran, James L. McClelland, and Felix Hill. Language models show human-like content effects on reasoning. CoRR, abs/2207.07051, 2022. doi: 10.48550/arXiv.2207.07051. URL https://doi. org/10.48550/arXiv. 2207.07051.</p>
<p>David Dohan, Winnie Xu, Aitor Lewkowycz, Jacob Austin, David Bieber, Raphael Gontijo Lopes, Yuhuai Wu, Henryk Michalewski, Rif A. Saurous, Jascha Sohl-Dickstein, Kevin Murphy, and Charles Sutton. Language model cascades. CoRR, abs/2207.10342, 2022. doi: 10.48550/arXiv. 2207.10342. URL https://doi.org/10.48550/arXiv.2207.10342.</p>
<p>Honghua Dong, Jiayuan Mao, Tian Lin, Chong Wang, Lihong Li, and Denny Zhou. Neural logic machines. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019. URL https://openreview.net/forum?id= B1xY-hRctX.</p>
<p>Simeng Han, Hailey Schoelkopf, Yilun Zhao, Zhenting Qi, Martin Riddell, Luke Benson, Lucy Sun, Ekaterina Zubova, Yujie Qiao, Matthew Burtell, David Peng, Jonathan Fan, Yixin Liu, Brian Wong, Malcolm Sailor, Ansong Ni, Linyong Nan, Jungo Kasai, Tao Yu, Rui Zhang, Shafiq R. Joty, Alexander R. Fabbri, Wojciech Kryscinski, Xi Victoria Lin, Caiming Xiong, and Dragomir Radev. FOLIO: natural language reasoning with first-order logic. CoRR, abs/2209.00840, 2022. doi: 10.48550/arXiv.2209.00840. URL https://doi.org/10.48550/arXiv.2209.00840.</p>
<p>Pavan Kapanipathi, Ibrahim Abdelaziz, Srinivas Ravishankar, Salim Roukos, Alexander G. Gray, Ramón Fernandez Astudillo, Maria Chang, Cristina Cornelio, Saswati Dana, Achille Fokoue, Dinesh Garg, Alfio Gliozzo, Sairam Gurajada, Hima Karanam, Naweed Khan, Dinesh Khandelwal, Young-Suk Lee, Yunyao Li, Francois P. S. Luus, Ndivhuwo Makondo, Nandana Mihindukulasooriya, Tahira Naseem, Sumit Neelam, Lucian Popa, Revanth Gangi Reddy, Ryan Riegel, Gaetano Rossiello, Udit Sharma, G. P. Shrivatsa Bhargav, and Mo Yu. Leveraging abstract meaning representation for knowledge base question answering. In Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli (eds.), Findings of the Association for Computational Linguistics: ACL/IJCNLP</p>
<p>2021, Online Event, August 1-6, 2021, volume ACL/IJCNLP 2021 of Findings of ACL, pp. 38843894. Association for Computational Linguistics, 2021. doi: 10.18653/v1/2021.findings-acl.339. URL https://doi.org/10.18653/v1/2021.findings-acl. 339.</p>
<p>Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay V. Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, Yuhuai Wu, Behnam Neyshabur, Guy Gur-Ari, and Vedant Misra. Solving quantitative reasoning problems with language models. CoRR, abs/2206.14858, 2022. doi: 10.48550/arXiv.2206.14858. URL https: //doi.org/10.48550/arXiv.2206.14858.</p>
<p>Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, and Pontus Stenetorp. Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity. In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio (eds.), Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022, pp. 8086-8098. Association for Computational Linguistics, 2022. doi: 10.18653/v1/2022.acl-long.556. URL https://doi.org/10.18653/v1/2022.acl-long. 556 .</p>
<p>Bill MacCartney and Christopher D. Manning. An extended model of natural logic. In Harry Bunt, Volha Petukhova, and Sander Wubben (eds.), Proceedings of the Eight International Conference on Computational Semantics, IWCS 2009, Tilburg, The Netherlands, January 7-9, 2009, pp. 140-156. Association for Computational Linguistics, 2009. URL https://aclanthology.org/ W09-3714/.</p>
<p>William Merrill, Ashish Sabharwal, and Noah A. Smith. Saturated Transformers are Constant-Depth Threshold Circuits. Transactions of the Association for Computational Linguistics, 10:843-856, 08 2022. ISSN 2307-387X. doi: 10.1162/tacl_a_00493. URL https://doi.org/10.1162/ tacl_a_00493.</p>
<p>Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul F. Christiano, Jan Leike, and Ryan Lowe. Training language models to follow instructions with human feedback. CoRR, abs/2203.02155, 2022. doi: 10.48550/arXiv.2203.02155. URL https://doi.org/10.48550/ arXiv.2203.02155.</p>
<p>Yasaman Razeghi, Robert L. Logan IV, Matt Gardner, and Sameer Singh. Impact of pretraining term frequencies on few-shot reasoning. CoRR, abs/2202.07206, 2022. URL https://arxiv.org/ abs/2202.07206.</p>
<p>Tim Rocktäschel and Sebastian Riedel. End-to-end differentiable proving. In Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett (eds.), Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, pp. 3788-3800, 2017. URL https://proceedings.neurips.cc/paper/2017/ hash/b2ab001909a8a6f04b51920306046ce5-Abstract.html.</p>
<p>Abulhair Saparov and Tom M. Mitchell. Towards general natural language understanding with probabilistic worldbuilding. Trans. Assoc. Comput. Linguistics, 10:325-342, 2022. doi: 10.1162/ tacl_a_00463. URL https://doi.org/10.1162/tacl_a_00463.</p>
<p>Oyvind Tafjord, Bhavana Dalvi, and Peter Clark. Proofwriter: Generating implications, proofs, and abductive statements over natural language. In Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli (eds.), Findings of the Association for Computational Linguistics: ACL/IJCNLP 2021, Online Event, August 1-6, 2021, volume ACL/IJCNLP 2021 of Findings of ACL, pp. 3621-3634. Association for Computational Linguistics, 2021. doi: 10.18653/v1/2021.findings-acl.317. URL https://doi.org/10.18653/v1/2021.findings-acl. 317.</p>
<p>Karthik Valmeekam, Alberto Olmo Hernandez, Sarath Sreedharan, and Subbarao Kambhampati. Large language models still can't plan (A benchmark for llns on planning and reasoning about change). CoRR, abs/2206.10498, 2022. doi: 10.48550/arXiv.2206.10498. URL https://doi. org/10.48550/arXiv.2206.10498.</p>
<p>Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V. Le, Ed H. Chi, and Denny Zhou. Selfconsistency improves chain of thought reasoning in language models. CoRR, abs/2203.11171, 2022. doi: 10.48550/arXiv.2203.11171. URL https://doi.org/10.48550/arXiv.2203.11171.</p>
<p>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed H. Chi, Quoc Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language models. CoRR, abs/2201.11903, 2022. URL https://arxiv.org/abs/2201.11903.</p>
<p>Sean Welleck, Jiacheng Liu, Ronan Le Bras, Hanna Hajishirzi, Yejin Choi, and Kyunghyun Cho. Naturalproofs: Mathematical theorem proving in natural language. In Joaquin Vanschoren and Sai-Kit Yeung (eds.), Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021, December 2021, virtual, 2021. URL https://datasets-benchmarks-proceedings.neurips.cc/paper/ 2021/hash/d9d4f495e875a2e075a1a4a6e1b9770f-Abstract-round1.html.</p>
<p>Jason Weston, Antoine Bordes, Sumit Chopra, and Tomás Mikolov. Towards ai-complete question answering: A set of prerequisite toy tasks. In Yoshua Bengio and Yann LeCun (eds.), 4th International Conference on Learning Representations, ICLR 2016, San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings, 2016. URL http://arxiv.org/abs/1502.05698.</p>
<p>Edwin B Wilson. Probable inference, the law of succession, and statistical inference. J. Am. Stat. Assoc., 22(158):209, June 1927.</p>
<p>Hanlin Zhang, Yi-Fan Zhang, Li Erran Li, and Eric Xing. The impact of symbolic representations on in-context learning for few-shot reasoning. In Neuro Causal and Symbolic AI Workshop at NeurIPS 2022, Virtual Workshop, December 9, 2022, 2022a. URL https://openreview.net/ pdf?id=qLgQpeQX3x1. To appear.</p>
<p>Honghua Zhang, Liunian Harold Li, Tao Meng, Kai-Wei Chang, and Guy Van den Broeck. On the paradox of learning to reason from data. CoRR, abs/2205.11502, 2022b. doi: 10.48550/arXiv. 2205.11502. URL https://doi.org/10.48550/arXiv.2205.11502.</p>
<h1>A APPENDIX</h1>
<h2>A. 1 DEDUCTION RULES</h2>
<p>Figure 6 outlines the two deduction rules that we utilize in PRONTOQA.</p>
<h2>Deduction rules in general form</h2>
<h2>Examples</h2>
<p>$$
\begin{aligned}
&amp; \frac{f(a) \forall x(f(x) \rightarrow g(x))}{g(a)} \text { Hop } \
&amp; \text { i.e., Given that "Fae is a cat" and "All cats are carnivores," } \
&amp; \text { we conclude that "Fae is a carnivore." }
\end{aligned}
$$</p>
<p>$$
\begin{aligned}
&amp; \frac{\bar{A} \text { Ax }}{}
\end{aligned}
$$</p>
<p>i.e., Assume that "Fae is a cat" is an axiom.</p>
<p>Figure 6: The two deduction rules that constitute the restricted proof calculus in our experiments. All proofs in PRONTOQA are composed of instances of only these two rules. Here, $\bar{A}$ is any expression, and $f(a)$ is any expression where the variable $x$ in $f$ is substituted with any term $a$ (and similarly for $g(a)$ ).</p>
<h2>A. 2 AVOIDING SHORTCUTS</h2>
<p>When generating examples in PrONTOQA, we have to be careful to remove any shortcuts in the question that would allow the model to "guess" the answer without reasoning. In fact, we find that without any distractors, INSTRUCTGPT is able to predict the "true"/"false" label almost perfectly. INSTRUCTGPT can utilize the heuristic that whether the queried property is mentioned in the context implies whether or not it is true. For instance, if the example is asking "Sally is a cat. True or false: Sally is a vertebrate," the model can simply look for a string "Every $\qquad$ is (not) a vertebrate,"</p>
<p>regardless of the content in the blank. Due to the generative process of these examples, this kind of sentence is guaranteed to appear exactly once in the context. Thus, to ensure that such a heuristic is not informative, we add a distractor sentence by generating a novel concept that is disconnected from the ontology tree, and we assign to this new concept the negation property that is queried by the question. So in the above example, if the ontology has the rule "Every mammal is a vertebrate," a possible distractor sentence is "Every insect is not a vertebrate." We insert this distractor sentence into a random position in the context.</p>
<h1>A. 3 EXAMPLE INSTRUCTGPT MISPREDICTION</h1>
<div class="codehilite"><pre><span></span><code><span class="n">Q</span><span class="o">:</span><span class="w"> </span><span class="n">Every</span><span class="w"> </span><span class="n">vumpus</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">numpus</span><span class="o">.</span><span class="w"> </span><span class="n">Each</span><span class="w"> </span><span class="n">vumpus</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="n">dull</span><span class="o">.</span><span class="w"> </span><span class="n">Dumpuses</span><span class="w"> </span><span class="n">are</span><span class="w"> </span><span class="n">vumpuses</span><span class="o">.</span>
<span class="n">Every</span><span class="w"> </span><span class="n">dumpus</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="n">not</span><span class="w"> </span><span class="n">hot</span><span class="o">.</span><span class="w"> </span><span class="n">Every</span><span class="w"> </span><span class="n">impus</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">dumpus</span><span class="o">.</span><span class="w"> </span><span class="n">Impuses</span><span class="w"> </span><span class="n">are</span><span class="w"> </span><span class="n">brown</span><span class="o">.</span>
<span class="n">Wumpuses</span><span class="w"> </span><span class="n">are</span><span class="w"> </span><span class="n">impuses</span><span class="o">.</span><span class="w"> </span><span class="n">Wumpuses</span><span class="w"> </span><span class="n">are</span><span class="w"> </span><span class="n">opaque</span><span class="o">.</span><span class="w"> </span><span class="n">Tumpuses</span><span class="w"> </span><span class="n">are</span><span class="w"> </span><span class="n">wumpuses</span><span class="o">.</span><span class="w"> </span><span class="n">Tumpuses</span>
<span class="n">are</span><span class="w"> </span><span class="n">small</span><span class="o">.</span><span class="w"> </span><span class="n">Every</span><span class="w"> </span><span class="n">yumpus</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="n">hot</span><span class="o">.</span><span class="w"> </span><span class="n">Zumpuses</span><span class="w"> </span><span class="n">are</span><span class="w"> </span><span class="n">tumpuses</span><span class="o">.</span><span class="w"> </span><span class="n">Zumpuses</span><span class="w"> </span><span class="n">are</span><span class="w"> </span><span class="n">feisty</span><span class="o">.</span>
<span class="n">Rompuses</span><span class="w"> </span><span class="n">are</span><span class="w"> </span><span class="n">zumpuses</span><span class="o">.</span><span class="w"> </span><span class="n">Every</span><span class="w"> </span><span class="n">rompus</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="n">not</span><span class="w"> </span><span class="n">kind</span><span class="o">.</span><span class="w"> </span><span class="n">Each</span><span class="w"> </span><span class="n">jompus</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">rompus</span><span class="o">.</span>
<span class="n">Jompuses</span><span class="w"> </span><span class="n">are</span><span class="w"> </span><span class="n">sour</span><span class="o">.</span><span class="w"> </span><span class="n">Alex</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">zumpus</span><span class="o">.</span><span class="w"> </span><span class="n">True</span><span class="w"> </span><span class="n">or</span><span class="w"> </span><span class="kc">false</span><span class="o">:</span><span class="w"> </span><span class="n">Alex</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="n">hot</span><span class="o">.</span>
<span class="n">A</span><span class="o">:</span>
</code></pre></div>

<p>Expected answer: Alex is a zumpus. Zumpuses are tumpuses. Alex is a tumpus. Tumpuses are wumpuses. Alex is a wumpus. Wumpuses are impuses. Alex is an impus. Every impus is a dumpus. Alex is a dumpus. Every dumpus is not hot. Alex is not hot. False</p>
<p>Predicted answer: Alex is a zumpus. Zumpuses are tumpuses. Alex is a tumpus. Tum puses are wumpuses. Alex is a wumpus. Wumpuses are impuses. Alex is an impus. Every impus is a dumpus. Alex is a dumpus. Dumpuses are vumpuses. Alex is a vumpus. Every vumpus is a numpus. Alex is a numpus. Numpuses are jompuses. Alex is a jompus. Jompuses are rompuses. Alex is a rompus. Rompuses are zumpuses. Alex is a zumpus. Zumpuses are feisty. Alex is feisty. True</p>
<p>Figure 7: An example from PrOntoQA (5 hops with fictional ontology and top-down traversal direction) along with the expected and predicted answer from InSTRUCTGPT. Note that most of the predicted steps are canonical (highlighted yellow). The model makes a single misleading step (highlighted purple) which causes it fail to prove the goal, and to eventually make an invalid step (highlighted red).</p>
<h1>A. 4 How We EVALUATE the CHAIN-OF-THOUGHT</h1>
<div class="codehilite"><pre><span></span><code><span class="n">Algorithm</span><span class="w"> </span><span class="mi">1</span><span class="p">:</span><span class="w"> </span><span class="n">Our</span><span class="w"> </span><span class="n">algorithm</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">reconstructing</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="n">evaluating</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">proof</span><span class="w"> </span><span class="n">from</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">predicted</span><span class="w"> </span><span class="n">chain</span><span class="o">-</span><span class="n">of</span><span class="o">-</span><span class="n">thought</span><span class="p">,</span>
<span class="ow">and</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">computing</span><span class="w"> </span><span class="n">whether</span><span class="w"> </span><span class="n">each</span><span class="w"> </span><span class="n">proof</span><span class="w"> </span><span class="n">step</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="n">valid</span><span class="w"> </span><span class="n">vs</span><span class="w"> </span><span class="n">invalid</span><span class="p">,</span><span class="w"> </span><span class="n">atomic</span><span class="w"> </span><span class="n">vs</span><span class="w"> </span><span class="n">non</span><span class="o">-</span><span class="n">atomic</span><span class="p">,</span><span class="w"> </span><span class="n">misleading</span><span class="w"> </span><span class="n">vs</span><span class="w"> </span><span class="n">correct</span><span class="o">.</span>
<span class="n">Here</span><span class="p">,</span><span class="w"> </span><span class="n">we</span><span class="w"> </span><span class="n">use</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">notation</span><span class="w"> </span>\<span class="p">(</span>\<span class="n">varphi</span><span class="p">[</span><span class="n">x</span><span class="w"> </span>\<span class="n">mapsto</span><span class="w"> </span><span class="n">c</span><span class="p">]</span>\<span class="p">)</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">denote</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">substitution</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">all</span><span class="w"> </span><span class="n">occurrences</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">symbol</span><span class="w"> </span>\<span class="p">(</span><span class="n">x</span>\<span class="p">)</span><span class="w"> </span><span class="n">with</span><span class="w"> </span>\<span class="p">(</span><span class="n">c</span>\<span class="p">)</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">the</span>
<span class="n">logical</span><span class="w"> </span><span class="n">form</span><span class="w"> </span>\<span class="p">(</span>\<span class="n">varphi</span>\<span class="p">)</span><span class="o">.</span><span class="w"> </span><span class="n">We</span><span class="w"> </span><span class="n">use</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">helper</span><span class="w"> </span><span class="n">function</span><span class="w"> </span><span class="n">is_provable</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">compute</span><span class="w"> </span><span class="n">whether</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">given</span><span class="w"> </span><span class="n">logical</span><span class="w"> </span><span class="n">form</span><span class="w"> </span>\<span class="p">(</span>\<span class="n">varphi</span>\<span class="p">)</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="n">provable</span>
<span class="n">from</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">set</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">axioms</span><span class="w"> </span><span class="n">with</span><span class="w"> </span><span class="n">one</span><span class="w"> </span><span class="ow">or</span><span class="w"> </span><span class="n">more</span><span class="w"> </span><span class="n">deduction</span><span class="w"> </span><span class="n">rules</span><span class="o">.</span><span class="w"> </span><span class="n">The</span><span class="w"> </span><span class="n">function</span><span class="w"> </span><span class="n">returns</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">tuple</span><span class="w"> </span>\<span class="p">((</span><span class="n">P</span><span class="p">,</span><span class="w"> </span><span class="n">k</span><span class="p">)</span>\<span class="p">)</span><span class="w"> </span><span class="n">where</span><span class="w"> </span><span class="k">if</span><span class="w"> </span>\<span class="p">(</span><span class="n">k</span><span class="w"> </span>\<span class="n">geq</span><span class="w"> </span><span class="mi">0</span><span class="p">,</span><span class="w"> </span>\<span class="n">varphi</span>\<span class="p">)</span><span class="w"> </span><span class="k">is</span>
<span class="n">provable</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span>\<span class="p">(</span><span class="n">k</span>\<span class="p">)</span><span class="w"> </span><span class="n">steps</span><span class="w"> </span><span class="n">using</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">premises</span><span class="w"> </span>\<span class="p">(</span><span class="n">P</span>\<span class="p">)</span><span class="o">.</span><span class="w"> </span><span class="n">Otherwise</span><span class="p">,</span><span class="w"> </span>\<span class="p">(</span>\<span class="n">varphi</span>\<span class="p">)</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="ow">not</span><span class="w"> </span><span class="n">provable</span><span class="o">.</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code><span class="n">function</span><span class="w"> </span><span class="n">evaluate_cot</span><span class="p">(</span><span class="n">context</span><span class="w"> </span><span class="n">sentences</span><span class="w"> </span>\<span class="p">(</span><span class="n">Q_</span><span class="p">{</span><span class="mi">1</span><span class="p">},</span><span class="w"> </span>\<span class="n">ldots</span><span class="p">,</span><span class="w"> </span><span class="n">Q_</span><span class="p">{</span><span class="n">m</span><span class="p">}</span>\<span class="p">),</span>
<span class="w">    </span><span class="n">predicted</span><span class="w"> </span><span class="n">chain</span><span class="o">-</span><span class="n">of</span><span class="o">-</span><span class="n">thought</span><span class="w"> </span><span class="n">sentences</span><span class="w"> </span>\<span class="p">(</span><span class="n">C_</span><span class="p">{</span><span class="mi">1</span><span class="p">},</span><span class="w"> </span>\<span class="n">ldots</span><span class="p">,</span><span class="w"> </span><span class="n">C_</span><span class="p">{</span><span class="n">n</span><span class="p">}</span>\<span class="p">),</span>
<span class="w">                        </span><span class="n">gold</span><span class="w"> </span><span class="n">chain</span><span class="o">-</span><span class="n">of</span><span class="o">-</span><span class="n">thought</span><span class="w"> </span><span class="n">sentences</span><span class="w"> </span>\<span class="p">(</span><span class="n">T_</span><span class="p">{</span><span class="mi">1</span><span class="p">},</span><span class="w"> </span>\<span class="n">ldots</span><span class="p">,</span><span class="w"> </span><span class="n">T_</span><span class="p">{</span><span class="n">r</span><span class="p">}</span>\<span class="p">)</span><span class="w"> </span><span class="p">)</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span>\<span class="p">(</span><span class="n">i</span><span class="w"> </span>\<span class="ow">in</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span>\<span class="n">ldots</span><span class="p">,</span><span class="w"> </span><span class="n">m</span>\<span class="p">)</span><span class="w"> </span><span class="n">do</span><span class="w"> </span><span class="o">/*</span><span class="w"> </span><span class="n">parse</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">context</span><span class="w"> </span><span class="o">*/</span>
<span class="w">    </span>\<span class="p">(</span><span class="n">L_</span><span class="p">{</span><span class="n">i</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="n">Q</span><span class="p">}</span><span class="o">=</span>\<span class="p">)</span><span class="w"> </span><span class="n">semantic_parse</span><span class="w"> </span>\<span class="p">(</span>\<span class="n">left</span><span class="p">(</span><span class="n">Q_</span><span class="p">{</span><span class="n">i</span><span class="p">}</span>\<span class="n">right</span><span class="p">)</span>\<span class="p">)</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span>\<span class="p">(</span><span class="n">i</span><span class="w"> </span>\<span class="ow">in</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span>\<span class="n">ldots</span><span class="p">,</span><span class="w"> </span><span class="n">r</span>\<span class="p">)</span><span class="w"> </span><span class="n">do</span><span class="w"> </span><span class="o">/*</span><span class="w"> </span><span class="n">parse</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">gold</span><span class="w"> </span><span class="n">chain</span><span class="o">-</span><span class="n">of</span><span class="o">-</span><span class="n">thought</span><span class="w"> </span><span class="o">*/</span>
<span class="w">    </span>\<span class="p">(</span><span class="n">L_</span><span class="p">{</span><span class="n">i</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="n">T</span><span class="p">}</span><span class="o">=</span>\<span class="p">)</span><span class="w"> </span><span class="n">semantic_parse</span><span class="w"> </span>\<span class="p">(</span>\<span class="n">left</span><span class="p">(</span><span class="n">T_</span><span class="p">{</span><span class="n">i</span><span class="p">}</span>\<span class="n">right</span><span class="p">)</span>\<span class="p">)</span>
<span class="w">    </span><span class="n">initialize</span><span class="w"> </span>\<span class="p">(</span><span class="n">S</span>\<span class="p">)</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="n">an</span><span class="w"> </span><span class="n">empty</span><span class="w"> </span><span class="n">set</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span>\<span class="p">(</span><span class="n">i</span><span class="w"> </span>\<span class="ow">in</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span>\<span class="n">ldots</span><span class="p">,</span><span class="w"> </span><span class="n">n</span>\<span class="p">)</span><span class="w"> </span><span class="n">do</span><span class="w"> </span><span class="o">/*</span><span class="w"> </span><span class="n">parse</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="n">evaluate</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">predicted</span><span class="w"> </span><span class="n">chain</span><span class="o">-</span><span class="n">of</span><span class="o">-</span><span class="n">thought</span><span class="w"> </span><span class="o">*/</span>
<span class="w">    </span>\<span class="p">(</span><span class="n">L_</span><span class="p">{</span><span class="n">i</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="n">C</span><span class="p">}</span><span class="o">=</span>\<span class="p">)</span><span class="w"> </span><span class="n">semantic_parse</span><span class="w"> </span>\<span class="p">(</span>\<span class="n">left</span><span class="p">(</span><span class="n">C_</span><span class="p">{</span><span class="n">i</span><span class="p">}</span>\<span class="n">right</span><span class="p">)</span>\<span class="p">)</span>
<span class="w">    </span>\<span class="p">((</span><span class="n">P</span><span class="p">,</span><span class="w"> </span><span class="n">k</span><span class="p">)</span><span class="o">=</span>\<span class="p">)</span><span class="w"> </span><span class="n">is_provable</span><span class="w"> </span>\<span class="p">(</span>\<span class="n">left</span><span class="p">(</span><span class="n">L_</span><span class="p">{</span><span class="n">i</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="n">C</span><span class="p">},</span>\<span class="n">left</span>\<span class="p">{</span><span class="n">L_</span><span class="p">{</span><span class="mi">1</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="n">Q</span><span class="p">},</span><span class="w"> </span>\<span class="n">ldots</span><span class="p">,</span><span class="w"> </span><span class="n">L_</span><span class="p">{</span><span class="n">m</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="n">Q</span><span class="p">}</span>\<span class="n">right</span>\<span class="p">},</span><span class="w"> </span><span class="n">S</span>\<span class="n">right</span><span class="p">)</span>\<span class="p">)</span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span>\<span class="p">(</span><span class="n">k</span><span class="w"> </span>\<span class="n">geq</span><span class="w"> </span><span class="mi">0</span>\<span class="p">)</span>
<span class="w">    </span><span class="o">/*</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="n">we</span><span class="w"> </span><span class="n">wish</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">use</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">stricter</span><span class="w"> </span><span class="n">metric</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">proof</span><span class="w"> </span><span class="n">accuracy</span><span class="p">,</span><span class="w"> </span><span class="n">we</span><span class="w"> </span><span class="n">can</span><span class="w"> </span><span class="n">add</span>
<span class="w">        </span><span class="n">conditions</span><span class="w"> </span><span class="n">here</span><span class="w"> </span><span class="p">(</span><span class="n">e</span><span class="o">.</span><span class="n">g</span><span class="o">.</span><span class="p">,</span><span class="w"> </span><span class="n">requiring</span><span class="w"> </span><span class="n">atomicity</span><span class="w"> </span><span class="n">by</span><span class="w"> </span><span class="n">checking</span><span class="w"> </span>\<span class="p">(</span><span class="n">k</span><span class="o">=</span><span class="mi">1</span>\<span class="p">)</span><span class="w"> </span><span class="p">)</span><span class="w"> </span><span class="o">*/</span>
<span class="w">    </span><span class="n">add</span><span class="w"> </span>\<span class="p">(</span><span class="n">L_</span><span class="p">{</span><span class="n">i</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="n">C</span><span class="p">}</span>\<span class="p">)</span><span class="w"> </span><span class="n">to</span><span class="w"> </span>\<span class="p">(</span><span class="n">S</span>\<span class="p">)</span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span>\<span class="p">(</span><span class="n">P</span><span class="w"> </span>\<span class="n">subseteq</span>\<span class="n">left</span>\<span class="p">{</span><span class="n">L_</span><span class="p">{</span><span class="mi">1</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="n">T</span><span class="p">},</span><span class="w"> </span>\<span class="n">ldots</span><span class="p">,</span><span class="w"> </span><span class="n">L_</span><span class="p">{</span><span class="n">r</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="n">T</span><span class="p">}</span>\<span class="n">right</span>\<span class="p">}</span>\<span class="p">)</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span>\<span class="p">(</span><span class="n">L_</span><span class="p">{</span><span class="n">i</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="n">C</span><span class="p">}</span><span class="w"> </span>\<span class="n">notin</span>\<span class="n">left</span>\<span class="p">{</span><span class="n">L_</span><span class="p">{</span><span class="mi">1</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="n">T</span><span class="p">},</span><span class="w"> </span>\<span class="n">ldots</span><span class="p">,</span><span class="w"> </span><span class="n">L_</span><span class="p">{</span><span class="n">r</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="n">T</span><span class="p">}</span>\<span class="n">right</span>\<span class="p">}</span>\<span class="p">)</span>
<span class="w">    </span><span class="o">/*</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">premises</span><span class="w"> </span><span class="n">are</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">gold</span><span class="w"> </span><span class="n">proof</span><span class="w"> </span><span class="n">but</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">conclusion</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="ow">not</span><span class="w"> </span><span class="o">*/</span>
<span class="w">    </span><span class="n">mark</span><span class="w"> </span>\<span class="p">(</span><span class="n">L_</span><span class="p">{</span><span class="n">i</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="n">C</span><span class="p">}</span>\<span class="p">)</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">misleading</span><span class="w"> </span><span class="n">step</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span>\<span class="p">(</span><span class="n">L_</span><span class="p">{</span><span class="n">i</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="n">T</span><span class="p">}</span><span class="w"> </span>\<span class="ow">in</span><span class="w"> </span><span class="n">S</span><span class="w"> </span>\<span class="n">quad</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="o">*</span>\<span class="p">)</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">proof</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="n">correct</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">final</span><span class="w"> </span><span class="n">conclusion</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="n">provable</span><span class="w"> </span><span class="o">*/</span>
<span class="n">function</span><span class="w"> </span><span class="n">is_provable</span><span class="p">(</span><span class="n">logical</span><span class="w"> </span><span class="n">form</span><span class="w"> </span>\<span class="p">(</span>\<span class="n">varphi</span>\<span class="p">),</span><span class="w"> </span><span class="n">set</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">axioms</span><span class="w"> </span>\<span class="p">(</span><span class="n">A</span>\<span class="p">),</span><span class="w"> </span><span class="n">previous</span><span class="w"> </span><span class="n">conclusions</span><span class="w"> </span>\<span class="p">(</span><span class="n">S</span>\<span class="p">)</span><span class="w"> </span><span class="p">)</span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span>\<span class="p">(</span>\<span class="n">varphi</span><span class="w"> </span>\<span class="ow">in</span><span class="w"> </span><span class="n">A</span>\<span class="p">)</span>
<span class="w">        </span><span class="k">return</span><span class="w"> </span>\<span class="p">(</span>\<span class="n">left</span><span class="p">(</span>\<span class="p">{</span>\<span class="n">varphi</span>\<span class="p">},</span><span class="w"> </span><span class="mi">1</span>\<span class="n">right</span><span class="p">)</span><span class="w"> </span>\<span class="n">quad</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="o">*</span>\<span class="p">)</span><span class="w"> </span><span class="n">provable</span><span class="w"> </span><span class="n">by</span><span class="w"> </span><span class="o">&amp;</span><span class="n">x</span><span class="w"> </span><span class="n">step</span><span class="w"> </span><span class="p">(</span><span class="n">strictly</span><span class="o">-</span><span class="n">valid</span><span class="p">)</span><span class="w"> </span><span class="o">*/</span>
<span class="w">    </span><span class="k">else</span><span class="w"> </span><span class="k">if</span><span class="w"> </span>\<span class="p">(</span>\<span class="n">varphi</span><span class="w"> </span>\<span class="ow">in</span><span class="w"> </span><span class="n">S</span>\<span class="p">)</span>
<span class="w">        </span><span class="k">return</span><span class="w"> </span>\<span class="p">(</span>\<span class="n">left</span><span class="p">(</span>\<span class="p">{</span>\<span class="n">varphi</span>\<span class="p">},</span><span class="w"> </span><span class="mi">0</span>\<span class="n">right</span><span class="p">)</span><span class="w"> </span>\<span class="n">quad</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="o">*</span>\<span class="p">)</span><span class="w"> </span><span class="n">already</span><span class="w"> </span><span class="n">proved</span><span class="w"> </span><span class="n">by</span><span class="w"> </span><span class="n">previous</span><span class="w"> </span><span class="n">step</span><span class="w"> </span><span class="o">*/</span>
<span class="w">    </span><span class="k">else</span><span class="w"> </span><span class="k">if</span><span class="w"> </span>\<span class="p">(</span>\<span class="n">varphi</span>\<span class="p">)</span><span class="w"> </span><span class="n">has</span><span class="w"> </span><span class="n">form</span><span class="w"> </span>\<span class="p">(</span><span class="n">g</span><span class="p">(</span><span class="n">c</span><span class="p">)</span>\<span class="p">)</span><span class="w"> </span><span class="ow">or</span><span class="w"> </span>\<span class="p">(</span>\<span class="n">neg</span><span class="w"> </span><span class="n">g</span><span class="p">(</span><span class="n">c</span><span class="p">)</span>\<span class="p">)</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">any</span><span class="w"> </span><span class="n">constants</span><span class="w"> </span>\<span class="p">(</span><span class="n">g</span>\<span class="p">)</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span>\<span class="p">(</span><span class="n">c</span>\<span class="p">)</span>
<span class="w">        </span><span class="k">for</span><span class="w"> </span>\<span class="p">(</span><span class="n">a</span><span class="w"> </span>\<span class="ow">in</span><span class="w"> </span><span class="n">A</span><span class="w"> </span>\<span class="n">cup</span><span class="w"> </span><span class="n">S</span>\<span class="p">)</span><span class="w"> </span><span class="n">do</span>
<span class="w">            </span><span class="k">if</span><span class="w"> </span>\<span class="p">(</span><span class="n">a</span>\<span class="p">)</span><span class="w"> </span><span class="n">has</span><span class="w"> </span><span class="n">form</span><span class="w"> </span>\<span class="p">(</span>\<span class="n">forall</span><span class="w"> </span><span class="n">x</span><span class="p">(</span>\<span class="n">psi</span><span class="w"> </span>\<span class="n">rightarrow</span><span class="w"> </span>\<span class="n">gamma</span><span class="p">)</span>\<span class="p">)</span><span class="w"> </span><span class="n">where</span><span class="w"> </span>\<span class="p">(</span>\<span class="n">gamma</span><span class="p">[</span><span class="n">x</span><span class="w"> </span>\<span class="n">mapsto</span><span class="w"> </span><span class="n">c</span><span class="p">]</span><span class="o">=</span>\<span class="n">varphi</span>\<span class="p">)</span>
<span class="w">            </span>\<span class="p">((</span><span class="n">P</span><span class="p">,</span><span class="w"> </span><span class="n">k</span><span class="p">)</span><span class="o">=</span>\<span class="p">)</span><span class="w"> </span><span class="n">is_provable</span><span class="w"> </span>\<span class="p">((</span>\<span class="n">psi</span><span class="p">[</span><span class="n">x</span><span class="w"> </span>\<span class="n">mapsto</span><span class="w"> </span><span class="n">c</span><span class="p">],</span><span class="w"> </span><span class="n">A</span><span class="p">,</span><span class="w"> </span><span class="n">S</span><span class="p">)</span>\<span class="p">)</span>
<span class="w">            </span><span class="k">if</span><span class="w"> </span>\<span class="p">(</span><span class="n">k</span><span class="w"> </span>\<span class="n">geq</span><span class="w"> </span><span class="mi">0</span>\<span class="p">)</span>
<span class="w">            </span><span class="k">return</span><span class="w"> </span>\<span class="p">((</span><span class="n">P</span><span class="w"> </span>\<span class="n">cup</span>\<span class="p">{</span><span class="n">a</span>\<span class="p">},</span><span class="w"> </span><span class="n">k</span><span class="o">+</span>\<span class="n">mathbb</span><span class="p">{</span><span class="mi">1</span><span class="p">}</span>\<span class="p">{</span><span class="n">a</span><span class="w"> </span>\<span class="ow">in</span><span class="w"> </span><span class="n">A</span>\<span class="p">})</span><span class="w"> </span>\<span class="n">quad</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="o">*</span>\<span class="p">)</span><span class="w"> </span><span class="n">provable</span><span class="w"> </span><span class="n">by</span><span class="w"> </span><span class="n">Hop</span><span class="w"> </span><span class="n">step</span><span class="w"> </span><span class="p">(</span><span class="n">strictly</span><span class="o">-</span><span class="n">valid</span><span class="p">)</span><span class="w"> </span><span class="o">*/</span>
<span class="w">    </span><span class="k">else</span><span class="w"> </span><span class="k">if</span><span class="w"> </span>\<span class="p">(</span>\<span class="n">varphi</span>\<span class="p">)</span><span class="w"> </span><span class="n">has</span><span class="w"> </span><span class="n">form</span><span class="w"> </span>\<span class="p">(</span>\<span class="n">forall</span><span class="w"> </span><span class="n">x</span><span class="p">(</span>\<span class="n">psi</span><span class="w"> </span>\<span class="n">rightarrow</span><span class="w"> </span>\<span class="n">gamma</span><span class="p">)</span>\<span class="p">)</span>
<span class="w">    </span><span class="o">/*</span><span class="w"> </span><span class="n">note</span><span class="p">:</span><span class="w"> </span><span class="n">we</span><span class="w"> </span><span class="n">precompute</span><span class="w"> </span><span class="n">this</span><span class="w"> </span><span class="n">graph</span><span class="w"> </span><span class="o">*/</span>
<span class="w">    </span><span class="n">let</span><span class="w"> </span>\<span class="p">(</span><span class="n">G</span>\<span class="p">)</span><span class="w"> </span><span class="n">be</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">graph</span><span class="w"> </span><span class="n">where</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">any</span><span class="w"> </span><span class="n">axiom</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span>\<span class="p">(</span><span class="n">A</span>\<span class="p">)</span><span class="w"> </span><span class="n">with</span><span class="w"> </span><span class="n">form</span><span class="w"> </span>\<span class="p">(</span>\<span class="n">forall</span><span class="w"> </span><span class="n">x</span><span class="p">(</span>\<span class="n">alpha</span><span class="w"> </span>\<span class="n">rightarrow</span><span class="w"> </span>\<span class="n">beta</span><span class="p">),</span><span class="w"> </span>\<span class="n">alpha</span>\<span class="p">)</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span>\<span class="p">(</span>\<span class="n">beta</span>\<span class="p">)</span><span class="w"> </span><span class="n">are</span><span class="w"> </span><span class="n">vertices</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="n">there</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="n">a</span>
<span class="w">        </span><span class="n">directed</span><span class="w"> </span><span class="n">edge</span><span class="w"> </span><span class="n">from</span><span class="w"> </span>\<span class="p">(</span>\<span class="n">alpha</span>\<span class="p">)</span><span class="w"> </span><span class="n">to</span><span class="w"> </span>\<span class="p">(</span>\<span class="n">beta</span>\<span class="p">)</span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="n">there</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">path</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span>\<span class="p">(</span><span class="n">G</span>\<span class="p">)</span><span class="w"> </span><span class="n">from</span><span class="w"> </span>\<span class="p">(</span>\<span class="n">psi</span>\<span class="p">)</span><span class="w"> </span><span class="n">to</span><span class="w"> </span>\<span class="p">(</span>\<span class="n">gamma</span>\<span class="p">)</span>
<span class="w">    </span><span class="o">/*</span><span class="w"> </span><span class="n">provable</span><span class="w"> </span><span class="n">with</span><span class="w"> </span><span class="n">additional</span><span class="w"> </span><span class="n">deduction</span><span class="w"> </span><span class="n">rules</span><span class="w"> </span><span class="p">(</span><span class="n">broadly</span><span class="o">-</span><span class="n">valid</span><span class="p">)</span><span class="w"> </span><span class="o">*/</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="p">(</span><span class="w"> </span><span class="n">axioms</span><span class="w"> </span><span class="n">corresponding</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">path</span><span class="w"> </span><span class="n">edges</span><span class="p">,</span><span class="w"> </span><span class="n">length</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">path</span><span class="w"> </span><span class="p">)</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span>\<span class="p">((</span>\<span class="n">varnothing</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="w"> </span>\<span class="n">quad</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="o">*</span>\<span class="p">)</span><span class="w"> </span><span class="n">this</span><span class="w"> </span><span class="n">step</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="ow">not</span><span class="w"> </span><span class="n">provable</span><span class="w"> </span><span class="p">(</span><span class="n">i</span><span class="o">.</span><span class="n">e</span><span class="o">.</span><span class="p">,</span><span class="w"> </span><span class="n">invalid</span><span class="p">)</span><span class="w"> </span><span class="o">*/</span>
</code></pre></div>

<h1>A. 5 Proof accuracy vs model size</h1>
<p>Figure 8: Proof accuracy versus model size, ontology type, and number of hops. Each bar is subdivided into six bars according to the types of proof steps that appear in the predicted chains-of-thought. The proof step types are detailed in figure 2. Top-down traversal direction is used in these experiments. We note that in the 3-hop experiments with fictional ontology, four of the 400 examples surpassed the 2049 token limit for all models (except text-davinci-002). These examples were ignored (so the effective number of examples is 396).</p>
<p>Fictional ontology, 3 hops
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>False ontology, 3 hops
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>True ontology, 3 hops
<img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Fictional ontology, 1 hop
<img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>False ontology, 1 hop
<img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>True ontology, 1 hop
<img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p><img alt="img-10.jpeg" src="img-10.jpeg" /></p>
<p>Figure 9: Proportion of incorrect proofs versus the type of the first error (i.e., non-canonical proof step), number of hops, and ontology traversal direction. The proof step types are detailed in figure 2. We note that in the 3-hop experiments with fictional ontology, four of the 400 examples surpassed the 2049 token limit for all models (except text-davinci-002). These examples were ignored (so the effective number of examples is 396). We omit the results for the 1 hop experiments here since there were too few incorrect proofs.</p>
<p><img alt="img-11.jpeg" src="img-11.jpeg" /></p>
<p>Figure 10: Histograms depicting the distribution of the number of steps in each proof after a strictly-valid atomic misleading step until returning to the gold proof.
<img alt="img-12.jpeg" src="img-12.jpeg" /></p>
<h1>A. 7 DO OTHER PROMPTING STRATEGIES HELP?</h1>
<h2>A.7.1 SELF-CONSISTENCY</h2>
<p>To what extent do our findings generalize to prompting strategies other than CoT with greedy decoding? To test this, we experimented with self-consistency prompting (Wang et al., 2022), where for each example, we queried the LLM for 40 sample predictions of the CoT, using a temperature setting of 0.7 . For each sample $\mathrm{CoT} s_{i}$, we compute the following quantity:</p>
<p>$$
\exp \left{\frac{1}{\left|s_{i}\right|} \sum_{j=1}^{\left|s_{i}\right|} \log p\left(s_{i, j} \mid s_{i, 1}, \ldots, s_{i, j-1}\right)\right}
$$</p>
<p>We parse each predicted CoT sample into a sequence of logical forms, and we find the logical form sequence with the highest sum of the above quantity over all the CoT samples that share the same semantic parse. This logical form sequence is the final prediction.
We run this experiment in our setting with 5 hops, fictional ontology, and top-down traversal direction, with 100 examples. The resulting valid proof accuracy is 0.56 compared to 0.545 which is not significantly different. Furthermore, inspecting specific examples of CoT samples (see figure 11), we see that for examples that the model gets wrong, the model is actually assigning higher overall probability to the incorrect proof than to the correct proof. This suggests that our results do in fact generalize to more sophisticated prompting/decoding strategies, and that strategies that endeavor to find proofs with higher probability globally (e.g. beam search) will not help the model in proof planning.</p>
<h2>A.7.2 CAN THE MODEL LEARN TO DO DEPTH-FIRST SEARCH FROM IN-CONTEXT EXAMPLES?</h2>
<p>Our earlier analysis also revealed a possible way forward to rectify the model's shortcoming in proof planning: Even after making a misleading step, InSTRUCTGPT sometimes "returned" to the correct proof. We could instead relax the constraint that the chains-of-thought always reflect the shortest correct proof of the answer. Instead, we allow the in-context examples to contain misleading steps, with the hope that the model learns to better recover from misleading steps. It is reasonable for humans to explore a space of possible solutions before arriving at the correct answer, and we could mimic this in LLMs by allowing the CoT to explore alternative paths in the space of proof steps, even if those paths are not ultimately part of the shortest proof. This kind of search strategy is analogous to depth-first search (DFS) in graphs.
To test whether this approach improves the model's reasoning ability, we conduct an experiment where we provide in-context examples of chains-of-thought that follow a DFS, with the hope that the model is able to learn to perform DFS when given a new test example, thereby improving the likelihood that it finds the correct answer. We run this experiment in our setting with 5 hops, fictional ontology, and top-down traversal direction, with 100 examples. The resulting valid proof accuracy is 0.55 compared to 0.545 which, again, is not significantly different.</p>
<p>Q: Each rompus is a wumpus. Every rompus is not opaque. Every jompus is a rompus. Every jompus is not sour. Vumpuses are jompuses. Each vumpus is not earthy. Yumpuses are vumpuses. Each yumpus is angry. Each tumpus is a yumpus. Tumpuses are large. Numpuses are tumpuses. Numpuses are not cold. Every zumpus is a numpus. Dumpuses are not angry. Zumpuses are shy. Impuses are zumpuses. Impuses are luminous. Polly is an impus. True or false: Polly is angry.
A:
Expected answer: Polly is an impus. Impuses are zumpuses. Polly is a zumpus. Every zumpus is a numpus. Polly is a numpus. Numpuses are tumpuses. Polly is a tumpus. Each tumpus is a yumpus. Polly is a yumpus. Each yumpus is angry. Polly is angry. True</p>
<p>Sample predicted answer: Polly is an impus. Impuses are zumpuses. Polly is a zum pus. Zumpuses are numpuses. Polly is a numpus. Numpuses are tumpuses. Polly is a tumpus. Every tumpus is a yumpus. Polly is a yumpus. Yumpuses are vumpuses. Polly is a vumpus. Vumpuses are jompuses. Polly is a jompus. Each jompus is a rompus. Polly is a rompus. Rompuses are wumpuses. Polly is a wumpus. Vumpuses are not angry. Polly is not angry. False
Sample average log probability: $-0.07272354467015028$</p>
<p>Sample predicted answer: Polly is an impus. Impuses are zumpuses. Polly is a zum pus. Zumpuses are numpuses. Polly is a numpus. Numpuses are tumpuses. Polly is a tumpus. Each tumpus is a yumpus. Polly is a yumpus. Yumpuses are angry. Polly is angry. True
Sample average log probability: $-0.04729328860082488$
Sample predicted answer: Polly is an impus. Impuses are zumpuses. Polly is a zum pus. Zumpuses are numpuses. Polly is a numpus. Numpuses are tumpuses. Polly is a tumpus. Each tumpus is a yumpus. Polly is a yumpus. Yumpuses are vumpuses. Polly is a vumpus. Vumpuses are jompuses. Polly is a jompus. Every jompus is a rompus. Polly is a rompus. Rompuses are not angry. Polly is not angry. False Sample average log probability: $-0.02227105943358164$</p>
<p>Figure 11: An example from PrOntoQA (5 hops with fictional ontology and top-down traversal direction) using self-consistency (each sample was produced using a temperature of 0.7 ), showing the expected and sample predicted answers from InSTRUCTGPT. Canonical steps are highlighted yellow, misleading steps purple, and invalid steps red. We note that the sample predicted CoTs that correspond to the gold proof are given lower overall probability than those that are incorrect.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{3}$ In natural deduction, this rule is actually a composition of two steps: given $\forall x(f(x) \rightarrow g(x))$, use universal elimination to conclude $f(a) \rightarrow g(a)$, and given $f(a)$, use implication elimination to conclude $g(a)$.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
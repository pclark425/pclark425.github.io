<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6011 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6011</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6011</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-121.html">extraction-schema-121</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <p><strong>Paper ID:</strong> paper-270559604</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2406.10421v3.pdf" target="_blank">SciEx: Benchmarking Large Language Models on Scientific Exams with Human Expert Grading and Automatic Grading</a></p>
                <p><strong>Paper Abstract:</strong> With the rapid development of Large Language Models (LLMs), it is crucial to have benchmarks which can evaluate the ability of LLMs on different domains. One common use of LLMs is performing tasks on scientific topics, such as writing algorithms, querying databases or giving mathematical proofs. Inspired by the way university students are evaluated on such tasks, in this paper, we propose SciEx - a benchmark consisting of university computer science exam questions, to evaluate LLMs’ ability on solving scientific tasks. SciEx is (1) multilingual, containing both English and German exams, and (2) multi-modal, containing questions that involve images, and (3) contains various types of freeform questions with different difficulty levels, due to the nature of university exams. We evaluate the performance of various state-of-the-art LLMs on our new benchmark. Since SciEx questions are freeform, it is not straightforward to evaluate LLM performance. Therefore, we provide human expert grading of the LLM outputs on SciEx. We show that the free-form exams in SciEx remain challenging for the current LLMs, where the best LLM only achieves 59.4% exam grade on average. We also provide detailed comparisons between LLM performance and student performance on SciEx. To enable future evaluation of new LLMs, we propose using LLM-as-a-judge to grade the LLM answers on SciEx. Our experiments show that, although they do not perform perfectly on solving the exams, LLMs are decent as graders, achieving 0.948 Pearson correlation with expert grading.</p>
                <p><strong>Cost:</strong> 0.013</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6011.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6011.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SciEx</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SciEx (Scientific Exams Benchmark)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A freeform, multimodal, multilingual benchmark of university-level computer science exam questions (converted to JSON) with LLM-generated answers, lecturer expert grading, and automatic LLM-based grading for evaluating LLM performance on scientific tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Benchmark evaluation of LLM outputs on university computer-science exam questions using (1) human expert grading by the course lecturers and (2) automatic grading via LLM-as-a-judge; experiments run per-question and per-exam with normalization to percent-scores and mapping to the German grade scale.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Numeric scores per question (0..max) normalized to 0–100%; German grade conversion (1.0–5.0 with 4.0 passing); exam-level (sum) and question-level analyses; Pearson correlation between automatic graders and expert grades; RMSE on original score scales; ancillary analyses by language, modality (image vs text), and difficulty.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>As examinees: Claude, GPT-4V, GPT-3.5 (gpt-3.5-turbo-0125), Mixtral, Qwen, Mistral, Llava, (Llama3 held-out). As graders: GPT-4V, Llama3, Mixtral.</td>
                        </tr>
                        <tr>
                            <td><strong>theory_domain</strong></td>
                            <td>Computer Science (university exam topics spanning NLP, AI, deep learning, CV, HCI, databases, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>Not a single scientific theory — the 'theories' are the freeform LLM-generated solutions, proofs, explanations, algorithms, and figure-descriptions produced in response to diverse CS exam questions; these outputs are judged as exam answers.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Dataset: 1,120 question-answer pairs. Expert grading: best-performing examinee Claude averaged 59.4% of maximum points (German grade 2.4), GPT-4V 58.2%; student average 45.3%. Automatic grading: GPT-4V-as-grader achieved exam-level Pearson correlation 0.948 to expert grades (Llama3 0.883). Question-level top correlations around 0.7. Stronger LLMs outperform students on many hard questions; multimodal graders (GPT-4V, Claude) outperform on image questions but overall image-handling remains weaker.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>SciEx dataset (this work): university CS exams from multiple courses and semesters, converted to JSON with image paths; 1120 Q-A pairs and expert-provided reference answers, difficulty labels, and average student grades.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human</strong></td>
                            <td>Expert lecturer grading used as gold standard; automatic LLM graders achieve high agreement with experts at exam-level (Pearson up to 0.948) but lower at question-level (~0.7). Strong proprietary models as examinees can outperform student averages on some subsets; however human graders provide qualitative comments and finer judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Small dataset relative to other benchmarks; grader anonymization imperfect and potential bias (lecturers can often discern LLM answers); LLMs have no time pressure and produce longer answers; multimodal evaluation limited (few image questions); grader biases (e.g., graders copying example grades, some graders tending to give full points); results may not generalize across domains or to evaluation of genuine 'scientific theories' beyond exam answers.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SciEx: Benchmarking Large Language Models on Scientific Exams with Human Expert Grading and Automatic Grading', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6011.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6011.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Expert Grading</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Human Expert Grading by Course Lecturers</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Manual scoring of each LLM-generated answer by the original exam lecturers (who designed the questions), using the same rubric and max-points as for student answers and a web UI for entering scores and comments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Direct human grading: lecturers evaluate anonymized LLM answers question-by-question, assign numeric scores (0..max), provide qualitative comments, and supply reference answers, average student performance, and difficulty labels for each question.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Per-question numeric scores up to lecturer-defined maxima, normalization to percentage, aggregation to exam-level sums, mapping to German grade scale (1.0 best, 4.0 passing), and qualitative error analysis by graders.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>As examinees graded: Claude, GPT-4V, GPT-3.5, Mixtral, Qwen, Mistral, Llava (Llama3 not used as examinee in main experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>theory_domain</strong></td>
                            <td>Computer Science (exam topics listed in SciEx: NLP, AI2, DLNN, DL4CV2, HCI, DBS, TGI, CG, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>LLM-produced exam solutions, proofs, algorithm descriptions, or figure descriptions — treated as student answers and judged for correctness, completeness, and adherence to instructions.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Used as gold-standard labels for all analyses. Expert grades show best LLM (Claude) at 59.4% average; experts provided comments noting systematic LLM behaviors (lengthy answers, occasional hallucinations, math/reasoning failures, modality limitations).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>Applied to SciEx exam questions (the benchmark described in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human</strong></td>
                            <td>Expert grades are the baseline for comparing LLM-as-examinee performance and automatic graders; they enable direct comparison to student averages and qualitative error analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Grader bias risks (incomplete anonymization, distinguishable LLM answers), non-scalability of expert grading, and potential mismatch between course-tailored student preparation and LLM training coverage.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SciEx: Benchmarking Large Language Models on Scientific Exams with Human Expert Grading and Automatic Grading', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6011.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6011.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-as-a-judge</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Automatic Grading with LLM-as-a-judge</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Using strong LLMs to assign numeric grades to other LLM-generated answers by prompting the grader LLM with the question, the answer, maximum score, reference answer, chain-of-thought requirement, and few-shot example(s).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Prompt-based automatic grading: graders (GPT-4V, Llama3, Mixtral) receive tuples (question, answer, max score) plus optional reference answers and few-shot examples; graders produce chain-of-thought reasoning and a final numeric grade (0..max). Three example-selection schemes tested: same-question (different examinee), same-exam (different question), different-exam (different exam).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Primary metric: Pearson correlation between automatic grader scores (normalized) and expert grades at exam-level and question-level. Secondary metric: RMSE on original score scales. Other diagnostics: precision on assigning full points, frequency of copying example grades, sensitivity to inclusion of reference answers and number/type of shots.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>Graders evaluated: GPT-4V, Llama3, Mixtral (also experiments with Llava where applicable).</td>
                        </tr>
                        <tr>
                            <td><strong>theory_domain</strong></td>
                            <td>Computer Science exam answers (same as SciEx); evaluates LLM-produced scientific-style answers rather than formal scientific theories.</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>The 'theory' in this context is the automatic judgment (numerical grade and chain-of-thought rationale) produced by a grader LLM for a candidate LLM answer to a scientific exam question.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Exam-level: GPT-4V grader achieved Pearson correlation 0.948 to expert grading (best), Llama3 reached 0.883. Question-level: top correlations around 0.7. Including reference answers and few-shot examples generally increased correlation. Observed grader-specific failure modes: Mixtral overassigns full points (67.6% full points zero-shot), graders sometimes copy example grades (Mixtral/GPT-4V copy grade ~25% vs Llama3 13%), and grader performance varies by examinee and difficulty level (GPT-4V better on hard questions).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>Applied to SciEx; grading prompts included few-shot exemplars sampled via same-question / same-exam / different-exam strategies and optionally included reference answers from instructors.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human</strong></td>
                            <td>Automatic LLM graders show high exam-level agreement with human experts (Pearson up to 0.948), lower but meaningful question-level agreement (~0.7). However, automatic graders exhibit systematic biases distinct from human graders (e.g., full-point bias, example-grade copying) and can be inconsistent when grader is weaker than examinee.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Grader biases (full-point tendency, grade-copying), sensitivity to prompt design and shot selection, limited multimodal capability for some graders (text-only graders excluded images), small number of image questions may limit generalization to vision-language grading, and risk that grader prefers its own outputs (self-favoring) without references.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SciEx: Benchmarking Large Language Models on Scientific Exams with Human Expert Grading and Automatic Grading', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6011.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6011.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prompting & Judging Techniques</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prompt Engineering Techniques for Automatic Grading (Chain-of-Thought, Few-Shot, Reference Answers, Shot Selection)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A set of prompt-engineering methods applied to LLM-as-a-judge: requiring chain-of-thought, supplying reference answers, and using few-shot exemplars chosen either from the same question, same exam, or different exam.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Chain-of-thought prompting elicits reasoning in large language models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Prompts asked graders to: (a) provide chain-of-thought rationale before the final grade (CoT), (b) include one or more few-shot grading examples (0/1/2 shots), and (c) optionally include instructor reference answers. Examples were selected by three strategies: same-question (different examinee), same-exam (different question), different-exam (different course).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Effect on grader quality measured by Pearson correlation to expert grades (exam-level and question-level), RMSE, precision on full-point assignment, and qualitative detection of copying behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>Used with grader LLMs GPT-4V, Llama3, Mixtral.</td>
                        </tr>
                        <tr>
                            <td><strong>theory_domain</strong></td>
                            <td>Applied to grading of CS exam answers (SciEx); methods are generalizable to other freeform answer evaluation tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>Technique set intended to improve grader LLM alignment to human scoring by guiding reasoning (CoT), providing ground-truth exemplars (few-shot), and clarifying desired outputs with reference answers.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Including reference answers and few-shot examples generally improved question-level Pearson correlations for graders; GPT-4V benefited most and became strongest grader. Single-shot examples from the same question could cause copying of the example's grade (worse performance); using more shots or including references reduced this copying effect. Chain-of-thought was requested in prompts (cited as per Wei et al., 2022) to elicit rationale.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>Technique evaluated on SciEx; few-shot examples were sampled from SciEx according to the three selection strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human</strong></td>
                            <td>Prompting techniques increased alignment with human graders, but did not eliminate systematic errors; human references remain important to avoid grader self-favoring and copying artifacts.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Shot-selection tradeoffs (relevance vs risk of grade-copying), reliance on high-quality reference answers, potential for CoT to expose grader heuristics that differ from human reasoning, and sensitivity to grader-examinee asymmetry (grader weaker/stronger than examinee).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SciEx: Benchmarking Large Language Models on Scientific Exams with Human Expert Grading and Automatic Grading', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Can large language models be an alternative to human evaluations? <em>(Rating: 2)</em></li>
                <li>A closer look into using large language models for automatic evaluation <em>(Rating: 2)</em></li>
                <li>Judging llm-as-a-judge with mt-bench and chatbot arena <em>(Rating: 2)</em></li>
                <li>Mllmas-a-judge: Assessing multimodal llm-as-a-judge with vision-language benchmark <em>(Rating: 2)</em></li>
                <li>Chain-of-thought prompting elicits reasoning in large language models. <em>(Rating: 2)</em></li>
                <li>ARC-DA: Direct-answer dataset for science QA (ARC-DA) <em>(Rating: 1)</em></li>
                <li>Qasper: A dataset of information-seeking questions and answers anchored in research papers <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6011",
    "paper_id": "paper-270559604",
    "extraction_schema_id": "extraction-schema-121",
    "extracted_data": [
        {
            "name_short": "SciEx",
            "name_full": "SciEx (Scientific Exams Benchmark)",
            "brief_description": "A freeform, multimodal, multilingual benchmark of university-level computer science exam questions (converted to JSON) with LLM-generated answers, lecturer expert grading, and automatic LLM-based grading for evaluating LLM performance on scientific tasks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method": "Benchmark evaluation of LLM outputs on university computer-science exam questions using (1) human expert grading by the course lecturers and (2) automatic grading via LLM-as-a-judge; experiments run per-question and per-exam with normalization to percent-scores and mapping to the German grade scale.",
            "evaluation_criteria": "Numeric scores per question (0..max) normalized to 0–100%; German grade conversion (1.0–5.0 with 4.0 passing); exam-level (sum) and question-level analyses; Pearson correlation between automatic graders and expert grades; RMSE on original score scales; ancillary analyses by language, modality (image vs text), and difficulty.",
            "llm_model_name": "As examinees: Claude, GPT-4V, GPT-3.5 (gpt-3.5-turbo-0125), Mixtral, Qwen, Mistral, Llava, (Llama3 held-out). As graders: GPT-4V, Llama3, Mixtral.",
            "theory_domain": "Computer Science (university exam topics spanning NLP, AI, deep learning, CV, HCI, databases, etc.)",
            "theory_description": "Not a single scientific theory — the 'theories' are the freeform LLM-generated solutions, proofs, explanations, algorithms, and figure-descriptions produced in response to diverse CS exam questions; these outputs are judged as exam answers.",
            "evaluation_results": "Dataset: 1,120 question-answer pairs. Expert grading: best-performing examinee Claude averaged 59.4% of maximum points (German grade 2.4), GPT-4V 58.2%; student average 45.3%. Automatic grading: GPT-4V-as-grader achieved exam-level Pearson correlation 0.948 to expert grades (Llama3 0.883). Question-level top correlations around 0.7. Stronger LLMs outperform students on many hard questions; multimodal graders (GPT-4V, Claude) outperform on image questions but overall image-handling remains weaker.",
            "benchmarks_or_datasets": "SciEx dataset (this work): university CS exams from multiple courses and semesters, converted to JSON with image paths; 1120 Q-A pairs and expert-provided reference answers, difficulty labels, and average student grades.",
            "comparison_to_human": "Expert lecturer grading used as gold standard; automatic LLM graders achieve high agreement with experts at exam-level (Pearson up to 0.948) but lower at question-level (~0.7). Strong proprietary models as examinees can outperform student averages on some subsets; however human graders provide qualitative comments and finer judgments.",
            "limitations_or_challenges": "Small dataset relative to other benchmarks; grader anonymization imperfect and potential bias (lecturers can often discern LLM answers); LLMs have no time pressure and produce longer answers; multimodal evaluation limited (few image questions); grader biases (e.g., graders copying example grades, some graders tending to give full points); results may not generalize across domains or to evaluation of genuine 'scientific theories' beyond exam answers.",
            "uuid": "e6011.0",
            "source_info": {
                "paper_title": "SciEx: Benchmarking Large Language Models on Scientific Exams with Human Expert Grading and Automatic Grading",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Expert Grading",
            "name_full": "Human Expert Grading by Course Lecturers",
            "brief_description": "Manual scoring of each LLM-generated answer by the original exam lecturers (who designed the questions), using the same rubric and max-points as for student answers and a web UI for entering scores and comments.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method": "Direct human grading: lecturers evaluate anonymized LLM answers question-by-question, assign numeric scores (0..max), provide qualitative comments, and supply reference answers, average student performance, and difficulty labels for each question.",
            "evaluation_criteria": "Per-question numeric scores up to lecturer-defined maxima, normalization to percentage, aggregation to exam-level sums, mapping to German grade scale (1.0 best, 4.0 passing), and qualitative error analysis by graders.",
            "llm_model_name": "As examinees graded: Claude, GPT-4V, GPT-3.5, Mixtral, Qwen, Mistral, Llava (Llama3 not used as examinee in main experiments).",
            "theory_domain": "Computer Science (exam topics listed in SciEx: NLP, AI2, DLNN, DL4CV2, HCI, DBS, TGI, CG, etc.)",
            "theory_description": "LLM-produced exam solutions, proofs, algorithm descriptions, or figure descriptions — treated as student answers and judged for correctness, completeness, and adherence to instructions.",
            "evaluation_results": "Used as gold-standard labels for all analyses. Expert grades show best LLM (Claude) at 59.4% average; experts provided comments noting systematic LLM behaviors (lengthy answers, occasional hallucinations, math/reasoning failures, modality limitations).",
            "benchmarks_or_datasets": "Applied to SciEx exam questions (the benchmark described in this paper).",
            "comparison_to_human": "Expert grades are the baseline for comparing LLM-as-examinee performance and automatic graders; they enable direct comparison to student averages and qualitative error analysis.",
            "limitations_or_challenges": "Grader bias risks (incomplete anonymization, distinguishable LLM answers), non-scalability of expert grading, and potential mismatch between course-tailored student preparation and LLM training coverage.",
            "uuid": "e6011.1",
            "source_info": {
                "paper_title": "SciEx: Benchmarking Large Language Models on Scientific Exams with Human Expert Grading and Automatic Grading",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "LLM-as-a-judge",
            "name_full": "Automatic Grading with LLM-as-a-judge",
            "brief_description": "Using strong LLMs to assign numeric grades to other LLM-generated answers by prompting the grader LLM with the question, the answer, maximum score, reference answer, chain-of-thought requirement, and few-shot example(s).",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method": "Prompt-based automatic grading: graders (GPT-4V, Llama3, Mixtral) receive tuples (question, answer, max score) plus optional reference answers and few-shot examples; graders produce chain-of-thought reasoning and a final numeric grade (0..max). Three example-selection schemes tested: same-question (different examinee), same-exam (different question), different-exam (different exam).",
            "evaluation_criteria": "Primary metric: Pearson correlation between automatic grader scores (normalized) and expert grades at exam-level and question-level. Secondary metric: RMSE on original score scales. Other diagnostics: precision on assigning full points, frequency of copying example grades, sensitivity to inclusion of reference answers and number/type of shots.",
            "llm_model_name": "Graders evaluated: GPT-4V, Llama3, Mixtral (also experiments with Llava where applicable).",
            "theory_domain": "Computer Science exam answers (same as SciEx); evaluates LLM-produced scientific-style answers rather than formal scientific theories.",
            "theory_description": "The 'theory' in this context is the automatic judgment (numerical grade and chain-of-thought rationale) produced by a grader LLM for a candidate LLM answer to a scientific exam question.",
            "evaluation_results": "Exam-level: GPT-4V grader achieved Pearson correlation 0.948 to expert grading (best), Llama3 reached 0.883. Question-level: top correlations around 0.7. Including reference answers and few-shot examples generally increased correlation. Observed grader-specific failure modes: Mixtral overassigns full points (67.6% full points zero-shot), graders sometimes copy example grades (Mixtral/GPT-4V copy grade ~25% vs Llama3 13%), and grader performance varies by examinee and difficulty level (GPT-4V better on hard questions).",
            "benchmarks_or_datasets": "Applied to SciEx; grading prompts included few-shot exemplars sampled via same-question / same-exam / different-exam strategies and optionally included reference answers from instructors.",
            "comparison_to_human": "Automatic LLM graders show high exam-level agreement with human experts (Pearson up to 0.948), lower but meaningful question-level agreement (~0.7). However, automatic graders exhibit systematic biases distinct from human graders (e.g., full-point bias, example-grade copying) and can be inconsistent when grader is weaker than examinee.",
            "limitations_or_challenges": "Grader biases (full-point tendency, grade-copying), sensitivity to prompt design and shot selection, limited multimodal capability for some graders (text-only graders excluded images), small number of image questions may limit generalization to vision-language grading, and risk that grader prefers its own outputs (self-favoring) without references.",
            "uuid": "e6011.2",
            "source_info": {
                "paper_title": "SciEx: Benchmarking Large Language Models on Scientific Exams with Human Expert Grading and Automatic Grading",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Prompting & Judging Techniques",
            "name_full": "Prompt Engineering Techniques for Automatic Grading (Chain-of-Thought, Few-Shot, Reference Answers, Shot Selection)",
            "brief_description": "A set of prompt-engineering methods applied to LLM-as-a-judge: requiring chain-of-thought, supplying reference answers, and using few-shot exemplars chosen either from the same question, same exam, or different exam.",
            "citation_title": "Chain-of-thought prompting elicits reasoning in large language models.",
            "mention_or_use": "use",
            "evaluation_method": "Prompts asked graders to: (a) provide chain-of-thought rationale before the final grade (CoT), (b) include one or more few-shot grading examples (0/1/2 shots), and (c) optionally include instructor reference answers. Examples were selected by three strategies: same-question (different examinee), same-exam (different question), different-exam (different course).",
            "evaluation_criteria": "Effect on grader quality measured by Pearson correlation to expert grades (exam-level and question-level), RMSE, precision on full-point assignment, and qualitative detection of copying behavior.",
            "llm_model_name": "Used with grader LLMs GPT-4V, Llama3, Mixtral.",
            "theory_domain": "Applied to grading of CS exam answers (SciEx); methods are generalizable to other freeform answer evaluation tasks.",
            "theory_description": "Technique set intended to improve grader LLM alignment to human scoring by guiding reasoning (CoT), providing ground-truth exemplars (few-shot), and clarifying desired outputs with reference answers.",
            "evaluation_results": "Including reference answers and few-shot examples generally improved question-level Pearson correlations for graders; GPT-4V benefited most and became strongest grader. Single-shot examples from the same question could cause copying of the example's grade (worse performance); using more shots or including references reduced this copying effect. Chain-of-thought was requested in prompts (cited as per Wei et al., 2022) to elicit rationale.",
            "benchmarks_or_datasets": "Technique evaluated on SciEx; few-shot examples were sampled from SciEx according to the three selection strategies.",
            "comparison_to_human": "Prompting techniques increased alignment with human graders, but did not eliminate systematic errors; human references remain important to avoid grader self-favoring and copying artifacts.",
            "limitations_or_challenges": "Shot-selection tradeoffs (relevance vs risk of grade-copying), reliance on high-quality reference answers, potential for CoT to expose grader heuristics that differ from human reasoning, and sensitivity to grader-examinee asymmetry (grader weaker/stronger than examinee).",
            "uuid": "e6011.3",
            "source_info": {
                "paper_title": "SciEx: Benchmarking Large Language Models on Scientific Exams with Human Expert Grading and Automatic Grading",
                "publication_date_yy_mm": "2024-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Can large language models be an alternative to human evaluations?",
            "rating": 2,
            "sanitized_title": "can_large_language_models_be_an_alternative_to_human_evaluations"
        },
        {
            "paper_title": "A closer look into using large language models for automatic evaluation",
            "rating": 2,
            "sanitized_title": "a_closer_look_into_using_large_language_models_for_automatic_evaluation"
        },
        {
            "paper_title": "Judging llm-as-a-judge with mt-bench and chatbot arena",
            "rating": 2,
            "sanitized_title": "judging_llmasajudge_with_mtbench_and_chatbot_arena"
        },
        {
            "paper_title": "Mllmas-a-judge: Assessing multimodal llm-as-a-judge with vision-language benchmark",
            "rating": 2,
            "sanitized_title": "mllmasajudge_assessing_multimodal_llmasajudge_with_visionlanguage_benchmark"
        },
        {
            "paper_title": "Chain-of-thought prompting elicits reasoning in large language models.",
            "rating": 2,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "ARC-DA: Direct-answer dataset for science QA (ARC-DA)",
            "rating": 1,
            "sanitized_title": "arcda_directanswer_dataset_for_science_qa_arcda"
        },
        {
            "paper_title": "Qasper: A dataset of information-seeking questions and answers anchored in research papers",
            "rating": 1,
            "sanitized_title": "qasper_a_dataset_of_informationseeking_questions_and_answers_anchored_in_research_papers"
        }
    ],
    "cost": 0.012964999999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>SciEx: Benchmarking Large Language Models on Scientific Exams with Human Expert Grading and Automatic Grading</p>
<p>Tu Anh Dinh 
Karlsruhe Institute of Technology
KarlsruheGermany</p>
<p>Carlos Mullov 
Karlsruhe Institute of Technology
KarlsruheGermany</p>
<p>Leonard Bärmann 
Karlsruhe Institute of Technology
KarlsruheGermany</p>
<p>Zhaolin Li 
Karlsruhe Institute of Technology
KarlsruheGermany</p>
<p>Danni Liu 
Karlsruhe Institute of Technology
KarlsruheGermany</p>
<p>Simon Reiß 
Karlsruhe Institute of Technology
KarlsruheGermany</p>
<p>Jueun Lee 
Karlsruhe Institute of Technology
KarlsruheGermany</p>
<p>Nathan Lerzer 
Karlsruhe Institute of Technology
KarlsruheGermany</p>
<p>Jianfeng Gao 
Karlsruhe Institute of Technology
KarlsruheGermany</p>
<p>Fabian Ternava 
Karlsruhe Institute of Technology
KarlsruheGermany</p>
<p>Tobias Röddiger 
Karlsruhe Institute of Technology
KarlsruheGermany</p>
<p>Alexander Waibel 
Karlsruhe Institute of Technology
KarlsruheGermany</p>
<p>Tamim Asfour 
Karlsruhe Institute of Technology
KarlsruheGermany</p>
<p>Michael Beigl 
Karlsruhe Institute of Technology
KarlsruheGermany</p>
<p>Rainer Stiefelhagen 
Karlsruhe Institute of Technology
KarlsruheGermany</p>
<p>Carsten Dachsbacher 
Karlsruhe Institute of Technology
KarlsruheGermany</p>
<p>Klemens Böhm 
Karlsruhe Institute of Technology
KarlsruheGermany</p>
<p>Jan Niehues 
Karlsruhe Institute of Technology
KarlsruheGermany</p>
<p>SciEx: Benchmarking Large Language Models on Scientific Exams with Human Expert Grading and Automatic Grading
EC7343A5BA659673453060C8AE6A9918
With the rapid development of Large Language Models (LLMs), it is crucial to have benchmarks which can evaluate the ability of LLMs on different domains.One common use of LLMs is performing tasks on scientific topics, such as writing algorithms, querying databases or giving mathematical proofs.Inspired by the way university students are evaluated on such tasks, in this paper, we propose SciEx -a benchmark consisting of university computer science exam questions, to evaluate LLMs' ability on solving scientific tasks.SciEx is (1) multilingual, containing both English and German exams, and (2) multi-modal, containing questions that involve images, and (3) contains various types of freeform questions with different difficulty levels, due to the nature of university exams.We evaluate the performance of various state-of-the-art LLMs on our new benchmark.Since SciEx questions are freeform, it is not straightforward to evaluate LLM performance.Therefore, we provide human expert grading of the LLM outputs on SciEx.We show that the free-form exams in SciEx remain challenging for the current LLMs, where the best LLM only achieves 59.4% exam grade on average.We also provide detailed comparisons between LLM performance and student performance on SciEx.To enable future evaluation of new LLMs, we propose using LLM-as-ajudge to grade the LLM answers on SciEx.Our experiments show that, although they do not perform perfectly on solving the exams, LLMs are decent as graders, achieving 0.948 Pearson correlation with expert grading.</p>
<p>Introduction</p>
<p>In recent years, Large Language Models (LLMs) have proven their usefulness across a wide range of tasks, from conversational agents to code generation (Rajkumar et al., 2022;Abbasian et al., 2023;Liao et al., 2023).Given the fast pace of development in the field, with an increasing number of LLMs being trained and released, it is important to have indicators of LLM performance on different domains.This can be achieved by establishing evaluation benchmarks that assess the capabilities of LLMs across diverse use cases.</p>
<p>One use case of LLMs is to handle scientific tasks.Some previous works have introduced benchmarks containing questions on science topics (Welbl et al., 2017;Lu et al., 2022;Gilson et al., 2022;Schubert et al., 2023;Zhang et al., 2024).However, these benchmarks are limited to multiple-choice questions.This restricts the variability of questions, such as instruction-follow ones like "write a mathematical proof for this statement ...".Additionally, it is difficult to ask certain types of questions in a multiple-choice way without including the answer in the question itself.Multiplechoice benchmarks therefore create a gap between testing and actual usage, since they only evaluate whether the LLMs choose the correct answer, whereas in real life, the users are more likely to ask open-ended questions to the LLMs.In contrast, some other works have introduced freeform question benchmarks.These works either convert multiple-choice questions to freeform questions (Bhakthavatsalam et al., 2021), or focus on a specific type of problem such as answering questions related to a paper (Dasigi et al., 2021), thus still limiting the variability of the questions.</p>
<p>In this paper, we introduce a new benchmark, termed SciEx (Scientific Exams), designed to evaluate this capability.Inspired by the way students are evaluated in university, we created the benchmark by evaluating the performance of LLMs on university computer science exams.SciEx's questions are in various formats, from multiple choice to open-ended, thus making it suitable to evaluate LLM's capabilities of generating free-text answers that fit the requirements of the questions.It is multilingual, containing exams in both German and English.It is multimodal, as exam questions can also contain figures.The set of questions is a good mix of different difficulty levels since they are designed for university exams.This enables us to evaluate LLMs on different levels, and we found that stronger LLMs tend to perform better on more difficult questions.</p>
<p>Unlike the previous multiple-choice benchmarks, the questions in SciEx are freeform, making it nontrivial how to evaluate the LLM output.Therefore, we make use of expert grading, i.e., having the lecturers grade the LLM output the same way they would grade student answers.We also ask the experts to perform qualitative analysis of the LLM output.With expert grading, we provide a highly reliable way of evaluating LLMs, which is more reliable than previous work that uses crowdsourced evaluation.Expert grading by lecturers also provides an opportunity to compare LLMs' performance to university student performance in a similar setting.We find that the stronger LLMs, i.e., Claude and GPT-4V, are able to outperform the student average.However, they are still far from perfect, achieving only 59% across SciEx exams.</p>
<p>Since new LLMs are constantly being released, we cannot fully rely on expert grading for evaluation.Therefore, we provide an automatic grading scheme by using LLM as a judge so that future LLMs can also be evaluated on SciEx.Interestingly, we find that, although LLMs do not perform too well as examinees, they perform well as graders, achieving over 0.948 Pearson correlation to expert grading in the best setting.</p>
<p>In summary, our contributions are as follows:</p>
<p>• SciEx1 -a freeform, multimodal, multilingual benchmark consisting of university computer science exams, outputs of various LLMs on the exams, and expert grading of the LLM output.</p>
<p>• Detailed quantitative and qualitative analysis comparing LLM to student performance.</p>
<p>• Automatic grading with 0.948 Pearson correlation to expert grading</p>
<p>Related Work</p>
<p>General-Purpose LLM Benchmarks In order to rank different LLMs, there are several commonly used public benchmarks.For example, Zheng et al.</p>
<p>(2024) introduced MT-bench and Chatbot Arena.MT-bench is a multi-turn question set; and Chatbot Arena is a crowdsourced battle platform for LLMs where the users can ask their questions and vote for the better LLM answer.Another benchmark is MMLU (Hendrycks et al., 2020), which is a multitask dataset covering multiple domains such as mathematics, US history and law.</p>
<p>Scientific LLM Benchmarks</p>
<p>To specifically focus on the scientific domain, previous studies have established benchmarks, such as SciQ (Welbl et al., 2017) and ScienceQA (Lu et al., 2022), which feature questions spanning various scientific subjects.</p>
<p>More recent works have focused on benchmarking LLMs on solving exam questions on some narrow science domains such as medical (Gilson et al., 2022) or neurology (Schubert et al., 2023).M3Exam (Zhang et al., 2024), in contrast, provides exam questions to benchmark LLMs which span over multiple topics and multiple educational levels (primary, middle, and high school).However, all benchmarks mentioned above are limited to multiple-choice questions.While this simplifies the evaluation process, it does not allow us to assess the LLMs' capability to generate natural text.</p>
<p>Other studies have instead provided scientific benchmarks with open-ended questions.Some examples are Qasper (Dasigi et al., 2021) and ARC-DA (Bhakthavatsalam et al., 2021).However, Qasper only focuses on questions about NLP papers rather than on general computer science topics.ARC-DA is closer to our work, since it contains open-ended questions taken from science exams and quiz sources.However, these are created by converting questions that were originally multiple-choice, thus not covering certain types of typical freeform questions (e.g.those that require mathematical proofs, or long explanations).</p>
<p>Different from these works, SciEx is created from university computer science exams, thus naturally providing diversity in the types of questions as well as having freeform format.</p>
<p>Freeform Answer Evaluation Compared to benchmarks with multiple-choice questions, evaluating LLMs' performance on freeform questions is not straightforward.Similar to evaluation conditions in tasks such as machine translation or summarization, there are multiple correct answers, or multiple ways to express a correct answer for a single input.Therefore, it is insufficient to evaluate a model's output by comparing it to a gold standard answer.Ideally, in these cases, we can evaluate by human judgment.For example, the ARC-DA benchmark (Bhakthavatsalam et al., 2021) uses a crowdscoring pipeline for evaluation.Chatbot Arena (Zheng et al., 2024) also uses crowdsourcing, where the users vote between pairs of LLM output.However, human evaluation is inherently non-scalable.Therefore, previous works have used automated metrics.Some traditional metrics such as BLEU (Papineni et al., 2002) or ROUGE (Lin, 2004) compare the model's output to some gold-standard answer on the surface level, i.e., word matching.More advanced metrics, such as s BERTScore (Zhang* et al., 2020), BLEURT (Sellam et al., 2020), and BARTScore (Yuan et al., 2021), are model-based, thus being able to evaluate answers on the semantic level.</p>
<p>One recent approach is to use LLMs for evaluation, termed "LLM-as-a-judge".Liu et al. (2023); Chiang and Lee (2023a); Zheng et al. (2024) find that, although still prone to biases, LLM-as-ajudge for textual modality has high agreement with human scoring when a strong judge LLM is used.However, when including images, Chen et al. (2024) find that the performance of LLMas-a-judge is no longer as well correlated to human judgment.Nevertheless, LLM-as-a-judge is a promising way to perform scalable evaluation.</p>
<p>In our work, we make use of LLM-as-a-judge for automatic grading of LLM answers on SciEx exams, and find that they have good correlation to human expert grading on both text-only and imagerelated questions.</p>
<p>The SciEx Benchmark</p>
<p>The components of SciEx are as follows.</p>
<p>Univeristy Exams SciEx contains university computer science exams in a unified JSON format.The exams are taken from the following computer science courses at the Karlsruhe Institute of Technology from the 2022/2023/2024 semesters:</p>
<p>• Natural Language Processing (NLP) LLM-Generated Answers SciEx contains answers produced by 7 LLMs on the exam questions.The details of the LLMs are shown in Table 2.In Table 2, only Llama3 was not used to solve the exam, since it was released at a later point of conducting this paper.In total, we obtained 1120 question-answer pairs.Expert Grading and Automatic Grading Each question-answer pair is assigned a score by an expert.In order to guide future work to evaluate new LLMs on SciEx without relying on human expert grading, we also provide automatic grading generated by Mixtral, Llama3 and GPT4V.</p>
<p>Data Creation</p>
<p>The data creation process is as follows.</p>
<p>Exam Collection We collect university exams from different courses.We additionally ask the lecturers to provide us with the reference answers, the difficulty level of each question, and the average student grades on each question.</p>
<p>Exam Formatting</p>
<p>We convert every exam into a unified JSON format.Each exam includes a list Full name</p>
<h1>Params Quant. Handle Image Proprietary Claude Claude-3-opus-20240229 - - yes GPT-4v gpt-4-vision-preview - - yes GPT-3.5 gpt-3.5-turbo-0125 - - no Open source Llama3 MaziyarPanahi/Meta-Llama-3-70B-Instruct-GGUF 70B 4 bit no Mixtral Mistralai/Mixtral-8x7B-Instruct-v0.1 8x7B 5 bit no Qwen Qwen/Qwen-72B 72B 2 bit no Mistral Mistralai/Mistral-7B-Instruct-v0.2 7B - no Llava Llava-hf/Llava-v1.6-Mistral-7b-hf 7B - yes</h1>
<p>Table 2: Details of the LLMs in consideration.</p>
<p>of questions, where each question includes an index, its content, and potentially path to any related images.An example is shown in Appendix B.</p>
<p>LLM-Generated Answers</p>
<p>We pass the exams to the LLMs listed in Table 2 (except Llama3 due to later release), one question at a time.Questions that contain images are handled differently depending on the LLM.For the text-only LLMs, we exclude the images and only pass the question text to the models.For Llava, since it is trained to handle only 1 image at a time, we concatenate the images into one, with blank padding around the images as separators before feeding it to the model.Claude and GPT-4V can take multiple images, however, there is no pre-defined way of referencing the image within the text.In our work, we reference the image by mentioning the image caption within the question text, and add the text caption to the image.</p>
<p>Since the considered LLMs can only output text, for questions asking to draw on images, we ask the LLMs to describe in text what should be drawn.</p>
<p>The detailed prompts for LLMs to generate the answers are shown in Appendix C.</p>
<p>Expert Grading</p>
<p>We then give the LLM answers back to the lecturers, who proceed with grading the LLM output the same way they would grade student answers.We anonymized the LLMs' names in order to avoid bias during exam grading.We also build a user interface for collecting the grades (see Appendix E for more details).</p>
<p>With expert grading, the evaluation of the LLM output is highly reliable.Most importantly, the expert graders are generally the ones who designed the exam questions.We additionally ask the expert graders to provide their comments on the LLM output to further understand LLMs' behaviors when solving the exams.</p>
<p>Automatic Grading</p>
<p>In addition to expert grading, we also provide automatic grading using LLM-as-a-judge, so that we can evaluate future LLMs on SciEx.We use the stronger models, i.e., Mixtral, Llama3 and GPT-4V, to conduct the grading.Given a tuple containing question, answer, and maximum score, we ask the LLMs to output a single score between 0 and the maximum.We include reference answers to the grading prompt.We ask the LLMs to provide chainof-thought reasoning (Wei et al., 2022;Chiang and Lee, 2023b) before giving the grade.We also include examples for grading in the prompt, so-called few-shot judge (Zheng et al., 2024).Each example is a tuple consisting of a question, an answer, and the expert-provided grade.We try out different settings to select the examples, as described below.</p>
<p>Let's say we want to grade Question M from Exam A, answered by Examinee X.Then the shot examples can be chosen in one of the three ways: Examinee Y and Question N are chosen randomly.</p>
<p>For Exam B, we opt to select the exams that do not heavily require images for simplicity in the prompt.</p>
<p>Intuitively, the example-selection settings above have decreasing levels of relevance to the actual grading query, but increasing easiness to collect.The detailed prompts for LLMs to grade answers are shown in Appendix D.</p>
<p>Experiments</p>
<p>In this section, we describe our experiments and results.For prompting the proprietary LLMs, we use their APIs, namely OpenAI2 and Anthropic3 .For the open-source models, we obtain model checkpoints from the Huggingface4 model hub.We perform inference with the LLMs using llama.cpp5with the default sampling strategy.The experiments with open-sourced models are conducted on an NVIDIA RTX A6000 GPU with 48GB VRAM.</p>
<p>For our analysis, we consider the exam-level and question-level grades.An exam-level grade is the sum of the grades of all questions in the exam.</p>
<p>Quantitative Analysis</p>
<p>We analyze the performance of the LLMs on SciEx with expert grading.For both exam level and question level, we normalize the grade to be between 0 and 100%, since they have different scales.The normalization is done by taking the scores obtained by the examinee divided by the maximum score possible per exam/question, where the maximum scores possible are predefined by the lecturers.</p>
<p>We also report on the German grade scale.In the German scale, the grades range from 1.0 to 5.0, where 1.0 is the highest grade and 4.0 is the passing threshold.The detailed mapping from the scores to the German grade scale is defined by the lectures, adjusted based on the overall performance of the students taking the exams.</p>
<p>We compare the performance of the LLMs to students from different aspects: language, difficulty level, and modality, i.e., questions with or without images.</p>
<p>General Observations</p>
<p>SciEx is Challenging The performance of the LLMs on SciEx provided by expert grading is shown in Table 3.The bigger-sized LLMs (Claude, GPT-4V, GPT-3.5, Mixtral and Qwen) can achieve exam passing grades (i.e., grades that are better than 4.0 in the German scale).However, the bestperforming model (Claude) only achieves 59.4% of the maximum points, which is far from perfect.</p>
<p>Compared to the student average, most LLMs have worse performance.Only the strongest proprietary LLMs, i.e., Claude and GPT-4V, can achieve grades that are better than the students'.</p>
<p>Influential Factors</p>
<p>Difficulty Levels Figure 1a shows the influence of the difficulty level on the examinee grades.As can be seen, the student performance aligns with the difficulty level of the questions: they perform better on easier questions.Some weaker LLMs, e.g., Mixtral, Qwen, GPT-3.5, Llava, align with the students.However, the stronger LLMs, i.e., Claude and GPT-4V, perform better on harder questions.This is an indication that difficulty levels from human perspective do not always align with LLMs' perspective.This is also confirmed by looking at the Pearson correlations between the LLMs' grades and the student average grades on the question level.These correlations are between 0.4 and  0.6, indicating that LLM grades and the student grades are not highly correlated.One possible explanation for the mismatch between LLMs performance and question difficulty level could be that, in some exams, there can be some "template questions", i.e., questions that are repeated over the years, where students can just learn by heart how to systematically solve them.While this would be marked as "easy" by the lecturer, it might not be as easy for the LLMs, since the LLMs are not previously exposed to these "template questions".Another potential explanation is that math-type easy questions are hard for the LLMs, while long-text hard questions are easy for them.</p>
<p>In Figure 1b, we plot the difference between LLM scores and student scores.The stronger LLMs, i.e., Claude and GPT-4V, outperform the students the most on hard questions.Weaker LLMs, on the other hand, generally fall behind students the most on hard questions.Looking at each difficulty level independently, we observe that the ranking of the LLMs changes across different levels.This aligns with the findings made by Li et al. (2024), where they show that the LLM rankings change on a subset of evaluation prompts that are artificially labeled as hard.ence between LLM and student scores.Recall that for the text-only LLMs, we exclude the images and only pass the question text to the models.Trivially, the text-only LLMs perform poorly on the imagerelated questions.The strong, multi-modal LLMs, i.e., Claude and GPT4, outperform the students on both image-related questions and text-only questions, but the performance gap is still larger for text-only questions.Llava, although can handle images, still falls behind student performance by a large margin on image-related questions.This shows that LLMs' image-handling capability is still not as advanced as for text.</p>
<p>Text-only versus Image-related Questions</p>
<p>Language Figure 3 shows the influence of languages on the difference between LLM scores and student scores.When the questions are in English, all LLMs, except for GPT-3.5, outperform the student average.However, for German, either the LLMs outperform students by a smaller gap, or fall behind student performance.It can be concluded that LLMs are still superior in English than other languages like German, although German can be considered a high-resource language.Since some models are not made to deal with images, or with languages other than English, we additionally analyze LLMs' performance on text-only and English-only questions.On this subset of questions, the grades obtained by the models are generally better, and more models would outperform the student average.More details can be found in Appendix F.</p>
<p>Qualitative Analysis</p>
<p>In this section, we summarize the observations made by the graders while grading the LLMs.</p>
<p>General Behaviours</p>
<p>The graders observed some common behaviors made by the LLMs.Some solutions of the LLMs were good language-wise but low-quality content-wise.For students, good language usually correlates strongly with good content.The LLMs tend to output lengthy answers, since, unlike the students, they do not have a time constraint.Some LLMs even ignore when the question specifies that they should "answer briefly".There are also some failure cases, although not frequent:</p>
<p>(1) Claude refuses to answer the question with "I apologize, but I do not feel comfortable providing answers related to ..." or (2) some LLMs get stuck in decoding loops.Sometimes, instead of answering the question, LLMs give some text that is (or seems) related to the task; rephrase the task; or describe how a task of this nature may be approached in general.</p>
<p>Knowledge-type Questions On some exams such as AI2, DL4CV2, DLNN, CG, questions which students can answer by learning the lecture content by heart are quite easy for the LLM.For the DL4CV2 exam, very specific questions about neural network architectures which are covered in our lecture seem to be quite common knowledge in the LLMs, which might be due to those papers being included in the training data.However, for other exams such as HCI, the models lacked specific course context, which was important for answering many theoretical and open-ended questions.</p>
<p>Math-related Ability</p>
<p>The LLMs tend to fail on the math-related questions, even the basic ones.For example, they miscount the number of words in a piece of text, or have trouble comparing numbers.For questions that require writing mathematical proof in the TGI exam, all LLMs except for GPT-4V and Claude failed.For GPT-4V and Claude, they are able to pass the TGI exam.Their mistakes are more in line with those that students would make.That is, they are often not successful when making actual proof, and the points where the proof breaks sometimes are the same as the students.Even the better models handle simple geometry questions poorly and/or struggle to follow the instructions of a simple algorithm.</p>
<p>Reasoning Ability The LLMs do not perform well on questions that require deep thinking and reasoning.For questions of the type "is this statement true or false; reason for your solution", the LLMs often said "true" and then just repeated the statement or reasoned for the opposite of their claim.This is a similar behavior often seen in students.Sometimes they make self-contradicting arguments: making a statement and then providing arguments for the other side.</p>
<p>Image Handling GPT-4V, Claude and Llava can handle images.However, only GPT-4V and Claude have reasonable performance.When the question is about drawing on top of the figures, sometimes the LLMs successfully describe in words what needs to be drawn, but occasionally they just hallucinate a non-existing figure file path.</p>
<p>Automatic Grading</p>
<p>In this section, we evaluate the performance of LLM-as-a-judge approach to automatic grading.We use the expert grades as the gold standard to evaluate automatic graders.We use Pearson correlation on the normalized scores as our metric.Since the LLMs are asked to provide the scores on the same scale as the expert scores, we also provide the Root Mean Squared Error (RMSE) on the originally-scaled scores as a secondary metric.Note that RMSE would correctly put more weight on the questions that have more points, however, it is not as easily interpretable as the Pearson correlation.Therefore, we only report RMSE in Appendix G.2.The main results are discussed as follows.</p>
<p>General</p>
<p>LLMs Perform Well as Graders On the exam level, LLM-as-a-judge performs well for automatic grading.The best Pearson correlation to expert grading on the exam level, at 0.948, is achieved by GPT-4V.The open-source Llama3 achieves 0.883 Pearson correlation to expert grading.</p>
<p>The LLM ranking based on average exam-level grades provided by the GPT-4V grader in comparison to expert grading is shown in Table 5.As can be seen, the ranking is quite identical, except for Mixtral and Qwen's positions being swapped.The high correlations between expert grading and LLM-as-a-judge grading indicate that, although being far from perfect in solving SciEx exams (discussed in Section 4.1.1),the stronger LLMs are quite reliable for grading the exams.This is useful since we would have to rely less on expert grading to evaluate newly developed LLMs' performance on SciEx.The details of graders' performance under different settings on the exam level are in Appendix G.1.</p>
<p>On the question level, the performance of LLMas-a-judge is shown in Table 4.The highest Pearson correlation to expert grading achieved by the LLMs is now around 0.7, which is lower than on the exam level, but still quite high.Surprisingly, the performance of GPT-4V on grading image-related questions is quite comparable to grading text-only questions.This contradicts the finding made by Chen et al. (2024).This could potentially be due to the small number of image-related questions in SciEx, thus the results might not be generalizable.</p>
<p>Few-shot and References Help</p>
<p>The performance of the graders on the question level is shown in Table 4.We observe that adding examples (shots) and adding reference answers in the prompt generally increases the performance of the LLM graders.GPT-4V is the strongest grader, followed by Llama3 and Mixtral.This shows that proprietary LLMs are still stronger as judges, aligning with previous studies (Zheng et al., 2024).</p>
<p>Grader-specific Behaviours</p>
<p>Mixtral Grader Tends to Give Full Points As can be seen from Table 4, Mixtral has the worst performance on grading the exams.We observe that Mixtral tends to give full points to the answers.Without reference and without examples (0-shot), the portion of answers where Mixtral outputs full points is 67.6%, significantly higher than Llama3 and GPT-4V, at 19.1% and 15.1%, respectively.As a result, Mixtral's precision on giving full points, at 0.181, is much lower than Llama3 and GPT-4V, at 0.380 and 0.527 respectively.As we add more examples and/or add the reference answer to the prompt, the problem is lessened.More details can be found in Appendix G.3.</p>
<p>Mixtral and GPT-4V Copy Grade of Example</p>
<p>For Mixtral and GPT-4V graders, when having one example (shot) from the same question in the prompt without reference, the performance is worse than having the example from the same exam or from a different exam.We hypothesize that this is due to these graders tend to copy the grades of the examples when having a chunk of duplicated text (i.e., the question description) in the example.This is verified when looking at the statistics: Mixtral and GPT-4V copy the grade of the example 25% of the time, whereas Llama3 does it 13% of the time.As a result, Llama3 can best make use of examples from the same question.The problem is reduced when having more than 1 shot or when the reference answer is included.</p>
<p>Influential Factors</p>
<p>Different Examinees As can be seen in Table 6, GPT-4V grader has better performance than others, but is more inconsistent: it does worse on grading some LLMs, especially Claude.This is potentially due to Claude being a better examinee than GPT-4V itself, as shown in Section 4.1.1.When using the scores from GPT-4V grader to rank the LLMs, we find that, without reference answer, GPT-4V always ranks itself higher than Claude.This emphasizes the importance of reference answers for grading, especially when the grader is weaker than the examinee.</p>
<p>Conclusion</p>
<p>In this paper, we proposed SciEx -a benchmark consisting of scientific university exams, along with expert grading and automatic grading, to evaluate the abilities of LLMs on science topics.SciEx is multilingual, multi-modal, and contains a variety of free-form questions.Our experiments show that SciEx is still quite challenging for current LLMs, where the best LLM can only achieve 59.4% of the exam score on average.Despite that, the LLMs perform well as graders, achieving 0.948 Pearson correlation to the expert grades.This is a promising observation, since we can use strong LLMs for automatic grading of new LLM examinees on SciEx, rather than relying on expert grading.We encourage the research community as well as LLM developers and users to make use of SciEx for evaluating LLMs' scientific capabilities.</p>
<p>Limitations</p>
<p>There are certain biases that can occur for SciEx.Firstly, the LLMs do not have time pressure.Therefore, they can output longer answers, which helps them get better grades, as there is a higher likelihood that something will be correct.Secondly, the grading process can not be fully anonymized.It is not easy to mix the LLM answers with student answers for the lecturer to grade, since student answers are usually handwritten.Additionally, the LLMs' answers content itself might also be easily distinguishable from the students', since the LLMs tend to, e.g., give longer answers or repeat the questions.Therefore, the lecturers know when they are grading an LLM, thus can bias the score they give.Thirdly, the comparison between the LLMs and the students might be unfair, since the students studied the centralized course material specifically for the exams, while this is not the case for the LLMs.Lastly, due to the reliance on expert resources, the size of SciEx is quite small compared to other scientific benchmarks.</p>
<p>Ethics</p>
<p>Our work makes use of student statistics to compare against LLMs' performance.However, we only use the average of the student grades, without disclosing any individual student's information.The student answers are never directly used, as we only ask for the average graders from the lecturers.Automatic grading, regardless of the high correlation to expert grading, can still be imperfect.</p>
<p>We are not suggesting to use LLMs to evaluate students, but to evaluate new models coming out when it is not possible to do human evaluation.</p>
<p>Regarding data consent, we had group meetings and email exchanges to come to an agreement from all lecturers that the data would be made public under the CC BY-NC-SA 4.0 license.</p>
<p>A Exam Description</p>
<p>The overall description of each exam in SciEx is as follows:</p>
<ol>
<li>
<p>Natural Language Processing (NLP): exam contains questions about word and sequence representation, language modeling, and pretrained models.</p>
</li>
<li>
<p>Advanced Artificial Intelligence (AI2): exam contains questions about natural language processing, signal processing, automatic speech recognition and cognitive robotics.</p>
</li>
</ol>
<p>B Exam Formatting</p>
<p>Originally, exams were in different formats, depending on their creator.We convert the exams into JSON format, with file paths to images if any.</p>
<p>An example is shown in Figure 4.</p>
<p>C LLM Answer Generation Prompts</p>
<p>We provide the prompt in the same language as the exam question to the LLMs to generate answers.The English prompt is shown in Figure 5, and the German prompt is shown in Figure 6.</p>
<p>D LLM Grader Prompts</p>
<p>We provide the prompt in the same language as the exam question to the LLMs to perform automatic grading.The English prompt is shown in Figure 7, and the German prompt is shown in Figure 8.</p>
<p>E User Interface for Expert Grading</p>
<p>We instructed the expert grader to use our user interface (UI) for grading.Figure 9 shows the open page of the UI, where the grader can choose their exam and enter their password.Figure 10 shows the page for the grading, where the expert is shown with the question, the LLM answer to the question, and a text box to enter the grade.The expert can choose the examinee to grade from the dropdown on the left-hand side.Figure 11 shows the page to enter additional information about the exam questions, including the maximal achievable score, average student performance, gold answer, and difficulty level.</p>
<p>Once the data is collected, we also ask the experts and have their consent to make the data public.</p>
<p>F Performance on Text-only, English-only Questions</p>
<p>The performance of the LLMs on text-only, English-only questions is shown in Table 8.Qn this  subset of questions, besides GPT-4V and Claude, we can see that Mixtral and Qwen also have better performance than the student average.G Grader Performance</p>
<p>G.1 Pearson Correlation on Exam Level</p>
<p>The performance of LLM-as-a-judge for automatic grading on the exam level is shown in Table 9.Note that Mixtral and Llava3 graders have disadvantage since they cannot take image input for image-related questions.</p>
<p>G.2 RMSE on Question-level</p>
<p>Since the LLM graders are asked to output the scores in the original scale, RMSE would be the most informative metric, since it also reflects the importance of the questions that have higher maximum scores.The LLM graders' performance in RMSE is shown in Table 10.</p>
<p>G.3 Performance on Giving Full Points</p>
<p>The performance of the LLM graders on assigning full points to the answers is shown in Table 11.</p>
<p>You are a university student.Please answer the following JSON-formatted exam question.</p>
<p>The subquestions (if any) are indexed.The provided figures (if any) each contains its path at the bottom, which matches the path provided in the JSON.</p>
<p>Please give the answers to the question and subquestions that were asked, and index them accordingly in your output.You do not have to provide your output in the JSON format.If you are asked to draw on the figure, then describe with words how you would draw it.</p>
<p>Please provide all answers in English.</p>
<p>Here is the question: <input text></p>
<p>Difference between LLM scores and student scores.</p>
<p>Figure 1 :
1
Figure 1: Question-level scores grouped by difficulty.</p>
<p>Figure  2shows the influence of images on the differ-</p>
<p>Figure 2 :
2
Figure 2: Difference between LLM scores and student scores, question-level, grouped by with/without images.Only Claude, GPT-4V and Llava can handle images.</p>
<p>Figure 3 :
3
Figure 3: Difference between LLM scores and student scores, question-level, grouped by languages.</p>
<p>(a) Exam in original PDF format.(b) Exam converted to JSON format.</p>
<p>Figure 4 :
4
Figure 4: Exam question before and after being converted to JSON format.</p>
<p>Figure 5 :Figure 6 :
56
Figure 5: Answer generation prompt in English.</p>
<p>Table 1 :
1
Question-level statistics for SciEx.
• Advanced Artificial Intelligence (AI2)• Deep Learning and Neural Networks (DLNN)• Deep Learning for Computer Vision(DL4CV2)• Human-Computer Interaction (HCI)• Databases (DBS) for the years 2022 and 2023</p>
<p>•</p>
<p>Same question: Select examples from the same Question M from Exam A, but answered by a different Examinee Y.This mimics the real-life scenario where we use the expert resource to grade some answers of the same exam, then use it to guide the LLM graders.
• Same exam: select examples from a differentQuestion N from the same Exam A, Answeredby a different Examinee Y. Here the examplesare in the same domain as the question-answerpair in consideration. This mimics the real-lifescenario where, e.g., we have expert gradingon exams of the same course from previousyears to guide LLM graders.
• Different exam: select examples from a different Question N, from a different Exam B, Answered by a different Examinee Y.This mimics the real-life scenario where, e.g., we have expert grading for an exam of another course to guide the LLM graders.</p>
<p>Table 3 :
3
Average performance of LLMs, exam level.
Grade (%) ↑ German Scale ↓ProprietaryClaude59.42.4GPT-4V58.22.5GPT-3.532.83.9Open sourceMixtral41.13.5Qwen35.43.7Mistral25.94.2Llava21.54.3Student avg.45.33.1SciEx Versus Other Benchmarks The rankingof the LLMs on SciEx in Table 3 generally agreeswith other public benchmarks. However, SciExseems to be more challenging. For example, thebest LLM accuracy achieved on MMLU's varioustasks is 88.8%. The best accuracy achieved onM3Exam multiple choice questions is 72.92%. Al-though these scores are not directly comparable, itindicates that SciEx provides a more challengingtest set for future LLMs.</p>
<p>Table 4 :
4
LLM grading's Pearson correlation to expert grading on the question level.Note that there are only single scores for zero-shot, since they do not have different shot settings.
without refwith refsame question same exam diff exam same question same exam diff examText-only questionsMixtral0 shot0.2320.3111 shot0.3520.3770.3640.3950.3330.2752 shot0.3950.2990.3160.3980.2710.255Llama30 shot0.4520.6031 shot0.5730.5470.5000.6720.5810.6452 shot0.5980.5220.5460.6440.5960.575GPT-4V 0 shot0.6070.6961 shot0.6050.6790.6160.6530.6930.7012 shot0.6720.6480.6740.7170.7270.678Image-related questionsGPT-4V 0 shot0.6770.5391 shot0.6400.6610.6110.6420.5390.7492 shot0.6130.6320.6730.7120.4650.696Expert graderGPT-4V graderExaminee Avg. grade Examinee Avg. grade(%, sorted)(%, sorted)Claude59.4Claude57.7GPT-4V58.2GPT-4V56.2Mixtral41.1Qwen42.0Qwen35.4Mixtral38.2GPT-3.532.8GPT-3.538.0Mistral25.9Mistral24.6Llava21.5Llava24.2Table 5: LLM examinees ranking with expert graderand GPT-4V grader.</p>
<p>Table 6 :
6
LLM graders performance (i.e., Pearson correlation to expert grading) on different examinees.
GradersMixtral Llama3 GPT-4VClaude0.3040.4600.482GPT-4V0.3530.5280.612Mixtral0.2510.4720.564Qwen0.3510.5560.736GPT-3.50.3330.5220.697Mistral0.2910.4670.601Llava0.3870.7160.812Difficulty Levels Looking at Table 7, the weakergraders, i.e., Mixtral and Llama3, perform betteron grading easier questions. In contrast, GPT-4Vperforms better in grading harder questions.GradersMixtral Llama3 GPT-4VEasy0.3740.6020.628Medium0.2930.5240.690Hard0.2240.4960.732</p>
<p>Table 7 :
7
LLM grader's performance (i.e., Pearson correlation to expert grading) on different difficulty levels.</p>
<p>Table 8 :
8
Average performance of the LLMs on the exam level, provided by expert grading, text-only and Englishonly questions.
Grade (%) German ScaleProprietaryGPT-4V70.81.4Claude69.21.6GPT-3.547.82.9Open sourceMixtral61.22.0Qwen56.82.4Mistral48.03.2Llava42.43.5Student avg.56.52.4
We release SciEx under CC BY-NC-SA 4.0 license. Code: https://github.com/TuAnh23/SciEx. Data: https: //huggingface.co/datasets/tuanh23/SciEx.
https://platform.openai.com/
https://console.anthropic.com/
https://huggingface.co/
https://github.com/ggerganov/llama.cpp
AcknowledgmentsThis work was supported by the Helmholtz Programme-oriented Funding, with project number 46.24.01, project name AI for Language Technologies.It was also supported by funding from the pilot program Core-Informatics of the Helmholtz Association (HGF).We thank the lecturers for their contribution during the creation of the dataset: Kunyu Peng, Alexander Jaus, David Schneider, Ruiping Liu, Zdravko Marinov, Yufan Chen, Miklós Borsi, Florian Kalinke, Federico Matteucci, Fabian Richter, Bela Böhnke, Jose Cribeiro-Ramallo, Daniel Ebi, Florian Kalinke, Adrian Feilhauer, Wendy Yi, Laura Merker, Miriam Goetze, Jean-Pierre von der Heydt, Max Göttlicher, Thomas Bläsius, Marcus Wilhelm, Michael ZündorfYou are a university professor.Please grade the following exam question.The exam question, examinee's answer, correct answer, and the maximum possible score are provided in the format:The question is provided in JSON format, but the answer can be freeform text.The provided figures in the question (if any) each contain its path at the bottom, which matches the path provided in the JSON.The answer is text-only.If the question asks to draw on the figure, then the answer should contain a text description of how the drawing should be.Please provide the grade between [0, <max_score>].Please provide the reasoning for your grade.Please provide your output in the format:Below you are provided with examples on how to perform the grading: <example text> Here is your input: <input text>
Iman Mahyar Abbasian, Azimi, Ramesh Amir M Rahmani, Jain, arXiv:2310.02374Conversational health agents: A personalized llm-powered agent framework. 2023arXiv preprint</p>
<p>Think you have solved direct-answer question answering? try arcda, the direct-answer ai2 reasoning challenge. Sumithra Bhakthavatsalam, Daniel Khashabi, Tushar Khot, Dalvi Bhavana, Kyle Mishra, Ashish Richardson, Carissa Sabharwal, Oyvind Schoenick, Peter Tafjord, Clark, arXiv:2102.033152021arXiv preprint</p>
<p>Dongping Chen, Ruoxi Chen, Shilin Zhang, Yinuo Liu, Yaochen Wang, Huichi Zhou, Qihui Zhang, Pan Zhou, Yao Wan, Lichao Sun, arXiv:2402.04788Mllmas-a-judge: Assessing multimodal llm-as-a-judge with vision-language benchmark. 2024arXiv preprint</p>
<p>Can large language models be an alternative to human evaluations?. Cheng- , Han Chiang, Hung-Yi Lee, 10.18653/v1/2023.acl-long.870Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. the 61st Annual Meeting of the Association for Computational LinguisticsToronto, CanadaAssociation for Computational Linguistics2023a</p>
<p>A closer look into using large language models for automatic evaluation. Cheng- , Han Chiang, Hung-Yi Lee, 10.18653/v1/2023.findings-emnlp.599Findings of the Association for Computational Linguistics: EMNLP 2023. Singapore2023bAssociation for Computational Linguistics</p>
<p>A dataset of information-seeking questions and answers anchored in research papers. Pradeep Dasigi, Kyle Lo, Iz Beltagy, Arman Cohan, Noah A Smith, Matt Gardner, 10.18653/v1/2021.naacl-main.365Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesAssociation for Computational Linguistics2021</p>
<p>How does chatgpt perform on the medical licensing exams? the implications of large language models for medical education and knowledge assessment. Aidan Gilson, Conrad Safranek, Thomas Huang, Vimig Socrates, Ling Chi, Andrew Taylor, David Chartash, MedRxiv. 2022</p>
<p>Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, Jacob Steinhardt, arXiv:2009.03300Measuring massive multitask language understanding. 2020arXiv preprint</p>
<p>Introducing hard prompts category in chatbot arena. Tianle Li, Wei-Lin Chiang, Lisa Dunlap, 2024Published on LMSYS</p>
<p>Proactive conversational agents in the post-chatgpt world. Lizi Liao, Grace Hui, Yang , Chirag Shah, 10.1145/3539618.3594250Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR '23. the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR '23New York, NY, USAAssociation for Computing Machinery2023</p>
<p>ROUGE: A package for automatic evaluation of summaries. Chin-Yew Lin, Text Summarization Branches Out. Barcelona, SpainAssociation for Computational Linguistics2004</p>
<p>G-eval: NLG evaluation using gpt-4 with better human alignment. Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, Chenguang Zhu, 10.18653/v1/2023.emnlp-main.153Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language ProcessingSingaporeAssociation for Computational Linguistics2023</p>
<p>Learn to explain: Multimodal reasoning via thought chains for science question answering. Pan Lu, Swaroop Mishra, Tony Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, Ashwin Kalyan, The 36th Conference on Neural Information Processing Systems (NeurIPS). 2022</p>
<p>Bleu: a method for automatic evaluation of machine translation. Kishore Papineni, Salim Roukos, Todd Ward, Wei-Jing Zhu, 10.3115/1073083.1073135Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics. the 40th Annual Meeting of the Association for Computational LinguisticsPhiladelphia, Pennsylvania, USAAssociation for Computational Linguistics2002</p>
<p>Evaluating the text-to-sql capabilities of large language models. Nitarshan Rajkumar, Raymond Li, Dzmitry Bahdanau, arXiv:2204.004982022arXiv preprint</p>
<p>Performance of large language models on a neurology board-style examination. Marc Cicero Schubert, Wolfgang Wick, Varun Venkataramani, JAMA network open. 6122023</p>
<p>BLEURT: Learning robust metrics for text generation. Thibault Sellam, Dipanjan Das, Ankur Parikh, 10.18653/v1/2020.acl-main.704Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsOnline. Association for Computational Linguistics2020</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Denny Quoc V Le, Zhou, Advances in neural information processing systems. 202235</p>
<p>Crowdsourcing multiple choice science questions. Johannes Welbl, Nelson F Liu, Matt Gardner, 10.18653/v1/W17-4413Proceedings of the 3rd Workshop on Noisy Usergenerated Text. the 3rd Workshop on Noisy Usergenerated TextCopenhagen, DenmarkAssociation for Computational Linguistics2017</p>
<p>Bartscore: Evaluating generated text as text generation. Weizhe Yuan, Graham Neubig, Pengfei Liu, Advances in Neural Information Processing Systems. Curran Associates, Inc202134</p>
<p>Bertscore: Evaluating text generation with bert. Tianyi Zhang, * , Varsha Kishore, * , Felix Wu, * , Kilian Q Weinberger, Yoav Artzi, International Conference on Learning Representations. 2020</p>
<p>M3exam: A multilingual, multimodal, multilevel benchmark for examining large language models. Wenxuan Zhang, Mahani Aljunied, Chang Gao, Ken Yew, Lidong Chia, Bing, Advances in Neural Information Processing Systems. 202436</p>
<p>Judging llm-as-a-judge with mt-bench and chatbot arena. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, Advances in Neural Information Processing Systems. 202436</p>            </div>
        </div>

    </div>
</body>
</html>
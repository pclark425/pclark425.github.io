<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5242 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5242</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5242</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-111.html">extraction-schema-111</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <p><strong>Paper ID:</strong> paper-261244034</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2308.14522v2.pdf" target="_blank">Graph Meets LLMs: Towards Large Graph Models</a></p>
                <p><strong>Paper Abstract:</strong> Large models have emerged as the most recent groundbreaking achievements in artificial intelligence, and particularly machine learning. However, when it comes to graphs, large models have not achieved the same level of success as in other fields, such as natural language processing and computer vision. In order to promote applying large models for graphs forward, we present a perspective paper to discuss the challenges and opportunities associated with developing large graph models. First, we discuss the desired characteristics of large graph models. Then, we present detailed discussions from three key perspectives: representation basis, graph data, and graph models. In each category, we provide a brief overview of recent advances and highlight the remaining challenges together with our visions. Finally, we discuss valuable applications of large graph models. We believe this perspective can encourage further investigations into large graph models, ultimately pushing us one step closer towards artificial general intelligence (AGI). We are the first to comprehensively study large graph models, to the best of our knowledge.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5242.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5242.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Adjacency/Edge-list linearization</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Adjacency list / Edge list linearization (textual serialization)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A simple graph-to-text strategy that serializes graph structure as plain text sequences such as adjacency lists or edge lists and injects them into LLMs as prompts for downstream graph analytical tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Adjacency / Edge-list linearization</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Convert the graph's edges or adjacency information into text lines (e.g., nodeA: neighbor1, neighbor2; or edge tuples nodeA-nodeB) and include these text lines in LLM prompts (often alongside natural-language task instructions). No graph-specific encoder is required; the LLM ingests the serialized textual description directly.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>General graphs (used for algorithmic reasoning tasks, knowledge graphs, small synthetic graphs)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Straightforward and human-interpretable; compactness depends on graph size (can be verbose for large graphs); not structure-preserving in an inductive-bias sense for models designed for graphs; may lose explicit multi-hop relational inductive biases and positional/structural encodings; easy to inspect and modify.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Algorithmic graph reasoning (connectivity, shortest path, max flow), neighbor/attribute retrieval, basic node/edge queries, graph analytic QA — as used in empirical evaluations of LLMs on graph tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Qualitative and task-accuracy style metrics reported in the cited empirical studies (accuracy/ability to answer graph queries); this perspective paper reports only that LLMs show preliminary ability but struggle on complex tasks (no numerical metrics given in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>Paper notes that simple textual serializations are a viable baseline but often underperform on complex graph reasoning compared to specialized graph models or more structured prompt designs; they can capture some patterns but tend to lose inner graph inductive biases.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Scales poorly to large graphs (verbosity), can hide multi-hop structure and long-range dependencies, prone to spurious correlations learned by LLMs rather than true reasoning, requires carefully engineered prompts to preserve structural information; likely inferior to graph-native inductive approaches without additional structure-encoding.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Graph Meets LLMs: Towards Large Graph Models', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5242.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5242.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SMILES</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SMILES: Simplified Molecular Input Line Entry System</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A linear string notation for representing molecular graphs as sequences (text) so that language models trained on text can consume and generate molecular structures.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Smiles, a chemical language and information system. 1. introduction to methodology and encoding rules.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>SMILES linearization</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Encode a molecular graph as a linear character/string sequence using SMILES rules (ordering atoms and bond descriptors into a canonical or valid string). This textual representation permits using standard sequence LMs for molecular generation and property prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Molecular graphs</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Compact sequence representation tailored to chemistry; preserves full connectivity for molecules when canonicalized; human- and chemistry-interpretable; includes implicit stereochemistry/bonding conventions but some structural nuances can be hard to capture without canonicalization or augmentation.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Molecular generation, molecular property prediction, and downstream molecular tasks (drug discovery pipelines, generative modeling).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Standard molecular metrics (validity, uniqueness, novelty, property prediction accuracy) are typically used in the literature, but this perspective paper only cites SMILES usage and does not provide numerical results.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>Paper contrasts SMILES (text-first approach) with graph-native molecule representations; notes SMILES enables LLM usage but graphs are a more natural representation that better preserves structural inductive biases and advantages for property prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>SMILES linearization can introduce syntactic brittleness (invalid strings), loses explicit graph positional encodings, and can obscure multi-dimensional structural properties; graph-based models may better capture molecular topology and relational inductive bias.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Graph Meets LLMs: Towards Large Graph Models', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5242.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5242.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPPT structure-token</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPPT: structure-token / task-token edge-prediction prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting strategy that unifies diverse graph tasks by mapping nodes to 'structure-tokens' and classes to 'task-tokens' and reformulating downstream tasks as edge-prediction between these tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Gppt: Graph pre-training and prompt tuning to generalize graph neural networks.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Structure-token / Task-token prompting (edge-prediction reformulation)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Each graph node is represented as a discrete 'structure-token' and each target class as a 'task-token'; tasks such as node classification are cast as predicting a (textual) edge/link between a structure-token and a task-token within a unified prompt or prediction format.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>General / text-attributed graphs (node-level, edge-level, and graph-level tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Unifies heterogeneous downstream tasks into a single edge-prediction formulation; compact and task-unifying; retains a mapping between nodes and token identities but abstracts away native graph message-passing; interpretable in prompt terms.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Node classification, link prediction, and graph classification reformulated as edge prediction in prompt-based or pretraining settings.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Typically classification/link prediction accuracy or F1 on graph benchmarks; this paper references GPPT but does not provide numeric results here.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>Presented as a way to unify tasks compared to disparate task-specific prompts; paper mentions it as one promising design among many prompt-based strategies but does not provide head-to-head numeric comparisons in this perspective.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>May abstract away inductive graph computations (multi-hop aggregations) and relies on how well prompts/token representations encode neighborhood/context; performance depends on how faithfully tokens preserve structural relations and on prompt design.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Graph Meets LLMs: Towards Large Graph Models', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5242.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e5242.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GraphPrompt prototypical-subgraph</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GraphPrompt: prototypical subgraph similarity prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompt design that reformulates multiple graph tasks as subgraph-similarity problems by describing node/graph classes as prototypical subgraphs and measuring similarity in the prompt space.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Graphprompt: Unifying pretraining and downstream tasks for graph neural networks.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Prototypical subgraph prompting (subgraph-similarity)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Represent classes (for node or graph classification) as prototypical subgraphs described in the prompt; downstream examples are compared to these prototypes via subgraph similarity reasoning or predictions cast as similarity judgments in the prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>General graphs, text-attributed graphs (node-, edge-, and graph-level tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Aims to capture local substructure motifs as class prototypes; more structure-aware than raw adjacency lines; interpretable via prototypical subgraphs; allows unifying tasks under similarity comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Node classification, link prediction, and graph classification (treated as subgraph-similarity tasks) in prompt-based or pretraining/fine-tuning setups.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported in original GraphPrompt work as improved downstream metrics (accuracy/F1) in certain settings — perspective paper references the approach but does not reproduce numeric results.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>GraphPrompt is framed as more structure-aware than naive text serializations (adjacency lists) and as providing a unifying view across tasks, but the perspective article does not supply direct quantitative comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Requires designing prototypical subgraphs (which may be nontrivial); similarity computation and prompt engineering are sensitive; scalability to very large graphs or highly diverse graph statistics may be limited.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Graph Meets LLMs: Towards Large Graph Models', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5242.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e5242.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ProG / Learnable prompts</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ProG (learnable prompt tokens inserted into node features) / Neural graph prompts</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Neural prompt methods that implement prompts as learnable tokens or vectors (added to node features) and can be optimized (e.g., via meta-learning) to adapt LLMs/GNNs to multiple graph tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Learnable neural prompts (ProG-style)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Treat prompts as trainable vectors/tokens (neural prompts) that are appended to node features or network inputs; optimize these prompts (possibly via meta-learning) so the downstream model (GNN/LLM) can leverage them across tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>General graphs; applicable to graph-level, node-level, and edge-level tasks, especially text-attributed graphs when combining with LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Parameter-efficient adaptation; preserves more of the model's internal representations rather than forcing everything into text; less brittle than hand-crafted text prompts; can be learned to capture structural cues implicitly.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Multi-task prompting scenarios, node classification, link prediction, graph classification, few-shot transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Typically measured by downstream accuracy/F1 and parameter-efficiency (number of tuned parameters); the perspective paper references ProG but does not provide numeric values.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>Neural prompts are compared conceptually to textual prompts — they are more flexible and can encode structural information in hidden space but are less human-interpretable and require training.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Less interpretable than textual prompts, require additional tuning data or meta-learning, and their success depends on the compatibility of the hidden space with the desired structural encodings; may not be usable with closed-source LLMs that do not expose embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Graph Meets LLMs: Towards Large Graph Models', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5242.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e5242.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>InstructGLM prompts</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>InstructGLM: instruction-style graph descriptive prompts for LLM tuning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting framework that creates scalable, instruction-like natural language descriptions of graph structures and features to tune LLMs for generative graph tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Instruction-style textual graph descriptions (InstructGLM)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Construct detailed natural-language instructions that describe graph topology and node/edge attributes and use these instructions as prompt templates (instruction tuning) so an LLM can perform graph tasks generatively (e.g., outputting node labels or graph answers).</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Text-attributed graphs and general graphs in experimental benchmarks (used for node classification and other graph tasks when converted to text prompts)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Human-interpretable prompts that leverage instruction-following capabilities of LLMs; scalable prompts intended for instruction-tuning of LLMs; may better exploit LLMs' strengths in following natural-language directions.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Graph tasks presented in generative format — node classification, link prediction (via generated answers), and other graph analytic tasks used in LLM benchmarking.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Evaluated with standard accuracy metrics for graph tasks in referenced experiments (perspective paper summarizes empirical promise but does not provide numbers).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>Presented as a more scalable and LLM-friendly alternative to raw adjacency serialization; however, perspective paper warns that purely language-based descriptions may lose graph inductive biases and require fine prompt design.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Risk of information loss and reduced structural inductive bias; success depends on instruction design and the LLM's innate structural reasoning; may struggle on complex graph reasoning tasks despite instruction tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Graph Meets LLMs: Towards Large Graph Models', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5242.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e5242.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-as-Pipelines (Enhancer/Predictor)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Graph-LLM pipelines: LLMs-as-Enhancers and LLMs-as-Predictors</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Two high-level pipelines for using LLMs with graph data: (1) LLMs-as-Enhancers augment text attributes for downstream GNNs; (2) LLMs-as-Predictors directly take textualized graph info and output predictions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>LLM pipeline patterns: Enhancer and Predictor</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>LLMs-as-Enhancers: LLMs process node text attributes to produce enriched representations (features) that are then fed into graph models (GNNs) — graph structure remains processed by graph model. LLMs-as-Predictors: Graphs are textualized (via adjacency/neighbor descriptions or attribute summaries) and supplied directly to LLMs which produce final labels/answers.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Text-attributed graphs primarily; general graphs when textualized for Predictor pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Enhancer pipeline preserves graph inductive bias by keeping a GNN for structure while using LLM strengths for text; Predictor pipeline is simpler (single model) but relies entirely on textual encodings and LLM reasoning capacity.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Node classification, link prediction, and other predictive tasks on text-attributed graph benchmarks used in the empirical comparisons discussed in the literature.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Empirical comparisons in Graph-LLM literature report that LLMs-as-Enhancers often provide gains in text-feature quality for downstream GNNs while LLMs-as-Predictors can be competitive on some tasks but generally struggle on highly-structural tasks; the perspective paper summarizes these insights qualitatively without numeric metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>Enhancer approach is described as a hybrid that leverages graph inductive bias plus LLM text understanding, often outperforming pure textualization (Predictor) on structure-heavy tasks; Predictor can be simpler but is more sensitive to prompt quality and LLM reasoning limits.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Predictor pipeline suffers from loss of structural inductive bias and LLM limitations on complex graph reasoning; Enhancer pipeline requires integrating two model families and potentially more compute; closed LLMs may not be tunable for best integration.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Graph Meets LLMs: Towards Large Graph Models', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5242.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e5242.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM textual-evaluation (NLGraph / GPT4Graph)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Empirical evaluations of LLMs on textualized graph tasks (NLGraph, GPT4Graph, related studies)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Empirical benchmarking efforts that convert graph problems into natural-language tasks (using textual encodings) and evaluate LLMs' graph reasoning capabilities across algorithmic and retrieval tasks, finding preliminary strengths but notable limits.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Textualized graph evaluation (various prompt encodings: adjacency, neighborhood, natural-language descriptions)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Benchmarking designs where graphs are encoded into natural-language prompts (adjacency lists, neighborhood descriptions, textual question formats) and fed to LLMs (GPT-3, GPT-4, Llama, etc.) to measure graph understanding and reasoning performance across a battery of tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Synthetic algorithmic graphs, small general graphs, knowledge/text-attributed graphs used in the cited evaluations</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Enables direct use of existing LLMs without architectural changes; interpretable prompt formats; however, representation fidelity varies with encoding choices and graph scale.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Connectivity, shortest path, maximum flow, neighbor/attribute retrieval, degree detection, node/edge queries, simulating GNNs, question answering on graphs.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Task accuracy / success rate on benchmark tasks; perspective paper reports qualitative outcomes: LLMs show preliminary abilities but fail on more complex reasoning and structured tasks (no numeric metrics in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>These benchmarks serve to compare different textual encodings and LLM architectures; overall message is that textual encodings enable some graph reasoning but typically do not match graph-specialized models on complex structural tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>LMs often rely on spurious correlations rather than faithful algorithmic reasoning; scaling textual encodings to large graphs is impractical; prompt engineering heavily affects results; inherent limitations in LLM structural understanding are evident on algorithmic tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Graph Meets LLMs: Towards Large Graph Models', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Can language models solve graph problems in natural language <em>(Rating: 2)</em></li>
                <li>Gpt4graph: Can large language models understand graph structured data? an empirical evaluation and benchmarking. <em>(Rating: 2)</em></li>
                <li>Graph-LLM <em>(Rating: 2)</em></li>
                <li>InstructGLM <em>(Rating: 2)</em></li>
                <li>Graphprompt: Unifying pretraining and downstream tasks for graph neural networks. <em>(Rating: 2)</em></li>
                <li>Gppt: Graph pre-training and prompt tuning to generalize graph neural networks. <em>(Rating: 2)</em></li>
                <li>Smiles, a chemical language and information system. 1. introduction to methodology and encoding rules. <em>(Rating: 2)</em></li>
                <li>Talk like a graph: Encoding graphs for large language models <em>(Rating: 2)</em></li>
                <li>Graphtext <em>(Rating: 2)</em></li>
                <li>NLGraph <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5242",
    "paper_id": "paper-261244034",
    "extraction_schema_id": "extraction-schema-111",
    "extracted_data": [
        {
            "name_short": "Adjacency/Edge-list linearization",
            "name_full": "Adjacency list / Edge list linearization (textual serialization)",
            "brief_description": "A simple graph-to-text strategy that serializes graph structure as plain text sequences such as adjacency lists or edge lists and injects them into LLMs as prompts for downstream graph analytical tasks.",
            "citation_title": "",
            "mention_or_use": "mention",
            "representation_name": "Adjacency / Edge-list linearization",
            "representation_description": "Convert the graph's edges or adjacency information into text lines (e.g., nodeA: neighbor1, neighbor2; or edge tuples nodeA-nodeB) and include these text lines in LLM prompts (often alongside natural-language task instructions). No graph-specific encoder is required; the LLM ingests the serialized textual description directly.",
            "graph_type": "General graphs (used for algorithmic reasoning tasks, knowledge graphs, small synthetic graphs)",
            "representation_properties": "Straightforward and human-interpretable; compactness depends on graph size (can be verbose for large graphs); not structure-preserving in an inductive-bias sense for models designed for graphs; may lose explicit multi-hop relational inductive biases and positional/structural encodings; easy to inspect and modify.",
            "evaluation_task": "Algorithmic graph reasoning (connectivity, shortest path, max flow), neighbor/attribute retrieval, basic node/edge queries, graph analytic QA — as used in empirical evaluations of LLMs on graph tasks.",
            "performance_metrics": "Qualitative and task-accuracy style metrics reported in the cited empirical studies (accuracy/ability to answer graph queries); this perspective paper reports only that LLMs show preliminary ability but struggle on complex tasks (no numerical metrics given in this paper).",
            "comparison_to_other_representations": "Paper notes that simple textual serializations are a viable baseline but often underperform on complex graph reasoning compared to specialized graph models or more structured prompt designs; they can capture some patterns but tend to lose inner graph inductive biases.",
            "limitations_or_challenges": "Scales poorly to large graphs (verbosity), can hide multi-hop structure and long-range dependencies, prone to spurious correlations learned by LLMs rather than true reasoning, requires carefully engineered prompts to preserve structural information; likely inferior to graph-native inductive approaches without additional structure-encoding.",
            "uuid": "e5242.0",
            "source_info": {
                "paper_title": "Graph Meets LLMs: Towards Large Graph Models",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "SMILES",
            "name_full": "SMILES: Simplified Molecular Input Line Entry System",
            "brief_description": "A linear string notation for representing molecular graphs as sequences (text) so that language models trained on text can consume and generate molecular structures.",
            "citation_title": "Smiles, a chemical language and information system. 1. introduction to methodology and encoding rules.",
            "mention_or_use": "mention",
            "representation_name": "SMILES linearization",
            "representation_description": "Encode a molecular graph as a linear character/string sequence using SMILES rules (ordering atoms and bond descriptors into a canonical or valid string). This textual representation permits using standard sequence LMs for molecular generation and property prediction.",
            "graph_type": "Molecular graphs",
            "representation_properties": "Compact sequence representation tailored to chemistry; preserves full connectivity for molecules when canonicalized; human- and chemistry-interpretable; includes implicit stereochemistry/bonding conventions but some structural nuances can be hard to capture without canonicalization or augmentation.",
            "evaluation_task": "Molecular generation, molecular property prediction, and downstream molecular tasks (drug discovery pipelines, generative modeling).",
            "performance_metrics": "Standard molecular metrics (validity, uniqueness, novelty, property prediction accuracy) are typically used in the literature, but this perspective paper only cites SMILES usage and does not provide numerical results.",
            "comparison_to_other_representations": "Paper contrasts SMILES (text-first approach) with graph-native molecule representations; notes SMILES enables LLM usage but graphs are a more natural representation that better preserves structural inductive biases and advantages for property prediction.",
            "limitations_or_challenges": "SMILES linearization can introduce syntactic brittleness (invalid strings), loses explicit graph positional encodings, and can obscure multi-dimensional structural properties; graph-based models may better capture molecular topology and relational inductive bias.",
            "uuid": "e5242.1",
            "source_info": {
                "paper_title": "Graph Meets LLMs: Towards Large Graph Models",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "GPPT structure-token",
            "name_full": "GPPT: structure-token / task-token edge-prediction prompting",
            "brief_description": "A prompting strategy that unifies diverse graph tasks by mapping nodes to 'structure-tokens' and classes to 'task-tokens' and reformulating downstream tasks as edge-prediction between these tokens.",
            "citation_title": "Gppt: Graph pre-training and prompt tuning to generalize graph neural networks.",
            "mention_or_use": "mention",
            "representation_name": "Structure-token / Task-token prompting (edge-prediction reformulation)",
            "representation_description": "Each graph node is represented as a discrete 'structure-token' and each target class as a 'task-token'; tasks such as node classification are cast as predicting a (textual) edge/link between a structure-token and a task-token within a unified prompt or prediction format.",
            "graph_type": "General / text-attributed graphs (node-level, edge-level, and graph-level tasks)",
            "representation_properties": "Unifies heterogeneous downstream tasks into a single edge-prediction formulation; compact and task-unifying; retains a mapping between nodes and token identities but abstracts away native graph message-passing; interpretable in prompt terms.",
            "evaluation_task": "Node classification, link prediction, and graph classification reformulated as edge prediction in prompt-based or pretraining settings.",
            "performance_metrics": "Typically classification/link prediction accuracy or F1 on graph benchmarks; this paper references GPPT but does not provide numeric results here.",
            "comparison_to_other_representations": "Presented as a way to unify tasks compared to disparate task-specific prompts; paper mentions it as one promising design among many prompt-based strategies but does not provide head-to-head numeric comparisons in this perspective.",
            "limitations_or_challenges": "May abstract away inductive graph computations (multi-hop aggregations) and relies on how well prompts/token representations encode neighborhood/context; performance depends on how faithfully tokens preserve structural relations and on prompt design.",
            "uuid": "e5242.2",
            "source_info": {
                "paper_title": "Graph Meets LLMs: Towards Large Graph Models",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "GraphPrompt prototypical-subgraph",
            "name_full": "GraphPrompt: prototypical subgraph similarity prompting",
            "brief_description": "A prompt design that reformulates multiple graph tasks as subgraph-similarity problems by describing node/graph classes as prototypical subgraphs and measuring similarity in the prompt space.",
            "citation_title": "Graphprompt: Unifying pretraining and downstream tasks for graph neural networks.",
            "mention_or_use": "mention",
            "representation_name": "Prototypical subgraph prompting (subgraph-similarity)",
            "representation_description": "Represent classes (for node or graph classification) as prototypical subgraphs described in the prompt; downstream examples are compared to these prototypes via subgraph similarity reasoning or predictions cast as similarity judgments in the prompt.",
            "graph_type": "General graphs, text-attributed graphs (node-, edge-, and graph-level tasks)",
            "representation_properties": "Aims to capture local substructure motifs as class prototypes; more structure-aware than raw adjacency lines; interpretable via prototypical subgraphs; allows unifying tasks under similarity comparisons.",
            "evaluation_task": "Node classification, link prediction, and graph classification (treated as subgraph-similarity tasks) in prompt-based or pretraining/fine-tuning setups.",
            "performance_metrics": "Reported in original GraphPrompt work as improved downstream metrics (accuracy/F1) in certain settings — perspective paper references the approach but does not reproduce numeric results.",
            "comparison_to_other_representations": "GraphPrompt is framed as more structure-aware than naive text serializations (adjacency lists) and as providing a unifying view across tasks, but the perspective article does not supply direct quantitative comparisons.",
            "limitations_or_challenges": "Requires designing prototypical subgraphs (which may be nontrivial); similarity computation and prompt engineering are sensitive; scalability to very large graphs or highly diverse graph statistics may be limited.",
            "uuid": "e5242.3",
            "source_info": {
                "paper_title": "Graph Meets LLMs: Towards Large Graph Models",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "ProG / Learnable prompts",
            "name_full": "ProG (learnable prompt tokens inserted into node features) / Neural graph prompts",
            "brief_description": "Neural prompt methods that implement prompts as learnable tokens or vectors (added to node features) and can be optimized (e.g., via meta-learning) to adapt LLMs/GNNs to multiple graph tasks.",
            "citation_title": "",
            "mention_or_use": "mention",
            "representation_name": "Learnable neural prompts (ProG-style)",
            "representation_description": "Treat prompts as trainable vectors/tokens (neural prompts) that are appended to node features or network inputs; optimize these prompts (possibly via meta-learning) so the downstream model (GNN/LLM) can leverage them across tasks.",
            "graph_type": "General graphs; applicable to graph-level, node-level, and edge-level tasks, especially text-attributed graphs when combining with LLMs",
            "representation_properties": "Parameter-efficient adaptation; preserves more of the model's internal representations rather than forcing everything into text; less brittle than hand-crafted text prompts; can be learned to capture structural cues implicitly.",
            "evaluation_task": "Multi-task prompting scenarios, node classification, link prediction, graph classification, few-shot transfer.",
            "performance_metrics": "Typically measured by downstream accuracy/F1 and parameter-efficiency (number of tuned parameters); the perspective paper references ProG but does not provide numeric values.",
            "comparison_to_other_representations": "Neural prompts are compared conceptually to textual prompts — they are more flexible and can encode structural information in hidden space but are less human-interpretable and require training.",
            "limitations_or_challenges": "Less interpretable than textual prompts, require additional tuning data or meta-learning, and their success depends on the compatibility of the hidden space with the desired structural encodings; may not be usable with closed-source LLMs that do not expose embeddings.",
            "uuid": "e5242.4",
            "source_info": {
                "paper_title": "Graph Meets LLMs: Towards Large Graph Models",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "InstructGLM prompts",
            "name_full": "InstructGLM: instruction-style graph descriptive prompts for LLM tuning",
            "brief_description": "A prompting framework that creates scalable, instruction-like natural language descriptions of graph structures and features to tune LLMs for generative graph tasks.",
            "citation_title": "",
            "mention_or_use": "mention",
            "representation_name": "Instruction-style textual graph descriptions (InstructGLM)",
            "representation_description": "Construct detailed natural-language instructions that describe graph topology and node/edge attributes and use these instructions as prompt templates (instruction tuning) so an LLM can perform graph tasks generatively (e.g., outputting node labels or graph answers).",
            "graph_type": "Text-attributed graphs and general graphs in experimental benchmarks (used for node classification and other graph tasks when converted to text prompts)",
            "representation_properties": "Human-interpretable prompts that leverage instruction-following capabilities of LLMs; scalable prompts intended for instruction-tuning of LLMs; may better exploit LLMs' strengths in following natural-language directions.",
            "evaluation_task": "Graph tasks presented in generative format — node classification, link prediction (via generated answers), and other graph analytic tasks used in LLM benchmarking.",
            "performance_metrics": "Evaluated with standard accuracy metrics for graph tasks in referenced experiments (perspective paper summarizes empirical promise but does not provide numbers).",
            "comparison_to_other_representations": "Presented as a more scalable and LLM-friendly alternative to raw adjacency serialization; however, perspective paper warns that purely language-based descriptions may lose graph inductive biases and require fine prompt design.",
            "limitations_or_challenges": "Risk of information loss and reduced structural inductive bias; success depends on instruction design and the LLM's innate structural reasoning; may struggle on complex graph reasoning tasks despite instruction tuning.",
            "uuid": "e5242.5",
            "source_info": {
                "paper_title": "Graph Meets LLMs: Towards Large Graph Models",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "LLM-as-Pipelines (Enhancer/Predictor)",
            "name_full": "Graph-LLM pipelines: LLMs-as-Enhancers and LLMs-as-Predictors",
            "brief_description": "Two high-level pipelines for using LLMs with graph data: (1) LLMs-as-Enhancers augment text attributes for downstream GNNs; (2) LLMs-as-Predictors directly take textualized graph info and output predictions.",
            "citation_title": "",
            "mention_or_use": "mention",
            "representation_name": "LLM pipeline patterns: Enhancer and Predictor",
            "representation_description": "LLMs-as-Enhancers: LLMs process node text attributes to produce enriched representations (features) that are then fed into graph models (GNNs) — graph structure remains processed by graph model. LLMs-as-Predictors: Graphs are textualized (via adjacency/neighbor descriptions or attribute summaries) and supplied directly to LLMs which produce final labels/answers.",
            "graph_type": "Text-attributed graphs primarily; general graphs when textualized for Predictor pipeline",
            "representation_properties": "Enhancer pipeline preserves graph inductive bias by keeping a GNN for structure while using LLM strengths for text; Predictor pipeline is simpler (single model) but relies entirely on textual encodings and LLM reasoning capacity.",
            "evaluation_task": "Node classification, link prediction, and other predictive tasks on text-attributed graph benchmarks used in the empirical comparisons discussed in the literature.",
            "performance_metrics": "Empirical comparisons in Graph-LLM literature report that LLMs-as-Enhancers often provide gains in text-feature quality for downstream GNNs while LLMs-as-Predictors can be competitive on some tasks but generally struggle on highly-structural tasks; the perspective paper summarizes these insights qualitatively without numeric metrics.",
            "comparison_to_other_representations": "Enhancer approach is described as a hybrid that leverages graph inductive bias plus LLM text understanding, often outperforming pure textualization (Predictor) on structure-heavy tasks; Predictor can be simpler but is more sensitive to prompt quality and LLM reasoning limits.",
            "limitations_or_challenges": "Predictor pipeline suffers from loss of structural inductive bias and LLM limitations on complex graph reasoning; Enhancer pipeline requires integrating two model families and potentially more compute; closed LLMs may not be tunable for best integration.",
            "uuid": "e5242.6",
            "source_info": {
                "paper_title": "Graph Meets LLMs: Towards Large Graph Models",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "LLM textual-evaluation (NLGraph / GPT4Graph)",
            "name_full": "Empirical evaluations of LLMs on textualized graph tasks (NLGraph, GPT4Graph, related studies)",
            "brief_description": "Empirical benchmarking efforts that convert graph problems into natural-language tasks (using textual encodings) and evaluate LLMs' graph reasoning capabilities across algorithmic and retrieval tasks, finding preliminary strengths but notable limits.",
            "citation_title": "",
            "mention_or_use": "mention",
            "representation_name": "Textualized graph evaluation (various prompt encodings: adjacency, neighborhood, natural-language descriptions)",
            "representation_description": "Benchmarking designs where graphs are encoded into natural-language prompts (adjacency lists, neighborhood descriptions, textual question formats) and fed to LLMs (GPT-3, GPT-4, Llama, etc.) to measure graph understanding and reasoning performance across a battery of tasks.",
            "graph_type": "Synthetic algorithmic graphs, small general graphs, knowledge/text-attributed graphs used in the cited evaluations",
            "representation_properties": "Enables direct use of existing LLMs without architectural changes; interpretable prompt formats; however, representation fidelity varies with encoding choices and graph scale.",
            "evaluation_task": "Connectivity, shortest path, maximum flow, neighbor/attribute retrieval, degree detection, node/edge queries, simulating GNNs, question answering on graphs.",
            "performance_metrics": "Task accuracy / success rate on benchmark tasks; perspective paper reports qualitative outcomes: LLMs show preliminary abilities but fail on more complex reasoning and structured tasks (no numeric metrics in this paper).",
            "comparison_to_other_representations": "These benchmarks serve to compare different textual encodings and LLM architectures; overall message is that textual encodings enable some graph reasoning but typically do not match graph-specialized models on complex structural tasks.",
            "limitations_or_challenges": "LMs often rely on spurious correlations rather than faithful algorithmic reasoning; scaling textual encodings to large graphs is impractical; prompt engineering heavily affects results; inherent limitations in LLM structural understanding are evident on algorithmic tasks.",
            "uuid": "e5242.7",
            "source_info": {
                "paper_title": "Graph Meets LLMs: Towards Large Graph Models",
                "publication_date_yy_mm": "2023-08"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Can language models solve graph problems in natural language",
            "rating": 2,
            "sanitized_title": "can_language_models_solve_graph_problems_in_natural_language"
        },
        {
            "paper_title": "Gpt4graph: Can large language models understand graph structured data? an empirical evaluation and benchmarking.",
            "rating": 2,
            "sanitized_title": "gpt4graph_can_large_language_models_understand_graph_structured_data_an_empirical_evaluation_and_benchmarking"
        },
        {
            "paper_title": "Graph-LLM",
            "rating": 2
        },
        {
            "paper_title": "InstructGLM",
            "rating": 2,
            "sanitized_title": "instructglm"
        },
        {
            "paper_title": "Graphprompt: Unifying pretraining and downstream tasks for graph neural networks.",
            "rating": 2,
            "sanitized_title": "graphprompt_unifying_pretraining_and_downstream_tasks_for_graph_neural_networks"
        },
        {
            "paper_title": "Gppt: Graph pre-training and prompt tuning to generalize graph neural networks.",
            "rating": 2,
            "sanitized_title": "gppt_graph_pretraining_and_prompt_tuning_to_generalize_graph_neural_networks"
        },
        {
            "paper_title": "Smiles, a chemical language and information system. 1. introduction to methodology and encoding rules.",
            "rating": 2,
            "sanitized_title": "smiles_a_chemical_language_and_information_system_1_introduction_to_methodology_and_encoding_rules"
        },
        {
            "paper_title": "Talk like a graph: Encoding graphs for large language models",
            "rating": 2,
            "sanitized_title": "talk_like_a_graph_encoding_graphs_for_large_language_models"
        },
        {
            "paper_title": "Graphtext",
            "rating": 2
        },
        {
            "paper_title": "NLGraph",
            "rating": 1
        }
    ],
    "cost": 0.016075,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Graph Meets LLMs: Towards Large Graph Models
11 Nov 2023</p>
<p>Ziwei Zhang zwzhang@tsinghua.edu.cn 
Department of Computer Science and Technology
Tsinghua University
100084BeijingChina</p>
<p>Haoyang Li 
Department of Computer Science and Technology
Tsinghua University
100084BeijingChina</p>
<p>Zeyang Zhang zy-zhang20@mails.tsinghua.edu.cn 
Department of Computer Science and Technology
Tsinghua University
100084BeijingChina</p>
<p>Yijian Qin 
Department of Computer Science and Technology
Tsinghua University
100084BeijingChina</p>
<p>Xin Wang xin_wang@tsinghua.edu.cn 
Department of Computer Science and Technology
Tsinghua University
100084BeijingChina</p>
<p>Wenwu Zhu wwzhu@tsinghua.edu.cn 
Department of Computer Science and Technology
Tsinghua University
100084BeijingChina</p>
<p>Graph Meets LLMs: Towards Large Graph Models
11 Nov 202335202674833018A4F726C453F8E5897DarXiv:2308.14522v2[cs.LG]
Large models have emerged as the most recent groundbreaking achievements in artificial intelligence, and particularly machine learning.However, when it comes to graphs, large models have not achieved the same level of success as in other fields, such as natural language processing and computer vision.In order to promote applying large models for graphs forward, we present a perspective paper to discuss the challenges and opportunities associated with developing large graph models 1 .First, we discuss the desired characteristics of large graph models.Then, we present detailed discussions from three key perspectives: representation basis, graph data, and graph models.In each category, we provide a brief overview of recent advances and highlight the remaining challenges together with our visions.Finally, we discuss valuable applications of large graph models.We believe this perspective can encourage further investigations into large graph models, ultimately pushing us one step closer towards artificial general intelligence (AGI).We are the first to comprehensively study large graph models, to the best of our knowledge.</p>
<p>Introduction</p>
<p>In recent years, there has been a growing interesting in large models for both research and practical applications.Large models have been particularly revolutionary in fields such as natural language processing (NLP) [1,2,3] and computer vision (CV) [4,5,6], where pre-training extremely large models on large-scale unlabeled data has yielded significant breakthroughs.However, graphs, which are commonly used to represent relationships between entities in various domains such as social networks, molecule graphs, and transportation networks, have not yet seen the same level of success with large models as other domains.In this paper, we present a perspective about the challenges and opportunities associated with developing large graph models.First, we introduce large graph models and outline four key desired characteristics, including graph models with scaling laws, graph foundation model, in-context graph understanding and processing abilities, and versatile graph reasoning capabilities.Then, we offer detailed perspectives from three aspects: (1) For graph representation basis, we discuss graph domains and transferability, as well as the alignment of graphs with natural languages.Our key takeaway is the significance of identifying a suitable and unified representation basis that spans diverse graph domains, which serves as a fundamental step towards constructing effective large graph models; (2) For graph data, we summarize and compare the existing graph datasets with other domains, and highlight that the availability of more large-scale high-quality graph data is resource-intensive yet indispensable; (3) For models, we systematically discuss backbone architectures, including graph neural networks and graph Transformers, as well as pre-training and post-processing techniques, such as prompting, parameter-efficient fine-tuning, and model compression.We also discuss LLMs as graph models, which is a newly trending direction.Finally, we discuss the significant impact that large graph models can have on various graph applications, including recommendation systems, knowledge graphs, molecules, finance, code 3 Graph Representation Basis</p>
<p>Graph Domains and Transferability</p>
<p>Large models, LLMs, serve as foundation models [24], as they can be adapted to a wide range of downstream tasks after being pre-trained.The remarkable ability of LLMs stems from the underlying assumption of the existence of a common representation basis for various NLP tasks.For instance, word tokens for natural language processing are universal and information-preserving data representations that do not rely on specific tasks.In contrast, graphs are general data structures that span a multitude of domains.Therefore, the raw input data, i.e., nodes and edges, may not always be the most suitable representation basis for handling all graph data.Nodes and edges in social networks, molecule graphs, and knowledge graphs, for instance, have distinct meanings with their unique feature and topological space.Thus, directly sharing information and transferring knowledge based on input graph data often poses significant challenges.</p>
<p>It is widely believed that there exist more high-level or abstract common graph patterns, which can be shared across different graphs and tasks within a certain domain.For example, many human interpretable patterns have been identified in classical network science [25], such as homophily, small-world phenomenon, power-law distribution of node degrees, etc.Nevertheless, even with these high-level shared knowledge, creating effective large models that can perform well across diverse graph domains is still non-trivial.</p>
<p>Aligning with Natural Languages</p>
<p>Another key competency of recent large models is their ability to interact with humans and follow instructions, as we are naturally capable of understanding languages and visual perceptions.In contrast, humans are less capable of handling graphs, especially more complex reasoning problems.As a result, communicating and instructing large models to behave for graph tasks the way we desire, especially using natural languages, is particularly challenging.We summarize three categories of strategies worth exploring to overcome this obstacle.</p>
<p>The first strategy is to align the representation basis of graphs and text through a large amount of paired data, similar to computer vision in principle.If successful, we will be able to interact with graph models using natural languages.For example, we can ask the model to generate molecule graphs with desired properties or ask the model to perform challenging graph reasoning tasks.Some initial attempts have been made for text-attributed graphs [26,27], which serve as a good starting point.However, collecting such data for general graphs is much more costly and challenging than image-text pairs.</p>
<p>The second strategy is to transform graphs into natural languages, and then work solely in the language basis.Some initial attempts using this strategy have been developed, where graph structures are transformed into text representations, such as the adjacency list or the edge list, and inserted into LLMs as prompts.Then, natural languages are used to perform graph analytical tasks.We provide more detailed discussions in Section 5.4.However, directly transforming graph data and tasks into languages may lose the inner structure and inductive bias for graphs, resulting in unsatisfactory task performance.More delicate designs, such as effective prompts to convert graph structures and tasks into texts, are required to further advance this strategy.</p>
<p>The last category is to find other representation basis as a middle ground for different graph tasks and natural languages.The most straight-forward way is to use some hidden space of neural networks.However, it faces the challenge that deep neural networks are largely not explainable at the moment, not to mention that finding the desired shared hidden space can be frustratingly challenging.On the other hand, although humans are not capable of directly handling graph data, we can design appropriate algorithms to solve graph tasks, including many well-known algorithms in graph theory such as finding shortest paths, dynamic programming, etc.Therefore, if we can align the behavior of graph models with these algorithms, we can understand and control the behaviors of these models to a certain extent.Some efforts have been devoted in this direction, known as algorithmic reasoning [28], which we believe contains rich potentials.</p>
<p>In summary, finding the suitable representation basis, potentially aligning with natural languages, and unifying various graph tasks across different domains is one fundamental step towards building successful large graph models.</p>
<p>Graph Data</p>
<p>The success of big models is largely dependent on the availability of high-quality, large-scale datasets.</p>
<p>For instance, GPT-3 was pre-trained on a corpus of approximately 500 billion tokens [1], while CLIP, a representative model that bridges natural language processing and computer vision, was trained on 400 million image-text pairs.It is reasonable to assume that even more data has been utilized in more recent large models, such as GPT-4 [29].This massive amount of data for NLP and CV tasks is typically sourced from publicly accessible human-generated content, such as web pages in CommonCrawl or user-posted photos in social media, which are easily collected from the web.</p>
<p>In contrast, large-scale graph data is not as easily accessible.There are typically two scenarios for graph data: numerous small-scale graphs, such as molecules, or a single/few large-scale graphs, such as social networks or citation graphs.For example, Open Graph Benchmark [30], one of the most representative public benchmarks for graph machine learning, includes two large graph datasets: MAG240M, which contains a large citation graph with approximately 240 million nodes and 1.3 billion edges, and PCQM4M, which contains approximately 4 million molecules.However, their scale is considerably lower than the datasets used in NLP or CV.If we treat each node in MAG240M as a token (though a node may contain arguably more information) or each graph in PCQM4M as an image, these graph datasets are at least 10 3 to 10 4 times smaller than their NLP or CV counterparts.</p>
<p>In addition to the data utilized for pre-training, commonly accepted and widely adopted benchmarks, such as SuperGLUE [31] and BIG-bench [32] for NLP and ImageNet [33] for CV, have been found to be beneficial in the development of large models.These benchmarks are especially useful in assessing model quality and determining the most promising technical routes during the early stages.Although there are numerous benchmarks available for graph learning, such as Open Graph Benchmark [30] and Benchmarking GNN [34], it is likely that their scope, including factors like scale, task and domain diversity, and evaluation protocols, may not be suitable or sufficient for evaluating large graph models.Therefore, the creation of more specialized benchmarks can further facilitate the progress of large graph models.</p>
<p>Next, we summarize some principles that are helpful while collecting more graph data.</p>
<p>• Domain diversity: To enable large graph models handle different graph applications, it is crucial to expose the model to different domains of interests, so that large graph models can be adopted across various fields and serve as the graph foundation model.• Type diversity: Graphs have rich types, including homogeneous and heterogeneous, homophily and heterophily, static and dynamic, directed and undirected, weighted and unweighted, signed and unsigned, etc.The diversity of graph type is also important to empower the large graph model handle diverse downstream graphs.• Statistics diversity: Graphs also have varying statistics, e.g., size, density, degree distribution, etc.</p>
<p>Such diversity should be considered to ensure the effectiveness of large graph model.• Task diversity: Graph tasks are also distinct, ranging from node-level, edge-level to graph-level, and from discriminative tasks such as classification and prediction to generative tasks such as graph generation.Increasing the task diversity in pre-training or post-processing phase can help developing and evaluating effective large graph models.• Modality diversity: Graphs, as general data representations, can also combine different modalities of data, such as text, images, and tabular data, which can further enrich the utility of the large graph model.</p>
<p>In summary, the availability of high-quality graph data is critical to the development of large graph models, which requires more resources and efforts.Since collecting such graph data is difficult and costly, community-wide collaboration may be essential to accelerate this process.</p>
<p>Graph Models</p>
<p>In this section, we continue the discussion from the graph model aspect.Similar to large models in other domains, we divide our discussion into three topics: backbone architecture, pre-training, and post-processing.We also discuss LLMs as graph models, which is a recently trending direction.</p>
<p>Backbone Architecture</p>
<p>To date, Transformers [35] have been the de facto standards for NLP and CV.However, no similar consensus has been reached for the graph domain.We briefly discuss two promising deep learning architectures for graphs: graph neural networks (GNNs) and graph transformers.</p>
<p>GNNs are the most popular deep learning architectures for graphs [36] and have been extensively studied.Most representative GNNs adopt a message-passing paradigm, where nodes exchange messages with their neighbors to update their representations.GNNs can incorporate both structural information and semantic information such as node and edge attributes in an end-to-end manner.However, despite achieving considerable successes in many graph tasks, one key obstacle for further advancing GNNs into large models is their limited model capacity.As opposed to the scaling law in large models [16], the performance of GNNs saturates or even dramatically drops as the model size grows.Many research efforts have been devoted to explain this problem, such as over-smoothing [37] and over-squashing [38], as well as strategies to alleviate it.Nevertheless, progress has not been groundbreaking.To date, most successful GNNs only have at most millions of parameters, and further scaling to billions of parameters leads to minimum or no additional improvement.</p>
<p>Graph Transformer is another architecture that extends and adapts the typical Transformers for graph data [39].In a nutshell, since classical Transformers cannot naturally process graph structures, Graph Transformer adopts various structure-encoding strategies to add graph structures to the input of Transformers [40].Graph Transformers evaluate the importance of each neighboring node, giving larger weights to nodes that provide more pertinent information.The self-attention mechanism empowers Graph Transformers the ability to dynamically adapt.One of the most successful graph Transformers is Graphormer [41], which ranked first in the PCQM4M molecule property prediction task of OGB Large-Scale Challenge [42] in 2021.More efforts further improve Graph Transformer from various aspects including architecture designs, efficiency, model expressiveness, etc.For example, Structure-Aware Transformer (SAT) [43] proposes a new self-attention mechanism to capture the structural similarity between nodes more effectively.AutoGT [40] proposes a unified graph transformer formulation for existing graph transformer architectures and enhances the model performance using AutoML.To improve efficiency, General, Powerful, and Scalable graph Transformer (GPS) [44] introduces a general framework with linear complexity by decoupling the local edge aggregation from the fully-connected Transformer.NAGphormer [45] also aims to address the complexity challenge of graph Transformers for large graphs by treating different hops of neighbors as a sequence of token vectors.For the expressiveness, SEG-WL test [46] introduces a graph isomorphism test algorithm, which can be used for assessing the structural discriminative power of graph Transformers.FeTA [47] analyzes the expressiveness of graph Transformers in the spectral domain and proposes to perform attention on the entire graph spectrum.</p>
<p>We briefly summarize the key differences between GNNs and graph Transformers, while more discussions for the relationships between transformers and GNNs can be found [48,49,50,51]:</p>
<p>• Aggregation vs. Attention: GNNs employ message passing functions to aggregate information from neighboring nodes, whereas Graph Transformers weigh contributions from neighbors using self-attentions, potentially enhancing the flexibility for large graph models.</p>
<p>• Modeling structures: GNNs naturally incorporate graph structures in the message passing functions as an inductive bias, while graph Transformers adopt pre-processing strategies, such as structureencoding, to incorporate structures.</p>
<p>• Depth and Over-smoothing: As aforementioned, deep GNNs may suffer from over-smoothing, leading to a decrease in their discriminative power.Graph Transformers, on the other hand, do not exhibit similar issues empirically.One plausible explanation is that Graph Transformers adaptively focus on more relevant nodes, enabling them to effectively filter and capture informative patterns.</p>
<p>• Scalability and Efficiency: GNNs, with their relatively simpler operations, may offer computational benefits for certain tasks.In contrast, the self-attention mechanism between node pairs in Graph Transformers can be computationally intensive, especially for large graphs.Considerable efforts have been dedicated to further enhancing the scalability and efficiency for both methods.</p>
<p>While both GNNs and Graph Transformers have made remarkable progress, it is not very clear which one, or some other architectures, may be best suited as the backbone for large graph models.Besides empirical evidence from trials and errors, further research into how large models work and what graph problems they may solve could bring principled advancements.It is also worth noting that most graph tasks relate to reasoning rather than perception.Therefore, the inductive bias in architecture designs usually does not come from mimicking human brains.</p>
<p>In our opinion, given the scale of existing graph datasets, GNNs are still a strong backbone model thanks to their strong inductive bias and expressive power.However, as the size of the training graph datasets continues to increase, graph Transformers may become more powerful through increasing the number of parameters and gradually become the prevailing approach.</p>
<p>Pre-training</p>
<p>Pre-training, as a widely adopted practice in NLP with well-known models like BERT [2] and GPT [52], involves training a model on a massive dataset before applying it for specific tasks.The primary objective is to capture general patterns or knowledge present in the data and subsequently adapt the pre-trained model to meet downstream requirements.Graph pre-training, also known as unsupervised or self-supervised graph learning, has received significant attention in recent years [53,54].It aims to capture the inherent structural patterns within the training graph data, analogous to how language models capture the syntax and semantics of languages.As explained in Section 2, we recognize pre-training as an essential paradigm for large graph models.Next, we provide a more detailed discussion of graph pre-training.</p>
<p>Compared to the straightforward yet effective masking operation used in language modeling, graph pre-training strategies are more diverse and complicated, ranging from contrastive to predictive/generative approaches.Generally, graph pre-training methods leverage the rich structural and semantic information in the graph to introduce pretext learning tasks.Through these tasks, the pre-trained model learns useful node, edge, or graph-level representations without relying on explicitly annotated labels.In contrastive pre-training methods, positive and negative graph samples are constructed through various graph data augmentation techniques, followed by optimizing contrastive objectives, such as maximizing the mutual information between positive and negative pairs.On the other hand, in generative and predictive methods, specific components of the graph data, such as node features and edges, are first hide by masking.Then, the graph model aims to reconstruct the masked portions, which serve as pseudo-labels for pre-training.For more details, we refer readers to dedicated surveys [53,54].</p>
<p>We summarize the desired benefits of graph pre-training using the following "four-E" principle:</p>
<p>• Encoding structural information: Unlike pre-training methods for other types of data, such as languages and images, which focus primarily on semantic information, graphs contain rich structural information.Pre-training large graph models essentially needs to integrate structural and semantic information from diverse graph datasets.This also highlights the unique challenges and opportunities of graph pre-training.</p>
<p>• Easing data sparsity and label scarcity: Large graph models, with their substantial model capacity, are prone to overfitting when confronted with specific tasks that have limited labeled data.Pretraining on a wide range of graph datasets and tasks can act as a regularizing mechanism, preventing the model from overfitting to a specific task and improving generalization performance.</p>
<p>• Expanding applicability domains: One of the hallmarks of pre-training is the ability to transfer learned knowledge across various domains.By pre-training large graph models on diverse graph datasets, they should be able to capture a wide range of structural patterns, which can then be applied, adapted, or fine-tuned to graph data in similar domains, maximizing the model's utility.</p>
<p>• Enhancing robustness and generalization.Pre-training methods can expose large graph models to diverse graphs with distinct characteristics, including varying sizes, structures, and complexities.This exposure can potentially lead to more robust models that are less sensitive to adversarial perturbations [55].Moreover, models trained in this manner are more likely to generalize well to unseen graph data or novel graph tasks.</p>
<p>In summary, graph pre-training is not merely a beneficial or supplementary step, but a pivotal and necessary paradigm for large graph models.</p>
<p>Post-processing</p>
<p>After obtaining a substantial amount of knowledge through pre-training, LLMs still require postprocessing to enhance their adaptability to downstream tasks.Representative post-processing techniques include prompting [56], parameter-efficient fine-tuning [57], reinforcement learning with human feedbacks [58], and model compression [59].For graphs, some recent efforts have also been devoted to study post-processing techniques for pre-trained models.</p>
<p>Prompting originally refers to methods that provide specific instructions to language models for generating desired contents for downstream tasks.Recently, constructing prompts with an in-context learning template demonstrates great effectiveness in LLMs [60].Language prompts usually contain a task description and a few examples to illustrate the downstream tasks.Graph prompts, which mimic natural language prompts to enhance downstream task performance with limited labels and enable interaction with the model to extract valuable knowledge, have been extensively studied [61].One significant challenge for graph prompts is the unification of diverse graph tasks, spanning from node-level and link-level to graph-level tasks.In contrast, tasks in natural language can be easily unified as language modeling under specific constraints.To tackle this challenge, GPPT [62] unifies graph tasks into edge prediction, considering that a typical node classification task can be reformulated as the link prediction task between the structure-token and the task-token.Each structure-token represents a node in the graph data, and each task-token corresponds to a class.GraphPrompt [61] further extends the idea and unifies link prediction, node classification, and graph classification as subgraph similarity calculation by describing node and graph classes as prototypical subgraphs.Similarly, ProG [63] reformulates node and edge-level tasks as graph-level tasks and further proposes multi-task prompting by realizing prompting as a learnable token that is directly added to the node feature, mirroring the prefix phrase prompting technique in NLP.ProG also employs meta learning to learn prompting for different tasks.Other graph prompts such as PRODIGY [64], GPF [65], Gare [66], SGL-PT [67], DeepGPT [68], G-Prompt [69], CPP [70], KGTransformer [71], SAP [72], HetGPT [73], ULTRA-DP [74], PGCL [75], and TAG [76] follow similar principles.</p>
<p>Parameter-efficient fine-tuning refers to techniques where only a small portions of model parameters are optimized, while the rest is kept fixed.Besides reducing computational costs, it also helps to enable the model to adapt to new tasks without forgetting the knowledge obtained in pre-training, preserving the general capabilities of the model while allowing for task-specific adaptation.Graph parameterefficient fine-tuning has also recently begun to received attention.For example, AdapterGNN [77] and G-Adapter [78] both investigate adapter-based fine-tuning techniques for graph models, aiming to reduce the number of tuneable parameters while preserving comparable accuracy.Specifically, AdapterGNN tunes GNNs by incorporating two adapters, one inserted before and another one after the message passing process.On the other hand, G-Adapter focuses on graph transformers and introduces a message passing process within the adapter to better utilize graph structural information.S2PGNN [79] further proposes to search for architecture modifications to improve the adaptivity of the fine-tuning stage.</p>
<p>Model compression aims to reduce the memory and computational demands of models through various techniques, including knowledge distillation, pruning, and quantization, which are particularly valuable when deploying large models in resource-constrained environments.Here, we focus on quantization, which has gained popularity and proven effectiveness in LLMs [3], and refer readers to dedicated surveys for other methods [80,81,82].Quantization entails reducing the precision of numerical values used by the model while preserving model performance to the greatest extent possible.In the case of large models, post-training quantization (PTQ) is particularly preferred, as it does not require retraining.PTQ in graph learning has also been explored in SGQuant [83], which proposes a multi-granularity quantization technique that operates at various levels, including graph topology, layers, and components within a layer.Other methods such as Degree-Quant [84], BiFeat [85], Tango [86], VQGraph [87],
A 2 Q [88],
and AdaQP [89] adopt a quantization-aware training scheme, which are inspiring but cannot be used standalone during the post-processing stage.</p>
<p>In summary, the success of post-processing techniques shown in LLMs has sparked interest in similar research in the graph domain.However, due to the unavailability of large graph models at present, the assessment of these methods is limited to relatively small models.Therefore, it is crucial to further verify their effectiveness when applied to large graph models, and more research challenges and opportunities may arise.</p>
<p>LLMs as Graph Models</p>
<p>Recent research has also explored the potential of directly utilizing LLMs for solving graph tasks.The essential idea is to transform graph data, including both graph structures and features, as well as graph tasks, into natural language representations, thereby treating graph problems as regular NLP problems.In the following discussion, we first provide a brief overview of representative methods, and then provide detailed comparisons of different methods.</p>
<p>NLGraph [90] conducts a systematic evaluation of LLMs, such as GPT-3 and GPT-4, on eight graph reasoning tasks in natural language, spanning varying levels of complexity, including connectivity, shortest path, maximum flow, simulating GNNs, etc.It empirically finds that LLMs show preliminary graph reasoning abilities, but struggle with more complex graph problems, potentially because they solely capture spurious correlations within the problem settings.Meanwhile, GPT4Graph [91] also conducts extensive experiments to evaluate the graph understanding capabilities of LLMs across ten distinct tasks, such as graph size and degree detection, neighbor and attribute retrieval, etc.It reveals the limitations of LLMs in graph reasoning and emphasizes the necessity of enhancing their structural understanding capabilities.LLMtoGraph [92] also tests GPT-3.5 and GPT-4 for various graph tasks and makes some interesting observations.</p>
<p>More recently, Graph-LLM [93] systematically investigates the utilization of LLMs in text-attributed graph through two strategies: LLMs-as-Enhancers, where LLMs enhance the representations of text attributes of nodes before passing them to GNNs, and LLMs-as-Predictors, where LLMs are directly employed as predictors.Comprehensive studies have been conducted on these two pipelines across various settings, and the empirical results provide valuable insights into further leveraging LLMs for graph machine learning.InstructGLM [94] further introduces scalable prompts designed to describe the graph structures and features for LLM instruction tuning, which enables tuned LLMs to perform various graph tasks during the inference stage in a generative manner.Experiments conducted on GNN benchmarks empirically show the strong potential of adopting LLMs for graph machine learning.</p>
<p>Next, we summarize and compare different models related to LLMs as graph models.The overall summarization is shown in Table 1.Specifically, we category the key features into three groups: model architectures, modeling Graph structure for LLMs, and graph data.</p>
<p>For model architectures, we summarize the following designs:</p>
<p>• Final Predictor: whether the model utilizes GNNs or LLMs to get the final prediction.</p>
<p>• LLMs: which LLMs are utilized in the model.Typical examples include GPT-3 [1], GPT-4 [29], Llama 2 [95], etc. • Need Fine-tuning: whether the model needs to be fine-tuned.Note that if the model does not necessarily require fine-tuning, but could be fine-tuned to further improve the performance, we mark it as no.Note that close-sourced LLMs such as GPT-3 and GPT-4 are not tunable.• Tunable Components: which parts of the model can be fine-tuned, such as GNNs, graph Transformers, and LLMs.• Receptive Field: how many hops of neighbors can be perceived when making predictions.K-hop indicates the receptive field is determined by the architectures, e.g., the number of layers in GNNs.</p>
<p>One key challenge of using LLMs as graph models is to model graph structures and inject them into LLMs.As this is usually achieved through prompts, we make the following summarization:</p>
<p>• Prompt Type: whether the model uses textual prompts (i.e., descriptions in texts) or neural prompts (e.g., through hidden layers in neural networks).• Prompt Details: the details of the prompt.Common textual prompts include adjacency lists and neighborhood descriptions, and typical neural prompts include GNNs and graph Transformers.• Advanced graph-specific prompt: the types of advanced graph-specific prompts, if they are proposed in the paper.</p>
<p>Lastly, we summarize the graph data used in the experiments.Note that we focus on the experiments conducted in the original papers, but extensions are possible, e.g., handling larger graphs by using more computational resources or applying the model to other tasks through minor modifications.</p>
<p>• Dataset Type: what type of graphs are utilized in the experiments, including synthetic graphs, TAGs, knowledge graphs (KGs), and general graphs.• Tasks: what type of tasks are considered in the experiments, including algorithmic tasks (various from degree counting to finding shortest paths, etc.), node classification, link prediction, question answering (QA), etc. • #Nodes: the approximate number of nodes handled by the model.If sampling is applied, we only count the sampled nodes.• Node Feature: whether and what type of node features can be utilized in the model, including no attributes, text attributes, and general attributes.</p>
<p>Although still in their early stages, these works highlight that LLMs also represent a promising avenue for developing large graph models, which is worthy further exploration and investigation.</p>
<p>Summary</p>
<p>To summarize, substantial research efforts have been devoted to studying various aspects of graph models.However, there is currently no clear framework for effectively integrating these techniques into large graph models.Consequently, more efforts are required to compare existing methods and develop advanced models.In this endeavor, automated graph machine learning techniques [111], such as graph neural architecture search, can be valuable in reducing human effort and accelerating the trial-and-error process.</p>
<p>Applications</p>
<p>Instead of attempting to overwhelmingly handle various graph domains and tasks, it may be more effective to focus on specific graph-related vertical fields by leveraging domain knowledge and  Text domain-specific datasets.In this section, we highlight several graph application scenarios that can significantly benefit from large graph models.</p>
<p>Recommendation System</p>
<p>Graph data naturally exists in recommendation systems.For example, the interaction between users and items can be modeled as a bipartite graph or more complex heterogeneous graphs that include clicks, buys, reviews, and more.Currently, LLMs for recommendation systems focus on modeling semantic information [112], while explicitly utilizing the structural information of graphs has the potential to yield better results [113].A potential challenge is that graphs in recommendation system are usually multi-modal [114], covering text, images, interactions, etc.Since large models for multimodal data are not yet mature, significant efforts are needed to develop truly effective large graph models for recommendation systems.</p>
<p>Knowledge Graph</p>
<p>Knowledge graphs are widely adopted to store and utilize ubiquitous knowledge in human society.LLMs have been used for various knowledge graph tasks [115,94], including construction, completion, and question answering.Despite their achievements, most of these methods focus primarily on the textual information, leaving the structural and relational information of knowledge graphs under-explored.Large graph models, potentially combined with existing LLMs, can greatly complement the status quo and further promote research and application of knowledge graphs.</p>
<p>Molecules</p>
<p>Graphs are natural representations for molecules, where nodes represent atoms and edges indicate bonds.Building effective graph models for molecules can advance various applications, including molecular property prediction and molecular dynamics simulations, ultimately benefiting drug discovery.Currently, some variants of LLMs are applied to molecules [116,117] by first transforming molecules into strings using SMILES [118], which allows molecules to be represented and generated as regular texts.Nevertheless, graphs serve as a more natural way to represent the structural information of molecules with numerous modeling advantages [119].Meanwhile, a great number of graph-based pre-training techniques have also been developed for molecules [120], including multi-modal strategies [121].Besides, molecule data is relatively easier to collect, e.g., ZINC20 [122] contains millions of purchasable compounds.Therefore, we believe graph-based or graph-enhanced large models for molecule modeling can soon to be expected.</p>
<p>Finance</p>
<p>Graph machine learning has proven to be beneficial for multiple financial tasks such as stock movement prediction and loan risk prediction [123].Moreover, the large abundance of financial data makes it possible to construct domain-specific large models, exemplified by BloombergGPT [124].By combining the strengths of both worlds, the application of large graph models in the field of finance holds great promise.A potential challenge lies in the sensitive and private nature of most financial data, making industries reluctant to release related models and data to the public.Efforts are required to promote open-source initiatives and democratization [125,126] to fully unleash the potential of large graph models in the finance area.</p>
<p>Code and Program</p>
<p>Thanks to the large amount of code data available on repository hosting platforms such as GitHub, LLMs show remarkable ability in understanding and generating codes and programs.Notable examples include CodeX [127], AlphaCode [128], and GPT-4 [29], which have exerted a significant impact on the programming landscape, potentially even reshaping it.In addition to treating codes and programs as textual data, graphs offer a natural means to represent the structural aspects of codes.For example, abstract syntax trees, including control flow graph, data flow graph, etc., effectively capture the syntactic structure of source codes [129].Studies have demonstrated that the integration of graphs can further enhance the performance of LLMs by providing complementary information [130].Therefore, large graph models hold valuable potential for a wide range of code and program-related tasks, including code completion and generation, code search, code review, program analysis and testing, among others.</p>
<p>Urban Computing and Transportation</p>
<p>Graph data is pervasive in the domains of urban computing and transportation, such as road networks.Therefore, graph machine learning can benefit many applications, including traffic forecasting, various urban planning and management tasks, crime prediction, and epidemic control [131,132].Moreover, large-scale urban data naturally exists, such as mobility data collected from GPS and diverse sensors.Currently, some LLM-based large models have been explored for urban computing and transportation, such as TransGPT [133].Nevertheless, their focus has primarily revolved around natural language related applications, leaving developing large graph models for broader and more comprehensive utilization still an open opportunity.One major technical challenge in the process lies in that graph data in urban and transportation contexts is dynamic in nature, containing complicated spatial-temporal patterns.Thus, a large graph model needs to effectively capture both structural and temporal information to achieve satisfactory performance.</p>
<p>Beyond</p>
<p>The application scenarios we have outlined above are by no means exhaustive.Considering that graph machine learning has been widely adopted across diverse domains ranging from industrial applications, such as fault diagnosis [134], IoT [135], power systems [136], and time-series analysis [137], to AI for science [138], such as physics [139,140], combinatorial optimization [141], material science [142], and neural science [143], exploring the usage of large graph models holds extremely rich potentials.</p>
<p>Conclusion</p>
<p>In summary, large graph models can potentially revolutionize the field of graph machine learning, but they also give rise to a multitude of challenges, ranging from the representation basis, graph data, graph models, and applications.Meanwhile, promising endeavors are being undertaken to tackle these challenges, creating exciting opportunities for both researchers and practitioners.We hope that our perspective will inspire continued efforts and advancements for large graph models.</p>
<p>Figure 1 :
1
Figure 1: An illustration of desired characteristics of a large graph model.</p>
<p>Table 1 :
1
A summarization of different models related to LLMs as graph models.
NodeFeature# NodesGraph DataTasksDatasetTypeModeling Graph Structure for LLMsPrompt Details Advanced Graph-specific PromptPromptTypeReceptiveFieldTunableComponentsArchitectureNeedFinetuneLLMsFinalPredictorMethodNLGraph [90]
There are also works to use graphs to improve large language models, such as enhancing their reasoning ability[7,8,9,10,11,12,13] or using graphs as tools[14,15], which is beyond the scope of this paper.
AcknowledgementThis work was supported by the National Key Research and Development Program of China No. 2020AAA0106300, National Natural Science Foundation of China (No. 62222209, 62250008, 62102222, 62206149), Beijing National Research Center for Information Science and Technology under Grant No. BNR2023RC01003, BNR2023TD03006, China National Postdoctoral Program for Innovative Talents No. BX20220185, China Postdoctoral Science Foundation No. 2022M711813, and Beijing Key Lab of Networked Multimedia.All opinions, findings, conclusions, and recommendations in this paper are those of the authors and do not necessarily reflect the views of the funding agencies.Xin Wang and Wenwu Zhu are corresponding authors.Conference of the North American Chapter of the Association for Computational Linguistics, pages 4171-4186, 2019.
Language models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam Mccandlish, Alec Radford, Ilya Sutskever, Dario Amodei, Advances in Neural Information Processing Systems. 2020</p>
<p>Bert: Pre-training of deep bidirectional transformers for language understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, Proceedings of the. the2019</p>
<p>Kun Wayne Xin Zhao, Junyi Zhou, Tianyi Li, Xiaolei Tang, Yupeng Wang, Yingqian Hou, Beichen Min, Junjie Zhang, Zican Zhang, Dong, arXiv:2303.18223A survey of large language models. 2023arXiv preprint</p>
<p>. Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, Piotr Dollár, Ross Girshick, arXiv:2304.026432023Segment anything. arXiv preprint</p>
<p>Learning transferable visual models from natural language supervision. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, International conference on machine learning. 2021</p>
<p>High-resolution image synthesis with latent diffusion models. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, Björn Ommer, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2022</p>
<p>Jiashuo Sun, Chengjin Xu, Lumingyuan Tang, Saizhuo Wang, Chen Lin, Yeyun Gong, Heung-Yeung Shum, Jian Guo, arXiv:2307.07697Think-on-graph: Deep and responsible reasoning of large language model with knowledge graph. 2023arXiv preprint</p>
<p>Beyond chain-of-thought, effective graph-of-thought reasoning in large language models. Yao Yao, Zuchao Li, Hai Zhao, arXiv:2305.165822023arXiv preprint</p>
<p>Thinking like an expert: Multimodal hypergraph-of-thought (hot) reasoning to boost foundation modals. Fanglong Yao, Changyuan Tian, Jintao Liu, Zequn Zhang, Qing Liu, Li Jin, Shuchao Li, Xiaoyu Li, Xian Sun, arXiv:2308.062072023arXiv preprint</p>
<p>Enhancing reasoning capabilities of large language models: A graph-based verification approach. Lang Cao, arXiv:2308.092672023arXiv preprint</p>
<p>Boosting logical reasoning in large language models through a new framework: The graph of thought. Bin Lei, Hung Pei, Chunhua Lin, Caiwen Liao, Ding, arXiv:2308.086142023arXiv preprint</p>
<p>Graph of thoughts: Solving elaborate problems with large language models. Maciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Lukas Gianinazzi, Joanna Gajda, Tomasz Lehmann, Michal Podstawski, Hubert Niewiadomski, Piotr Nyczyk, Torsten Hoefler, arXiv:2308.096872023arXiv preprint</p>
<p>Mindmap: Knowledge graph prompting sparks graph of thoughts in large language models. Yilin Wen, Zifeng Wang, Jimeng Sun, arXiv:2308.097292023arXiv preprint</p>
<p>Graph-toolformer: To empower llms with graph reasoning ability via prompt augmented by chatgpt. Jiawei Zhang, arXiv:2304.111162023arXiv preprint</p>
<p>Jinhao Jiang, Kun Zhou, Zican Dong, Keming Ye, Wayne Xin Zhao, Ji-Rong Wen, arXiv:2305.09645Structgpt: A general framework for large language model to reason over structured data. 2023arXiv preprint</p>
<p>Jared Kaplan, Sam Mccandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, Dario Amodei, arXiv:2001.08361Scaling laws for neural language models. 2020arXiv preprint</p>
<p>Emergent abilities of large language models. Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, William Fedus, Transactions on Machine Learning Research. 2022</p>
<p>A systematic survey on deep generative models for graph generation. Xiaojie Guo, Liang Zhao, IEEE Transactions on Pattern Analysis and Machine Intelligence. 4552023</p>
<p>Few-shot learning on graphs. Chuxu Zhang, Kaize Ding, Jundong Li, Xiangliang Zhang, Yanfang Ye, Nitesh V Chawla, Huan Liu, 31st International Joint Conference on Artificial Intelligence, IJCAI 2022. 2022</p>
<p>Multi-task self-supervised graph neural networks enable stronger task generalization. Mingxuan Ju, Tong Zhao, Qianlong Wen, Wenhao Yu, Neil Shah, Yanfang Ye, Chuxu Zhang, The Eleventh International Conference on Learning Representations. 2022</p>
<p>Out-of-distribution generalization on graphs: A survey. Haoyang Li, Xin Wang, Ziwei Zhang, Wenwu Zhu, arXiv:2202.079872022arXiv preprint</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Denny Quoc V Le, Zhou, Advances in Neural Information Processing Systems. 202235</p>
<p>Explainability in graph neural networks: A taxonomic survey. Haiyang Hao Yuan, Shurui Yu, Shuiwang Gui, Ji, IEEE transactions on pattern analysis and machine intelligence. 202245</p>
<p>Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Michael S Sydney Von Arx, Jeannette Bernstein, Antoine Bohg, Emma Bosselut, Erik Brunskill, Shyamal Brynjolfsson, Dallas Buch, Rodrigo Card, Niladri Castellon, Annie Chatterji, Kathleen Chen, Jared Quincy Creel, Dora Davis, Chris Demszky, Moussa Donahue, Esin Doumbouya, Stefano Durmus, John Ermon, Kawin Etchemendy, Li Ethayarajh, Chelsea Fei-Fei, Trevor Finn, Lauren Gale, Karan Gillespie, Noah Goel, Shelby Goodman, Neel Grossman, Tatsunori Guha, Peter Hashimoto, John Henderson, Daniel E Hewitt, Jenny Ho, Kyle Hong, Jing Hsu, Thomas Huang, Saahil Icard, Dan Jain, Pratyusha Jurafsky, Siddharth Kalluri, Geoff Karamcheti, Fereshte Keeling, Omar Khani, Pang Wei Khattab, Mark Koh, Ranjay Krass, Rohith Krishna, Ananya Kuditipudi, Faisal Kumar, Mina Ladhak, Tony Lee, Jure Lee, Isabelle Leskovec, Levent, Lisa Xiang, Xuechen Li, Tengyu Li, Ali Ma, Christopher D Malik, Suvir Manning, Eric Mirchandani, Zanele Mitchell, Suraj Munyikwa, Avanika Nair, Deepak Narayan, Ben Narayanan, Allen Newman, Juan Carlos Nie, Hamed Niebles, Julian Nilforoshan, Giray Nyarko, Andy Ogut, Krishnan Shih, Alex Srinivasan, Rohan Tamkin, Armin W Taori, Florian Thomas, Rose E Tramèr, William Wang, Bohan Wang, Jiajun Wu, Yuhuai Wu, Sang Wu, Michihiro Michael Xie, Jiaxuan Yasunaga, Matei You, Michael Zaharia, Tianyi Zhang, Xikun Zhang, Yuhui Zhang, Lucia Zhang, Kaitlyn Zheng, Percy Zhou, Liang, arXiv:2303.18223On the opportunities and risks of foundation models. Chris Piech, Eva Portelance, Christopher Potts, Aditi Raghunathan, Rob Reich, Hongyu Ren, Frieda Rong, Yusuf Roohani, Camilo Ruiz, Jack Ryan, Christopher Ré, Dorsa Sadigh, Shiori Sagawa, Keshav Santhanam, Laurel Orr, Isabel Papadimitriou2022arXiv preprintJoon Sung Park</p>
<p>. Mark Newman, Networks, 2018Oxford university press</p>
<p>Explanations as features: Llm-based features for text-attributed graphs. Xiaoxin He, Xavier Bresson, Thomas Laurent, Bryan Hooi, arXiv:2305.195232023arXiv preprint</p>
<p>Qian Keyu Duan, Tat-Seng Liu, Shuicheng Chua, Wei Tsang Yan, Qizhe Ooi, Junxian Xie, He, arXiv:2308.02565Simteg: A frustratingly simple approach improves textual graph learning. 2023arXiv preprint</p>
<p>Petar Veličković, Charles Blundell, Neural algorithmic reasoning. Patterns. 20212100273</p>
<p>arXiv:2303.08774OpenAI. Gpt-4 technical report. 2023arXiv preprint</p>
<p>Open graph benchmark: Datasets for machine learning on graphs. Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, Michele Catasta, Jure Leskovec, Advances in neural information processing systems. 202033</p>
<p>Superglue: A stickier benchmark for general-purpose language understanding systems. Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, Samuel Bowman, Advances in neural information processing systems. 322019</p>
<p>Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. BIG bench authors. 2023</p>
<p>Imagenet: A largescale hierarchical image database. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, Li Fei-Fei, 2009 IEEE conference on computer vision and pattern recognition. 2009</p>
<p>Benchmarking graph neural networks. Vijay Prakash Dwivedi, Chaitanya K Joshi, Anh Tuan Luu, Thomas Laurent, Yoshua Bengio, Xavier Bresson, Journal of Machine Learning Research. 24432023</p>
<p>Advances in neural information processing systems. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, Illia Polosukhin, 201730Attention is all you need</p>
<p>Deep learning on graphs: A survey. Ziwei Zhang, Peng Cui, Wenwu Zhu, IEEE Transactions on Knowledge and Data Engineering. 3412020</p>
<p>A survey on oversmoothing in graph neural networks. Konstantin Rusch, Michael Bronstein, Siddhartha Mishra, 2023. 2023SAM Research Report</p>
<p>Understanding over-squashing and bottlenecks on graphs via curvature. Jake Topping, Francesco Di Giovanni, Benjamin Paul Chamberlain, Xiaowen Dong, Michael M Bronstein, International Conference on Learning Representations. 2022</p>
<p>Transformer for graphs: An overview from architecture perspective. Erxue Min, Runfa Chen, Yatao Bian, Tingyang Xu, Kangfei Zhao, Wenbing Huang, Peilin Zhao, Junzhou Huang, Sophia Ananiadou, Yu Rong, arXiv:2202.084552022arXiv preprint</p>
<p>Autogt: Automated graph transformer architecture search. Zizhao Zhang, Xin Wang, Chaoyu Guan, Ziwei Zhang, Haoyang Li, Wenwu Zhu, The Eleventh International Conference on Learning Representations. 2023</p>
<p>Do transformers really perform badly for graph representation?. Chengxuan Ying, Tianle Cai, Shengjie Luo, Shuxin Zheng, Guolin Ke, Di He, Yanming Shen, Tie-Yan Liu, Advances in Neural Information Processing Systems. 342021</p>
<p>Ogb-lsc: A large-scale challenge for machine learning on graphs. Weihua Hu, Matthias Fey, Hongyu Ren, Maho Nakata, Yuxiao Dong, Jure Leskovec, Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track. 2021</p>
<p>Structure-aware transformer for graph representation learning. Dexiong Chen, O' Leslie, Karsten Bray, Borgwardt, Proceedings of the 39th International Conference on Machine Learning. the 39th International Conference on Machine Learning2022162</p>
<p>Recipe for a general, powerful, scalable graph transformer. Ladislav Rampášek, Michael Galkin, Vijay Prakash Dwivedi, Anh Tuan Luu, Guy Wolf, Dominique Beaini, Advances in Neural Information Processing Systems. 202235</p>
<p>NAGphormer: A tokenized graph transformer for node classification in large graphs. Jinsong Chen, Kaiyuan Gao, Gaichao Li, Kun He, The Eleventh International Conference on Learning Representations. 2023</p>
<p>On structural expressive power of graph transformers. Wenhao Zhu, Tianyu Wen, Guojie Song, Liang Wang, Bo Zheng, Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining2023</p>
<p>How expressive are transformers in spectral domain for graphs?. Anson Bastos, Abhishek Nadgeri, Kuldeep Singh, Hiroki Kanezashi, Toyotaro Suzumura, Isaiah Onando, Mulang , Transactions on Machine Learning Research. 2022</p>
<p>Transformers are graph neural networks. The Gradient. Chaitanya Joshi, 2020</p>
<p>Everything is connected: Graph neural networks. Petar Veličković, Current Opinion in Structural Biology. 791025382023</p>
<p>Attending to graph transformers. Luis Müller, Mikhail Galkin, Christopher Morris, Ladislav Rampášek, arXiv:2302.041812023arXiv preprint</p>
<p>Pure transformers are powerful graph learners. Jinwoo Kim, Dat Nguyen, Seonwoo Min, Sungjun Cho, Moontae Lee, Honglak Lee, Seunghoon Hong, Advances in Neural Information Processing Systems. 202235</p>
<p>Improving language understanding by generative pre-training. Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, 2018</p>
<p>Graph self-supervised learning: A survey. Yixin Liu, Ming Jin, Shirui Pan, Chuan Zhou, Yu Zheng, Feng Xia, Philip Yu, IEEE Transactions on Knowledge and Data Engineering. 3562022</p>
<p>Self-supervised learning on graphs: Contrastive, generative, or predictive. Lirong Wu, Haitao Lin, Cheng Tan, Zhangyang Gao, Stan Z Li, IEEE Transactions on Knowledge and Data Engineering. 2021</p>
<p>Using pre-training can improve model robustness and uncertainty. Dan Hendrycks, Kimin Lee, Mantas Mazeika, International conference on machine learning. 2019</p>
<p>Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing. Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, Graham Neubig, ACM Computing Surveys. 5592023</p>
<p>Parameter-efficient fine-tuning of large-scale pre-trained language models. Ning Ding, Yujia Qin, Guang Yang, Fuchao Wei, Zonghan Yang, Yusheng Su, Shengding Hu, Yulin Chen, Chi-Min Chan, Weize Chen, Nature Machine Intelligence. 532023</p>
<p>Training language models to follow instructions with human feedback. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, Advances in Neural Information Processing Systems. 202235</p>
<p>Xunyu Zhu, Jian Li, Yong Liu, Can Ma, Weiping Wang, arXiv:2308.07633A survey on model compression for large language models. 2023arXiv preprint</p>
<p>Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, Zhifang Sui, arXiv:2301.00234A survey for in-context learning. 2022arXiv preprint</p>
<p>Graphprompt: Unifying pretraining and downstream tasks for graph neural networks. Zemin Liu, Xingtong Yu, Yuan Fang, Xinming Zhang, Proceedings of the ACM Web Conference 2023. the ACM Web Conference 20232023</p>
<p>Gppt: Graph pre-training and prompt tuning to generalize graph neural networks. Mingchen Sun, Kaixiong Zhou, Xin He, Ying Wang, Xin Wang, Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining2022</p>
<p>All in one: Multi-task prompting for graph neural networks. Xiangguo Sun, Hong Cheng, Jia Li, Bo Liu, Jihong Guan, Proceedings of the 26th ACM SIGKDD international conference on knowledge discovery &amp; data mining. the 26th ACM SIGKDD international conference on knowledge discovery &amp; data mining2023</p>
<p>Prodigy: Enabling in-context learning over graphs. Qian Huang, Hongyu Ren, Peng Chen, Gregor Kržmanc, Daniel Zeng, Percy Liang, Jure Leskovec, Thirty-seventh Conference on Neural Information Processing Systems. 2023</p>
<p>Universal prompt tuning for graph neural networks. Taoran Fang, Yunchao Zhang, Yang Yang, Chunping Wang, Lei Chen, Thirty-seventh Conference on Neural Information Processing Systems. 2023</p>
<p>Deep graph reprogramming. Yongcheng Jing, Chongbin Yuan, Li Ju, Yiding Yang, Xinchao Wang, Dacheng Tao, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2023</p>
<p>Sgl-pt: A strong graph learner with graph prompt tuning. Yun Zhu, Jianhao Guo, Siliang Tang, arXiv:2302.124492023arXiv preprint</p>
<p>Deep prompt tuning for graph transformers. Reza Shirkavand, Heng Huang, arXiv:2309.101312023arXiv preprint</p>
<p>Prompt-based node feature extractor for few-shot learning on text-attributed graphs. Xuanwen Huang, Kaiqiao Han, Dezheng Bao, Quanjin Tao, Zhisheng Zhang, Yang Yang, Qi Zhu, arXiv:2309.028482023arXiv preprint</p>
<p>Chain of propagation prompting for node classification. Yonghua Zhu, Zhenyun Deng, Yang Chen, Robert Amor, Michael Witbrock, Proceedings of the 31st ACM International Conference on Multimedia. the 31st ACM International Conference on Multimedia2023</p>
<p>Structure pretraining and prompt tuning for knowledge graph transfer. Wen Zhang, Yushan Zhu, Mingyang Chen, Yuxia Geng, Yufeng Huang, Yajing Xu, Wenting Song, Huajun Chen, Proceedings of the ACM Web Conference 2023. the ACM Web Conference 20232023</p>
<p>Enhancing graph neural networks with structure-based prompt. Qingqing Ge, Zeyuan Zhao, Yiding Liu, Anfeng Cheng, Xiang Li, Shuaiqiang Wang, Dawei Yin, arXiv:2310.173942023arXiv preprint</p>
<p>Hetgpt: Harnessing the power of prompt tuning in pre-trained heterogeneous graph neural networks. Yihong Ma, Ning Yan, Jiayu Li, Masood Mortazavi, Nitesh V Chawla, arXiv:2310.153182023arXiv preprint</p>
<p>Ultra-dp: Unifying graph pre-training with multi-task graph dual prompt. Mouxiang Chen, Zemin Liu, Chenghao Liu, Jundong Li, Qiheng Mao, Jianling Sun, arXiv:2310.148452023arXiv preprint</p>
<p>Prompt tuning for multi-view graph contrastive learning. Chenghua Gong, Xiang Li, Jianxiang Yu, Cheng Yao, Jiaqi Tan, Chengcheng Yu, Dawei Yin, arXiv:2310.103622023arXiv preprint</p>
<p>Prompt-based zero-and few-shot node classification: A multimodal approach. Yuexin Li, Bryan Hooi, arXiv:2307.115722023arXiv preprint</p>
<p>Adaptergnn: Efficient delta tuning improves generalization ability in graph neural networks. Shengrui Li, Xueting Han, Jing Bai, arXiv:2304.095952023arXiv preprint</p>
<p>G-adapter: Towards structure-aware parameterefficient transfer learning for graph transformer networks. Anchun Gui, Jinqiang Ye, Han Xiao, arXiv:2305.103292023arXiv preprint</p>
<p>Search to fine-tune pre-trained graph neural networks for graph-level tasks. Zhili Wang, Shimin Di, Lei Chen, Xiaofang Zhou, arXiv preprint:2308.069602023</p>
<p>Knowledge distillation on graphs: A survey. Yijun Tian, Shichao Pei, Xiangliang Zhang, Chuxu Zhang, Nitesh V Chawla, arXiv:2302.002192023arXiv preprint</p>
<p>Shichang Zhang, Atefeh Sohrabizadeh, Cheng Wan, Zijie Huang, Ziniu Hu, Yewen Wang, Yingyan, Jason Lin, Yizhou Cong, Sun, arXiv:2306.14052A survey on graph neural network acceleration: Algorithms, systems, and customized hardware. 2023arXiv preprint</p>
<p>Computing graph neural networks: A survey from algorithms to accelerators. Akshay Sergi Abadal, Robert Jain, Jorge Guirado, Eduard López-Alonso, Alarcón, ACM Computing Surveys. 5492021</p>
<p>Sgquant: Squeezing the last bit on graph neural networks with specialized quantization. Boyuan Feng, Yuke Wang, Xu Li, Shu Yang, Xueqiao Peng, Yufei Ding, 2020 IEEE 32nd International Conference on Tools with Artificial Intelligence. 2020</p>
<p>Degree-quant: Quantization-aware training for graph neural networks. Javier Shyam Anil Tailor, Nicholas Fernandez-Marques, Lane Donald, International Conference on Learning Representations. 2020</p>
<p>Yuxin Ma, Ping Gong, Jun Yi, Zhewei Yao, Cheng Li, Yuxiong He, Feng Yan, Bifeat, arXiv:2207.14696Supercharge gnn training via graph feature quantization. 2022arXiv preprint</p>
<p>Tango: rethinking quantization for graph neural network training on gpus. Shiyang Chen, Da Zheng, Caiwen Ding, Chengying Huan, Yuede Ji, Hang Liu, arXiv:2308.008902023arXiv preprint</p>
<p>Vqgraph: Graph vector-quantization for bridging gnns and mlps. Ling Yang, Ye Tian, Minkai Xu, Zhongyi Liu, Shenda Hong, Wei Qu, Wentao Zhang, Bin Cui, Muhan Zhang, Jure Leskovec, arXiv:2308.021172023arXiv preprint</p>
<p>q: Aggregation-aware quantization for graph neural networks. Zeyu Zhu, Fanrong Li, Zitao Mo, Qinghao Hu, Gang Li, Zejian Liu, Xiaoyao Liang, Jian Cheng, The Eleventh International Conference on Learning Representations. 2022</p>
<p>Adaptive message quantization and parallelization for distributed full-graph gnn training. Juntao Borui Wan, Chuan Zhao, Wu, Proceedings of Machine Learning and Systems. Machine Learning and Systems20235</p>
<p>Can language models solve graph problems in natural language. Heng Wang, Shangbin Feng, Tianxing He, Zhaoxuan Tan, Xiaochuang Han, Yulia Tsvetkov, Thirty-seventh Conference on Neural Information Processing Systems. 2023</p>
<p>Gpt4graph: Can large language models understand graph structured data? an empirical evaluation and benchmarking. Jiayan Guo, Lun Du, Hengyu Liu, arXiv:2305.150662023arXiv preprint</p>
<p>Evaluating large language models on graphs: Performance insights and comparative analysis. Chang Liu, Bo Wu, arXiv:2308.112242023arXiv preprint</p>
<p>Exploring the potential of large language models (llms) in learning on graphs. Zhikai Chen, Haitao Mao, Hang Li, Wei Jin, Hongzhi Wen, Xiaochi Wei, Shuaiqiang Wang, Dawei Yin, Wenqi Fan, Hui Liu, Jiliang Tang, arXiv:2307.033932023arXiv preprint</p>
<p>Natural language is all a graph needs. Ruosong Ye, Caiqi Zhang, Runhui Wang, Shuyuan Xu, Yongfeng Zhang, arXiv:2308.071342023arXiv preprint</p>
<p>Llama 2: Open foundation and fine-tuned chat models. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, arXiv:2307.092882023arXiv preprint</p>
<p>Can llms effectively leverage graph structural information: When and why. Jin Huang, Xingjian Zhang, Qiaozhu Mei, Jiaqi Ma, arXiv:2309.165952023arXiv preprint</p>
<p>One for all: Towards training one graph model for all classification tasks. Hao Liu, Jiarui Feng, Lecheng Kong, Ningyue Liang, Dacheng Tao, Yixin Chen, Muhan Zhang, arXiv:2310.001492023arXiv preprint</p>
<p>Jianan Zhao, Le Zhuo, Yikang Shen, Meng Qu, Kai Liu, Michael Bronstein, Zhaocheng Zhu, Jian Tang, Graphtext, arXiv:2310.01089Graph reasoning in text space. 2023arXiv preprint</p>
<p>Talk like a graph: Encoding graphs for large language models. Bahare Fatemi, Jonathan Halcrow, Bryan Perozzi, arXiv:2310.045602023arXiv preprint</p>
<p>Beyond text: A deep dive into large language models' ability on understanding graph data. Yuntong Hu, Zheng Zhang, Liang Zhao, arXiv:2310.049442023arXiv preprint</p>
<p>Ziwei Chai, Tianjie Zhang, Liang Wu, Kaiqiao Han, Xiaohai Hu, Xuanwen Huang, Yang Yang, arXiv:2310.05845Graphllm: Boosting graph reasoning ability of large language model. 2023arXiv preprint</p>
<p>Label-free node classification on graphs with large language models (llms). Zhikai Chen, Haitao Mao, Hongzhi Wen, Haoyu Han, Wei Jin, Haiyang Zhang, Hui Liu, Jiliang Tang, arXiv:2310.046682023arXiv preprint</p>
<p>Empower text-attributed graphs learning with large language models (llms). Jianxiang Yu, Yuxiang Ren, Chenghua Gong, Jiaqi Tan, Xiang Li, Xuecang Zhang, arXiv:2310.098722023arXiv preprint</p>
<p>Pretrained language models to solve graph tasks in natural language. Frederik Wenkel, Guy Wolf, Boris Knyazev, ICML 2023 Workshop on Structured Probabilistic Inference &amp; Generative Modeling. 2023</p>
<p>Graphgpt: Graph instruction tuning for large language models. Jiabin Tang, Yuhao Yang, Wei Wei, Lei Shi, Lixin Su, Suqi Cheng, Dawei Yin, Chao Huang, arXiv:2310.130232023arXiv preprint</p>
<p>Llm4dyg: Can large language models solve problems on dynamic graphs?. Zeyang Zhang, Xin Wang, Ziwei Zhang, Haoyang Li, Yijian Qin, Simin Wu, Wenwu Zhu, arXiv:2310.171102023arXiv preprint</p>
<p>Disentangled representation learning with large language models for text-attributed graphs. Yijian Qin, Xin Wang, Ziwei Zhang, Wenwu Zhu, arXiv:2310.181522023arXiv preprint</p>
<p>Gpt-4 doesn't know it's wrong: An analysis of iterative prompting for reasoning problems. Kaya Stechly, Matthew Marquez, Subbarao Kambhampati, arXiv:2310.123972023arXiv preprint</p>
<p>Yijun Tian, Huan Song, Zichen Wang, Haozhu Wang, Ziqing Hu, Fang Wang, Nitesh V Chawla, Panpan Xu, arXiv:2309.15427Graph neural prompting with large language models. 2023arXiv preprint</p>
<p>Linhao Luo, Yuan-Fang Li, Gholamreza Haffari, Shirui Pan, arXiv:2310.01061Reasoning on graphs: Faithful and interpretable large language model reasoning. 2023arXiv preprint</p>
<p>Automated machine learning on graphs: A survey. Ziwei Zhang, Xin Wang, Wenwu Zhu, Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, IJCAI-21. the Thirtieth International Joint Conference on Artificial Intelligence, IJCAI-212021</p>
<p>A survey on large language models for recommendation. Likang Wu, Zhi Zheng, Zhaopeng Qiu, Hao Wang, Hongchao Gu, Tingjia Shen, Chuan Qin, Chen Zhu, Hengshu Zhu, Qi Liu, arXiv:2305.198602023arXiv preprint</p>
<p>Graph neural networks in recommender systems: a survey. Shiwen Wu, Fei Sun, Wentao Zhang, Xu Xie, Bin Cui, ACM Computing Surveys. 5552022</p>
<p>Mgat: Multimodal graph attention network for recommendation. Zhulin Tao, Yinwei Wei, Xiang Wang, Xiangnan He, Xianglin Huang, Tat-Seng Chua, Information Processing &amp; Management. 5751022772020</p>
<p>Unifying large language models and knowledge graphs: A roadmap. Linhao Shirui Pan, Yufei Luo, Chen Wang, Jiapu Chen, Xindong Wang, Wu, arXiv:2306.083022023arXiv preprint</p>
<p>Molgpt: molecular generation using a transformer-decoder model. Viraj Bagal, Rishal Aggarwal, Deva Vinod, Priyakumar, Journal of Chemical Information and Modeling. 6292021</p>
<p>Can large language models empower molecular property prediction?. Chen Qian, Huayi Tang, Zhirui Yang, Hong Liang, Yong Liu, arXiv:2307.074432023arXiv preprint</p>
<p>Smiles, a chemical language and information system. 1. introduction to methodology and encoding rules. David Weininger, Journal of chemical information and computer sciences. 2811988</p>
<p>A compact review of molecular property prediction with graph neural networks. Oliver Wieder, Stefan Kohlbacher, Mélaine Kuenemann, Arthur Garon, Pierre Ducrot, Thomas Seidel, Thierry Langer, Drug Discovery Today: Technologies. 372020</p>
<p>A systematic survey of molecular pre-trained models. Jun Xia, Yanqiao Zhu, Yuanqi Du, Yue Liu, Stan Z Li, arXiv:2210.164842022arXiv preprint</p>
<p>Git-mol: A multi-modal large language model for molecular science with graph, image, and text. Pengfei Liu, Yiming Ren, Zhixiang Ren, arXiv:2308.069112023arXiv preprint</p>
<p>Zinc20-a free ultralarge-scale chemical database for ligand discovery. Khanh G John J Irwin, Jennifer Tang, Chinzorig Young, Dandarchuluun, Munkhzul Benjamin R Wong, Khurelbaatar, S Yurii, John Moroz, Roger A Mayfield, Sayle, Journal of chemical information and modeling. 60122020</p>
<p>Jianian Wang, Sheng Zhang, Yanghua Xiao, Rui Song, arXiv:2111.15367A review on graph neural network methods in financial applications. 2022arXiv preprint</p>
<p>Shijie Wu, Ozan Irsoy, Steven Lu, Vadim Dabravolski, Mark Dredze, Sebastian Gehrmann, Prabhanjan Kambadur, David Rosenberg, Gideon Mann, arXiv:2303.17564Bloomberggpt: A large language model for finance. 2023arXiv preprint</p>
<p>Fingpt: Democratizing internet-scale data for financial large language models. Xiao-Yang Liu, Guoxuan Wang, Daochen Zha, arXiv:2307.104852023arXiv preprint</p>
<p>Hongyang Yang, Xiao-Yang Liu, Christina Dan Wang, arXiv:2306.06031Fingpt: Open-source financial large language models. 2023arXiv preprint</p>
<p>. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet ; Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, arXiv:2107.03374Felipe Petroski Such. Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech ZarembaJan Leike. 2021arXiv preprintEvaluating large language models trained on code</p>
<p>Competition-level code generation with alphacode. Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Rémi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, Thomas Hubert, Peter Choy, Cyprien De Masson D'autume, Igor Babuschkin, Xinyun Chen, Po-Sen Huang, Johannes Welbl, Sven Gowal, Alexey Cherepanov, James Molloy, Daniel J Mankowitz, Esme Sutherland Robson, Nando de Freitas, Koray Kavukcuoglu, and Oriol Vinyals. 2022378Science</p>
<p>Learning to represent programs with graphs. Miltiadis Allamanis, Marc Brockschmidt, Mahmoud Khademi, International Conference on Learning Representations. 2018</p>
<p>Graphcode{bert}: Pre-training code representations with data flow. Daya Guo, Shuo Ren, Shuai Lu, Zhangyin Feng, Duyu Tang, Liu Shujie, Long Zhou, Nan Duan, Alexey Svyatkovskiy, Shengyu Fu, Michele Tufano, Colin Shao Kun Deng, Dawn Clement, Neel Drain, Jian Sundaresan, Daxin Yin, Ming Jiang, Zhou, International Conference on Learning Representations. 2021</p>
<p>Graph neural networks for intelligent transportation systems: A survey. Saeed Rahmani, Asiye Baghbani, Nizar Bouguila, Zachary Patterson, IEEE Transactions on Intelligent Transportation Systems. 2482023</p>
<p>Guangyin Jin, Yuxuan Liang, Yuchen Fang, Jincai Huang, Junbo Zhang, Yu Zheng, arXiv:2303.14483Spatiotemporal graph neural networks for predictive learning in urban computing: A survey. 2023arXiv preprint</p>
<p>. Duomo, Transgpt, 2023</p>
<p>Graph neural network-based fault diagnosis: a review. Zhiwen Chen, Jiamin Xu, Cesare Alippi, Yuri Steven X Ding, Tao Shardt, Chunhua Peng, Yang, arXiv:2111.081852021arXiv preprint</p>
<p>Graph neural networks in iot: a survey. Guimin Dong, Mingyue Tang, Zhiyuan Wang, Jiechao Gao, Sikun Guo, Lihua Cai, Robert Gutierrez, Bradford Campbel, Laura E Barnes, Mehdi Boukhechba, ACM Transactions on Sensor Networks. 1922023</p>
<p>A review of graph neural networks and their applications in power systems. Wenlong Liao, Birgitte Bak-Jensen, Jayakrishnan Radhakrishna Pillai, Yuelong Wang, Yusen Wang, Journal of Modern Power Systems and Clean Energy. 1022021</p>
<p>Ming Jin, Yee Huan, Qingsong Koh, Daniele Wen, Cesare Zambon, Geoffrey I Alippi, Irwin Webb, Shirui King, Pan, arXiv:2307.03759A survey on graph neural networks for time series: Forecasting, classification, imputation, and anomaly detection. 2023arXiv preprint</p>
<p>Scientific discovery in the age of artificial intelligence. Hanchen Wang, Tianfan Fu, Yuanqi Du, Wenhao Gao, Kexin Huang, Ziming Liu, Payal Chandak, Shengchao Liu, Peter Van Katwyk, Andreea Deac, Nature. 62079722023</p>
<p>Graph neural networks in particle physics. Jonathan Shlomi, Peter Battaglia, Jean-Roch Vlimant, Machine Learning: Science and Technology. 22210012020</p>
<p>Graph neural networks at the large hadron collider. Gage Dezoort, Peter W Battaglia, Catherine Biscarat, Jean-Roch Vlimant, Nature Reviews Physics. 2023</p>
<p>Combinatorial optimization and reasoning with graph neural networks. Quentin Cappart, Didier Chételat, Elias B Khalil, Andrea Lodi, Christopher Morris, Petar Velickovic, Journal of Machine Learning Research. 242023</p>
<p>Graph neural networks for materials science and chemistry. Patrick Reiser, Marlen Neubert, André Eberhard, Luca Torresi, Chen Zhou, Chen Shao, Houssam Metni, Clint Van Hoesel, Henrik Schopmans, Timo Sommer, Communications Materials. 31932022</p>
<p>Graph neural networks in network neuroscience. Alaa Bessadok, Mohamed Ali Mahjoub, Islem Rekik, IEEE Transactions on Pattern Analysis and Machine Intelligence. 4552022</p>            </div>
        </div>

    </div>
</body>
</html>
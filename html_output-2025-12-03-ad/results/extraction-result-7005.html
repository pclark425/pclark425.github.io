<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7005 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7005</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7005</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-135.html">extraction-schema-135</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <p><strong>Paper ID:</strong> paper-17769145</p>
                <p><strong>Paper Title:</strong> Inducing Document Plans for Concept-to-Text Generation</p>
                <p><strong>Paper Abstract:</strong> In a language generation system, a content planner selects which elements must be included in the output text and the ordering be-tween them. Recent empirical approaches perform content selection without any ordering and have thus no means to ensure that the output is coherent. In this paper we focus on the problem of generating text from a database and present a trainable end-to-end generation system that includes both content selection and ordering. Content plans are represented intuitively by a set of grammar rules that operate on the document level and are acquired automatically from training data. We develop two approaches: the ﬁrst one is inspired from Rhetorical Structure Theory and represents the document as a tree of discourse relations between database records; the second one requires little linguistic sophistication and uses tree structures to represent global patterns of database record sequences within a document. Experimental evaluation on two domains yields considerable improvements over the state of the art for both approaches.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7005.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7005.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>G_RSE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Record-Sequence Document Plan Grammar (G_RSE)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A data-driven document-plan representation that models a document as a sequence of sentences, each sentence as an ordered sequence of record types; extracted as PCFG productions from aligned training data and embedded on top of a grammar that lexicalizes fields to text.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Record-type sequence PCFG (G_RSE)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Encodes a document as a sequence of SENT non-terminals, where each SENT expands to an ordered list of terminals R(r.t) corresponding to record types; the grammar is extracted from observed record-type sequences in aligned training data, binarized and associated with production probabilities. After types are chosen, later grammar layers select record tokens, fields and words.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>hierarchical; sequential (sentence-of-record-sequences) encoded as PCFG productions</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>Extraction from word-by-word alignments: map aligned record tokens to types, merge adjacent same-type spans, segment on punctuation to sentences, create and binarize trees; decoding/generation uses PCFG expansion with modified CYK/Viterbi intersected with an n-gram language model.</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>WEATHERGOV; WINHELP</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Concept-to-text generation / document planning (database → text)</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Extended PCFG generator with G_RSE document planner (this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Grammar-based joint model (extends Konstas & Lapata 2012) where G_RSE rules define document-level record-type sequences; grammar weights learned via EM/inside-outside; decoding via modified CYK/Viterbi intersected with a trigram LM (k-best beam kept).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>BLEU-4; human ratings (fluency, semantic correctness, coherence on 5-point scale)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>BLEU-4 with improved (heuristic) alignments: 39.23 (WEATHERGOV), 41.35 (WINHELP). Human ratings (WEATHERGOV): Fluency=4.25, Semantic correctness=3.75, Coherence=4.18. Human ratings (WINHELP): Fluency=3.59, Semantic correctness=3.21, Coherence=3.35.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>Not used for pre-training LMs; when embedded in the joint PCFG generator the representation improved generation quality (higher BLEU with better alignments) and consistently improved human-rated fluency, semantic correctness and coherence compared to the baseline without document planning.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Extraction depends heavily on quality of record-to-word alignments; on small datasets grammar is sparse (necessitating horizontal markovization); rules with low frequency were discarded (frequency < 3), potentially losing rare but valid patterns; no canonical ordering enforced—grammar reflects observed (noisy) sequences.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>Per paper, G_RSE performs comparably in BLEU to the RST-based planner (G_RST) and both outperform the baseline Konstas & Lapata (2012) model; Angeli et al. (2010) outperforms on WEATHERGOV (due to specialized numeric lexicalization) but performs worse on WINHELP.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Inducing Document Plans for Concept-to-Text Generation', 'publication_date_yy_mm': '2013-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7005.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7005.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>G_RST</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RST-inspired Document Plan Grammar (G_RST)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A document-plan representation that maps database records to EDUs and uses an RST discourse parser to produce a tree of rhetorical relations whose leaf nodes are record types; the resulting productions form a PCFG capturing hierarchical discourse structure used during generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>RST-discourse-tree PCFG (G_RST)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Represents documents as hierarchical discourse trees (internal nodes = rhetorical relations augmented with nucleus/satellite information, leaves = terminals R(r.t) for record types). Trees from a discourse parser are converted to grammar productions (RST relations → child nodes) and integrated into the generation grammar.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>hierarchical; tree-structured (RST discourse tree) encoded as PCFG productions</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>Segment text into EDUs via record-to-word alignments; run an automatic RST discourse parser to obtain a discourse tree over EDUs; substitute leaf EDUs with record-type terminals and collect/binarize productions as grammar rules; generation uses these productions within the PCFG and decodes with modified CYK plus an n-gram LM.</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>WEATHERGOV; WINHELP</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Concept-to-text generation / discourse-aware document planning</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Extended PCFG generator with G_RST document planner (this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Grammar-based joint generator embedding RST-derived PCFG productions; uses an external RST parser (Feng & Hirst 2012) trained on RST-DT to produce discourse trees for training examples; grammar weights estimated by maximum likelihood and inside-outside EM for remaining rules; decoding via modified CYK intersected with a trigram LM.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>BLEU-4; human ratings (fluency, semantic correctness, coherence on 5-point scale)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Human ratings (WEATHERGOV): Fluency=4.10, Semantic correctness=3.68, Coherence=4.10. Human ratings (WINHELP): Fluency=3.45, Semantic correctness=3.29, Coherence=3.22. (BLEU numeric values for G_RST not reported explicitly in the text; authors state G_RSE and G_RST perform comparably in BLEU.)</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>Used to derive document plans that improve end-to-end generation quality and coherence relative to a model without document planning; RST-based plans yield similar benefits to data-driven sequence plans.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Requires a text-level discourse parser and RST resources (currently available mainly for English), making it resource-intensive and less portable across languages/domains; quality dependent on accuracy of segmentations/alignments and parser outputs; on small datasets tree extraction required vertical markovization to address sparsity.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>Per paper, G_RST yields comparable BLEU to G_RSE and both outperform the baseline Konstas & Lapata (2012) model. G_RST is linguistically-motivated (advantages in being theory-grounded) but more resource-intensive than the linguistically naive G_RSE.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Inducing Document Plans for Concept-to-Text Generation', 'publication_date_yy_mm': '2013-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7005.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7005.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PCFG-Generator (K&L 2012)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PCFG joint content-selection and surface-realization model (Konstas & Lapata, 2012)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A probabilistic CFG that models database → text generation by encoding records, fields and words as non-terminals and terminals; training uses EM to learn hidden alignments; decoding finds the most probable derivation via a modified CYK intersected with an n-gram language model.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Unsupervised concept-to-text generation with hypergraphs</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>PCFG / hypergraph representation of database mappings</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Non-recursive CFG rules capture the hierarchy: S → R(start); R(type) expands to sequences of records; FS(r, f) expands to fields; F(r,f) expands to words; W rules emit lexical items or integer generators g(f.v). The grammar is associated with probabilistic weights learned via EM; generation equates to finding highest-probability derivations.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>hierarchical; grammar-based (PCFG / hypergraph)</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>Encode input database structure into non-terminals and production rules; learn rule weights by EM given observed texts; decoding performed by maximizing p(g)·p(g,h|d) using a modified CYK parser intersected with an n-gram language model (trigram).</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>WEATHERGOV; WINHELP (used as baseline/comparison in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Concept-to-text generation (database → text)</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PCFG joint generator (Konstas & Lapata, 2012) as baseline</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Domain-independent PCFG mapping records→fields→words, EM-trained; decoding via modified CYK with n-gram LM; lexicalization supported for categorical/integer fields (extended in this paper to also verbatim string generation).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>BLEU-4; human ratings (fluency, semantic correctness, coherence on 5-point scale)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Human ratings reported in this paper for K&L baseline: WEATHERGOV — Fluency=3.73, Semantic correctness=3.25, Coherence=3.59. WINHELP — Fluency=3.27, Semantic correctness=2.97, Coherence=2.93. (BLEU numbers for K&L in this paper are summarized in Table 2 but explicit BLEU values are not reproduced in the provided text.)</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>Acts as a joint generation model; when extended with document-planning grammars (G_RSE/G_RST) generation quality and coherence improved; baseline without document planning shows lower BLEU / human ratings.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Original model performed local record-selection (markovized chaining) and lacked global document planning, which led to less coherent multi-sentence outputs; originally lexicalized only categorical/integer field values (this paper extended it to verbatim string emission).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>This PCFG baseline is outperformed by both G_RSE and G_RST in automatic and human evaluations in this paper; Angeli et al. (2010) performs comparably or better on some surface-level metrics in WEATHERGOV due to specialized numeric lexicalization patterns.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Inducing Document Plans for Concept-to-Text Generation', 'publication_date_yy_mm': '2013-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7005.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7005.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Liang-align</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Unsupervised record-to-word alignment model (Liang et al., 2009)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An (unsupervised) alignment model used to map database records to words/segments in the training texts; alignments supply the input-to-text correspondences required to extract document-plan grammars.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Learning semantic correspondences with less supervision</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Record-to-word alignment outputs (used to create grammar trees)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Produces word-by-word alignments between database records and text; these alignments are converted to record-token sequences or EDUs which are then used to extract G_RSE or to segment EDUs before RST parsing for G_RST.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>lossy alignments (many-to-many mappings); token-based alignment output</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>Unsupervised semantic correspondence learning producing alignments between record tokens and words/phrases in text; used as pre-processing to derive grammar extraction trees.</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>WEATHERGOV; WINHELP (used in this paper to produce alignments)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Alignment for concept-to-text grammar extraction</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Liang et al. (2009) unsupervised alignment model</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Unsupervised model for learning semantic correspondences between database records (concepts) and textual tokens; used here to produce alignments that drive grammar extraction for document-planning rules.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Not evaluated as a generation representation here; quality of alignments correlated with grammar quality and BLEU of generated text.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Not reported numerically in this paper, but authors note improved alignments (heuristic/anchor-based) yield higher BLEU for extracted G_RSE (see G_RSE BLEU values).</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>Alignment quality strongly affects the extracted grammar and thus downstream generation quality; better (heuristic) alignments led to notable BLEU improvements for G_RSE.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Alignments are noisy when unsupervised; noisy alignments yield noisier grammars and hurt generation BLEU; the paper also used handcrafted heuristics to produce cleaner alignments that improved results.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>Authors note any semi- or fully-supervised alignment method could be used; they used Liang et al.'s unsupervised model but show that heuristic (anchor) alignments lead to better grammar extraction and higher BLEU.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Inducing Document Plans for Concept-to-Text Generation', 'publication_date_yy_mm': '2013-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Unsupervised concept-to-text generation with hypergraphs <em>(Rating: 2)</em></li>
                <li>Learning semantic correspondences with less supervision <em>(Rating: 2)</em></li>
                <li>A simple domain-independent probabilistic approach to generation <em>(Rating: 1)</em></li>
                <li>Text-level discourse parsing with rich linguistic features <em>(Rating: 2)</em></li>
                <li>Reinforcement learning for mapping instructions to actions <em>(Rating: 2)</em></li>
                <li>Rhetorical structure theory: Toward a functional theory of text organization <em>(Rating: 1)</em></li>
                <li>Deriving rhetorical complexity data from the RST-DT corpus <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7005",
    "paper_id": "paper-17769145",
    "extraction_schema_id": "extraction-schema-135",
    "extracted_data": [
        {
            "name_short": "G_RSE",
            "name_full": "Record-Sequence Document Plan Grammar (G_RSE)",
            "brief_description": "A data-driven document-plan representation that models a document as a sequence of sentences, each sentence as an ordered sequence of record types; extracted as PCFG productions from aligned training data and embedded on top of a grammar that lexicalizes fields to text.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "Record-type sequence PCFG (G_RSE)",
            "representation_description": "Encodes a document as a sequence of SENT non-terminals, where each SENT expands to an ordered list of terminals R(r.t) corresponding to record types; the grammar is extracted from observed record-type sequences in aligned training data, binarized and associated with production probabilities. After types are chosen, later grammar layers select record tokens, fields and words.",
            "representation_type": "hierarchical; sequential (sentence-of-record-sequences) encoded as PCFG productions",
            "encoding_method": "Extraction from word-by-word alignments: map aligned record tokens to types, merge adjacent same-type spans, segment on punctuation to sentences, create and binarize trees; decoding/generation uses PCFG expansion with modified CYK/Viterbi intersected with an n-gram language model.",
            "canonicalization": false,
            "average_token_length": null,
            "dataset_name": "WEATHERGOV; WINHELP",
            "task_name": "Concept-to-text generation / document planning (database → text)",
            "model_name": "Extended PCFG generator with G_RSE document planner (this paper)",
            "model_description": "Grammar-based joint model (extends Konstas & Lapata 2012) where G_RSE rules define document-level record-type sequences; grammar weights learned via EM/inside-outside; decoding via modified CYK/Viterbi intersected with a trigram LM (k-best beam kept).",
            "performance_metric": "BLEU-4; human ratings (fluency, semantic correctness, coherence on 5-point scale)",
            "performance_value": "BLEU-4 with improved (heuristic) alignments: 39.23 (WEATHERGOV), 41.35 (WINHELP). Human ratings (WEATHERGOV): Fluency=4.25, Semantic correctness=3.75, Coherence=4.18. Human ratings (WINHELP): Fluency=3.59, Semantic correctness=3.21, Coherence=3.35.",
            "impact_on_training": "Not used for pre-training LMs; when embedded in the joint PCFG generator the representation improved generation quality (higher BLEU with better alignments) and consistently improved human-rated fluency, semantic correctness and coherence compared to the baseline without document planning.",
            "limitations": "Extraction depends heavily on quality of record-to-word alignments; on small datasets grammar is sparse (necessitating horizontal markovization); rules with low frequency were discarded (frequency &lt; 3), potentially losing rare but valid patterns; no canonical ordering enforced—grammar reflects observed (noisy) sequences.",
            "comparison_with_other": "Per paper, G_RSE performs comparably in BLEU to the RST-based planner (G_RST) and both outperform the baseline Konstas & Lapata (2012) model; Angeli et al. (2010) outperforms on WEATHERGOV (due to specialized numeric lexicalization) but performs worse on WINHELP.",
            "uuid": "e7005.0",
            "source_info": {
                "paper_title": "Inducing Document Plans for Concept-to-Text Generation",
                "publication_date_yy_mm": "2013-10"
            }
        },
        {
            "name_short": "G_RST",
            "name_full": "RST-inspired Document Plan Grammar (G_RST)",
            "brief_description": "A document-plan representation that maps database records to EDUs and uses an RST discourse parser to produce a tree of rhetorical relations whose leaf nodes are record types; the resulting productions form a PCFG capturing hierarchical discourse structure used during generation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "RST-discourse-tree PCFG (G_RST)",
            "representation_description": "Represents documents as hierarchical discourse trees (internal nodes = rhetorical relations augmented with nucleus/satellite information, leaves = terminals R(r.t) for record types). Trees from a discourse parser are converted to grammar productions (RST relations → child nodes) and integrated into the generation grammar.",
            "representation_type": "hierarchical; tree-structured (RST discourse tree) encoded as PCFG productions",
            "encoding_method": "Segment text into EDUs via record-to-word alignments; run an automatic RST discourse parser to obtain a discourse tree over EDUs; substitute leaf EDUs with record-type terminals and collect/binarize productions as grammar rules; generation uses these productions within the PCFG and decodes with modified CYK plus an n-gram LM.",
            "canonicalization": null,
            "average_token_length": null,
            "dataset_name": "WEATHERGOV; WINHELP",
            "task_name": "Concept-to-text generation / discourse-aware document planning",
            "model_name": "Extended PCFG generator with G_RST document planner (this paper)",
            "model_description": "Grammar-based joint generator embedding RST-derived PCFG productions; uses an external RST parser (Feng & Hirst 2012) trained on RST-DT to produce discourse trees for training examples; grammar weights estimated by maximum likelihood and inside-outside EM for remaining rules; decoding via modified CYK intersected with a trigram LM.",
            "performance_metric": "BLEU-4; human ratings (fluency, semantic correctness, coherence on 5-point scale)",
            "performance_value": "Human ratings (WEATHERGOV): Fluency=4.10, Semantic correctness=3.68, Coherence=4.10. Human ratings (WINHELP): Fluency=3.45, Semantic correctness=3.29, Coherence=3.22. (BLEU numeric values for G_RST not reported explicitly in the text; authors state G_RSE and G_RST perform comparably in BLEU.)",
            "impact_on_training": "Used to derive document plans that improve end-to-end generation quality and coherence relative to a model without document planning; RST-based plans yield similar benefits to data-driven sequence plans.",
            "limitations": "Requires a text-level discourse parser and RST resources (currently available mainly for English), making it resource-intensive and less portable across languages/domains; quality dependent on accuracy of segmentations/alignments and parser outputs; on small datasets tree extraction required vertical markovization to address sparsity.",
            "comparison_with_other": "Per paper, G_RST yields comparable BLEU to G_RSE and both outperform the baseline Konstas & Lapata (2012) model. G_RST is linguistically-motivated (advantages in being theory-grounded) but more resource-intensive than the linguistically naive G_RSE.",
            "uuid": "e7005.1",
            "source_info": {
                "paper_title": "Inducing Document Plans for Concept-to-Text Generation",
                "publication_date_yy_mm": "2013-10"
            }
        },
        {
            "name_short": "PCFG-Generator (K&L 2012)",
            "name_full": "PCFG joint content-selection and surface-realization model (Konstas & Lapata, 2012)",
            "brief_description": "A probabilistic CFG that models database → text generation by encoding records, fields and words as non-terminals and terminals; training uses EM to learn hidden alignments; decoding finds the most probable derivation via a modified CYK intersected with an n-gram language model.",
            "citation_title": "Unsupervised concept-to-text generation with hypergraphs",
            "mention_or_use": "use",
            "representation_name": "PCFG / hypergraph representation of database mappings",
            "representation_description": "Non-recursive CFG rules capture the hierarchy: S → R(start); R(type) expands to sequences of records; FS(r, f) expands to fields; F(r,f) expands to words; W rules emit lexical items or integer generators g(f.v). The grammar is associated with probabilistic weights learned via EM; generation equates to finding highest-probability derivations.",
            "representation_type": "hierarchical; grammar-based (PCFG / hypergraph)",
            "encoding_method": "Encode input database structure into non-terminals and production rules; learn rule weights by EM given observed texts; decoding performed by maximizing p(g)·p(g,h|d) using a modified CYK parser intersected with an n-gram language model (trigram).",
            "canonicalization": null,
            "average_token_length": null,
            "dataset_name": "WEATHERGOV; WINHELP (used as baseline/comparison in this paper)",
            "task_name": "Concept-to-text generation (database → text)",
            "model_name": "PCFG joint generator (Konstas & Lapata, 2012) as baseline",
            "model_description": "Domain-independent PCFG mapping records→fields→words, EM-trained; decoding via modified CYK with n-gram LM; lexicalization supported for categorical/integer fields (extended in this paper to also verbatim string generation).",
            "performance_metric": "BLEU-4; human ratings (fluency, semantic correctness, coherence on 5-point scale)",
            "performance_value": "Human ratings reported in this paper for K&L baseline: WEATHERGOV — Fluency=3.73, Semantic correctness=3.25, Coherence=3.59. WINHELP — Fluency=3.27, Semantic correctness=2.97, Coherence=2.93. (BLEU numbers for K&L in this paper are summarized in Table 2 but explicit BLEU values are not reproduced in the provided text.)",
            "impact_on_training": "Acts as a joint generation model; when extended with document-planning grammars (G_RSE/G_RST) generation quality and coherence improved; baseline without document planning shows lower BLEU / human ratings.",
            "limitations": "Original model performed local record-selection (markovized chaining) and lacked global document planning, which led to less coherent multi-sentence outputs; originally lexicalized only categorical/integer field values (this paper extended it to verbatim string emission).",
            "comparison_with_other": "This PCFG baseline is outperformed by both G_RSE and G_RST in automatic and human evaluations in this paper; Angeli et al. (2010) performs comparably or better on some surface-level metrics in WEATHERGOV due to specialized numeric lexicalization patterns.",
            "uuid": "e7005.2",
            "source_info": {
                "paper_title": "Inducing Document Plans for Concept-to-Text Generation",
                "publication_date_yy_mm": "2013-10"
            }
        },
        {
            "name_short": "Liang-align",
            "name_full": "Unsupervised record-to-word alignment model (Liang et al., 2009)",
            "brief_description": "An (unsupervised) alignment model used to map database records to words/segments in the training texts; alignments supply the input-to-text correspondences required to extract document-plan grammars.",
            "citation_title": "Learning semantic correspondences with less supervision",
            "mention_or_use": "use",
            "representation_name": "Record-to-word alignment outputs (used to create grammar trees)",
            "representation_description": "Produces word-by-word alignments between database records and text; these alignments are converted to record-token sequences or EDUs which are then used to extract G_RSE or to segment EDUs before RST parsing for G_RST.",
            "representation_type": "lossy alignments (many-to-many mappings); token-based alignment output",
            "encoding_method": "Unsupervised semantic correspondence learning producing alignments between record tokens and words/phrases in text; used as pre-processing to derive grammar extraction trees.",
            "canonicalization": null,
            "average_token_length": null,
            "dataset_name": "WEATHERGOV; WINHELP (used in this paper to produce alignments)",
            "task_name": "Alignment for concept-to-text grammar extraction",
            "model_name": "Liang et al. (2009) unsupervised alignment model",
            "model_description": "Unsupervised model for learning semantic correspondences between database records (concepts) and textual tokens; used here to produce alignments that drive grammar extraction for document-planning rules.",
            "performance_metric": "Not evaluated as a generation representation here; quality of alignments correlated with grammar quality and BLEU of generated text.",
            "performance_value": "Not reported numerically in this paper, but authors note improved alignments (heuristic/anchor-based) yield higher BLEU for extracted G_RSE (see G_RSE BLEU values).",
            "impact_on_training": "Alignment quality strongly affects the extracted grammar and thus downstream generation quality; better (heuristic) alignments led to notable BLEU improvements for G_RSE.",
            "limitations": "Alignments are noisy when unsupervised; noisy alignments yield noisier grammars and hurt generation BLEU; the paper also used handcrafted heuristics to produce cleaner alignments that improved results.",
            "comparison_with_other": "Authors note any semi- or fully-supervised alignment method could be used; they used Liang et al.'s unsupervised model but show that heuristic (anchor) alignments lead to better grammar extraction and higher BLEU.",
            "uuid": "e7005.3",
            "source_info": {
                "paper_title": "Inducing Document Plans for Concept-to-Text Generation",
                "publication_date_yy_mm": "2013-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Unsupervised concept-to-text generation with hypergraphs",
            "rating": 2,
            "sanitized_title": "unsupervised_concepttotext_generation_with_hypergraphs"
        },
        {
            "paper_title": "Learning semantic correspondences with less supervision",
            "rating": 2,
            "sanitized_title": "learning_semantic_correspondences_with_less_supervision"
        },
        {
            "paper_title": "A simple domain-independent probabilistic approach to generation",
            "rating": 1,
            "sanitized_title": "a_simple_domainindependent_probabilistic_approach_to_generation"
        },
        {
            "paper_title": "Text-level discourse parsing with rich linguistic features",
            "rating": 2,
            "sanitized_title": "textlevel_discourse_parsing_with_rich_linguistic_features"
        },
        {
            "paper_title": "Reinforcement learning for mapping instructions to actions",
            "rating": 2,
            "sanitized_title": "reinforcement_learning_for_mapping_instructions_to_actions"
        },
        {
            "paper_title": "Rhetorical structure theory: Toward a functional theory of text organization",
            "rating": 1,
            "sanitized_title": "rhetorical_structure_theory_toward_a_functional_theory_of_text_organization"
        },
        {
            "paper_title": "Deriving rhetorical complexity data from the RST-DT corpus",
            "rating": 1,
            "sanitized_title": "deriving_rhetorical_complexity_data_from_the_rstdt_corpus"
        }
    ],
    "cost": 0.014693499999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Inducing Document Plans for Concept-to-text Generation
Association for Computational LinguisticsCopyright Association for Computational Linguistics18-21 October 2013. 2013</p>
<p>Ioannis Konstas ikonstas@inf.ed.ac.uk 
Institute for Language, Cognition and Computation School of Informatics
University of Edinburgh
10 Crichton StreetEH8 9ABEdinburgh</p>
<p>Mirella Lapata 
Institute for Language, Cognition and Computation School of Informatics
University of Edinburgh
10 Crichton StreetEH8 9ABEdinburgh</p>
<p>Inducing Document Plans for Concept-to-text Generation</p>
<p>Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing
the 2013 Conference on Empirical Methods in Natural Language ProcessingSeattle, Washington, USAAssociation for Computational Linguistics18-21 October 2013. 2013
In a language generation system, a content planner selects which elements must be included in the output text and the ordering between them. Recent empirical approaches perform content selection without any ordering and have thus no means to ensure that the output is coherent. In this paper we focus on the problem of generating text from a database and present a trainable end-to-end generation system that includes both content selection and ordering. Content plans are represented intuitively by a set of grammar rules that operate on the document level and are acquired automatically from training data. We develop two approaches: the first one is inspired from Rhetorical Structure Theory and represents the document as a tree of discourse relations between database records; the second one requires little linguistic sophistication and uses tree structures to represent global patterns of database record sequences within a document. Experimental evaluation on two domains yields considerable improvements over the state of the art for both approaches.</p>
<p>Introduction</p>
<p>Concept-to-text generation broadly refers to the task of automatically producing textual output from nonlinguistic input (Reiter and Dale, 2000). Depending on the application and the domain at hand, the input may assume various representations including databases, expert system knowledge bases, simulations of physical systems, or formal meaning representations. Generation systems typically follow a pipeline architecture consisting of three components: content planning (selecting and ordering the parts of the input to be mentioned in the output text), sentence planning (determining the structure and lexical content of individual sentences), and surface realization (verbalizing the chosen content in natural language). Traditionally, these components are hand-engineered in order to ensure output of high quality.</p>
<p>More recently there has been growing interest in the application of learning methods because of their promise to make generation more robust and adaptable. Examples include learning which content should be present in a document (Duboue and McKeown, 2002;Barzilay and Lapata, 2005), how it should be aligned to utterances (Liang et al., 2009), and how to select a sentence plan among many alternatives (Stent et al., 2004). Beyond isolated components, a few approaches have emerged that tackle concept-to-text generation end-to-end. Due to the complexity of the task, most models simplify the generation process, e.g., by treating sentence planning and surface realization as one component (Angeli et al., 2010), by implementing content selection without any document planning (Konstas and Lapata, 2012;Angeli et al., 2010;Kim and Mooney, 2010), or by eliminating content planning entirely (Belz, 2008;Wong and Mooney, 2007).</p>
<p>In this paper we present a trainable end-to-end generation system that captures all components of the traditional pipeline, including document planning. Rather than breaking up the generation process into a sequence of local decisions, each learned separately (Reiter et al., 2005;Belz, 2008;Chen and Mooney, 2008;Kim and Mooney, 2010), our model performs content planning (i.e., document planning and content selection), sentence planning (i.e., lex-Database Records temp(time: 6-21, min:9, mean:15, max:21) wind-spd(time: 6-21, min:15, mean:20, max:30) sky-cover(time:6-9, percent:25-50) sky-cover(time:9-12, percent:50-75) wind-dir(time:6-21, mode:SSE) gust(time: 6-21, min:20, mean:30, max:40) Output Text Cloudy, with a high around 20. South southeast wind between 15 and 30 mph. Gusts as high as 40 mph.</p>
<p>(a) WEATHERGOV Database Records desktop(cmd:lclick, name:start, type:button) start(cmd:lclick, name:settings, type:button) start-target(cmd:lclick, name:control panel, type:button) win-target(cmd:dblclick, name:users and passwords, type:item) contMenu(cmd:lclick, name:advanced, type:tab) action-contMenu(cmd:lclick, name:advanced, type:button)</p>
<p>Output Text Click start, point to settings, and then click control panel. Doubleclick users and passwords. On the advanced tab, click advanced.</p>
<p>(b) WINHELP Figure 1: Database records and corresponding text for (a) weather forecasting and (b) Windows troubleshooting. Each record has a type (e.g., win-target), and a set of fields. Each field has a value, which can be categorical (in typewriter), an integer (in bold), or a literal string (in italics).</p>
<p>icalization of input entries), and surface realization jointly. We focus on the problem of generating text from a database. The input to our model is a set of database records and collocated descriptions, examples of which are shown in Figure 1.</p>
<p>Given this input, we define a probabilistic context-free grammar (PCFG) that captures the structure of the database and how it can be verbalized. Specifically, we extend the model of Konstas and Lapata (2012) which also uses a PCFG to perform content selection and surface realization, but does not capture any aspect of document planning. We represent content plans with grammar rules which operate on the document level and are embedded on top of the original PCFG. We essentially learn a discourse grammar following two approaches. The first one is linguistically naive but applicable to multiple languages and domains; it extracts rules representing global patterns of record sequences within a sentence and among sentences from a training corpus. The second approach learns document plans based on Rhetorical Structure Theory (RST; Mann and Thomson, 1988); it therefore has a solid linguistic foundation, but is resource intensive as it assumes access to a text-level discourse parser.</p>
<p>We learn document plans automatically using both representations and develop a tractable decoding algorithm for finding the best output, i.e., derivation in our grammar. To the best of our knowledge, this is the first data-driven model to incorporate document planning in a joint end-to-end system. Experimental evaluation on the WEATHERGOV (Liang et al., 2009) and WINHELP (Branavan et al., 2009) do-mains shows that our approach improves over Konstas and Lapata (2012) by a wide margin.</p>
<p>Related Work</p>
<p>Content planning is a fundamental component in a natural generation system. Not only does it determine which information-bearing units to talk about, but also arranges them into a structure that creates coherent output. It is therefore not surprising that many content planners have been based on theories of discourse coherence (Hovy, 1993;Scott and de Souza, 1990). Other work has relied on generic planners (Dale, 1988) or schemas (Duboue and McKeown, 2002). In all cases, content plans are created manually, sometimes through corpus analysis. A few researchers recognize that this top-down approach to planning is too inflexible and adopt a generate-and-rank architecture instead (Mellish et al., 1998;Karamanis, 2003;Kibble and Power, 2004). The idea is to produce a large set of candidate plans and select the best one according to a ranking function. The latter is typically developed manually taking into account constraints relating to discourse coherence and the semantics of the domain. Duboue and McKeown (2001) present perhaps the first empirical approach to content planning. They use techniques from computational biology to learn the basic patterns contained within a plan and the ordering among them. Duboue and McKeown (2002) learn a tree-like planner from an aligned corpus of semantic inputs and corresponding human-authored outputs using evolutionary al-gorithms. More recent data-driven work focuses on end-to-end systems rather than individual components, however without taking document planning into account. For example, Kim and Mooney (2010) first define a generative model similar to Liang et al. (2009) that selects which database records to talk about and then use an existing surface realizer (Wong and Mooney, 2007) to render the chosen records in natural language. Their content planner has no notion of coherence. Angeli et al. (2010) adopt a more unified approach that builds on top of the alignment model of Liang et al. (2009). They break record selection into a series of locally coherent decisions, by first deciding on what records to talk about. Each choice is based on a history of previous decisions, which is encoded in the form of discriminative features in a log-linear model. Analogously, they choose fields for each record, and finally verbalize the input using automatically extracted domain-specific templates from training data. Konstas and Lapata (2012) propose a joint model, which recasts content selection and surface realization into a parsing problem. Their model optimizes the choice of records, fields and words simultaneously, however they still select and order records locally. We replace their content selection mechanism (which is based on a simple markovized chaining of records) with global document representations. A plan in our model is identified either as a sequence of sentences, each containing a sequence of records, or as a tree where the internal nodes denote discourse information and the leaf nodes correspond to records.</p>
<p>Problem Formulation</p>
<p>The generator takes as input a set of database records d and outputs a text g that verbalizes some of these records. Each record token r i ∈ d, with 1 ≤ i ≤ |d|, has a type r i .t and a set of fields f associated with it. Fields have different values f .v and types f .t (i.e., integer, categorical, or literal strings). For example, in Figure 1b, win-target is a record type with three fields: cmd (denotes the action the user must perform on an object on their screen, e.g., left-click), name (denotes the name of the object), and type (denotes the type of the object). The values of these fields are dblclick, users and passwords, and item; name is a literal string, the rest are categorical.
Grammar Rules 1. S → R(start) 2. R(r i .t) → FS(r j , start) R(r j .t) | FS(r j , start) 3. FS(r, r. f i ) → F(r, r. f j ) FS(r, r. f j ) | F(r, r. f j ) 4. F(r, r. f ) → W(r, r. f ) F(r, r. f ) | W(r, r. f ) 5. W(r, r. f ) → α | g( f .v)
During training, our algorithm is given a corpus consisting of several scenarios, i.e., database records paired with texts w (see Figure 1). For each scenario, the model first decides on a global document plan, i.e., it selects which types of records belong to each sentence (or phrase) and how these sentences (or phrases) should be ordered. Then it selects appropriate record tokens for each type and progressively chooses the most relevant fields; then, based on the values of the fields, it generates the final text, word by word.</p>
<p>Original Model</p>
<p>Our work builds on the model developed by Konstas and Lapata (2012). The latter is essentially a PCFG which captures both the structure of the input database and the way it renders into natural language. This grammar-based approach lends itself well to the incorporation of document planning which has traditionally assumed tree-like representations. We first briefly describe the original model and then present our extensions in Section 5.</p>
<p>Grammar Grammar G in Figure 2 defines a set of non-recursive CFG rewrite rules that capture the structure of the database, i.e., the relationship between records, records and fields, fields and words. These rules are domain-independent and could be applied to any database provided it follows the same structure. Non-terminal symbols are in capitals, the terminal symbol α corresponds to the vocabulary of the training set and g( f .v) is a function which generates integers given the field value f .v. Note that all non-terminals have features (in parentheses) which act as constraints and impose non-recursion (e.g., in rule (2) i = j, so that a record cannot emit itself).</p>
<p>Rule (1) defines the expansion from the start symbol S to the first record R of type start. The rules in (2) implement content selection, by choosing appropriate records from the database and generating a sequence. R(r i .t) is the source record, R(r j .t) is the target record and FS(r j .start) is a place-holder symbol for the set of fields of record token r j . This method is locally optimal, since it only keeps track of the previous type of record for each re-write. The rules in (3) conclude content selection on the field level, i.e., after we have chosen a record, we select and order the corresponding fields. Finally, the rules in (4) and (5) correspond to surface realization. The former rule binarizes the sequence of words emitted by a particular field r. f in an attempt to capture local dependencies between words, such as multi-word expressions (e.g., right click, radio button). The latter rule defines the emission of words and integer numbers 1 , given a field type and its value. Note that the original model lexicalizes field values of categorical and integer type only.</p>
<p>Training The rules of grammar G are associated with weights that are learned using the EM algorithm (Dempster et al., 1977). During training, the records, fields and values of database d and the words w from the associated text are observed, and the model learns the mapping between them. Notice that we use w to denote the gold-standard text and g to refer to the words generated by the model. The mapping between the database and the observed text is unknown and thus the weights of the rules define a hidden correspondence h between records, fields and their values.</p>
<p>Decoding Given a trained grammar G and an input scenario from a database d, the model generates text by finding the most likely derivation, i.e., sequence of rewrite rules for the input. Although resembling parsing, the generation task is subtly different. In parsing, we observe a string of words and our goal is to find the most probable syntactic structure, i.e., hidden correspondenceĥ. In generation, however, the string is not observed; instead, we must find the best textĝ, by maximizing both over h and g, where g = g 1 . . . g N is a sequence of words licensed by G. More formally:
g = f arg max g,h P (g, h)
(1)</p>
<p>where f is a function that takes as input a derivation tree (g, h) and returnsĝ. Konstas and Lapata (2012) use a modified version of the CYK parser (Kasami, 1965;Younger, 1967) to findĝ. Specifically, they intersect grammar G with a n-gram language model and calculate the most probable generationĝ as:
g = f arg max g,h p(g) · p( g, h | d)(2)
where p(g, h | d) is the decoding likelihood for a sequence of words g = g 1 . . . g N of length N and the hidden correspondence h that emits it, i.e., the likelihood of the grammar for a given database input scenario d. p(g) is a measure of the quality of each output and is provided by the n-gram language model.</p>
<p>Extensions</p>
<p>In this section we extend the model of Konstas and Lapata (2012) by developing two more sophisticated content selection approaches which are informed by a global plan of the document to be generated.</p>
<p>Planning with Record Sequences</p>
<p>Grammar Our key idea is to replace the content selection mechanism of the original model with a document plan which essentially defines a grammar on record types. We split a document into sentences, each terminated by a full-stop. Then a sentence is further split into a sequence of record types. Contrary to the original model, we observe a complete sequence 2 of record types, split into sentences. This way we learn domain-specific patterns of frequently occurring record type sequences among the sentences of a document, as well as more local structures within a sentence. We thus substitute rules (1)-(2) in Figure 2 with sub-grammar G RSE based on record type sequences:</p>
<p>Definition 1 (G RSE grammar)
G RSE = {Σ R , N RSE , P RSE , D}
where Σ R is a set of terminal symbols R(r.t), and N RSE is a set of non-terminal symbols:
N RSE = {D, SENT }
where D represents the start symbol and SENT a sequence of records. P RSE is a set of production rules of the form:
(a) D → SENT (t i , . . . , t j ) . . . SENT (t l , . . . , t m ) (b) SENT (t i , . . . , t j ) → R(r a .t i ) . . . R(r k .t j ) ·
where t is a record type, t i , t j , t l and t m may overlap and r a , r k are record tokens of type t i and t j respectively. The corresponding weights for the production rules P RSE are:
Definition 2 (G RSE weights) (a) p(t i , . . . , t j , . . . t l , . . . ,t m | D) (b) p(t i ) · ... · p(t j ) = 1 |s(t i )| · . . . · 1 |s(t j )|
where s(t) is a function that returns the set of records with type t (Liang et al., 2009). Rule (a) defines the expansion from the start symbol D to a sequence of sentences, each represented by the non-terminal SENT . Similarly to the original grammar G, we employ the use of features (in parentheses) to denote a sequence of record types. The same record types may recur in different sentences, but not in the same one. The weight of rule (a) is simply the joint probability of all the record types present, ordered and segmented appropriately into sentences in the document, given the start symbol.</p>
<p>Once record types have been selected (on a per sentence basis) we move on to rule (b) which describes how each non-terminal SENT expands to an ordered sequence of records R, as they are observed within a sentence (see the terminal symbol '.' at the end of the rule). Notice that a record type t i may correspond to several record tokens r a . Rules (3)-(5) in grammar G make decisions on these tokens based on the overall content of the database and the field/value selection. The weight of this rule is the product of the weights of each record type. This is set to the uniform distribution over {1, ..., |s(t)|} for record type t, where |s(t)| is the number of records with that type. Figure 3d shows an example tree for the database input in Figure 1b, using G RSE and assuming that the alignments between records and text are given. The top level of the tree refers to the sequence of record types as they are observed in the text. The first sentence contains three records with types 'desktop', 'start' and 'start-target', each corresponding to the textual segments click start, point to settings, and then click control panel. The next level on the tree, denotes the choice of record tokens for each sentence, provided that we have decided on the choice and order of their types (see Figure 3b). In Figure 3d, the bottom-left sub-tree corresponds to the choice of the first three records of Figure 1b.</p>
<p>Training A straightforward way to train the extended model would be to embed the parameters of G RSE in the original model and then run the EM algorithm using inside-outside at the E-step. Unfortunately, this method will induce a prohibitively large search space. Rule (a) enumerates all possible combinations of record type sequences and the number grows exponentially even for a few record types and a small sequence size. To tackle this problem, we extracted rules for G RSE from the training data, based on the assumption that there will be far fewer unique sequences of record types per dataset than exhaustively enumerating all possibilities.</p>
<p>For each scenario, we obtain a word-by-word alignment between the database records and the corresponding text. In our experiments we used Liang et al.'s (2009) unsupervised model, however any other semi-or fully supervised method could be used. As we show in Section 7, the quality of the alignment inevitably correlates with the quality of the extracted grammar and the decoder's output. We then map the aligned record tokens to their corresponding types, merge adjacent words with the same type and segment on punctuation (see Figure 3b). Next, we create the corresponding tree according to G RSE (Figure 3d) and binarize it. We experimented both with left and right binarization and adhered to the latter, as it obtained a more compact set of rules. Finally, we collectively count the rule weights on the resulting treebank and extract a rule set, discarding rules with frequency less than three.</p>
<p>Using the extracted (weighted) G RSE rules, we run the EM algorithm via inside-outside and learn the weights for the remaining rules in G. Decoding remains the same as in Konstas and Lapata (2012); the only requirement is that the extracted grammar remains binarized in order to guarantee the cubic 
R(a-c 1 .t) R(c 1 .t) SENT(w-t) R(w-t 1 .t) SENT(d, s, s-t) R(s-t 1 .t) R(s 1 .t) R(d 1 .t) (d) Document plan using the G RSE grammar D Elaboration[N][S] Elaboration[N][S] R(a-c 1 .t) R(c 1 .t) Elaboration[N][S] R(w-t 1 .t) Elaboration[N][S] Joint[N][N] R(s-t 1 .t) R(s 1 .t) R(d 1 .t)
(e) Document plan using the G RST grammar Figure 3: Grammar extraction example from the WINHELP domain using G RSE and G RST . For G RSE , we take the alignments of records on words and map them to their corresponding types (a); we then segment record types into sentences (b); and finally, create a tree using grammar G RSE (c). For G RST , we segment the text into EDUs based on the records they align to (d) and output the discourse tree (omitted here for brevity's sake); we build the document plan once we substitute the EDUs with their corresponding record types (e).</p>
<p>bound of the Viterbi search algorithm. Note that the original grammar is limited to the generation of categorical and integer values. We extend it to support the generation of strings. The following rule adds a simple verbatim lexicalization for string values:
W(r, r. f ) → gen str( f .v, i) gen str( f .v, i) : V → V, f .v ∈ V
where V is the set of words for the fields of type string, and gen str is a function that takes the value of a string-typed field f .v, and the position i in the string, and generates the corresponding word at that position. For example, gen str(users and passwords, 3) = passwords. The weight of this rule is set to 1.</p>
<p>Planning with Rhetorical Structure Theory</p>
<p>Grammar RST (Mann and Thompson, 1988) is a theory of text organization which provides a framework for analyzing text. A basic tenet of the theory is that a text consists of hierarchically organized text spans or elementary discourse units (EDUs) that stand in various relations to one another (e.g., Elaboration, Attribution). These "rhetorical relations" hold between two adjacent parts of the text, where typically, one part is "nuclear" and one a "satellite". An analysis of a text consists in identifying the relations holding between successively larger parts of the text, yielding a natural hierarchical description of the rhetorical organization of the text. From its very inception, RST was conceived as a way to characterize text and textual relations for the purpose of text generation.</p>
<p>In order to create a RST-inspired document plan for our input (i.e., database records paired with texts), we make the following assumption: each record corresponds to a unique non-overlapping span in the collocated text, and can be therefore mapped to an EDU. Assuming the text has been segmented and aligned to a sequence of records, we can create a discourse tree with record types (in place of their corresponding EDUs) as leaf nodes. Again, we define a sub-grammar G RST which replaces rules (1)-(2) from Figure 2:
Definition 3 (G RST grammar) G RST = {Σ R , N RST , P RST , D}
where Σ R is the alphabet of leaf nodes as defined in Section 5.1, N RST is a set of non-terminals corresponding to rhetorical relations augmented with nucleus-satellite information (e.g., Elaboration[N][S] stands for the elaboration relation between the nucleus EDU left-adjoining with the satellite EDU), P RST is the set of production rules of the form P RST ⊆ N RST × {N RST ∪ Σ R } × {N RST ∪ Σ R } associated with a weight for each rule, and D ∈ N RST is the root symbol. Figure 3e gives the discourse tree for the database input of Figure 1b, using G RST .</p>
<p>Training In order to obtain the weighted productions of G RST , we use an existing state-of-the-art discourse parser 3 (Feng and Hirst, 2012) trained on the RST-DT corpus . The latter contains a selection of 385 Wall Street Journal articles which have been annotated using the framework of RST and an inventory of 78 rhetorical relations, classified into 18 coarse-grained categories . Figure 4 gives a comparison of the distribution of relations extracted for the two datasets we used, against the gold-standard annotation of RST-DT. The statistics for the RST-DT corpus are taken from Williams and Power (2008). The relative frequencies of relations on both datasets follow closely the distribution of those in RST-DT, thus empirically supporting the application of the RST framework to our data.</p>
<p>We segment each document in our training set into EDUs based on the record-to-text alignments given by the model of Liang et al. (2009) (see Figure 3c). We then run the discourse parser on the resulting EDUs, and retrieve the corresponding discourse tree; the internal nodes are labelled with one of the RST relations. Finally, we replace the leaf EDUs with their respective terminal symbols R(r.t) ∈ Σ R (Figure 3e) and collect the resulting grammar productions; their weights are calculated via maximum likelihood estimation based on their collective counts in the parse trees licensed by G RST . Training and decoding of the extended generation model (after we embed G RST in the original grammar G) is performed identically to Section 5.1. 3 Publicly available from http://www.cs.toronto.edu/ weifeng/software.html.</p>
<p>Experimental Design</p>
<p>Data Since our aim was to evaluate the planning component of our model, we used datasets whose documents are at least a few sentences long. Specifically, we generated weather forecasts and troubleshooting guides for an operating system. For the first domain (henceforth WEATHERGOV) we used the dataset of Liang et al. (2009), which consists of 29,528 weather scenarios for 3,753 major US cities (collected over four days). The database has 12 record types, each scenario contains on average 36 records, 5.8 out of which are mentioned in the text. A document has 29.3 words and is four sentences long. The vocabulary is 345 words. We used 25,000 scenarios from WEATHERGOV for training, 1,000 scenarios for development and 3,528 scenarios for testing.</p>
<p>For the second domain (henceforth WINHELP) we used the dataset of Branavan et al. (2009), which consists of 128 scenarios. These are articles from Microsoft's Help and Support website 4 and contain step-by-step instructions on how to perform tasks on the Windows 2000 operating system. In its original format, the database provides a semantic representation of the textual guide, i.e., it represents the user's actions on the operating system's UI. We semiautomatically converted this representation into a schema of records, fields and values, following the conventions adopted in Branavan et al. (2009). 5 The final database has 13 record types. Each scenario has 9.2 records and each document 51.92 words with 4.3 sentences. The vocabulary is 629 words. We performed 10-fold cross-validation on the entire dataset for training and testing. Compared to WEATHER-GOV, WINHELP documents are longer with a larger vocabulary. More importantly, due to the nature of the domain, i.e., giving instructions, content selection is critical not only in terms of what to say but also in what order.</p>
<p>Grammar Extraction and Parameter Setting</p>
<p>We obtained alignments between database records and textual segments for both domains and grammars (G RSE and G RST ) using the unsupervised model of Liang et al. (2009). On WEATHERGOV, we extracted a G RSE grammar with 663 rules (after bi- narization). The WINHELP dataset is considerably smaller, and as a result the procedure described in Section 5.1 yields a very sparse grammar. To alleviate this, we horizontally markovized the righthand side of each rule (Collins, 1999;Klein and Manning, 2003). 6 After markovization, we obtained a G RSE grammar with 516 rules. On WEATHERGOV, we extracted 434 rules for G RST . On WINHELP we could not follow the horizontal markovization procedure, since the discourse trees are already binarized. Instead, we performed vertical markovization, i.e., annotated each non-terminal with their parent node (Johnson, 1998) and obtained a G RST grammar with 419 rules. The model of Konstas and Lapata (2012) has two parameters, namely the number of k-best lists to keep in each derivation, and the order of the language model. We tuned k experimentally on the development set and obtained best results with 60 for WEATHERGOV and 120 for WIN-HELP. We used a trigram model for both domains, trained on each training set.</p>
<p>Evaluation We compared two configurations of our system, one with a content planning component based on record type sequences (G RSE ) and 6 When horizontally markovizing, we can encode an arbitrary amount of context in the intermediate non-terminals that result from this process; in our case we store h=1 horizontal siblings plus the mother left-hand side (LHS) non-terminal, in order to uniquely identify the Markov chain. For example,
A → B C D becomes A → B A . . . B , A . . . B → C A . . .C , A . . .C → D.
another one based on RST (G RST ). In both cases content plans were extracted from (noisy) unsupervised alignments. As a baseline, we used the original model of Konstas and Lapata (2012). We also compared our model to Angeli et al.'s system (2010), which is state of the art on WEATHERGOV.</p>
<p>System output was evaluated automatically, using the BLEU modified precision score (Papineni et al., 2002) with the human-written text as reference. In addition, we evaluated the generated text by eliciting human judgments. Participants were presented with a scenario and its corresponding verbalization and were asked to rate the latter along three dimensions: fluency (is the text grammatical?), semantic correctness (does the meaning conveyed by the text correspond to the database input?) and coherence (is the text comprehensible and logically structured?). Participants used a five point rating scale where a high number indicates better performance. We randomly selected 12 documents from the test set (for each domain) and produced output with the system of Konstas and Lapata (2012) (henceforth K&amp;L), our two models using G RSE and G RST , respectively, and Angeli et al. (2010) (henceforth ANGELI). We also included the original text (HUMAN) as gold-standard. We obtained ratings for 60 (12 × 5) scenario-text pairs for each domain. Examples of the documents shown to the participants are given in Table 1 </p>
<p>Results</p>
<p>The results of the automatic evaluation are summarized in Table 2. Overall, our models outperform K&amp;L's system by a wide margin on both datasets. The two content planners (G RSE and G RST ) perform comparably in terms of BLEU. This suggests that document plans induced solely from data are of similar quality to those informed by RST. This is an encouraging result given that RST-style discourse parsers are currently available only for English. AN-GELI performs better on WEATHERGOV possibly due to better output quality on the surface level. Their system defines trigger patterns that specifically lexicalize record fields containing numbers. In contrast, on WINHELP it is difficult to explicitly specify such patterns, as none of the record fields are numeric; as a result their system performs poorly compared to the other models.</p>
<p>To assess the impact of the alignment on the content planner, we also extracted G RSE from cleaner alignments which we obtained automatically via human-crafted heuristics for each domain. The heuristics performed mostly anchor matching between database records and words in the text (e.g., the value Lkly of the field rainChance, matches with the string rain likely in the text). Using these alignments, G RSE obtained a BLEU score of 39.23 on WEATHERGOV and 41.35 on WIN-HELP. These results indicate that improved alignments would lead to more accurate grammar rules. WEATHERGOV seems more sensitive to the alignments than WINHELP. This is probably because the dataset shows more structural variations in the choice of record types at the document level, and therefore the grammar extracted from the unsupervised alignments is noisier. Unfortunately, performing this kind of analysis for G RST would require gold standard segmentation of our training corpus into EDUs which we neither have nor can easily approximate via heuristics.</p>
<p>The results of our human evaluation study are shown in Table 3. We carried out an Analysis of Variance (ANOVA) to examine the effect of system   Table 3: Mean ratings for fluency (FL), semantic correctness (SC) and coherence (CO) on system output elicited by humans.</p>
<p>type (G RSE , G RST , K&amp;L, ANGELI, and HUMAN) on fluency, semantic correctness and coherence ratings. Means differences of 0.2 or more are significant at the 0.05 level using a post-hoc Tukey test. Interestingly, we observe that document planning improves system output overall, not only in terms of coherence. Across all dimensions our models are perceived better than K&amp;L and ANGELI. As far as coherence is concerned, the two content planners are rated comparably (differences in the means are not significant). Both G RSE and G RST are significantly better than the comparison systems (ANGELI and K&amp;L). Table 1 illustrates examples of system output along with the gold standard content selection for reference, for the WEATHERGOV and WINHELP domains, respectively.</p>
<p>In sum, we observe that integrating document planning either via G RSE or G RST boosts performance. Document plans induced from record sequences exhibit similar performance, compared to those generated using expert-derived linguistic knowledge. Our systems are consistently better than K&amp;L both in terms of automatic and human evaluation and are close or better than the supervised model of Angeli et al. (2010). We also show that feeding the system with a grammar of better quality can achieve state-of-the-art performance, without further changes to the model.</p>
<p>Conclusions</p>
<p>In this paper, we have proposed an end-to-end system that generates text from database input and captures all components of the traditional generation pipeline, including document planning. Document plans are induced automatically from training data and are represented intuitively by PCFG rules capturing the structure of the database and the way it renders to text. We proposed two complementary approaches to inducing content planners. In a first linguistically naive approach, a document is modelled as a sequence of sentences and each sentence as a sequence of records. Our second approach draws inspiration from Rhetorical Structure Theory (Mann and Thomson, 1988) and represents a document as a tree with intermediate nodes corresponding to discourse relations, and leaf nodes to database records.</p>
<p>Experiments with both approaches demonstrate improvements over models that do not incorporate document planning. In the future, we would like to tackle more challenging domains, such as NFL recaps, financial articles and biographies . Our models could also benefit from the development of more sophisticated planners either via grammar refinement or more expressive grammar formalisms (Cohn et al., 2010).</p>
<p>Figure 2 :
2Grammar G of the original model. Parentheses denote features, and impose constraints on the grammar.</p>
<p>Figure 4 :
4Distribution of RST relations on WEATHERGOV, WINHELP, and the RST-DT(Williams and Power, 2008).</p>
<p>Double-click users and passwords. Click start,] desktop 1 .t [point to settings, ] start 1 .t [and then click control panel.] start−target 1 .t [Double-click users and passwords.] win−target 1 .t [On the advanced tab,] contMenu 1 .t [click advanced.] action−contMenu 1 .tdesktop 1 </p>
<p>Click start, </p>
<p>start 1 </p>
<p>point to settings, </p>
<p>start-target 1 </p>
<p>and then click control panel. </p>
<p>win-target 1 </p>
<p>contMenu 1 </p>
<p>On the advanced tab ,p </p>
<p>action-contMenu 1 </p>
<p>click advanced. </p>
<p>(a) Record token alignments </p>
<p>desktop start start-target win-target contMenu action-contMenu </p>
<p>(b) Record type segmentation </p>
<p>[(c) Segmentation of text into EDUs </p>
<p>D </p>
<p>SENT(c, a-c) </p>
<p>. Right-click my network places, and then click properties. Right-click local area connection, and click properties. Click to select the file and printer sharing for Microsoft networks, and then click ok. Right-click my network places, and then click properties. Right-click local area connection. Click file and printer sharing for Microsoft networks, and click ok. Right-click my network places, and then click properties on the tools menu, and then click properties. Right-click local area connection, and then click properties. Click file and printer sharing for Microsoft networks, and then click ok. Right-click my network places, and then click properties. Right-click local area connection, and then click properties. Click to select the file and printer sharing for Microsoft networks check box. Click ok.The study was conducted over the Internet us-
WEATHERGOV </p>
<p>WINHELP </p>
<p>G </p>
<p>RSE </p>
<p>Showers before noon. Cloudy, with a high near 
38. Southwest wind between 3 and 8 mph. 
Chance of precipitation is 55 %. </p>
<p>G </p>
<p>RST </p>
<p>Showers likely. Mostly cloudy, with a high around 
38. South wind between 1 and 8 mph. Chance of 
precipitation is 55 %. </p>
<p>K&amp;L 
A chance of showers. Otherwise, cloudy, with a 
high near 38. Southwest wind between 3 and 8 
mph. </p>
<p>Right-click my network places, click properties. 
Right-click local area connection. Click to select the 
file and printer sharing for Microsoft networks, and 
then click ok. </p>
<p>ANGELI 
A chance of rain or drizzle after 9am. Mostly 
cloudy, with a high near 38. Southwest wind be-
tween 3 and 8 mph. Chance of precipitation is 50 
%. </p>
<p>HUMAN 
A 50 percent chance of showers. Cloudy, with a 
high near 38. Southwest wind between 3 and 6 
mph. </p>
<p>Table 1 :
1Human-authored text and system output on WEATHERGOV and WINHELP.ing Amazon Mechanical Turk 7 , and involved 200 volunteers (100 for WEATHERGOV, and 100 for WINHELP), all self reported native English speakers. For WINHELP, we made sure participants were computer-literate and familiar with the Windows operating system by administering a short questionnaire prior to the experiment.</p>
<p>Table 2 :
2Automatic evaluation of system output using BLEU-4.WEATHERGOV 
WINHELP </p>
<p>Model FL SC CO FL SC CO 
G RSE 
4.25 3.75 4.18 3.59 3.21 3.35 
G RST 
4.10 3.68 4.10 3.45 3.29 3.22 
K&amp;L 
3.73 3.25 3.59 3.27 2.97 2.93 
ANGELI 3.90 3.44 3.82 3.44 2.79 2.97 
HUMAN 4.22 3.72 4.11 4.20 4.41 4.25 </p>
<p>The function g( f .v) : Z → Z, generates an integer in the following six ways(Liang et al., 2009): identical, rounding up/down to a multiple of 5, rounding off a multiple of 5 and adding or subtracting some noise modelled by a geometric distribution.
Note that a sequence is different from a permutation, as we may allow repetitions or omissions of certain record types.
support.microsoft.com 5 The dataset can be downloaded from http://homepages. inf.ed.ac.uk/ikonstas/index.php?page=resources
https://www.mturk.com
AcknowledgmentsWe are grateful to Percy Liang and Gabor Angeli for providing us with their code and data. Thanks to Giorgio Satta and Charles Sutton for helpful comments and suggestions. We also thank the members of the Probabilistic Models reading group at the University of Edinburgh for useful feedback.
A simple domain-independent probabilistic approach to generation. Gabor Angeli, Percy Liang, Dan Klein, Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing. the 2010 Conference on Empirical Methods in Natural Language ProcessingCambridge, MAGabor Angeli, Percy Liang, and Dan Klein. 2010. A simple domain-independent probabilistic approach to generation. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 502-512, Cambridge, MA.</p>
<p>Automatic generation of weather forecast texts using comprehensive probabilistic generation-space models. Regina Barzilay, Mirella Lapata, Proceedings of Human Language Technology and Empirical Methods in Natural Language Processing. Human Language Technology and Empirical Methods in Natural Language ProcessingVancouver, British Columbia. Anja Belz14Collective content selection for concept-to-text generationRegina Barzilay and Mirella Lapata. 2005. Collec- tive content selection for concept-to-text generation. In Proceedings of Human Language Technology and Empirical Methods in Natural Language Processing, pages 331-338, Vancouver, British Columbia. Anja Belz. 2008. Automatic generation of weather forecast texts using comprehensive probabilis- tic generation-space models. Natural Language Engi- neering, 14(4):431-455.</p>
<p>Reinforcement learning for mapping instructions to actions. S R K Branavan, Harr Chen, Luke Zettlemoyer, Regina Barzilay, Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP. the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLPSuntec, SingaporeS.R.K. Branavan, Harr Chen, Luke Zettlemoyer, and Regina Barzilay. 2009. Reinforcement learning for mapping instructions to actions. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP, pages 82-90, Suntec, Singapore.</p>
<p>Discourse tagging reference manual. L Carlson, D Marcu, Univ. of Southern California / Information Sciences InstituteTechnical reportL. Carlson and D. Marcu. 2001. Discourse tagging ref- erence manual. Technical report, Univ. of Southern California / Information Sciences Institute.</p>
<p>Building a discourse-tagged corpus in the framework of rhetorical structure theory. Lynn Carlson, Daniel Marcu, Mary Ellen Okurowski, Proceedings of the Second SIGdial Workshop on Discourse. the Second SIGdial Workshop on DiscourseStroudsburg, PA, USAAssociation for Computational Linguistics16SIGDIAL '01Lynn Carlson, Daniel Marcu, and Mary Ellen Okurowski. 2001. Building a discourse-tagged corpus in the framework of rhetorical structure theory. In Proceed- ings of the Second SIGdial Workshop on Discourse and Dialogue -Volume 16, SIGDIAL '01, pages 1-10, Stroudsburg, PA, USA. Association for Computational Linguistics.</p>
<p>Trevor Cohn, Phil Blunsom, and Sharon Goldwater. 2010. Inducing tree-substitution grammars. L David, Raymond J Chen, Mooney, Proceedings of International Conference on Machine Learning. International Conference on Machine LearningHelsinki, Finland11Learning to sportscast: A test of grounded language acquisitionDavid L. Chen and Raymond J. Mooney. 2008. Learn- ing to sportscast: A test of grounded language acqui- sition. In Proceedings of International Conference on Machine Learning, pages 128-135, Helsinki, Finland. Trevor Cohn, Phil Blunsom, and Sharon Goldwater. 2010. Inducing tree-substitution grammars. Journal of Machine Learning Research, 11(November):3053- 3096.</p>
<p>Head-Driven Statistical Models for Natural Language Parsing. M Collins, University of PennsylvaniaPh.D. thesisM. Collins. 1999. Head-Driven Statistical Models for Natural Language Parsing. Ph.D. thesis, University of Pennsylvania.</p>
<p>Generating referring expressions in a domain of objects and processes. Robert Dale, University of EdinburghPh.D. thesisRobert Dale. 1988. Generating referring expressions in a domain of objects and processes. Ph.D. thesis, Uni- versity of Edinburgh.</p>
<p>Maximum likelihood from incomplete data via the em algorithm. A P Dempster, N M Laird, D B Rubin, Journal of the royal statistical society, series B. 391A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977. Maximum likelihood from incomplete data via the em algorithm. Journal of the royal statistical society, se- ries B, 39(1):1-38.</p>
<p>Empirically estimating order constraints for content planning in generation. Pablo A Duboue, Kathleen R Mckeown, Proceedings of the 39th Annual Meeting on Association for Computational Linguistics. the 39th Annual Meeting on Association for Computational LinguisticsPablo A. Duboue and Kathleen R. McKeown. 2001. Em- pirically estimating order constraints for content plan- ning in generation. In Proceedings of the 39th An- nual Meeting on Association for Computational Lin- guistics, pages 172-179.</p>
<p>Content planner construction via evolutionary algorithms and a corpus-based fitness function. Pablo A Duboue, Kathleen R Mckeown, Proceedings of International Natural Language Generation. International Natural Language GenerationRamapo Mountains, NYPablo A. Duboue and Kathleen R. McKeown. 2002. Content planner construction via evolutionary algo- rithms and a corpus-based fitness function. In Pro- ceedings of International Natural Language Genera- tion, pages 89-96, Ramapo Mountains, NY.</p>
<p>Text-level discourse parsing with rich linguistic features. Vanessa Wei Feng, Graeme Hirst, Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics. the 50th Annual Meeting of the Association for Computational LinguisticsJeju Island, KoreaVanessa Wei Feng and Graeme Hirst. 2012. Text-level discourse parsing with rich linguistic features. In Pro- ceedings of the 50th Annual Meeting of the Associa- tion for Computational Linguistics, pages 60-68, Jeju Island, Korea.</p>
<p>Automated discourse generation using discourse structure relations. Eduard Hovy, Artificial Intelligence. 63Eduard Hovy. 1993. Automated discourse generation using discourse structure relations. Artificial Intelli- gence, 63:341-385.</p>
<p>Domain adaptable semantic clustering in statistical nlg. Blake Howald, Ravikumar Kondadadi, Frank Schilder, Proceedings of the 10th International Conference on Computational Semantics (IWCS 2013) -Long Papers. the 10th International Conference on Computational Semantics (IWCS 2013) -Long PapersPotsdam, GermanyAssociation for Computational LinguisticsBlake Howald, Ravikumar Kondadadi, and Frank Schilder. 2013. Domain adaptable semantic clustering in statistical nlg. In Proceedings of the 10th Interna- tional Conference on Computational Semantics (IWCS 2013) -Long Papers, pages 143-154, Potsdam, Ger- many, March. Association for Computational Linguis- tics.</p>
<p>Pcfg models of linguistic tree representations. Mark Johnson, Computational Linguistics. 244Mark Johnson. 1998. Pcfg models of linguistic tree rep- resentations. Computational Linguistics, 24(4):613- 632, December.</p>
<p>Entity Coherence for Descriptive Text Structuring. Nikiforos Karamanis, University of EdinburghPh.D. thesisNikiforos Karamanis. 2003. Entity Coherence for De- scriptive Text Structuring. Ph.D. thesis, University of Edinburgh.</p>
<p>An efficient recognition and syntax analysis algorithm for context-free languages. Tadao Kasami, AFCRL-65-758Bedford, MAAir Force Cambridge Research LabTechnical ReportTadao Kasami. 1965. An efficient recognition and syntax analysis algorithm for context-free languages. Techni- cal Report AFCRL-65-758, Air Force Cambridge Re- search Lab, Bedford, MA.</p>
<p>Optimising referential coherence in text generation. Rodger Kibble, Richard Power, Computational Linguistics. 304Rodger Kibble and Richard Power. 2004. Optimising referential coherence in text generation. Computa- tional Linguistics, 30(4):401-416.</p>
<p>Generative alignment and semantic parsing for learning from ambiguous supervision. Joohyun Kim, Raymond Mooney, Proceedings of the 23rd Conference on Computational Linguistics. the 23rd Conference on Computational LinguisticsBeijing, ChinaJoohyun Kim and Raymond Mooney. 2010. Generative alignment and semantic parsing for learning from am- biguous supervision. In Proceedings of the 23rd Con- ference on Computational Linguistics, pages 543-551, Beijing, China.</p>
<p>Accurate unlexicalized parsing. Dan Klein, D Christopher, Manning, Proceedings of the 41st Annual Meeting on Association for Computational Linguistics. the 41st Annual Meeting on Association for Computational LinguisticsMorristown, NJ, USAAssociation for Computational LinguisticsDan Klein and Christopher D. Manning. 2003. Accurate unlexicalized parsing. In Proceedings of the 41st An- nual Meeting on Association for Computational Lin- guistics, pages 423-430. Association for Computa- tional Linguistics Morristown, NJ, USA.</p>
<p>Unsupervised concept-to-text generation with hypergraphs. Ioannis Konstas, Mirella Lapata, Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMontréal, CanadaIoannis Konstas and Mirella Lapata. 2012. Unsupervised concept-to-text generation with hypergraphs. In Pro- ceedings of the 2012 Conference of the North Ameri- can Chapter of the Association for Computational Lin- guistics: Human Language Technologies, pages 752- 761, Montréal, Canada.</p>
<p>Learning semantic correspondences with less supervision. Percy Liang, Michael Jordan, Dan Klein, Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP. the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLPSuntec, SingaporePercy Liang, Michael Jordan, and Dan Klein. 2009. Learning semantic correspondences with less supervi- sion. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th Interna- tional Joint Conference on Natural Language Process- ing of the AFNLP, pages 91-99, Suntec, Singapore.</p>
<p>Rhetorical structure theory: Toward a functional theory of text organization. C William, Sandra A Mann, Thompson, Text. 83William C. Mann and Sandra A. Thompson. 1988. Rhetorical structure theory: Toward a functional the- ory of text organization. Text, 8(3):243-281.</p>
<p>C William, Sandra A Mann, Thomson, Rhetorical structure theory. Text. 8William C. Mann and Sandra A. Thomson. 1988. Rhetorical structure theory. Text, 8(3):243-281.</p>
<p>Experiments using stochastic search for text planning. Chris Mellish, Alisdair Knott, Jon Oberlander, Mick O&apos; Donnell, Proceedings of International Natural Language Generation. International Natural Language GenerationNew Brunswick, NJChris Mellish, Alisdair Knott, Jon Oberlander, and Mick O'Donnell. 1998. Experiments using stochastic search for text planning. In Proceedings of Interna- tional Natural Language Generation, pages 98-107, New Brunswick, NJ.</p>
<p>Bleu: a method for automatic evaluation of machine translation. Kishore Papineni, Salim Roukos, Todd Ward, Wei-Jing Zhu, Proceedings of 40th. 40thKishore Papineni, Salim Roukos, Todd Ward, and Wei- Jing Zhu. 2002. Bleu: a method for automatic eval- uation of machine translation. In Proceedings of 40th</p>
<p>Annual Meeting of the Association for Computational Linguistics. Philadelphia, PennsylvaniaAnnual Meeting of the Association for Computational Linguistics, pages 311-318, Philadelphia, Pennsylva- nia.</p>
<p>Building natural language generation systems. Ehud Reiter, Robert Dale, Cambridge University PressNew York, NYEhud Reiter and Robert Dale. 2000. Building natural language generation systems. Cambridge University Press, New York, NY.</p>
<p>Choosing words in computer-generated weather forecasts. Ehud Reiter, Somayajulu Sripada, Jim Hunter, Ian Davy, Artificial Intelligence. 167Ehud Reiter, Somayajulu Sripada, Jim Hunter, and Ian Davy. 2005. Choosing words in computer-generated weather forecasts. Artificial Intelligence, 167:137- 169.</p>
<p>Gennext: A consolidated domain adaptable nlg system. Frank Schilder, Blake Howald, Ravi Kondadadi, Proceedings of the 14th European Workshop on Natural Language Generation. the 14th European Workshop on Natural Language GenerationSofia, Bulgaria, AugustAssociation for Computational LinguisticsFrank Schilder, Blake Howald, and Ravi Kondadadi. 2013. Gennext: A consolidated domain adaptable nlg system. In Proceedings of the 14th European Work- shop on Natural Language Generation, pages 178- 182, Sofia, Bulgaria, August. Association for Compu- tational Linguistics.</p>
<p>Getting the message across in RST-based text generation. Donia Scott, Clarisse Sieckenius De Souza, Current Research in Natural Language Generation. Robert Dale, Chris Mellish, and Michael ZockNew YorkAcademic PressDonia Scott and Clarisse Sieckenius de Souza. 1990. Getting the message across in RST-based text gener- ation. In Robert Dale, Chris Mellish, and Michael Zock, editors, Current Research in Natural Language Generation, pages 47-73. Academic Press, New York.</p>
<p>Trainable sentence planning for complex information presentation in spoken dialog systems. Amanda Stent, Rashmi Prasad, Marilyn Walker, Proceedings of Association for Computational Linguistics. Association for Computational LinguisticsBarcelona, SpainAmanda Stent, Rashmi Prasad, and Marilyn Walker. 2004. Trainable sentence planning for complex infor- mation presentation in spoken dialog systems. In Pro- ceedings of Association for Computational Linguis- tics, pages 79-86, Barcelona, Spain.</p>
<p>Deriving rhetorical complexity data from the rst-dt corpus. Sandra Williams, Richard Power, Proceedings of the Sixth International Language Resources and Evaluation (LREC'08). the Sixth International Language Resources and Evaluation (LREC'08)Sandra Williams and Richard Power. 2008. Deriving rhetorical complexity data from the rst-dt corpus. In Proceedings of the Sixth International Language Re- sources and Evaluation (LREC'08), May.</p>
<p>Generation by inverting a semantic parser that uses statistical machine translation. Yuk Wah Wong, Raymond Mooney, Proceedings of the Human Language Technology and the Conference of the North American Chapter of the Association for Computational Linguistics. the Human Language Technology and the Conference of the North American Chapter of the Association for Computational LinguisticsRochester, NYYuk Wah Wong and Raymond Mooney. 2007. Gener- ation by inverting a semantic parser that uses statis- tical machine translation. In Proceedings of the Hu- man Language Technology and the Conference of the North American Chapter of the Association for Com- putational Linguistics, pages 172-179, Rochester, NY.</p>
<p>Recognition and parsing for context-free languages in time n 3 . Information and Control. H Daniel, Younger, 10Daniel H Younger. 1967. Recognition and parsing for context-free languages in time n 3 . Information and Control, 10(2):189-208.</p>            </div>
        </div>

    </div>
</body>
</html>
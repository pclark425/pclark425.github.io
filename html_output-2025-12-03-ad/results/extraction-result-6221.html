<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6221 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6221</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6221</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-122.html">extraction-schema-122</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, including reported differences, limitations, failure cases, and mitigation strategies.</div>
                <p><strong>Paper ID:</strong> paper-e58231569f45704a2460bb6de3ec6f52efc5cf95</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/e58231569f45704a2460bb6de3ec6f52efc5cf95" target="_blank">Automated Annotation with Generative AI Requires Validation</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> It is argued that any automated annotation process using an LLM must validate the LLM's performance against labels generated by humans, and an outline of a workflow to harness the annotation potential of LLMs in a principled, efficient way is outlined.</p>
                <p><strong>Paper Abstract:</strong> Generative large language models (LLMs) can be a powerful tool for augmenting text annotation procedures, but their performance varies across annotation tasks due to prompt quality, text data idiosyncrasies, and conceptual difficulty. Because these challenges will persist even as LLM technology improves, we argue that any automated annotation process using an LLM must validate the LLM's performance against labels generated by humans. To this end, we outline a workflow to harness the annotation potential of LLMs in a principled, efficient way. Using GPT-4, we validate this approach by replicating 27 annotation tasks across 11 datasets from recent social science articles in high-impact journals. We find that LLM performance for text annotation is promising but highly contingent on both the dataset and the type of annotation task, which reinforces the necessity to validate on a task-by-task basis. We make available easy-to-use software designed to implement our workflow and streamline the deployment of LLMs for automated annotation.</p>
                <p><strong>Cost:</strong> 0.009</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6221.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6221.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, including reported differences, limitations, failure cases, and mitigation strategies.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4 vs Humans</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 used as an automated annotator compared to expert human labels</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Systematic validation of GPT-4 annotations against subject-matter-expert human labels across 27 binary annotation tasks (11 datasets) to measure agreement and error patterns; includes quantitative performance metrics and qualitative observations about when the LLM succeeds or fails.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Text annotation / content classification across multiple social-science tasks (e.g., hate/fear speech, topic classification, rhetorical elements, temporal focus, survey response coding).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>GPT-4 (OpenAI API)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Human labels were taken from the original studies and treated as ground truth; recommended validation used at least two subject-matter experts who annotate the same text samples using the same codebook as the LLM; typical suggested human validation sample sizes ranged from 250 to 1,250 random text samples (larger for rare classes); the workflow used a held-out test set for final evaluation after optional codebook refinements.</td>
                        </tr>
                        <tr>
                            <td><strong>metrics_compared</strong></td>
                            <td>Accuracy, precision, recall, F1 (per binary dimension), true positive rate (TPR), true negative rate (TNR), plus a consistency score for repeated LLM annotations.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_differences</strong></td>
                            <td>Across 27 tasks median accuracy = 0.850 and median F1 = 0.707; LLM recall tended to be stronger than precision (20/27 tasks had higher recall than precision). However, performance varied substantially by task and dataset: nine tasks had either precision or recall < 0.5, three tasks had both precision and recall < 0.5, and the lowest observed F1 was 0.06. Within a single dataset performance could vary widely (example: two Card et al. tasks had F1 = 0.259 and 0.811). LLM classifications that were fully consistent across repeated runs were ~19.4 percentage points more accurate than inconsistent ones.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_specific_limitations</strong></td>
                            <td>High heterogeneity in performance depending on dataset and conceptual difficulty; susceptibility to poor performance on nuanced/judgmental categories; precision deficits (false positives) in many tasks despite strong recall; some tasks produced extremely low F1; LLM is a black-box whose performance may change as models evolve; prompt/codebook sensitivity (though prompt tweaks often produced only modest gains).</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_cases</strong></td>
                            <td>One-third of tasks had very poor performance (precision or recall < 0.5); three tasks had both precision and recall < 0.5. The worst-case F1 was 0.06. Performance could flip dramatically even across related labels inside the same dataset (example within Card et al. where F1 ranged 0.259–0.811). Tasks requiring complex judgment rather than factual determination (e.g., nuanced hate speech/fear speech distinctions or subtle rhetorical attributions) frequently yielded more errors. The paper does not name the exact tasks per low metric in the main text but documents aggregate worst-case measures and within-dataset variability.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_strategies</strong></td>
                            <td>Always validate LLM outputs against a subset of high-quality human labels; use shared codebooks so humans and LLM use identical instructions; perform human-in-the-loop codebook updates (prompt engineering) when initial LLM performance is poor; compute a consistency score by repeating LLM classification at non-zero temperature and flag inconsistent examples for human review; disaggregate complex labels (e.g., break 'hate speech' into component indicators) to simplify tasks; if LLM achieves high recall, use it to identify candidate positives for human verification; use LLM-labeled data to fine-tune supervised classifiers when appropriate; abandon LLM use on dimensions with persistently unsatisfactory performance.</td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>The paper emphasizes validation on a task-by-task basis and demonstrates both the promise (low cost, speed, many high-performing tasks) and concrete quantitative limits (several low-performing tasks and large within-dataset variance).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automated Annotation with Generative AI Requires Validation', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6221.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6221.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, including reported differences, limitations, failure cases, and mitigation strategies.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Consistency score</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM annotation consistency score (modal-agreement across repeated runs)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An empirical measure of how stably the LLM assigns the same label to the same sample when classifications are repeated with stochasticity (temperature > 0); used as a proxy for LLM confidence/edge-case detection and compared against human labels.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>LLM-based annotation reliability assessment across text classification tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>GPT-4 (OpenAI API), repeated with temperature = 0.6 across seven runs in the study (recommendation: at least 3 runs with temperature > 0).</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Consistency was measured relative to the human-labeled ground truth described above; no extra human annotators were required specifically for consistency beyond the held-out human test set.</td>
                        </tr>
                        <tr>
                            <td><strong>metrics_compared</strong></td>
                            <td>Consistency (proportion of repeated LLM labels equal to the modal label), and its relationship to accuracy, true positive rate, and true negative rate versus human ground truth.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_differences</strong></td>
                            <td>Samples with a consistency score of 1.0 were 19.4 percentage points more accurate than samples with consistency < 1.0; TPR and TNR were 16.4 and 21.4 percentage points higher, respectively, for fully consistent classifications. 85.1% of labeled samples had a consistency score of 1.0 in the study.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_specific_limitations</strong></td>
                            <td>Consistency is an indirect proxy for correctness and can be high for consistently wrong behavior; choice of temperature and number of repetitions affect the metric; computing multiple runs increases cost and latency.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_cases</strong></td>
                            <td>Cases with lower consistency correlated with lower accuracy and thus represent edge cases where the LLM disagrees with human labels more often; the paper notes that such cases should be prioritized for human review but does not list specific text examples.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_strategies</strong></td>
                            <td>Use repeated-stochastic LLM runs to compute consistency and triage low-consistency items for human annotation; choose a moderate temperature (the study used 0.6) and repeat several times (they used seven) to estimate variance; combine consistency filtering with human review workflows to focus labeling effort where the LLM is unstable.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automated Annotation with Generative AI Requires Validation', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6221.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6221.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, including reported differences, limitations, failure cases, and mitigation strategies.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Codebook updates / prompt engineering</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Human-in-the-loop codebook refinement (prompt engineering) for LLM annotation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Iterative process where humans examine LLM misclassifications on a labeled subset, update the codebook/instructions to correct consistent errors, then re-run LLM annotations and re-evaluate on held-out data.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Improving LLM annotation performance via prompt/codebook adjustments across social-science annotation tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>GPT-4 (OpenAI API) used with updated codebooks; updates performed at most once per task in the study.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Researchers used the same human-labeled subset to detect systematic LLM errors, then subject-matter experts edited the codebook and used the revised instructions for subsequent LLM labeling; final evaluation used held-out human-labeled samples.</td>
                        </tr>
                        <tr>
                            <td><strong>metrics_compared</strong></td>
                            <td>Change in accuracy, precision, recall, and F1 on the training subset after one round of codebook updates (and comparison to held-out performance).</td>
                        </tr>
                        <tr>
                            <td><strong>reported_differences</strong></td>
                            <td>Codebook updates typically produced modest improvements in accuracy and F1; precision improved in a majority of cases while recall more often decreased than increased after updates; overall the magnitude of improvement was generally small, indicating limited leverage from single-round prompt edits.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_specific_limitations</strong></td>
                            <td>Prompt/codebook changes sometimes trade off precision and recall (improving one while hurting the other); one round of manual updates is often insufficient to fix hard conceptual errors; overfitting to the training subset is a risk if codebook updates are guided only by that subset.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_cases</strong></td>
                            <td>In some tasks, recall decreased after codebook updates even while precision improved, illustrating that prompt edits can worsen certain error modes; no single codebook tweak universally resolved poor-performing tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_strategies</strong></td>
                            <td>Use iterative human-in-the-loop cycles but validate improvements on held-out human-labeled data (not only the training subset) to avoid overfitting; if single-round prompt edits do not sufficiently improve performance, either (a) collect more human labels and repeat the refinement, (b) disaggregate complex labels into simpler components, or (c) stop using the LLM for that dimension.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automated Annotation with Generative AI Requires Validation', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Chatgpt outperforms crowd-workers for textannotation tasks <em>(Rating: 2)</em></li>
                <li>Testing the reliability of chatgpt for text annotation and classification: A cautionary remark <em>(Rating: 2)</em></li>
                <li>Annollm: Making large language models to be better crowdsourced annotators <em>(Rating: 2)</em></li>
                <li>Can chatgpt reproduce humangenerated labels? a study of social computing tasks <em>(Rating: 1)</em></li>
                <li>Can large language models transform computational social science? Working paper <em>(Rating: 1)</em></li>
                <li>Judging facts, judging norms: Training machine learning models to judge humans requires a modified approach to labeling data <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6221",
    "paper_id": "paper-e58231569f45704a2460bb6de3ec6f52efc5cf95",
    "extraction_schema_id": "extraction-schema-122",
    "extracted_data": [
        {
            "name_short": "GPT-4 vs Humans",
            "name_full": "GPT-4 used as an automated annotator compared to expert human labels",
            "brief_description": "Systematic validation of GPT-4 annotations against subject-matter-expert human labels across 27 binary annotation tasks (11 datasets) to measure agreement and error patterns; includes quantitative performance metrics and qualitative observations about when the LLM succeeds or fails.",
            "citation_title": "here",
            "mention_or_use": "use",
            "task_domain": "Text annotation / content classification across multiple social-science tasks (e.g., hate/fear speech, topic classification, rhetorical elements, temporal focus, survey response coding).",
            "llm_judge_model": "GPT-4 (OpenAI API)",
            "human_evaluation_setup": "Human labels were taken from the original studies and treated as ground truth; recommended validation used at least two subject-matter experts who annotate the same text samples using the same codebook as the LLM; typical suggested human validation sample sizes ranged from 250 to 1,250 random text samples (larger for rare classes); the workflow used a held-out test set for final evaluation after optional codebook refinements.",
            "metrics_compared": "Accuracy, precision, recall, F1 (per binary dimension), true positive rate (TPR), true negative rate (TNR), plus a consistency score for repeated LLM annotations.",
            "reported_differences": "Across 27 tasks median accuracy = 0.850 and median F1 = 0.707; LLM recall tended to be stronger than precision (20/27 tasks had higher recall than precision). However, performance varied substantially by task and dataset: nine tasks had either precision or recall &lt; 0.5, three tasks had both precision and recall &lt; 0.5, and the lowest observed F1 was 0.06. Within a single dataset performance could vary widely (example: two Card et al. tasks had F1 = 0.259 and 0.811). LLM classifications that were fully consistent across repeated runs were ~19.4 percentage points more accurate than inconsistent ones.",
            "llm_specific_limitations": "High heterogeneity in performance depending on dataset and conceptual difficulty; susceptibility to poor performance on nuanced/judgmental categories; precision deficits (false positives) in many tasks despite strong recall; some tasks produced extremely low F1; LLM is a black-box whose performance may change as models evolve; prompt/codebook sensitivity (though prompt tweaks often produced only modest gains).",
            "notable_failure_cases": "One-third of tasks had very poor performance (precision or recall &lt; 0.5); three tasks had both precision and recall &lt; 0.5. The worst-case F1 was 0.06. Performance could flip dramatically even across related labels inside the same dataset (example within Card et al. where F1 ranged 0.259–0.811). Tasks requiring complex judgment rather than factual determination (e.g., nuanced hate speech/fear speech distinctions or subtle rhetorical attributions) frequently yielded more errors. The paper does not name the exact tasks per low metric in the main text but documents aggregate worst-case measures and within-dataset variability.",
            "mitigation_strategies": "Always validate LLM outputs against a subset of high-quality human labels; use shared codebooks so humans and LLM use identical instructions; perform human-in-the-loop codebook updates (prompt engineering) when initial LLM performance is poor; compute a consistency score by repeating LLM classification at non-zero temperature and flag inconsistent examples for human review; disaggregate complex labels (e.g., break 'hate speech' into component indicators) to simplify tasks; if LLM achieves high recall, use it to identify candidate positives for human verification; use LLM-labeled data to fine-tune supervised classifiers when appropriate; abandon LLM use on dimensions with persistently unsatisfactory performance.",
            "notes": "The paper emphasizes validation on a task-by-task basis and demonstrates both the promise (low cost, speed, many high-performing tasks) and concrete quantitative limits (several low-performing tasks and large within-dataset variance).",
            "uuid": "e6221.0",
            "source_info": {
                "paper_title": "Automated Annotation with Generative AI Requires Validation",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "Consistency score",
            "name_full": "LLM annotation consistency score (modal-agreement across repeated runs)",
            "brief_description": "An empirical measure of how stably the LLM assigns the same label to the same sample when classifications are repeated with stochasticity (temperature &gt; 0); used as a proxy for LLM confidence/edge-case detection and compared against human labels.",
            "citation_title": "here",
            "mention_or_use": "use",
            "task_domain": "LLM-based annotation reliability assessment across text classification tasks.",
            "llm_judge_model": "GPT-4 (OpenAI API), repeated with temperature = 0.6 across seven runs in the study (recommendation: at least 3 runs with temperature &gt; 0).",
            "human_evaluation_setup": "Consistency was measured relative to the human-labeled ground truth described above; no extra human annotators were required specifically for consistency beyond the held-out human test set.",
            "metrics_compared": "Consistency (proportion of repeated LLM labels equal to the modal label), and its relationship to accuracy, true positive rate, and true negative rate versus human ground truth.",
            "reported_differences": "Samples with a consistency score of 1.0 were 19.4 percentage points more accurate than samples with consistency &lt; 1.0; TPR and TNR were 16.4 and 21.4 percentage points higher, respectively, for fully consistent classifications. 85.1% of labeled samples had a consistency score of 1.0 in the study.",
            "llm_specific_limitations": "Consistency is an indirect proxy for correctness and can be high for consistently wrong behavior; choice of temperature and number of repetitions affect the metric; computing multiple runs increases cost and latency.",
            "notable_failure_cases": "Cases with lower consistency correlated with lower accuracy and thus represent edge cases where the LLM disagrees with human labels more often; the paper notes that such cases should be prioritized for human review but does not list specific text examples.",
            "mitigation_strategies": "Use repeated-stochastic LLM runs to compute consistency and triage low-consistency items for human annotation; choose a moderate temperature (the study used 0.6) and repeat several times (they used seven) to estimate variance; combine consistency filtering with human review workflows to focus labeling effort where the LLM is unstable.",
            "uuid": "e6221.1",
            "source_info": {
                "paper_title": "Automated Annotation with Generative AI Requires Validation",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "Codebook updates / prompt engineering",
            "name_full": "Human-in-the-loop codebook refinement (prompt engineering) for LLM annotation",
            "brief_description": "Iterative process where humans examine LLM misclassifications on a labeled subset, update the codebook/instructions to correct consistent errors, then re-run LLM annotations and re-evaluate on held-out data.",
            "citation_title": "here",
            "mention_or_use": "use",
            "task_domain": "Improving LLM annotation performance via prompt/codebook adjustments across social-science annotation tasks.",
            "llm_judge_model": "GPT-4 (OpenAI API) used with updated codebooks; updates performed at most once per task in the study.",
            "human_evaluation_setup": "Researchers used the same human-labeled subset to detect systematic LLM errors, then subject-matter experts edited the codebook and used the revised instructions for subsequent LLM labeling; final evaluation used held-out human-labeled samples.",
            "metrics_compared": "Change in accuracy, precision, recall, and F1 on the training subset after one round of codebook updates (and comparison to held-out performance).",
            "reported_differences": "Codebook updates typically produced modest improvements in accuracy and F1; precision improved in a majority of cases while recall more often decreased than increased after updates; overall the magnitude of improvement was generally small, indicating limited leverage from single-round prompt edits.",
            "llm_specific_limitations": "Prompt/codebook changes sometimes trade off precision and recall (improving one while hurting the other); one round of manual updates is often insufficient to fix hard conceptual errors; overfitting to the training subset is a risk if codebook updates are guided only by that subset.",
            "notable_failure_cases": "In some tasks, recall decreased after codebook updates even while precision improved, illustrating that prompt edits can worsen certain error modes; no single codebook tweak universally resolved poor-performing tasks.",
            "mitigation_strategies": "Use iterative human-in-the-loop cycles but validate improvements on held-out human-labeled data (not only the training subset) to avoid overfitting; if single-round prompt edits do not sufficiently improve performance, either (a) collect more human labels and repeat the refinement, (b) disaggregate complex labels into simpler components, or (c) stop using the LLM for that dimension.",
            "uuid": "e6221.2",
            "source_info": {
                "paper_title": "Automated Annotation with Generative AI Requires Validation",
                "publication_date_yy_mm": "2023-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Chatgpt outperforms crowd-workers for textannotation tasks",
            "rating": 2
        },
        {
            "paper_title": "Testing the reliability of chatgpt for text annotation and classification: A cautionary remark",
            "rating": 2
        },
        {
            "paper_title": "Annollm: Making large language models to be better crowdsourced annotators",
            "rating": 2
        },
        {
            "paper_title": "Can chatgpt reproduce humangenerated labels? a study of social computing tasks",
            "rating": 1
        },
        {
            "paper_title": "Can large language models transform computational social science? Working paper",
            "rating": 1
        },
        {
            "paper_title": "Judging facts, judging norms: Training machine learning models to judge humans requires a modified approach to labeling data",
            "rating": 2
        }
    ],
    "cost": 0.008881749999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Automated Annotation with Generative AI Requires Validation</h1>
<p>Nicholas Pangakis, Samuel Wolken, and Neil Fasching ${ }^{\ddagger}$</p>
<h2>Abstract</h2>
<p>Generative large language models (LLMs) can be a powerful tool for augmenting text annotation procedures, but their performance varies across annotation tasks due to prompt quality, text data idiosyncrasies, and conceptual difficulty. Because these challenges will persist even as LLM technology improves, we argue that any automated annotation process using an LLM must validate the LLM's performance against labels generated by humans. To this end, we outline a workflow to harness the annotation potential of LLMs in a principled, efficient way. Using GPT-4, we validate this approach by replicating 27 annotation tasks across 11 datasets from recent social science articles in high-impact journals. We find that LLM performance for text annotation is promising but highly contingent on both the dataset and the type of annotation task, which reinforces the necessity to validate on a task-by-task basis. We make available easy-to-use software designed to implement our workflow and streamline the deployment of LLMs for automated annotation.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h1>1 Introduction</h1>
<p>Many tasks in natural language processing (NLP) depend on high-quality, manually-labeled text data for training and validation. The manual annotation process, however, poses non-trivial challenges. In addition to being time consuming and expensive, human annotators, usually crowdsourced workers ${ }^{1}$ or undergraduate assistants, often suffer from a limited attention span, fatigue, and changing perceptions of the underlying conceptual categories throughout the labeling procedures (Grimmer and Stewart, 2013; Neuendorf, 2016). When labeling large amounts of data, these limitations can lead to labeled text data that suffer from inconsistencies and errors that may be unobservable and correlated-especially when using coders with similar demographic backgrounds.</p>
<p>To address these challenges, researchers have recently explored the potential of generative large language models (LLMs), such as ChatGPT, to replace human annotators. LLMs are faster, cheaper, reproducible, and not susceptible to some of the pitfalls in human annotation. Prior applications using LLMs in this capacity, however, have yielded mixed results. Gilardi et al. (2023) claim that ChatGPT outperforms MTurkers on a variety of annotation tasks. Reiss (2023), on the other hand, finds that LLMs perform poorly on text annotation and argues that this tool should be used cautiously. Numerous other studies present similar analyses with varying results and recommendations (e.g., He et al., 2023; Wang et al., 2021; Ziems et al., 2023; Zhu et al., 2023; Ding et al., 2022).</p>
<p>Although existing research provides valuable insights into both the benefits and potential constraints of utilizing LLMs for text annotation, it is unclear whether the automated labeling workflows used in these studies can be confidently applied to other datasets and tasks, given that they report minimal performance metrics (i.e., only accuracy or intercoder agreement) and analyze</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>a few insular tasks on a small number of datasets. Moreover, due to the rapid development of LLM technology, it is likely that any binary recommendations about the competence of LLMs at one-off tasks will be quickly outdated with additional LLM advancements. While several studies so far have applied LLMs to a broader range of datasets (Ziems et al., 2023; Zhu et al., 2023), they exclusively test LLM annotation performance on popular, publicly available benchmark datasets. As such, these tests are plausibly affected by contamination (Brown et al., 2020), meaning that the datasets may be included in the LLM's training data and that strong performance may reflect memorization, which will not generalize to new datasets and tasks. Without clear guidance on a recommended workflow to use these tools safely and effectively, academics and practitioners alike could deploy this type of tool when it has suboptimal performance.</p>
<p>Our primary argument is that a researcher using an LLM for automated annotation must always validate the LLM's performance against a subset of high-quality human-annotated labels. ${ }^{2}$ While LLMs can be an effective tool to augment the annotation process, there are circumstances where LLMs fail to deliver accurate results due to factors such as ineffective prompts, noisy text data, and difficult annotation tasks. Because these challenges will persist even as LLM technology improves, ${ }^{3}$ researchers augmenting their text annotation approach with LLMs must always validate on a task-by-task basis. Rigorous validation can help researchers craft effective prompts and determine whether LLM annotation is viable for their classification tasks and datasets. To this end, we outline a recommended workflow to harness the potential of LLMs for text annotation in a principled, efficient way.</p>
<p>In this paper, we propose and test a workflow for augmenting and automating annotation</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>projects with LLMs. We validate our approach and test the capabilities of LLMs on a wide range of annotation tasks from different, non-public datasets using appropriate performance metrics. We use LLMs to replicate 27 different annotation processes from 11 non-public datasets used from articles recently featured in high-impact publications. In total, we classified over 200,000 text samples using an LLM (i.e., GPT-4). ${ }^{4}$</p>
<p>Our findings indicate that LLM performance for text annotation is promising but highly contingent on both the dataset and the type of annotation task. Across all tasks, we report a median accuracy of 0.850 and a median F1 of 0.707 . Despite the strong overall performance, nine of the 27 tasks had either precision or recall below 0.5 , which reinforces the necessity of researcher validation. Given the variation in performance across tasks, we identify four different use cases for automated annotation procedures guided by the LLMs validation performance. These include using an LLM to check the quality of human-labeled data, using an LLM to identify cases to prioritize for human review, using an LLM to produce labeled data to finetune and validate a supervised classifier, and using an LLM to classify an entire text corpus.</p>
<p>In addition to recommending a standardized process to validate when and how to use LLMs, we also introduce several novel tools in our automated annotation procedures. ${ }^{5}$ First, we make available easy-to-use software in Python designed to implement our methods and streamline the deployment of LLMs for automated annotation. Second, we show the utility of a consistency score. To measure how consistently an LLM predicts a particular text sample's label, we repeatedly classify each text sample at an LLM temperature of 0.6 . Treating the modal answer as the predicted LLM label, we approximate a degree of "consistency" across each LLM classification. Because there is a</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Workflow for augmenting text annotation with an LLM
strong correlation between higher consistency scores and the probability of the classification being correct, consistency scores are an effective way for researchers to identify edge cases.</p>
<h1>2 Workflow and validation</h1>
<p>In the absence of clear guidance on a recommended workflow for utilizing LLMs for annotation tasks, researchers run the risk of deploying these tools despite poor performance. Displayed in Figure 1, we offer a five-step workflow for incorporating LLMs in a way that foregrounds human judgment, includes an opportunity for human-in-the-loop refinement of instructions, and gives a clear indication of LLM performance with minimal ex-ante investment of resources. We designed</p>
<p>this workflow with two motivations: to validate LLM performance and to refine prompts for LLM classification when possible.</p>
<p>As Figure 1 shows, researchers should first craft a set of task-specific instructions (i.e., a codebook) ${ }^{6}$ and then have at least two subject matter experts and an LLM annotate the same text samples-with the sample size depending on the type of task and class imbalance. ${ }^{7}$ Critically, both the human coders and the LLM should use the same codebook for annotation, where the codebook serves as the LLM's prompt instructions. ${ }^{8}$ Then, the researcher should evaluate performance (i.e., accuracy, recall, precision, and F1) by comparing the predicted LLM labels against the human labels. As an initial step, researchers should first have the LLM annotate a subset of the humanlabeled text samples. If LLM performance is low on this subset, researchers can refine the codebook instructions by emphasizing incorrect classifications (repeating this process if necessary). ${ }^{9}$ Finally, using the updated codebook, researchers should test the LLM performance against the remaining human-labeled samples. Performance on this held-out data should be used to determine whether an LLM can be effectively utilized for automated annotation. The code made available in our GitHub repository offers a simple and efficient way to implement the procedures outlined in the above workflow.</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>To validate the proposed workflow, we use an LLM to replicate 27 annotation tasks across 11 datasets. To ensure these tasks represent a range of annotation tasks in contemporary social science research, we draw from research published in outlets across a spectrum of disciplines ranging from interdisciplinary publications (e.g., Science Advances and Proceedings of the National Academy of Sciences) to high-impact field journals in political science (e.g., American Political Science Review and American Journal of Political Science) and psychology (e.g., Journal of Personality and Social Psychology). In the Appendix, Table A1 includes the complete list of replication articles and Table A2 provides brief descriptions of the annotation tasks in this articles. ${ }^{10}$ These annotation tasks cover an extensive range of social science applications, from identifying whether Cold War-era texts pertained to foreign affairs or military matters (Schub, 2022) to analyzing open-ended survey responses to classify how people conceptualize where beliefs come from (Cusimano and Goodwin, 2020).</p>
<p>In each case, we replicate an annotation task using the human-labeled data from the original study as the ground truth. To avoid the potential for contamination, we rely exclusively on datasets stored in password-protected data archives (e.g., Dataverse) or datasets secured through direct outreach to authors. ${ }^{11}$ Whenever possible, we begin with the exact codebook used in the original research design. If this codebook is not available, we either quote or paraphrase text from the article or supplementary materials that describes the concepts of interest. ${ }^{12}$</p>
<p>Across all 27 tasks, we annotate slightly over 200,000 text samples using OpenAI's GPT-4 API. The overall cost was approximately $\$ 420$ USD. On average, a dataset with 1,000 text samples</p>
<p><sup id="fnref6:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>took approximately 2-3 hours to complete seven iterations (see "Consistency scores" section below). Together, the low cost and relatively rapid speed demonstrate the potential value of LLM-augmented annotation for many social science text analysis tasks.</p>
<h1>3 Results</h1>
<p>Classification results are shown in Table 1. The results reported here are based on "held out" text samples (i.e., not the text samples used in the codebook update process from Step 4 of the workflow). Across the 27 tasks, LLM classification performance achieved a median F1 score of 0.707 . Figure 2 shows performance on precision and recall for each classification task. As is apparent in this figure, LLM classification performance is stronger in recall than precision for 20 of the 27 tasks. On eight of the 27 tasks, the LLM achieves remarkably strong performance with precision and recall both exceeding 0.7 .</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Metric</th>
<th style="text-align: center;">Minimum</th>
<th style="text-align: center;">25th percentile</th>
<th style="text-align: center;">Mean</th>
<th style="text-align: center;">Median</th>
<th style="text-align: center;">75th percentile</th>
<th style="text-align: center;">Maximum</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Accuracy</td>
<td style="text-align: center;">0.674</td>
<td style="text-align: center;">0.808</td>
<td style="text-align: center;">0.855</td>
<td style="text-align: center;">0.85</td>
<td style="text-align: center;">0.905</td>
<td style="text-align: center;">0.981</td>
</tr>
<tr>
<td style="text-align: left;">Precision</td>
<td style="text-align: center;">0.033</td>
<td style="text-align: center;">0.472</td>
<td style="text-align: center;">0.615</td>
<td style="text-align: center;">0.650</td>
<td style="text-align: center;">0.809</td>
<td style="text-align: center;">0.957</td>
</tr>
<tr>
<td style="text-align: left;">Recall</td>
<td style="text-align: center;">0.25</td>
<td style="text-align: center;">0.631</td>
<td style="text-align: center;">0.749</td>
<td style="text-align: center;">0.829</td>
<td style="text-align: center;">0.899</td>
<td style="text-align: center;">0.982</td>
</tr>
<tr>
<td style="text-align: left;">F1</td>
<td style="text-align: center;">0.059</td>
<td style="text-align: center;">0.557</td>
<td style="text-align: center;">0.660</td>
<td style="text-align: center;">0.707</td>
<td style="text-align: center;">0.830</td>
<td style="text-align: center;">0.969</td>
</tr>
</tbody>
</table>
<p>Table 1: LLM classification performance across 27 tasks from 11 datasets.</p>
<p>Despite the strong overall performance, nine of the 27 tasks had either precision or recall below 0.5 - and three tasks had both precision and recall below 0.5 . Thus, for a full one-third of tasks, the LLM either missed at least half of the true positive cases, had more false positives than true positives, or both. As shown in Table 2, the aggregate performance ranged as low as an F1</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Precision and recall for each of 27 replicated classification tasks. Color reflects dataset, such that points sharing the same color are conducted on the same text data.</p>
<p>score of 0.06 . Moreover, LLM performance varied substantially across tasks within a single dataset. In the most extreme example, F1 ranged from 0.259 to 0.811 on two separate tasks within Card et al. (2022), a difference of 0.552 . These results demonstrate the variability of LLM annotation and, accordingly, underscore the need for task-specific validation.</p>
<h1>3.1 Consistency scores</h1>
<p>By inducing randomness in the LLM through the use of the temperature hyperparameter and by repeating the annotation task, we can generate an empirical measure of variance in the label that we deem a "consistency score." We recommend that the researcher have the LLM classify each sample at least three times with a temperature above $0 .{ }^{13}$ For our analyses, we use a temperature of 0.6 to label each text sample a minimum of seven times with the same codebook. ${ }^{14}$ As shown in Figure 3, consistency score is correlated with accuracy, true positive rate, and true negative rate. Given a vector of classifications, $C$, with length $l$ for a given classification task, consistency is measured as the proportion of classifications that match the modal classification $\left(\frac{1}{l} \sum_{i=1}^{l} C_{i}==C_{\text {mode }}\right)$. Across tasks, accuracy is 19.4 percentage points higher for text samples labeled with a consistency score of 1.0 compared with those labeled with a consistency score less than 1.0. The true positive rate and true negative rate are 16.4 percentage points and 21.4 percentage points higher, respectively, for fully consistent classifications. Of all the labeled samples, $85.1 \%$ have a consistency score of 1.0. Therefore, consistency scores offer a useful way of identifying edge cases or more difficult annotations.</p>
<p><sup id="fnref7:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Relationship between consistency score and accuracy, TPR, and TNR.</p>
<h1>3.2 Codebook updates</h1>
<p>For each of the 27 classification tasks, we follow the previously outlined workflow. In steps 3 and 4 of our workflow, we tested LLM classification performance on a subset of data and then, if relevant, made iterative human-in-the-loop updates to the codebook to optimize the prompt</p>
<p>for LLM classification performance. This step can be thought of as an application of "prompt engineering" in which the researchers attempts to identify patterns in LLM misclassifications and change the codebook to correct any consistent misperceptions. For each task, we did at most one round of codebook updates. To measure the effect that codebook updates had on LLM labeling, we re-label the training data subsets using the final codebooks.</p>
<p>Figure 4 shows the distributions of change in performance metrics after updating the codebook. This analysis demonstrates whether and how the codebook update process affects LLM annotation, holding constant the data and conceptual categories. In most cases, the codebook update process led to modest improvement in accuracy and F1. Recall decreased in more cases than improved after codebook updates. Precision, on the other hand, improved in a majority of cases, driving the improvement in accuracy and F1. Given these results, the subtleties of prompt construction do not appear to be a significant lever on performance. Still, although the magnitude of improvement was generally small, researchers experiencing subpar LLM classification performance on their text data can use human-in-the-loop codebook refinement to ensure that their instructions are not to blame.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Change in LLM annotation performance on training data after one round of codebook updates.</p>
<h1>4 Use cases</h1>
<p>If an LLM achieves satisfactory performance on domain-specific benchmarks for a given annotation task, there are numerous ways that the LLM could be integrated into a text analysis project. How researchers augment their project with an LLM may depend on the LLM performance against the human labeled data, their budget, the size of the dataset, and the availability of human annotators. Table 2 displays four potential use cases.</p>
<p>If the LLM's performance on some or all dimensions is poor, then the researcher can continue to refine the codebook using new labeled data or abandon the use of LLMs for the dimensions with unsatisfactory performance. If researchers opt to continue using an LLM despite initial poor performance, one strategy to simplify complex annotations may be disaggregation. For instance, if a</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Use case</th>
<th style="text-align: left;">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">1) Confirming the quality of <br> human-labeled data</td>
<td style="text-align: left;">If a researcher has data that is already labeled by human <br> annotators, an LLM may augment the text analysis process <br> by confirming the quality of the human labels. If LLM per- <br> formance against the human labels is high, this signals that <br> both the LLM and humans made similar annotation decisions. <br> If LLM performance is low, this indicates that either the hu- <br> mans, the LLM, or both made mistakes during the annotation <br> process.</td>
</tr>
<tr>
<td style="text-align: left;">2) Identifying cases to priori- <br> itize for human review</td>
<td style="text-align: left;">A researcher can manually review annotations with consis- <br> tency scores lower than 1.0. Additionally, if the particular <br> LLM task achieved high recall, then the researcher may use <br> the LLM to identify potential positive cases in previously un- <br> seen data. Then, human annotators can manually review all <br> positive-labeled cases.</td>
</tr>
<tr>
<td style="text-align: left;">3) Producing labeled data <br> to finetune and validate a <br> supervised classifier</td>
<td style="text-align: left;">There are various situations in which a researcher may use <br> an LLM to procure training data to finetune a supervised <br> classifier for labeling their corpus.</td>
</tr>
<tr>
<td style="text-align: left;">4) Classifying the entire cor- <br> pus directly</td>
<td style="text-align: left;">In the simplest case, a researcher may find LLM performance <br> to be satisfactory and opt to use the LLM to classify the entire <br> remaining corpus.</td>
</tr>
</tbody>
</table>
<p>Table 2: Use cases for automated annotation with LLMs.</p>
<p>researcher finds poor annotation performance on the dimension of "hate speech," they may achieve better performance by disaggregating "hate speech" into component indicators (such as threats, slurs, stereotyping, and so on) and adding each of the separate indicators as new dimensions into the codebook, then beginning again at Step 1 in the workflow.</p>
<h1>5 Conclusion</h1>
<p>We observe significant heterogeneity in LLM performance across a range of social science annotation tasks. Performance depends on both attributes of the text and the conceptual categories being measured. For instance, whether a classification task is a factual question about a text sample or requires judgement has significant bearing on classification strategy (Balagopalan et al., 2023). To address the myriad sources of variation in annotation performance, we recommend a flexible approach to LLM-augmented annotation that foregrounds human annotation.</p>
<p>To address these challenges, we present an original workflow for augmenting text annotation with an LLM along with several use cases for this workflow. We validate the workflow by replicating 27 annotation tasks taken from 11 social science articles published in high-impact journals. We find that LLMs can offer high-quality labels on a wide variety of tasks for a fraction of the cost and time of alternative options, such as crowd-sourced workers and undergraduate research assistants. Still, it is imperative that researchers validate the performance of LLMs on a task-by-task basis, as we find significant heterogeneity in performance, even across tasks within a single dataset.</p>
<h1>References</h1>
<p>Balagopalan, A., Madras, D., Yang, D. H., Hadfield-Menell, D., Hadfield, G. K., and Ghassemi, M. (2023). Judging facts, judging norms: Training machine learning models to judge humans requires a modified approach to labeling data. Science Advances, 9(19):eabq0701.</p>
<p>Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever, I., and Amodei, D. (2020). Language models are few-shot learners. In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., and Lin, H., editors, Advances in Neural Information Processing Systems, volume 33, pages 1877-1901. Curran Associates, Inc.</p>
<p>Card, D., Chang, S., Becker, C., Mendelsohn, J., Voigt, R., Boustan, L., Abramitzky, R., and Jurafsky, D. (2022). Computational analysis of 140 years of us political speeches reveals more positive but increasingly polarized framing of immigration. Proceedings of the National Academy of Sciences of the United States of America, 31.</p>
<p>Chmielewski, M. and Kucker, S. C. (2020). An mturk crisis? shifts in data quality and the impact on study results. Social Psychological and Personality Science, 11(4):464-473.</p>
<p>Crabtree, B. F. and Miller, W. L. (1992). A template approach to text analysis: Developing and using codebooks. In Doing qualitative research. Sage Publications.</p>
<p>Cusimano, C. and Goodwin, G. P. (2020). People judge others to have more voluntary control over beliefs than they themselves do. Journal of Personality and Social Psychology, 119.</p>
<p>Ding, B., Qin, C., Liu, L., Bing, L., Joty, S., and Li, B. (2022). Is gpt-3 a good data annotator?
Douglas, B. D., Ewell, P. J., and Braue, M. (2023). Data quality in online human-subjects research: Comparisons between mturk, prolific, cloudresearch, qualtrics, and sona. PLoS One, 18.</p>
<p>Gilardi, F., Alizadeh, M., and Kubli, M. (2023). Chatgpt outperforms crowd-workers for textannotation tasks.</p>
<p>Grimmer, J. and Stewart, B. M. (2013). Text as data: The promise and pitfalls of automatic content analysis methods for political texts. Political Analysis, 21(3):267-297.</p>
<p>He, X., Lin, Z., Gong, Y., Jin, A.-L., Zhang, H., Lin, C., Jiao, J., Yiu, S. M., Duan, N., and Chen, W. (2023). Annollm: Making large language models to be better crowdsourced annotators.</p>
<p>Kennedy, B., Atari, M., Davani, A. M., Yeh, L., Omrani, A., Kim, Y., Coombs, K., Havaldar, S., Portillo-Wightman, G., Gonzalez, E., Hoover, J., Azatian, A., Hussain, A., Lara, A., Olmos, G., Omary, A., Park, C., Wijaya, C., Wang, X., Zhang, Y., and Dehghani, M. (2022). Introducing the gab hate corpus: defining and applying hate-based rhetoric to social media posts at scale. Lang Resources $\mathcal{G}$ Evaluation.</p>
<p>Krippendorff, K. (2018). Content Analysis: An Introduction to Its Methodology. Sage, 4 edition.
MacQueen, K. M., McLellan, E., Kay, K., and Milstein, B. (1998). Codebook development for team-based qualitative analysis. CAM Journal, 10(2):31-36.</p>
<p>Neuendorf, K. A. (2016). The Content Analysis Guidebook. Sage Publications.
Reiss, M. (2023). Testing the reliability of chatgpt for text annotation and classification: A cautionary remark. Working paper.</p>
<p>Saha, P., Narla, Kalyan, K., and Mukherjee, A. (2023). On the rise of fear speech in online social media. Proceedings of the National Academy of Sciences of the United States of America.</p>
<p>Schub, R. (2022). Informing the leader: Bureaucracies and international crises. American Political Science Review, 116.</p>
<p>Spirling, A. (2023). Why open-source generative ai models are an ethical way forward for science. Nature, 413 .</p>
<p>Wang, S., Liu, Y., Xu, Y., Zhu, C., and Zeng, M. (2021). Want to reduce labeling cost? GPT-3 can help. In Findings of the Association for Computational Linguistics: EMNLP 2021, pages 4195-4205, Punta Cana, Dominican Republic. Association for Computational Linguistics.</p>
<p>Zhu, Y., Zhang, P., Haq, E.-U., Hui, P., and Tyson, G. (2023). Can chatgpt reproduce humangenerated labels? a study of social computing tasks.</p>
<p>Ziems, C., Held, W., Shaikh, O., Chen, J., , Zhang, Z., and Yang, D. (2023). Can large language models transform computational social science? Working paper.</p>
<h1>Appendix</h1>
<table>
<thead>
<tr>
<th style="text-align: center;">Author(s)</th>
<th style="text-align: center;">Title</th>
<th style="text-align: center;">Journal</th>
<th style="text-align: center;">Year</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Gohdes</td>
<td style="text-align: center;">Repression Technology: Internet Accessibility and State Violence</td>
<td style="text-align: center;">American Journal of Political Science</td>
<td style="text-align: center;">2020</td>
</tr>
<tr>
<td style="text-align: center;">Hopkins, Lelkes, and Wolken</td>
<td style="text-align: center;">The Rise of and Demand for IdentityOriented Media Coverage</td>
<td style="text-align: center;">American Journal of Political Science</td>
<td style="text-align: center;">(R\&amp;R)</td>
</tr>
<tr>
<td style="text-align: center;">Schub</td>
<td style="text-align: center;">Informing the Leader: Bureaucracies and International Crises</td>
<td style="text-align: center;">American Political Science Review</td>
<td style="text-align: center;">2022</td>
</tr>
<tr>
<td style="text-align: center;">Busby, and Gubler, Hawkins</td>
<td style="text-align: center;">Framing and blame attribution in populist rhetoric</td>
<td style="text-align: center;">Journal of Politics</td>
<td style="text-align: center;">2019</td>
</tr>
<tr>
<td style="text-align: center;">Müller</td>
<td style="text-align: center;">The Temporal Focus of Campaign Communication</td>
<td style="text-align: center;">Journal of Politics</td>
<td style="text-align: center;">2021</td>
</tr>
<tr>
<td style="text-align: center;">Cusimano and Goodwin</td>
<td style="text-align: center;">People judge others to have more voluntary control over beliefs than they themselves do</td>
<td style="text-align: center;">Journal of Personality and Social Psychology</td>
<td style="text-align: center;">2020</td>
</tr>
<tr>
<td style="text-align: center;">Yu and Zhang</td>
<td style="text-align: center;">The Impact of Social Identity Conflict on Planning Horizons</td>
<td style="text-align: center;">Journal of Personality and Social Psychology</td>
<td style="text-align: center;">2022</td>
</tr>
<tr>
<td style="text-align: center;">Card et al.</td>
<td style="text-align: center;">Computational analysis of 140 years of US political speeches reveals more positive but increasingly polarized framing of immigration</td>
<td style="text-align: center;">PNAS</td>
<td style="text-align: center;">2022</td>
</tr>
<tr>
<td style="text-align: center;">Peng, Romero, and Horvat</td>
<td style="text-align: center;">Dynamics of cross-platform attention to retracted papers</td>
<td style="text-align: center;">PNAS</td>
<td style="text-align: center;">2022</td>
</tr>
<tr>
<td style="text-align: center;">Saha et al.</td>
<td style="text-align: center;">On the rise of fear speech in online social media ${ }^{15}$</td>
<td style="text-align: center;">PNAS</td>
<td style="text-align: center;">2022</td>
</tr>
<tr>
<td style="text-align: center;">Wojcieszak et al.</td>
<td style="text-align: center;">Most users do not follow political elites on Twitter; those who do show overwhelming preferences for ideological congruity</td>
<td style="text-align: center;">Science Advances</td>
<td style="text-align: center;">2022</td>
</tr>
</tbody>
</table>
<p>Table A1: Sources of annotation tasks that replicated in analysis.</p>
<p><sup id="fnref8:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: center;">Study</th>
<th style="text-align: center;">Annotation tasks</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Gohdes (2020)</td>
<td style="text-align: center;">Code Syrian death records for specific type of killing: targeted or untargeted</td>
</tr>
<tr>
<td style="text-align: center;">Hopkins, Lelkes, \&amp; Wolken (2023)</td>
<td style="text-align: center;">Coding headlines, tweets, and Facebook share blurbs to identify references to social groups defined by a) race/ethnicity; b) gender/sexuality; c) politics; d) religion</td>
</tr>
<tr>
<td style="text-align: center;">Schub (2020)</td>
<td style="text-align: center;">Code presidential-level deliberation texts from the Cold War as political or military</td>
</tr>
<tr>
<td style="text-align: center;">Busby, Gubler, \&amp; Hawkins (2019)</td>
<td style="text-align: center;">Code open-ended responses for three rhetorical elements: attribution of blame to a specific actor, the attribution of blame to a nefarious elite actor, and a positive mention of the collective people</td>
</tr>
<tr>
<td style="text-align: center;">Müller (2021)</td>
<td style="text-align: center;">Code sentences from party manifestos for temporal direction: past, present, or future</td>
</tr>
<tr>
<td style="text-align: center;">Cusimano \&amp; Goodwin (2020)</td>
<td style="text-align: center;">Code respondents' written statements on climate change for the presence of either (a) generic reasoning about beliefs or (b) supporting evidence for the belief</td>
</tr>
<tr>
<td style="text-align: center;">Yu \&amp; Zhang (2023)</td>
<td style="text-align: center;">Code respondents' plans for the future into two categories:proximate future and distant future</td>
</tr>
<tr>
<td style="text-align: center;">$\begin{array}{lll} \text { Card } &amp; \text { et } &amp; \text { al. } \ (2022) \end{array}$</td>
<td style="text-align: center;">Code congressional speeches for whether they are about immigration, along with an accompanying tone: proimmigration, antiimmigration, or neutral</td>
</tr>
<tr>
<td style="text-align: center;">Peng, Romero, \&amp; Horvat (2022)</td>
<td style="text-align: center;">Code whether tweets express criticism with respect to the findings of academic papers</td>
</tr>
<tr>
<td style="text-align: center;">$\begin{array}{lll}\text { Saha } &amp; \text { et } &amp; \text { al. } \ (2020) \end{array}$</td>
<td style="text-align: center;">Code Gab posts as a) fear speech, b) hate speech, or c) normal. Further, a post could have both fear and hate components, and,thus, these were annotated with multiple labels</td>
</tr>
<tr>
<td style="text-align: center;">$\begin{array}{lll}\text { Wojcieszak et al. } \ (2020) \end{array}$</td>
<td style="text-align: center;">Code whether a quote tweet was negative, neutral or positive toward the message and/or the political actor, independently of the tone of the original message</td>
</tr>
</tbody>
</table>
<p>Table A2: Descriptions of annotation tasks replicated in analysis.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{15}$ This article uses the Gab hate speech corpus (Kennedy et al., 2022). We include Saha et al. (2023) here, rather than the original source of the labeled data, to emphasize the application of these data in applied social science research.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref6:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref7:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref8:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
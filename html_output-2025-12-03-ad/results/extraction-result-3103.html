<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3103 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3103</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3103</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-77.html">extraction-schema-77</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models solving puzzle games that require spatial knowledge (such as Sudoku), including details about the models, the puzzles, the methods used, performance, and any analysis of how the models solve these tasks.</div>
                <p><strong>Paper ID:</strong> paper-270688598</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2406.14852v2.pdf" target="_blank">Is A Picture Worth A Thousand Words? Delving Into Spatial Reasoning for Vision Language Models</a></p>
                <p><strong>Paper Abstract:</strong> Large language models (LLMs) and vision-language models (VLMs) have demonstrated remarkable performance across a wide range of tasks and domains. Despite this promise, spatial understanding and reasoning -- a fundamental component of human cognition -- remains under-explored. We propose SpatialEval, a novel benchmark that covers diverse aspects of spatial reasoning such as relationship understanding, navigation, and counting. We conduct a comprehensive evaluation of competitive language and vision-language models. Our findings reveal several counter-intuitive insights that have been overlooked in the literature: (1) Spatial reasoning poses significant challenges where competitive models can fall behind random guessing; (2) Despite additional visual input, VLMs often under-perform compared to their LLM counterparts; (3) When both textual and visual information is available, multi-modal language models become less reliant on visual information if sufficient textual clues are provided. Additionally, we demonstrate that leveraging redundancy between vision and text can significantly enhance model performance. We hope our study will inform the development of multimodal models to improve spatial intelligence and further close the gap with human intelligence.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3103.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3103.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models solving puzzle games that require spatial knowledge (such as Sudoku), including details about the models, the puzzles, the methods used, performance, and any analysis of how the models solve these tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Spatial-Grid (aggregate)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Spatial-Grid task from SpatialEval benchmark</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A grid-based synthetic spatial reasoning task in SpatialEval where each cell contains an object and questions probe counting and coordinate-based lookup; evaluated in text-only (TQA), vision-only (VQA) and vision+text (VTQA) modalities across many LLMs and VLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Various LLMs and VLMs (LLaMA, Mistral, LLaVA family, Bunny, InstructBLIP, CogVLM, proprietary GPT/Gemini/Claude)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>The paper evaluates a broad set of open-source LLMs (LLaMA family, Mistral-7B, Vicuna, Phi-2, Nous-Hermes etc.) and multimodal / vision-language models (LLaVA family, InstructBLIP, Bunny variants, CogVLM, CogAgent), plus proprietary models (GPT-4, GPT-4V, GPT-4o, Gemini Pro, Claude 3 Opus). VLMs convert image inputs via a visual encoder into tokens for the language backbone and are compared to pure LLMs on text-only inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Spatial-Grid</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_description</strong></td>
                            <td>A rigid grid of cells where each cell contains an object (icons/images). Questions ask e.g., how many of a given object exist, or which object is at a specified row/column — requiring counting, spatial indexing and coordinate reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>input_representation</strong></td>
                            <td>Three modalities studied: (1) TQA (text-only): full textual representation of the grid (rows/columns listed); (2) VQA (vision-only): image of the grid only; (3) VTQA (vision+text): image plus long textual caption describing the grid (redundant information).</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>All models were prompted with the same appended instruction: 'First, provide a concise answer in one sentence. Then, elaborate on the reasoning behind your answer in a detailed, step-by-step explanation.' Deterministic decoding for some models; top-p/temperature averaged over runs for nondeterministic models.</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_reasoning_analysis</strong></td>
                            <td>Aggregate analyses show VLMs often fail to leverage raw images for Spatial-Grid; textual descriptions (TQA/VTQA) substantially help. Ablations include replacing the image with no image, with a random image, and with noise: removing the original image often improved VLM performance (indicating models rely on text more than vision). VTQA (vision+text) outperforms VQA (vision-only), showing redundancy helps; however, VLMs sometimes rely less on vision when text is present.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported accuracies averaged across questions. Examples given in paper: LLaMA-3 (LLM) achieved 71.9% on Spatial-Grid (TQA), Mistral-7B-Instruct ≈ 62.1% (TQA). LLaVA-1.6-Mistral-7B (VLM) yielded 47.1% (VQA) on Spatial-Grid — a 15% drop vs the LLM backbone. Proprietary GPT-4o achieved 0.989 accuracy on Spatial-Grid in VTQA (reported in Table 7). Vision-only to vision+text improvements: GPT-4V improved by 25.6% on Spatial-Grid when switching from VQA to VTQA.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_modes</strong></td>
                            <td>Many VLMs perform at-or-below random-guessing baseline (25% for 4-choice questions) in vision-only settings. VLMs appear to perform worse when provided only the raw image compared to when given the corresponding textual description; adding visual input can even hurt performance unless accompanied by a descriptive caption. Random/mismatched images do not always degrade VLM performance, implying weak reliance on image content.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models_or_humans</strong></td>
                            <td>LLMs on text-only often outperform VLMs on vision-only for the same language backbone; LLMs (text-only) approach far better accuracy than VLMs given only images. Proprietary models outperform open-source broadly (GPT-4/GPT-4V/GPT-4o > open-source), but the same modality trends hold (vision-only < text-only/vision+text). Human performance on these synthetic grid tasks is near-perfect (paper notes humans do very well), so even best models still lag human-level performance on pure visual variants.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Is A Picture Worth A Thousand Words? Delving Into Spatial Reasoning for Vision Language Models', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3103.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3103.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models solving puzzle games that require spatial knowledge (such as Sudoku), including details about the models, the puzzles, the methods used, performance, and any analysis of how the models solve these tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Maze-Nav (aggregate)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Maze-Nav task from SpatialEval benchmark</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A maze navigation task in SpatialEval where the maze is presented as an image or ASCII/text and questions ask e.g., number of turns from S to E or relative positions, testing sequential path and spatial-turn reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Various LLMs and VLMs (LLaMA family, LLaVA family, InstructBLIP, Bunny variants, proprietary models)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Same set of LLMs and VLMs as evaluated across SpatialEval. Maze-Nav inputs include colored visual mazes (start S, end E, walls) and equivalent ASCII textual representations.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Maze-Nav</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_description</strong></td>
                            <td>Navigation problem where a designated route (marked X or blue path) from S to E is specified; questions require counting right/left turns and determining spatial relationships between S and E, requiring understanding of path geometry and sequence of moves.</td>
                        </tr>
                        <tr>
                            <td><strong>input_representation</strong></td>
                            <td>Evaluated as Vision-only (image of maze), Text-only (ASCII textual representation describing path), and Vision+Text (image plus textual description).</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Same step-by-step explanation prompt as for other tasks; paper reports experiments with no-image, random-image and noise-image ablations to probe reliance on vision vs text.</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_reasoning_analysis</strong></td>
                            <td>VLMs often improved when the visual image was removed (No Image ablation) and instead text-only (ASCII) was used, indicating VLM visual pipelines were not reliably extracting the route or turn-count information. Replacing the original image with random but task-related images sometimes improved VLM performance, again suggesting weak or noisy use of vision features.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Averaged accuracies across questions; paper states that for Maze-Nav many multimodal models perform at or near random in vision-only; exact per-model numbers are provided in appendices (not fully enumerated in main text).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_modes</strong></td>
                            <td>Failure modes include inability to trace the path from raw visual maze images, miscounting turns, and being distracted by irrelevant visual content. VLMs sometimes perform worse with original image than with no image, implying visual encoder introduces noise.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models_or_humans</strong></td>
                            <td>LLMs on text-only (ASCII) frequently outperform VLMs on vision-only. Proprietary models still follow same trends but with higher absolute accuracy; humans solve Maze-Nav easily while many models struggle.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Is A Picture Worth A Thousand Words? Delving Into Spatial Reasoning for Vision Language Models', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3103.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3103.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models solving puzzle games that require spatial knowledge (such as Sudoku), including details about the models, the puzzles, the methods used, performance, and any analysis of how the models solve these tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Spatial-Map (aggregate)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Spatial-Map task from SpatialEval benchmark</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A synthetic map reasoning task where named locations are placed with pairwise relations (e.g., 'A is southeast of B') and questions ask relative position and counting by spatial criteria.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Various LLMs and VLMs (LLaMA, Mistral, LLaVA, Bunny variants, proprietary models)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Evaluations employ LLMs and VLMs with textual pairwise relation representations or image-like map visualizations. The textual representation explicitly lists pairwise relations; visuals show map layout.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Spatial-Map</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_description</strong></td>
                            <td>A map-like environment with K objects/locations; requires relational spatial reasoning across named landmarks (e.g., determining which locations are north/south/east/west of others or counting locations meeting spatial criteria).</td>
                        </tr>
                        <tr>
                            <td><strong>input_representation</strong></td>
                            <td>TQA: text listing pairwise relations; VQA: image of map; VTQA: both map image and textual relation list.</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Step-by-step explanation prompt; ablations include no-image and random-image replacements to test reliance on vision.</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_reasoning_analysis</strong></td>
                            <td>VLMs rarely improved over their LLM backbone when relying on vision-only; text descriptions often suffice and sometimes lead to better results. VTQA (vision+text) improves over VQA for many models (redundancy helps), but VLMs sometimes under-use vision when text is present.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported average accuracies: many VLMs in vision-only perform near or below random guessing; specific model averages are shown in paper figures and appendices (aggregate trends emphasized).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_modes</strong></td>
                            <td>VLMs may misinterpret spatial relationships from images; translation of visual layout to language tokens in existing VLM pipelines can be noisy, causing worse-than-text-only performance.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models_or_humans</strong></td>
                            <td>LLMs with TQA often outperform VLMs with VQA; VTQA generally outperforms VQA but not always TQA. Humans again perform near-perfectly on Spatial-Map tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Is A Picture Worth A Thousand Words? Delving Into Spatial Reasoning for Vision Language Models', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3103.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e3103.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models solving puzzle games that require spatial knowledge (such as Sudoku), including details about the models, the puzzles, the methods used, performance, and any analysis of how the models solve these tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Spatial-Real (aggregate)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Spatial-Real task (real images with long captions) from SpatialEval</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Spatial reasoning questions curated from the Densely Captioned Images (DCI) dataset where each image has very long captions (~1000 words) and multiple-choice spatial reasoning questions are asked.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Various LLMs and VLMs (evaluated in the paper)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Uses real-image captions (DCI) paired with curated multiple-choice spatial questions; evaluates the same model families and proprietary models under TQA, VQA and VTQA formats.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Spatial-Real</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_description</strong></td>
                            <td>Natural images with dense captions; questions probe counting, relation and position understanding about real-world scenes, requiring interpretation of both long textual captions and complex images.</td>
                        </tr>
                        <tr>
                            <td><strong>input_representation</strong></td>
                            <td>TQA: dense caption text-only; VQA: image-only; VTQA: both image and dense caption.</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Step-by-step explanation prompt; same ablations and comparisons as synthetic tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_reasoning_analysis</strong></td>
                            <td>Trends from synthetic tasks persist: VTQA > VQA, and many VLMs rely more on text than vision. The modality gap (VTQA - VQA) is larger on Spatial-Real (≈30.0%) than on synthetic tasks (≈7.0%), indicating real images create a bigger advantage when accompanied by text.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Paper reports higher absolute accuracies on Spatial-Real than on synthetic tasks across all modalities, but the modality gap widens (VTQA improves much more over VQA on real images). Exact per-model tables are in appendix (Table 6).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_modes</strong></td>
                            <td>VLMs have difficulty extracting spatial relations from complex real images alone; they benefit disproportionately when dense textual captions are provided.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models_or_humans</strong></td>
                            <td>Although proprietary models perform better, same modality trends hold; humans would be expected to perform well given images.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Is A Picture Worth A Thousand Words? Delving Into Spatial Reasoning for Vision Language Models', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3103.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e3103.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models solving puzzle games that require spatial knowledge (such as Sudoku), including details about the models, the puzzles, the methods used, performance, and any analysis of how the models solve these tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaMA-3-8B (example)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaMA-3-8B (LLM evaluated on SpatialEval)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An 8B-parameter LLaMA-3 family language model evaluated in text-only (TQA) on SpatialEval tasks; used as an LLM baseline for comparison with VLM variants.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA-3-8B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Part of the LLaMA family (meta's LLaMA-3), a transformer-based large language model used as an LLM backbone in experiments; evaluated in text-only mode for SpatialEval.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>8B</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Spatial-Grid (representative)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_description</strong></td>
                            <td>Grid reasoning requiring counting and coordinate lookup; presented as text-only description of grid contents for LLaMA-3-8B.</td>
                        </tr>
                        <tr>
                            <td><strong>input_representation</strong></td>
                            <td>Text-only (TQA): textual grid descriptions enumerating objects per cell.</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Step-by-step explanation prompt appended to each question.</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_reasoning_analysis</strong></td>
                            <td>LLaMA-3-8B (TQA) achieved substantially higher accuracy on Spatial-Grid than its VLM counterpart (LLaVA variant), indicating LLaMA's language reasoning with explicit textual representation maps well to spatial tasks when grounded by text.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported accuracy: LLaMA-3 achieved 71.9% on Spatial-Grid (text-only) as reported in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_modes</strong></td>
                            <td>Relies on a comprehensive textual representation; without text (vision-only) other models must extract layout from pixels and often fail. LLaMA-3 cannot be evaluated on vision-only without a visual front-end in this study.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models_or_humans</strong></td>
                            <td>Outperforms corresponding VLM variants when those only receive vision input; still below human-level on purely visual tasks when humans can see the image.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Is A Picture Worth A Thousand Words? Delving Into Spatial Reasoning for Vision Language Models', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3103.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e3103.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models solving puzzle games that require spatial knowledge (such as Sudoku), including details about the models, the puzzles, the methods used, performance, and any analysis of how the models solve these tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaVA-1.6-34B (example)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaVA-1.6-34B (multimodal language model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multimodal LLaVA model with a 34B-parameter language backbone evaluated on SpatialEval; shows counter-intuitive improvement when visual input is removed for some tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaVA-1.6-34B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A vision-language model (LLaVA family) built by combining a visual encoder with a large language model (34B parameter backbone); used in VQA and VTQA formats in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>34B (language backbone)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Spatial-Grid (example) / Spatial-Map / Maze-Nav (evaluated across tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_description</strong></td>
                            <td>See respective task descriptions; evaluated both with original image and with the image removed (No Image ablation).</td>
                        </tr>
                        <tr>
                            <td><strong>input_representation</strong></td>
                            <td>Vision-only (VQA), Vision+Text (VTQA), and a 'No Image' ablation (text-only fed to the VLM).</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Step-by-step explanation prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_reasoning_analysis</strong></td>
                            <td>Ablation showed that removing the original image (No Image) improved performance on Spatial-Grid by 20.1% relative to when the original image was presented, demonstrating that the visual pipeline sometimes injects noise and that the model relies heavily on textual cues when available.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Improved by 20.1% on Spatial-Grid when the original image was removed (No Image) compared to using the original image (as reported in main text). Exact absolute accuracies are provided in appendices.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_modes</strong></td>
                            <td>Struggles to extract accurate spatial relations from raw images; visual-to-language translation in the model pipeline can degrade performance compared to explicit textual representations.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models_or_humans</strong></td>
                            <td>Although LLaVA-1.6-34B is large and multimodal, it underperforms the LLaMA-3-8B LLM on some text-based tasks when relying on vision-only inputs; removal of image often makes it behave more like a text-tuned model and improves results.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Is A Picture Worth A Thousand Words? Delving Into Spatial Reasoning for Vision Language Models', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3103.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e3103.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models solving puzzle games that require spatial knowledge (such as Sudoku), including details about the models, the puzzles, the methods used, performance, and any analysis of how the models solve these tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4 / GPT-4V / GPT-4o (proprietary)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OpenAI GPT-4 family including GPT-4, GPT-4V (vision), and GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Proprietary state-of-the-art multimodal and text models evaluated on SpatialEval, showing top performance among tested models but still following the modality trends observed for open-source models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4, GPT-4V, GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI's family of large models; GPT-4 is a powerful LLM, GPT-4V supports image inputs (multimodal), GPT-4o is a newer variant evaluated in vision-text settings. Exact training data/statistics are not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Spatial-Grid (representative) / Spatial-Map / Maze-Nav / Spatial-Real</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_description</strong></td>
                            <td>Same SpatialEval tasks; GPT variants evaluated in text-only, vision-only and vision+text formats depending on capability.</td>
                        </tr>
                        <tr>
                            <td><strong>input_representation</strong></td>
                            <td>TQA, VQA, VTQA depending on model (GPT-4V/GPT-4o evaluated with vision+text where available).</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Step-by-step explanation prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_reasoning_analysis</strong></td>
                            <td>Proprietary models outperform open-source models across modalities but still show modality-dependent behavior: vision-only performance is often weaker than vision+text or text-only; adding text improves performance substantially. GPT-4 family shows capability to utilize VTQA better than most open-source VLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Examples: GPT-4o achieved accuracy of 0.989 on Spatial-Grid in VTQA (reported in Table 7). GPT-4V's performance on Spatial-Grid improved by 25.6% when switching from VQA (vision-only) to VTQA (vision+text). Proprietary models significantly outperform random guessing and many open-source models across tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_modes</strong></td>
                            <td>Despite strong absolute performance, these models still do not appear to fully leverage pure visual inputs as well as humans do; the same qualitative modality trends (VQA < VTQA/TQA) remain.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models_or_humans</strong></td>
                            <td>Proprietary GPT models outperform tested open-source LLMs/VLMs on SpatialEval, but humans still have advantage on pure visual tasks; trends observed with open-source models persist (textual cues help most).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Is A Picture Worth A Thousand Words? Delving Into Spatial Reasoning for Vision Language Models', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3103.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e3103.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models solving puzzle games that require spatial knowledge (such as Sudoku), including details about the models, the puzzles, the methods used, performance, and any analysis of how the models solve these tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CogVLM (exception case)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CogVLM (vision-language model from THUDM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A VLM evaluated in the paper that is an exception to the general observation that VLMs outperform their LLM backbones on text-only inputs — CogVLM did not outperform the LLM backbone in TQA.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>CogVLM</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A visual-expert fused VLM (THUDM's CogVLM) that can accept text-only inputs and was evaluated in the TQA (No Img) setting in the paper's ablation study.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Spatial-Map / Spatial-Grid / Maze-Nav (evaluated as part of suite)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_description</strong></td>
                            <td>See respective task descriptions; CogVLM assessed in text-only mode as part of comparison between LLMs and VLMs on TQA inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>input_representation</strong></td>
                            <td>Text-only (TQA) used to compare LLM backbones to VLMs operating with no image.</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Step-by-step explanation prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_reasoning_analysis</strong></td>
                            <td>Most VLMs (when run with No Image) outperformed their corresponding LLM backbones on TQA, suggesting multimodal fine-tuning improved language backbone spatial reasoning. CogVLM was an exception and did not outperform its LLM counterpart in text-only setting, indicating variation across VLM training and integrations.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Relative comparisons in Figure 12 indicate CogVLM did not show the same text-only benefit as other VLMs; exact numbers are in detailed tables in the appendix.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_modes</strong></td>
                            <td>Shows that not all multimodal fine-tuning yields improved textual spatial reasoning — implementation and training details of the VLM matter.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models_or_humans</strong></td>
                            <td>Contrasts with the general trend where VLMs (text-only) outperform corresponding LLMs; highlights heterogeneity among VLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Is A Picture Worth A Thousand Words? Delving Into Spatial Reasoning for Vision Language Models', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts <em>(Rating: 2)</em></li>
                <li>Geomverse: A systematic evaluation of large models for geometric reasoning <em>(Rating: 2)</em></li>
                <li>Evaluating spatial understanding of large language models <em>(Rating: 2)</em></li>
                <li>Mathverse: Does your multi-modal llm truly see the diagrams in visual math problems? <em>(Rating: 1)</em></li>
                <li>Isobench: Benchmarking multimodal foundation models on isomorphic representations <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3103",
    "paper_id": "paper-270688598",
    "extraction_schema_id": "extraction-schema-77",
    "extracted_data": [
        {
            "name_short": "Spatial-Grid (aggregate)",
            "name_full": "Spatial-Grid task from SpatialEval benchmark",
            "brief_description": "A grid-based synthetic spatial reasoning task in SpatialEval where each cell contains an object and questions probe counting and coordinate-based lookup; evaluated in text-only (TQA), vision-only (VQA) and vision+text (VTQA) modalities across many LLMs and VLMs.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Various LLMs and VLMs (LLaMA, Mistral, LLaVA family, Bunny, InstructBLIP, CogVLM, proprietary GPT/Gemini/Claude)",
            "model_description": "The paper evaluates a broad set of open-source LLMs (LLaMA family, Mistral-7B, Vicuna, Phi-2, Nous-Hermes etc.) and multimodal / vision-language models (LLaVA family, InstructBLIP, Bunny variants, CogVLM, CogAgent), plus proprietary models (GPT-4, GPT-4V, GPT-4o, Gemini Pro, Claude 3 Opus). VLMs convert image inputs via a visual encoder into tokens for the language backbone and are compared to pure LLMs on text-only inputs.",
            "model_size": null,
            "puzzle_name": "Spatial-Grid",
            "puzzle_description": "A rigid grid of cells where each cell contains an object (icons/images). Questions ask e.g., how many of a given object exist, or which object is at a specified row/column — requiring counting, spatial indexing and coordinate reasoning.",
            "input_representation": "Three modalities studied: (1) TQA (text-only): full textual representation of the grid (rows/columns listed); (2) VQA (vision-only): image of the grid only; (3) VTQA (vision+text): image plus long textual caption describing the grid (redundant information).",
            "prompting_method": "All models were prompted with the same appended instruction: 'First, provide a concise answer in one sentence. Then, elaborate on the reasoning behind your answer in a detailed, step-by-step explanation.' Deterministic decoding for some models; top-p/temperature averaged over runs for nondeterministic models.",
            "spatial_reasoning_analysis": "Aggregate analyses show VLMs often fail to leverage raw images for Spatial-Grid; textual descriptions (TQA/VTQA) substantially help. Ablations include replacing the image with no image, with a random image, and with noise: removing the original image often improved VLM performance (indicating models rely on text more than vision). VTQA (vision+text) outperforms VQA (vision-only), showing redundancy helps; however, VLMs sometimes rely less on vision when text is present.",
            "performance_metrics": "Reported accuracies averaged across questions. Examples given in paper: LLaMA-3 (LLM) achieved 71.9% on Spatial-Grid (TQA), Mistral-7B-Instruct ≈ 62.1% (TQA). LLaVA-1.6-Mistral-7B (VLM) yielded 47.1% (VQA) on Spatial-Grid — a 15% drop vs the LLM backbone. Proprietary GPT-4o achieved 0.989 accuracy on Spatial-Grid in VTQA (reported in Table 7). Vision-only to vision+text improvements: GPT-4V improved by 25.6% on Spatial-Grid when switching from VQA to VTQA.",
            "limitations_or_failure_modes": "Many VLMs perform at-or-below random-guessing baseline (25% for 4-choice questions) in vision-only settings. VLMs appear to perform worse when provided only the raw image compared to when given the corresponding textual description; adding visual input can even hurt performance unless accompanied by a descriptive caption. Random/mismatched images do not always degrade VLM performance, implying weak reliance on image content.",
            "comparison_to_other_models_or_humans": "LLMs on text-only often outperform VLMs on vision-only for the same language backbone; LLMs (text-only) approach far better accuracy than VLMs given only images. Proprietary models outperform open-source broadly (GPT-4/GPT-4V/GPT-4o &gt; open-source), but the same modality trends hold (vision-only &lt; text-only/vision+text). Human performance on these synthetic grid tasks is near-perfect (paper notes humans do very well), so even best models still lag human-level performance on pure visual variants.",
            "uuid": "e3103.0",
            "source_info": {
                "paper_title": "Is A Picture Worth A Thousand Words? Delving Into Spatial Reasoning for Vision Language Models",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Maze-Nav (aggregate)",
            "name_full": "Maze-Nav task from SpatialEval benchmark",
            "brief_description": "A maze navigation task in SpatialEval where the maze is presented as an image or ASCII/text and questions ask e.g., number of turns from S to E or relative positions, testing sequential path and spatial-turn reasoning.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Various LLMs and VLMs (LLaMA family, LLaVA family, InstructBLIP, Bunny variants, proprietary models)",
            "model_description": "Same set of LLMs and VLMs as evaluated across SpatialEval. Maze-Nav inputs include colored visual mazes (start S, end E, walls) and equivalent ASCII textual representations.",
            "model_size": null,
            "puzzle_name": "Maze-Nav",
            "puzzle_description": "Navigation problem where a designated route (marked X or blue path) from S to E is specified; questions require counting right/left turns and determining spatial relationships between S and E, requiring understanding of path geometry and sequence of moves.",
            "input_representation": "Evaluated as Vision-only (image of maze), Text-only (ASCII textual representation describing path), and Vision+Text (image plus textual description).",
            "prompting_method": "Same step-by-step explanation prompt as for other tasks; paper reports experiments with no-image, random-image and noise-image ablations to probe reliance on vision vs text.",
            "spatial_reasoning_analysis": "VLMs often improved when the visual image was removed (No Image ablation) and instead text-only (ASCII) was used, indicating VLM visual pipelines were not reliably extracting the route or turn-count information. Replacing the original image with random but task-related images sometimes improved VLM performance, again suggesting weak or noisy use of vision features.",
            "performance_metrics": "Averaged accuracies across questions; paper states that for Maze-Nav many multimodal models perform at or near random in vision-only; exact per-model numbers are provided in appendices (not fully enumerated in main text).",
            "limitations_or_failure_modes": "Failure modes include inability to trace the path from raw visual maze images, miscounting turns, and being distracted by irrelevant visual content. VLMs sometimes perform worse with original image than with no image, implying visual encoder introduces noise.",
            "comparison_to_other_models_or_humans": "LLMs on text-only (ASCII) frequently outperform VLMs on vision-only. Proprietary models still follow same trends but with higher absolute accuracy; humans solve Maze-Nav easily while many models struggle.",
            "uuid": "e3103.1",
            "source_info": {
                "paper_title": "Is A Picture Worth A Thousand Words? Delving Into Spatial Reasoning for Vision Language Models",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Spatial-Map (aggregate)",
            "name_full": "Spatial-Map task from SpatialEval benchmark",
            "brief_description": "A synthetic map reasoning task where named locations are placed with pairwise relations (e.g., 'A is southeast of B') and questions ask relative position and counting by spatial criteria.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Various LLMs and VLMs (LLaMA, Mistral, LLaVA, Bunny variants, proprietary models)",
            "model_description": "Evaluations employ LLMs and VLMs with textual pairwise relation representations or image-like map visualizations. The textual representation explicitly lists pairwise relations; visuals show map layout.",
            "model_size": null,
            "puzzle_name": "Spatial-Map",
            "puzzle_description": "A map-like environment with K objects/locations; requires relational spatial reasoning across named landmarks (e.g., determining which locations are north/south/east/west of others or counting locations meeting spatial criteria).",
            "input_representation": "TQA: text listing pairwise relations; VQA: image of map; VTQA: both map image and textual relation list.",
            "prompting_method": "Step-by-step explanation prompt; ablations include no-image and random-image replacements to test reliance on vision.",
            "spatial_reasoning_analysis": "VLMs rarely improved over their LLM backbone when relying on vision-only; text descriptions often suffice and sometimes lead to better results. VTQA (vision+text) improves over VQA for many models (redundancy helps), but VLMs sometimes under-use vision when text is present.",
            "performance_metrics": "Reported average accuracies: many VLMs in vision-only perform near or below random guessing; specific model averages are shown in paper figures and appendices (aggregate trends emphasized).",
            "limitations_or_failure_modes": "VLMs may misinterpret spatial relationships from images; translation of visual layout to language tokens in existing VLM pipelines can be noisy, causing worse-than-text-only performance.",
            "comparison_to_other_models_or_humans": "LLMs with TQA often outperform VLMs with VQA; VTQA generally outperforms VQA but not always TQA. Humans again perform near-perfectly on Spatial-Map tasks.",
            "uuid": "e3103.2",
            "source_info": {
                "paper_title": "Is A Picture Worth A Thousand Words? Delving Into Spatial Reasoning for Vision Language Models",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Spatial-Real (aggregate)",
            "name_full": "Spatial-Real task (real images with long captions) from SpatialEval",
            "brief_description": "Spatial reasoning questions curated from the Densely Captioned Images (DCI) dataset where each image has very long captions (~1000 words) and multiple-choice spatial reasoning questions are asked.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Various LLMs and VLMs (evaluated in the paper)",
            "model_description": "Uses real-image captions (DCI) paired with curated multiple-choice spatial questions; evaluates the same model families and proprietary models under TQA, VQA and VTQA formats.",
            "model_size": null,
            "puzzle_name": "Spatial-Real",
            "puzzle_description": "Natural images with dense captions; questions probe counting, relation and position understanding about real-world scenes, requiring interpretation of both long textual captions and complex images.",
            "input_representation": "TQA: dense caption text-only; VQA: image-only; VTQA: both image and dense caption.",
            "prompting_method": "Step-by-step explanation prompt; same ablations and comparisons as synthetic tasks.",
            "spatial_reasoning_analysis": "Trends from synthetic tasks persist: VTQA &gt; VQA, and many VLMs rely more on text than vision. The modality gap (VTQA - VQA) is larger on Spatial-Real (≈30.0%) than on synthetic tasks (≈7.0%), indicating real images create a bigger advantage when accompanied by text.",
            "performance_metrics": "Paper reports higher absolute accuracies on Spatial-Real than on synthetic tasks across all modalities, but the modality gap widens (VTQA improves much more over VQA on real images). Exact per-model tables are in appendix (Table 6).",
            "limitations_or_failure_modes": "VLMs have difficulty extracting spatial relations from complex real images alone; they benefit disproportionately when dense textual captions are provided.",
            "comparison_to_other_models_or_humans": "Although proprietary models perform better, same modality trends hold; humans would be expected to perform well given images.",
            "uuid": "e3103.3",
            "source_info": {
                "paper_title": "Is A Picture Worth A Thousand Words? Delving Into Spatial Reasoning for Vision Language Models",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "LLaMA-3-8B (example)",
            "name_full": "LLaMA-3-8B (LLM evaluated on SpatialEval)",
            "brief_description": "An 8B-parameter LLaMA-3 family language model evaluated in text-only (TQA) on SpatialEval tasks; used as an LLM baseline for comparison with VLM variants.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "LLaMA-3-8B",
            "model_description": "Part of the LLaMA family (meta's LLaMA-3), a transformer-based large language model used as an LLM backbone in experiments; evaluated in text-only mode for SpatialEval.",
            "model_size": "8B",
            "puzzle_name": "Spatial-Grid (representative)",
            "puzzle_description": "Grid reasoning requiring counting and coordinate lookup; presented as text-only description of grid contents for LLaMA-3-8B.",
            "input_representation": "Text-only (TQA): textual grid descriptions enumerating objects per cell.",
            "prompting_method": "Step-by-step explanation prompt appended to each question.",
            "spatial_reasoning_analysis": "LLaMA-3-8B (TQA) achieved substantially higher accuracy on Spatial-Grid than its VLM counterpart (LLaVA variant), indicating LLaMA's language reasoning with explicit textual representation maps well to spatial tasks when grounded by text.",
            "performance_metrics": "Reported accuracy: LLaMA-3 achieved 71.9% on Spatial-Grid (text-only) as reported in the paper.",
            "limitations_or_failure_modes": "Relies on a comprehensive textual representation; without text (vision-only) other models must extract layout from pixels and often fail. LLaMA-3 cannot be evaluated on vision-only without a visual front-end in this study.",
            "comparison_to_other_models_or_humans": "Outperforms corresponding VLM variants when those only receive vision input; still below human-level on purely visual tasks when humans can see the image.",
            "uuid": "e3103.4",
            "source_info": {
                "paper_title": "Is A Picture Worth A Thousand Words? Delving Into Spatial Reasoning for Vision Language Models",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "LLaVA-1.6-34B (example)",
            "name_full": "LLaVA-1.6-34B (multimodal language model)",
            "brief_description": "A multimodal LLaVA model with a 34B-parameter language backbone evaluated on SpatialEval; shows counter-intuitive improvement when visual input is removed for some tasks.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "LLaVA-1.6-34B",
            "model_description": "A vision-language model (LLaVA family) built by combining a visual encoder with a large language model (34B parameter backbone); used in VQA and VTQA formats in experiments.",
            "model_size": "34B (language backbone)",
            "puzzle_name": "Spatial-Grid (example) / Spatial-Map / Maze-Nav (evaluated across tasks)",
            "puzzle_description": "See respective task descriptions; evaluated both with original image and with the image removed (No Image ablation).",
            "input_representation": "Vision-only (VQA), Vision+Text (VTQA), and a 'No Image' ablation (text-only fed to the VLM).",
            "prompting_method": "Step-by-step explanation prompt.",
            "spatial_reasoning_analysis": "Ablation showed that removing the original image (No Image) improved performance on Spatial-Grid by 20.1% relative to when the original image was presented, demonstrating that the visual pipeline sometimes injects noise and that the model relies heavily on textual cues when available.",
            "performance_metrics": "Improved by 20.1% on Spatial-Grid when the original image was removed (No Image) compared to using the original image (as reported in main text). Exact absolute accuracies are provided in appendices.",
            "limitations_or_failure_modes": "Struggles to extract accurate spatial relations from raw images; visual-to-language translation in the model pipeline can degrade performance compared to explicit textual representations.",
            "comparison_to_other_models_or_humans": "Although LLaVA-1.6-34B is large and multimodal, it underperforms the LLaMA-3-8B LLM on some text-based tasks when relying on vision-only inputs; removal of image often makes it behave more like a text-tuned model and improves results.",
            "uuid": "e3103.5",
            "source_info": {
                "paper_title": "Is A Picture Worth A Thousand Words? Delving Into Spatial Reasoning for Vision Language Models",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "GPT-4 / GPT-4V / GPT-4o (proprietary)",
            "name_full": "OpenAI GPT-4 family including GPT-4, GPT-4V (vision), and GPT-4o",
            "brief_description": "Proprietary state-of-the-art multimodal and text models evaluated on SpatialEval, showing top performance among tested models but still following the modality trends observed for open-source models.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4, GPT-4V, GPT-4o",
            "model_description": "OpenAI's family of large models; GPT-4 is a powerful LLM, GPT-4V supports image inputs (multimodal), GPT-4o is a newer variant evaluated in vision-text settings. Exact training data/statistics are not specified in this paper.",
            "model_size": null,
            "puzzle_name": "Spatial-Grid (representative) / Spatial-Map / Maze-Nav / Spatial-Real",
            "puzzle_description": "Same SpatialEval tasks; GPT variants evaluated in text-only, vision-only and vision+text formats depending on capability.",
            "input_representation": "TQA, VQA, VTQA depending on model (GPT-4V/GPT-4o evaluated with vision+text where available).",
            "prompting_method": "Step-by-step explanation prompt.",
            "spatial_reasoning_analysis": "Proprietary models outperform open-source models across modalities but still show modality-dependent behavior: vision-only performance is often weaker than vision+text or text-only; adding text improves performance substantially. GPT-4 family shows capability to utilize VTQA better than most open-source VLMs.",
            "performance_metrics": "Examples: GPT-4o achieved accuracy of 0.989 on Spatial-Grid in VTQA (reported in Table 7). GPT-4V's performance on Spatial-Grid improved by 25.6% when switching from VQA (vision-only) to VTQA (vision+text). Proprietary models significantly outperform random guessing and many open-source models across tasks.",
            "limitations_or_failure_modes": "Despite strong absolute performance, these models still do not appear to fully leverage pure visual inputs as well as humans do; the same qualitative modality trends (VQA &lt; VTQA/TQA) remain.",
            "comparison_to_other_models_or_humans": "Proprietary GPT models outperform tested open-source LLMs/VLMs on SpatialEval, but humans still have advantage on pure visual tasks; trends observed with open-source models persist (textual cues help most).",
            "uuid": "e3103.6",
            "source_info": {
                "paper_title": "Is A Picture Worth A Thousand Words? Delving Into Spatial Reasoning for Vision Language Models",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "CogVLM (exception case)",
            "name_full": "CogVLM (vision-language model from THUDM)",
            "brief_description": "A VLM evaluated in the paper that is an exception to the general observation that VLMs outperform their LLM backbones on text-only inputs — CogVLM did not outperform the LLM backbone in TQA.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "CogVLM",
            "model_description": "A visual-expert fused VLM (THUDM's CogVLM) that can accept text-only inputs and was evaluated in the TQA (No Img) setting in the paper's ablation study.",
            "model_size": null,
            "puzzle_name": "Spatial-Map / Spatial-Grid / Maze-Nav (evaluated as part of suite)",
            "puzzle_description": "See respective task descriptions; CogVLM assessed in text-only mode as part of comparison between LLMs and VLMs on TQA inputs.",
            "input_representation": "Text-only (TQA) used to compare LLM backbones to VLMs operating with no image.",
            "prompting_method": "Step-by-step explanation prompt.",
            "spatial_reasoning_analysis": "Most VLMs (when run with No Image) outperformed their corresponding LLM backbones on TQA, suggesting multimodal fine-tuning improved language backbone spatial reasoning. CogVLM was an exception and did not outperform its LLM counterpart in text-only setting, indicating variation across VLM training and integrations.",
            "performance_metrics": "Relative comparisons in Figure 12 indicate CogVLM did not show the same text-only benefit as other VLMs; exact numbers are in detailed tables in the appendix.",
            "limitations_or_failure_modes": "Shows that not all multimodal fine-tuning yields improved textual spatial reasoning — implementation and training details of the VLM matter.",
            "comparison_to_other_models_or_humans": "Contrasts with the general trend where VLMs (text-only) outperform corresponding LLMs; highlights heterogeneity among VLMs.",
            "uuid": "e3103.7",
            "source_info": {
                "paper_title": "Is A Picture Worth A Thousand Words? Delving Into Spatial Reasoning for Vision Language Models",
                "publication_date_yy_mm": "2024-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts",
            "rating": 2,
            "sanitized_title": "mathvista_evaluating_mathematical_reasoning_of_foundation_models_in_visual_contexts"
        },
        {
            "paper_title": "Geomverse: A systematic evaluation of large models for geometric reasoning",
            "rating": 2,
            "sanitized_title": "geomverse_a_systematic_evaluation_of_large_models_for_geometric_reasoning"
        },
        {
            "paper_title": "Evaluating spatial understanding of large language models",
            "rating": 2,
            "sanitized_title": "evaluating_spatial_understanding_of_large_language_models"
        },
        {
            "paper_title": "Mathverse: Does your multi-modal llm truly see the diagrams in visual math problems?",
            "rating": 1,
            "sanitized_title": "mathverse_does_your_multimodal_llm_truly_see_the_diagrams_in_visual_math_problems"
        },
        {
            "paper_title": "Isobench: Benchmarking multimodal foundation models on isomorphic representations",
            "rating": 1,
            "sanitized_title": "isobench_benchmarking_multimodal_foundation_models_on_isomorphic_representations"
        }
    ],
    "cost": 0.01716175,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Is A Picture Worth A Thousand Words? Delving Into Spatial Reasoning for Vision Language Models
4 Nov 2024</p>
<p>Jiayu Wang 
University of Wisconsin</p>
<p>Yifei Ming yifei.ming@salesforce.com 
Salesforce AI Research</p>
<p>Zhenmei Shi zhmeishi@cs.wisc.edu 
University of Wisconsin</p>
<p>Vibhav Vineet vibhav.vineet@microsoft.com 
Microsoft Research</p>
<p>Xin Wang milawang@cs.wisc.edu 
Microsoft Research</p>
<p>Yixuan Li 
University of Wisconsin</p>
<p>Neel Joshi 
Microsoft Research</p>
<ul>
<li>Madison 
Is A Picture Worth A Thousand Words? Delving Into Spatial Reasoning for Vision Language Models
4 Nov 20244CCDE80EEBDC99A51E1C398F22E6C0C7arXiv:2406.14852v2[cs.CV]
Large language models (LLMs) and vision-language models (VLMs) have demonstrated remarkable performance across a wide range of tasks and domains.Despite this promise, spatial understanding and reasoning-a fundamental component of human cognition-remains under-explored.We propose SpatialEval, a novel benchmark that covers diverse aspects of spatial reasoning such as relationship understanding, navigation, and counting.We conduct a comprehensive evaluation of competitive language and vision-language models.Our findings reveal several counter-intuitive insights that have been overlooked in the literature: (1) Spatial reasoning poses significant challenges where competitive models can fall behind random guessing; (2) Despite additional visual input, VLMs often under-perform compared to their LLM counterparts; (3) When both textual and visual information is available, multi-modal language models become less reliant on visual information if sufficient textual clues are provided.Additionally, we demonstrate that leveraging redundancy between vision and text can significantly enhance model performance.We hope our study will inform the development of multimodal models to improve spatial intelligence and further close the gap with human intelligence.Our code is available at https://github.com/jiayuww/SpatialEval.</li>
</ul>
<p>Introduction</p>
<p>The recent breakthroughs in foundation models have had a transformative effect on research and industry, and we have seen these models rapidly integrated into products and new businesses that are shaping people's lives for the better.This sea change was initially driven by large language models (LLMs), which have shown at times near unbelievable, human-level performance across a wide range of tasks.Over the past year, many of these models have been extended to handle images in addition to text, leading to a significant increase in vision-language models (VLMs), especially multimodal large language models (MLLMs), which demonstrate groundbreaking performance in image-related tasks as in the text domain.</p>
<p>However, the reality is not quite as rosy as advertised.While these models have been instrumental in advancing the state-of-the-art in complex reasoning tasks such as common sense reasoning, mathematical problem solving, and scientific question answering [17,30,41,72,75], they have not been as effective for a number of problem domains.In particular, as we will show in this paper, they have limited performance on tasks that require detailed visual understanding and reasoning of images.</p>
<p>Visual understanding and reasoning-an intrinsic part of human perception and cognitive ability-have been largely under-explored when it comes to LLMs and VLMs.In fact, it is often argued that the visual sense is the dominant sense in people, yet when it comes to current models, it seems quite secondary.Spatial reasoning, in particular, is fundamental to everyday human activities such as navigating environments, understanding maps, and manipulating objects.It encompasses skills that are crucial for both survival and higher-order cognition, including the ability to navigate through space, recognize patterns, and deduce relationships from spatial configurations.</p>
<p>In this paper, we propose SpatialEval, a novel benchmark containing four tasks (Spatial-Map, Maze-Nav, Spatial-Grid, and Spatial-Real, Section 2.1) to explore the performance of LLMs and VLMs on diverse aspects of spatial reasoning, including relationship, navigation, position understanding, and object counting.Humans excel at such tasks, making them essential capabilities for intelligent systems to emulate for safe and effective deployment in the real world.</p>
<p>Our dataset, however, is constructed with a key twist -each problem in our benchmark has an image and a text representation that is sufficient for answering each spatial understanding question.We denote the use of these sources as VQA, which is the standard task of visual-question answering that consists of a vision-only input and a question, TQA, text-only input and a question, and VTQA, a combination of the previous with vision and text input.</p>
<p>We conduct a systematic and comprehensive evaluation of a wide range of open-source and proprietary LLMs and VLMs.We perform in-depth analysis and unveil several novel and surprising results that challenge the current understanding of how these models process spatial information:</p>
<p>• Spatial reasoning remains challenging: VLMs frequently struggle with spatial reasoning tasks, with some competitive models performing worse than random guessing.</p>
<p>• Visual vs. textual inputs: Without detailed textual descriptions, multimodal models rarely surpass the performance of their LLM backbones when relying solely on visual inputs.This underscores the critical role of text in enhancing model performance on spatial reasoning.</p>
<p>• Reduced reliance on visual information: When both textual and visual inputs are provided, multimodal language models tend to rely less on the visual component if sufficient textual clues are available.</p>
<p>• Textual performance of VLMs: VLMs often outperform their LLM counterparts with text-only inputs, indicating that the language model backbones within VLMs benefit from multimodal training, despite a lack of similar benefits from the visual components.</p>
<p>We surmise the limitations in VLMs' spatial understanding stem from the overly simplistic handling of visual information in current architectures and training pipelines.We believe the contributions of this paper will drive changes in model design, accelerating improvements that could unlock more robust spatial reasoning capabilities, and help bridge the gap toward human-like intelligence.</p>
<p>2 Dataset and Task Construction</p>
<p>Dataset Setup</p>
<p>To evaluate the spatial reasoning abilities of LLMs and VLMs, we construct four diverse tasks including spatial relationships, navigation, position understanding, and counting.To systematically study the impact of modality, we design three types of input formats for each task: (1) TQA (Textonly): the input is purely textual and contains all necessary information for a person to answer the questions.( 2) VQA (Vision-only): the input consists solely of an image, which provides sufficient details for a person to easily answer, a format also referred to as Visual Question Answering (VQA) in the literature.( 3) VTQA (Vision-text): the input includes both an image and its textual representation with detailed descriptions, rendering the information in both modalities redundant.We evaluate LLMs using Text-only inputs and VLMs using Text-only, Vision-only, and Vision-text inputs on the same set of questions (    Vision-text.We evaluate language models (w.TQA input) and vision-language models (w.VQA and VTQA inputs) on the same set of questions.</p>
<p>Spatial-Map.Understanding the spatial relationships among objects on a map is a fundamental aspect of human cognitive abilities.To simulate this environment, we create a map-like dataset termed Spatial-Map with K objects, where K is configurable.Each object is associated with a unique location name, such as Unicorn Umbrellas and Gale Gifts.To study the impact of modality, the textual representation of each input consists of pairwise relations such as Brews Brothers Pub is to the Southeast of Whale's Watches.An example with K = 6 is shown in Figure 1, with Text-only, Vision-only, and Vision-text inputs.These questions include asking about the spatial relationships between two locations and the number of objects that meet specific spatial criteria.</p>
<p>Maze-Nav.Navigation through complex spaces is essential for intelligent systems.To evaluate such abilities, we have developed a maze-like dataset named Maze-Nav.Visually, each sample can be</p>
<p>Here is a Maze represented in ASCII code (LHS) where the symbols have the following meanings: # represents walls that are impassable barriers." " (space) represents the navigable path within the maze, ....A right turn is defined as a change in movement direction that is 90 degrees clockwise relative to the previous direction.A left turn is defined as a change in movement direction that is 90 degrees anticlockwise relative to the previous direction.represented as colored blocks where different colors signify distinct elements: a green block marks the starting point (S), a red block indicates the exit (E), black blocks represent impassable walls, white blocks denote navigable paths, and blue blocks trace the path from S to E. The objective is to navigate from S to E following the blue path, with movement permitted in the four cardinal directions (up, down, left, right).Alternatively, each input can be depicted in a textual format using ASCII code.An example is illustrated in Figure 2, featuring Text-only, Vision-only, and Vision-text inputs.We construct this task based on an open-sourced library [26].The questions asked include counting the number of turns from S to E and determining the spatial relationship between S and E. While such questions are easy for humans, we will show in Section 3 that they still pose significant challenges for modern multimodal language models.Spatial-Grid.To investigate spatial understanding within structured environments, we introduce a grid-like dataset named Spatial-Grid, contrasting with the Spatial-Map where objects are positioned arbitrarily.Visually, each input consists of a grid of cells, each containing an image (e.g., a rabbit).An example is illustrated in Figure 3.Alternatively, this grid can also be represented in a purely textual format; for instance, the first row might be described as: elephant | cat | giraffe | elephant | cat.The evaluations focus on tasks such as counting specific objects (e.g., rabbits) and identifying the object located at a specific coordinate in the grid (e.g., first row, second column).</p>
<p>Spatial-Real.To extend the evaluation of spatial reasoning beyond synthetic environments, we introduce Spatial-Real, a task built on the Densely Captioned Images (DCI) dataset [67], where each image has a detailed caption with more than 1,000 words on average.As DCI does not contain questions, we curate multiple-choice questions regarding spatial reasoning (object counting, relation, and position understanding) and annotate the answers.An example is shown in Figure 4. We provide detailed analysis on Spatial-Real in Appendix E.</p>
<p>Models</p>
<p>We consider a wide range of competitive open-source language models with different scales, including Phi2-2.7B[35], the LLaMA family (LLaMA-2-7B, LLaMA-2-13B, and LLaMA-3-8B) [65], Mistral-7B [27], the Vicuna family (Vicuna-7B-1.5and Vicuna-13B-1.5)[11], and Nous-Hermes-2-Yi-34B.</p>
<p>For multimodal language models, we consider the Bunny family (Bunny-Phi-2-SigLIP, Bunny-Phi-1.5-SigLIP,Bunny-Phi-2-EVA, and Bunny-Phi-1.5-EVA)[22], CogVLM [69], CogAgent [23], InstructBLIP family (InstructBLIP-Vicuna-7B and InstructBLIP-Vicuna-13B) [13], and LLaVA family (LLaVA-1.6-Mistral-7B,LLaVA-1.6-Vicuna-7B,LLaVA-1.6-Vicuna-13B, and LLaVA-1.6-34B) [38].We also evaluate the proprietary models: Open AI's GPT-4V, GPT-4o, GPT-4, Google Gemini Pro 1.0, and Anthropic Claude 3 Opus.</p>
<p>Evaluation.As each question contains four options, we use accuracy as the main evaluation metric.</p>
<p>The same user prompt is appended at the end of each question: First, provide a concise answer in one sentence.Then, elaborate on the reasoning behind your answer in a detailed, step-by-step explanation.For each model, we adopt the default configurations and decoding strategies, e.g., argmax for deterministic decoding and top-p for non-deterministic decoding.For non-deterministic decoding, the results are averaged over three independent runs for open-source models.For proprietary models, due to their limited availability and increased compute time and cost, we only perform one run.We summarize the terminologies regarding input modalities for LLMs and VLMs in We describe the Text-only, Vision-only, and Vision-text input modalities based on how we feed the image information to the models.Vision-only input means the image is fed directly to the models without textual description, while all questions are presented in text.</p>
<p>Main Results and Analysis</p>
<p>Spatial reasoning remains surprisingly challenging.The evaluation results on open-source models on Spatial-Map, Maze-Nav, and Spatial-Grid are shown in Figure 5.For each task, the reported accuracy is averaged over all questions.For vision-language models, we choose the Vision-only input format, commonly used in Visual Question Answering (VQA).We use a dashed red line in each figure to indicate the expected accuracy if answering by random guessing.Our findings reveal several notable insights: (1) Vision-only inputs: Despite the simplicity of these tasks for humans, most competitive multimodal models perform at levels similar to or barely above random guessing.</p>
<p>Maze-Nav</p>
<p>Original Image No Image VLMs exhibit improved performance when visual input is absent.We conducted experiments by entirely removing the Original Image and relying solely on the textual description.The results are shown in Figure 8.For each task, we report the accuracy averaged across all questions.Remarkably, the absence of visual input leads to better performance across a range of VLM architectures.For instance, the performance of LLaVA-1.6-34B on the Spatial-Grid task improved by 20.1% when no image was presented compared to scenarios with the original image.This observation underscores that when textual information alone can address the questions, additional visual inputs do not necessarily enhance, and may even hinder the performance, a sharp contrast to human capabilities where visual cues significantly aid in understanding.The removal of visual input forces the models to utilize the textual information to solve spatial reasoning tasks.</p>
<p>Maze-Nav</p>
<p>Original Image Random Image Mismatched image-text does not necessarily hurt.To build on previous findings, we further investigate the effects of replacing the Original Image with a Random Image (illustrated in Figure 7).Unlike a noise image, a random image is task-related but may provide conflicting information compared to the textual description.Intuitively, one might expect that such random images would degrade VLM performance due to contradictory cues.However, as demonstrated in Figure 10, this expectation does not always hold true.For instance, Random Image in the Maze-Nav task leads to improved performance across various VLM architectures.This outcome implies that VLMs are not heavily reliant on visual information, particularly when adequate textual clues are provided.</p>
<p>Leveraging Redundancy in Multimodal Inputs</p>
<p>Multimodal language models offer considerable versatility in handling multimodal inputs.While the visual input alone often provides sufficient details for humans to address spatial reasoning tasks with ease, we propose that VLMs significantly benefit from the inclusion of textual descriptions alongside visual data, even if this introduces substantial redundancy.We verify this hypothesis by comparing VQA (Vision-only input) and VTQA (Vision-text input) across diverse VLM architectures.The results are shown in Figure 11, where each vertex on the spider plot represents the average accuracy of a (Vision-only, Vision-text) pair based on the same VLM.For Spatial-Map and Spatial-Grid, we can clearly see that having the additional textual input (VTQA) enhances the performance compared Text-only input with LLM vs. Text-only input with VLM.Given the demonstrated efficacy of textonly inputs, we conducted an ablation study to compare LLMs and VLMs using text-only inputs.We consider VLMs that are capable of processing text without accompanying visual data.The results are illustrated in Figure 12.Except for CogVLM, the majority of VLMs outperform their corresponding LLM backbones.This suggests that the language model backbones in VLMs demonstrate enhanced spatial reasoning abilities through multimodal learning.Conversely, the addition of visual information does not necessarily provide further benefits.</p>
<p>Proprietary vs. Open-Source Models</p>
<p>As many recent benchmarks have shown that proprietary models generally outperform open-source models, it is important to understand if our observed trends hold with proprietary models.The performance of several top proprietary models (GPT-4, GPT-4V, GPT-4o, Gemini Pro 1.0, and Claude 3 Opus) are shown in Figure 13.We have the following salient observations: (1) A significant performance gap exists between SoTA open-source models and proprietary models, as expected.Furthermore, with both Text-only and Vision-text formats, GPT-4V and GPT-4o significantly outperform random guessing across all tasks.For instance, in the Vision-text format, GPT-4o achieves</p>
<p>Comparison Results</p>
<p>Summary of Findings TQA (LLM) vs. VQA Figure 6 VQA rarely enhances the performance compared to TQA (LLM).</p>
<p>VTQA vs. TQA (VLM) Figure 8 VLMs exhibit improved performance in spatial reasoning tasks when the image input is absent.</p>
<p>VQA vs. VTQA Figure 11 Given the same image input, additional textual description enhances VLM's performance.TQA (VLM) vs. TQA (LLM) Figure 12 Multimodal fine-tuning enhances LLM's spatial reasoning ability.TQA (LLM) vs. VTQA Figure 16 No definitive winner.an accuracy of 0.989 on Spatial-Grid (Table 7).( 2) Yet, the trends we observed with open source models hold, VQA consistently under-performs compared to TQA and VTQA, for example, GPT-4V's performance improves by 25.6% on Spatial-Grid when switching from Vision-only to Vision-text input; and again no clear winner between TQA and VTQA (see Appendix D for further details), showing that proprietary models, even the new GPT-4o model, still do not appear to fully leverage visual inputs.We summarize the key findings of this work in Table 2.</p>
<p>Related Work</p>
<p>Large language models.Large language models (LLMs) have achieved outstanding performance across a diverse array of fields, including finance [34], bioinformatics [63], law [60], education [29], coding [24], and creative tasks [2,36].LLM architectures have gone through significant changes in recent years, with notable developments such as BERT [14], OPT [76], PaLM [12], Gemma family [62], Mistral family [27], GPT family [2,10], Claude family [5], and LLaMA family [3,64,66].These models have demonstrated emergent abilities and revolutionized numerous domains, supporting capabilities such as in-context learning [46,51,59], compositional reasoning [16,20,70], and taskspecific adaptation [57,58,71].However, visual understanding and reasoning-an intrinsic part of human cognitive ability-remains largely under-explored for LLMs.</p>
<p>Vision-language models and multi-modal language models.The success of LLMs has propelled the adoption of the Transformer architecture [68] within the computer vision community, such as ViT [15], Beit [9], CLIP [54], MAE [21], Swin [42,43], and DiT [53].Building on the capabilities of powerful LLMs, multi-modal language models (MLLMs) such as Flamingo [4], LLaMA-Adapter [19,74], LLava [37,39], stable-diffusion [55], BLIP [32,33], MiniGPT-4 [77], Qwen [7,8], Gemini [61], MM1 [45] have significantly expanded the range of problems that can be addressed with improved reliability [49].These models adeptly handle inputs from diverse modalities and have demonstrated remarkable performance on diverse tasks such as mathematical reasoning [44],</p>
<p>image-text retrieval [47,73], and visual reasoning [6,18,25,28,31,40,50].</p>
<p>Spatial understanding and reasoning.Spatial reasoning entails comprehending and manipulating spatial relationships, a task significantly more challenging than visual grounding [1,28,52].Although progress in natural language processing, evaluations, and benchmarks on LLMs such as GPT-4 [2] and Claude3 [5] have predominantly focused on textual or relational reasoning.This focus often overlooks the intricate nature of spatial reasoning tasks.Notably, recent studies [17,30,41,48,56,72,75] have conducted thorough evaluations across a diverse range of tasks.Yet, they demonstrate a scant exploration of spatial reasoning capabilities, highlighting a gap in assessing this complex cognitive skill in current benchmarks.In particular, Zhang et al. [75] suggest that MLLMs primarily leverage textual cues rather than visual diagrams to solve math problems while focusing only on math problems.Yamada et al. [72] design simple navigation tasks and finds LLMs appear to capture certain aspects of spatial structure implicitly, but room for improvement remains, while our benchmarks are more complicated than their navigation tasks based on simple geometry maps.Fu et al. [17] propose a benchmark on science and games with limited exploration related to spatial reasoning, while we are dedicated diverse spatial reasoning tasks with in-depth analysis.</p>
<p>Discussion and Conclusions</p>
<p>We explored the spatial understanding capabilities of VLMs and LLMs.Our experiments resulted in several surprising conclusions across SoTA open-source and proprietary models.(1) VLMs struggle with spatial reasoning tasks, (2) multimodal models rarely surpass LLMs when relying on visual inputs, (3) when both textual and visual inputs are provided, multimodal language models rely less on the visual inputs, and (4) VLMs often outperform their LLM counterparts with text-only inputs.This challenges the belief that current VLMs are highly performant at a wide range of vision-text tasks.However, with further thought, perhaps this is to be expected after all.The currently known architectures for VLMs attempt to "translate" the vision input into the language space and all reasoning is then performed in the language domain.It is logical that this automatic translation path is worse than a human provided translation to text, as in our text-only scenario.Thus our work shows the limits of the translation approach.Instead, to bridge the gap to human performance, future models require new architectures that treat vision input as a first-class source of information and reason in a joint vision-language space.It is our hope that our work informs development on this path.</p>
<p>C.2 Hyperparameters and Error Bars</p>
<p>The hyperparameters discussed in this paper pertain to the decoding strategies of each model.For deterministic decoding, we adhere to the default settings specified for each model.The models employing deterministic decoding (argmax) include Bunny-Phi-2-SigLIP, CogAgent, CogVLM, InstructBLIP-Vicuna-13B, and InstructBLIP-Vicuna-7B.For non-deterministic decoding, we utilize the default hyperparameters provided by Hugging Face (for instance, Top-P is set at 0.9 and the temperature at 0.2 for LLaVA-1.6).Error bars for the main results (Figure 5) are obtained with three independent runs.</p>
<p>C.3 Model Checkpoints</p>
<p>For most open-source models, we use the checkpoints provided by Hugging Face as shown in Table 3.</p>
<p>For Bunny variants (Bunny-Phi-1.5-EVA,Bunny-Phi-1.5-SigLIPand Bunny-Phi-2-EVA), we use merged weights following instructions in https://github.com/BAAI-DCAI/Bunny/.</p>
<p>Model Name</p>
<p>Table 3: Model checkpoints from Hugging Face.</p>
<p>C.4 Detailed Illustration of Datasets</p>
<p>In the main text, we abbreviated the textual descriptions in Figure 1, Figure 2 due to space constraints.In this section, we provide the full descriptions for each task to facilitate a better understanding of spatial reasoning tasks.The complete illustrations are displayed in Figure 14 for Spatial-Map and Figure 15 for Maze-Nav, respectively.Three questions (termed Q1 to Q3) are associated with each sample in tasks Spaitial-Map, Maze-Nav, and Spatial-Grid.</p>
<p>Here is a Maze represented in ASCII code (LHS), where the symbols have the following meanings:</p>
<p>-# represents walls that are impassable barriers.</p>
<p>-" " (space) represents the navigable path within the maze, but not necessarily the correct path to the exit.</p>
<p>-S denotes the starting point of the maze.</p>
<p>-E marks the endpoint or the exit the maze.</p>
<p>-X marks the specific route you should follow to navigate from the starting point 'S' to the 'E'.-The objective is to navigate from S to E by following the by Movement is allowed in any of the four cardinal (up, down, left, right) along spaces, but to solve the maze, follow designated by X.</p>
<p>-turn defined as a change in movement direction that is 90 degrees clockwise relative to the previous direction.</p>
<p>-A left turn is defined as a change in movement direction that is degrees anticlockwise relative to the previous direction.-" " (space) represents the navigable path within the maze, not necessarily the correct path to the exit.-X marks the specific route you should follow to navigate from the starting point 'S' to endpoint 'E'.-S denotes starting point of the -E marks the endpoint or the the maze.</p>
<p>-The objective is to navigate from S to E by following the path marked by X. Movement is allowed in any of the four cardinal (up, down, right) the spaces, but to solve the maze, follow the path designated by X.</p>
<p>Figure 1 :
1
Figure1: Illustration of the Spatial-Map task, which simulates a map with multiple locations.To investigate the impact of modality, we consider three input formats: Text-only, Vision-only, and Vision-text.We evaluate language models (w.TQA input) and vision-language models (w.VQA and VTQA inputs) on the same set of questions.</p>
<p><Img> 5 QFigure 2 :
52
Figure 2: Illustration of the Maze-Nav task, which evaluates the model's ability to navigate from the starting point (S) to the exit (E).</p>
<p>Figure 3 :
3
Figure 3: Illustration of the Spatial-Grid task, which evaluates the model's spatial reasoning ability in a rigid grid structure.</p>
<p>Figure 4 :
4
Figure 4: Illustration of the Spatial-Real task, which is built on real images with long captions, featuring detailed descriptions averaging over 1,000 words per image.</p>
<p>Figure 6 :
6
Figure 6: TQA (LLM) vs. VQA on spatial reasoning tasks.Each vertex on the spider plot represents the Avg Acc of a (VLM, LLM) pair with the same language backbone, i.e., LLM v.s.VLM further finetuned on that.VLMs are depicted in red, and LLMs in blue.We can see that VLMs rarely enhance the performance compared to their LLM counterparts.</p>
<p>4</p>
<p>Delving Into Spatial Reasoning for Vision-Language Models 4.1 Seeing Without Understanding: The Blindness of Multimodal Language Models Original Image Random Image Noise Image</p>
<p>Figure 7 :
7
Figure 7: Illustration of Random Image and Noise Image for the example in Figure 2. To better understand how VLMs process visual information, we conduct a series of controlled experiments in the VTQA (Vision-text input) setting.For each sample, we replace the original image input (that matches the textual description) with either: (1) No Image: only keep the textual input without the image input, (2) Noise Image: a Gaussian noise image irrelevant to the task, and (3) Random Image: a random image from the dataset that does not match the textual description, as shown in Figure 7.</p>
<p>Figure 8 :
8
Figure 8: VTQA vs. TQA (VLM) on spatial reasoning tasks.VLMs exhibit improved performance in spatial reasoning tasks when visual input is absent.</p>
<p>Figure 10 :
10
Figure 10: Original Image vs. Random Image in VTQA.On Maze-Nav, replacing the original image with a random image leads to performance improvement across diverse VLM architectures.</p>
<p>( c )Figure 11 :
c11
Figure 11: VQA vs. VTQA on spatial reasoning tasks.Each vertex on the spider plot represents the Avg Acc of a (Vision-only, Vision-text) pair with the same VLM model.We can see that having the additional textual input (VTQA) enhances the performance compared to only using images (VQA).</p>
<p>Figure 12 :
12
Figure 12: Comparison of Text-only input with LLM (TQA) vs. Text-only input with VLM (No Img).We consider VLMs that support text-only inputs.Each vertex on the spider plot represents the Avg Acc of a (LLM, VLM) pair with the same language model backbone.</p>
<p>Figure 13 :
13
Figure 13: Results with proprietary models.Similar trends are observed as with open-source models.</p>
<p><Img></p>
<p>The figure represents a Maze, where the colored blocks have the following meanings: -Black blocks represent walls that are impassable barriers.White blocks represent navigable paths within the maze, but not necessarily the path to the exit.-Green block denotes the starting point (S) of the maze.-Red block marks the endpoint or the exit (E) of the maze.-The objective is to navigate from S to E by following the path marked by Blue.Movement is allowed in any of the four cardinal directions (up, down, left, right) along the White blocks, but to solve the maze, follow the path designated by Blue blocks.-A right turn is defined as a change in movement direction is 90 degrees clockwise relative to the previous direction.-A turn is defined as a change in movement direction that 90 degrees anticlockwise relative to the previous direction.<Img> The represents a Maze, where the colored blocks have the meanings: -Black blocks represent walls that are impassable barriers.-White blocks represent navigable paths within the maze, but necessarily the correct path to the -Green block denotes the starting point (S) of the maze.-Red block the endpoint or exit (E) of the maze.-The objective is to navigate S to E by following the path marked by Blue.Movement is allowed in any of the four cardinal directions (up, down, left, right) along White blocks, but to solve the maze, follow the path designated by Blue blocks.The same Maze can be represented in ASCII code (LHS), where the symbols have the following meanings: -# represents walls that are impassable barriers.</p>
<p>QuestionsQ:Figure</p>
<p>Figure Illustration of the Maze-Nav task with complete textual descriptions in TQA, VQA, and VTQA.</p>
<p>Table</p>
<p>Table 1 :
1Model Input Modality TermDescriptionLLMText-onlyTQA (LLM)Text-only input that includes all necessary infor-mation to answer questions without visual context.VLMText-onlyTQA (VLM)Text-only input as in TQA (LLM) but applied to VLMs (e.g., the LLaVA family).VLMVision-onlyVQAInput only includes an image without correspond-ing textual description.VLMVision-textVTQAInput includes both an image and its textual de-scription.</p>
<p>Table 1 :
1
Terms regarding input modalities for LLMs and VLMs.</p>
<p>Table 2 :
2
Summary of main findings.</p>
<p>AcknowledgementThe authors would like to thank NeurIPS anonymous reviewers for their insightful feedback and helpful discussions.Yifei  Ming and Yixuan Li are funded in part by the AFOSR Young Investigator Program under award number FA9550-23-1-0184, National Science Foundation (NSF) Award No. IIS-2237037 &amp; IIS-2331669, and Office of Naval Research under grant number N00014-23-1-2643.Link LLaMA-2-13B https://huggingface.co/meta-llama/Llama-2-13b-chat-hf LLaMA-2-7B https://huggingface.co/meta-llama/Llama-2-7b-chat-hf LLaMA-3-8B https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct Mistral-7B https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2Nous-Hermes-2-Yi-34B https://huggingface.co/NousResearch/Nous-Hermes-2-Yi-34B Phi-2 https://huggingface.co/microsoft/phi-2 Vicuna-13B-1.5 https://huggingface.co/lmsys/vicuna-13b-v1.5 Vicuna-7B-1.5 https://huggingface.co/lmsys/vicuna-7b-v1.5 LLaVA-1.6-34Bhttps://huggingface.co/liuhaotian/llava-v1.6-34b LLaVA-1.6-Mistral-7Bhttps://huggingface.co/liuhaotian/llava-v1.6-mistral-7b LLaVA-1.6-Vicuna-13Bhttps://huggingface.co/liuhaotian/llava-v1.6-vicuna-13b LLaVA-1.6-Vicuna-7Bhttps://huggingface.co/liuhaotian/llava-v1.6-vicuna-7b Bunny-Phi-2-SigLIP https://huggingface.co/BAAI/Bunny-v1_0-3B Qwen-VL https://huggingface.co/Qwen/Qwen-VL-Chat CogAgent https://huggingface.co/THUDM/cogagent-vqa-hf CogVLM https://huggingface.co/THUDM/cogvlm-chat-hf InstructBLIP-Vicuna-13B https://huggingface.co/Salesforce/instructblip-vicuna-13b InstructBLIP-Vicuna-7B https://huggingface.co/Salesforce/instructblip-vicuna-7b* The work was completed in part during Yifei Ming's internship at Microsoft Research, as well as PhD thesis research at UW-Madison.38th Conference on Neural Information Processing Systems (NeurIPS 2024).AppendixA How Does SpatialEval Expand Beyond Traditional VQA?We highlight several key rationales for SpatialEval in comparison to existing VQA benchmarks:• Task scope and focus: Existing benchmarks such as Visual Genome[31], GQA[25], and BLINK[18]focus only on VQA, where the image is required but the text description is often omitted or optional.In contrast, SpatialEval further explores spatial reasoning across different settings: TQA (LLM), TQA (VLM), and VTQA, where images or texts can be optional, thereby broadening the scope of tasks.• Evaluation scheme: we primarily focus on generative LLMs and VLMs which can "elaborate on the reasoning behind your answer in a detailed, step-by-step explanation" (Section 2.2).The task in prior VQA benchmarks is often treated as discriminative with no explicit reasoning.Therefore, it remains unknown if previous observations can be naturally transferred to foundation models pre-trained on web-scale data.• The Textual Representation of Images: The textual descriptions in prior VQA benchmarks are often brief or directly imply the answers.While these datasets are valuable, they do not consistently offer the level of complexity we require, such as numerous objects containing dense visual information along with detailed natural language captions that fully convey the image content.In SpatialEval, we provide long and dense captions for each image.As a result, answers cannot be easily inferred.We also aim to isolate object detection capability from spatial reasoning ability by simplifying objects to symbols (e.g., Spatial-Map).• IQ test for VLMs and LLMs: Tasks in SpatialEval are designed to serve as cognitive tests that evaluate basic capabilities of multi-modal foundation models.While three tasks feature synthetic visual content, humans can solve them with near-perfect accuracy.This indicates that the tasks are within the realm of human cognitive capabilities.B Limitations and Societal ImpactLimitations.In this work, we introduce four novel tasks and conduct a comprehensive evaluation of diverse large language models (LLMs) and vision-language models (VLMs), presenting a controlled and thorough evaluation of spatial reasoning capabilities.While our analysis is detailed and extensive, it remains primarily empirical.We believe that embarking on a formal theoretical study, despite its challenges, would significantly enrich our understanding of pre-trained multimodal language models.Moreover, our focus has been on in-depth analysis rather than on developing new training strategies or adaptation algorithms to enhance spatial reasoning.Recognizing these points, we identify them as valuable directions for future research.Societal impact.Regarding the positive societal impact, our evaluations and observations could catalyze the development of new algorithms that enhance the spatial reasoning abilities of LLMs and VLMs.Improved understanding of these models has the potential to significantly benefit sectors requiring robust spatial understanding and navigation.This could lead to more reliable and efficient systems that enhance safety and user experience.As for potential negative societal impacts, our work primarily involves empirical evaluations using synthetic datasets designed to probe spatial reasoning.Therefore, we do not anticipate any direct negative societal impacts arising from our current research.C Experimental DetailsC.   Impact of prompting techniques.In line with our to sampling strategies, our primary goal in prompting techniques is to report the best model performance given the same question.The prompt technique we use, which asks for step-by-step explanation is the most effective query among others in our initial studies.As a concrete example, we compare the original prompting strategy, "First, provide a concise answer in one sentence.Then, elaborate on the reasoning behind your answer in a detailed, step-by-step explanation" (step-by-step explanation), with a simpler prompt, Answer: (completion).Results in Table4show that the simpler completion prompt consistently underperforms compared to the step-by-step explanation prompt.Impact of temperature for decoding strategies.We conducted an ablation study to examine how different temperatures affect the model performance.A higher temperature allows for more diversity in model responses.Most models consistently underperform when the temperature is set to 1.0 compared to 0.2 (our default) as shown in Table5.Input Modality ModelAvg Acc (temperature=1) Avg Acc (temperature=0.E Results on Spatial-Real taskTable6presents the performance on the Spatial-Real task.The same trends observed in synthetic tasks persist with real images (see VQA vs. VTQA, TQA (LLM) vs. VTQA, TQA (LLM) vs. VQA in Table2).Notably, compared to synthetic tasks (Figure5and Figure11), the overall accuracy on Spatial-Real is increased across all three input types (TQA, VQA, and VTQA).However, the modality gap (accuracy difference between VTQA and VQA) widens significantly, from 7.0% on synthetic tasks to 30.0% on Spatial-Real.This that the performance disparity is more pronounced on natural images.F Detailed Experimental ResultsResults for proprietary models are summarized in Table7.In Section 3 and Section 4, to clearly illustrate the impact of modality, we present the results averaged over all questions in Figure6for VQA vs. TQA and Figure11for VQA vs. VTQA.This section provides a comprehensive breakdown of results for individual questions.Detailed comparative results for Spatial-Map, Maze-Nav, and Spatial-Grid are shown in Table8, Table9, and Table10, respectively.We compare LLM and VLM with the same language model backbone.For the VLM assessments, we consider inputs in both vision-only (VQA) and vision-text (VTQA) formats.
LLaMA-2-13B LLaMA-2-7B LLaMA-3-8B Mistral-7B. </p>
<p>. Nous-Hermes, -2-Yi-34B Phi-2 Vicuna-13B-1.5 Vicuna-7B-1.5 LLaVA-1.6-34B</p>
<p>InstructBLIP-Vicuna-13B InstructBLIP-Vicuna-7B Spatial-Map. -7b Llava-1.6-Mistral, -13b Llava-1.6-Vicuna, -7b Llava-1.6-Vicuna, -Vl Bunny-Phi-2-Siglip Qwen, Bunny-Phi-2-Eva Bunny-Phi-1.5-Eva Bunny-Phi-1.5-Siglip, Cogvlm Cogagent, </p>
<p>Model Vision-Language Model Figure 5: Performance overview on spatial reasoning tasks. We report the accuracy averaged over all questions. We consider the VQA (Vision-only) format for vision-language models. The dashed red line denotes the expected accuracy for random guessing. For Spatial-Map and Maze-Nav tasks, only a few models outperform random guessing by a notable margin. </p>
<p>Despite these successes, the performance of these models still lags significantly behind human levels. These results underscore the need for further development of techniques tailored to spatial understanding and reasoning. The impact of input modality. To investigate the impact of modality, we compare the performance of a large language model (LLM) and a vision-language model (VLM) with the same language backbone. We consider the VQA (Vision-only) format for vision-language models. The results are shown in Figure 6. Each vertex on the spider plot represents the average accuracy of a (VLM, LLM) pair. We observe that on Spatial-Map and Spatial-Grid, the majority of VLMs yield worse performance compared to their LLM counterpart, despite having an additional visual encoder. For example, on Spatial-Grid. where Llama-3 achieves an accuracy of 71.9%, followed by Mistral-7B-Instruct at 62.1%, both notably surpassing random guessing. Mixtral-7B achieves an average accuracy of 62.1%, while LLaVA-v1.6-Mistral-7B only yields an accuracy of 47.1% (15% ↓). Detailed results can be seen in Appendix F. (c) TQA vs. VQA on Spatial-Grid</p>
<p>VQA on Spatial-Map (b) TQA vs. VQA on Maze-Nav LLaVA-1.6-Mistral-7B. TQA vs. </p>
<p>LLaVA-1.6-Vicuna-13B LLaVA-1.6-Vicuna-7B. </p>
<p>LLaVA-1.6-34B InstructBLIP-Vicuna-7B CogAgent CogVLM LLaVA-1.6-Mistral-7B LLaVA-1.6-Vicuna-13B LLaVA-1.6-Vicuna-7B. </p>
<p>LLaVA-1.6-34B InstructBLIP-Vicuna-13B. </p>
<p>CogAgent CogVLM LLaVA-1.6-Mistral-7B LLaVA-1.6-Vicuna-13B LLaVA-1.6-Vicuna-7B. </p>
<p>LLaVA-1.6-34B InstructBLIP-Vicuna-13B. </p>
<p>CogAgent CogVLM InstructBLIP-Vicuna-13B InstructBLIP-Vicuna-7B InstructBLIP-Vicuna-7B. </p>
<p>LLaVA-1.6-Mistral-7B. </p>
<p>LLaVA-1.6-Vicuna-13B LLaVA-1.6-Vicuna-7B. </p>
<p>LLaVA-1.6-34B InstructBLIP-Vicuna-7B CogAgent CogVLM InstructBLIP-Vicuna-13B LLaVA-1.6-Mistral-7B LLaVA-1.6-Vicuna-13B LLaVA-1.6-Vicuna-7B. </p>
<p>LLaVA-1.6-34B InstructBLIP-Vicuna-7B CogAgent CogVLM InstructBLIP-Vicuna-13B LLaVA-1.6-Mistral-7B LLaVA-1.6-Vicuna-13B LLaVA-1.6-Vicuna-7B. </p>
<p>LLaVA-1.6-34B InstructBLIP-Vicuna-7B CogAgent CogVLM InstructBLIP-Vicuna-13B. </p>
<p>No Img on Spatial-Grid (a) TQA vs. No Img on Spatial-Map (b) TQA vs. No Img on Maze-Nav LLaVA-1.6-34B LLaVA-1.6-Vicuna-7B. TQA vs. </p>
<p>CogVLM LLaVA-1.6-Vicuna-7B Bunny-Phi-2-SigLIP LLaVA-1.6-34B LLaVA-1.6-Vicuna-7B. </p>
<p>Can language models encode perceptual structure without grounding? a case study in color. Mostafa Abdou, Artur Kulmizev, Daniel Hershcovich, Stella Frank, Ellie Pavlick, Anders Søgaard, arXiv:2109.061292021arXiv preprint</p>
<p>Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, arXiv:2303.08774Shyamal Anadkat, et al. Gpt-4 technical report. 2023arXiv preprint</p>
<p>Introducing meta llama 3: The most capable openly available llm to date. A I Meta, 2024</p>
<p>Flamingo: a visual language model for few-shot learning. Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, Advances in neural information processing systems. 202235</p>
<p>The claude 3 model family: Opus, sonnet, haiku. Anthropic, 2024</p>
<p>Vqa: Visual question answering. Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, Lawrence Zitnick, Devi Parikh, Proceedings of the IEEE international conference on computer vision. the IEEE international conference on computer vision2015</p>
<p>. Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, arXiv:2309.166092023Qwen technical report. arXiv preprint</p>
<p>Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, Jingren Zhou, arXiv:2308.12966Qwen-vl: A versatile vision-language model for understanding, localization, text reading, and beyond. 2023arXiv preprint</p>
<p>Beit: Bert pre-training of image transformers. Hangbo Bao, Li Dong, Songhao Piao, Furu Wei, arXiv:2106.082542021arXiv preprint</p>
<p>Language models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Advances in neural information processing systems. 202033</p>
<p>Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality. March 2023</p>
<p>Palm: Scaling language modeling with pathways. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Journal of Machine Learning Research. 242402023</p>
<p>Instructblip: Towards general-purpose vision-language models with instruction tuning. Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng, Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale N Fung, Steven Hoi, Advances in Neural Information Processing Systems. 362024</p>
<p>Bert: Pre-training of deep bidirectional transformers for language understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, arXiv:1810.048052018arXiv preprint</p>
<p>An image is worth 16x16 words: Transformers for image recognition at scale. Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, arXiv:2010.119292020arXiv preprint</p>
<p>Faith and fate: Limits of transformers on compositionality. Nouha Dziri, Ximing Lu, Melanie Sclar, Lorraine Xiang, Liwei Li, Bill Jiang, Sean Yuchen Lin, Peter Welleck, Chandra West, Ronan Bhagavatula, Le Bras, Advances in Neural Information Processing Systems. 202436</p>
<p>Deqing Fu, Ghazal Khalighinejad, Ollie Liu, Bhuwan Dhingra, Dani Yogatama, Robin Jia, Willie Neiswanger, arXiv:2404.01266Isobench: Benchmarking multimodal foundation models on isomorphic representations. 2024arXiv preprint</p>
<p>Xingyu Fu, Yushi Hu, Bangzheng Li, Yu Feng, Haoyu Wang, Xudong Lin, Dan Roth, Noah A Smith, Wei-Chiu Ma, Ranjay Krishna, arXiv:2404.12390Blink: Multimodal large language models can see but not perceive. 2024arXiv preprint</p>
<p>Peng Gao, Jiaming Han, Renrui Zhang, Ziyi Lin, Shijie Geng, Aojun Zhou, Wei Zhang, Pan Lu, Conghui He, Xiangyu Yue, arXiv:2304.15010Llama-adapter v2: Parameter-efficient visual instruction model. 2023arXiv preprint</p>
<p>Jiuxiang Gu, Chenyang Li, Yingyu Liang, Zhenmei Shi, Zhao Song, Tianyi Zhou, arXiv:2402.09469Fourier circuits in neural networks: Unlocking the potential of large language models in mathematical reasoning and modular arithmetic. 2024arXiv preprint</p>
<p>Masked autoencoders are scalable vision learners. Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, Ross Girshick, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition2022</p>
<p>Efficient multimodal learning from data-centric perspective. Muyang He, Yexin Liu, Boya Wu, Jianhao Yuan, Yueze Wang, Tiejun Huang, Bo Zhao, arXiv:2402.115302024arXiv preprint</p>
<p>Cogagent: A visual language model for gui agents. Wenyi Hong, Weihan Wang, Qingsong Lv, Jiazheng Xu, Wenmeng Yu, Junhui Ji, Yan Wang, Zihan Wang, Yuxuan Zhang, Juanzi Li, Bin Xu, Yuxiao Dong, Ming Ding, Jie Tang, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition2024</p>
<p>Large language models for software engineering: A systematic literature review. Xinyi Hou, Yanjie Zhao, Yue Liu, Zhou Yang, Kailong Wang, Li Li, Xiapu Luo, David Lo, John Grundy, Haoyu Wang, 2024</p>
<p>Gqa: A new dataset for real-world visual reasoning and compositional question answering. A Drew, Christopher D Hudson, Manning, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition2019</p>
<p>Rusheb Michael Igorevich Ivanitskiy, Alex F Shah, Tilman Spies, Dan Räuker, Can Valentine, Lucia Rager, Chris Quirke, Guillaume Mathwin, Corlouer, Cecilia Diniz Behn, and Samy Wu Fung. A configurable library for generating and manipulating maze datasets. 2023</p>
<p>. Alexandre Albert Q Jiang, Arthur Sablayrolles, Chris Mensch, Devendra Bamford, Diego Singh Chaplot, Florian De Las Casas, Gianna Bressand, Guillaume Lengyel, Lucile Lample, Saulnier, arXiv:2310.068252023Mistral 7b. arXiv preprint</p>
<p>Clevr: A diagnostic dataset for compositional language and elementary visual reasoning. Justin Johnson, Bharath Hariharan, Laurens Van Der Maaten, Li Fei-Fei, Lawrence Zitnick, Ross Girshick, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition2017</p>
<p>Chatgpt for good? on opportunities and challenges of large language models for education. Enkelejda Kasneci, Kathrin Seßler, Stefan Küchemann, Maria Bannert, Daryna Dementieva, Frank Fischer, Urs Gasser, Georg Groh, Stephan Günnemann, Eyke Hüllermeier, Learning and individual differences. 1031022742023</p>
<p>Geomverse: A systematic evaluation of large models for geometric reasoning. Mehran Kazemi, Hamidreza Alvari, Ankit Anand, Jialin Wu, Xi Chen, Radu Soricut, arXiv:2312.122412023arXiv preprint</p>
<p>Visual genome: Connecting language and vision using crowdsourced dense image annotations. Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A Shamma, International journal of computer vision. 1232017</p>
<p>Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. Junnan Li, Dongxu Li, Silvio Savarese, Steven Hoi, International conference on machine learning. PMLR2023</p>
<p>Blip: Bootstrapping languageimage pre-training for unified vision-language understanding and generation. Junnan Li, Dongxu Li, Caiming Xiong, Steven Hoi, International conference on machine learning. PMLR2022</p>
<p>Large language models in finance: A survey. Yinheng Li, Shaofei Wang, Han Ding, Hang Chen, Proceedings of the Fourth ACM International Conference on AI in Finance. the Fourth ACM International Conference on AI in Finance2023</p>
<p>Yuanzhi Li, Sébastien Bubeck, Ronen Eldan, Allie Del Giorno, Suriya Gunasekar, Yin Tat, Lee , arXiv:2309.05463Textbooks are all you need ii: phi-1.5 technical report. 2023arXiv preprint</p>
<p>Monitoring ai-modified content at scale: A case study on the impact of chatgpt on ai conference peer reviews. Weixin Liang, Zachary Izzo, Yaohui Zhang, Haley Lepp, Hancheng Cao, Xuandong Zhao, Lingjiao Chen, Haotian Ye, Sheng Liu, Zhi Huang, arXiv:2403.071832024arXiv preprint</p>
<p>Haotian Liu, Chunyuan Li, Yuheng Li, Yong Jae Lee, arXiv:2310.03744Improved baselines with visual instruction tuning. 2023arXiv preprint</p>
<p>Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, Yong Jae Lee, Llava-next: Improved reasoning, ocr, and world knowledge. January 2024</p>
<p>Visual instruction tuning. Haotian Liu, Chunyuan Li, Qingyang Wu, Yong Jae Lee, Advances in neural information processing systems. 202436</p>
<p>Are llms capable of data-based statistical and causal reasoning? benchmarking advanced quantitative reasoning with data. Xiao Liu, Zirui Wu, Xueqing Wu, Pan Lu, Kai-Wei Chang, Yansong Feng, arXiv:2402.176442024arXiv preprint</p>
<p>Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, arXiv:2307.06281Is your multi-modal model an all-around player?. 2023arXiv preprint</p>
<p>Swin transformer v2: Scaling up capacity and resolution. Ze Liu, Han Hu, Yutong Lin, Zhuliang Yao, Zhenda Xie, Yixuan Wei, Jia Ning, Yue Cao, Zheng Zhang, Li Dong, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition2022</p>
<p>Swin transformer: Hierarchical vision transformer using shifted windows. Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, Baining Guo, Proceedings of the IEEE/CVF international conference on computer vision. the IEEE/CVF international conference on computer vision2021</p>
<p>Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, Jianfeng Gao, Mathvista, arXiv:2310.02255Evaluating mathematical reasoning of foundation models in visual contexts. 2023arXiv preprint</p>
<p>Brandon Mckinzie, Zhe Gan, Jean-Philippe Fauconnier, Sam Dodge, Bowen Zhang, Philipp Dufter, Dhruti Shah, Xianzhi Du, Futang Peng, Floris Weers, arXiv:2403.09611Methods, analysis &amp; insights from multimodal llm pre-training. 2024arXiv preprint</p>
<p>Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, Luke Zettlemoyer, arXiv:2202.12837Rethinking the role of demonstrations: What makes in-context learning work?. 2022arXiv preprint</p>
<p>Understanding retrieval-augmented task adaptation for visionlanguage models. Yifei Ming, Yixuan Li, Proceedings of the 41st International Conference on Machine Learning. the 41st International Conference on Machine LearningJul 2024</p>
<p>Roshanak Mirzaee, Rajaby Hossein, Qiang Faghihi, Parisa Ning, Kordjmashidi, Spartqa, arXiv:2104.05832A textual question answering benchmark for spatial reasoning. 2021arXiv preprint</p>
<p>Generalized out-of-distribution detection and beyond in vision language model era: A survey. Atsuyuki Miyai, Jingkang Yang, Jingyang Zhang, Yifei Ming, Yueqian Lin, Qing Yu, Go Irie, Shafiq Joty, Yixuan Li, Hai Li, Ziwei Liu, Toshihiko Yamasaki, Kiyoharu Aizawa, arXiv:2407.217942024arXiv preprint</p>
<p>Unsolvable problem detection: Evaluating trustworthiness of vision language models. Atsuyuki Miyai, Jingkang Yang, Jingyang Zhang, Yifei Ming, Qing Yu, Go Irie, Yixuan Li, Hai Li, Ziwei Liu, Kiyoharu Aizawa, arXiv:2403.203312024arXiv preprint</p>
<p>Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova Dassarma, Tom Henighan, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, arXiv:2209.11895-context learning and induction heads. 2022arXiv preprint</p>
<p>Mapping language models to grounded conceptual spaces. Roma Patel, Ellie Pavlick, International conference on learning representations. 2021</p>
<p>Scalable diffusion models with transformers. William Peebles, Saining Xie, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer Vision2023</p>
<p>Learning transferable visual models from natural language supervision. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, International conference on machine learning. PMLR2021</p>
<p>Highresolution image synthesis with latent diffusion models. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, Björn Ommer, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition2022</p>
<p>Stepgame: A new benchmark for robust multihop spatial reasoning in texts. Zhengxiang Shi, Qiang Zhang, Aldo Lipani, Proceedings of the AAAI conference on artificial intelligence. the AAAI conference on artificial intelligence2022</p>
<p>The trade-off between universality and label efficiency of representations from contrastive learning. Zhenmei Shi, Jiefeng Chen, Kunyang Li, Jayaram Raghuram, Xi Wu, Yingyu Liang, Somesh Jha, The Eleventh International Conference on Learning Representations. 2022</p>
<p>Domain generalization via nuclear norm regularization. Zhenmei Shi, Yifei Ming, Ying Fan, Frederic Sala, Yingyu Liang, Conference on Parsimony and Learning (Proceedings Track. 2023</p>
<p>Why larger language models do in-context learning differently. Zhenmei Shi, Junyi Wei, Zhuoyan Xu, Yingyu Liang, R0-FoMo:Robustness of Few-shot and Zero-shot Learning in Large Foundation Models. 2023</p>
<p>A short survey of viewing large language models in legal aspect. Zhongxiang Sun, arXiv:2303.091362023arXiv preprint</p>
<p>Gemini: a family of highly capable multimodal models. Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, arXiv:2312.118052023arXiv preprint</p>
<p>Gemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Shreya Pathak, Laurent Sifre, Morgane Rivière, Mihir Sanjay Kale, Juliette Love, arXiv:2403.08295Open models based on gemini research and technology. 2024arXiv preprint</p>
<p>Large language models in medicine. Arun James Thirunavukarasu, Darren Shu, Jeng Ting, Kabilan Elangovan, Laura Gutierrez, Ting Fang Tan, Daniel Shu, Wei Ting, Nature medicine. 2982023</p>
<p>Thibaut Hugo Touvron, Gautier Lavril, Xavier Izacard, Marie-Anne Martinet, Timothée Lachaux, Baptiste Lacroix, Naman Rozière, Eric Goyal, Hambro, arXiv:2302.13971Faisal Azhar, et al. Llama: Open and efficient foundation language models. 2023arXiv preprint</p>
<p>Llama 2: Open foundation and fine-tuned chat models. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, arXiv:2307.092882023arXiv preprint</p>
<p>Llama 2: Open foundation and fine-tuned chat models. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, arXiv:2307.092882023arXiv preprint</p>
<p>A picture is worth more than 77 text tokens: Evaluating clip-style models on dense captions. Jack Urbanek, Florian Bordes, Pietro Astolfi, Mary Williamson, Vasu Sharma, Adriana Romero-Soriano, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2024</p>
<p>Advances in neural information processing systems. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, Illia Polosukhin, 201730Attention is all you need</p>
<p>Weihan Wang, Qingsong Lv, Wenmeng Yu, Wenyi Hong, Ji Qi, Yan Wang, Junhui Ji, Zhuoyi Yang, Lei Zhao, Xixuan Song, Jiazheng Xu, Bin Xu, Juanzi Li, Yuxiao Dong, Ming Ding, and Jie Tang. Cogvlm: Visual expert for pretrained language models. 2023</p>
<p>Do large language models have compositional ability? an investigation into limitations and scalability. Zhuoyan Xu, Zhenmei Shi, Yingyu Liang, ICLR 2024 Workshop on Mathematical and Empirical Understanding of Foundation Models. 2024</p>
<p>Towards few-shot adaptation of foundation models via multitask finetuning. Zhuoyan Xu, Zhenmei Shi, Junyi Wei, Fangzhou Mu, Yin Li, Yingyu Liang, The Twelfth International Conference on Learning Representations. 2023</p>
<p>Evaluating spatial understanding of large language models. Yutaro Yamada, Yihan Bao, Andrew Kyle Lampinen, Jungo Kasai, Ilker Yildirim, Transactions on Machine Learning Research. 2024</p>
<p>Context-aware attention network for image-text retrieval. Qi Zhang, Zhen Lei, Zhaoxiang Zhang, Stan Z Li, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition2020</p>
<p>Llama-adapter: Efficient fine-tuning of language models with zero-init attention. Renrui Zhang, Jiaming Han, Aojun Zhou, Xiangfei Hu, Shilin Yan, Pan Lu, Hongsheng Li, Peng Gao, Yu Qiao, arXiv:2303.161992023arXiv preprint</p>
<p>Mathverse: Does your multi-modal llm truly see the diagrams in visual math problems?. Renrui Zhang, Dongzhi Jiang, Yichi Zhang, Haokun Lin, Ziyu Guo, Pengshuo Qiu, Aojun Zhou, Pan Lu, Kai-Wei Chang, Peng Gao, arXiv:2403.146242024arXiv preprint</p>
<p>Opt: Open pre-trained transformer language models. Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, arXiv:2205.010682022arXiv preprint</p>
<p>Minigpt-4: Enhancing vision-language understanding with advanced large language models. Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, Mohamed Elhoseiny, arXiv:2304.105922023arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
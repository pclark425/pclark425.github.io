<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7573 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7573</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7573</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-139.html">extraction-schema-139</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being used to detect anomalies in lists or tabular data, including the methods, datasets, evaluation metrics, and results.</div>
                <p><strong>Paper ID:</strong> paper-38c0543aa72b68d1ded4237e8cc5333b165ea249</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/38c0543aa72b68d1ded4237e8cc5333b165ea249" target="_blank">LogBERT: Log Anomaly Detection via BERT</a></p>
                <p><strong>Paper Venue:</strong> IEEE International Joint Conference on Neural Network</p>
                <p><strong>Paper TL;DR:</strong> LogBERT is proposed, a self-supervised framework for log anomaly detection based on Bidirectional Encoder Representations from Transformers (BERT), which outperforms state-of-the-art approaches for anomaly detection.</p>
                <p><strong>Paper Abstract:</strong> Detecting anomalous events in online computer systems is crucial to protect the systems from malicious attacks or malfunctions. System logs, which record detailed information of computational events, are widely used for system status analysis. In this paper, we propose LogBERT, a self-supervised framework for log anomaly detection based on Bidirectional Encoder Representations from Transformers (BERT). LogBERT learns the patterns of normal log sequences by two novel self-supervised training tasks, masked log message prediction and volume of hypersphere minimization. After training, LogBERT is able to capture the patterns of normal log sequences and further detect anomalies where the underlying patterns deviate from expected patterns. The experimental results on three log datasets show that LogBERT outperforms state-of-the-art approaches for anomaly detection.</p>
                <p><strong>Cost:</strong> 0.011</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7573.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7573.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being used to detect anomalies in lists or tabular data, including the methods, datasets, evaluation metrics, and results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LogBERT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LogBERT: Log Anomaly Detection via BERT</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A BERT-inspired Transformer encoder trained with two self-supervised objectives (masked log-key prediction and volume-of-hypersphere minimization) to model normal log sequences and detect anomalous sequences by checking predicted masked tokens against a top-g candidate set and counting anomalies per sequence.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BERT-style Transformer (LogBERT)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Encoder-only Transformer (BERT-style) adapted to log keys; uses input token embeddings + sinusoidal position embeddings, 2 Transformer layers, multi-head self-attention, and contextual embedding of a DIST token as sequence representation.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>not reported (implementation uses 2 transformer layers; input embedding dim=50, hidden dim=256)</td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_detection_approach</strong></td>
                            <td>Self-supervised fine-tuning on normal data: (1) masked log-key prediction (MLKP) to learn bidirectional context, (2) volume-of-hypersphere minimization (VHM) to compact normal sequence representations; detection via masked-token prediction likelihoods (top-g candidate set) and labeling a sequence anomalous if > r anomalous tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>prompt_template</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Trained only on normal log sequences (around 5,000 normal sequences used for training per dataset as stated); no external labeled anomalies used for training.</td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>Log entries / ordered sequences of parsed log keys (discrete categorical sequence data)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>HDFS, BGL, Thunderbird (subsample of Thunderbird as described)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Precision, Recall, F1 score (reported per-dataset)</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>HDFS: Precision=87.02, Recall=78.10, F1=82.32; BGL: Precision=89.40, Recall=92.32, F1=90.83; Thunderbird: Precision=96.75, Recall=96.52, F1=96.64.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared to PCA, iForest, OCSVM, LogCluster, DeepLog, LogAnomaly on same datasets (see table). Example baseline F1s: DeepLog — HDFS 77.34, BGL 86.12, Thunderbird 93.08; LogAnomaly — HDFS 56.19, BGL 74.08, Thunderbird 92.73; LogCluster — HDFS 53.99, BGL 76.63, Thunderbird 59.61; PCA/iForest/OCSVM much lower F1s.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Fully self-supervised / one-class style (trained only on normal data; no labeled anomalies used in training)</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>VHM alone performs poorly; VHM helps for short sequences but has less effect for long sequences (BGL and Thunderbird have long average sequence lengths where MLKP dominates). Performance sensitive to hyperparameters (mask ratio m, candidate set size g, threshold r); requires log parsing step (Drain) to extract log keys; computational cost and model parameter counts not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LogBERT: Log Anomaly Detection via BERT', 'publication_date_yy_mm': '2021-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7573.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7573.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being used to detect anomalies in lists or tabular data, including the methods, datasets, evaluation metrics, and results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DeepLog</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DeepLog: Anomaly Detection and Diagnosis from System Logs through Deep Learning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LSTM-based model that models normal log-key sequences by predicting the next log key given previous ones and flags sequences as anomalous when the actual next key is not within a top-k predicted candidate set; used here as a state-of-the-art baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>DeepLog: Anomaly Detection and Diagnosis from System Logs through Deep Learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DeepLog (LSTM-based RNN)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Recurrent neural network (LSTM) trained to predict the next log key in a sequence; anomaly detection based on whether true next key is among top-k predictions.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>not reported</td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_detection_approach</strong></td>
                            <td>Sequence next-event prediction (RNN/LSTM); anomaly if observed event not in top-k predicted next-events.</td>
                        </tr>
                        <tr>
                            <td><strong>prompt_template</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Trained on normal log sequences (as used in the comparison experiments in this paper); exact training sample count follows the paper's training split (~5000 normal sequences used for training in this evaluation).</td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>Log entries / ordered sequences of parsed log keys</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>HDFS, BGL, Thunderbird (as used in this paper's baselines)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Precision, Recall, F1 score</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Reported in this paper's comparison (Table 2): HDFS Precision=88.44, Recall=69.49, F1=77.34; BGL Precision=89.74, Recall=82.78, F1=86.12; Thunderbird Precision=87.34, Recall=99.61, F1=93.08.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared directly to LogBERT and other baselines in the paper; LogBERT outperforms DeepLog across datasets in F1 (see LogBERT entry).</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Trained on normal data only (one-class/sequence modeling training); not zero/few-shot.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Paper criticizes RNN-based models for only using previous context (unidirectional next-event prediction) and for not explicitly encoding common patterns shared by all normal sequences; this can limit detection when bidirectional context is informative.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LogBERT: Log Anomaly Detection via BERT', 'publication_date_yy_mm': '2021-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7573.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7573.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being used to detect anomalies in lists or tabular data, including the methods, datasets, evaluation metrics, and results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LogAnomaly</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LogAnomaly: Unsupervised Detection of Sequential and Quantitative Anomalies in Unstructured Logs</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A deep-learning based method designed to detect both sequential and quantitative anomalies in logs; included as a baseline in the LogBERT experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>LogAnomaly: Unsupervised Detection of Sequential and Quantitative Anomalies in Unstructured Logs</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LogAnomaly (deep learning-based)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Deep learning approach (architecture described in original LogAnomaly paper) for detecting sequential and quantitative anomalies in logs; here evaluated as a baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>not reported</td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_detection_approach</strong></td>
                            <td>Unsupervised deep-learning based anomaly detection on log sequences; specifics not re-specified in this paper beyond citing the original work.</td>
                        </tr>
                        <tr>
                            <td><strong>prompt_template</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Evaluated using same experimental splits as other baselines in this paper (trained on normal log sequences provided by authors' setup).</td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>Log entries / ordered sequences of parsed log keys</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>HDFS, BGL, Thunderbird (as evaluated in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Precision, Recall, F1 score</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Reported in this paper's comparison (Table 2): HDFS Precision=94.15, Recall=40.47, F1=56.19; BGL Precision=73.12, Recall=76.09, F1=74.08; Thunderbird Precision=86.72, Recall=99.63, F1=92.73.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared against LogBERT and other baselines in the paper; LogBERT achieves higher balanced F1 across datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Trained on normal data only (unsupervised/one-class-style training in the evaluation context).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Paper shows LogAnomaly achieves high precision/recall trade-offs on some datasets but poor balance (e.g., very high precision but low recall on HDFS leading to low F1); exact failure modes depend on dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LogBERT: Log Anomaly Detection via BERT', 'publication_date_yy_mm': '2021-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7573.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7573.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being used to detect anomalies in lists or tabular data, including the methods, datasets, evaluation metrics, and results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BERT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A pretrained bidirectional Transformer encoder (BERT) that learns contextual token representations via masked-language modeling and next-sentence prediction; cited as the inspiration and architectural template for LogBERT.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BERT</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Pre-trained bidirectional Transformer encoder (masked language modeling); encoder-only architecture used for contextual embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>various sizes exist (e.g., BERT-Base 110M, BERT-Large 340M) — not directly used or specified in this paper</td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_detection_approach</strong></td>
                            <td>Mentioned as inspiration; standard BERT is not directly applied to anomaly detection in this paper (LogBERT adapts BERT architecture to log keys and trains from normal data).</td>
                        </tr>
                        <tr>
                            <td><strong>prompt_template</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>general text (original BERT); here referenced for modeling sequences of log keys</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Paper motivates using BERT-style bidirectional context to overcome limitations of unidirectional RNN next-event prediction for logs.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LogBERT: Log Anomaly Detection via BERT', 'publication_date_yy_mm': '2021-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding <em>(Rating: 2)</em></li>
                <li>DeepLog: Anomaly Detection and Diagnosis from System Logs through Deep Learning <em>(Rating: 2)</em></li>
                <li>LogAnomaly: Unsupervised Detection of Sequential and Quantitative Anomalies in Unstructured Logs <em>(Rating: 2)</em></li>
                <li>Deep One-Class Classification <em>(Rating: 2)</em></li>
                <li>Drain: An Online Log Parsing Approach with Fixed Depth Tree <em>(Rating: 1)</em></li>
                <li>Detecting large-scale system problems by mining console logs <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7573",
    "paper_id": "paper-38c0543aa72b68d1ded4237e8cc5333b165ea249",
    "extraction_schema_id": "extraction-schema-139",
    "extracted_data": [
        {
            "name_short": "LogBERT",
            "name_full": "LogBERT: Log Anomaly Detection via BERT",
            "brief_description": "A BERT-inspired Transformer encoder trained with two self-supervised objectives (masked log-key prediction and volume-of-hypersphere minimization) to model normal log sequences and detect anomalous sequences by checking predicted masked tokens against a top-g candidate set and counting anomalies per sequence.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "BERT-style Transformer (LogBERT)",
            "model_description": "Encoder-only Transformer (BERT-style) adapted to log keys; uses input token embeddings + sinusoidal position embeddings, 2 Transformer layers, multi-head self-attention, and contextual embedding of a DIST token as sequence representation.",
            "model_size": "not reported (implementation uses 2 transformer layers; input embedding dim=50, hidden dim=256)",
            "anomaly_detection_approach": "Self-supervised fine-tuning on normal data: (1) masked log-key prediction (MLKP) to learn bidirectional context, (2) volume-of-hypersphere minimization (VHM) to compact normal sequence representations; detection via masked-token prediction likelihoods (top-g candidate set) and labeling a sequence anomalous if &gt; r anomalous tokens.",
            "prompt_template": null,
            "training_data": "Trained only on normal log sequences (around 5,000 normal sequences used for training per dataset as stated); no external labeled anomalies used for training.",
            "data_type": "Log entries / ordered sequences of parsed log keys (discrete categorical sequence data)",
            "dataset_name": "HDFS, BGL, Thunderbird (subsample of Thunderbird as described)",
            "evaluation_metric": "Precision, Recall, F1 score (reported per-dataset)",
            "performance": "HDFS: Precision=87.02, Recall=78.10, F1=82.32; BGL: Precision=89.40, Recall=92.32, F1=90.83; Thunderbird: Precision=96.75, Recall=96.52, F1=96.64.",
            "baseline_comparison": "Compared to PCA, iForest, OCSVM, LogCluster, DeepLog, LogAnomaly on same datasets (see table). Example baseline F1s: DeepLog — HDFS 77.34, BGL 86.12, Thunderbird 93.08; LogAnomaly — HDFS 56.19, BGL 74.08, Thunderbird 92.73; LogCluster — HDFS 53.99, BGL 76.63, Thunderbird 59.61; PCA/iForest/OCSVM much lower F1s.",
            "zero_shot_or_few_shot": "Fully self-supervised / one-class style (trained only on normal data; no labeled anomalies used in training)",
            "limitations_or_failure_cases": "VHM alone performs poorly; VHM helps for short sequences but has less effect for long sequences (BGL and Thunderbird have long average sequence lengths where MLKP dominates). Performance sensitive to hyperparameters (mask ratio m, candidate set size g, threshold r); requires log parsing step (Drain) to extract log keys; computational cost and model parameter counts not reported.",
            "computational_cost": null,
            "uuid": "e7573.0",
            "source_info": {
                "paper_title": "LogBERT: Log Anomaly Detection via BERT",
                "publication_date_yy_mm": "2021-03"
            }
        },
        {
            "name_short": "DeepLog",
            "name_full": "DeepLog: Anomaly Detection and Diagnosis from System Logs through Deep Learning",
            "brief_description": "An LSTM-based model that models normal log-key sequences by predicting the next log key given previous ones and flags sequences as anomalous when the actual next key is not within a top-k predicted candidate set; used here as a state-of-the-art baseline.",
            "citation_title": "DeepLog: Anomaly Detection and Diagnosis from System Logs through Deep Learning",
            "mention_or_use": "use",
            "model_name": "DeepLog (LSTM-based RNN)",
            "model_description": "Recurrent neural network (LSTM) trained to predict the next log key in a sequence; anomaly detection based on whether true next key is among top-k predictions.",
            "model_size": "not reported",
            "anomaly_detection_approach": "Sequence next-event prediction (RNN/LSTM); anomaly if observed event not in top-k predicted next-events.",
            "prompt_template": null,
            "training_data": "Trained on normal log sequences (as used in the comparison experiments in this paper); exact training sample count follows the paper's training split (~5000 normal sequences used for training in this evaluation).",
            "data_type": "Log entries / ordered sequences of parsed log keys",
            "dataset_name": "HDFS, BGL, Thunderbird (as used in this paper's baselines)",
            "evaluation_metric": "Precision, Recall, F1 score",
            "performance": "Reported in this paper's comparison (Table 2): HDFS Precision=88.44, Recall=69.49, F1=77.34; BGL Precision=89.74, Recall=82.78, F1=86.12; Thunderbird Precision=87.34, Recall=99.61, F1=93.08.",
            "baseline_comparison": "Compared directly to LogBERT and other baselines in the paper; LogBERT outperforms DeepLog across datasets in F1 (see LogBERT entry).",
            "zero_shot_or_few_shot": "Trained on normal data only (one-class/sequence modeling training); not zero/few-shot.",
            "limitations_or_failure_cases": "Paper criticizes RNN-based models for only using previous context (unidirectional next-event prediction) and for not explicitly encoding common patterns shared by all normal sequences; this can limit detection when bidirectional context is informative.",
            "computational_cost": null,
            "uuid": "e7573.1",
            "source_info": {
                "paper_title": "LogBERT: Log Anomaly Detection via BERT",
                "publication_date_yy_mm": "2021-03"
            }
        },
        {
            "name_short": "LogAnomaly",
            "name_full": "LogAnomaly: Unsupervised Detection of Sequential and Quantitative Anomalies in Unstructured Logs",
            "brief_description": "A deep-learning based method designed to detect both sequential and quantitative anomalies in logs; included as a baseline in the LogBERT experiments.",
            "citation_title": "LogAnomaly: Unsupervised Detection of Sequential and Quantitative Anomalies in Unstructured Logs",
            "mention_or_use": "use",
            "model_name": "LogAnomaly (deep learning-based)",
            "model_description": "Deep learning approach (architecture described in original LogAnomaly paper) for detecting sequential and quantitative anomalies in logs; here evaluated as a baseline.",
            "model_size": "not reported",
            "anomaly_detection_approach": "Unsupervised deep-learning based anomaly detection on log sequences; specifics not re-specified in this paper beyond citing the original work.",
            "prompt_template": null,
            "training_data": "Evaluated using same experimental splits as other baselines in this paper (trained on normal log sequences provided by authors' setup).",
            "data_type": "Log entries / ordered sequences of parsed log keys",
            "dataset_name": "HDFS, BGL, Thunderbird (as evaluated in this paper)",
            "evaluation_metric": "Precision, Recall, F1 score",
            "performance": "Reported in this paper's comparison (Table 2): HDFS Precision=94.15, Recall=40.47, F1=56.19; BGL Precision=73.12, Recall=76.09, F1=74.08; Thunderbird Precision=86.72, Recall=99.63, F1=92.73.",
            "baseline_comparison": "Compared against LogBERT and other baselines in the paper; LogBERT achieves higher balanced F1 across datasets.",
            "zero_shot_or_few_shot": "Trained on normal data only (unsupervised/one-class-style training in the evaluation context).",
            "limitations_or_failure_cases": "Paper shows LogAnomaly achieves high precision/recall trade-offs on some datasets but poor balance (e.g., very high precision but low recall on HDFS leading to low F1); exact failure modes depend on dataset.",
            "computational_cost": null,
            "uuid": "e7573.2",
            "source_info": {
                "paper_title": "LogBERT: Log Anomaly Detection via BERT",
                "publication_date_yy_mm": "2021-03"
            }
        },
        {
            "name_short": "BERT",
            "name_full": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
            "brief_description": "A pretrained bidirectional Transformer encoder (BERT) that learns contextual token representations via masked-language modeling and next-sentence prediction; cited as the inspiration and architectural template for LogBERT.",
            "citation_title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
            "mention_or_use": "mention",
            "model_name": "BERT",
            "model_description": "Pre-trained bidirectional Transformer encoder (masked language modeling); encoder-only architecture used for contextual embeddings.",
            "model_size": "various sizes exist (e.g., BERT-Base 110M, BERT-Large 340M) — not directly used or specified in this paper",
            "anomaly_detection_approach": "Mentioned as inspiration; standard BERT is not directly applied to anomaly detection in this paper (LogBERT adapts BERT architecture to log keys and trains from normal data).",
            "prompt_template": null,
            "training_data": null,
            "data_type": "general text (original BERT); here referenced for modeling sequences of log keys",
            "dataset_name": null,
            "evaluation_metric": null,
            "performance": null,
            "baseline_comparison": null,
            "zero_shot_or_few_shot": null,
            "limitations_or_failure_cases": "Paper motivates using BERT-style bidirectional context to overcome limitations of unidirectional RNN next-event prediction for logs.",
            "computational_cost": null,
            "uuid": "e7573.3",
            "source_info": {
                "paper_title": "LogBERT: Log Anomaly Detection via BERT",
                "publication_date_yy_mm": "2021-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
            "rating": 2
        },
        {
            "paper_title": "DeepLog: Anomaly Detection and Diagnosis from System Logs through Deep Learning",
            "rating": 2
        },
        {
            "paper_title": "LogAnomaly: Unsupervised Detection of Sequential and Quantitative Anomalies in Unstructured Logs",
            "rating": 2
        },
        {
            "paper_title": "Deep One-Class Classification",
            "rating": 2
        },
        {
            "paper_title": "Drain: An Online Log Parsing Approach with Fixed Depth Tree",
            "rating": 1
        },
        {
            "paper_title": "Detecting large-scale system problems by mining console logs",
            "rating": 1
        }
    ],
    "cost": 0.011297,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>LogBERT: Log Anomaly Detection via BERT</h1>
<p>Haixuan Guo ${ }^{1}$, Shuhan Yuan ${ }^{1}$, and Xintao $\mathrm{Wu}^{2}$<br>${ }^{1}$ Utah State University, Logan UT<br>ghaixan95@aggiemail.usu.edu, shuhan.yuan@usu.edu<br>${ }^{2}$ University of Arkansas, Fayetteville AR<br>xintaowu@uark.edu</p>
<h4>Abstract</h4>
<p>Detecting anomalous events in online computer systems is crucial to protect the systems from malicious attacks or malfunctions. System logs, which record detailed information of computational events, are widely used for system status analysis. In this paper, we propose LogBERT, a self-supervised framework for log anomaly detection based on Bidirectional Encoder Representations from Transformers (BERT). LogBERT learns the patterns of normal log sequences by two novel selfsupervised training tasks and is able to detect anomalies where the underlying patterns deviate from normal log sequences. The experimental results on three log datasets show that LogBERT outperforms state-of-the-art approaches for anomaly detection.</p>
<p>Keywords: anomaly detection $\cdot$ log sequences $\cdot$ BERT</p>
<h2>1 Introduction</h2>
<p>Online computer systems are vulnerable to various malicious attacks in cyberspace. Detecting anomalous events from online computer systems in a timely manner is the fundamental step to protect the systems. System logs, which record detailed information about computational events generated by computer systems, play an important role in anomaly detection nowadays.</p>
<p>Currently, many traditional machine learning models are proposed for identifying anomalous events from log messages. These approaches extract useful features from log messages and adopt machine learning algorithms to analyze the log data. Due to the data imbalance issue, it is infeasible to train a binary classifier to detect anomalous log sequences. As a result, many unsupervised learning models, such as Principal Component Analysis (PCA) [19], or one class classification models, such as one-class SVM [516], are widely-used to detect anomalies. However, traditional machine learning models, such as one-class SVM, are hard to capture the temporal information of discrete log messages.</p>
<p>Recently, deep learning models, especially recurrent neural networks (RNNs), are widely used for log anomaly detection since they are able to model the sequential data [2|317]. However, there are still some limitations of using RNN for modeling log data. First, although RNN can capture the sequential information by the recurrence formula, it cannot make each $\log$ in a sequence encoding the</p>
<p>context information from both the left and right context. However, it is crucial to observe the complete context information instead of only the information from previous steps when detecting malicious attacks based on log messages. Second, current RNN-based anomaly detection models are trained to capture the patterns of normal sequences by prediction the next log message given previous log messages. This training objective mainly focuses on capturing the correlation among the log messages in normal sequences. When such correlation in a log sequence is violated, the RNN model cannot correctly predict the next log message based on previous ones. Then, we will label the sequence as anomalous. However, only using the prediction of next log message as objective function cannot not explicitly encode the common patterns shared by all normal sequences.</p>
<p>To tackle the existing limitations of RNN-based models, in this work, we propose LogBERT, a self-supervised framework for log anomaly detection based on Bidirectional Encoder Representations from Transformers (BERT). Inspired by the great success of BERT in modeling sequential text data [1], we leverage BERT to capture patterns of normal log sequences. By using the structure of BERT, we expect the contextual embedding of each log entry can capture the information of whole log sequences. To achieve that, we propose two selfsupervised training tasks: 1) masked log key prediction, which aims to correctly predict log keys in normal log sequences that are randomly masked; 2) volume of hypersphere minimization, which aims to make the normal log sequences close to each other in the embedding space. After training, we expect LogBERT encodes the information about normal log sequences. We then derive a criterion to detect anomalous log sequences based on LogBERT. Experimental results on three log datasets show that LogBERT achieves the best performance on log anomaly detection by comparing with various state-of-the-art baselines.</p>
<h1>2 Related Work</h1>
<p>System logs are widely used by large online computer systems for troubleshooting, where each log message is usually a semi-structured text string. The traditional approaches explicitly use the keywords (e.g., "fail") or regular expressions to detect anomalous log entries. However, these approaches cannot detect malicious attacks based on a sequence of operations, where each log entry looks normal, but the whole sequence is anomalous. To tackle this challenge, many rule-based approaches are proposed to identify anomalous events [1120]. Although rule based approaches can achieve high accuracy, they can only identify pre-defined anomalous scenarios and require heavy manual engineering.</p>
<p>As malicious attacks become more complicated, learning-based approaches are proposed. The typical pipeline for these approaches consists of three steps [4]. First, a log parser is adopted to transform log messages to log keys. A feature extraction approach, such as TF-IDF, is then used to build a feature vector to represent a sequence of log keys in a sliding window. Finally, in most cases, an unsupervised approach is applied for detecting the anomalous sequences [189].</p>
<p>Recently, many deep learning-based log anomaly detection approaches are proposed for log anomaly detection [21|2|22|23|8|17]. Most of the existing approaches adopt recurrent neural networks, especially long-short term memory (LSTM) or gated recurrent unit (GRU) to model the normal log key sequences and derive anomalous scores to detect the anomalous log sequences [2|23|17]. In this work, we explore the advanced BERT model to capture the information of log sequences and propose two novel self-supervised tasks to train the model.</p>
<h1>3 LogBERT</h1>
<p>In this section, we introduce our framework, LogBERT, for log sequence anomaly detection. Inspired by BERT [1], LogBERT leverages the Transformer encoder to model log sequences and is trained by novel self-supervised tasks to capture the patterns of normal sequences. Figure 1 shows the whole framework of LogBERT.
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Fig. 1: The overview of LogBERT</p>
<h3>3.1 Framework</h3>
<p>Given a sequence of unstructured log messages, we aim to detect whether this sequence is normal or anomalous. In order to represent log messages, following a typical pre-processing approach, we first extract log keys (string templates) from log messages via a log parser (shown in Figure 2). Then, we can define a log sequence as a sequence of ordered log keys $S=\left{k_{1}, \ldots, k_{t}, \ldots, k_{T}\right}$, where $k_{t} \in \mathcal{K}$ indicates the log key in the $t$-th position, and $\mathcal{K}$ indicates a set of log keys extracted from log messages. The goal of this task is to predict whether a new log sequence $S$ is anomalous based on a training dataset $\mathcal{D}=\left{S^{j}\right}_{j=1}^{N}$ that consists of only normal log sequences. To achieve that, LogBERT models the normal sequences and further derive an anomaly detection criterion to identify anomalous sequences.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Log Messages</th>
<th style="text-align: center;">Log Keys</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\vdash$ CE sym $\mathrm{s}^{\mathrm{s}} \mathrm{s}$, at $\mathrm{s}^{\mathrm{s}} \mathrm{s}$, mask $\mathrm{s}^{\mathrm{s}} \mathrm{s}$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\vdash$ ddr: activating redundant bit steering: rank $1 / 2$ by $\mathrm{rad} / \mathrm{s}^{\mathrm{s}}$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\vdash$ rank $1 / 2$ by $\mathrm{rad} / \mathrm{s}^{\mathrm{s}}$</td>
</tr>
</tbody>
</table>
<p>Fig. 2: Log messages in the BGL dataset and the corresponding log keys extracted by a log parser. The message with red underscore indicates the detailed computational event.</p>
<p>Input Representation. Given a normal log sequence $S^{j}$, we first add a special token, DIST, at the beginning of $S^{j}=\left{k_{1}^{j}, \ldots, k_{t}^{j}, \ldots, k_{T}^{j}\right}$ as the first log key, which is used to represent the whole log sequence based on the structure of Transformer encoder. LogBERT then represents each log key $k_{t}^{j}$ as an input representation $\mathbf{x}<em t="t">{t}^{j}$, where the representation $\mathbf{x}</em>}^{j}$ is a summation of a log key embedding and a position embedding. In this work, we randomly generate a matrix $\mathbf{E} \in \mathbb{R}^{|\mathcal{K}| * d}$ as the log key embedding matrix, where $d$ is the dimension of log key embedding, while the position embeddings $\mathbf{T} \in \mathbb{R}^{T * d}$ are generated by using a sinusoid function to encode the position information of log keys in a sequence [1]. Finally, the input representation of the log key $k_{t}$ is defined as: $\mathbf{x<em k__t="k_{t">{t}^{j}=\mathbf{e}</em>}^{j}}+\mathbf{t<em t="t">{k</em>$.
Transformer Encoder. LogBERT adopts Transformer encoder to learn the contextual relations among log keys in a sequence. Transformer encoder consists of multiple transformer layers. Each transformer layer includes a multi-head self-attention and a position-wise feed forward sub-layer in which a residual connection is employed around each of two sub-layers, followed by layer normalization [15]. The multi-head attention employs $H$ parallel self-attentions to jointly capture different aspect information at different positions over the input log sequence. Formally, for the $l$-th head of the attention layer, the scaled dot-product self-attention is defined as:}^{j}</p>
<p>$$
\text { head }<em l="l">{l}=\operatorname{Attention}\left(\mathbf{X}^{j} \mathbf{W}</em>}^{Q}, \mathbf{X}^{j} \mathbf{W<em l="l">{l}^{K}, \mathbf{X}^{j} \mathbf{W}</em>\right)
$$}^{V</p>
<p>where $\operatorname{Attention}(\mathbf{Q}, \mathbf{K}, \mathbf{V})=\operatorname{softmax}\left(\frac{\mathbf{Q} \mathbf{K}^{T}}{\sqrt{d_{v}}}\right) \mathbf{V} ; \mathbf{X}^{j} \in \mathbb{R}^{T * d}$ is the input representation of the log sequence; $\mathbf{W}<em l="l">{l}^{Q}, \mathbf{W}</em>}^{K}$ and $\mathbf{W<em v="v">{l}^{V}$ are linear projection weights with dimensions $\mathbb{R}^{d * d</em>$ is the dimension for one head ot the attention layer. Each self-attention makes each key attend to all the log keys in an input sequence and computes the hidden representation for each log key with an attention distribution over the sequence.}}$ for the $l$-th head, and $d_{v</p>
<p>The multi-head attention employs a parallel of self-attentions to jointly capture different aspect information at different log keys. Formally, the multi-head attention concatenates $H$ parallel heads together as:</p>
<p>$$
f\left(\mathbf{X}^{j}\right)=\operatorname{Concat}\left(\text { head }<em H="H">{1}, \ldots, \text { head }</em>
$$}\right) \mathbf{W}^{O</p>
<p>where $\mathbf{W}^{O} \in \mathbb{R}^{h d_{v} * d_{o}}$ is a projection matrix, and $d_{o}$ is the dimension for the output of multi-head attention sub-layer.</p>
<p>Then, the position-wise feed forward sub-layer with a ReLU activation is applied to the hidden representation of each activity separately. Finally, by combining the position-wise feed forward sub-layer and multi-head attention, a transformer layer is defined as:</p>
<p>$$
\text { transformer_layer }\left(\mathbf{X}^{j}\right)=F F N\left(f\left(\mathbf{X}^{j}\right)\right)=\operatorname{ReLU}\left(f\left(\mathbf{X}^{j}\right) \mathbf{W}<em 2="2">{1}\right) \mathbf{W}</em>
$$</p>
<p>where $\mathbf{W}<em 2="2">{1}$ and $\mathbf{W}</em>$ are trained projection matrices.
The Transformer encoder usually consists of multiple transformer layers. We denote $\mathbf{h}<em t="t">{t}^{j}$ as the contextual embedding vector of the log key $k</em>}^{j}$ produced by the Transformer encoder, i.e., $\mathbf{h<em t="t">{t}^{j}=\operatorname{Transformer}\left(x</em>\right)$.}^{j</p>
<h1>3.2 Objective Function</h1>
<p>In order to train the LogBERT model, we propose two self-supervised training tasks to capture the patterns of normal log sequences.
Task I: Masked Log Key Prediction (MLKP). In order to capture the bidirectional context information of log sequences, we train LogBERT to predict the masked log keys in log sequences. In our scenario, LogBERT takes log sequences with random masks as inputs, where we randomly replace a ratio of log keys in a sequence with a specific MASK token. The training objective is to accurately predict the randomly masked log keys. The purpose is to make LogBERT encode the prior knowledge of normal log sequences.</p>
<p>To achieve that, we feed the contextual embedding vector of the $i$-th MASK token in the $j$-th log sequence $\mathbf{h}<em i="i">{\left[\operatorname{MASK}</em>$ :}\right]}^{j}$ to a softmax function, which will output a probability distribution over the entire set of log keys $\mathcal{K</p>
<p>$$
\hat{\mathbf{y}}<em i="i">{\left[\operatorname{MASK}</em>}\right]}^{j}=\operatorname{Softmax}\left(\mathbf{W<em _left_operatorname_MASK="\left[\operatorname{MASK">{C} \mathbf{h}</em><em C="C">{i}\right]}^{j}+\mathbf{b}</em>\right)
$$</p>
<p>where $\mathbf{W}<em C="C">{C}$ and $\mathbf{b}</em>$ are trainable parameters. Then, we adopt the cross entropy loss as the objective function for masked log key prediction, which is defined as:</p>
<p>$$
\mathcal{L}<em j="1">{M L K P}=-\frac{1}{N} \sum</em>}^{N} \sum_{i=1}^{M} \mathbf{y<em i="i">{\left[\operatorname{MASK}</em>}\right]}^{j} \log \hat{\mathbf{y}<em i="i">{\left[\operatorname{MASK}</em>
$$}\right]}^{j</p>
<p>where $\mathbf{y}<em _left_right.="\left[\right.">{\left[\right.}^{j}{ }</em>$ indicates the real log key for the $i$-th masked token, and $M$ is the total number of masked tokens in the $j$-th log sequence. Since the patterns of normal and anomalous log sequences are different, we expect once LogBERT is able to correctly predict the masked log keys, it can distinguish the normal and anomalous log sequences.
Task II: Volume of Hypersphere Minimization (VHM). Inspired by the Deep SVDD approach [13], where the objective is to minimize the volume of a data-enclosing hypersphere, we propose a spherical objective function to regulate the distribution of normal log sequences. The motivation is that normal log sequences should be concentrated and close to each other in the embedding space, while the anomalous log sequences are far to the center of the sphere. We</p>
<p>first derive the representations of normal log sequences and then compute the center representation based on the mean operation. In particular, we consider the contextual embedding vector of the DIST token $\mathbf{h}<em _mathrm_DIST="\mathrm{DIST">{\text {DIST }}^{j}$, which encodes the information of entire log sequence based on the Transformer encoder, as the representation of a log sequence in the embedding space. To make the representations of normal log sequences close to each other, we further derive the center representation of normal log sequences $\mathbf{c}$ in the training set by a mean operation, i.e., $\mathbf{c}=\operatorname{Mean}\left(\mathbf{h}</em>$ close to the center representation c:}}^{j}\right)$. Then, the objective function is to make the representation of normal log sequence $\mathbf{h}_{\text {DIST }}^{j</p>
<p>$$
\mathcal{L}<em j="1">{V H M}=\frac{1}{N} \sum</em>
$$}^{N}\left|\mathbf{h}_{\mathrm{DIST}}^{j}-\mathbf{c}\right|^{2</p>
<p>By minimizing the Equation 6, we expect all the normal log sequences in the training set are close to the center, while the anomalous log sequences have a larger distance to the center. Meanwhile, another advantage of the spherical objective function is that by making the sequence representations close to the center, the Transformer encoder can also leverage the information from other log sequences via the center representation $\mathbf{c}$, since $\mathbf{c}$ encodes all the information of normal log sequences. As a result, the model should be able to predict the masked log keys with higher accuracy for normal log sequences because the normal log sequences should share similar patterns.</p>
<p>Finally, the objective function for training the LogBERT is defined as below:</p>
<p>$$
\mathcal{L}=\mathcal{L}<em H="H" M="M" V="V">{M L K P}+\alpha \mathcal{L}</em>
$$</p>
<p>where $\alpha$ is a hyper-parameter to balance two training tasks.</p>
<h1>3.3 Anomaly Detection</h1>
<p>After training, we can deploy LogBERT for anomalous log sequence detection. The idea of applying LogBERT for log anomaly detection is that since LogBERT is trained on normal log sequences, it can achieve high prediction accuracy on predicting the masked log keys if a testing log sequence is normal. Hence, we can derive the anomalous score of a log sequence based on the prediction results on the MASK tokens. To this end, given a testing log sequence, similar to the training process, we first randomly replace a ratio log keys with MASK tokens and use the randomly-masked log sequence as an input to LogBERT. Then, given a MASK token, the probability distribution calculated based on Equation 4 indicates the likelihood of a log key appeared in the position of the MASK token. Similar to the strategy in DeepLog [2], we build a candidate set consisting of $g$ normal log keys with the top $g$ highest likelihoods computed by $\hat{\mathbf{y}}<em i="i">{\left[\right.$ MASK $\left.</em>$. If the real log key is in the candidate set, we treat the key as normal. In other words, if the observed log key is not in the top- $g$ candidate set predicted by LogBERT, we consider the log key as an anomalous log key. Then, when a log sequence consists of more than $r$ anomalous log keys, we will label this log sequence as}\right]</p>
<p>anomalous. Both $g$ and $r$ are hyper-parameters and will be tuned based on a validation set.</p>
<h1>4 Experiments</h1>
<h3>4.1 Experimental Setup</h3>
<p>Datasets. We evaluate the proposed LogBERT on three log datasets, HDFS, BGL, and Thunderbird. Table 1 shows the statistics of the datasets. For all datasets, we adopt around 5000 normal log sequences for training. The number in the brackets under the column "# Log Keys" indicates the number of unique log keys in the training dataset.</p>
<p>Table 1: Statistics of evaluation datasets</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Dataset</th>
<th style="text-align: center;"># Log Messages</th>
<th style="text-align: center;"># Anomalies</th>
<th style="text-align: center;"># Log Keys</th>
<th style="text-align: center;"># of Log Sequences in Test Dataset</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Normal</td>
<td style="text-align: center;">Anomalous</td>
</tr>
<tr>
<td style="text-align: center;">HDFS</td>
<td style="text-align: center;">$11,172,157$</td>
<td style="text-align: center;">284,818</td>
<td style="text-align: center;">46 (15)</td>
<td style="text-align: center;">553,366</td>
<td style="text-align: center;">10,647</td>
</tr>
<tr>
<td style="text-align: center;">BGL</td>
<td style="text-align: center;">$4,747,963$</td>
<td style="text-align: center;">348,460</td>
<td style="text-align: center;">334 (175)</td>
<td style="text-align: center;">10,045</td>
<td style="text-align: center;">2,630</td>
</tr>
<tr>
<td style="text-align: center;">Thunderbird-mimi</td>
<td style="text-align: center;">20,000,000</td>
<td style="text-align: center;">758,562</td>
<td style="text-align: center;">1,165 (866)</td>
<td style="text-align: center;">71,155</td>
<td style="text-align: center;">45,385</td>
</tr>
</tbody>
</table>
<ul>
<li>Hadoop Distributed File System (HDFS) [18]. HDFS dataset is generated by running Hadoop-based map-reduce jobs on Amazon EC2 nodes and manually labeled through handcrafted rules to identify anomalies. HDFS dataset consists of $11,172,157 \log$ messages, of which 284,818 are anomalous. For HDFS, we group log keys into log sequences based on the session ID in each log message. The average length of log sequences is 19 .</li>
<li>BlueGene/L Supercomputer System (BGL) [10]. BGL dataset is collected from a BlueGene/L supercomputer system at Lawrence Livermore National Labs (LLNL). Logs contain alert and non-alert messages identified by alert category tags. The alert messages are considered as anomalous. BGL dataset consists of $4,747,963 \log$ messages, of which 348,460 are anomalous. For BGL, we define a time sliding window as 5 minutes to generate log sequences, where the average length is 562 .</li>
<li>Thunderbird [10]. Thunderbird dataset is another large log dataset collected from a supercomputer system. We select the first $20,000,000 \log$ messages from the original Thunderbird dataset to compose our dataset, of which 758,562 are anomalous. For Thunderbird, we also adopt a time sliding window as 1 minute to generate log sequences, where the average length is 326 .</li>
</ul>
<p>Baselines. We compare our LogBERT model with the following baselines.</p>
<ul>
<li>
<p>Principal Component Analysis (PCA) [19]. PCA builds counting matrix based on the frequency of log keys sequences and then reduces the original counting matrix into a low dimensional space to detect anomalous sequences.</p>
</li>
<li>
<p>One-Class SVM (OCSVM) [14]. One-Class SVM is a well-known one-class classification model and widely used for log anomaly detection [5,16] by only observing the normal data.</p>
</li>
<li>IsolationForest (iForest) [7]. Isolation forest is an unsupervised learning algorithm for anomaly detection by representing features as tree structures.</li>
<li>LogCluster [6]. LogCluster is a clustering based approach, where the anomalous log sequences are detected by comparing with the existing clusters.</li>
<li>DeepLog [2]. DeepLog is a state-of-the-art log anomaly detection approach. DeepLog adopts recurrent neural network to capture patterns of normal log sequences and further identifies the anomalous log sequences based on the performance of log key predictions.</li>
<li>LogAnomaly [23]. Log Anomaly is a deep learning-based anomaly detection approach and able to detect sequential and quantitative log anomalies.</li>
</ul>
<p>Implementation Details. We adopt Drain [3] to parse the log messages into log keys. Regarding baselines, we leverage the package Loglizer [4] to evaluate PCA, OCSVM, iForest as well as LogCluster for anomaly detection and adopt the open source deep learning-based log analysis toolkit to evaluate DeepLog and LogAnomaly ${ }^{3}$. For LogBERT, we construct a Transformer encoder by using two Transformer layers. The dimensions for the input representation and hidden vectors are 50 and 256, respectively. The hyper-parameters, including $\alpha$ in Equation 7, $m$ the ratio of masked log keys for the MKLP task, $r$ the number of predicted anomalous log keys, and $g$ the size of top- $g$ candidate set for anomaly detection are tuned based on a small validation set. In our experiments, both training and detection phases have the same ratio of masked log keys $m$. The code of LogBERT is available online ${ }^{4}$.</p>
<h1>4.2 Experimental Results</h1>
<p>Table 2: Experimental Results on HDFS, BGL, and Thunderbird Datasets</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Method</th>
<th style="text-align: center;">HDFS</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">BGL</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Thunderbird</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Precision</td>
<td style="text-align: center;">Recall</td>
<td style="text-align: center;">F-1 score</td>
<td style="text-align: center;">Precision</td>
<td style="text-align: center;">Recall</td>
<td style="text-align: center;">F-1 score</td>
<td style="text-align: center;">Precision</td>
<td style="text-align: center;">Recall</td>
<td style="text-align: center;">F-1 score</td>
</tr>
<tr>
<td style="text-align: center;">PCA</td>
<td style="text-align: center;">5.89</td>
<td style="text-align: center;">100.00</td>
<td style="text-align: center;">11.12</td>
<td style="text-align: center;">9.07</td>
<td style="text-align: center;">98.23</td>
<td style="text-align: center;">16.61</td>
<td style="text-align: center;">37.35</td>
<td style="text-align: center;">100.00</td>
<td style="text-align: center;">54.39</td>
</tr>
<tr>
<td style="text-align: center;">iForest</td>
<td style="text-align: center;">53.60</td>
<td style="text-align: center;">69.41</td>
<td style="text-align: center;">60.49</td>
<td style="text-align: center;">99.70</td>
<td style="text-align: center;">18.11</td>
<td style="text-align: center;">30.65</td>
<td style="text-align: center;">34.45</td>
<td style="text-align: center;">1.68</td>
<td style="text-align: center;">3.20</td>
</tr>
<tr>
<td style="text-align: center;">OCSVM</td>
<td style="text-align: center;">2.54</td>
<td style="text-align: center;">100.00</td>
<td style="text-align: center;">4.95</td>
<td style="text-align: center;">1.06</td>
<td style="text-align: center;">12.24</td>
<td style="text-align: center;">1.96</td>
<td style="text-align: center;">18.89</td>
<td style="text-align: center;">39.11</td>
<td style="text-align: center;">25.48</td>
</tr>
<tr>
<td style="text-align: center;">LogCluster</td>
<td style="text-align: center;">99.26</td>
<td style="text-align: center;">37.08</td>
<td style="text-align: center;">53.99</td>
<td style="text-align: center;">95.46</td>
<td style="text-align: center;">64.01</td>
<td style="text-align: center;">76.63</td>
<td style="text-align: center;">98.28</td>
<td style="text-align: center;">42.78</td>
<td style="text-align: center;">59.61</td>
</tr>
<tr>
<td style="text-align: center;">DeepLog</td>
<td style="text-align: center;">88.44</td>
<td style="text-align: center;">69.49</td>
<td style="text-align: center;">77.34</td>
<td style="text-align: center;">89.74</td>
<td style="text-align: center;">82.78</td>
<td style="text-align: center;">86.12</td>
<td style="text-align: center;">87.34</td>
<td style="text-align: center;">99.61</td>
<td style="text-align: center;">93.08</td>
</tr>
<tr>
<td style="text-align: center;">LogAnomaly</td>
<td style="text-align: center;">94.15</td>
<td style="text-align: center;">40.47</td>
<td style="text-align: center;">56.19</td>
<td style="text-align: center;">73.12</td>
<td style="text-align: center;">76.09</td>
<td style="text-align: center;">74.08</td>
<td style="text-align: center;">86.72</td>
<td style="text-align: center;">99.63</td>
<td style="text-align: center;">92.73</td>
</tr>
<tr>
<td style="text-align: center;">LogBERT</td>
<td style="text-align: center;">87.02</td>
<td style="text-align: center;">78.10</td>
<td style="text-align: center;">82.32</td>
<td style="text-align: center;">89.40</td>
<td style="text-align: center;">92.32</td>
<td style="text-align: center;">90.83</td>
<td style="text-align: center;">96.75</td>
<td style="text-align: center;">96.52</td>
<td style="text-align: center;">96.64</td>
</tr>
</tbody>
</table>
<p>Performance on Log Anomaly Detection. Table 2 shows the results of LogBERT as well as baselines on three datasets. We can notice that PCA, Isolation Forest, and OCSVM have poor performance on log anomaly detection.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Although these methods could achieve extremely high precision or recall values, they cannot balance the performance on both precision and recall, which lead to extremely low F1 scores. This could be because using the counting vector to represent a log sequence leads to the loss of temporal information from sequences. LogCluster, which is designed for log anomaly detection, achieves better performance than the PCA, Isolation Forest, and OCSVM. Meanwhile, two deep learning-based baselines, DeepLog and LogAnomaly, significantly outperform the traditional approaches and achieve reasonable F1 scores on three datasets, which show the advantage to adopt deep learning models to capture the patterns of log sequences. Moreover, our proposed LogBERT achieves the highest F1 scores on three datasets with large margins by comparing with all baselines. It indicates that by using self-supervised training tasks, LogBERT can successfully model the normal log sequences and further identify anomalous sequences with high accuracy.</p>
<p>Table 3: Performance of LogBERT base on One Self-supervised Training Task</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">HDFS</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">BGL</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Thunderbird</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">Precision</td>
<td style="text-align: center;">Recall</td>
<td style="text-align: center;">F-1 score</td>
<td style="text-align: center;">Precision</td>
<td style="text-align: center;">Recall</td>
<td style="text-align: center;">F-1 score</td>
<td style="text-align: center;">Precision</td>
<td style="text-align: center;">Recall</td>
<td style="text-align: center;">F-1 score</td>
</tr>
<tr>
<td style="text-align: left;">MLKP</td>
<td style="text-align: center;">77.54</td>
<td style="text-align: center;">78.65</td>
<td style="text-align: center;">78.09</td>
<td style="text-align: center;">93.16</td>
<td style="text-align: center;">86.46</td>
<td style="text-align: center;">89.69</td>
<td style="text-align: center;">97.07</td>
<td style="text-align: center;">95.90</td>
<td style="text-align: center;">96.48</td>
</tr>
<tr>
<td style="text-align: left;">VHM</td>
<td style="text-align: center;">2.43</td>
<td style="text-align: center;">39.17</td>
<td style="text-align: center;">4.58</td>
<td style="text-align: center;">71.04</td>
<td style="text-align: center;">43.84</td>
<td style="text-align: center;">54.22</td>
<td style="text-align: center;">56.58</td>
<td style="text-align: center;">43.87</td>
<td style="text-align: center;">49.42</td>
</tr>
<tr>
<td style="text-align: left;">Both</td>
<td style="text-align: center;">87.02</td>
<td style="text-align: center;">78.10</td>
<td style="text-align: center;">82.32</td>
<td style="text-align: center;">89.40</td>
<td style="text-align: center;">92.32</td>
<td style="text-align: center;">90.83</td>
<td style="text-align: center;">96.75</td>
<td style="text-align: center;">96.52</td>
<td style="text-align: center;">96.64</td>
</tr>
</tbody>
</table>
<p>Ablation Studies. In order to further understand our proposed LogBERT, we conduct ablation experiments on three log datasets. LogBERT is trained by two self-supervised tasks. We evaluate the performance of LogBERT by only using one training task each time. When the model is only trained by minimizing the volume of hypersphere, we identify anomalous log sequences by computing distances of the log sequence representations to the center of normal log sequences $\mathbf{c}$. If the distance is larger than a threshold, we consider a log sequence is anomalous. Table 3 shows the experimental results. We can notice that when only using the task of masked log key prediction to train the model, we can still get very good performance on log anomaly detection, which shows the effectiveness of training the model by predicting masked log keys. We can also notice that even we do not train the LogBERT with the task of the volume of hypersphere minimization, LogBERT achieves higher F1 scores than DeepLog on all three datasets, which shows that compared with LSTM, Transformer encoder is better at capturing the patterns of log sequences. Meanwhile, we can observe that when only training the model for minimizing the volume of hypersphere, the performance is poor. It indicates that only using distance as a measure to identify anomalous log sequences cannot achieve good performance. However, combining two self-supervised tasks to train LogBERT can achieve better performance than the models only trained by one task. Especially, for the HDFS dataset, LogBERT trained by two self-supervised tasks gains a large margin in terms of F1 score (82.32) compared with the model only trained by MLKP (78.09). For BGL and</p>
<p>Thunderbird, the improvement of LogBERT is not as significant as the model in HDFS. This could be because the average length of log sequences in BGL (562) and Thunderbird (326) datasets are much larger than the log sequences in HDFS (19). For longer sequences, only predicting the masked log keys can capture the most important patterns of log sequences since there are many more mask tokens in longer sequences. On the other hand, for short log sequences, we cannot have many masks tokens. As a result, the task of the volume of hypersphere minimization can help to boost the performance. Hence, based on Table 3, we can conclude that using two self-supervised tasks to train LogBERT can achieve better performance, especially when the log sequences are relatively short.
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Fig. 3: Visualization of log sequences by using the contextual embedding of DIST tokens $\mathbf{h}_{\text {DIST }}$. The blue dots indicate the normal log sequences, while the orange ' $x$ ' symbols indicate anomalous log sequences.</p>
<p>Visualization. In order to visualize the log sequences, we adopt locally linear embedding (LLE) algorithm [12] to map the log sequence representations into a two dimensional space, where the contextual embedding of DIST token $\mathbf{h}_{\text {DIST }}$ is used as the representation of a log sequence. We randomly select 1000 normal and 1000 anomalous sequences from the HDFS dataset for visualization. Figure 3 shows the visualization results of log sequences trained by LogBERT with and without the VHM task. We can notice that the normal and anomalous log sequences are mixed together when we trained the model without the VHM task (shown in Figure 3a). On the contrary, as shown in Figure 3b, by incorporating the VHM task, the normal and anomalous log sequences are clearly separated in the latent space, and the normal log sequences group together. Therefore, the visualization presents that the VHM task is effective in regulating the model to split the normal and abnormal data in latent space.
Parameter analysis. We analyze the sensitivity of model performance by tuning various hyper-parameters. Figure 4a shows that the model performance is relatively stable by setting different $\alpha$ values in Equation 7. This is because, for the BGL dataset, the loss from the masked log key prediction dominates the final loss value due to the long log sequences. As a result, the weight for the VHM</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Fig. 4: Parameter analysis on the BGL dataset.
task does not have much influence on the performance. Figure 4b shows the performance with different ratios of masked log keys. Note that we use the same ratio in both training and detection phases. We can notice that increasing the ratios of masked log keys in the sequences from 0.1 to 0.5 can slightly increase the F1 scores while keeping increasing the ratios makes the performance worse. This is because while the masked log keys increase in a reasonable range, the model can capture more information about the sequence. However, if a sequence contains too many masked log keys, it loses too much information for making the predictions. Figure 4c shows that when increasing the size of the candidate set as normal log keys, the precision for anomaly detection keeps increasing while the recall is reducing, which meets our expectation. Hence, we need to find the appropriate size of the candidate set to balance the precision and recall for the anomaly detection.</p>
<h1>5 Conclusion</h1>
<p>Log anomaly detection is essential to protect online computer systems from malicious attacks or malfunctions. In this paper, we have developed LogBERT, a novel log anomaly detection model based on BERT. In order to train LogBERT only based on normal log sequences, we have proposed two self-supervised training tasks. One is to predict the masked log keys in log sequences, while the other is to make the normal log sequences close to each other in the embedding space. After training over normal log sequences, LogBERT is able to detect anomalous log sequences. Experimental results on three log datasets have shown that LogBERT outperforms the state-of-the-art approaches for log anomaly detection.</p>
<h2>References</h2>
<ol>
<li>Devlin, J., Chang, M.W., Lee, K., Toutanova, K.: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. arXiv:1810.04805 [cs] (2018)</li>
<li>
<p>Du, M., Li, F., Zheng, G., Srikumar, V.: DeepLog: Anomaly Detection and Diagnosis from System Logs through Deep Learning. In: Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security. (2017).</p>
</li>
<li>
<p>He, P., Zhu, J., Zheng, Z., Lyu, M.R.: Drain: An Online Log Parsing Approach with Fixed Depth Tree. In: 2017 IEEE International Conference on Web Services (ICWS). (2017).</p>
</li>
<li>He, S., Zhu, J., He, P., Lyu, M.R.: Experience Report: System Log Analysis for Anomaly Detection. In: 2016 IEEE 27th International Symposium on Software Reliability Engineering (ISSRE). (2016).</li>
<li>Li, K.L., Huang, H.K., Tian, S.F., Xu, W.: Improving one-class SVM for anomaly detection. In: Proceedings of the 2003 International Conference on Machine Learning and Cybernetics. (2003).</li>
<li>Lin, Q., Zhang, H., Lou, J., Zhang, Y., Chen, X.: Log Clustering Based Problem Identification for Online Service Systems. In: 2016 IEEE/ACM 38th International Conference on Software Engineering Companion (2016)</li>
<li>Liu, F.T., Ting, K.M., Zhou, Z.: Isolation Forest. In: 2008 Eighth IEEE International Conference on Data Mining. pp. 413-422 (2008).</li>
<li>Liu, F., Wen, Y., Zhang, D., Jiang, X., Xing, X., Meng, D.: Log2vec: A Heterogeneous Graph Embedding Based Approach for Detecting Cyber Threats within Enterprise. In: Proceedings of the 2019 ACM SIGSAC Conference on Computer and Communications Security. (2019).</li>
<li>Lou, J.G., Fu, Q., Yang, S., Xu, Y., Li, J.: Mining invariants from console logs for system problem detection. In: Proceedings of the 2010 USENIX Conference on USENIX Annual Technical Conference. (2010)</li>
<li>Oliner, A., Stearley, J.: What supercomputers say: A study of five system logs. In: 37th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN'07). pp. 575-584. IEEE (2007)</li>
<li>Pecchia, A., Cinque, M., Cotroneo, D.: Event logs for the analysis of software failures: A rule-based approach. IEEE Transactions on Software Engineering 39(06), $806-821(2013)$.</li>
<li>Roweis, S.T., Saul, L.K.: Nonlinear dimensionality reduction by locally linear embedding. science 290(5500), 2323-2326 (2000)</li>
<li>Ruff, L., Vandermeulen, R., Goernitz, N., Deecke, L., Siddiqui, S.A., Binder, A., Müller, E., Kloft, M.: Deep One-Class Classification. In: International Conference on Machine Learning. pp. 4393-4402. PMLR (2018)</li>
<li>Schölkopf, B., Platt, J.C., Shawe-Taylor, J., Smola, A.J., Williamson, R.C.: Estimating the Support of a High-Dimensional Distribution. Neural Computation 13(7), 1443-1471 (2001).</li>
<li>Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, L., Polosukhin, I.: Attention Is All You Need. arXiv:1706.03762 [cs] (2017)</li>
<li>Wang, Y., Wong, J., Miner, A.: Anomaly intrusion detection using one class SVM. In: Proceedings from the Fifth Annual IEEE SMC Information Assurance Workshop, 2004. pp. 358-364 (2004).</li>
<li>Wang, Z., Chen, Z., Ni, J., Liu, H., Chen, H., Tang, J.: Multi-Scale One-Class Recurrent Neural Networks for Discrete Event Sequence Anomaly Detection. In: WSDM (2021)</li>
<li>Xu, W., Huang, L., Fox, A., Patterson, D., Jordan, M.: Online system problem detection by mining patterns of console logs. In: 2009 Ninth IEEE International Conference on Data Mining. pp. 588-597. IEEE (2009)</li>
<li>
<p>Xu, W., Huang, L., Fox, A., Patterson, D., Jordan, M.I.: Detecting large-scale system problems by mining console logs. In: Proceedings of the ACM SIGOPS 22nd Symposium on Operating Systems Principles. (2009).</p>
</li>
<li>
<p>Yen, T.F., Oprea, A., Onarlioglu, K., Leetham, T., Robertson, W., Juels, A., Kirda, E.: Beehive: Large-scale log analysis for detecting suspicious activity in enterprise networks. In: Proceedings of the 29th Annual Computer Security Applications Conference. pp. 199-208 (2013)</p>
</li>
<li>Zhang, K., Xu, J., Min, M.R., Jiang, G., Pelechrinis, K., Zhang, H.: Automated IT system failure prediction: A deep learning approach. In: 2016 IEEE International Conference on Big Data (Big Data). pp. 1291-1300 (2016).</li>
<li>Zhang, X., Xu, Y., Lin, Q., Qiao, B., Zhang, H., Dang, Y., Xie, C., Yang, X., Cheng, Q., Li, Z., Chen, J., He, X., Yao, R., Lou, J.G., Chintalapati, M., Shen, F., Zhang, D.: Robust log-based anomaly detection on unstable log data. In: Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering. (2019).</li>
<li>Zhou, R., Sun, P., Tao, S., Zhang, R., Meng, W., Liu, Y., Zhu, Y., Liu, Y., Pei, D., Zhang, S., Chen, Y.: LogAnomaly: Unsupervised Detection of Sequential and Quantitative Anomalies in Unstructured Logs. In: IJCAI. pp. 4739-4745 (2019)</li>
<li>He, S., Zhu, J., He, P., Lyu, M.R.: Loghub: A Large Collection of System Log Datasets towards Automated Log Analytics. arXiv:2008.06448 [cs] (Aug 2020)</li>
</ol>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{3}$ https://github.com/donglee-afar/logdeep
${ }^{4}$ https://github.com/HelenGuohx/logbert&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5342 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5342</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5342</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-111.html">extraction-schema-111</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <p><strong>Paper ID:</strong> paper-678cfca72b7de72b005f71207bc8a522ea9ac62c</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/678cfca72b7de72b005f71207bc8a522ea9ac62c" target="_blank">Knowledge-Augmented Language Model Prompting for Zero-Shot Knowledge Graph Question Answering</a></p>
                <p><strong>Paper Venue:</strong> MATCHING</p>
                <p><strong>Paper TL;DR:</strong> The framework, Knowledge-Augmented language model PromptING (KAPING), requires no model training, thus completely zero-shot, and aims to answer the user’s question based on facts over a knowledge graph, on which the authors' outperforms relevant zero- shot baselines by up to 48% in average, across multiple LLMs of various sizes.</p>
                <p><strong>Paper Abstract:</strong> Large Language Models (LLMs) are capable of performing zero-shot closed-book question answering tasks, based on their internal knowledge stored in parameters during pre-training. However, such internalized knowledge might be insufficient and incorrect, which could lead LLMs to generate factually wrong answers. Furthermore, fine-tuning LLMs to update their knowledge is expensive. To this end, we propose to augment the knowledge directly in the input of LLMs. Specifically, we first retrieve the relevant facts to the input question from the knowledge graph based on semantic similarities between the question and its associated facts. After that, we prepend the retrieved facts to the input question in the form of the prompt, which is then forwarded to LLMs to generate the answer. Our framework, Knowledge-Augmented language model PromptING (KAPING), requires no model training, thus completely zero-shot. We validate the performance of our KAPING framework on the knowledge graph question answering task, that aims to answer the user’s question based on facts over a knowledge graph, on which ours outperforms relevant zero-shot baselines by up to 48% in average, across multiple LLMs of various sizes.</p>
                <p><strong>Cost:</strong> 0.013</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5342.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5342.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Triple-form linearization</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Linear triple verbalization (subject-relation-object concatenation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A simple graph-to-text representation that converts each KG triple (s, r, o) into a linear textual string by concatenating subject, relation, and object (e.g., "(Lady Susan, written by, Jane Austen)"). Used as the default verbalizer in the paper for retrieval and for prepending facts to LM prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>triple-form linearization</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Each knowledge graph triple (s, r, o) is transformed to text by straightforward concatenation/serialization into a triple string, e.g. "(subject, relation, object)". No learned paraphrasing or template generation is applied; triples are fed as-is (line-by-line) into prompts, optionally preceded by an instruction header (e.g., "Below are facts in the form of the triple meaningful to answer the question.").</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>knowledge graph</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Compact (very short per triple), faithful to the original symbolic triple (low semantic drift), highly interpretable, easy to generate (no learning), robust for dense embedding retrieval (empirically better retrieval metrics than a learned free-form verbalizer); potential drawback: terseness may be less natural/language-like for some LMs (but empirically works well).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>1) Retrieval of question-relevant facts (1-hop retrieval MRR/Top-k). 2) Knowledge-augmented LM prompting for zero-shot Knowledge Graph Question Answering (KGQA) where verbalized triples are prepended to the question prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Retriever metrics (on WebQSP w/ Wikidata, MPNet): MRR=43.46, Top-1=33.36, Top-10=64.39, Top-30=70.67 (Table 11). KGQA generation accuracy (KAPING with triple-form verbalization) on WebQSP w/ Wikidata: T5 (3B) 40.38, OPT (6.7B) 53.34, T0 (3B) 49.86, T0 (11B) 58.73 (Table 12). Additional downstream aggregated metrics (Table 1) show large accuracy improvements of KAPING over baselines (e.g., average accuracy up to ~50.69 on WebQSP w/ Wikidata across LMs). Evaluation also used accuracy, F1, Exact Match for generation (where reported).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>Compared directly to a learned free-form graph-to-text verbalizer (Ma et al., 2022) in retrieval and KGQA experiments. Triple-form verbalization achieved better retrieval results (MPNet: MRR 43.46 vs 41.33; Top-1 33.36 vs 31.11; Top-10 64.39 vs 62.07; Top-30 70.67 vs 69.92; Table 11). For KGQA generation (Table 12) results were mixed by LM: free-form helped T5 (3B) somewhat (43.25 vs triple 40.38), while triple-form was better for instruction-tuned models (T0 variants) and comparable for OPT; overall triple-form is at least as effective and simpler, with stronger retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Depends strongly on the retriever: when retriever returns irrelevant triples, LM performance degrades (paper discusses sensitivity to incorrect retrieval). Triple-form is simplistic and may not be optimal for all LMs (e.g., some base LMs like T5 sometimes benefited from free-form verbalization). Multi-hop (2-hop+) retrieval remains challenging; triple-form alone doesn't solve multi-hop composition. Also, generation evaluation with sentence outputs versus gold KG entity labels complicates metric interpretation (EM/F1 penalize longer sentence outputs).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Knowledge-Augmented Language Model Prompting for Zero-Shot Knowledge Graph Question Answering', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5342.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5342.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Free-form graph-to-text verbalizer</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Learned free-form graph-to-text verbalization (graph->text model from Ma et al., 2022)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A learned graph-to-text transformation that converts KG triples into free-form natural language sentences using a trained model (Ma et al., 2022); evaluated in this paper as an alternative verbalizer for retrieval and KGQA prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Open domain question answering with A unified knowledge interface</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>free-form learned verbalization (graph->text)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>A trained graph-to-text model (as in Ma et al., 2022) generates a natural-language sentence from triples (or small subgraphs) instead of a literal triple string. The produced free-form text is intended to be more natural and fluent than triple serialization and is used both as retriever corpus items (to compute embeddings) and as input facts prepended to LM prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>knowledge graph</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>More natural / human-like / fluent outputs (higher language quality) but risks semantic drift (the paraphrasing can change or drop information), which can harm embedding-based retrieval; potentially more LM-friendly for some base models that expect natural sentences. Requires a trained model (non-trivial to obtain) and may introduce errors from generation.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Same as triple-form: retrieval of question-relevant facts (1-hop retrieval MRR/Top-k) and KGQA generation (KAPING framework using free-form verbalized facts prepended to prompts).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Retriever metrics (MPNet, WebQSP w/ Wikidata): MRR=41.33, Top-1=31.11, Top-10=62.07, Top-30=69.92 (Table 11). KGQA generation accuracy (KAPING with free-form verbalization) examples: T5 (3B) 43.25 (better than its triple-form 40.38), OPT (6.7B) 53.00 (vs triple 53.34), T0 (3B) 47.75 (vs triple 49.86), T0 (11B) 53.21 (vs triple 58.73) (Table 12).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>Directly compared to the simple triple-form serialization. Free-form verbalizer gave worse retrieval metrics (Table 11) — interpreted as generated paraphrases sometimes changing semantics and reducing retrievability — but KGQA generation effects were mixed: free-form improved performance for some base LMs (e.g., T5) but underperformed for instruction-finetuned models (T0). Overall, triple-form had better retrieval; generation performance depends on LM type.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Can introduce semantic drift (verbalizer may paraphrase or alter triples), degrading dense retrieval quality. Requires training data and a graph-to-text model (added complexity). In practice, free-form can help some LMs but hurt others; choice of verbalizer should consider both retriever behavior and the target LM. The paper notes free-form verbalization can generate outputs semantically different from original triples, leading to degraded retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Knowledge-Augmented Language Model Prompting for Zero-Shot Knowledge Graph Question Answering', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Open domain question answering with A unified knowledge interface <em>(Rating: 2)</em></li>
                <li>Unik-qa: Unified representations of structured and unstructured knowledge for open-domain question answering <em>(Rating: 1)</em></li>
                <li>Graph-to-text transformation model proposed in Ma et al. (2022) <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5342",
    "paper_id": "paper-678cfca72b7de72b005f71207bc8a522ea9ac62c",
    "extraction_schema_id": "extraction-schema-111",
    "extracted_data": [
        {
            "name_short": "Triple-form linearization",
            "name_full": "Linear triple verbalization (subject-relation-object concatenation)",
            "brief_description": "A simple graph-to-text representation that converts each KG triple (s, r, o) into a linear textual string by concatenating subject, relation, and object (e.g., \"(Lady Susan, written by, Jane Austen)\"). Used as the default verbalizer in the paper for retrieval and for prepending facts to LM prompts.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "triple-form linearization",
            "representation_description": "Each knowledge graph triple (s, r, o) is transformed to text by straightforward concatenation/serialization into a triple string, e.g. \"(subject, relation, object)\". No learned paraphrasing or template generation is applied; triples are fed as-is (line-by-line) into prompts, optionally preceded by an instruction header (e.g., \"Below are facts in the form of the triple meaningful to answer the question.\").",
            "graph_type": "knowledge graph",
            "representation_properties": "Compact (very short per triple), faithful to the original symbolic triple (low semantic drift), highly interpretable, easy to generate (no learning), robust for dense embedding retrieval (empirically better retrieval metrics than a learned free-form verbalizer); potential drawback: terseness may be less natural/language-like for some LMs (but empirically works well).",
            "evaluation_task": "1) Retrieval of question-relevant facts (1-hop retrieval MRR/Top-k). 2) Knowledge-augmented LM prompting for zero-shot Knowledge Graph Question Answering (KGQA) where verbalized triples are prepended to the question prompt.",
            "performance_metrics": "Retriever metrics (on WebQSP w/ Wikidata, MPNet): MRR=43.46, Top-1=33.36, Top-10=64.39, Top-30=70.67 (Table 11). KGQA generation accuracy (KAPING with triple-form verbalization) on WebQSP w/ Wikidata: T5 (3B) 40.38, OPT (6.7B) 53.34, T0 (3B) 49.86, T0 (11B) 58.73 (Table 12). Additional downstream aggregated metrics (Table 1) show large accuracy improvements of KAPING over baselines (e.g., average accuracy up to ~50.69 on WebQSP w/ Wikidata across LMs). Evaluation also used accuracy, F1, Exact Match for generation (where reported).",
            "comparison_to_other_representations": "Compared directly to a learned free-form graph-to-text verbalizer (Ma et al., 2022) in retrieval and KGQA experiments. Triple-form verbalization achieved better retrieval results (MPNet: MRR 43.46 vs 41.33; Top-1 33.36 vs 31.11; Top-10 64.39 vs 62.07; Top-30 70.67 vs 69.92; Table 11). For KGQA generation (Table 12) results were mixed by LM: free-form helped T5 (3B) somewhat (43.25 vs triple 40.38), while triple-form was better for instruction-tuned models (T0 variants) and comparable for OPT; overall triple-form is at least as effective and simpler, with stronger retrieval.",
            "limitations_or_challenges": "Depends strongly on the retriever: when retriever returns irrelevant triples, LM performance degrades (paper discusses sensitivity to incorrect retrieval). Triple-form is simplistic and may not be optimal for all LMs (e.g., some base LMs like T5 sometimes benefited from free-form verbalization). Multi-hop (2-hop+) retrieval remains challenging; triple-form alone doesn't solve multi-hop composition. Also, generation evaluation with sentence outputs versus gold KG entity labels complicates metric interpretation (EM/F1 penalize longer sentence outputs).",
            "uuid": "e5342.0",
            "source_info": {
                "paper_title": "Knowledge-Augmented Language Model Prompting for Zero-Shot Knowledge Graph Question Answering",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "Free-form graph-to-text verbalizer",
            "name_full": "Learned free-form graph-to-text verbalization (graph-&gt;text model from Ma et al., 2022)",
            "brief_description": "A learned graph-to-text transformation that converts KG triples into free-form natural language sentences using a trained model (Ma et al., 2022); evaluated in this paper as an alternative verbalizer for retrieval and KGQA prompting.",
            "citation_title": "Open domain question answering with A unified knowledge interface",
            "mention_or_use": "use",
            "representation_name": "free-form learned verbalization (graph-&gt;text)",
            "representation_description": "A trained graph-to-text model (as in Ma et al., 2022) generates a natural-language sentence from triples (or small subgraphs) instead of a literal triple string. The produced free-form text is intended to be more natural and fluent than triple serialization and is used both as retriever corpus items (to compute embeddings) and as input facts prepended to LM prompts.",
            "graph_type": "knowledge graph",
            "representation_properties": "More natural / human-like / fluent outputs (higher language quality) but risks semantic drift (the paraphrasing can change or drop information), which can harm embedding-based retrieval; potentially more LM-friendly for some base models that expect natural sentences. Requires a trained model (non-trivial to obtain) and may introduce errors from generation.",
            "evaluation_task": "Same as triple-form: retrieval of question-relevant facts (1-hop retrieval MRR/Top-k) and KGQA generation (KAPING framework using free-form verbalized facts prepended to prompts).",
            "performance_metrics": "Retriever metrics (MPNet, WebQSP w/ Wikidata): MRR=41.33, Top-1=31.11, Top-10=62.07, Top-30=69.92 (Table 11). KGQA generation accuracy (KAPING with free-form verbalization) examples: T5 (3B) 43.25 (better than its triple-form 40.38), OPT (6.7B) 53.00 (vs triple 53.34), T0 (3B) 47.75 (vs triple 49.86), T0 (11B) 53.21 (vs triple 58.73) (Table 12).",
            "comparison_to_other_representations": "Directly compared to the simple triple-form serialization. Free-form verbalizer gave worse retrieval metrics (Table 11) — interpreted as generated paraphrases sometimes changing semantics and reducing retrievability — but KGQA generation effects were mixed: free-form improved performance for some base LMs (e.g., T5) but underperformed for instruction-finetuned models (T0). Overall, triple-form had better retrieval; generation performance depends on LM type.",
            "limitations_or_challenges": "Can introduce semantic drift (verbalizer may paraphrase or alter triples), degrading dense retrieval quality. Requires training data and a graph-to-text model (added complexity). In practice, free-form can help some LMs but hurt others; choice of verbalizer should consider both retriever behavior and the target LM. The paper notes free-form verbalization can generate outputs semantically different from original triples, leading to degraded retrieval.",
            "uuid": "e5342.1",
            "source_info": {
                "paper_title": "Knowledge-Augmented Language Model Prompting for Zero-Shot Knowledge Graph Question Answering",
                "publication_date_yy_mm": "2023-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Open domain question answering with A unified knowledge interface",
            "rating": 2
        },
        {
            "paper_title": "Unik-qa: Unified representations of structured and unstructured knowledge for open-domain question answering",
            "rating": 1
        },
        {
            "paper_title": "Graph-to-text transformation model proposed in Ma et al. (2022)",
            "rating": 1
        }
    ],
    "cost": 0.01327775,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Knowledge-Augmented Language Model Prompting for Zero-Shot Knowledge Graph Question Answering</h1>
<p>Jinheon Baek ${ }^{1 *}$ Alham Fikri Aji ${ }^{2}$ Amir Saffari ${ }^{3}$<br>KAIST $^{1}$ MBZUAI ${ }^{2}$ Amazon $^{3}$<br>jinheon.baek@kaist.ac.kr alham.fikri@mbzuai.ac.ae amsafari@amazon.com</p>
<h4>Abstract</h4>
<p>Large Language Models (LLMs) are capable of performing zero-shot closed-book question answering tasks, based on their internal knowledge stored in parameters during pre-training. However, such internalized knowledge might be insufficient and incorrect, which could lead LLMs to generate factually wrong answers. Furthermore, fine-tuning LLMs to update their knowledge is expensive. To this end, we propose to augment the knowledge directly in the input of LLMs. Specifically, we first retrieve the relevant facts to the input question from the knowledge graph based on semantic similarities between the question and its associated facts. After that, we prepend the retrieved facts to the input question in the form of the prompt, which is then forwarded to LLMs to generate the answer. Our framework, KnowledgeAugmented language model PromptING (KAPING), requires no model training, thus completely zero-shot. We validate the performance of our KAPING framework on the knowledge graph question answering task, that aims to answer the user's question based on facts over a knowledge graph, on which ours outperforms relevant zero-shot baselines by up to $48 \%$ in average, across multiple LLMs of various sizes.</p>
<h2>1 Introduction</h2>
<p>Pre-trained Language Models (LMs) (Devlin et al., 2019; Raffel et al., 2020), which are trained on a large amount of text corpora with self-supervised learning, can perform closed-book Question Answering (QA) tasks that aim to answer the user's question based only on their internal knowledge in parameters, without using any external knowledge (Petroni et al., 2019; Roberts et al., 2020). Also, when we increase the LM sizes, Large Language Models (LLMs) can generate the answer for the question without any additional fine-tuning</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: (a) For the input question in the prompt, the large language model, GPT-3 (Brown et al., 2020), can generate the answer based on its internal knowledge in parameters, but hallucinates it which is highlighted in yellow. (b) Our Knowledge-Augmented language model PrompTING (KAPING) framework first retrieves the relevant facts in the knowledge graph from the entities in the question, and then augments them to the prompt, to generate the factually correct answer.
steps, called $L M$ prompting (Brown et al., 2020; Liu et al., 2021). However, since the knowledge in LLMs might be incomplete, incorrect, and outdated, they often generate factually wrong answers, known as hallucination (Rohrbach et al., 2018) (See Figure 1a). Also, refining the knowledge in LLMs with parameter updates is costly, especially when knowledge is constantly changing (e.g., exchange rates of money). Lastly, whether LLMs are fetching the correct knowledge for QA is unclear.</p>
<p>To overcome those limitations, we propose to retrieve and inject the relevant knowledge directly as an input, called a prompt, to LLMs (Figure 1b). As a knowledge source, we use a Knowledge Graph (KG) consisting of symbolic knowledge in the form of a triple: (head entity, relation, tail entity). Therefore, to extract the relevant facts to the input question, we first match entities in the question with entities in the KG. After that, triples associated to</p>
<p>entities in the KG are verbalized (i.e., transforming the symbolic relational knowledge to the textual string) and prepended to the input question, which are then forwarded to LLMs to generate the answer. Consequently, LLMs conditioned on the factual knowledge are able to generate the factual answers, alleviating the hallucination issue, while keeping LLMs' parameters unchanged: fine-tuning is not required for knowledge updates. We refer to our overall framework as Knowledge-Augmented language model PromptING (KAPING), which is completely zero-shot and can be done with any off-the-shelf LLMs, without additional training.</p>
<p>While the above scheme looks simple yet effective, there is a couple of challenges. First, most retrieved triples associated with the question entities are unrelated to answer the given question. For example, when we retrieve the associated triples for the question entity (e.g., Poseidon) in Figure 1 in the Wikidata KG (Vrandecic and Krötzsch, 2014), there exist 60 triples, and most of them (e.g., genre, publication date, to name a few) are irrelevant to answer the question. Therefore, they might mislead the model into generating incorrect answers. On the other hand, the number of triples for the question entities is occasionally large (e.g., $27 \%$ samples for the WebQSP dataset (Yih et al., 2016) have more than 1,000 triples), thereby encoding all triples including unnecessary ones yields high computational costs, especially on LLMs.</p>
<p>To overcome such challenges, we further propose to filter out unnecessary triples based on their semantic similarities to the input question, inspired by the information retrieval (Bast et al., 2016). To be specific, we first represent the question and its associated verbalized triples in the embedding space. Then, we retrieve the small number of triples whose embeddings are more close to the input question's embedding than others. By doing so, we can prepend only the more relevant triples to the given question, which can effectively prevent LLMs from generating irrelevant answers with high computational efficiencies, unlike the one that augments all triples. Note that, our filtering approach uses off-the-shelf sentence embedding models (Song et al., 2020; Hofstätter et al., 2021); thus no additional training is required in every part of our pipeline.</p>
<p>We then validate our KAPING framework on Knowledge Graph Question Answering (KGQA) tasks. The results show that our KAPING significantly outperforms relevant zero-shot baselines.</p>
<p>Also, the detailed analyses support the importance of knowledge retrieval and augmentation schemes.</p>
<p>Our contributions in this work are threefold:</p>
<ul>
<li>We present a new knowledge-augmented LM prompting framework that leverages the factual knowledge from KGs, for zero-shot QA.</li>
<li>We propose to retrieve and augment relevant facts from KGs, based on semantic similarities between the question and its associated triples.</li>
<li>We validate our KAPING on KGQA benchmark datasets, on which ours impressively outperforms relevant zero-shot baselines.</li>
</ul>
<h2>2 Related Work</h2>
<p>Language Model Prompting Language model pre-training, which trains Transformers (Vaswani et al., 2017) on unannotated text corpora with autoencoding (Devlin et al., 2019; Liu et al., 2019) or auto-regressive (Yang et al., 2019; Radford et al., 2018) objectives, becomes an essential approach for natural language tasks. Also, Large Language Models (LLMs) (Brown et al., 2020; Raffel et al., 2020; Chowdhery et al., 2022; Soltan et al., 2022) are able to perform zero-shot learning, for example, generating the answer for the input textual prompt, based on the knowledge stored in pre-trained parameters (Petroni et al., 2019; Roberts et al., 2020; Sung et al., 2021), without additional parameter updates as well as labeled datasets. To further improve their performances, some work (Rubin et al., 2022; Liu et al., 2022a) proposes retrieving relevant samples to the input question from the training dataset and prepending them in the prompt under few-show learning. Recent few work (Sanh et al., 2022; Wei et al., 2022a) further shows that, when LLMs are fine-tuned on a collection of instructions phrased from natural language tasks, they can have strong generalization performance on unseen zero-shot tasks. However, the knowledge inside LMs might be insufficient to tackle factual questions, which gives rise to knowledge-augmented LMs. Notably, our LM prompting is different from prompt-tuning literature (Lester et al., 2021a; Chen et al., 2022a) that additionally tunes LMs with model training (See Appendix C for discussions).</p>
<p>Knowledge-Augmented LMs Recent work proposes to integrate the knowledge, such as documents from unstructured corpora (e.g., Wikipedia) and facts from Knowledge Graphs (KGs), into LMs. To mention a few, REALM (Guu et al., 2020) and</p>
<p>RAG (Lewis et al., 2020) learn to retrieve documents and augment LMs with them. In addition, KGs could be another knowledge source, where the knowledge is succinctly encoded in the most compact form, and some methods augment such facts in KGs into LMs (Galetzka et al., 2021; Rony et al., 2022; Kang et al., 2022). However, all aforementioned approaches require massive amount of training data and model updates for downstream tasks. While more recent work (Izacard et al., 2022) shows retrieval-augmented LM can have strong performance with few-shot learning, it still requires extra training steps, which is different from ours focusing on $L M$ prompting for entirely zero-shot.</p>
<p>Recently, there are few studies augmenting the knowledge in the LM prompting scheme. At first, some work proposes to extract the knowledge in the parameters of LLMs themselves via prompting, and then use the extracted knowledge to answer the question (Kojima et al., 2022; Liu et al., 2022b; Wei et al., 2022b; Wang et al., 2022). However, since LLMs' parameters might be insufficient to store all the world knowledge, the extracted knowledge and generated answers might be inaccurate. On the other hand, most recently, Lazaridou et al. (2022) propose to use the Google Search to retrieve documents on the Web, and then prepend the retrieved documents to the input question along with few-shot demonstrations, to answer the question under few-shot LLM prompting schemes. However, our focus on zero-shot prompting with KGs is orthogonal to the previous study working on documents with few-shot prompting, and leveraging KGs can bring additional advantages. Specifically, since KGs can succinctly encode the knowledge in the compact triple form, for QA tasks, ours makes LLM prompting more efficient (i.e., reducing the input sequence length compared to the document case), as well as more effective on the zero-shot QA scheme: LLMs need to select one triple containing the answer entity in the prompt, instead of looking through lengthy documents having various entities.</p>
<p>Knowledge Graph Question Answering The goal of our target Knowledge Graph Question Answering (KGQA) tasks is to answer the input question based on a set of facts over KGs (Chakraborty et al., 2019; Fu et al., 2020). Previous approaches are broadly classified into neural semantic parsingbased methods (Yih et al., 2015; Bao et al., 2016; Luo et al., 2018), information retrieval-based methods (Sun et al., 2018; Saxena et al., 2020; Yasunaga
et al., 2021), and differentiable KG-based methods (Cohen et al., 2020; Saffari et al., 2021; Sen et al., 2021), which, however, require annotated data with additional model training. While Zhou et al. (2021) aim to transfer the KGQA model to the target language domains without any training data on them, this work indeed needs the labeled data to train the model on data-rich source domains first before transferring the model to the target domains. In contrast to all the aforementioned methods, we explore the novel zero-shot KGQA mechanism, which does not require any annotated QA pairs and additional training, leveraging LM prompting.</p>
<h2>3 Method</h2>
<p>We now describe our Knowledge-Augmented language model PromptING (KAPING) framework.</p>
<h3>3.1 LM Prompting for Zero-Shot QA</h3>
<p>We begin with the zero-shot question answering, and then explain the language model prompting.
Zero-Shot Question Answering Given an input question $\boldsymbol{x}$, the Question Answering (QA) system returns an answer $\boldsymbol{y}$, where $\boldsymbol{x}$ and $\boldsymbol{y}$ consist of sequences of tokens: $\boldsymbol{x}=\left[w_{1}, w_{2}, \ldots, w_{|\boldsymbol{x}|}\right]$. Let $P$ be a QA model based on the generative Language Model (LM) (Raffel et al., 2020; Brown et al., 2020), which generates the conditional probability of answer $\boldsymbol{y}$ for question $\boldsymbol{x}$ as follows: $P(\boldsymbol{y} \mid \boldsymbol{x})$. Then, in contrast to supervised learning that trains model $P$ with a set of annotated $(\boldsymbol{x}, \boldsymbol{y})$ pairs, zeroshot learning does not use any labeled samples and model training. Notably, we are interested in this zero-shot QA, since collecting the dataset and then fine-tuning the existing LMs for every new domain are known to be expensive and sometimes infeasible (Houlsby et al., 2019; Lester et al., 2021b).
LM Prompting LMs are often pre-trained by predicting the next token based on previous tokens, which is known as auto-regressive language modeling (Radford et al., 2018; Raffel et al., 2020). Then, thanks to this pre-training objective, LLMs can perform zero-shot instruction learning. Specifically, when we provide a question as well as an instruction (e.g., "Please answer the question: Who is the author of Lady Susan?") to the LLM (i.e., $P$ ), such the LLM, conditioned by the input text, can sequentially generate the probability of output tokens, which might be an answer, "Jane Austen".</p>
<p>To be more formal, for every input question $\boldsymbol{x}$, we first modify it with a particular instruction tem-</p>
<p>plate $T$ into a textual string $\boldsymbol{x}^{\prime}$ called a prompt, as follows: $T: \boldsymbol{x} \mapsto \boldsymbol{x}^{\prime}$. For example, if we have the previous question $\boldsymbol{x}=$ "Who is the author of Lady Susan?" along with the previous instruction template "Please answer the question:", the resulting prompt $\boldsymbol{x}^{\prime}$ would be $T(\boldsymbol{x})=$ "Please answer the question: Who is the author of Lady Susan?". Then, we forward the prompt $\boldsymbol{x}^{\prime}$ to the LLM (i.e., $P$ ), which then generates the answer (i.e., $\boldsymbol{y}$ ) through $P\left(\boldsymbol{y} \mid \boldsymbol{x}^{\prime}\right)$. Note that this LM prompting scheme does not require any additional model parameter updates (i.e., fine-tuning) on the labeled data, thus appropriate for the target zero-shot QA task.</p>
<p>However, there are multiple challenges in this naive zero-shot prompting for QA. First, LLMs, which rely on the knowledge in parameters, are vulnerable from generating the factually incorrect answer, since the knowledge in LLMs might be inaccurate, and outdated: knowledge can be emerged and changed over time. Also, refining the internalized knowledge with additional parameter updates is expensive, while it is necessary to reflect the wrong and ever growing knowledge. Lastly, which knowledge LLMs memorize and utilize when generating the answer to the question prompt is unclear, which limits their explainability on the outputs.</p>
<h3>3.2 Knowledge-Augmented LM Prompting</h3>
<p>In order to tackle the aforementioned limitations of the existing LM prompting scheme, we propose to inject the relevant knowledge to the input question from the Knowledge Graph (KG), which we refer to as Knowledge-Augmented language model PromptING (KAPING). In this subsection, we first define the main objective of our KAPING framework, and then introduce the ingredients for augmenting the knowledge over KGs to LM prompts.</p>
<p>LM Prompting with Knowledge Graphs Instead of relying on the knowledge internalized in parameters, we propose to additionally access and inject the knowledge from the external KG, which contains accurate and up-to-date facts helpful to answer the question. Formally, a knowledge graph $\mathcal{G}$ consists of a set of factual triples ${(s, r, o)}$, where $s$ and $o$ denote subject and object entities, and $r$ is a specific type of a relation between them. For example, one relational knowledge "Lady Susan was written by Jane Austen" can be represented as a triple consisting of two entities $s=$ "Lady Susan" and $o=$ "Jane Austen" along with a relation $r=$ "written by". Then, for the question prompt $\boldsymbol{x}^{\prime}$
transformed from the example question $\boldsymbol{x}=$ "Who is the author of Lady Susan?" via the template $T$, we additionally augment its relevant triple: (Lady Susan, written by, Jane Austen), to the LM prompting scheme. By doing so, LLMs can generate the correct answer with regard to the augmented knowledge from KGs, formalized as follows: $P\left(\boldsymbol{y} \mid \boldsymbol{x}^{\prime}, \mathcal{G}\right)$. Note that, since we can provide specific and valid facts in KGs to LLMs whenever they exist, our framework can alleviate hallucination issue, originated from inaccurate and outdated knowledge in LLMs, without costly updating their model parameters. Furthermore, we can confirm whether LLMs generate answers based on augmented facts, thus improving the explainability of LM prompting.</p>
<p>The remaining questions are then how to access the relational symbolic facts over the KG from the input question, verbalize the symbolic knowledge to the textual string, and inject the verbalized knowledge into the LM prompting scheme. We explain them one by one in the following paragraphs.</p>
<p>Knowledge Access In order to utilize the related facts to the input question, we first extract the entities in the question. For example, for the question "Who is the author of Lady Susan?", we extract the entity "Lady Susan". Then, based on the extracted entity, we find its corresponding entity over the KG, whose incident triples then become associated facts to the input question. Note that entity matching can be done by existing entity linking techniques (Wu et al., 2020; Li et al., 2020; Ayoola et al., 2022).</p>
<p>Knowledge Verbalization LLMs are working on textual inputs, whereas factual triples are represented over the symbolic graph. Therefore, before injecting the symbolic fact from KGs to LLMs, we first transform the triple consisting of $(s, r, o)$ into its textual string, called verbalization. While there exists recent methods (Oguz et al., 2022; Ma et al., 2022) that particularly design or even learn the graph-to-text transformation, in this work, we use the linear verbalization: concatenating the subject, relation, and object texts in the triple, which we observe works well in LM prompting (See Appendix B.5). For instance, one triple (Lady Susan, written by, Jane Austen) is used as is: "(Lady Susan, written by, Jane Austen)", for an LLM's input.</p>
<p>Knowledge Injection Based on verbalized facts associated with the input question, the remaining step is to realize the knowledge injection mechanism, which allows LLMs to be grounded on the</p>
<p>external knowledge, useful to generate the answer. Let assume we have a set of $N$ associated triples $\boldsymbol{k}=\left{\left(s_{i}, r_{i}, o_{i}\right)\right}_{i=1}^{N}$ for question $\boldsymbol{x}$. Then, similar to instruction template $T: \boldsymbol{x} \mapsto \boldsymbol{x}^{\prime}$ described in Section 3.1, we modify $N$ verbalized triples $\boldsymbol{k}$ along with the instruction for the knowledge injection into the knowledge prompt $\boldsymbol{k}^{\prime}$, as follows: $T: \boldsymbol{k} \mapsto \boldsymbol{k}^{\prime}$. One particular template we use for constructing the prompt is that, we first enumerate $N$ verbalized triples line-by-line and then add the specific instruction: "Below are facts in the form of the triple meaningful to answer the question.", at the top of the prompt. After that, such the knowledge prompt string, $\boldsymbol{k}^{\prime}$, is prepended to the question prompt $\boldsymbol{x}^{\prime}$, and LLMs conditioned by knowledge and question prompts then sequentially generate the answer tokens, formalized as follows: $P\left(\boldsymbol{y} \mid\left[\boldsymbol{k}^{\prime}, \boldsymbol{x}^{\prime}\right]\right)$, where $[\cdot]$ denotes concatenation.</p>
<h3>3.3 Question-Relevant Knowledge Retrieval</h3>
<p>The proposed KAPING framework in Section 3.2, allows LLMs to leverage the knowledge from KGs for zero-shot QA. However, there are critical challenges that the number of triples associated to questions is often too large to forward in LLMs. Also, most of them are unrelated to the question, misleading LLMs into generating the irrelevant answer.</p>
<p>Knowledge Retriever To overcome those limitations, we further propose to retrieve and augment only the relevant triples to the question. Note that there exists a document-retrieval scheme (Lin et al., 2021), whose goal is to retrieve relevant documents for the given query based on their embedding similarities, which motivates us to retrieve, in our case, the triples for the user's question. In particular, thanks to the verbalizer defined in Section 3.2, we can play with triples, obtained from symbolic KGs, over the text space. Therefore, for the verbalized triple and the question, we first embed them onto the representation space with off-the-shelf sentence embedding models for text retrieval (Song et al., 2020; Karpukhin et al., 2020; Xiong et al., 2021), and then calculate their similarities. After that, we use only the top- $K$ similar triples, instead of using all $N$ triples, associated to the given question. Note that, unlike few recent studies (Oguz et al., 2022; Ma et al., 2022; Kang et al., 2022) that aim at improving KG retrievers themselves under supervised training, we focus on zero-shot LM prompting with KGs, thus we use any off-the-shelf retrievers as a tool to filter out unnecessary triples for questions.</p>
<h2>4 Experimental Setups</h2>
<p>We explain datasets, models, metrics, and implementations. For additional details, see Appendix A.</p>
<h3>4.1 Datasets</h3>
<p>We evaluate our Knowledge-Augmented language model PromptING (KAPING) framework on two Knowledge Graph Question Answering (KGQA) datasets, namely WebQuestionsSP and Mintaka.</p>
<p>WebQuestionsSP (WebQSP) This dataset (Berant et al., 2013; Yih et al., 2016) is designed with a Freebase KG (Bollacker et al., 2008). It consists of 1,639 test samples, which we use for zero-shot evaluation. Additionally, since Freebase is outdated, we further use the Wikidata KG (Vrandecic and Krötzsch, 2014) by using available mappings from Freebase ids to Wikidata (Diefenbach et al., 2017). This additional dataset consists of 1,466 samples.</p>
<p>Mintaka This dataset (Sen et al., 2022) is recently designed with the Wikidata KG for complex KGQA tasks. Among 8 different languages, we use English test sets consisting of 4,000 samples.</p>
<h3>4.2 Large Language Models</h3>
<p>To verify the performance of our KAPING framework on Large Language Models (LLMs), as well as benchmarking them on zero-shot KGQA, we use various LLMs with different sizes. Specifically, we use T5 (Raffel et al., 2020) (0.8B, 3B, 11B), T0 (Sanh et al., 2022) (3B, 11B), OPT (Zhang et al., 2022) (2.7B, 6.7B) and GPT-3 (Brown et al., 2020) $(6.7 \mathrm{~B}, 175 \mathrm{~B})$. We provide details in Appendix A.2.</p>
<h3>4.3 Baselines and Our Model</h3>
<p>In this subsection, we explain four zero-shot LM prompting baselines and our KAPING framework.
No Knowledge This is a naive LM prompting baseline, which generates answers from input questions without knowledge augmentation from KGs.
Random Knowledge This is an LM prompting baseline, which additionally augments the randomly sampled $K$ triples, associated to the entities appeared in the question, to the prompt.
Popular Knowledge This is an LM prompting baseline, which augments $K$ popular triples among all triples from the question entities, based on relations that appear the most frequently in the KG.
Generated Knowledge This is an LM prompting baseline, which first extracts the knowledge from LLMs themselves based on prompting, and then</p>
<p>Table 1: Main results of language model prompting, where we report the generation accuracy. The number inside the parentheses in the first row denotes the parameter size of language models, and best scores are emphasized in bold.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Datasets</th>
<th style="text-align: center;">Methods</th>
<th style="text-align: center;">T5 (6.08)</th>
<th style="text-align: center;">T5 (10)</th>
<th style="text-align: center;">T5 (118)</th>
<th style="text-align: center;">OPT (2/78)</th>
<th style="text-align: center;">OPT (6/78)</th>
<th style="text-align: center;">OPT (118)</th>
<th style="text-align: center;">T0 (10)</th>
<th style="text-align: center;">T0 (118)</th>
<th style="text-align: center;">GPT-3 (6/78)</th>
<th style="text-align: center;">GPT-3 (1/78)</th>
<th style="text-align: center;">AlexaTM (208)</th>
<th style="text-align: center;">Average</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">WebQSP w/ Freebase</td>
<td style="text-align: center;">No Knowledge</td>
<td style="text-align: center;">6.95</td>
<td style="text-align: center;">13.40</td>
<td style="text-align: center;">9.48</td>
<td style="text-align: center;">19.85</td>
<td style="text-align: center;">29.37</td>
<td style="text-align: center;">28.58</td>
<td style="text-align: center;">21.41</td>
<td style="text-align: center;">40.77</td>
<td style="text-align: center;">44.03</td>
<td style="text-align: center;">63.59</td>
<td style="text-align: center;">46.79</td>
<td style="text-align: center;">29.55</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Random Knowledge</td>
<td style="text-align: center;">21.55</td>
<td style="text-align: center;">19.15</td>
<td style="text-align: center;">17.57</td>
<td style="text-align: center;">28.07</td>
<td style="text-align: center;">31.73</td>
<td style="text-align: center;">33.31</td>
<td style="text-align: center;">32.62</td>
<td style="text-align: center;">51.20</td>
<td style="text-align: center;">51.01</td>
<td style="text-align: center;">65.87</td>
<td style="text-align: center;">57.37</td>
<td style="text-align: center;">37.22</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Popular Knowledge</td>
<td style="text-align: center;">15.50</td>
<td style="text-align: center;">16.88</td>
<td style="text-align: center;">18.39</td>
<td style="text-align: center;">28.32</td>
<td style="text-align: center;">28.13</td>
<td style="text-align: center;">24.21</td>
<td style="text-align: center;">27.05</td>
<td style="text-align: center;">47.22</td>
<td style="text-align: center;">45.58</td>
<td style="text-align: center;">62.26</td>
<td style="text-align: center;">54.91</td>
<td style="text-align: center;">33.48</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Generated Knowledge</td>
<td style="text-align: center;">6.19</td>
<td style="text-align: center;">7.84</td>
<td style="text-align: center;">6.76</td>
<td style="text-align: center;">7.48</td>
<td style="text-align: center;">11.50</td>
<td style="text-align: center;">8.22</td>
<td style="text-align: center;">19.41</td>
<td style="text-align: center;">38.81</td>
<td style="text-align: center;">45.89</td>
<td style="text-align: center;">62.14</td>
<td style="text-align: center;">35.13</td>
<td style="text-align: center;">22.67</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">KAPING (Ours)</td>
<td style="text-align: center;">34.70</td>
<td style="text-align: center;">25.41</td>
<td style="text-align: center;">24.91</td>
<td style="text-align: center;">41.09</td>
<td style="text-align: center;">43.93</td>
<td style="text-align: center;">40.20</td>
<td style="text-align: center;">52.28</td>
<td style="text-align: center;">62.85</td>
<td style="text-align: center;">60.37</td>
<td style="text-align: center;">73.89</td>
<td style="text-align: center;">67.67</td>
<td style="text-align: center;">47.94</td>
</tr>
<tr>
<td style="text-align: center;">WebQSP w/ Wikidata</td>
<td style="text-align: center;">No Knowledge</td>
<td style="text-align: center;">10.30</td>
<td style="text-align: center;">18.42</td>
<td style="text-align: center;">15.21</td>
<td style="text-align: center;">23.94</td>
<td style="text-align: center;">33.77</td>
<td style="text-align: center;">32.40</td>
<td style="text-align: center;">24.58</td>
<td style="text-align: center;">44.20</td>
<td style="text-align: center;">48.50</td>
<td style="text-align: center;">67.60</td>
<td style="text-align: center;">42.41</td>
<td style="text-align: center;">32.85</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Random Knowledge</td>
<td style="text-align: center;">17.94</td>
<td style="text-align: center;">22.78</td>
<td style="text-align: center;">24.28</td>
<td style="text-align: center;">37.24</td>
<td style="text-align: center;">35.61</td>
<td style="text-align: center;">38.27</td>
<td style="text-align: center;">28.85</td>
<td style="text-align: center;">47.68</td>
<td style="text-align: center;">52.05</td>
<td style="text-align: center;">60.64</td>
<td style="text-align: center;">55.63</td>
<td style="text-align: center;">38.27</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Popular Knowledge</td>
<td style="text-align: center;">15.35</td>
<td style="text-align: center;">20.80</td>
<td style="text-align: center;">20.74</td>
<td style="text-align: center;">30.83</td>
<td style="text-align: center;">30.01</td>
<td style="text-align: center;">27.83</td>
<td style="text-align: center;">24.83</td>
<td style="text-align: center;">48.02</td>
<td style="text-align: center;">47.41</td>
<td style="text-align: center;">63.37</td>
<td style="text-align: center;">53.92</td>
<td style="text-align: center;">34.83</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Generated Knowledge</td>
<td style="text-align: center;">11.94</td>
<td style="text-align: center;">13.30</td>
<td style="text-align: center;">12.28</td>
<td style="text-align: center;">11.26</td>
<td style="text-align: center;">17.53</td>
<td style="text-align: center;">14.19</td>
<td style="text-align: center;">22.92</td>
<td style="text-align: center;">41.34</td>
<td style="text-align: center;">48.77</td>
<td style="text-align: center;">65.89</td>
<td style="text-align: center;">31.16</td>
<td style="text-align: center;">26.42</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">KAPING (Ours)</td>
<td style="text-align: center;">23.67</td>
<td style="text-align: center;">40.38</td>
<td style="text-align: center;">35.47</td>
<td style="text-align: center;">49.52</td>
<td style="text-align: center;">53.34</td>
<td style="text-align: center;">51.57</td>
<td style="text-align: center;">49.86</td>
<td style="text-align: center;">58.73</td>
<td style="text-align: center;">60.44</td>
<td style="text-align: center;">69.58</td>
<td style="text-align: center;">65.04</td>
<td style="text-align: center;">50.69</td>
</tr>
<tr>
<td style="text-align: center;">Mintaka w/ Wikidata</td>
<td style="text-align: center;">No Knowledge</td>
<td style="text-align: center;">11.23</td>
<td style="text-align: center;">14.25</td>
<td style="text-align: center;">17.06</td>
<td style="text-align: center;">19.76</td>
<td style="text-align: center;">27.19</td>
<td style="text-align: center;">26.83</td>
<td style="text-align: center;">14.75</td>
<td style="text-align: center;">23.74</td>
<td style="text-align: center;">34.65</td>
<td style="text-align: center;">36.33</td>
<td style="text-align: center;">41.97</td>
<td style="text-align: center;">26.16</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Random Knowledge</td>
<td style="text-align: center;">17.59</td>
<td style="text-align: center;">18.19</td>
<td style="text-align: center;">18.83</td>
<td style="text-align: center;">28.11</td>
<td style="text-align: center;">26.58</td>
<td style="text-align: center;">28.36</td>
<td style="text-align: center;">16.10</td>
<td style="text-align: center;">26.15</td>
<td style="text-align: center;">32.98</td>
<td style="text-align: center;">51.56</td>
<td style="text-align: center;">46.02</td>
<td style="text-align: center;">28.22</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Popular Knowledge</td>
<td style="text-align: center;">17.56</td>
<td style="text-align: center;">18.09</td>
<td style="text-align: center;">18.73</td>
<td style="text-align: center;">26.97</td>
<td style="text-align: center;">27.08</td>
<td style="text-align: center;">23.10</td>
<td style="text-align: center;">16.74</td>
<td style="text-align: center;">27.15</td>
<td style="text-align: center;">32.48</td>
<td style="text-align: center;">53.16</td>
<td style="text-align: center;">46.41</td>
<td style="text-align: center;">27.95</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Generated Knowledge</td>
<td style="text-align: center;">13.61</td>
<td style="text-align: center;">14.61</td>
<td style="text-align: center;">14.29</td>
<td style="text-align: center;">11.87</td>
<td style="text-align: center;">14.96</td>
<td style="text-align: center;">16.24</td>
<td style="text-align: center;">14.46</td>
<td style="text-align: center;">23.13</td>
<td style="text-align: center;">33.12</td>
<td style="text-align: center;">55.65</td>
<td style="text-align: center;">34.58</td>
<td style="text-align: center;">22.41</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">KAPING (Ours)</td>
<td style="text-align: center;">19.72</td>
<td style="text-align: center;">22.00</td>
<td style="text-align: center;">22.85</td>
<td style="text-align: center;">32.94</td>
<td style="text-align: center;">32.37</td>
<td style="text-align: center;">33.37</td>
<td style="text-align: center;">20.68</td>
<td style="text-align: center;">29.50</td>
<td style="text-align: center;">35.61</td>
<td style="text-align: center;">56.86</td>
<td style="text-align: center;">49.08</td>
<td style="text-align: center;">32.27</td>
</tr>
</tbody>
</table>
<p>Table 2: Retriever results. We compare random model, popular model, and MPNet (Song et al., 2020), on 1- and 2-hop retrievals.
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<h2>5 Experimental Results and Analyses</h2>
<p>We provide the overall results of our KAPING framework along with its comprehensive analyses.</p>
<p>Main Results As shown in Table 1, our KAPING framework significantly outperforms all LM prompting baselines, on zero-shot KGQA tasks. In particular, the generated knowledge model mostly degenerates the performance compared to the no knowledge model, since the extracted knowledge from LLMs themselves might be inaccurate. On the other hand, the random and popular knowledge baselines bring performance improvements, since the augmented knowledge from KGs are sometimes useful to answer the question. However, ours outperforms them, which suggests that, for zero-shot LM prompting for QA, the knowledge internalized in LLMs is insufficient to generate factual answers, and it is important to use only the relevant facts.</p>
<p>In addition, we also observe larger performance improvements when LMs are relatively small. In other words, since smaller models have insufficient parameter spaces to memorize the knowledge during pre-training, they are more likely to generate factually incorrect answers. However, when the appropriate knowledge is given to them, their performances sometimes become similar to larger models (e.g., different sizes of OPT have similar performances by our KAPING). Therefore, for tasks that require factual knowledge under low-resource setups (e.g., production), augmenting the knowledge would be beneficial, instead of increasing model sizes to handle the huge volume of knowledge.</p>
<p>Figure 3: Comparisons of correct and incorrect retrieval for the generation performance on the GPT-3 (6.7B) model.
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 4: Performances with varying the knowledge order, where we change the location - top, bottom, or random - of more relevant triples for the question in the prompt of LLMs.</p>
<p>Retriever Results To see how relevant the augmented knowledge is, we further measure the retrieval performances. As shown in Table 2, the existing retrieval model (i.e., MPNet) shows superior performances against naive models: random and popular retrievers. This result suggests that our simple graph-to-text verbalization works well with the existing retriever, which further confirms that our KAPING augments useful facts in the LM prompt. Regarding the number of hops for the candidate triples to retrieve, we observe that, when we increase the hop-size from one to two, the retriever is more likely to retrieve irrelevant triples that does not include answer entities, as shown in Table 2. Therefore, in our experiments, we retrieve knowledge among 1-hop triples of question entities.</p>
<p>Additionally, since we can alternatively answer the input question based on entities in the Top-1 triple from the retriever, we compare the generation performance of LLMs to the retrieval performance. As shown in Figure 2, LM prompting schemes even without knowledge augmentation (i.e., no knowledge) are superior than simply answering with the entity in the retrieved triple, except for the WebQSP w/ Freebase dataset. Also, we observe huge gaps between our KAPING framework and the simple retrieval scheme on all datasets. These results suggest that, for zero-shot KGQA, it would be helpful to leverage LLMs to generate answers based on their internalized and external facts, instead of directly searching answer entities over KGs.</p>
<p>Impact of Correct \&amp; Incorrect Retrievals We conduct analyses on how much the correctly retrieved triples, having answer entities, bring performance improvements, and how performances are affected by the incorrectly retrieved triples, which</p>
<p>Figure 5: Performances with varying knowledge amount, where we change the number of retrieved triples to augment.
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Table 3: Efficiencies with varying the knowledge amount, where we measure the wall-clock time of every model for generating the answer on the WebQSP w/ Wikidata dataset.
do not include answer entities. As shown in Figure 3, when retrieved triples contain answer entities, performances of LLMs are significantly improved, compared to models without knowledge augmentation. However, when retrievers fail, performances are lower than models of no knowledge augmentation. These results suggest, when relevant knowledge is augmented, LLMs can contextualize and generate answers accurately. Meanwhile, incorrectly retrieved knowledge makes LLMs condition on irrelevant facts, and generate wrong answers.</p>
<p>Varying the Amount of Knowledge We change the number of facts, to see which triple amounts are optimal to augment in the prompt, by comparing trade-off between the generation performance and the wall-clock time. First of all, as shown in Figure 5, most LLMs reach the somewhat highest performance, when the number of triples is 5 or 10. Also, when we further increase the augmented triple size to 15 and 30 , performances of OPT models are largely decreasing. This result suggests that some LMs might be distracted by irrelevant triples when their volumes are high, therefore, failing to select and generate the answer entity.</p>
<p>We then measure the wall-clock time of the answer generation, for the encoder-decoder (T0) and decoder-only (OPT) models with varying the number of augmented triples in the prompt. As shown in Table 3, regarding the encoder-decoder model, our KAPING framework with less than 10 triples is faster than the model without knowledge augmentation. We observe this is because, when the knowledge is augmented to the model, the model tends to generate shorter answers, which can reduce the decoding time. More specifically, the length of generated tokens for the T0 model with 10 triples is</p>
<p>Table 4: Generation examples of the prompted GPT-3 for the input question with augmented triples from the retriever, where, in the last row, we change the knowledge of augmented facts to see whether the model is able to adapt to the changed knowledge.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Question: Where did Alex Chilton die?</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Retrieved triples: (Alex Chilton, place of death, New Orleans, (Alex Chilton, manner of death, natural causes), (Alex Chilton, cause of death, myocardial infarction), (Alex Chilton, date of death, time: +2010-03-17), ... <br> Answer: New Orleans</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Prompts</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Outputs</td>
</tr>
<tr>
<td style="text-align: center;">No Knowledge</td>
<td style="text-align: center;">Question: Where did Alex Chilton die? Answer:</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Alex Chilton died in a car accident on February-10, 1979.</td>
</tr>
<tr>
<td style="text-align: center;">KAPING (Ours)</td>
<td style="text-align: center;">Below are facts in the form of the triple meaningful to answer the question. ... (Alex Chilton, date of death, time: +2010-03-17) (Alex Chilton, cause of death, myocardial infarction) (Alex Chilton, manner of death, natural causes) (Alex Chilton, place of death, New Orleans. Question: Where did Alex Chilton die? Answer:</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Alex Chilton died on March 17, 2010 in New Orleans, Louisiana due to a myocardial infarction.</td>
</tr>
<tr>
<td style="text-align: center;">KAPING (Ours) w/ Knowledge Change</td>
<td style="text-align: center;">Below are facts in the form of the triple meaningful to answer the question. ... (Alex Chilton, date of death, time: +2000-09-01) (Alex Chilton, cause of death, pancreatic cancer) (Alex Chilton, manner of death, natural causes) (Alex Chilton, place of death, Los Angeles. Question: Where did Alex Chilton die? Answer:</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Alex Chilton died in Los Angeles, California on September 1, 2000 from pancreatic cancer.</td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th style="text-align: center;">Models</th>
<th style="text-align: center;">T5.cm:</th>
<th style="text-align: center;">T5.cm:</th>
<th style="text-align: center;">OPT.cm:</th>
<th style="text-align: center;">OPT.cm:</th>
<th style="text-align: center;">T0.cm:</th>
<th style="text-align: center;">T0.cm:</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">No Knowledge</td>
<td style="text-align: center;">14.25</td>
<td style="text-align: center;">17.06</td>
<td style="text-align: center;">19.76</td>
<td style="text-align: center;">26.83</td>
<td style="text-align: center;">14.75</td>
<td style="text-align: center;">23.74</td>
</tr>
<tr>
<td style="text-align: center;">Random Knowledge</td>
<td style="text-align: center;">18.19</td>
<td style="text-align: center;">18.83</td>
<td style="text-align: center;">28.11</td>
<td style="text-align: center;">28.36</td>
<td style="text-align: center;">16.10</td>
<td style="text-align: center;">26.15</td>
</tr>
<tr>
<td style="text-align: center;">Random Knowledge w/ EL</td>
<td style="text-align: center;">15.99</td>
<td style="text-align: center;">17.98</td>
<td style="text-align: center;">23.10</td>
<td style="text-align: center;">26.47</td>
<td style="text-align: center;">15.60</td>
<td style="text-align: center;">24.66</td>
</tr>
<tr>
<td style="text-align: center;">KAPING</td>
<td style="text-align: center;">22.00</td>
<td style="text-align: center;">22.85</td>
<td style="text-align: center;">32.94</td>
<td style="text-align: center;">33.37</td>
<td style="text-align: center;">20.68</td>
<td style="text-align: center;">29.50</td>
</tr>
<tr>
<td style="text-align: center;">KAPING w/ EL</td>
<td style="text-align: center;">18.94</td>
<td style="text-align: center;">20.58</td>
<td style="text-align: center;">26.87</td>
<td style="text-align: center;">28.39</td>
<td style="text-align: center;">18.51</td>
<td style="text-align: center;">27.11</td>
</tr>
</tbody>
</table>
<p>Table 5: Results with entity linking, where the model w/ EL uses entities extracted from the entity linking technique (Ayoola et al., 2022), instead of using labeled ones, on Mintaka.
15, whereas, the no knowledge model generates 32 tokens in average. However, for the decoder-only model (OPT), the more knowledge we augment, the slower the model becomes, because of its autoregressive characteristic for digesting the input.</p>
<p>Impact of Orders of Retrieved Triples In fewshot LM prompting where LLMs additionally observe few examples in the prompt, they are known to be sensitive to the order of examples (Lu et al., 2022), and they tend to follow the answer in the last example (Zhao et al., 2021). Based on those observations, we also conduct an analysis on whether the order of retrieved triples affects the performance. In particular, we vary the location of more similar triples for the question, by locating them at the Top, Bottom, or Random position of the prompt. As shown in Figure 4, our KAPING is not sensitive to the location of retrieved triples, except for the OPT model on the WebQSP dataset. In other words, the OPT model tends to generate the entity located at the first part of the prompt input. Meanwhile, other LLMs can contextualize the entire prompt input, and generate the entity regardless of its position.</p>
<p>Effectiveness with Entity Linking Following the conventional KGQA evaluation (Cohen et al., 2020), we use question entities labeled in datasets, to retrieve facts in KGs. However, to see the performance with entities identified by Entity Linking (EL) technique, we further conduct experiments
with the EL model, namely ReFinED (Ayoola et al., 2022). As shown in Table 5, while the performance of KAPING w/ EL is slightly decreasing from the model with labeled entities due to the performance of EL, we consistently observe meaningful performance improvements from a No Knowledge model.</p>
<p>Case Study We conduct a case study in Table 4. In particular, when the knowledge is not given to the LM, it hallucinates the factually incorrect answer. However, when related facts are retrieved and augmented in the prompt, it can generate the correct answer. In addition, we analyze whether our KAPING can adapt to the updated knowledge, motivated by that some knowledge can be changed over time, while the knowledge in LMs remains static. To do so, as shown in the last row of Table 4, we replace object entities of triples, and then forward the prompt with the modified facts to the LM. Then, the result shows that the LM can generate the output based on the updated facts, which suggests the potential of adapting LMs without costly updating their parameters.</p>
<p>Additional Results Note that we further provide additional experimental results in Appendix B. In particular, we compare the performance of retrievers in Appendix B.1, conduct the sensitivity analysis on template texts in Appendix B.2, provide the results with additional metrics including human evaluation in Appendix B.3, validate our KAPING under few-shot setups in Appendix B.4, provide the analysis on verbalization in Appendix B.5, and provide the efficiencies in Appendix B.6.</p>
<h2>6 Conclusion</h2>
<p>In this work, we focused on the limitation of existing LM prompting schemes, which rely on the</p>
<p>static knowledge internalized in parameters; therefore, when such knowledge are incomplete, inaccurate, and outdated, LLMs may generate factually incorrect answers. To tackle this challenge, we introduced a novel Knowledge-Augmented language model PrompTING (KAPING) framework, which augments the knowledge for the input question from KGs directly in the input prompt of LLMs, with the fact retriever to inject only the relevant knowledge. The proposed framework is completely zero-shot, and versatile with any LMs, without additional parameter updates and training datasets. We validated that our KAPING yields huge performance gaps from the LM prompting model relying on its internal knowledge, especially with smaller LMs, on the KGQA tasks. We believe our new mechanism for augmenting facts from KGs to the LM prompt will bring substantial practical impacts in generating knowledge-grounded answers.</p>
<h2>Limitations</h2>
<p>In this section, we faithfully discuss the current limitations and potential avenues for future research.</p>
<p>First of all, the generation performance of our knowledge-augmentation framework largely depends on the efficacy of retrievers. In other words, if the retriever fails to retrieve the relevant facts to the input question, the prompted LLM, conditioned on the irrelevant facts, is likely to generate the incorrect answer (See Figure 3). Similarly, if the retriever is not designed to retrieve the facts in 2-hop neighborhoods of the question entities, LLMs are less likely to generate the answer requiring 2-hop knowledge. Note that, for the Mintaka dataset (Sen et al., 2022), the number of answerable questions with 1-hop facts is only $40 \%$ of total samples. However, when we include 2-hop triples, the number of answerable questions becomes $62 \%$, which suggests the necessity of 2-hop retrievals, which is yet challenging (See Table 2). Thus, future work may improve the retrieval scheme itself to provide more accurate facts including multi-hops to the LLM, or may develop the mechanism to prevent the LLM from being misled by unrelated facts.</p>
<p>On the other hand, the evaluation metric for the generation performance of prompted LLMs may be further improved. Specifically, regarding our target KGQA tasks, the answer for the question is the entity in KGs. However, the prompted LLMs without additional training (i.e., zero-shot) tend to generate the answer as the sentence. For instance, the
label entity for the question (e.g., Where did Alex Chilton die?) in Table 4 is "New Orleans", however, the LLMs often generate the sentence-level output: "Alex Chilton died on March 17, 2010 in New Orleans, Louisiana due to a myocardial infarction". We currently evaluate the model performance by measuring whether generated tokens contain the answer entity or not; however, it would be worthwhile to develop the additional metric to compare the sentence-level output from LLMs to the word-level answer in KGs in a more effective way. Note that we also try other available metrics (See Appendix B.3), such as F1 and Exact Match (EM) scores (Rajpurkar et al., 2016), however, they largely penalize the longer sentences (e.g., EM of correct examples in Table 4 are 0), thus may not be appropriate for evaluating LM prompting schemes.</p>
<p>Lastly, since we focus on the improvement of knowledge injection in LM prompting, we use the labeled entities in KGQA datasets when evaluating models, following the existing KGQA evaluation setups (Cohen et al., 2020; Sen et al., 2021). However, in real-world applications where the entities in the question are mostly not provided, we first need to extract entities in the question with existing entity linking techniques; therefore, our model performance depends on the efficacy of entity linking. In particular, regarding the result with entity linking in Table 5, the portion of answerable questions from labeled entities in the dataset is $40 \%$, however, the portion of them with entities from the entity linking model (Ayoola et al., 2022) is 22\%. Therefore, since the improved entity linking performance would contribute to the performance gain of our KAPING framework, for KGQA tasks, future work may advance such the entity linking scheme.</p>
<h2>Ethics Statement</h2>
<p>For a user's question, our knowledge-augmentation scheme can allow prompted LMs generate a factually correct answer, grounded by the provided knowledge, for KGQA tasks. However, the performance of our KAPING framework is still far from perfect, due to potential failures in entity linking, fact retrieval, and knowledge generation itself. Thus, we should be aware whether LMs generate correct answers, especially on high-risk domains.</p>
<h2>Acknowledgements</h2>
<p>We thank the members of the End-to-End Reasoning team of Alexa AI at Amazon and the anonymous reviewers for their constructive comments.</p>
<h2>References</h2>
<p>Tom Ayoola, Shubhi Tyagi, Joseph Fisher, Christos Christodoulopoulos, and Andrea Pierleoni. 2022. Refined: An efficient zero-shot-capable approach to end-to-end entity linking. arXiv preprint arXiv:2207.04108.</p>
<p>Junwei Bao, Nan Duan, Zhao Yan, Ming Zhou, and Tiejun Zhao. 2016. Constraint-based question answering with knowledge graph. In COLING. ACL.</p>
<p>Hannah Bast, Björn Buchhold, and Elmar Haussmann. 2016. Semantic search on text and knowledge bases. Found. Trends Inf. Retr.</p>
<p>Jonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang. 2013. Semantic parsing on freebase from question-answer pairs. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, EMNLP 2013, 18-21 October 2013, Grand Hyatt Seattle, Seattle, Washington, USA, A meeting of SIGDAT, a Special Interest Group of the ACL. ACL.</p>
<p>Kurt D. Bollacker, Colin Evans, Praveen K. Paritosh, Tim Sturge, and Jamie Taylor. 2008. Freebase: a collaboratively created graph database for structuring human knowledge. In Proceedings of the ACM SIGMOD International Conference on Management of Data, SIGMOD 2008, Vancouver, BC, Canada, June 10-12, 2008. ACM.</p>
<p>Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. In NeurIPS.</p>
<p>Nilesh Chakraborty, Denis Lukovnikov, Gaurav Maheshwari, Priyansh Trivedi, Jens Lehmann, and Asja Fischer. 2019. Introduction to neural network based approaches for question answering over knowledge graphs. arXiv preprint arXiv:1907.09361.</p>
<p>Xiang Chen, Lei Li, Ningyu Zhang, Xiaozhuan Liang, Shumin Deng, Chuanqi Tan, Fei Huang, Luo Si, and Huajun Chen. 2022a. Decoupling knowledge from memorization: Retrieval-augmented prompt learning. arXiv preprint arXiv:2205.14704.</p>
<p>Xiang Chen, Ningyu Zhang, Xin Xie, Shumin Deng, Yunzhi Yao, Chuanqi Tan, Fei Huang, Luo Si, and Huajun Chen. 2022b. Knowprompt: Knowledgeaware prompt-tuning with synergistic optimization for relation extraction. In WWW, pages 2778-2788. ACM.</p>
<p>Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. 2022. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311.</p>
<p>William W. Cohen, Haitian Sun, R. Alex Hofer, and Matthew Siegler. 2020. Scalable neural methods for reasoning with a symbolic knowledge base. In $I C L R$.</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: pre-training of deep bidirectional transformers for language understanding. In NAACL. Association for Computational Linguistics.</p>
<p>Dennis Diefenbach, Thomas Pellissier Tanon, Kamal Deep Singh, and Pierre Maret. 2017. Question answering benchmarks for wikidata. In Proceedings of the ISWC 2017 Posters \&amp; Demonstrations and Industry Tracks co-located with 16th International Semantic Web Conference (ISWC 2017), Vienna, Austria, October 23rd - to - 25th, 2017, CEUR Workshop Proceedings. CEUR-WS.org.</p>
<p>Bin Fu, Yunqi Qiu, Chengguang Tang, Yang Li, Haiyang Yu, and Jian Sun. 2020. A survey on complex question answering over knowledge base: Recent advances and challenges. arXiv preprint arXiv:2007.13069.</p>
<p>Fabian Galetzka, Jewgeni Rose, David Schlangen, and Jens Lehmann. 2021. Space efficient context encoding for non-task-oriented dialogue generation with graph attention transformer. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021, August 1-6, 2021. Association for Computational Linguistics.</p>
<p>Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang. 2020. REALM: retrievalaugmented language model pre-training. arXiv preprint arXiv:2002.08909.</p>
<p>Sebastian Hofstätter, Sheng-Chieh Lin, Jheng-Hong Yang, Jimmy Lin, and Allan Hanbury. 2021. Efficiently teaching an effective dense retriever with balanced topic aware sampling. In SIGIR '21: The 44th International ACM SIGIR Conference on Research and Development in Information Retrieval, Canada, July 11-15, 2021. ACM.</p>
<p>Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin de Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. 2019. Parameter-efficient transfer learning for NLP. In ICML, Proceedings of Machine Learning Research. PMLR.</p>
<p>Shengding Hu, Ning Ding, Huadong Wang, Zhiyuan Liu, Jingang Wang, Juanzi Li, Wei Wu, and Maosong Sun. 2022. Knowledgeable prompt-tuning: Incorporating knowledge into prompt verbalizer for text classification. In ACL, pages 2225-2240, Dublin, Ireland. Association for Computational Linguistics.</p>
<p>Gautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane Dwivedi-Yu, Armand Joulin, Sebastian Riedel, and Edouard Grave. 2022. Few-shot learning with retrieval augmented language models. arXiv preprint arXiv:2208.03299.</p>
<p>Minki Kang, Jin Myung Kwak, Jinheon Baek, and Sung Ju Hwang. 2022. Knowledge-consistent dialogue generation with knowledge graphs. In ICML 2022 Workshop on Knowledge Retrieval and Language Models.</p>
<p>Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick S. H. Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. 2020. Dense passage retrieval for open-domain question answering. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020, November 16-20, 2020. Association for Computational Linguistics.</p>
<p>Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. 2022. Large language models are zero-shot reasoners. arXiv preprint arXiv:2205.11916.</p>
<p>Angeliki Lazaridou, Elena Gribovskaya, Wojciech Stokowiec, and Nikolai Grigorev. 2022. Internetaugmented language models through few-shot prompting for open-domain question answering. arXiv preprint arXiv:2203.05115.</p>
<p>Brian Lester, Rami Al-Rfou, and Noah Constant. 2021a. The power of scale for parameter-efficient prompt tuning. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic, 7-11 November, 2021, pages 30453059. Association for Computational Linguistics.</p>
<p>Brian Lester, Rami Al-Rfou, and Noah Constant. 2021b. The power of scale for parameter-efficient prompt
tuning. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021 / Punta Cana, Dominican Republic, 7-11 November, 2021. Association for Computational Linguistics.</p>
<p>Patrick S. H. Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, Sebastian Riedel, and Douwe Kiela. 2020. Retrieval-augmented generation for knowledge-intensive NLP tasks. In NeurIPS.</p>
<p>Belinda Z. Li, Sewon Min, Srinivasan Iyer, Yashar Mehdad, and Wen-tau Yih. 2020. Efficient one-pass end-to-end entity linking for questions. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020, November 16-20, 2020. Association for Computational Linguistics.</p>
<p>Jimmy Lin, Rodrigo Nogueira, and Andrew Yates. 2021. Pretrained Transformers for Text Ranking: BERT and Beyond. Synthesis Lectures on Human Language Technologies. Morgan \&amp; Claypool Publishers.</p>
<p>Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, and Weizhu Chen. 2022a. What makes good in-context examples for gpt-3? In Proceedings of Deep Learning Inside Out: The 3rd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures, DeeLIO@ACL 2022, Dublin, Ireland and Online, May 27, 2022, pages 100-114. Association for Computational Linguistics.</p>
<p>Jiacheng Liu, Alisa Liu, Ximing Lu, Sean Welleck, Peter West, Ronan Le Bras, Yejin Choi, and Hannaneh Hajishirzi. 2022b. Generated knowledge prompting for commonsense reasoning. In ACL. Association for Computational Linguistics.</p>
<p>Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. 2021. Pretrain, prompt, and predict: A systematic survey of prompting methods in natural language processing. arXiv preprint arXiv:2107.13586.</p>
<p>Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized BERT pretraining approach. arXiv preprint arXiv:1907.11692.</p>
<p>Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, and Pontus Stenetorp. 2022. Fantastically ordered prompts and where to find them: Overcoming fewshot prompt order sensitivity. In ACL. Association for Computational Linguistics.</p>
<p>Kangqi Luo, Fengli Lin, Xusheng Luo, and Kenny Q. Zhu. 2018. Knowledge base question answering via encoding of complex query graphs. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Brussels, Belgium, October 31 - November 4, 2018. Association for Computational Linguistics.</p>
<p>Kaixin Ma, Hao Cheng, Xiaodong Liu, Eric Nyberg, and Jianfeng Gao. 2022. Open domain question answering with A unified knowledge interface. In ACL. Association for Computational Linguistics.</p>
<p>Vaibhav Mavi, Anubhav Jangra, and Adam Jatowt. 2022. A survey on multi-hop question answering and generation. arXiv preprint arXiv:2204.09140.</p>
<p>Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan Majumder, and Li Deng. 2016. MS MARCO: A human generated machine reading comprehension dataset. In Proceedings of the Workshop on Cognitive Computation: Integrating neural and symbolic approaches 2016 co-located with the 30th Annual Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain, December 9, 2016, CEUR Workshop Proceedings. CEUR-WS.org.</p>
<p>Barlas Oguz, Xilun Chen, Vladimir Karpukhin, Stan Peshterliev, Dmytro Okhonko, Michael Sejr Schlichtkrull, Sonal Gupta, Yashar Mehdad, and Scott Yih. 2022. Unik-qa: Unified representations of structured and unstructured knowledge for opendomain question answering. In Findings of the Association for Computational Linguistics: NAACL. Association for Computational Linguistics.</p>
<p>Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. 2019. Pytorch: An imperative style, high-performance deep learning library. In NeurIPS. Curran Associates, Inc.</p>
<p>Ted Pedersen, Siddharth Patwardhan, and Jason Michelizzi. 2004. Wordnet: : Similarity - measuring the relatedness of concepts. In Proceedings of the Nineteenth National Conference on Artificial Intelligence, Sixteenth Conference on Innovative Applications of Artificial Intelligence, July 25-29, 2004, San Jose, California, USA, pages 1024-1025. AAAI Press / The MIT Press.</p>
<p>Fabio Petroni, Tim Rocktäschel, Sebastian Riedel, Patrick S. H. Lewis, Anton Bakhtin, Yuxiang Wu, and Alexander H. Miller. 2019. Language models as knowledge bases? In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019, Hong Kong, China, November 3-7, 2019. Association for Computational Linguistics.</p>
<p>Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. 2018. Improving language understanding by generative pre-training.</p>
<p>Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou,</p>
<p>Wei Li, and Peter J. Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. J. Mach. Learn. Res.</p>
<p>Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. Squad: 100, 000+ questions for machine comprehension of text. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, EMNLP 2016, Austin, Texas, USA, November 1-4, 2016. The Association for Computational Linguistics.</p>
<p>Adam Roberts, Colin Raffel, and Noam Shazeer. 2020. How much knowledge can you pack into the parameters of a language model? In EMNLP.</p>
<p>Anna Rohrbach, Lisa Anne Hendricks, Kaylee Burns, Trevor Darrell, and Kate Saenko. 2018. Object hallucination in image captioning. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Brussels, Belgium, October 31 - November 4, 2018. Association for Computational Linguistics.</p>
<p>Md. Rashad Al Hasan Rony, Ricardo Usbeck, and Jens Lehmann. 2022. Dialokg: Knowledge-structure aware task-oriented dialogue generation. In Findings of the Association for Computational Linguistics: NAACL. Association for Computational Linguistics.</p>
<p>Ohad Rubin, Jonathan Herzig, and Jonathan Berant. 2022. Learning to retrieve prompts for in-context learning. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL 2022, Seattle, WA, United States, July 10-15, 2022, pages 2655-2671. Association for Computational Linguistics.</p>
<p>Amir Saffari, Armin Oliya, Priyanka Sen, and Tom Ayoola. 2021. End-to-end entity resolution and question answering using differentiable knowledge graphs. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021 / Punta Cana, Dominican Republic, 7-11 November, 2021. Association for Computational Linguistics.</p>
<p>Victor Sanh, Albert Webson, Colin Raffel, Stephen Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Arun Raja, Manan Dey, M Saiful Bari, Canwen Xu, Urmish Thakker, Shanya Sharma Sharma, Eliza Szczechla, Taewoon Kim, Gunjan Chhablani, Nihal V. Nayak, Debajyoti Datta, Jonathan Chang, Mike Tian-Jian Jiang, Han Wang, Matteo Manica, Sheng Shen, Zheng Xin Yong, Harshit Pandey, Rachel Bawden, Thomas Wang, Trishala Neeraj, Jos Rozen, Abheesht Sharma, Andrea Santilli, Thibault Févry, Jason Alan Fries, Ryan Teehan, Teven Le Scao, Stella Biderman, Leo Gao, Thomas Wolf, and Alexander M. Rush. 2022. Multitask prompted training enables zero-shot task generalization. In $I C L R$.</p>
<p>Apoorv Saxena, Aditay Tripathi, and Partha P. Talukdar. 2020. Improving multi-hop question answering over</p>
<p>knowledge graphs using knowledge base embeddings. In ACL. Association for Computational Linguistics.</p>
<p>Priyanka Sen, Alham Fikri Aji, and Amir Saffari. 2022. Mintaka: A complex, natural, and multilingual dataset for end-to-end question answering. In COLING. International Committee on Computational Linguistics.</p>
<p>Priyanka Sen, Armin Oliya, and Amir Saffari. 2021. Expanding end-to-end question answering on differentiable knowledge graphs with intersection. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021 / Punta Cana, Dominican Republic, 7-11 November, 2021. Association for Computational Linguistics.</p>
<p>Taylor Shin, Yasaman Razeghi, Robert L. Logan IV, Eric Wallace, and Sameer Singh. 2020. Autoprompt: Eliciting knowledge from language models with automatically generated prompts. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020, Online, November 16-20, 2020, pages 4222-4235. Association for Computational Linguistics.</p>
<p>Saleh Soltan, Shankar Ananthakrishnan, Jack FitzGerald, Rahul Gupta, Wael Hamza, Haidar Khan, Charith Peris, Stephen Rawls, Andy Rosenbaum, Anna Rumshisky, Chandana Satya Prakash, Mukund Sridhar, Fabian Triefenbach, Apurv Verma, Gökhan Tür, and Prem Natarajan. 2022. Alexatm 20b: Few-shot learning using a large-scale multilingual seq2seq model. arXiv preprint arXiv:2208.01448.</p>
<p>Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and TieYan Liu. 2020. Mpnet: Masked and permuted pretraining for language understanding. In NeurIPS.</p>
<p>Robyn Speer, Joshua Chin, and Catherine Havasi. 2017. Conceptnet 5.5: An open multilingual graph of general knowledge. In Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence, February 4-9, 2017, San Francisco, California, USA, pages 4444-4451. AAAI Press.</p>
<p>Haitian Sun, Bhuwan Dhingra, Manzil Zaheer, Kathryn Mazaitis, Ruslan Salakhutdinov, and William W. Cohen. 2018. Open domain question answering using early fusion of knowledge bases and text. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Brussels, Belgium, October 31 - November 4, 2018. Association for Computational Linguistics.</p>
<p>Mujeen Sung, Jinhyuk Lee, Sean S. Yi, Minji Jeon, Sungdong Kim, and Jaewoo Kang. 2021. Can language models be biomedical knowledge bases? In EMNLP. Association for Computational Linguistics.</p>
<p>Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In NeurIPS.</p>
<p>Denny Vrandecic and Markus Krötzsch. 2014. Wikidata: a free collaborative knowledgebase. Commun. $A C M, 57(10): 78-85$.</p>
<p>Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V. Le, Ed H. Chi, and Denny Zhou. 2022. Selfconsistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171.</p>
<p>Jason Wei, Maarten Bosma, Vincent Y. Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, and Quoc V. Le. 2022a. Finetuned language models are zero-shot learners. In ICLR.</p>
<p>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed H. Chi, Quoc Le, and Denny Zhou. 2022b. Chain of thought prompting elicits reasoning in large language models. arXiv preprint arXiv:2201.11903.</p>
<p>Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. 2020. Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations. Association for Computational Linguistics.</p>
<p>Ledell Wu, Fabio Petroni, Martin Josifoski, Sebastian Riedel, and Luke Zettlemoyer. 2020. Scalable zeroshot entity linking with dense entity retrieval. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020, November 16-20, 2020. Association for Computational Linguistics.</p>
<p>Lee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang, Jialin Liu, Paul N. Bennett, Junaid Ahmed, and Arnold Overwijk. 2021. Approximate nearest neighbor negative contrastive learning for dense text retrieval. In ICLR.</p>
<p>Zhilin Yang, Zihang Dai, Yiming Yang, Jaime G. Carbonell, Ruslan Salakhutdinov, and Quoc V. Le. 2019. Xlnet: Generalized autoregressive pretraining for language understanding. In NeurIPS.</p>
<p>Michihiro Yasunaga, Hongyu Ren, Antoine Bosselut, Percy Liang, and Jure Leskovec. 2021. QA-GNN: reasoning with language models and knowledge graphs for question answering. In NAACL. Association for Computational Linguistics.</p>
<p>Wen-tau Yih, Ming-Wei Chang, Xiaodong He, and Jianfeng Gao. 2015. Semantic parsing via staged query graph generation: Question answering with knowledge base. In ACL. The Association for Computer Linguistics.</p>
<p>Wen-tau Yih, Matthew Richardson, Christopher Meek, Ming-Wei Chang, and Jina Suh. 2016. The value of semantic parse labeling for knowledge base question</p>
<p>answering. In ACL. The Association for Computer Linguistics.</p>
<p>Jun Yin, Xin Jiang, Zhengdong Lu, Lifeng Shang, Hang Li, and Xiaoming Li. 2016. Neural generative question answering. In Proceedings of the Twenty-Fifth International Joint Conference on Artificial Intelligence, IJCAI 2016, New York, NY, USA, 9-15 July 2016, pages 2972-2978. IJCAI/AAAI Press.</p>
<p>Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, and Luke Zettlemoyer. 2022. OPT: open pretrained transformer language models. arXiv preprint arXiv:2205.01068.</p>
<p>Zihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh. 2021. Calibrate before use: Improving few-shot performance of language models. In ICML, Proceedings of Machine Learning Research. PMLR.</p>
<p>Yucheng Zhou, Xiubo Geng, Tao Shen, Wenqiang Zhang, and Daxin Jiang. 2021. Improving zero-shot cross-lingual transfer for multilingual question answering over knowledge graph. In NAACL. Association for Computational Linguistics.</p>
<h2>A Additional Experimental Setups</h2>
<p>Here we provide additional experimental setups.</p>
<h2>A. 1 Datasets</h2>
<p>We provide the additional details for two Knowledge Graph Question Answering (KGQA) datasets, namely WebQuestionsSP and Mintaka, which we use for evaluating baselines and our model.</p>
<p>WebQuestionsSP (WebQSP) A question and its corresponding answer are annotated with Freebase entities (Bollacker et al., 2008), and refined with additional cleaning steps (Yih et al., 2016): filtering out samples with invalid annotations, from the original WebQuestions dataset (Berant et al., 2013).</p>
<p>Mintaka This dataset (Sen et al., 2022) is designed for complex KGQA tasks including superlative and comparative questions, where questionanswer pairs are collected from crowdsourcing with Wikidata entities (Vrandecic and Krötzsch, 2014).</p>
<h2>A. 2 Large Language Models</h2>
<p>We describe the specific details of Large Language Models (LLMs) that we use for LM prompting.</p>
<p>T5 This model (Raffel et al., 2020) is an encoderdecoder model, and, among different variants, we use the LM-adapted version ${ }^{1}$, which is additionally pre-trained with auto-regressive language modeling objective (Radford et al., 2018) for LM prompting.</p>
<p>T0 This model (Sanh et al., 2022) is further finetuned from T5 (Raffel et al., 2020) over prompted text-to-text tasks, for improved zero-shot generalization performance with LM prompting.</p>
<p>GPT-3 This model (Brown et al., 2020) is a decoder only model, which we access via API ${ }^{2}$.</p>
<p>OPT This model (Zhang et al., 2022) is a decoder only model, freely available for researchers.</p>
<p>AlexaTM This model (Soltan et al., 2022) is an encoder-decoder model, pre-trained with denoising, which reconstructs the context of $15 \%$ dropped tokens, and auto-regressive, which predicts the next tokens based on their previous tokens, objectives.</p>
<h2>A. 3 Evaluation Metrics</h2>
<p>We provide more details for evaluation metrics.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Aliases For generative question answering tasks, there can be alternative names of entities, called aliases, and we consider them for evaluation. For example, one Wikidata entity, "William Shakespeare" (Q692), has alternative names, such as "Shakespeare" and "The Bard", and we consider them when measuring the generation performance.</p>
<p>Filtering Unnamed Entities For evaluating generative models, the name of entities are required. However, we sometime cannot find the name of the answer entities from their ids on Freebase and Wikidata KGs. This is because the annotated answer entities are sometimes not entities but categories, and the entity ids in KGs could be changed but we cannot find the KG dumps that are used to annotate datasets. Therefore, we filter out samples that do not have literal name texts for the answer entities. This filtering step results in 1,582 test samples for the WebQSP w/ Freebase dataset, 1,466 test samples for the WebQSP w/ Wikidata dataset, and 2,814 test samples for the Mintaka dataset.</p>
<h2>A. 4 Implementation Details</h2>
<p>In this subsection, we provide additional details for implementing our KAPING framework.</p>
<p>Knowledge Injection Schemes There are different choices in knowledge injection schemes, from the number of facts to retrieve, to the number of hops for candidate triples, to the order of retrieved facts in the prompt (i.e., where the most relevant knowledge should be located in the prompt), to the template of prompts including their instruction texts. While search spaces of them are extremely huge, we aim to to find the optimal one (See analyses in Section 5). Specifically, as reported in Section 4.5, the best settings we find are the number of retrieved facts of 10 , and the number of hops for the triples to retrieve from the question entities of one. Also, we locate more relevant triples to the input question closer to the question text in the prompt, inspired by the observation that the model tends to rewrite answers that appeared at the end of the prompt (Zhao et al., 2021). Further, we examine different instruction templates for generating answers, such as "Question: ${x}$ Answer: " or "Please answer the following question: ${x}$ ", where $x$ is the literal question. Regarding instruction templates, we observe that the performances of LLMs are sensitive across different instructions (See Appendix B.2), therefore, we try both of them and then report the best result.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: left;"></th>
<th style="text-align: left;">1-Hop Retrieval</th>
<th style="text-align: left;"></th>
<th style="text-align: left;"></th>
<th style="text-align: left;"></th>
<th style="text-align: left;">2-Hop Retrieval</th>
<th style="text-align: left;"></th>
<th style="text-align: left;"></th>
<th style="text-align: left;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Datasets</td>
<td style="text-align: left;">Retrievers</td>
<td style="text-align: left;">MRR</td>
<td style="text-align: left;">Top-1</td>
<td style="text-align: left;">Top-10</td>
<td style="text-align: left;">Top-30</td>
<td style="text-align: left;">MRR</td>
<td style="text-align: left;">Top-1</td>
<td style="text-align: left;">Top-10</td>
<td style="text-align: left;">Top-30</td>
</tr>
<tr>
<td style="text-align: left;">WebQSP</td>
<td style="text-align: left;">MPNet</td>
<td style="text-align: left;">47.27</td>
<td style="text-align: left;">40.27</td>
<td style="text-align: left;">60.56</td>
<td style="text-align: left;">64.48</td>
<td style="text-align: left;">41.64</td>
<td style="text-align: left;">33.12</td>
<td style="text-align: left;">38.47</td>
<td style="text-align: left;">45.23</td>
</tr>
<tr>
<td style="text-align: left;">w/ Frontbase</td>
<td style="text-align: left;">TAS-B</td>
<td style="text-align: left;">51.62</td>
<td style="text-align: left;">45.76</td>
<td style="text-align: left;">61.76</td>
<td style="text-align: left;">64.41</td>
<td style="text-align: left;">57.08</td>
<td style="text-align: left;">25.83</td>
<td style="text-align: left;">38.66</td>
<td style="text-align: left;">64.48</td>
</tr>
<tr>
<td style="text-align: left;">WebQSP</td>
<td style="text-align: left;">MPNet</td>
<td style="text-align: left;">43.46</td>
<td style="text-align: left;">33.36</td>
<td style="text-align: left;">64.39</td>
<td style="text-align: left;">70.67</td>
<td style="text-align: left;">40.42</td>
<td style="text-align: left;">30.56</td>
<td style="text-align: left;">62.62</td>
<td style="text-align: left;">71.56</td>
</tr>
<tr>
<td style="text-align: left;">w/ Wikidata</td>
<td style="text-align: left;">TAS-B</td>
<td style="text-align: left;">46.68</td>
<td style="text-align: left;">37.65</td>
<td style="text-align: left;">65.08</td>
<td style="text-align: left;">70.67</td>
<td style="text-align: left;">41.92</td>
<td style="text-align: left;">32.20</td>
<td style="text-align: left;">62.21</td>
<td style="text-align: left;">72.17</td>
</tr>
<tr>
<td style="text-align: left;">Mintaka</td>
<td style="text-align: left;">MPNet</td>
<td style="text-align: left;">13.01</td>
<td style="text-align: left;">7.50</td>
<td style="text-align: left;">25.44</td>
<td style="text-align: left;">35.43</td>
<td style="text-align: left;">13.00</td>
<td style="text-align: left;">4.82</td>
<td style="text-align: left;">26.65</td>
<td style="text-align: left;">40.01</td>
</tr>
<tr>
<td style="text-align: left;">w/ Wikidata</td>
<td style="text-align: left;">TAS-B</td>
<td style="text-align: left;">13.21</td>
<td style="text-align: left;">7.57</td>
<td style="text-align: left;">25.20</td>
<td style="text-align: left;">35.04</td>
<td style="text-align: left;">12.56</td>
<td style="text-align: left;">6.79</td>
<td style="text-align: left;">24.13</td>
<td style="text-align: left;">36.07</td>
</tr>
</tbody>
</table>
<p>Table 6: Results of two different retrievers, namely MPNet (Song et al., 2020) and TAS-B (Hofstätter et al., 2021).</p>
<p>Retrieval Models To augment only the relevant triples to the input question under the zero-shot setup, we use off-the-shelf text-based retriever models. Specifically, we experiment with two different types of retrievers: symmetric retriever that uses the same encoder for question and triples; asymmetric one that uses individual encoders for them. For the symmetric retriever, we use MPNet (Song et al., 2020), which is trained on 1B sentence pairs ${ }^{3}$. Also, for the asymmetric retriever, we use TAS-B (Hofstätter et al., 2021), which is trained on the MSMARCO dataset (Nguyen et al., 2016). We mainly report the results with MPNet, unless noted, since there performances are similar (See Appendix B.1).</p>
<h2>A. 5 Hyperparameters and Resources</h2>
<p>We evaluate all models with PyTorch (Paszke et al., 2019) and Transformers (Wolf et al., 2020) libraries. We set the maximum number of input token lengths of LMs as 1,024 and the maximum number of output token lengths as 128 , for encoderdecoder models. For decoder-only models, we set the maximum token lengths as $1,152(1,024+128)$. For computing resources, we run all models with 8 V100 GPUs, having $8 \times 32$ GB GPU memory, in which every model is runnable within one day. Note that, due to the expensive computational costs for model prompting with LLMs, we run every model one time, and then report the results, without additional hyperparameter tuning unless noted.</p>
<h2>B Additional Experiment Results</h2>
<p>In this section, we provide additional experimental results, on the comparisons of available text-based retrieval models in Section B.1, the sensitive analyses on template texts of the prompt in Section B.2, and the extra evaluation metrics in Section B.3.</p>
<h2>B. 1 Performance Comparisons of Retrievers</h2>
<p>In Table 6, we compare existing symmetric and asymmetric retrievers named MPNet (Song et al.,</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Table 7: Results with varying instruction templates, for various LLMs on the WebQSP and Mintaka datasets.
2020) and TAS-B (Hofstätter et al., 2021), explained in Section A.4, on 1- and 2-hop retrievals. As shown in Table 6, we observe similar performances between symmetric (MPNet) and asymmetric (TAS-B) retrievers, which suggests that our simple graph-to-text verbalization is robust across different text-based retrieval schemes. Note that, since retrieval performances of both are similar, we conduct experiments mainly with MPNet, to reduce expensive computational costs for GPU usages.</p>
<h2>B. 2 Sensitivity Analyses on Template Texts</h2>
<p>Following the observation in Zhao et al. (2021), the performances of LLMs vary across different templates in the prompt. In our experiments, since it is computationally infeasible to try all different prompt templates on various LLMs, we consider two types of question templates, described in Appendix A.4. In particular, for the question $x$, we use either "Question: ${x}$ Answer: ", which we refer to as default template, or "Please answer the following question: ${x}$ ", referred to as please template. As shown in Table 7, for the T5 model, the default template is superior than the please template. Meanwhile, for the OPT model, the please template is superior than the other. However, for T0 and GPT-3 models, performance differences between default and please templates are marginal. Therefore, these results suggest that we may need to select instruction templates carefully across different LLMs for achieving optimal performances.</p>
<p>Additionally, regarding the knowledge-injection template described in Section 3.2, we also observe that the generation performance of GPT-3 depends on the instruction text in the template. In particular, we mainly conduct experiments with the template: "Below are facts in the form of the triple meaningful to answer the question."; however, we observe the performance degeneration when the augmented triples are irrelevant to the given question as shown</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Datasets</th>
<th style="text-align: center;">Methods</th>
<th style="text-align: center;">T5 (1:10)</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">T5 (1:10)</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">T5 (1:10)</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">GPT (1:70)</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">GPT (1:70)</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">GPT (1:10)</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Acc.</td>
<td style="text-align: center;">F1</td>
<td style="text-align: center;">EM</td>
<td style="text-align: center;">Acc.</td>
<td style="text-align: center;">F1</td>
<td style="text-align: center;">EM</td>
<td style="text-align: center;">Acc.</td>
<td style="text-align: center;">F1</td>
<td style="text-align: center;">EM</td>
<td style="text-align: center;">Acc.</td>
<td style="text-align: center;">F1</td>
<td style="text-align: center;">EM</td>
<td style="text-align: center;">Acc.</td>
<td style="text-align: center;">F1</td>
<td style="text-align: center;">EM</td>
<td style="text-align: center;">Acc.</td>
<td style="text-align: center;">F1</td>
<td style="text-align: center;">EM</td>
</tr>
<tr>
<td style="text-align: center;">WebQNP w/ Freebase</td>
<td style="text-align: center;">No Knowledge</td>
<td style="text-align: center;">6.95</td>
<td style="text-align: center;">5.20</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">13.40</td>
<td style="text-align: center;">8.11</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">9.48</td>
<td style="text-align: center;">8.25</td>
<td style="text-align: center;">0.06</td>
<td style="text-align: center;">19.85</td>
<td style="text-align: center;">7.20</td>
<td style="text-align: center;">6.38</td>
<td style="text-align: center;">29.77</td>
<td style="text-align: center;">10.60</td>
<td style="text-align: center;">0.06</td>
<td style="text-align: center;">28.38</td>
<td style="text-align: center;">7.92</td>
<td style="text-align: center;">0.70</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Random Knowledge</td>
<td style="text-align: center;">21.55</td>
<td style="text-align: center;">9.74</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">19.15</td>
<td style="text-align: center;">8.08</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">17.57</td>
<td style="text-align: center;">7.50</td>
<td style="text-align: center;">0.19</td>
<td style="text-align: center;">28.07</td>
<td style="text-align: center;">13.33</td>
<td style="text-align: center;">0.06</td>
<td style="text-align: center;">31.73</td>
<td style="text-align: center;">13.01</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">33.31</td>
<td style="text-align: center;">12.41</td>
<td style="text-align: center;">0.00</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Popular Knowledge</td>
<td style="text-align: center;">15.50</td>
<td style="text-align: center;">8.75</td>
<td style="text-align: center;">0.06</td>
<td style="text-align: center;">16.88</td>
<td style="text-align: center;">8.19</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">18.39</td>
<td style="text-align: center;">8.95</td>
<td style="text-align: center;">0.19</td>
<td style="text-align: center;">28.32</td>
<td style="text-align: center;">13.78</td>
<td style="text-align: center;">0.06</td>
<td style="text-align: center;">28.13</td>
<td style="text-align: center;">12.21</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">24.21</td>
<td style="text-align: center;">9.86</td>
<td style="text-align: center;">0.00</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Generated Knowledge</td>
<td style="text-align: center;">6.19</td>
<td style="text-align: center;">7.96</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">7.84</td>
<td style="text-align: center;">7.56</td>
<td style="text-align: center;">0.06</td>
<td style="text-align: center;">6.76</td>
<td style="text-align: center;">6.51</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">7.46</td>
<td style="text-align: center;">4.59</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">11.50</td>
<td style="text-align: center;">4.95</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">8.22</td>
<td style="text-align: center;">4.59</td>
<td style="text-align: center;">0.00</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">KAPING (Ours)</td>
<td style="text-align: center;">34.70</td>
<td style="text-align: center;">15.39</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">25.41</td>
<td style="text-align: center;">8.31</td>
<td style="text-align: center;">0.06</td>
<td style="text-align: center;">24.91</td>
<td style="text-align: center;">11.02</td>
<td style="text-align: center;">0.32</td>
<td style="text-align: center;">41.09</td>
<td style="text-align: center;">16.32</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">43.93</td>
<td style="text-align: center;">15.15</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">40.20</td>
<td style="text-align: center;">13.32</td>
<td style="text-align: center;">0.00</td>
</tr>
<tr>
<td style="text-align: center;">WebQNP w/ Wikibase</td>
<td style="text-align: center;">No Knowledge</td>
<td style="text-align: center;">10.30</td>
<td style="text-align: center;">5.60</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">18.42</td>
<td style="text-align: center;">8.48</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">15.21</td>
<td style="text-align: center;">8.94</td>
<td style="text-align: center;">0.07</td>
<td style="text-align: center;">23.94</td>
<td style="text-align: center;">7.80</td>
<td style="text-align: center;">6.48</td>
<td style="text-align: center;">33.77</td>
<td style="text-align: center;">11.41</td>
<td style="text-align: center;">0.07</td>
<td style="text-align: center;">32.46</td>
<td style="text-align: center;">8.45</td>
<td style="text-align: center;">0.71</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Random Knowledge</td>
<td style="text-align: center;">17.94</td>
<td style="text-align: center;">7.81</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">22.78</td>
<td style="text-align: center;">7.74</td>
<td style="text-align: center;">0.07</td>
<td style="text-align: center;">24.28</td>
<td style="text-align: center;">9.41</td>
<td style="text-align: center;">0.34</td>
<td style="text-align: center;">37.24</td>
<td style="text-align: center;">16.78</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">35.61</td>
<td style="text-align: center;">12.54</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">38.27</td>
<td style="text-align: center;">14.61</td>
<td style="text-align: center;">0.07</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Popular Knowledge</td>
<td style="text-align: center;">11.21</td>
<td style="text-align: center;">5.92</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">18.60</td>
<td style="text-align: center;">8.48</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">16.17</td>
<td style="text-align: center;">8.92</td>
<td style="text-align: center;">0.19</td>
<td style="text-align: center;">28.83</td>
<td style="text-align: center;">13.01</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">33.52</td>
<td style="text-align: center;">11.25</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">37.52</td>
<td style="text-align: center;">14.17</td>
<td style="text-align: center;">0.00</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Generated Knowledge</td>
<td style="text-align: center;">11.94</td>
<td style="text-align: center;">8.64</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">13.30</td>
<td style="text-align: center;">8.19</td>
<td style="text-align: center;">0.07</td>
<td style="text-align: center;">12.28</td>
<td style="text-align: center;">7.11</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">11.26</td>
<td style="text-align: center;">5.06</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">17.53</td>
<td style="text-align: center;">5.60</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">14.19</td>
<td style="text-align: center;">4.94</td>
<td style="text-align: center;">0.00</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">KAPING (Ours)</td>
<td style="text-align: center;">23.67</td>
<td style="text-align: center;">10.46</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">40.38</td>
<td style="text-align: center;">13.25</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">35.47</td>
<td style="text-align: center;">11.50</td>
<td style="text-align: center;">0.34</td>
<td style="text-align: center;">49.52</td>
<td style="text-align: center;">20.17</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">53.34</td>
<td style="text-align: center;">16.42</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">51.57</td>
<td style="text-align: center;">16.75</td>
<td style="text-align: center;">0.14</td>
</tr>
<tr>
<td style="text-align: center;">Mintake w/ Wikibase</td>
<td style="text-align: center;">No Knowledge</td>
<td style="text-align: center;">11.23</td>
<td style="text-align: center;">6.77</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">14.25</td>
<td style="text-align: center;">9.81</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">17.06</td>
<td style="text-align: center;">10.28</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">19.76</td>
<td style="text-align: center;">6.63</td>
<td style="text-align: center;">6.28</td>
<td style="text-align: center;">27.19</td>
<td style="text-align: center;">10.60</td>
<td style="text-align: center;">0.04</td>
<td style="text-align: center;">26.83</td>
<td style="text-align: center;">9.82</td>
<td style="text-align: center;">0.43</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Random Knowledge</td>
<td style="text-align: center;">17.59</td>
<td style="text-align: center;">10.48</td>
<td style="text-align: center;">0.18</td>
<td style="text-align: center;">18.19</td>
<td style="text-align: center;">9.24</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">18.83</td>
<td style="text-align: center;">9.82</td>
<td style="text-align: center;">0.57</td>
<td style="text-align: center;">28.11</td>
<td style="text-align: center;">14.47</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">26.58</td>
<td style="text-align: center;">12.80</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">28.36</td>
<td style="text-align: center;">14.02</td>
<td style="text-align: center;">0.11</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Popular Knowledge</td>
<td style="text-align: center;">17.56</td>
<td style="text-align: center;">9.88</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">18.09</td>
<td style="text-align: center;">10.47</td>
<td style="text-align: center;">0.07</td>
<td style="text-align: center;">18.73</td>
<td style="text-align: center;">10.07</td>
<td style="text-align: center;">0.53</td>
<td style="text-align: center;">26.97</td>
<td style="text-align: center;">13.76</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">27.08</td>
<td style="text-align: center;">12.95</td>
<td style="text-align: center;">0.07</td>
<td style="text-align: center;">23.10</td>
<td style="text-align: center;">11.28</td>
<td style="text-align: center;">0.00</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Generated Knowledge</td>
<td style="text-align: center;">13.61</td>
<td style="text-align: center;">9.23</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">14.61</td>
<td style="text-align: center;">8.85</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">14.29</td>
<td style="text-align: center;">7.51</td>
<td style="text-align: center;">0.04</td>
<td style="text-align: center;">11.87</td>
<td style="text-align: center;">6.34</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">14.96</td>
<td style="text-align: center;">5.81</td>
<td style="text-align: center;">0.04</td>
<td style="text-align: center;">16.24</td>
<td style="text-align: center;">7.14</td>
<td style="text-align: center;">0.00</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">KAPING (Ours)</td>
<td style="text-align: center;">19.72</td>
<td style="text-align: center;">11.36</td>
<td style="text-align: center;">0.04</td>
<td style="text-align: center;">22.00</td>
<td style="text-align: center;">11.17</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">22.85</td>
<td style="text-align: center;">10.91</td>
<td style="text-align: center;">0.43</td>
<td style="text-align: center;">32.94</td>
<td style="text-align: center;">14.99</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">32.37</td>
<td style="text-align: center;">14.37</td>
<td style="text-align: center;">0.04</td>
<td style="text-align: center;">33.37</td>
<td style="text-align: center;">14.65</td>
<td style="text-align: center;">0.11</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">T0 (1:10)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">T0 (1:10)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">AlexaTM (2:00)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">GPT-3 (4:70)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">GPT-3 (5:70)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Average</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Datasets</td>
<td style="text-align: center;">Methods</td>
<td style="text-align: center;">Acc.</td>
<td style="text-align: center;">F1</td>
<td style="text-align: center;">EM</td>
<td style="text-align: center;">Acc.</td>
<td style="text-align: center;">F1</td>
<td style="text-align: center;">EM</td>
<td style="text-align: center;">Acc.</td>
<td style="text-align: center;">F1</td>
<td style="text-align: center;">EM</td>
<td style="text-align: center;">Acc.</td>
<td style="text-align: center;">F1</td>
<td style="text-align: center;">EM</td>
<td style="text-align: center;">Acc.</td>
<td style="text-align: center;">F1</td>
<td style="text-align: center;">EM</td>
<td style="text-align: center;">Acc.</td>
<td style="text-align: center;">F1</td>
<td style="text-align: center;">EM</td>
</tr>
<tr>
<td style="text-align: center;">WebQNP w/ Freebase</td>
<td style="text-align: center;">No Knowledge</td>
<td style="text-align: center;">21.45</td>
<td style="text-align: center;">22.70</td>
<td style="text-align: center;">9.99</td>
<td style="text-align: center;">40.77</td>
<td style="text-align: center;">46.10</td>
<td style="text-align: center;">14.39</td>
<td style="text-align: center;">46.79</td>
<td style="text-align: center;">17.65</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">44.63</td>
<td style="text-align: center;">21.22</td>
<td style="text-align: center;">1.77</td>
<td style="text-align: center;">63.59</td>
<td style="text-align: center;">32.75</td>
<td style="text-align: center;">8.47</td>
<td style="text-align: center;">29.55</td>
<td style="text-align: center;">17.05</td>
<td style="text-align: center;">5.07</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Random Knowledge</td>
<td style="text-align: center;">32.02</td>
<td style="text-align: center;">36.48</td>
<td style="text-align: center;">26.55</td>
<td style="text-align: center;">51.20</td>
<td style="text-align: center;">55.98</td>
<td style="text-align: center;">46.90</td>
<td style="text-align: center;">57.37</td>
<td style="text-align: center;">20.91</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">51.01</td>
<td style="text-align: center;">28.04</td>
<td style="text-align: center;">6.19</td>
<td style="text-align: center;">65.87</td>
<td style="text-align: center;">41.28</td>
<td style="text-align: center;">18.46</td>
<td style="text-align: center;">37.22</td>
<td style="text-align: center;">22.43</td>
<td style="text-align: center;">8.94</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Popular Knowledge</td>
<td style="text-align: center;">27.05</td>
<td style="text-align: center;">31.38</td>
<td style="text-align: center;">20.23</td>
<td style="text-align: center;">57.22</td>
<td style="text-align: center;">52.44</td>
<td style="text-align: center;">42.64</td>
<td style="text-align: center;">54.91</td>
<td style="text-align: center;">26.45</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">55.58</td>
<td style="text-align: center;">25.94</td>
<td style="text-align: center;">4.87</td>
<td style="text-align: center;">62.26</td>
<td style="text-align: center;">38.84</td>
<td style="text-align: center;">17.60</td>
<td style="text-align: center;">33.48</td>
<td style="text-align: center;">20.98</td>
<td style="text-align: center;">7.68</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Generated Knowledge</td>
<td style="text-align: center;">19.41</td>
<td style="text-align: center;">23.15</td>
<td style="text-align: center;">10.56</td>
<td style="text-align: center;">38.81</td>
<td style="text-align: center;">43.43</td>
<td style="text-align: center;">31.23</td>
<td style="text-align: center;">35.13</td>
<td style="text-align: center;">14.42</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">45.89</td>
<td style="text-align: center;">27.98</td>
<td style="text-align: center;">9.48</td>
<td style="text-align: center;">62.14</td>
<td style="text-align: center;">38.79</td>
<td style="text-align: center;">17.57</td>
<td style="text-align: center;">22.67</td>
<td style="text-align: center;">16.72</td>
<td style="text-align: center;">6.26</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">KAPING (Ours)</td>
<td style="text-align: center;">52.28</td>
<td style="text-align: center;">55.27</td>
<td style="text-align: center;">48.04</td>
<td style="text-align: center;">62.85</td>
<td style="text-align: center;">66.11</td>
<td style="text-align: center;">58.53</td>
<td style="text-align: center;">67.67</td>
<td style="text-align: center;">23.16</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">60.37</td>
<td style="text-align: center;">32.89</td>
<td style="text-align: center;">8.34</td>
<td style="text-align: center;">73.89</td>
<td style="text-align: center;">43.15</td>
<td style="text-align: center;">26.67</td>
<td style="text-align: center;">47.94</td>
<td style="text-align: center;">27.28</td>
<td style="text-align: center;">12.36</td>
</tr>
<tr>
<td style="text-align: center;">WebQNP w/ Wikibase</td>
<td style="text-align: center;">No Knowledge</td>
<td style="text-align: center;">24.56</td>
<td style="text-align: center;">24.20</td>
<td style="text-align: center;">10.98</td>
<td style="text-align: center;">44.20</td>
<td style="text-align: center;">49.27</td>
<td style="text-align: center;">37.65</td>
<td style="text-align: center;">42.41</td>
<td style="text-align: center;">16.63</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">48.50</td>
<td style="text-align: center;">24.01</td>
<td style="text-align: center;">3.96</td>
<td style="text-align: center;">67.60</td>
<td style="text-align: center;">34.31</td>
<td style="text-align: center;">10.30</td>
<td style="text-align: center;">32.85</td>
<td style="text-align: center;">18.09</td>
<td style="text-align: center;">5.84</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Random Knowledge</td>
<td style="text-align: center;">28.85</td>
<td style="text-align: center;">33.08</td>
<td style="text-align: center;">22.37</td>
<td style="text-align: center;">47.68</td>
<td style="text-align: center;">52.34</td>
<td style="text-align: center;">42.50</td>
<td style="text-align: center;">55.63</td>
<td style="text-align: center;">19.88</td>
<td style="text-align: center;">0.06</td>
<td style="text-align: center;">52.05</td>
<td style="text-align: center;">25.37</td>
<td style="text-align: center;">2.18</td>
<td style="text-align: center;">60.64</td>
<td style="text-align: center;">36.88</td>
<td style="text-align: center;">13.92</td>
<td style="text-align: center;">38.27</td>
<td style="text-align: center;">21.49</td>
<td style="text-align: center;">7.41</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Popular Knowledge</td>
<td style="text-align: center;">24.83</td>
<td style="text-align: center;">27.89</td>
<td style="text-align: center;">16.03</td>
<td style="text-align: center;">48.02</td>
<td style="text-align: center;">52.84</td>
<td style="text-align: center;">41.88</td>
<td style="text-align: center;">53.92</td>
<td style="text-align: center;">19.77</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">47.41</td>
<td style="text-align: center;">24.36</td>
<td style="text-align: center;">3.75</td>
<td style="text-align: center;">63.37</td>
<td style="text-align: center;">37.08</td>
<td style="text-align: center;">14.73</td>
<td style="text-align: center;">34.83</td>
<td style="text-align: center;">20.78</td>
<td style="text-align: center;">8.96</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Generated Knowledge</td>
<td style="text-align: center;">22.92</td>
<td style="text-align: center;">25.28</td>
<td style="text-align: center;">11.80</td>
<td style="text-align: center;">41.34</td>
<td style="text-align: center;">45.70</td>
<td style="text-align: center;">33.83</td>
<td style="text-align: center;">31.16</td>
<td style="text-align: center;">13.36</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">48.77</td>
<td style="text-align: center;">29.72</td>
<td style="text-align: center;">11.19</td>
<td style="text-align: center;">65.89</td>
<td style="text-align: center;">39.52</td>
<td style="text-align: center;">17.87</td>
<td style="text-align: center;">26.42</td>
<td style="text-align: center;">17.56</td>
<td style="text-align: center;">6.80</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">KAPING (Ours)</td>
<td style="text-align: center;">49.86</td>
<td style="text-align: center;">50.75</td>
<td style="text-align: center;">41.27</td>
<td style="text-align: center;">58.73</td>
<td style="text-align: center;">61.90</td>
<td style="text-align: center;">53.27</td>
<td style="text-align: center;">65.04</td>
<td style="text-align: center;">22.72</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">60.44</td>
<td style="text-align: center;">31.18</td>
<td style="text-align: center;">6.82</td>
<td style="text-align: center;">69.58</td>
<td style="text-align: center;">41.63</td>
<td style="text-align: center;">19.71</td>
<td style="text-align: center;">30.69</td>
<td style="text-align: center;">27.01</td>
<td style="text-align: center;">11.05</td>
</tr>
<tr>
<td style="text-align: center;">Mintake w/ Wikibase</td>
<td style="text-align: center;">No Knowledge</td>
<td style="text-align: center;">16.75</td>
<td style="text-align: center;">20.84</td>
<td style="text-align: center;">11.34</td>
<td style="text-align: center;">23.74</td>
<td style="text-align: center;">28.69</td>
<td style="text-align: center;">20.86</td>
<td style="text-align: center;">41.97</td>
<td style="text-align: center;">17.05</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">34.65</td>
<td style="text-align: center;">17.67</td>
<td style="text-align: center;">2.31</td>
<td style="text-align: center;">56.33</td>
<td style="text-align: center;">26.77</td>
<td style="text-align: center;">6.11</td>
<td style="text-align: center;">26.16</td>
<td style="text-align: center;">14.99</td>
<td style="text-align: center;">3.76</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Random Knowledge</td>
<td style="text-align: center;">18.10</td>
<td style="text-align: center;">21.09</td>
<td style="text-align: center;">14.14</td>
<td style="text-align: center;">26.13</td>
<td style="text-align: center;">31.70</td>
<td style="text-align: center;">22.85</td>
<td style="text-align: center;">46.02</td>
<td style="text-align: center;">17.02</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">32.98</td>
<td style="text-align: center;">17.55</td>
<td style="text-align: center;">1.59</td>
<td style="text-align: center;">51.96</td>
<td style="text-align: center;">29.58</td>
<td style="text-align: center;">6.28</td>
<td style="text-align: center;">28.22</td>
<td style="text-align: center;">16.92</td>
<td style="text-align: center;">4.14</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Popular Knowledge</td>
<td style="text-align: center;">16.74</td>
<td style="text-align: center;">23.13</td>
<td style="text-align: center;">14.53</td>
<td style="text-align: center;">27.15</td>
<td style="text-align: center;">32.17</td>
<td style="text-align: center;">23.45</td>
<td style="text-align: center;">46.41</td>
<td style="text-align: center;">17.31</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">32.48</td>
<td style="text-align: center;">20.07</td>
<td style="text-align: center;">4.41</td>
<td style="text-align: center;">53.16</td>
<td style="text-align: center;">27.44</td>
<td style="text-align: center;">6.86</td>
<td style="text-align: center;">27.95</td>
<td style="text-align: center;">17.14</td>
<td style="text-align: center;">4.54</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Generated Knowledge</td>
<td style="text-align: center;">14.46</td>
<td style="text-align: center;">20.08</td>
<td style="text-align: center;">11.98</td>
<td style="text-align: center;">23.13</td>
<td style="text-align: center;">27.34</td>
<td style="text-align: center;">18.76</td>
<td style="text-align: center;">34.58</td>
<td style="text-align: center;">14.91</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">33.12</td>
<td style="text-align: center;">18.29</td>
<td style="text-align: center;">3.09</td>
<td style="text-align: center;">55.65</td>
<td style="text-align: center;">30.69</td>
<td style="text-align: center;">11.73</td>
<td style="text-align: center;">22.41</td>
<td style="text-align: center;">14.20</td>
<td style="text-align: center;">4.15</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">KAPING (Ours)</td>
<td style="text-align: center;">20.68</td>
<td style="text-align: center;">27.80</td>
<td style="text-align: center;">18.12</td>
<td style="text-align: center;">29.50</td>
<td style="text-align: center;">34.83</td>
<td style="text-align: center;">26.23</td>
<td style="text-align: center;">49.08</td>
<td style="text-align: center;">17.90</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">35.61</td>
<td style="text-align: center;">20.80</td>
<td style="text-align: center;">3.79</td>
<td style="text-align: center;">56.86</td>
<td style="text-align: center;">28.63</td>
<td style="text-align: center;">7.64</td>
<td style="text-align: center;">32.27</td>
<td style="text-align: center;">18.86</td>
<td style="text-align: center;">5.31</td>
</tr>
</tbody>
</table>
<p>Table 8: LM prompting results with additional metrics: F1 and Exact Match (EM), along with accuracy (Acc.) scores.
in Figure 3. Therefore, to improve the performance on incorrect retrievals, we further experiment with the additional template: "Below are facts in the form of the triple that might be meaningful to answer the question.". Then, the GPT-3 (175B) model with the previous template achieves 74.16 and 42.80 accuracies for correct and incorrect retrievals, respectively. Meanwhile, the same model with the instruction template containing "might be" achieves 72.91 and 51.38 accuracies for correct and incorrect retrievals, respectively. Thus, these results suggest that the knowledge-injection template with "might be" statement makes the model less selective on the augmented triples while focusing more on the internalized knowledge in parameters, thus improving the incorrect retrieval performance while degenerating the correct retrieval.</p>
<h2>B. 3 Additional Evaluation Metrics</h2>
<p>As described in Section 4.4, we evaluate the performance of LLMs based on whether generated tokens for the input question contain answer entities or not. This is because, as explained in Section 6 of the limitation, pre-trained LLMs without further finetuning tend to generate the answer as the sentence, while the answer for the KGQA task is the entity consisting of few tokens. In this subsection, we further provide experiment results with additional evaluation metrics (Rajpurkar et al., 2016), namely F1 and Exact Match (EM) scores. Note that they are frequently used for evaluating extractive QA
models, whose goal is to classify the answer span in the given context, without generation. As shown in Table 8, since the F1 score penalizes the longer sentence too much, the performances of LLMs evaluated by F1 scores are largely decreasing, except for the T0 model that is further fine-tuned by prompted text-to-text tasks, including QA, thus capable of generating entity-level outputs. Similarly, except for the T0, it is highly suboptimal to evaluate the performance of prompted LMs with EM scores, due to differences in output lengths. Thus, it would be promising direction to further develop better evaluation metrics for KGQA under LM prompting schemes, which we leave as future work.</p>
<p>While such F1 and EM scores, used for extractive QA tasks, might be suboptimal to evaluate generative LM prompting schemes, our KAPING framework consistently outperforms all the other baselines based on averaged F1 and EM scores as well, by large margins. Note that the superior EM and F1 scores of the generated knowledge baseline with GPT-3 on few cases, even though they are rarely happen, is because, for this baseline, the GPT-3 model generates entity-level outputs, unlike ours that generates sentence-level outputs. In other words, the sentence-level outputs from our KAPING is often longer than the answer entities, since our model is grounded by retrieved facts from KGs as shown in Table 15; however, longer sentences penalize F1 and EM scores. More specifically, the average number of output sequence lengths of the</p>
<table>
<thead>
<tr>
<th style="text-align: left;">LLMs</th>
<th style="text-align: left;">Models</th>
<th style="text-align: center;">Correct</th>
<th style="text-align: center;">Semi-Correct</th>
<th style="text-align: center;">Incorrect</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">T0 (3B)</td>
<td style="text-align: left;">No Knowledge</td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">22</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">KAPING (Ours)</td>
<td style="text-align: center;">17</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">13</td>
</tr>
<tr>
<td style="text-align: left;">T0 (11B)</td>
<td style="text-align: left;">No Knowledge</td>
<td style="text-align: center;">14</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">16</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">KAPING (Ours)</td>
<td style="text-align: center;">20</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">10</td>
</tr>
<tr>
<td style="text-align: left;">GPT-3 (6.7B)</td>
<td style="text-align: left;">No Knowledge</td>
<td style="text-align: center;">12</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">14</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">KAPING (Ours)</td>
<td style="text-align: center;">19</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">17</td>
</tr>
<tr>
<td style="text-align: left;">GPT-3 (175B)</td>
<td style="text-align: left;">No Knowledge</td>
<td style="text-align: center;">22</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">7</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">KAPING (Ours)</td>
<td style="text-align: center;">26</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">3</td>
</tr>
</tbody>
</table>
<p>Table 9: Human evaluation results, where we randomly sample 30 examples from the WebQSP w/ Freebase dataset.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Models</th>
<th style="text-align: left;">Shots</th>
<th style="text-align: center;">T5 (3B)</th>
<th style="text-align: center;">OPT (6.7B)</th>
<th style="text-align: center;">T0 (11B)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">No Knowledge</td>
<td style="text-align: left;">Zero-Shot</td>
<td style="text-align: center;">18.42</td>
<td style="text-align: center;">33.77</td>
<td style="text-align: center;">44.20</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">One-Shot</td>
<td style="text-align: center;">18.28</td>
<td style="text-align: center;">36.90</td>
<td style="text-align: center;">41.13</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Three-Shots</td>
<td style="text-align: center;">17.87</td>
<td style="text-align: center;">37.65</td>
<td style="text-align: center;">37.38</td>
</tr>
<tr>
<td style="text-align: left;">KAPING (Ours)</td>
<td style="text-align: left;">Zero-Shot</td>
<td style="text-align: center;">40.38</td>
<td style="text-align: center;">53.34</td>
<td style="text-align: center;">58.73</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">One-Shot</td>
<td style="text-align: center;">18.42</td>
<td style="text-align: center;">52.25</td>
<td style="text-align: center;">48.70</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Three-Shots</td>
<td style="text-align: center;">10.16</td>
<td style="text-align: center;">50.34</td>
<td style="text-align: center;">43.45</td>
</tr>
</tbody>
</table>
<p>Table 10: KGQA results with few-shot learning. We vary the number of examples (i.e., shots) in the prompt, and report the performances on the WebQSP w/ Wikidata dataset.
generated knowledge model is 67.77 , meanwhile, ours is 74.92 . However, when we compare the generated knowledge baseline to our KAPING with other LLMs but also with other metrics, our KAPING significantly outperforms this baseline.</p>
<p>Human Evaluation Additionally, similar to the previous generative QA work (Roberts et al., 2020), we manually inspect 30 samples from the WebQSP w/ Freebase dataset, to see whether the generated sentence is factually correct to the input question. For this experiment, we evaluate four LLMs: T0 (3B), T0 (11B), GPT-3 (6.7B), and GPT-3 (175B), with no knowledge baseline and our KAPING. Also, we use three different ratings for each generation example: 1) we label it as correct if all information in the generated sentence is factually correct to the question; 2) we label it as semi-correct if some information in the generated sentence is factually incorrect which yet contains at least one answer entity; 3) we label it as incorrect for all the other cases. As shown in Table 9, we observe that our KAPING framework can generate the factually correct answer more, compared to the no knowledge baseline, which are consistent with the results from available evaluation metrics in Table 1 and Table 8. We provide generated answers, which we use for human evaluation in Table 9, for GPT-3 (175B) and T0 (3B) models in Table 15 and Table 16.</p>
<h2>B. 4 Performances of Few-Shot Learning</h2>
<p>While the focus of our work is zero-shot as outlined in the main paper, in this subsection, we additionally extend this zero-shot setting to the few-shot</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Retrievers</th>
<th style="text-align: left;">MRR</th>
<th style="text-align: center;">Top-1</th>
<th style="text-align: center;">Top-10</th>
<th style="text-align: center;">Top-30</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Random Retrieval</td>
<td style="text-align: left;">9.50</td>
<td style="text-align: center;">3.62</td>
<td style="text-align: center;">22.58</td>
<td style="text-align: center;">40.72</td>
</tr>
<tr>
<td style="text-align: left;">Popular Retrieval</td>
<td style="text-align: left;">8.52</td>
<td style="text-align: center;">4.57</td>
<td style="text-align: center;">15.89</td>
<td style="text-align: center;">35.47</td>
</tr>
<tr>
<td style="text-align: left;">Retrieval with Free-Form Texts</td>
<td style="text-align: left;">41.33</td>
<td style="text-align: center;">31.11</td>
<td style="text-align: center;">62.07</td>
<td style="text-align: center;">69.92</td>
</tr>
<tr>
<td style="text-align: left;">Retrieval with Triple-Form Texts</td>
<td style="text-align: left;">43.46</td>
<td style="text-align: center;">33.36</td>
<td style="text-align: center;">64.39</td>
<td style="text-align: center;">70.67</td>
</tr>
</tbody>
</table>
<p>Table 11: Retrieval results with different verbalizers. We use the graph-to-text transformation model proposed in Ma et al. (2022) for obtaining free-form texts. For triple-form texts, we use the verbalization technique described in Section 3.2. MPNet (Song et al., 2020) is used as the retriever, and the performance is reported on WebQSP w/ Wikidata.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Retrievers</th>
<th style="text-align: left;">T5 (3B)</th>
<th style="text-align: left;">OPT (6.7B)</th>
<th style="text-align: left;">T0 (3B)</th>
<th style="text-align: left;">T0 (11B)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">No Knowledge</td>
<td style="text-align: left;">18.42</td>
<td style="text-align: left;">33.77</td>
<td style="text-align: left;">24.56</td>
<td style="text-align: left;">44.20</td>
</tr>
<tr>
<td style="text-align: left;">KAPING with Free-Form Texts</td>
<td style="text-align: left;">43.25</td>
<td style="text-align: left;">53.00</td>
<td style="text-align: left;">47.75</td>
<td style="text-align: left;">53.21</td>
</tr>
<tr>
<td style="text-align: left;">KAPING with Triple-Form Texts</td>
<td style="text-align: left;">40.38</td>
<td style="text-align: left;">53.34</td>
<td style="text-align: left;">49.86</td>
<td style="text-align: left;">58.73</td>
</tr>
</tbody>
</table>
<p>Table 12: KGQA results with different verbalizers. We use the graph-to-text transformation model proposed in Ma et al. (2022) for obtaining free-form texts. For triple-form texts, we use the verbalization technique described in Section 3.2. We then inject the verbalized triples in the input prompt. We report the generation accuracy on WebQSP w/ Wikidata.
setting, where we prepend the few examples about the input-output pairs in the prompt of LLMs. As shown in Table 10, for the KGQA task, the performances are decreasing when we increase the number of samples (i.e., shots) in the input prompt, except for the OPT model. We suggest this might be because, the injected examples in the prompt are less relevant to the given factual question, misleading the model to focus on unrelated contexts on the injected examples. This phenomenon is even more severe in our KAPING framework; this is similarly because our KAPING augments the retrieved facts, and if the facts on the other few-shot examples are further injected in the input prompt, the model is more likely to be confused by those irrelevant facts. For the OPT model, we observe a slight performance improvement in the No Knowledge model, since few injected examples provide a hint on how the output format looks like. We leave further extending our zero-shot KAPING framework to the few-shot learning mechanism as future work.</p>
<h2>B. 5 Analyses on Knowledge Verbalization</h2>
<p>As described in the Knowledge Verbalization paragraph of Section 3.2, we use the linear triple verbalization technique, which simply concatenates the tokens of subject, relation, and object in the triple, instead of using the sophisticated techniques that use the particular graph-to-text transformation methods (Oguz et al., 2022; Ma et al., 2022). This is because, we observe that our simple verbalization technique works well, and, in this subsection, we concretely show performance differences between our and existing verbalization techniques in</p>
<table>
<thead>
<tr>
<th>Models</th>
<th># of Augmented Knowledge</th>
<th>Relative Time</th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td>T5 (0.8B)</td>
<td>T5 (3B)</td>
<td>T5 (11B)</td>
<td>OPT (2.7B)</td>
<td>OPT (6.7B)</td>
<td>OPT (13B)</td>
<td>T0 (3B)</td>
<td>T0 (11B)</td>
</tr>
<tr>
<td>No Knowledge</td>
<td>0</td>
<td>1.00</td>
<td>1.00</td>
<td>1.00</td>
<td>1.00</td>
<td>1.00</td>
<td>1.00</td>
<td>1.00</td>
<td>1.00</td>
</tr>
<tr>
<td>Document (Web) Augmentation</td>
<td>1</td>
<td>1.20</td>
<td>1.45</td>
<td>2.13</td>
<td>1.43</td>
<td>1.65</td>
<td>1.61</td>
<td>1.60</td>
<td>2.29</td>
</tr>
<tr>
<td></td>
<td>4</td>
<td>2.70</td>
<td>4.16</td>
<td>6.80</td>
<td>3.42</td>
<td>3.90</td>
<td>3.66</td>
<td>2.98</td>
<td>9.01</td>
</tr>
<tr>
<td></td>
<td>10</td>
<td>OOL</td>
<td>OOL</td>
<td>OOL</td>
<td>6.44</td>
<td>7.36</td>
<td>6.97</td>
<td>OOL</td>
<td>OOL</td>
</tr>
<tr>
<td></td>
<td>15</td>
<td>OOL</td>
<td>OOL</td>
<td>OOL</td>
<td>9.35</td>
<td>10.71</td>
<td>OOM</td>
<td>OOL</td>
<td>OOL</td>
</tr>
<tr>
<td></td>
<td>30</td>
<td>OOL</td>
<td>OOL</td>
<td>OOL</td>
<td>OOL</td>
<td>OOL</td>
<td>OOL</td>
<td>OOL</td>
<td>OOL</td>
</tr>
<tr>
<td>KAPING (Ours)</td>
<td>1</td>
<td>1.08</td>
<td>0.97</td>
<td>1.35</td>
<td>1.12</td>
<td>1.21</td>
<td>1.19</td>
<td>0.49</td>
<td>1.28</td>
</tr>
<tr>
<td></td>
<td>4</td>
<td>1.22</td>
<td>1.50</td>
<td>2.13</td>
<td>1.48</td>
<td>1.65</td>
<td>1.60</td>
<td>0.73</td>
<td>2.18</td>
</tr>
<tr>
<td></td>
<td>10</td>
<td>1.53</td>
<td>2.10</td>
<td>3.11</td>
<td>1.89</td>
<td>2.20</td>
<td>2.10</td>
<td>1.07</td>
<td>3.83</td>
</tr>
<tr>
<td></td>
<td>15</td>
<td>1.84</td>
<td>2.74</td>
<td>4.02</td>
<td>2.36</td>
<td>2.76</td>
<td>2.58</td>
<td>1.54</td>
<td>4.59</td>
</tr>
<tr>
<td></td>
<td>30</td>
<td>2.82</td>
<td>4.42</td>
<td>6.05</td>
<td>3.77</td>
<td>4.28</td>
<td>4.06</td>
<td>2.49</td>
<td>7.76</td>
</tr>
</tbody>
</table>
<p>Table 13: Efficiencies results, where we measure the wall-clock time of every model for generating answers on the WebQSP w/ Wikidata dataset. The document augmentation model (Lazaridou et al., 2022) augments documents listed in their paper, meanwhile, ours augments relevant triples to the question retrieved from KGs. We set the maximum number of input sequences for T5 and T0 models as 1,024, and for OPT as 2,048. OOL denotes the out-of-length errors, where the input prompt length exceeds the maximum input token lengths. OOM denotes the out-of-memory error on the machine having eight V100 GPUs. both the knowledge retrieval and injection steps. Note that, for the comparison, we use the trained knowledge verbalizer proposed in Ma et al. (2022).</p>
<p>We first provide the fact retrieval performances across the different knowledge verbalization methods in Table 11. As shown in Table 11, we observe that our simple triple-form text verbalization is superior to the free-form text verbalization in the fact retrieval. This might be because the free-form verbalization model, transforming the graph to the text, might generate the incorrect output that is semantically different from the original triple, leading to the degenerated retrieval performances.</p>
<p>On the other hand, we also report the generation results of KGQA with two different knowledge verbalizers on our KAPING framework in Table 12. As shown in Table 12, we observe that the performances between the free-form texts and the triple-form texts are comparable when augmented to LLMs with our KAPING framework. More specifically, for the T5 model, which is pre-trained on the unlabeled corpus without additional instruction tuning, the free-form text works well. Meanwhile, for the T0 model, which is further fine-tuned with natural language instruction tasks, it is beneficial to use our linear triple verbalizaton scheme.</p>
<h3>B.6 Additional Efficiency Comparisons</h3>
<p>In this subsection, we further provide efficiency results of all LLMs that we use in our main experiments across three different models: no knowledge model, document augmentation (i.e., web augmentation) model (Lazaridou et al., 2022), and our KAPING framework. We note that, as discussed in the Knowledge-Augmented LMs paragraph of Section 2, the web augmentation method augments documents searched from Google with the fewshot learning setup. However, as we discuss there, this web augmentation is orthogonal to ours, since we use the completely different knowledge source (i.e., KGs) and our work is under the zero-shot learning setup; from which our core mechanisms of how to retrieve and augment relevant knowledge with LM prompting is clearly different and novel. Furthermore, as discussed in Section 2, this web augmentation method is infeasible to experimentally compare as well, since individual researches cannot freely access the Google Search API to retrieve documents for every question in the world. Also, it is computationally expensive to augment documents consisting of hundreds to thousands tokens (Lazaridou et al., 2022) in LLMs, unlike our triple cases consisting of few tokens. In this subsection, to experimentally validate the latter issue, we further make the comparisons of computational costs between document augmentation and our fact augmentation. In particular, as shown in Table 13, the answer generation speed of the web augmentation mechanism is significantly slower than our triple augmentation mechanism, since it requires more time to encode and condition documents in the input prompt compared to triples. Also, following the original paper (Lazaridou et al., 2022), the suggested number of documents to augment is 15 , however, in the most cases, we observe out-oflength (OOL) errors, since the length of the input prompt with 15 documents is longer than the maximum input sequence length of LLMs. While our fact augmentation scheme is slower than the model without augmentation, we believe that, given the substantially improved performance in Table 1 and the high efficiency compared to document augmentation in Table 13, KAPING is highly beneficial.</p>
<h3>B. 7 Result Analyses Across Question Types</h3>
<p>For the Mintaka dataset (Sen et al., 2022), each question is belong to one of the following categories: Generic, Multihop, Intersection, Difference, Comparative, Superlative, Ordinal, Count, and Yes/No, which defines the complexity of ques-</p>
<p>tions. Therefore, to see which complexity category our knowledge-augmentation framework is helpful, and which category we should further improve on, we breakdown the performance of LLMs according to question types in Table 14. Note that, following the evaluation protocol in Section A. 3 where we filter out questions that do not have answer names, the Yes/No type questions are not considered.</p>
<p>As shown in the last row of Table 14 where we average the performance of all LLMs per category, our KAPING framework brings significant performance improvements on all categories except for the Comparative type. One particular comparativetype question is "Who has won more NBA Season MVPs, LeBron James or Steph Curry", and, since it is hard to retrieve and associate relevant triples for such the comparative-type question, our KAPING underperforms simple knowledge-injection baselines: random knowledge and popular knowledge. However, the KG-augmented models (e.g., random knowledge, popular knowledge, and our KAPING) outperform other baselines, which suggests that knowledge-augmentation mechanism is meaningful to tackle comparative questions, and one might further improve the retrieval scheme or the input prompt itself, which we leave as future work.</p>
<p>On the other point we would like to mention is that, for the Count category, performances of T0 models are significantly low compared to other LLMs. This is surprising, since T0 models are further fine-tuned on the prompted text-to-text tasks, and they have strong performances on the other categories, thanks to fine-tuning. We believe such the low performance on the Count category is because, in the fine-tuning of T0 models, there are no prompted tasks related to counting, which makes T0 models hard to count particular instances. Therefore, to further improve the generalization performance of T0 models, one may additionally include more diverse prompted tasks, including the counting one, during the fine-tuning process.</p>
<h2>B. 8 Generation Examples</h2>
<p>We provide generation examples for comparisons between the no knowledge baseline and our KAPING framework in Table 15 and Table 16 for GPT3 and T0 language models, respectively. We also provide retrieved and generation examples of our KAPING framework with four different LLMs: T5 (11B), OPT (13B), T0 (11B), and GPT-3 (175B) on the WebQSP w/ Wikidata dataset in Table 17.</p>
<h2>C Discussions on Prompt Design/Tuning</h2>
<p>We discuss differences between prompt design and prompt tuning, along with additional relevant work in the prompt tuning literature. As described in Section 3.1, given an input question, the large language model can generate the answer text, which is called LM prompting (Brown et al., 2020; Liu et al., 2021). However, to further enhance the performance of models under the LM prompting scheme, prior work particularly designs the content in the prompt, which is called prompt design (Shin et al., 2020; Lu et al., 2022). More specifically, Shin et al. (2020) additionally include the particular trigger tokens, meaningful to the down-stream tasks, in the prompt, and Lu et al. (2022) change the order of demonstrations in the prompt under the few-shot LM prompting setup. Our method is in line with such the prompt design literature, and we introduce the method of knowledge augmentation in the input prompt with facts from KGs, to allow LLMs condition on factual knowledge for zero-shot QA.</p>
<p>On the other hand, there exists prompt tuning literature (Lester et al., 2021a), which additionally trains the prompt-relevant parameters with supervised learning objectives, while keeping the parameters of LLMs unchanged. While this prompt tuning approach can be beneficial in few-shot learning scenarios where the model is additionally tuned with few training examples, it is not suitable for our zero-shot learning. Also, unlike the prompt design approach, it is difficult to interpret and manipulate the prompt represented in the embedding space.</p>
<p>Note that, recently, there are few knowledgeaware prompt tuning work (Chen et al., 2022b; Hu et al., 2022; Chen et al., 2022a), and, while they are fundamentally different from our LM prompting (i.e., prompt design), we additionally discuss them. First of all, Chen et al. (2022b) tackle the relation extraction problem with prompt tuning, where they propose to embed the particular words related to the relation class in the embedding space. For example, for the relation type to classify: "county of birth", they embed person and country information in the representation space with training signals from supervised learning, for improved relation classification performance. Also, Hu et al. (2022) tackle the text classification task with prompt tuning, where they propose to not only consider the classification label word itself, but also the label word's related words. For example, for the sentence label "science", they further consider its related words:</p>
<p>"physics" and "mathematics", defined in particular knowledge bases, such as WordNet (Pedersen et al., 2004) and ConceptNet (Speer et al., 2017). Lastly, Chen et al. (2022a) tackle the similar text classification task with prompt tuning, where they propose to retrieve the data instance (i.e., a sentence and its label) in the training dataset based on the retriever training with supervised classification objectives.</p>
<p>However, all the above knowledge-aware prompt tuning methods are clearly different from our proposed KAPING framework. At first, they are restricted to cloze-style prediction, in which they first include the particular mask token in the input prompt, and then classify the label (e.g., sentiment of the sentence, or relation in the given sentence) of the mask token, similar to the masked language modeling objective (Devlin et al., 2019; Liu et al., 2019). Therefore, their cloze-style prediction schemes cannot be used for QA tasks, since the answer of the user's question is not the single token, and it is unclear to convert the predicted label token from the masked token to all different answers in the world. In contrast to them, our KAPING does not rely on the masked token classification scheme, thus ours is more flexible, and not restricted to cloze-style classification; suitable for answering any user's questions. Furthermore, some of them (Chen et al., 2022a,b) rely on training signals from the training dataset with supervised learning, meanwhile, ours is completely zero-shot. While Chen et al. (2022a) show the model's zeroshot ability, they require the training dataset as discussed in their paper, thus not suitable for our zero-shot QA as well. Lastly, we augment the factual knowledge by matching the entity in the question to its associated triples in KGs, however, prior work considers different knowledge source, which might not be helpful for QA tasks, such as relationships between words (Hu et al., 2022), relationships between the relation class and particular words (Chen et al., 2022b), and a pair of sentence and its label in training data (Chen et al., 2022a).</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{3}$ https://huggingface.co/sentence-transformers/all-mpnet-base-v2&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
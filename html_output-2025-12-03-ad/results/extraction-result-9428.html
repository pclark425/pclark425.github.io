<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9428 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9428</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9428</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-162.html">extraction-schema-162</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using language models for detecting anomalies in lists, sequences, or tabular data, including details of the models, data types, methods, results, comparisons, and limitations.</div>
                <p><strong>Paper ID:</strong> paper-30d1f759ef4bd54ee96e95fb5da4df41d0bc2c7d</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/30d1f759ef4bd54ee96e95fb5da4df41d0bc2c7d" target="_blank">MultiGED-2023 shared task at NLP4CALL: Multilingual Grammatical Error Detection</a></p>
                <p><strong>Paper Venue:</strong> NLP4CALL</p>
                <p><strong>Paper Abstract:</strong> This paper reports on the NLP4CALL shared task on Multilingual Grammatical Error Detection (MultiGED-2023), which included five languages: Czech, English, German, Italian and Swedish. It is the first shared task organized by the Computational SLA 1 working group, whose aim is to promote less represented languages in the fields of Grammatical Error Detection and Correction, and other related fields. The MultiGED datasets have been produced based on second language (L2) learner corpora for each particular language. In this paper we introduce the task as a whole, elaborate on the dataset generation process and the design choices made to obtain MultiGED datasets, provide details of the evaluation metrics and CodaLab setup. We further briefly describe the systems used by participants and report the results.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9428.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9428.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using language models for detecting anomalies in lists, sequences, or tabular data, including details of the models, data types, methods, results, comparisons, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>EliCoDe (XLM-R)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>EliCoDe system (fine-tuned XLM-RoBERTa per-language)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Team EliCoDe fine-tuned separate XLM-RoBERTa models (one per dataset/language) for token-level binary grammatical-error detection, achieving the best average performance across MultiGED languages.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Et.tCoDt: at MultiGED2023: fine-tuning XLM-RoBERTa for multilingual grammatical error detection</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>XLM-RoBERTa (large)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Transformer (multilingual masked-language model)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>large (not specified in paper, referred to as XLM-RoBERTa (large))</td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>token sequences (one token per row; tabular CoNLL-style format)</td>
                        </tr>
                        <tr>
                            <td><strong>data_domain</strong></td>
                            <td>L2 learner essays (multilingual: Czech, English, German, Italian, Swedish)</td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_type</strong></td>
                            <td>token-level learner errors (grammatical/orthographic/lexical/morphological/syntactic errors)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Fine-tune pre-trained XLM-RoBERTa per language on token-level binary classification (labels 'c' or 'i'), with a stacked linear classifier and dropout on top.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_methods</strong></td>
                            <td>Monolingual/multilingual LSTM-based systems, character-based LSTM, majority vote ensemble, and previously published BERT/ELECTRA/XLNet baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Precision (P), Recall (R), F0.5 (token-based)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Top per-language F0.5 scores from EliCoDe: German 82.32, Italian 82.15, Swedish 78.16, Czech 73.44, English-FCE 67.40 (reported in Table 5 and Table 4).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baseline</strong></td>
                            <td>Substantially better than LSTM/char-LSTM approaches on most languages; monolingual fine-tuned XLM-R models (EliCoDe) outperformed the single multilingual XLM-R model fine-tuned on all datasets (DSL-MIM-HUS) for most languages, except English-REALEC.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Performance varies by dataset split and annotation paradigm; lower performance on English-REALEC (no training split and different annotation style). Token-based F0.5 evaluation inflates multi-token edit rewards. Real-word errors are notably harder to detect than non-word (OOV) errors.</td>
                        </tr>
                        <tr>
                            <td><strong>unique_insights</strong></td>
                            <td>Per-language fine-tuning of a large multilingual LM yielded top results, but monolingual vs. multilingual fine-tuning trade-offs exist; models generalize poorly to corpora with different annotation conventions or without in-domain training data.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MultiGED-2023 shared task at NLP4CALL: Multilingual Grammatical Error Detection', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9428.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9428.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using language models for detecting anomalies in lists, sequences, or tabular data, including details of the models, data types, methods, results, comparisons, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DSL-MIM-HUS (XLM-R joint)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DSL-MIM-HUS team (single multilingual XLM-RoBERTa fine-tuned jointly)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>This team fine-tuned a single pre-trained XLM-RoBERTa model jointly on all MultiGED datasets to perform token-level error detection across languages.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Two Neural Models for Multilingual Grammatical Error Detection</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>XLM-RoBERTa</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Transformer (multilingual masked-language model)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>unspecified (multilingual XLM-RoBERTa from HuggingFace)</td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>token sequences (tabular per-token labels)</td>
                        </tr>
                        <tr>
                            <td><strong>data_domain</strong></td>
                            <td>L2 learner essays across multiple languages</td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_type</strong></td>
                            <td>token-level learner errors (grammatical/orthographic/lexical/morphological/syntactic)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Fine-tune a single pre-trained XLM-RoBERTa model jointly on all provided language datasets for token-level binary classification.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_methods</strong></td>
                            <td>Per-language fine-tuned large LMs (EliCoDe), LSTM-based models, character LSTM (VLP-char), majority vote.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Precision, Recall, F0.5</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>DSL-MIM-HUS achieved best F0.5 on English-REALEC (50.86/50.87). On other languages their scores were competitive but generally lower than EliCoDe (e.g., Czech F0.5 57.76, English-FCE 61.18, German 70.75, Italian 63.55, Swedish 66.05).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baseline</strong></td>
                            <td>Joint multilingual fine-tuning produced competitive results but generally underperformed per-language fine-tuning (EliCoDe) except where there was no in-domain training data (REALEC), where the joint model outperformed language-specific models.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Joint multilingual fine-tuning can underperform language-specific fine-tuning when in-domain training data exist; annotation differences across corpora hurt cross-dataset generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>unique_insights</strong></td>
                            <td>A single multilingual model fine-tuned on all languages can generalize better to datasets without training splits (e.g., REALEC) than per-language models, indicating benefits of shared multilingual representations when in-domain labeled data is unavailable.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MultiGED-2023 shared task at NLP4CALL: Multilingual Grammatical Error Detection', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9428.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9428.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using language models for detecting anomalies in lists, sequences, or tabular data, including details of the models, data types, methods, results, comparisons, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Brainstorm (mBERT)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Brainstorm Thinkers system (mBERT-based)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A participating team used multilingual BERT (mBERT) to perform token-level grammatical error detection across datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>mBERT (multilingual BERT)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Transformer (multilingual BERT)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>unspecified</td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>token sequences (tabular per-token labels)</td>
                        </tr>
                        <tr>
                            <td><strong>data_domain</strong></td>
                            <td>L2 learner essays</td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_type</strong></td>
                            <td>token-level grammatical/orthographic/lexical errors</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Fine-tune mBERT for sequence labeling / token classification on the GED datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_methods</strong></td>
                            <td>Compared against XLM-RoBERTa-based approaches and LSTM-based systems.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Precision, Recall, F0.5</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Performance was moderate; example scores include Czech F0.5 46.81, English-FCE F0.5 59.81, German F0.5 69.11, Italian F0.5 59.49, Swedish F0.5 63.11 (see Table 4).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baseline</strong></td>
                            <td>mBERT underperformed compared to top XLM-RoBERTa-based submissions but was competitive with some LSTM-based approaches on certain datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Lower recall on some datasets; overall lagged behind large XLM-R variants.</td>
                        </tr>
                        <tr>
                            <td><strong>unique_insights</strong></td>
                            <td>Multilingual BERT provides a reasonable baseline for multilingual GED but may be outperformed by larger or more recently pre-trained multilingual encoders.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MultiGED-2023 shared task at NLP4CALL: Multilingual Grammatical Error Detection', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9428.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e9428.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using language models for detecting anomalies in lists, sequences, or tabular data, including details of the models, data types, methods, results, comparisons, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>VLP-char</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>VLP-char (character-based LSTM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A character-based two-layer unidirectional LSTM sequence-labeling model used per-dataset (excluding REALEC) to detect token-level learner errors without external data.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Two Neural Models for Multilingual Grammatical Error Detection</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>character-based LSTM</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Recurrent Neural Network (LSTM, character-level)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>unspecified (two recurrent layers)</td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>token sequences (character-level modeling leading to token labels)</td>
                        </tr>
                        <tr>
                            <td><strong>data_domain</strong></td>
                            <td>L2 learner essays</td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_type</strong></td>
                            <td>token-level grammatical/orthographic/lexical errors</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Train separate character-level LSTM sequence-labelers for each dataset (no external data), mapping character sequences to token-level binary labels.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_methods</strong></td>
                            <td>Compared against transformer-based (XLM-R, mBERT) and other LSTM/GRU systems.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Precision, Recall, F0.5</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Generally lower F0.5 scores: Czech 38.42, English-FCE 22.07, German 27.56, Italian 28.14, Swedish 29.46 (see Table 4). Notably, VLP-char had relatively higher recall on some datasets (e.g., Czech recall 63.95, Swedish recall 55.00).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baseline</strong></td>
                            <td>Character LSTM shows higher recall but far lower precision and F0.5 compared to transformer-based methods.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Lower precision causes poor F0.5; struggles without pretraining on large corpora; excluded REALEC dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>unique_insights</strong></td>
                            <td>Character-based models can yield higher recall (find more potential errors) but at the expense of precision; useful where recall is prioritized and external resources are limited.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MultiGED-2023 shared task at NLP4CALL: Multilingual Grammatical Error Detection', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9428.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e9428.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using language models for detecting anomalies in lists, sequences, or tabular data, including details of the models, data types, methods, results, comparisons, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>NTNU-TRH</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>NTNU-TRH system (LSTMs/GRUs with Flair embeddings)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multilingual system combining LSTMs, GRUs and standard RNNs with multilingual Flair embeddings in a sequence-to-sequence labeling multitask setup for GED.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>NTNU-TRH System at the MultiGED-2023 Shared Task on Multilingual Grammatical Error Detection</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LSTM/GRU-based sequence models with Flair embeddings</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Recurrent Neural Networks (LSTM, GRU) with contextualized Flair embeddings</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>unspecified</td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>token sequences (tabular per-token labels)</td>
                        </tr>
                        <tr>
                            <td><strong>data_domain</strong></td>
                            <td>L2 learner essays</td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_type</strong></td>
                            <td>token-level learner errors</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Use multilingual Flair embeddings as input to RNN-based sequence-labeling models (LSTM/GRU/RNN) in a multitask setup for token error classification.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_methods</strong></td>
                            <td>Compared against transformer-based approaches and character-LSTM.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Precision, Recall, F0.5</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Performed poorly on recall in many datasets (e.g., Czech F0.5 24.54, English-FCE 8.45, German 44.61, Italian 53.62, Swedish 20.31).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baseline</strong></td>
                            <td>Underperformed compared to transformer-based approaches but sometimes achieved high precision (e.g., Italian precision 93.38) with very low recall.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Tends to detect fewer errors (very low recall) or produce false negatives; requires careful tuning and benefits less from pretraining than large transformers.</td>
                        </tr>
                        <tr>
                            <td><strong>unique_insights</strong></td>
                            <td>RNN-based approaches can achieve high precision on some languages/datasets but at the cost of recall, showing the importance of embedding choice and architecture for GED.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MultiGED-2023 shared task at NLP4CALL: Multilingual Grammatical Error Detection', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9428.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e9428.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using language models for detecting anomalies in lists, sequences, or tabular data, including details of the models, data types, methods, results, comparisons, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>su-dali (distant MT)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>su-dali distantly-supervised transformer MT system</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A distantly-supervised transformer-based MT approach trained solely on a large artificial dataset (200 million sentences) mimicking Swedish error distributions, applied for GED without supervised fine-tuning on labeled Swedish data.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>A distantly supervised Grammatical Error Detection/Correction system for Swedish</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Transformer-based machine translation model (distantly supervised)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Transformer (MT-style model adapted for GED)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>unspecified (trained on artificial dataset of 200M sentences)</td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>token sequences (tabular per-token labels) but produced via MT-style correction outputs</td>
                        </tr>
                        <tr>
                            <td><strong>data_domain</strong></td>
                            <td>Artificially generated learner-like Swedish sentences; evaluated on real Swedish L2 essays</td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_type</strong></td>
                            <td>token-level learner errors (detection/correction)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Train an MT transformer on synthetic parallel data created to mimic the error distribution of the Swedish corpus in a distantly supervised manner; apply to GED/GEC tasks without supervised in-domain training.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_methods</strong></td>
                            <td>Compared to supervised transformer and LSTM methods using labeled Swedish data.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Precision, Recall, F0.5</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>On Swedish achieved F0.5 58.60 (precision 82.41, recall 27.18), competitive given absence of labeled training data.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baseline</strong></td>
                            <td>Competitive for Swedish despite no supervised labels; lower recall but decent precision compared to fully supervised transformer models.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Lower recall indicates missing many errors; success depends on quality of synthetic data and how well it models real error distributions.</td>
                        </tr>
                        <tr>
                            <td><strong>unique_insights</strong></td>
                            <td>Artificial/synthetic data with realistic error distributions can enable competitive GED performance for low-resource languages without manual annotations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MultiGED-2023 shared task at NLP4CALL: Multilingual Grammatical Error Detection', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9428.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e9428.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using language models for detecting anomalies in lists, sequences, or tabular data, including details of the models, data types, methods, results, comparisons, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ELECTRA (prior SOTA)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ELECTRA (pretraining as discriminators rather than generators)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>ELECTRA is a pretraining approach whose discriminative objective was found conceptually similar to GED and achieved state-of-the-art results on English FCE (reported in Yuan et al., 2021).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>ELECTRA: Pretraining Text Encoders as Discriminators Rather Than Generators</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ELECTRA</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Transformer (discriminative pretraining)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>unspecified in this paper (reported in Yuan et al. 2021 results)</td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>token sequences (GED benchmarks like FCE)</td>
                        </tr>
                        <tr>
                            <td><strong>data_domain</strong></td>
                            <td>L2 learner essays (English FCE)</td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_type</strong></td>
                            <td>token-level grammatical/orthographic errors</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Previously: fine-tune ELECTRA pre-trained encoder for token-level GED; discriminative pretraining aligns with detecting incorrect tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_methods</strong></td>
                            <td>Compared to BERT, XLNet, prior approaches from Kaneko & Komachi and Bell et al.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Precision, Recall, F0.5</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Cited prior result: ELECTRA achieved F0.5 = 72.93 on English FCE (Yuan et al., 2021) â€” reported as state-of-the-art referenced in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baseline</strong></td>
                            <td>ELECTRA outperformed previous BERT-based GED models and set the then state-of-the-art on FCE.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Not detailed in this paper beyond generalization challenges across annotation styles and datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>unique_insights</strong></td>
                            <td>Discriminative pretraining objectives (like ELECTRA) are conceptually well aligned with GED tasks and yield strong performance on English benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MultiGED-2023 shared task at NLP4CALL: Multilingual Grammatical Error Detection', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9428.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e9428.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using language models for detecting anomalies in lists, sequences, or tabular data, including details of the models, data types, methods, results, comparisons, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BERT / XLNet (prior)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BERT and XLNet (pretrained transformers referenced for GED comparisons)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>BERT and XLNet are referenced as pre-trained transformers previously used for GED; they showed substantial improvements over older methods but were outperformed by ELECTRA in prior work.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BERT; XLNet</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Transformer encoders (BERT) and autoregressive-permutation model (XLNet)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>unspecified</td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>token sequences for GED benchmarks</td>
                        </tr>
                        <tr>
                            <td><strong>data_domain</strong></td>
                            <td>L2 learner essays (English FCE and other GED datasets)</td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_type</strong></td>
                            <td>token-level errors</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Fine-tune pre-trained BERT/XLNet variants for token-level sequence labeling for GED.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_methods</strong></td>
                            <td>Compared to ELECTRA and later XLM-R based approaches in this shared task.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Precision, Recall, F0.5</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Previously reported: Yuan-2021 results include BERT F0.5 = 67.88 and XLNet F0.5 = 69.75 on English FCE (as listed in Table 6 of the paper).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baseline</strong></td>
                            <td>Both improved over earlier non-transformer approaches; ELECTRA later outperformed them on FCE.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Subject to dataset split and annotation style mismatch; require sizeable in-domain labeled data to excel.</td>
                        </tr>
                        <tr>
                            <td><strong>unique_insights</strong></td>
                            <td>Contextual embeddings (BERT/XLNet) are powerful for GED; model pretraining objective affects alignment with GED performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MultiGED-2023 shared task at NLP4CALL: Multilingual Grammatical Error Detection', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9428.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e9428.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using language models for detecting anomalies in lists, sequences, or tabular data, including details of the models, data types, methods, results, comparisons, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Majority baseline</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Majority-vote token labeling across submitted systems</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A simple ensemble baseline constructed by taking the majority predicted token label across all system outputs; ties broken by best-system fallback in some datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>majority-vote ensemble (non-parametric)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>ensemble rule (voting over token labels)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>token sequences (tabular per-token labels)</td>
                        </tr>
                        <tr>
                            <td><strong>data_domain</strong></td>
                            <td>L2 learner essays across languages</td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_type</strong></td>
                            <td>token-level learner errors</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>At each token, choose the label predicted by the majority of participating systems; for ties (even number of systems) fallback to the best system's label.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_methods</strong></td>
                            <td>Serves as an ensemble baseline; compared to individual systems (XLM-R, mBERT, LSTMs, etc.).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Precision, Recall, F0.5</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Majority ensemble yields high precision and lower recall; example F0.5: Czech 70.85, English-FCE 64.39, English-REALEC 51.11, German 76.21, Italian 72.74, Swedish 75.15 (Table 4). Would rank 2nd in most languages; would be 1st for English-REALEC if included.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baseline</strong></td>
                            <td>Majority vote often achieves higher precision than individual systems but lower recall; competitive overall and sometimes surpasses many individual systems.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Lower recall; depends on diversity and number of participating systems; tie-breaking heuristic can influence results.</td>
                        </tr>
                        <tr>
                            <td><strong>unique_insights</strong></td>
                            <td>Simple ensembling across heterogeneous GED systems is surprisingly strong in precision and can be an effective baseline when multiple models are available.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MultiGED-2023 shared task at NLP4CALL: Multilingual Grammatical Error Detection', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Unsupervised cross-lingual representation learning at scale <em>(Rating: 2)</em></li>
                <li>ELECTRA: Pretraining Text Encoders as Discriminators Rather Than Generators <em>(Rating: 2)</em></li>
                <li>Multi-Class Grammatical Error Detection for Correction: A Tale of Two Systems <em>(Rating: 2)</em></li>
                <li>Et.tCoDt: at MultiGED2023: fine-tuning XLM-RoBERTa for multilingual grammatical error detection <em>(Rating: 2)</em></li>
                <li>Two Neural Models for Multilingual Grammatical Error Detection <em>(Rating: 2)</em></li>
                <li>NTNU-TRH System at the MultiGED-2023 Shared Task on Multilingual Grammatical Error Detection <em>(Rating: 2)</em></li>
                <li>A distantly supervised Grammatical Error Detection/Correction system for Swedish <em>(Rating: 2)</em></li>
                <li>Context is key: Grammatical error detection with contextual word representations <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9428",
    "paper_id": "paper-30d1f759ef4bd54ee96e95fb5da4df41d0bc2c7d",
    "extraction_schema_id": "extraction-schema-162",
    "extracted_data": [
        {
            "name_short": "EliCoDe (XLM-R)",
            "name_full": "EliCoDe system (fine-tuned XLM-RoBERTa per-language)",
            "brief_description": "Team EliCoDe fine-tuned separate XLM-RoBERTa models (one per dataset/language) for token-level binary grammatical-error detection, achieving the best average performance across MultiGED languages.",
            "citation_title": "Et.tCoDt: at MultiGED2023: fine-tuning XLM-RoBERTa for multilingual grammatical error detection",
            "mention_or_use": "use",
            "model_name": "XLM-RoBERTa (large)",
            "model_type": "Transformer (multilingual masked-language model)",
            "model_size": "large (not specified in paper, referred to as XLM-RoBERTa (large))",
            "data_type": "token sequences (one token per row; tabular CoNLL-style format)",
            "data_domain": "L2 learner essays (multilingual: Czech, English, German, Italian, Swedish)",
            "anomaly_type": "token-level learner errors (grammatical/orthographic/lexical/morphological/syntactic errors)",
            "method_description": "Fine-tune pre-trained XLM-RoBERTa per language on token-level binary classification (labels 'c' or 'i'), with a stacked linear classifier and dropout on top.",
            "baseline_methods": "Monolingual/multilingual LSTM-based systems, character-based LSTM, majority vote ensemble, and previously published BERT/ELECTRA/XLNet baselines.",
            "performance_metrics": "Precision (P), Recall (R), F0.5 (token-based)",
            "performance_results": "Top per-language F0.5 scores from EliCoDe: German 82.32, Italian 82.15, Swedish 78.16, Czech 73.44, English-FCE 67.40 (reported in Table 5 and Table 4).",
            "comparison_to_baseline": "Substantially better than LSTM/char-LSTM approaches on most languages; monolingual fine-tuned XLM-R models (EliCoDe) outperformed the single multilingual XLM-R model fine-tuned on all datasets (DSL-MIM-HUS) for most languages, except English-REALEC.",
            "limitations_or_failure_cases": "Performance varies by dataset split and annotation paradigm; lower performance on English-REALEC (no training split and different annotation style). Token-based F0.5 evaluation inflates multi-token edit rewards. Real-word errors are notably harder to detect than non-word (OOV) errors.",
            "unique_insights": "Per-language fine-tuning of a large multilingual LM yielded top results, but monolingual vs. multilingual fine-tuning trade-offs exist; models generalize poorly to corpora with different annotation conventions or without in-domain training data.",
            "uuid": "e9428.0",
            "source_info": {
                "paper_title": "MultiGED-2023 shared task at NLP4CALL: Multilingual Grammatical Error Detection",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "DSL-MIM-HUS (XLM-R joint)",
            "name_full": "DSL-MIM-HUS team (single multilingual XLM-RoBERTa fine-tuned jointly)",
            "brief_description": "This team fine-tuned a single pre-trained XLM-RoBERTa model jointly on all MultiGED datasets to perform token-level error detection across languages.",
            "citation_title": "Two Neural Models for Multilingual Grammatical Error Detection",
            "mention_or_use": "use",
            "model_name": "XLM-RoBERTa",
            "model_type": "Transformer (multilingual masked-language model)",
            "model_size": "unspecified (multilingual XLM-RoBERTa from HuggingFace)",
            "data_type": "token sequences (tabular per-token labels)",
            "data_domain": "L2 learner essays across multiple languages",
            "anomaly_type": "token-level learner errors (grammatical/orthographic/lexical/morphological/syntactic)",
            "method_description": "Fine-tune a single pre-trained XLM-RoBERTa model jointly on all provided language datasets for token-level binary classification.",
            "baseline_methods": "Per-language fine-tuned large LMs (EliCoDe), LSTM-based models, character LSTM (VLP-char), majority vote.",
            "performance_metrics": "Precision, Recall, F0.5",
            "performance_results": "DSL-MIM-HUS achieved best F0.5 on English-REALEC (50.86/50.87). On other languages their scores were competitive but generally lower than EliCoDe (e.g., Czech F0.5 57.76, English-FCE 61.18, German 70.75, Italian 63.55, Swedish 66.05).",
            "comparison_to_baseline": "Joint multilingual fine-tuning produced competitive results but generally underperformed per-language fine-tuning (EliCoDe) except where there was no in-domain training data (REALEC), where the joint model outperformed language-specific models.",
            "limitations_or_failure_cases": "Joint multilingual fine-tuning can underperform language-specific fine-tuning when in-domain training data exist; annotation differences across corpora hurt cross-dataset generalization.",
            "unique_insights": "A single multilingual model fine-tuned on all languages can generalize better to datasets without training splits (e.g., REALEC) than per-language models, indicating benefits of shared multilingual representations when in-domain labeled data is unavailable.",
            "uuid": "e9428.1",
            "source_info": {
                "paper_title": "MultiGED-2023 shared task at NLP4CALL: Multilingual Grammatical Error Detection",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "Brainstorm (mBERT)",
            "name_full": "Brainstorm Thinkers system (mBERT-based)",
            "brief_description": "A participating team used multilingual BERT (mBERT) to perform token-level grammatical error detection across datasets.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "mBERT (multilingual BERT)",
            "model_type": "Transformer (multilingual BERT)",
            "model_size": "unspecified",
            "data_type": "token sequences (tabular per-token labels)",
            "data_domain": "L2 learner essays",
            "anomaly_type": "token-level grammatical/orthographic/lexical errors",
            "method_description": "Fine-tune mBERT for sequence labeling / token classification on the GED datasets.",
            "baseline_methods": "Compared against XLM-RoBERTa-based approaches and LSTM-based systems.",
            "performance_metrics": "Precision, Recall, F0.5",
            "performance_results": "Performance was moderate; example scores include Czech F0.5 46.81, English-FCE F0.5 59.81, German F0.5 69.11, Italian F0.5 59.49, Swedish F0.5 63.11 (see Table 4).",
            "comparison_to_baseline": "mBERT underperformed compared to top XLM-RoBERTa-based submissions but was competitive with some LSTM-based approaches on certain datasets.",
            "limitations_or_failure_cases": "Lower recall on some datasets; overall lagged behind large XLM-R variants.",
            "unique_insights": "Multilingual BERT provides a reasonable baseline for multilingual GED but may be outperformed by larger or more recently pre-trained multilingual encoders.",
            "uuid": "e9428.2",
            "source_info": {
                "paper_title": "MultiGED-2023 shared task at NLP4CALL: Multilingual Grammatical Error Detection",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "VLP-char",
            "name_full": "VLP-char (character-based LSTM)",
            "brief_description": "A character-based two-layer unidirectional LSTM sequence-labeling model used per-dataset (excluding REALEC) to detect token-level learner errors without external data.",
            "citation_title": "Two Neural Models for Multilingual Grammatical Error Detection",
            "mention_or_use": "use",
            "model_name": "character-based LSTM",
            "model_type": "Recurrent Neural Network (LSTM, character-level)",
            "model_size": "unspecified (two recurrent layers)",
            "data_type": "token sequences (character-level modeling leading to token labels)",
            "data_domain": "L2 learner essays",
            "anomaly_type": "token-level grammatical/orthographic/lexical errors",
            "method_description": "Train separate character-level LSTM sequence-labelers for each dataset (no external data), mapping character sequences to token-level binary labels.",
            "baseline_methods": "Compared against transformer-based (XLM-R, mBERT) and other LSTM/GRU systems.",
            "performance_metrics": "Precision, Recall, F0.5",
            "performance_results": "Generally lower F0.5 scores: Czech 38.42, English-FCE 22.07, German 27.56, Italian 28.14, Swedish 29.46 (see Table 4). Notably, VLP-char had relatively higher recall on some datasets (e.g., Czech recall 63.95, Swedish recall 55.00).",
            "comparison_to_baseline": "Character LSTM shows higher recall but far lower precision and F0.5 compared to transformer-based methods.",
            "limitations_or_failure_cases": "Lower precision causes poor F0.5; struggles without pretraining on large corpora; excluded REALEC dataset.",
            "unique_insights": "Character-based models can yield higher recall (find more potential errors) but at the expense of precision; useful where recall is prioritized and external resources are limited.",
            "uuid": "e9428.3",
            "source_info": {
                "paper_title": "MultiGED-2023 shared task at NLP4CALL: Multilingual Grammatical Error Detection",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "NTNU-TRH",
            "name_full": "NTNU-TRH system (LSTMs/GRUs with Flair embeddings)",
            "brief_description": "A multilingual system combining LSTMs, GRUs and standard RNNs with multilingual Flair embeddings in a sequence-to-sequence labeling multitask setup for GED.",
            "citation_title": "NTNU-TRH System at the MultiGED-2023 Shared Task on Multilingual Grammatical Error Detection",
            "mention_or_use": "use",
            "model_name": "LSTM/GRU-based sequence models with Flair embeddings",
            "model_type": "Recurrent Neural Networks (LSTM, GRU) with contextualized Flair embeddings",
            "model_size": "unspecified",
            "data_type": "token sequences (tabular per-token labels)",
            "data_domain": "L2 learner essays",
            "anomaly_type": "token-level learner errors",
            "method_description": "Use multilingual Flair embeddings as input to RNN-based sequence-labeling models (LSTM/GRU/RNN) in a multitask setup for token error classification.",
            "baseline_methods": "Compared against transformer-based approaches and character-LSTM.",
            "performance_metrics": "Precision, Recall, F0.5",
            "performance_results": "Performed poorly on recall in many datasets (e.g., Czech F0.5 24.54, English-FCE 8.45, German 44.61, Italian 53.62, Swedish 20.31).",
            "comparison_to_baseline": "Underperformed compared to transformer-based approaches but sometimes achieved high precision (e.g., Italian precision 93.38) with very low recall.",
            "limitations_or_failure_cases": "Tends to detect fewer errors (very low recall) or produce false negatives; requires careful tuning and benefits less from pretraining than large transformers.",
            "unique_insights": "RNN-based approaches can achieve high precision on some languages/datasets but at the cost of recall, showing the importance of embedding choice and architecture for GED.",
            "uuid": "e9428.4",
            "source_info": {
                "paper_title": "MultiGED-2023 shared task at NLP4CALL: Multilingual Grammatical Error Detection",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "su-dali (distant MT)",
            "name_full": "su-dali distantly-supervised transformer MT system",
            "brief_description": "A distantly-supervised transformer-based MT approach trained solely on a large artificial dataset (200 million sentences) mimicking Swedish error distributions, applied for GED without supervised fine-tuning on labeled Swedish data.",
            "citation_title": "A distantly supervised Grammatical Error Detection/Correction system for Swedish",
            "mention_or_use": "use",
            "model_name": "Transformer-based machine translation model (distantly supervised)",
            "model_type": "Transformer (MT-style model adapted for GED)",
            "model_size": "unspecified (trained on artificial dataset of 200M sentences)",
            "data_type": "token sequences (tabular per-token labels) but produced via MT-style correction outputs",
            "data_domain": "Artificially generated learner-like Swedish sentences; evaluated on real Swedish L2 essays",
            "anomaly_type": "token-level learner errors (detection/correction)",
            "method_description": "Train an MT transformer on synthetic parallel data created to mimic the error distribution of the Swedish corpus in a distantly supervised manner; apply to GED/GEC tasks without supervised in-domain training.",
            "baseline_methods": "Compared to supervised transformer and LSTM methods using labeled Swedish data.",
            "performance_metrics": "Precision, Recall, F0.5",
            "performance_results": "On Swedish achieved F0.5 58.60 (precision 82.41, recall 27.18), competitive given absence of labeled training data.",
            "comparison_to_baseline": "Competitive for Swedish despite no supervised labels; lower recall but decent precision compared to fully supervised transformer models.",
            "limitations_or_failure_cases": "Lower recall indicates missing many errors; success depends on quality of synthetic data and how well it models real error distributions.",
            "unique_insights": "Artificial/synthetic data with realistic error distributions can enable competitive GED performance for low-resource languages without manual annotations.",
            "uuid": "e9428.5",
            "source_info": {
                "paper_title": "MultiGED-2023 shared task at NLP4CALL: Multilingual Grammatical Error Detection",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "ELECTRA (prior SOTA)",
            "name_full": "ELECTRA (pretraining as discriminators rather than generators)",
            "brief_description": "ELECTRA is a pretraining approach whose discriminative objective was found conceptually similar to GED and achieved state-of-the-art results on English FCE (reported in Yuan et al., 2021).",
            "citation_title": "ELECTRA: Pretraining Text Encoders as Discriminators Rather Than Generators",
            "mention_or_use": "mention",
            "model_name": "ELECTRA",
            "model_type": "Transformer (discriminative pretraining)",
            "model_size": "unspecified in this paper (reported in Yuan et al. 2021 results)",
            "data_type": "token sequences (GED benchmarks like FCE)",
            "data_domain": "L2 learner essays (English FCE)",
            "anomaly_type": "token-level grammatical/orthographic errors",
            "method_description": "Previously: fine-tune ELECTRA pre-trained encoder for token-level GED; discriminative pretraining aligns with detecting incorrect tokens.",
            "baseline_methods": "Compared to BERT, XLNet, prior approaches from Kaneko & Komachi and Bell et al.",
            "performance_metrics": "Precision, Recall, F0.5",
            "performance_results": "Cited prior result: ELECTRA achieved F0.5 = 72.93 on English FCE (Yuan et al., 2021) â€” reported as state-of-the-art referenced in this paper.",
            "comparison_to_baseline": "ELECTRA outperformed previous BERT-based GED models and set the then state-of-the-art on FCE.",
            "limitations_or_failure_cases": "Not detailed in this paper beyond generalization challenges across annotation styles and datasets.",
            "unique_insights": "Discriminative pretraining objectives (like ELECTRA) are conceptually well aligned with GED tasks and yield strong performance on English benchmarks.",
            "uuid": "e9428.6",
            "source_info": {
                "paper_title": "MultiGED-2023 shared task at NLP4CALL: Multilingual Grammatical Error Detection",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "BERT / XLNet (prior)",
            "name_full": "BERT and XLNet (pretrained transformers referenced for GED comparisons)",
            "brief_description": "BERT and XLNet are referenced as pre-trained transformers previously used for GED; they showed substantial improvements over older methods but were outperformed by ELECTRA in prior work.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "BERT; XLNet",
            "model_type": "Transformer encoders (BERT) and autoregressive-permutation model (XLNet)",
            "model_size": "unspecified",
            "data_type": "token sequences for GED benchmarks",
            "data_domain": "L2 learner essays (English FCE and other GED datasets)",
            "anomaly_type": "token-level errors",
            "method_description": "Fine-tune pre-trained BERT/XLNet variants for token-level sequence labeling for GED.",
            "baseline_methods": "Compared to ELECTRA and later XLM-R based approaches in this shared task.",
            "performance_metrics": "Precision, Recall, F0.5",
            "performance_results": "Previously reported: Yuan-2021 results include BERT F0.5 = 67.88 and XLNet F0.5 = 69.75 on English FCE (as listed in Table 6 of the paper).",
            "comparison_to_baseline": "Both improved over earlier non-transformer approaches; ELECTRA later outperformed them on FCE.",
            "limitations_or_failure_cases": "Subject to dataset split and annotation style mismatch; require sizeable in-domain labeled data to excel.",
            "unique_insights": "Contextual embeddings (BERT/XLNet) are powerful for GED; model pretraining objective affects alignment with GED performance.",
            "uuid": "e9428.7",
            "source_info": {
                "paper_title": "MultiGED-2023 shared task at NLP4CALL: Multilingual Grammatical Error Detection",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "Majority baseline",
            "name_full": "Majority-vote token labeling across submitted systems",
            "brief_description": "A simple ensemble baseline constructed by taking the majority predicted token label across all system outputs; ties broken by best-system fallback in some datasets.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "majority-vote ensemble (non-parametric)",
            "model_type": "ensemble rule (voting over token labels)",
            "model_size": null,
            "data_type": "token sequences (tabular per-token labels)",
            "data_domain": "L2 learner essays across languages",
            "anomaly_type": "token-level learner errors",
            "method_description": "At each token, choose the label predicted by the majority of participating systems; for ties (even number of systems) fallback to the best system's label.",
            "baseline_methods": "Serves as an ensemble baseline; compared to individual systems (XLM-R, mBERT, LSTMs, etc.).",
            "performance_metrics": "Precision, Recall, F0.5",
            "performance_results": "Majority ensemble yields high precision and lower recall; example F0.5: Czech 70.85, English-FCE 64.39, English-REALEC 51.11, German 76.21, Italian 72.74, Swedish 75.15 (Table 4). Would rank 2nd in most languages; would be 1st for English-REALEC if included.",
            "comparison_to_baseline": "Majority vote often achieves higher precision than individual systems but lower recall; competitive overall and sometimes surpasses many individual systems.",
            "limitations_or_failure_cases": "Lower recall; depends on diversity and number of participating systems; tie-breaking heuristic can influence results.",
            "unique_insights": "Simple ensembling across heterogeneous GED systems is surprisingly strong in precision and can be an effective baseline when multiple models are available.",
            "uuid": "e9428.8",
            "source_info": {
                "paper_title": "MultiGED-2023 shared task at NLP4CALL: Multilingual Grammatical Error Detection",
                "publication_date_yy_mm": "2023-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Unsupervised cross-lingual representation learning at scale",
            "rating": 2
        },
        {
            "paper_title": "ELECTRA: Pretraining Text Encoders as Discriminators Rather Than Generators",
            "rating": 2
        },
        {
            "paper_title": "Multi-Class Grammatical Error Detection for Correction: A Tale of Two Systems",
            "rating": 2
        },
        {
            "paper_title": "Et.tCoDt: at MultiGED2023: fine-tuning XLM-RoBERTa for multilingual grammatical error detection",
            "rating": 2
        },
        {
            "paper_title": "Two Neural Models for Multilingual Grammatical Error Detection",
            "rating": 2
        },
        {
            "paper_title": "NTNU-TRH System at the MultiGED-2023 Shared Task on Multilingual Grammatical Error Detection",
            "rating": 2
        },
        {
            "paper_title": "A distantly supervised Grammatical Error Detection/Correction system for Swedish",
            "rating": 2
        },
        {
            "paper_title": "Context is key: Grammatical error detection with contextual word representations",
            "rating": 1
        }
    ],
    "cost": 0.01840025,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>MultiGED-2023 shared task at NLP4CALL: Multilingual Grammatical Error Detection</h1>
<p>Elena Volodina ${ }^{1}$, Christopher Bryant ${ }^{2}$, Andrew Caines ${ }^{2}$, OrphÃ©e De Clercq ${ }^{3}$, Jennifer-Carmen Frey ${ }^{4}$, Elizaveta Ershova ${ }^{5}$, Alexandr Rosen ${ }^{6}$, Olga Vinogradova ${ }^{7}$<br>${ }^{1}$ University of Gothenburg, Sweden, elena.volodina@svenska.gu.se<br>${ }^{2}$ ALTA Institute, University of Cambridge, UK, {cjb255, apc38}@cam.ac.uk<br>${ }^{3}$ LT3, Ghent University, Belgium, orphee.declercq@ugent.be<br>${ }^{4}$ EURAC Research, Italy, JenniferCarmen.Frey@eurac.edu<br>${ }^{5}$ JetBrains, Cyprus, elizaveta.ershova@jetbrains.com<br>${ }^{6}$ Charles University, Czech Republic, alexandr.rosen@ff.cuni.cz<br>${ }^{7}$ Independent researcher, Israel, olgavinogr@gmail.com</p>
<h4>Abstract</h4>
<p>This paper reports on the NLP4CALL shared task on Multilingual Grammatical Error Detection (MultiGED-2023), which included five languages: Czech, English, German, Italian and Swedish. It is the first shared task organized by the Computational SLA ${ }^{1}$ working group, whose aim is to promote less represented languages in the fields of Grammatical Error Detection and Correction, and other related fields. The MultiGED datasets have been produced based on second language (L2) learner corpora for each particular language. In this paper we introduce the task as a whole, elaborate on the dataset generation process and the design choices made to obtain MultiGED datasets, provide details of the evaluation metrics and CodaLab setup. We further briefly describe the systems used by participants and report the results.</p>
<h2>1 Introduction</h2>
<p>Shared tasks are competitions that challenge researchers around the world to solve practical research problems in controlled conditions (e.g., Nissim et al., 2017; Parra EscartÃ­n et al., 2017). Within the field of (second) language acquisition</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>and linguistic issues related to language learning, there have now been several shared tasks on various topics, including:</p>
<ul>
<li>argumentative essay analysis for feedback generation ${ }^{2}$ (e.g., Picou et al., 2021), where the challenge was to classify text sections into argumentative discourse elements, such as claim, rebuttal, evidence, etc.;</li>
<li>essay grading / proficiency level prediction (e.g., Ballier et al., 2020), where, given an essay, the major task was to assign a corresponding CEFR proficiency level (A1, A2, B1, B2, etc);</li>
<li>second language acquisition modeling (e.g., Settles et al., 2018), where the challenge was to predict where a learner might make an error given their error history;</li>
</ul>
<p>Most prominent, though, have been challenges on so-called grammatical error detection (GED) and correction (GEC), where the task has been to either detect tokens in need of correction, or to produce a correction. Note that the attribute grammatical is used traditionally rather than descriptively, since other types of errors (e.g. lexical, orthographical, syntactical) are also targeted. GEC and GED have complemented each other over the years, and the historical interest in the two tasks is visualized in Figure 1. In their comprehensive overview of approaches to GEC, Bryant et al.</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Terms <em>grammatical error detection</em> and <em>grammatical error correction</em> in Google N-grams (1990â€“2019)</p>
<p>(2023) observe that most GEC shared tasks have focused only on English, including HOO-2011/12 (Dale and Kilgarriff, 2011; Dale et al., 2012), CoNLL-2013/14 (Ng et al., 2013, 2014), AESW-2016 (Daudaravicius et al., 2016) and BEA-2019 (Bryant et al., 2019), with only a few exploring other languages, such as QALB-2014 and QALB-2015 for Arabic (Mohit et al., 2014; Rozovskaya et al., 2015) and NLPTEA 2014â€“2020 (Rao et al., 2020) and NLPCC-2018 (Zhao et al., 2018) for Mandarin Chinese.</p>
<p>Though datasets do exist for languages other than English â€“ including for GEC and GED tasks â€“ these rarely feature in shared tasks<sup id="fnref:3"><a class="footnote-ref" href="#fn:3">3</a></sup>. Examples of such GEC/GED initiatives are NÃ¡plava and Straka (2019) for Czech, Rozovskaya and Roth (2019) for Russian, Davidson et al. (2020) for Spanish, Syvokon and Nahorna (2022) for Ukrainian, Cotet et al. (2020) for Romanian, Boyd (2018) for German, Ã–stling and KurfalÄ± (2022) and Nyberg (2022) for Swedish, to name just a few.</p>
<p><strong>The Matthew effect in GEC and GED?</strong> It can be said that the current state of NLP reflects the Matthew effect â€“ i.e., 'the rich get richer, and the poor get poorer' (Perc, 2014; Bol et al., 2018). The Matthew effect has been observed and studied in various disciplines, including economics, sociology, biology, education and even research funding, but is similarly applicable to NLP, as SÃ¸gaard (2022) convincingly argued in the article with the provocative title "Should We Ban English NLP for a Year?". The growing bias of NLP research, models and datasets towards English ('the rich') creates inequality by not only making English a 'better equipped language', but also by lowering chances of being cited for researchers working on other languages than English ('the poor'). We witness therefore a tendency in NLP research where researchers prefer to work on English as it is both the best resourced and best cited language.</p>
<p>To counter-balance the current dynamics in the field towards English dominance, we have taken the initiative to form a <em>Computational SLA working group</em> whose main aim is to support and promote work on less represented languages in the area of GED, GEC and other potential tasks in SLA. The MultiGED-2023 shared task is the first one organized by this Computational SLA working group. By bringing non-English datasets, in combination with the English ones, to the attention of the international NLP community, we aim to foster an increasing interest in working on these languages.</p>
<h3>2 Task and challenges</h3>
<p>The main focus of the first Computational SLA shared task was <strong>error detection</strong>, which we argue should be given more attention as a first step towards pedagogical feedback generation. Through this task, several needs and challenges became clearer which we summarize below.</p>
<p>(i) <em>Use of authentic L2 data for training al-</em></p>
<p>gorithms. Leacock et al. (2014) convincingly showed that tools for error correction and feedback for foreign language learners benefit from being trained on real L2 students' texts, and that these systems are better suited for use in Intelligent Computer-Assisted Language Learning (ICALL) or Automatic Writing Evaluation (AWE) contexts. Hence the importance of authentic language learner data.
(ii) Focus on less represented languages in GEC/GED. Both GEC and GED have predominantly been explored in the context of English data. There is a strong incentive to broaden the language spectrum and draw the attention of the international NLP community to other, less represented, languages. We therefore target a few of the less represented languages, namely Czech, German, Italian and Swedish, along with English for comparison with previous work.
(iii) The requirement (i) to use authentic L2 data for the task sets further challenges. First of all, it brings attention to the scarceness of authentic learner data for a number of languages. Most languages have modest or tiny collections of L2 data, if any, which contain error annotation and correction. As a consequence, the data is too small to be offered for a shared task by itself. As a way to overcome that problem, we suggest that several languages with smaller datasets coordinate their efforts in a multilingual low-resource context, creating possibilities for augmentation of data and/or use of datasets from several languages through domain adaptation, transfer learning, and other modern techniques. The low-resource context above refers to a limitation on dataset sizes: there is a maximum of $\approx 36,000$ sentences for each MultiGED language to stimulate creativity in solving problems relating to data scarcity, the smallest datasets comprising $\approx 8,000$ sentences.
(iv) However, (iii) brings further the need to harmonize datasets between the languages participating in a multilingual shared task. Harmonization includes both data formatting and data annotation (i.e., converting all language-specific error tags into a set of shared tags). This in itself is a tremendous challenge since languages differ in both linguistic terms and in terms of the annotation approaches and taxonomies adopted by research teams who collated the various corpora. Our initial attempts to convert existing error taxonomies for the five languages to a set of five head categories -</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Token</th>
<th style="text-align: center;">Label</th>
<th style="text-align: left;">Token</th>
<th style="text-align: center;">Label</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">I</td>
<td style="text-align: center;">c</td>
<td style="text-align: left;">I</td>
<td style="text-align: center;">c</td>
</tr>
<tr>
<td style="text-align: left;">saws</td>
<td style="text-align: center;">i</td>
<td style="text-align: left;">saws</td>
<td style="text-align: center;">i</td>
</tr>
<tr>
<td style="text-align: left;">the</td>
<td style="text-align: center;">c</td>
<td style="text-align: left;">show</td>
<td style="text-align: center;">i</td>
</tr>
<tr>
<td style="text-align: left;">show</td>
<td style="text-align: center;">c</td>
<td style="text-align: left;">last</td>
<td style="text-align: center;">c</td>
</tr>
<tr>
<td style="text-align: left;">last</td>
<td style="text-align: center;">c</td>
<td style="text-align: left;">nigt</td>
<td style="text-align: center;">i</td>
</tr>
<tr>
<td style="text-align: left;">nigt</td>
<td style="text-align: center;">i</td>
<td style="text-align: left;">.</td>
<td style="text-align: center;">c</td>
</tr>
<tr>
<td style="text-align: left;">.</td>
<td style="text-align: center;">c</td>
<td style="text-align: left;"></td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Table 1: Data example with two sentences. The sentence on the right demonstrates an error that requires the addition of an extra token, which is indicated by ' $i$ ' attached to the next token (see ' $i$ ' attached to the token show to indicate the missing article the before show)
punctuation, orthography, lexis, morphology and syntax [POLMS] (Casademont Moner and Volodina, 2022) - proved to be more challenging than expected. As a result, we simplified the task from a multi-class error detection to a binary error detection task, leaving the idea of multi-class detection for future work.</p>
<p>MultiGED task in a nutshell The above challenges defined the way the task of multilingual grammatical error detection in low-resource contexts was formulated:</p>
<p>Given an authentic, learner-written sentence, detect tokens within the sentence that contain errors (i.e. perform binary classification on a per-token level) for each provided language separately, or as a multilingual system.</p>
<p>The tokens should be labeled as either correct ('c') or incorrect ('i'), as shown in Table 1.</p>
<p>We encouraged development of multilingual systems that would process all or several languages using a single model, but this was not a mandatory requirement. The submitted systems were evaluated using per-language precision, recall, and $\mathrm{F}<em 0.5="0.5">{0.5}$ scores. $\mathrm{F}</em>$ gives a double weighting to precision over recall, and is conventionally used as the primary metric for GED and GEC on the basis that high precision is more important than high recall for educational applications (Section 4).</p>
<p>The shared task was organized as an open track, in the sense that teams were freely permitted to enhance the provided training and development data for all languages, provided they report the use of additional data, and share them for research</p>
<table>
<thead>
<tr>
<th>Language</th>
<th>Source corpus</th>
<th>Nr. sentences</th>
<th>Nr. tokens</th>
<th>Nr. errors</th>
<th>Error rate</th>
<th>MultiGED License</th>
</tr>
</thead>
<tbody>
<tr>
<td>Czech</td>
<td>GECCC</td>
<td>35,453</td>
<td>399,742</td>
<td>84,041</td>
<td>0.210</td>
<td>CC BY-SA 4.0</td>
</tr>
<tr>
<td>English</td>
<td>FCE</td>
<td>33,243</td>
<td>531,416</td>
<td>50,860</td>
<td>0.096</td>
<td>custom</td>
</tr>
<tr>
<td>English</td>
<td>REALEC*</td>
<td>8,136</td>
<td>177,769</td>
<td>16,608</td>
<td>0.093</td>
<td>CC BY-SA 4.0</td>
</tr>
<tr>
<td>German</td>
<td>Falko-MERLIN</td>
<td>24,079</td>
<td>381,134</td>
<td>57,897</td>
<td>0.152</td>
<td>CC BY-SA 4.0</td>
</tr>
<tr>
<td>Italian</td>
<td>MERLIN</td>
<td>7,949</td>
<td>99,698</td>
<td>14,893</td>
<td>0.149</td>
<td>CC BY-SA 4.0</td>
</tr>
<tr>
<td>Swedish</td>
<td>SweLL-gold ${ }^{\dagger}$</td>
<td>8,553</td>
<td>145,507</td>
<td>27,274</td>
<td>0.187</td>
<td>CC BY-SA 4.0</td>
</tr>
</tbody>
</table>
<ul>
<li>We only provide a dev and test set for English-REALEC.
${ }^{\dagger}$ The original SweLL-gold corpus is released under a CLARIN ID+BY+PRIV+NORED license.
Table 2: MultiGED data statistics.
use and replication studies. This contrasts with a closed track shared task, where teams are prohibited from using additional training and development data beyond that provided by the organizers.</li>
</ul>
<p>The task aimed to promote research into languages which have received less attention in GED or GEC (Czech, Italian, German, and Swedish alongside English), and for which appropriately annotated datasets are available, even if modest in size ( $8,000-36,000$ sentences).</p>
<p>Our main contributions are three-fold.</p>
<ol>
<li>We present the first shared task on GED that includes original L2 learner data from Swedish, Italian, German and Czech.</li>
<li>We introduce a new dataset of Russian learner English, the REALEC corpus, for the first time.</li>
<li>We standardize the formats of several multilingual datasets to faciliate development of multilingual models.</li>
</ol>
<h2>3 Data</h2>
<p>We provided training, development and test data for each of the five languages: Czech, English, German, Italian and Swedish. ${ }^{4}$ Test sets were released during the test phase through CodaLab and are available there for future work and system comparisons. ${ }^{5}$ It is important to note that most corpora are made available on a CC BY-SA 4.0 data license, however the English-FCE uses a custom license, and the original SweLL-gold corpus uses a CLARIN PRIV+ID+BY+NORED license.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h3>3.1 Source data</h3>
<p>For each language, a MultiGED dataset was generated from a corpus of original error-annotated learner essays. Table 2 provides an overview of the source corpora, and data statistics of the resulting MultiGED datasets expressed in number of sentences, tokens, errors and error rates. Some of the source corpora mentioned in the Table have already been used in Grammatical Error Detection/Correction research, but we also release two new datasets: one based on REALEC (English) and another on SweLL-gold (Swedish). Where possible, we use the same train/dev/test splits as established in previous work (as is the case for GECCC, FCE, Falko-MERLIN), and only create new splits when necessary (REALEC, Italian MERLIN, SweLL). All datasets were derived from error-annotated L2 learner essays. Below, we provide an overview of each of the source corpora used to create these datasets.</p>
<p>Czech The Grammar Error Correction Corpus for Czech - GECCC (NÃ¡plava et al., 2022), consisting of 83,000 sentences, is based on native and non-native texts collected in several earlier projects. ${ }^{6}$ The native part consists of essays written by children and teenagers attending primary and secondary schools, either (i) native in standard Czech, or (ii) in its Romani ethnolect, and (iii) informal website texts. However, only the nonnative part of GECCC is included in the MultiGED datasets: (iv) essays written by learners of Czech as a foreign or second language, collected mostly for the CzeSL project (Rosen et al., 2020) at nearly all levels of proficiency, from beginners to advanced learners ${ }^{7}$ (Rosen et al., 2020),</p>
<p><sup id="fnref2:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p>but also for the Czech section of MERLIN (Boyd et al., 2014). Instead of relying on the manual and automatic error annotations available in CzeSL and MERLIN, errors in spelling and grammar in the entire GECCC were detected and normalized manually, then categorized automatically using the ERRor ANnotation Toolkit - ERRANT (Bryant et al., 2017), which was modified for Czech. ${ }^{8}$ The GECCC corpus is available in its raw untokenized form and in $\mathrm{M}^{2}$ format (Dahlmeier and $\mathrm{Ng}, 2012$ ). Basic metadata are available about sex, age and L1 family, with links to a richer set.</p>
<p>English-FCE The FCE Corpus (Yannakoudakis et al., 2011) consists of essays written by candidates for the First Certificate in English (FCE) exam (now "B2 First") designed by Cambridge English to certify learners of English at CEFR level B2. It is part of the larger Cambridge Learner Corpus that has been annotated for grammatical errors (Nicholls, 2003). The FCE Corpus has been used in grammatical error detection (and correction) experiments on numerous occasions, including the BEA 2019 Shared Task (Bryant et al., 2019).</p>
<p>English-REALEC REALEC (Russian ErrorAnnotated Learner English Corpus) is a corpus of essays written by Russian L1 university students in their final English language examinations designed for students at B1-B2 CEFR levels (Vinogradova and Lyashevskaya, 2022). The requirements for the two types of essays in this examination are the same as in IELTS ${ }^{9}$ Task 1 and Task 2. The grammar errors in these essays were annotated manually by specially trained students in the Linguistics Bachelor program. The sentences from all essays were shuffled for the MultiGED shared task to avoid any breach of anonymity, and sentences without any errors identified by the annotators were manually double-checked once more. At both stages of annotating errors and processing sentences for the MultiGED shared task, no stylistic improvements were suggested; all sentences remained authentic.</p>
<p>German For German L2 data, we made use of the Falko-MERLIN GEC corpus as introduced in</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Boyd (2018). Falko-MERLIN involved the amalgamation of the Falko Corpus - specifically the 248 texts from 'FalkoEssayL2' v2.42 and the 196 texts from 'FalkoEssayWhig' v2.02 (Reznicek et al., 2012) - and 1033 texts from the German section of MERLIN v1.1 (Boyd et al., 2014). Both corpora were annotated in a similar fashion, according to guidelines which demanded only minimal corrections for grammaticality. Falko contains essays at a more advanced proficiency level whereas MERLIN covers a broader range of proficiencies.</p>
<p>Italian The Italian data is drawn from the trilingual learner corpus MERLIN, which contains not only Czech and German texts but also 813 Italian written learner productions (letters and emails), collected within the framework of standardised language tests (Boyd et al., 2014). Similar to the German texts, the handwritten originals of the Italian texts in MERLIN were transcribed and normalised manually, with error annotations added on various levels of linguistic accuracy. Like in the German data, for the shared task we also used the provided minimal corrections for grammaticality, which ignore uncommon stylistic choices.</p>
<p>Swedish For Swedish, we used the SweLL-gold corpus (Volodina et al., 2019), that contains 502 essays written by adult learners at different proficiency levels. The essays were manually transcribed, pseudonymized, normalized and correction annotated. Due to the presence of personal information in the texts, the corpus is under GDPR protection ${ }^{10}$ and is distributed for individual use on signing an agreement form. For this reason, texts in their entirety cannot be freely distributed, for example, for use in shared tasks. Shuffling of sentences and removal of demographic information was therefore necessary to make SweLL-gold data openly available for the MultiGED shared task.</p>
<h3>3.2 Data pre-processing</h3>
<p>The starting point for the corpora featuring in MultiGED varied from dataset to dataset. We took steps to reformat and reshape the corpora so that they were in a common format, as described in Section 3.3 and shown in Table 1. This meant that each corpus needed to be transformed into tabular form with one token per row in the first col-</p>
<p><sup id="fnref3:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p>umn and labels in the second column, in line with one of the conventional formats for GED and NLP tasks used more widely. Pre-processing steps for each corpus are described below, starting with the three corpora which have been previously used for GED experiments: Czech GECCC, English FCE and German Falko-MERLIN.</p>
<h3>3.2.1 Established GED corpora</h3>
<p>For Czech, we retained only the learner section of the corpus, which involved first obtaining a list of identifiers for the texts written by L2 learners of Czech (recorded in the 'Domain' field of the metadata file). The GECCC text ID file is aligned with the 'input' file of one sentence per line, but not with the error annotations file (in $\mathrm{M}^{2}$ format: because $\mathrm{M}^{2}$ format involves multiple lines per sentence). We therefore attempted to align the original input sentences with the tokenized sentences given in the $\mathrm{M}^{2}$ file, where tokenization meant that exact matches were often unlikely. We used optimal string alignment as implemented in the stringdist package for R (van der Loo, 2014), allowing for a distance up to two-thirds the character length of the original sentence, and breaking any ties manually. Text sequences ${ }^{11}$ written by L2 learners were then converted from $\mathrm{M}^{2}$ to CoNLL format. We used the training, development and test splits already defined in the GECCC.</p>
<p>For the English-FCE we started with the $\mathrm{M}^{2}$ format files made available in the BEA-2019 shared task ${ }^{12}$. The train/dev/test splits are longestablished for the FCE Corpus: we simply converted the $\mathrm{M}^{2}$ files to CoNLL-format and left the splits as they are. To produce files for GED - i.e. with binary error labels - we labelled any token bearing a correction (or following a missing word) as ' $i$ ' and all other tokens were labelled ' $c$ '.</p>
<p>Boyd (2018) described the German FalkoMERLIN corpus and defined the train/dev/test splits that we use. We obtained the dataset as $\mathrm{M}^{2}$ files from Adriane Boyd's GitHub repository ${ }^{13}$; note that the data link there carries a security warning and so we made the files available in the German directory of the MultiGED GitHub reposi-</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup>tory. We converted the $\mathrm{M}^{2}$ files to CoNLL format ${ }^{14}$, and again used the error corrections to arrive at our final token labels, binary ' $c$ ' (correct) or ' $i$ ' (incorrect).</p>
<h3>3.2.2 New GED corpora</h3>
<p>Next, we turn to the three corpora which have not previously featured in GED experiments to the best of our knowledge: English REALEC, Italian MERLIN and Swedish SweLL.</p>
<p>Using manually annotated parts of English REALEC in .brat format from https://re alec.org/index.xhtml#/exam/, a tabular representation was produced. Given that the manually annotated subsection of REALEC is relatively small, we only released a development set and a test set for this corpus (i.e., not a training set), randomly assigning each sentence to dev or test. The annotation style in REALEC is different from the other corpora in the shared task: errors are annotated over spans at least one token long. As a result, non-errorful tokens may be included in the span; e.g., [present-day rythme $\rightarrow$ the present-day rhythm], which means it is less straightforward to precisely map edit labels to tokens. We nevertheless attempted to automatically infer which tokens should be marked as incorrect using heuristics; e.g. by removing unchanged tokens from the peripheries of both sides of the edit span. Because this conversion process became noisier the longer the error span however, we opted not to attempt it for spans longer than eight tokens, meaning that these longer corrections (just $2.9 \%$ of the multiword corrections) are left as they are (i.e. all tokens are labelled as incorrect).</p>
<p>For Italian MERLIN we started with the Exmaralda ${ }^{15}$ files provided with the 2018 release of the MERLIN corpus (v1.1) ${ }^{16}$. The .exb files contain manually corrected tokenisation and annotations on various layers, including span annotations for error annotation and correction, or token level annotation for edit operations, etc. While the corpus contains annotations for both TH1 (i.e. target hypothesis 1 , which only contains form-based corrections of linguistic accuracy) and TH2 (i.e. target hypothesis 2 , which also contains meaningbased corrections considering semantics) as de-</p>
<p><sup id="fnref4:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p>fined in Reznicek et al. (2013), we only used the aligned original and TH1 layers of the multilayer annotation.</p>
<p>We transferred the aligned layers into a vertical tab-separated table format, marking any corrections in the normal way as ' $i$ ' and uncorrected tokens as ' $c$ '. We omitted lines with unreadable tokens in the original (marked with '-unreadable-' in the token layer), segmented the text where we found sentence-final punctuation in order to insert empty lines between sequences, and applied corrections involving token insertion to the following token in the sequence (in the multilayer annotation of Exmaralda these are indicated against empty tokens). We randomly assigned each sequence to train/dev/test with a probability of $.8, .1, .1$ respectively.</p>
<p>Finally, for Swedish we started with the tabular representation of the data first produced by Casademont Moner and Volodina (2022), which was derived from SweLL-gold in JSON format. As part of processing the corpus, we removed \$ symbols (indicating illegible characters), replaced the "-gen" marker with a possessive 's' suffix, and randomly selected one of four options wherever we encountered an anonymisation placeholder. For instance, for any occurrence of the "<em>-hemland" ('homeland') placeholder, we sampled one of {'Brasil', 'Spanien', 'Irak', 'Kina'} (Brazil, Spain, Iraq, China); and for any occurrence of the "</em>-svensk-stad" ('Swedish town') placeholder, we sampled a made-up placename from {'Sydden', 'Norrebock', 'Rosaborg', 'Ã–gglestad'}. Similar fake replacements were made for '<em>-geoplats' ('geolocation'), '</em>-plats' ('place'), '<em>-institution', '</em>-skola' ('school'), '<em>'and' ('country'), '</em>-region', '<em>-stad' ('town'), '</em>'injen' ('transport line').</p>
<p>As a GDPR-related requirement of using SweLL, we randomly shuffled the order of sentences in order to protect individual privacy. We then assigned the sentences to train/dev/test splits with a probability of $.8, .1, .1$ respectively. As with Italian MERLIN, in SweLL the insertion correction type is marked against an empty token: therefore we carried such annotations forward to the next token, in line with other corpora in MultiGED, and omitted the empty tokens. Subsequently, the usual ' $i$ ' and ' $c$ ' labels were generated based on the presence of corrections (or not) against each token in the file.</p>
<h3>3.3 Data format</h3>
<p>MultiGED data is, thus, provided in a tabseparated format consisting of two columns and no headers: the first column contains the token and the second column contains the label (c or i), as shown in Table 1. Each sequence is separated by an empty line, and double quotes are escaped ( $\backslash$ "). Error labels ( $\perp$ ) are attached on the same line where the errors are, with one exception: if an insertion is necessary, the $\perp$ label is attached to the next token; e.g., the right-hand side of Table 1. System outputs should be generated in the same format.</p>
<h2>4 Evaluation</h2>
<p>System evaluation was carried out in terms of token-based $\mathrm{F}<em 0.5="0.5">{0.5}$ to be consistent with previous work in error detection (Bell et al., 2019; Kaneko and Komachi, 2019; Yuan et al., 2021). It has been customary to evaluate GED/GEC systems in terms of $\mathrm{F}</em>$ ) were hence calculated in the standard way based on the total number of true positives (TP), false positives (FP) and false negatives (FN) (Equation 1-3) with the parameter $\beta=0.5$.}$, which weights precision twice as much as recall, since the CoNLL-2014 shared task, given that it is more important to an end user that a system makes a correct prediction than to necessarily detect all errors ( Ng et al., 2014). Precision (P), Recall (R) and F-score ( $\mathrm{F}_{\beta</p>
<p>$$
\begin{gathered}
P=\frac{T P}{T P+F P} \
F_{\beta}=\left(1+\beta^{2}\right) \times \frac{P \times R}{\left(\beta^{2} \times P\right)+R}
\end{gathered}
$$</p>
<p>One notable limitation of token-based $\mathrm{F}_{0.5}$ is that systems will receive multiple rewards for detecting each erroneous token in a multi-word edit, e.g. [In other hand $\rightarrow$ On the other hand], when it might otherwise be more realistic to treat such cases as a single error. This approximation is generally acceptible, however, given that multi-token errors are typically much rarer than single token errors, and it may in fact be beneficial to reward systems for the partial detection of multi-token errors. It is nevertheless worth keeping this property of token-based evaluation in mind.</p>
<table>
<thead>
<tr>
<th>Team</th>
<th>System description</th>
</tr>
</thead>
<tbody>
<tr>
<td>EliCoDe <br> Colla et al. (2023)</td>
<td>XLM-RoBERTa language model pretrained on $\approx 100$ languages with a stacked linear classifier on top, with a dropout layer in-between fine-tuned 5 different models for 5 languages on train (or train+dev) data</td>
</tr>
<tr>
<td>DSL-MIM-HUS <br> Ngo et al. (2023)</td>
<td>XLM-RoBERTa language model from the HuggingFace repo pretrained on $\approx 100$ languages, fine-tuned jointly on all MultiGED datasets i.e. there is only one trained model for prediction of all the test datasets</td>
</tr>
<tr>
<td>Brainstorm Thinkers</td>
<td>mBERT, for all six datasets</td>
</tr>
<tr>
<td>VLP-char (no eng-realec) <br> Ngo et al. (2023)</td>
<td>character-based LSTM model with two recurrent layers, unidirectional supervised approach, separate model for each dataset, REALEC excluded no external datasets</td>
</tr>
<tr>
<td>NTNU-TRH <br> Bungum et al. (2023)</td>
<td>multilingual system based on LSTMs, GRUs and standard RNNs with multilingual Flair embeddings for a sequence-to-sequence labeling multitask learning</td>
</tr>
<tr>
<td>su-dali (only swe) <br> Kurfali and Ã–stling (2023)</td>
<td>distantly-supervised transformer-based machine translation (MT) system trained solely on artificial dataset of 200 million sentences, only Swedish no supervision, training or fine-tuning on any labeled data</td>
</tr>
</tbody>
</table>
<p>Table 3: Overview of submitted systems, lister in the order of registration</p>
<h3>4.1 CodaLab</h3>
<p>Evaluation was formally carried out on the CodeLab competition platform ${ }^{17}$, with participants being allowed to anonymously make a maximum of 2 submissions on the test data during the test phase. Each submission was expected to contain output for as many languages as the team wished to participate in, and so participants could effectively make a maximum of 2 submissions for each dataset in the shared task.</p>
<p>It is extremely important to note that we treated the best score from either submission as the official result for each team. This means that if a team scored 50 in Language A and 60 in Language B from Submission 1, but 45 in Language A and 70 in Language B from Submission 2, the official score for the team is 50 in Language A (Submission 1) and 70 in Language B (Submission 2). In other words, we did not penalise teams for uploading their best system output in different submissions.</p>
<h2>5 Teams, Approaches, Results</h2>
<p>In total, six teams participated in the task, representing five different countries: China, Italy, Norway, Sweden and Vietnam. Four teams developed systems for all five languages (and six datasets): EliCoDe (Colla et al., 2023), NTNUTRH (Bungum et al., 2023), DDSL-MIM-HUS</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup>(Ngo et al., 2023, System 1) and Brainstorm Thinkers (no submitted system description); one team submitted results for all five languages excluding the English-REALEC dataset: VLP-char (Ngo et al., 2023, System 2); and one team submitted results for Swedish only: su-dali (Kurfali and Ã–stling, 2023).</p>
<p>The different approaches that each team took are summarized in Table 3. The most successful approaches relied on BERT-like large language models (see Table 4). The team with the best average result across all languages, EliCoDe, fine-tuned a different model for each dataset and showed considerably superior recall capabilities on most datasets (Colla et al., 2023). The secondbest average result came from the DSL-MIM-HUS team, who fine-tuned one pre-trained model on all 6 datasets at once (Ngo et al., 2023). The same team also trained a character-based LSTM, VLPchar. The NTNU-TRH team used LSTMs as well, implementing their systems with FlairNLP and comparing monolingual and multilingual scenarios (Bungum et al., 2023). These latter approaches require less data for training but show weaker performance in recall and precision, either tending to detect fewer errors or produce a greater number of false positives. The su-dali team used artificial data mimicking the error distribution from the Swedish source corpus, and achieved very good results on Swedish showing that access to manually annotated training data can be avoided (Kur-</p>
<p>a. Results on Czech</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Team</th>
<th style="text-align: center;">$\mathbf{P}$</th>
<th style="text-align: center;">$\mathbf{R}$</th>
<th style="text-align: center;">$\mathbf{F}_{0.5} \downarrow$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">EliCoDe</td>
<td style="text-align: center;">82.01</td>
<td style="text-align: center;">51.79</td>
<td style="text-align: center;">$\mathbf{7 3 . 4 4}$</td>
</tr>
<tr>
<td style="text-align: left;">DSL-MIM-HUS</td>
<td style="text-align: center;">58.31</td>
<td style="text-align: center;">55.69</td>
<td style="text-align: center;">57.76</td>
</tr>
<tr>
<td style="text-align: left;">Brainstorm Thinkers</td>
<td style="text-align: center;">62.35</td>
<td style="text-align: center;">23.44</td>
<td style="text-align: center;">46.81</td>
</tr>
<tr>
<td style="text-align: left;">VLP-char</td>
<td style="text-align: center;">34.93</td>
<td style="text-align: center;">63.95</td>
<td style="text-align: center;">38.42</td>
</tr>
<tr>
<td style="text-align: left;">NTNU-TRH</td>
<td style="text-align: center;">80.65</td>
<td style="text-align: center;">6.49</td>
<td style="text-align: center;">24.54</td>
</tr>
<tr>
<td style="text-align: left;">Majority</td>
<td style="text-align: center;">84.32</td>
<td style="text-align: center;">43.22</td>
<td style="text-align: center;">70.85</td>
</tr>
</tbody>
</table>
<p>b. Results on English - FCE</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Team</th>
<th style="text-align: center;">$\mathbf{P}$</th>
<th style="text-align: center;">$\mathbf{R}$</th>
<th style="text-align: center;">$\mathbf{F}_{0.5} \downarrow$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">EliCoDe</td>
<td style="text-align: center;">73.64</td>
<td style="text-align: center;">50.34</td>
<td style="text-align: center;">$\mathbf{6 7 . 4 0}$</td>
</tr>
<tr>
<td style="text-align: left;">DSL-MIM-HUS</td>
<td style="text-align: center;">72.36</td>
<td style="text-align: center;">37.81</td>
<td style="text-align: center;">61.18</td>
</tr>
<tr>
<td style="text-align: left;">Brainstorm Thinkers</td>
<td style="text-align: center;">70.21</td>
<td style="text-align: center;">37.55</td>
<td style="text-align: center;">59.81</td>
</tr>
<tr>
<td style="text-align: left;">VLP-char</td>
<td style="text-align: center;">20.76</td>
<td style="text-align: center;">29.53</td>
<td style="text-align: center;">22.07</td>
</tr>
<tr>
<td style="text-align: left;">NTNU-TRH</td>
<td style="text-align: center;">81.37</td>
<td style="text-align: center;">1.84</td>
<td style="text-align: center;">8.45</td>
</tr>
<tr>
<td style="text-align: left;">Majority</td>
<td style="text-align: center;">85.35</td>
<td style="text-align: center;">32.48</td>
<td style="text-align: center;">64.39</td>
</tr>
</tbody>
</table>
<p>c. Results on English - REALEC</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Team</th>
<th style="text-align: center;">$\mathbf{P}$</th>
<th style="text-align: center;">$\mathbf{R}$</th>
<th style="text-align: center;">$\mathbf{F}_{0.5} \downarrow$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">DSL-MIM-HUS</td>
<td style="text-align: center;">62.81</td>
<td style="text-align: center;">28.88</td>
<td style="text-align: center;">$\mathbf{5 0 . 8 6}$</td>
</tr>
<tr>
<td style="text-align: left;">EliCoDe</td>
<td style="text-align: center;">44.32</td>
<td style="text-align: center;">40.73</td>
<td style="text-align: center;">43.55</td>
</tr>
<tr>
<td style="text-align: left;">Brainstorm Thinkers</td>
<td style="text-align: center;">48.19</td>
<td style="text-align: center;">31.22</td>
<td style="text-align: center;">43.46</td>
</tr>
<tr>
<td style="text-align: left;">NTNU-TRH</td>
<td style="text-align: center;">51.34</td>
<td style="text-align: center;">1.13</td>
<td style="text-align: center;">5.19</td>
</tr>
<tr>
<td style="text-align: left;">Majority</td>
<td style="text-align: center;">65.46</td>
<td style="text-align: center;">27.23</td>
<td style="text-align: center;">51.11</td>
</tr>
</tbody>
</table>
<p>d. Results on German</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Team</th>
<th style="text-align: center;">$\mathbf{P}$</th>
<th style="text-align: center;">$\mathbf{R}$</th>
<th style="text-align: center;">$\mathbf{F}_{0.5} \downarrow$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">EliCoDe</td>
<td style="text-align: center;">84.78</td>
<td style="text-align: center;">73.75</td>
<td style="text-align: center;">$\mathbf{8 2 . 3 2}$</td>
</tr>
<tr>
<td style="text-align: left;">DSL-MIM-HUS</td>
<td style="text-align: center;">77.80</td>
<td style="text-align: center;">51.92</td>
<td style="text-align: center;">70.75</td>
</tr>
<tr>
<td style="text-align: left;">Brainstorm Thinkers</td>
<td style="text-align: center;">77.94</td>
<td style="text-align: center;">47.55</td>
<td style="text-align: center;">69.11</td>
</tr>
<tr>
<td style="text-align: left;">NTNU-TRH</td>
<td style="text-align: center;">83.56</td>
<td style="text-align: center;">15.58</td>
<td style="text-align: center;">44.61</td>
</tr>
<tr>
<td style="text-align: left;">VLP-char</td>
<td style="text-align: center;">25.18</td>
<td style="text-align: center;">44.27</td>
<td style="text-align: center;">27.56</td>
</tr>
<tr>
<td style="text-align: left;">Majority</td>
<td style="text-align: center;">87.80</td>
<td style="text-align: center;">49.88</td>
<td style="text-align: center;">76.21</td>
</tr>
</tbody>
</table>
<p>e. Results on Italian</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Team</th>
<th style="text-align: center;">$\mathbf{P}$</th>
<th style="text-align: center;">$\mathbf{R}$</th>
<th style="text-align: center;">$\mathbf{F}_{0.5} \downarrow$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">EliCoDe</td>
<td style="text-align: center;">86.67</td>
<td style="text-align: center;">67.96</td>
<td style="text-align: center;">$\mathbf{8 2 . 1 5}$</td>
</tr>
<tr>
<td style="text-align: left;">DSL-MIM-HUS</td>
<td style="text-align: center;">75.72</td>
<td style="text-align: center;">38.67</td>
<td style="text-align: center;">63.55</td>
</tr>
<tr>
<td style="text-align: left;">Brainstorm Thinkers</td>
<td style="text-align: center;">70.65</td>
<td style="text-align: center;">36.46</td>
<td style="text-align: center;">59.49</td>
</tr>
<tr>
<td style="text-align: left;">NTNU-TRH</td>
<td style="text-align: center;">93.38</td>
<td style="text-align: center;">19.84</td>
<td style="text-align: center;">53.62</td>
</tr>
<tr>
<td style="text-align: left;">VLP-char</td>
<td style="text-align: center;">25.79</td>
<td style="text-align: center;">44.24</td>
<td style="text-align: center;">28.14</td>
</tr>
<tr>
<td style="text-align: left;">Majority</td>
<td style="text-align: center;">90.25</td>
<td style="text-align: center;">40.95</td>
<td style="text-align: center;">72.74</td>
</tr>
</tbody>
</table>
<p>f. Results on Swedish</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Team</th>
<th style="text-align: center;">$\mathbf{P}$</th>
<th style="text-align: center;">$\mathbf{R}$</th>
<th style="text-align: center;">$\mathbf{F}_{0.5} \downarrow$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">EliCoDe</td>
<td style="text-align: center;">81.80</td>
<td style="text-align: center;">66.34</td>
<td style="text-align: center;">$\mathbf{7 8 . 1 6}$</td>
</tr>
<tr>
<td style="text-align: left;">DSL-MIM-HUS</td>
<td style="text-align: center;">74.85</td>
<td style="text-align: center;">44.92</td>
<td style="text-align: center;">66.05</td>
</tr>
<tr>
<td style="text-align: left;">Brainstorm Thinkers</td>
<td style="text-align: center;">73.81</td>
<td style="text-align: center;">39.94</td>
<td style="text-align: center;">63.11</td>
</tr>
<tr>
<td style="text-align: left;">su-dali</td>
<td style="text-align: center;">82.41</td>
<td style="text-align: center;">27.18</td>
<td style="text-align: center;">58.60</td>
</tr>
<tr>
<td style="text-align: left;">VLP-char</td>
<td style="text-align: center;">26.40</td>
<td style="text-align: center;">55.00</td>
<td style="text-align: center;">29.46</td>
</tr>
<tr>
<td style="text-align: left;">NTNU-TRH</td>
<td style="text-align: center;">80.12</td>
<td style="text-align: center;">5.09</td>
<td style="text-align: center;">20.31</td>
</tr>
<tr>
<td style="text-align: left;">Majority</td>
<td style="text-align: center;">89.90</td>
<td style="text-align: center;">45.37</td>
<td style="text-align: center;">75.15</td>
</tr>
</tbody>
</table>
<p>Table 4: Results for each language and team in terms of Precision (P), Recall (R) and F-score ( $\mathrm{F}<em 0.5="0.5">{0.5}$ ). The Majority score is based on the majority predicted tokenbased labels across all systems.
fali and Ã–stling, 2023).
Czech Systems that relied on Transformerbased architectures (the top three in Table 4) achieved the top-3 $\mathrm{F}</em>$ scores. Despite that, the best recall comes from the LSTM-based system (VPL-char).</p>
<p>English-FCE The performance of the RoBERTa-based architecture, fine-tuned exclusively on the FCE dataset by EliCoDe team, outperformed other architectures in all evaluation metrics, indicating its superior efficacy for the FCE dataset.</p>
<p>English-REALEC The results obtained from the REALEC dataset were relatively low compared to other datasets, which may be attributed to the different annotation style in REALEC (see Section 3.2), and the fact that REALEC was both released later in the shared task and without a training split.</p>
<p>German The highest scores were obtained by all teams on the German Falko-MERLIN dataset. Remarkably, the teams NTNU-TRH and VLPchar, who did not use external data, exhibited substantially better performance on the German dataset.</p>
<p>Italian The solutions submitted for the German and Italian datasets exhibited the highest performance levels compared to the other datasets. This finding could potentially be attributed to the fact that these datasets were sourced from the MERLIN corpus and possessed a high level of consistency in their annotations.</p>
<p>Swedish The Swedish dataset received the highest participation rate among all the datasets. The best performance was achieved by Transformerbased architectures, which is consistent with the performance on other datasets. Nevertheless, satisfactory results were also achieved by solutions using LSTMs without pre-training or additional data.</p>
<p>Altogether, shared task participants submitted different systems representing a variety of approaches, including machine translation, LSTMs, mBERT and XLM-RoBERTa (Table 3). The best results were achieved by teams employing the multilingual XLM-RoBERTa (large) language model pre-trained on $\approx 100$ languages (Conneau et al., 2020). The systems trained and fine-tuned</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Language</th>
<th style="text-align: left;">Team</th>
<th style="text-align: right;">Best $\mathbf{F}_{0.5}$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">German</td>
<td style="text-align: left;">EliCoDe</td>
<td style="text-align: right;">82.32</td>
</tr>
<tr>
<td style="text-align: left;">Italian</td>
<td style="text-align: left;">EliCoDe</td>
<td style="text-align: right;">82.15</td>
</tr>
<tr>
<td style="text-align: left;">Swedish</td>
<td style="text-align: left;">EliCoDe</td>
<td style="text-align: right;">78.16</td>
</tr>
<tr>
<td style="text-align: left;">Czech</td>
<td style="text-align: left;">EliCoDe</td>
<td style="text-align: right;">73.44</td>
</tr>
<tr>
<td style="text-align: left;">Eng-FCE</td>
<td style="text-align: left;">EliCoDe</td>
<td style="text-align: right;">67.40</td>
</tr>
<tr>
<td style="text-align: left;">Eng-REALEC</td>
<td style="text-align: left;">DSL-MIM-HUS</td>
<td style="text-align: right;">50.87</td>
</tr>
</tbody>
</table>
<p>Table 5: Best results for each language dataset.
separately for each language dataset by the EliCoDe team performed substantially better than the ones that used one multilingual model for all languages (team DSL-MIM-HUS), with the exception of the English-REALEC dataset, where the results were reversed (see the results for the topperforming systems in Table 5). This is an important insight, because the EliCoDe team also showed that for some language datasets multilingual models, fine-tuned on all datasets, performed better than monolingually fine-tuned ones (Colla et al., 2023). On the one hand, it is intuitive that monolingual models might perform better than multilingual models because they are more specially trained for a particular target language, but on the other hand, multilingual models might be expected to perform better because they have access to richer multilingual representations from linguistically-related languages. In either case, both approaches have different advantages which are worth exploring further.</p>
<p>Table 4 also lists the scores from a token-based majority vote for each language in gray. This is based on the performance of a system relying on a majority vote among all system outputs. For the two languages with an even number of system outputs - English-REALEC and Swedish - a fallback was implemented in case of a tie, namely to choose the output of the best system (EliCoDe in both languages). As can be observed, this majority system led to better precision in all languages and lower recall. If this score were to be included in the ranking, it would end up on place two for all languages, except for English-REALEC where, with an $\mathrm{F}_{0.5}$ of 51.11 it would obtain first place.</p>
<p>In Figure 2 we combine all system output to get more insights in the error detection (the i labels). The blue bars (on the left) represent the percentage of errors that were detected by all participatings systems in each language, whereas the orange
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Percentage of errors in the test set which were either detected by all (blue bars, on the left) or none (orange bars, on the right) of the participating teams.
bars (on the right) illustrate the percentage of errors none of which the systems were able to detect. What draws the attention are the high percentages of errors none of the approaches were able to detect for English ( $33 \%$ for English FCE and 53\% for English REALEC, respectively). Also, when ranked by best results for all languages (Table 5) it is counter-intuitive to see that English comes at the bottom, as English has typically received the most attention in GED. REALEC is a special case - we did not provide training data for it, and obviously models trained on other languages or other datasets for the same language did not generalize well to REALEC - hypothetically because REALEC had a different type of annotation approach. However, an interesting question is why performance on the English-FCE dataset was lower than on all other languages? In this respect, the EliCoDe team (Colla et al., 2023) carried out an analysis of training/development splits versus the test split per language for linguistic similarity and identified bigger differences between English splits than any other MultiGED languages; they conclude this may be the reason why scores were lower on English.</p>
<p>A short look at the six system output files for Swedish shows that most of the errors that all systems missed (i.e. labeled them as c instead of i) are those that cover:</p>
<ul>
<li>lexical choices, for example non-idiomatic use of vocabulary, e.g. Jag tror att religion <em>har ingen roll... ${ }^{18}$ ('I think that religion </em>has no role...')</li>
<li>verb tense harmonization with other verb
${ }^{18}$ The missed token shown in bold.</li>
</ul>
<p>tenses used in the sentence, e.g. Hon tycker att Hans Ã¤r hennes Ã¤kta kÃ¤rlek men sÃ¥ Â»var det inte ('She thinks that Hans is her real love, but it *was not the case')</p>
<ul>
<li>a few preposition and syntactic construction choices, e.g. Hur gÃ¥r det Â»med dig? ('How is it going *with you?')</li>
<li>few of the errors missed by all systems would in fact require longer context than one sentence for determiniting the need of a correction</li>
</ul>
<p>Note that these are only indicative insights and a more thorough analysis would be necessary to draw any proper conclusions.</p>
<p>Rather obviously, spelling errors resulting in 'non-words' (OOVs - out-of-vocabulary strings) were easier to detect than errors resulting in some existing word forms ('real-word errors'). Whereas the entire Czech test data included $6.937 \%$ of nonwords, there were much fewer non-words among the 1716 incorrect word forms that all the systems failed to detect: $0.047 \%$. The almost $15: 1$ ratio was lower for the English data (about 7:1 for FCE: $1.440 \%$ vs. $0.199 \% ; 4: 1$ for REALEC: $1.135 \%$ vs. $0.310 \%$ ), but it is still clear that real-word errors were harder to detect.</p>
<p>In future, it would be useful to see error distributions made by systems by types of (gold) error labels [e.g. POLMS ${ }^{19}$ ] and account for their effect on different language systems performance. Another possible interesting analysis could be to correlate system performance with learners' language proficiency, their first languages, as well as with the effect of essay tasks on system performance.</p>
<h2>6 Comparison with previous work</h2>
<p>To provide some context for the MultiGED results on the English FCE benchmark, we present Table 6, which summarisee results on English GED in the past five years. The state-of-the-art has been gradually pushed: Bell et al. (2019) explored the effect of using different contexual embeddings and their generalizability to different datasets, showing the potential of "leveraging information learned in an unsupervised manner from high volumes of unlabeled data" and their sensitivity to error types,</p>
<p><sup id="fnref6:0"><a class="footnote-ref" href="#fn:0">1</a></sup>| System / English FCE | P | R | $\mathbf{F}_{0.5}$ |
| :-- | :--: | :--: | :--: |
| MultiGED-23 |  |  |  |
| EliCoDe | 73.64 | 50.34 | $\mathbf{6 7 . 4 0}$ |
| DSL-MIM-HUS | 72.36 | 37.81 | 61.18 |
| State-of-the-art |  |  |  |
| Yuan-2021, BERT | 75.73 | 47.98 | 67.88 |
| Yuan-2021, XLNet | 77.50 | 49.81 | 69.75 |
| Yuan-2021, ELECTRA | 82.05 | 50.49 | $\mathbf{7 2 . 9 3}$ |
| Previous results |  |  |  |
| Kaneko-Komachi-2019 | 68.87 | 43.45 | 61.65 |
| Bell-2019, BERT BASE | 64.96 | 38.89 | 57.28 |</p>
<p>Table 6: Comparison to previous GED results on English FCE dataset (Yuan et al., 2021; Kaneko and Komachi, 2019; Bell et al., 2019).
with BERT embeddings (Peters et al., 2017) being especially promising ( $\mathrm{F}<em 0.5="0.5">{0.5} 57.28$ ). Kaneko and Komachi (2019) complemented BERT BASE with a Multi-Head Multi-Layer Attention (MHMLA) function to achieve a new state of the art for GED, reaching $\mathrm{F}</em> 72.93$ on the FCE benchmark. Two years later, the results by Yuan et al. (2021) are still state-of-the-art. The bulk of work on English provides potential ways for improvement on other MultiGED languages if nothing else, to see whether the same trends hold cross-linguistically.} 61.65$ on FCE. Yuan et al. (2021) meanwhile showed that ELECTRA (Clark et al., 2020) has a "discriminative pre-training objective that is conceptually similar to GED", which improved GED results by a large margin on several public English datasets, reaching $\mathrm{F}_{0.5</p>
<p>We are unable to make similar comparisons for the other languages in MultiGED because this is the first time these languages have been evaluated in the context of GED. More specifically:</p>
<ul>
<li>For Czech, previous research explores grammatical error correction (GEC) rather than detection (e.g. NÃ¡plava and Straka, 2019; NÃ¡plava et al., 2022). There has been some previous work on the evaluation of Czech error detection in the context of a spellchecking tool, Korektor (Ramasamy et al., 2015), however, this is not fully compatible with the scope of errors in MultiGED.</li>
<li>For German, although there is some work on sentence-level error detection (e.g. Boyd, 2012) and error correction (e.g. Boyd, 2018; Sun et al., 2022; PajÄ…k and PajÄ…k, 2022), there is no previous work on token-level GED.</li>
</ul>
<table>
<thead>
<tr>
<th style="text-align: left;">Feedback type</th>
<th style="text-align: left;">Example</th>
<th style="text-align: left;">NLP task</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">1. correct/incorrect</td>
<td style="text-align: left;">incorrect</td>
<td style="text-align: left;">sentence-level acceptability judgment</td>
</tr>
<tr>
<td style="text-align: left;">2. highlighting</td>
<td style="text-align: left;">I saw show last night .</td>
<td style="text-align: left;">GED - grammatical error detection (per token)</td>
</tr>
<tr>
<td style="text-align: left;">3. metalinguistic</td>
<td style="text-align: left;">note definiteness / morphology</td>
<td style="text-align: left;">multi-class GED</td>
</tr>
<tr>
<td style="text-align: left;">4. error explanation</td>
<td style="text-align: left;">note rules for noun definiteness</td>
<td style="text-align: left;">instructive feedback generation</td>
</tr>
<tr>
<td style="text-align: left;">5. correct answer</td>
<td style="text-align: left;">I saw the show last night .</td>
<td style="text-align: left;">GEC - grammatical error correction</td>
</tr>
<tr>
<td style="text-align: left;">6. level/grade</td>
<td style="text-align: left;">CEFR level A2</td>
<td style="text-align: left;">AEG - automatic essay grading</td>
</tr>
</tbody>
</table>
<p>Table 7: NLP tasks for different feedback types</p>
<ul>
<li>For Italian, we are unaware of any work on GED or GEC at all.</li>
<li>For Swedish, rule-based error detection was developed within the Granska project, (e.g. Birn, 2000; Arppe, 2000), however, it is difficult to use these results for comparison since the evaluation metrics and test sets are different, as is the scope of errors.</li>
</ul>
<p>We can therefore conclude that the MultiGED2023 shared task has established a new set of benchmark datasets and state-of-the-art GED baselines for four new languages in this domain: Czech, German, Italian and Swedish.</p>
<h2>7 Concluding remarks</h2>
<p>We have presented datasets and results for the task of multilingual grammatical error detection for five languages and six corpora, three of which have not previously featured in the domain of GED.</p>
<p>We view this contribution primarily as a step towards empowering "smaller" languages and decreasing the Matthew effect in this field (SÃ¸gaard, 2022; Perc, 2014; Bol et al., 2018). It is our hope that the availability of these datasets and baselines will spark further GED research for these languages. Secondly, we view this shared task as a step towards instructional feedback generation in ICALL tutoring systems - corrections, error classification and grammar explanations being reserved as potential future shared tasks, see Table 7 for some ideas.</p>
<p>Besides this, we summarise a few of our insights that might be useful to keep in mind for further GED experiments:</p>
<ol>
<li>Pre-trained large language models have no doubt pushed the field far forward (cf. Yuan et al., 2021; Colla et al., 2023; Ngo et al., 2023). It is left to see in the future how GPT ${ }^{20}$</li>
</ol>
<p><sup id="fnref7:0"><a class="footnote-ref" href="#fn:0">1</a></sup>models can influence the field (e.g. Radford et al., 2018; Wu et al., 2023; Lund and Wang, 2023).
2. Monolingual fine-tuning tends to outperform multilingual approaches, however, there are some exceptions (Colla et al., 2023; Ngo et al., 2023; Bungum et al., 2023), and more attention should be given to multilingual approaches.
3. Embeddings of various types can have a significant impact on system performance (Bungum et al., 2023).
4. Artificial data containing error distributions similar to the test data facilitates reaching competitive performance with relatively low costs (KurfalÄ± and Ã–stling, 2023), and is a promising way to go.
5. The quality of data annotation is critical for high performance, as has been indicated by the results on different MultiGED languages, the ones coming from MERLIN (German and Italian) showing better results compared to other annotation paradigms (see Section 5 for descriptions of Italian).</p>
<p>Finally, we would like to encourage those who have L2 data and are willing to use it for a shared task on L2 language in combination with other languages, to make contact with the Computational SLA working group. ${ }^{21}$ It would be especially welcome if languages from beyond the Indo-European group could feature in future shared tasks.</p>
<h2>Acknowledgements</h2>
<p>The first author has been supported by the Swedish SprÃ¥kbanken Text and by HumInfra through funding from the Swedish Research Council (contracts</p>
<p><sup id="fnref5:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p>2017-00626 and 2021-00176). The second and third authors are supported by Cambridge University Press \&amp; Assessment.</p>
<h2>References</h2>
<p>Antti Arppe. 2000. Developing a grammar checker for Swedish. In Proceedings of the 12th Nordic Conference of Computational Linguistics (NODALIDA 1999), pages 13-27, Trondheim, Norway. Department of Linguistics, Norwegian University of Science and Technology, Norway.</p>
<p>Nicolas Ballier, StÃ©phane Canu, Caroline Petitjean, Gilles Gasso, Carlos Balhana, Theodora Alexopoulou, and Thomas Gaillat. 2020. Machine learning for learner English: A plea for creating learner data challenges. International Journal of Learner Corpus Research, 6(1):72-103.</p>
<p>Samuel Bell, Helen Yannakoudakis, and Marek Rei. 2019. Context is key: Grammatical error detection with contextual word representations. In Proceedings of the Fourteenth Workshop on Innovative Use of NLP for Building Educational Applications, pages 103-115, Florence, Italy. Association for Computational Linguistics.</p>
<p>Juhani Birn. 2000. Detecting grammar errors with lingsoft's Swedish grammar checker. In Proceedings of the 12th Nordic Conference of Computational Linguistics (NODALIDA 1999), pages 28-40, Trondheim, Norway. Department of Linguistics, Norwegian University of Science and Technology, Norway.</p>
<p>Thijs Bol, Mathijs de Vaan, and Arnout van de Rijt. 2018. The Matthew effect in science funding. Proceedings of the National Academy of Sciences, 115(19):4887-4890.</p>
<p>Adriane Boyd. 2012. Detecting and diagnosing grammatical errors for beginning learners of german: From learner corpus annotation to constraint satisfaction problems. Ph.D. thesis, The Ohio State University.</p>
<p>Adriane Boyd. 2018. Using Wikipedia Edits in Low Resource Grammatical Error Correction. In Proceedings of the 2018 EMNLP Workshop W-NUT: The 4th Workshop on Noisy User-generated Text, pages 79-84, Brussels, Belgium. Association for Computational Linguistics.</p>
<p>Adriane Boyd, Jirka Hana, Lionel Nicolas, Detmar Meurers, Katrin Wisniewski, Andrea Abel, Karin SchÃ¶ne, Barbora Å tindlovÃ¡, and Chiara Vettori. 2014. The MERLIN corpus: Learner Language and the CEFR. In Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC'14), pages 1281-1288, Reykjavik, Iceland. European Language Resources Association (ELRA).</p>
<p>Christopher Bryant, Mariano Felice, Ã˜istein E. Andersen, and Ted Briscoe. 2019. The BEA-2019 Shared Task on Grammatical Error Correction. In Proceedings of the Fourteenth Workshop on Innovative Use of NLP for Building Educational Applications, pages 52-75, Florence, Italy. Association for Computational Linguistics.</p>
<p>Christopher Bryant, Mariano Felice, and Ted Briscoe. 2017. Automatic Annotation and Evaluation of Error Types for Grammatical Error Correction. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 793-805, Vancouver, Canada. Association for Computational Linguistics.</p>
<p>Christopher Bryant, Zheng Yuan, Muhammad Reza Qorib, Hannan Cao, Hwee Tou Ng, and Ted Briscoe. 2023. Grammatical Error Correction: A Survey of the State of the Art. arXiv preprint arXiv:2211.05166.</p>
<p>Lars Bungum, BjÃ¶rn GambÃ¤ck, and Arild Brandrud NÃ¦ss. 2023. NTNU-TRH System at the MultiGED-2023 Shared Task on Multilingual Grammatical Error Detection. In Proceedings of the 12th Workshop on NLP for Computer Assisted Language Learning (NLP4CALL).</p>
<p>Judit Casademont Moner and Elena Volodina. 2022. Swedish MuClaGED: A new dataset for Grammatical Error Detection in Swedish. In Proceedings of the 11th Workshop on NLP for Computer Assisted Language Learning, pages 36-45.</p>
<p>Kevin Clark, Minh-Thang Luong, Quoc V. Le, and Christopher D. Manning. 2020. ELECTRA: Pretraining Text Encoders as Discriminators Rather Than Generators. In 8th International Conference on Learning Representations, ICLR. OpenReview.net.</p>
<p>Davide Colla, Matteo Delsanto, and Elisa Di Nuovo. 2023. Et.tCoDt: at MultiGED2023: fine-tuning XLM-RoBERTa for multilingual grammatical error detection. In Proceedings of the 12th Workshop on NLP for Computer Assisted Language Learning (NLP4CALL).</p>
<p>Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco GuzmÃ¡n, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. 2020. Unsupervised cross-lingual representation learning at scale. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 84408451, Online. Association for Computational Linguistics.</p>
<p>Teodor-Mihai Cotet, Stefan Ruseti, and Mihai Dascalu. 2020. Neural grammatical error correction for romanian. In 2020 IEEE 32nd International Conference on Tools with Artificial Intelligence (ICTAI), pages 625-631.</p>
<p>Daniel Dahlmeier and Hwee Tou Ng. 2012. Better Evaluation for Grammatical Error Correction. In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 568-572, MontrÃ©al, Canada. Association for Computational Linguistics.</p>
<p>Robert Dale, Ilya Anisimoff, and George Narroway. 2012. HOO 2012: A Report on the Preposition and Determiner Error Correction Shared Task. In Proceedings of the Seventh Workshop on Building Educational Applications Using NLP, pages 54-62, MontrÃ©al, Canada. Association for Computational Linguistics.</p>
<p>Robert Dale and Adam Kilgarriff. 2011. Helping Our Own: The HOO 2011 Pilot Shared Task. In Proceedings of the 13th European Workshop on Natural Language Generation, pages 242-249, Nancy, France. Association for Computational Linguistics.</p>
<p>Vidas Daudaravicius, Rafael E. Banchs, Elena Volodina, and Courtney Napoles. 2016. A Report on the Automatic Evaluation of Scientific Writing Shared Task. In Proceedings of the 11th Workshop on Innovative Use of NLP for Building Educational Applications, pages 53-62, San Diego, CA. Association for Computational Linguistics.</p>
<p>Sam Davidson, Aaron Yamada, Paloma Fernandez Mira, Agustina Carando, Claudia H. Sanchez Gutierrez, and Kenji Sagae. 2020. Developing NLP Tools with a New Corpus of Learner Spanish. In Proceedings of the Twelfth Language Resources and Evaluation Conference, pages 7238-7243, Marseille, France. European Language Resources Association.</p>
<p>Masahiro Kaneko and Mamoru Komachi. 2019. MultiHead Multi-Layer Attention to Deep Language Representations for Grammatical Error Detection. ComputaciÃ³n y Sistemas, 23(3).</p>
<p>Murathan KurfalÄ± and Robert Ã–stling. 2023. A distantly supervised Grammatical Error Detection/Correction system for Swedish. In Proceedings of the 12th Workshop on NLP for Computer Assisted Language Learning (NLP4CALL).</p>
<p>Claudia Leacock, Martin Chodorow, Michael Gamon, and Joel Tetreault. 2014. Automated Grammatical Error Detection for Language Learners. Morgan and Claypool.</p>
<p>Mark P.J. van der Loo. 2014. The stringdist Package for Approximate String Matching. The R Journal, 6(1):111-122.</p>
<p>Brady D Lund and Ting Wang. 2023. Chatting about ChatGPT: how may AI and GPT impact academia and libraries? Library Hi Tech News.</p>
<p>Behrang Mohit, Alla Rozovskaya, Nizar Habash, Wajdi Zaghouani, and Ossama Obeid. 2014. The First QALB Shared Task on Automatic Text Correction
for Arabic. In Proceedings of the EMNLP 2014 Workshop on Arabic Natural Language Processing (ANLP), pages 39-47, Doha, Qatar. Association for Computational Linguistics.</p>
<p>Jakub NÃ¡plava and Milan Straka. 2019. Grammatical Error Correction in Low-Resource Scenarios. In Proceedings of the 5th Workshop on Noisy Usergenerated Text (W-NUT 2019), pages 346-356, Hong Kong, China. Association for Computational Linguistics.</p>
<p>Jakub NÃ¡plava, Milan Straka, Jana StrakovÃ¡, and Alexandr Rosen. 2022. Czech grammar error correction with a large and diverse corpus. Transactions of the Association for Computational Linguistics, 10:452-467.</p>
<p>Hwee Tou Ng, Siew Mei Wu, Ted Briscoe, Christian Hadiwinoto, Raymond Hendy Susanto, and Christopher Bryant. 2014. The CoNLL-2014 shared task on grammatical error correction. In Proceedings of the Eighteenth Conference on Computational Natural Language Learning: Shared Task, pages 114, Baltimore, Maryland. Association for Computational Linguistics.</p>
<p>Hwee Tou Ng, Siew Mei Wu, Yuanbin Wu, Christian Hadiwinoto, and Joel Tetreault. 2013. The CoNLL2013 shared task on grammatical error correction. In Proceedings of the Seventeenth Conference on Computational Natural Language Learning: Shared Task, pages 1-12, Sofia, Bulgaria. Association for Computational Linguistics.</p>
<p>The Quyen Ngo, Thi Minh Huyen Nguyen, and Phuong Le-Hong. 2023. Two Neural Models for Multilingual Grammatical Error Detection. In Proceedings of the 12th Workshop on NLP for Computer Assisted Language Learning (NLP4CALL).</p>
<p>Diane Nicholls. 2003. The Cambridge Learner Corpus: error coding and analysis for lexicography and ELT. In Proceedings of the Corpus Linguistics 2003 conference; UCREL technical paper number 16. Lancaster University.</p>
<p>Malvina Nissim, Lasha Abzianidze, Kilian Evang, Rob van der Goot, Hessel Haagsma, Barbara Plank, and Martijn Wieling. 2017. Last words: Sharing is caring: The future of shared tasks. Computational Linguistics, 43(4):897-904.</p>
<p>Martina Nyberg. 2022. Grammatical Error Correction for Learners of Swedish as a Second Language. Master's thesis, Uppsala university.</p>
<p>Krzysztof PajÄ…k and Dominik PajÄ…k. 2022. Multilingual fine-tuning for grammatical error correction. Expert Systems with Applications, 200:116948.</p>
<p>Carla Parra EscartÃ­n, Wessel Reijers, Teresa Lynn, Joss Moorkens, Andy Way, and Chao-Hong Liu. 2017. Ethical Considerations in NLP Shared Tasks. In Proceedings of the First ACL Workshop on Ethics in</p>
<p>Natural Language Processing, pages 66-73, Valencia, Spain. Association for Computational Linguistics.</p>
<p>MatjaÅ¾ Perc. 2014. The Matthew effect in empirical data. Journal of The Royal Society Interface, 11(98):20140378.</p>
<p>Matthew E. Peters, Waleed Ammar, Chandra Bhagavatula, and Russell Power. 2017. Semi-supervised sequence tagging with bidirectional language models. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1756-1765, Vancouver, Canada. Association for Computational Linguistics.</p>
<p>Aigner Picou, Alex Franklin, Maggie Meg Benner, Perpetual Baffour, Phil Culliton, Ryan Holbrook, Scott Crossley, and Terry_yutian Ulrichboser. 2021. Feedback Prize - Evaluating Student Writing.</p>
<p>Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. 2018. Improving language understanding by generative pre-training. OpenAI.</p>
<p>Loganathan Ramasamy, Alexandr Rosen, and Pavel StranÃ¡k. 2015. Improvements to Korektor: A Case Study with Native and Non-Native Czech. In Proceedings ITAT 2015: Information Technologies - Applications and Theory, volume 1422 of CEUR Workshop Proceedings, pages 73-80. CEUR-WS.org.</p>
<p>Gaoqi Rao, Erhong Yang, and Baolin Zhang. 2020. Overview of NLPTEA-2020 shared task for Chinese grammatical error diagnosis. In Proceedings of the 6th Workshop on Natural Language Processing Techniques for Educational Applications, pages 2535, Suzhou, China. Association for Computational Linguistics.</p>
<p>Marc Reznicek, Anke LÃ¼deling, and Hagen Hirschmann. 2013. Competing target hypotheses in the falko corpus. Automatic treatment and analysis of learner corpus data, 59:101-123.</p>
<p>Marc Reznicek, Anke LÃ¼deling, Cedric Krummes, and Franziska Schwantuschke. 2012. Das FalkoHandbuch. Korpusaufbau und Annotationen Version 2.0.</p>
<p>Alexandr Rosen, JiÅ™Ã­ Hana, Barbora HladkÃ¡, TomÃ¡Å¡ JelÃ­nek, Svatava Å kodovÃ¡, and Barbora Å tindlovÃ¡. 2020. Compiling and annotating a learner corpus for a morphologically rich language - CzeSL, a corpus of non-native Czech. Karolinum, Charles University Press, Praha.</p>
<p>Alla Rozovskaya, Houda Bouamor, Nizar Habash, Wajdi Zaghouani, Ossama Obeid, and Behrang Mohit. 2015. The second QALB shared task on automatic text correction for Arabic. In Proceedings of the Second Workshop on Arabic Natural Language Processing, pages 26-35, Beijing, China. Association for Computational Linguistics.</p>
<p>Alla Rozovskaya and Dan Roth. 2019. Grammar error correction in morphologically rich languages: The case of Russian. Transactions of the Association for Computational Linguistics, 7:1-17.</p>
<p>Burr Settles, Chris Brust, Erin Gustafson, Masato Hagiwara, and Nitin Madnani. 2018. Second language acquisition modeling. In Proceedings of the Thirteenth Workshop on Innovative Use of NLP for Building Educational Applications, pages 56-65, New Orleans, Louisiana. Association for Computational Linguistics.</p>
<p>Anders SÃ¸gaard. 2022. Should we ban English NLP for a year? In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 5254-5260, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.</p>
<p>Xin Sun, Tao Ge, Shuming Ma, Jingjing Li, Furu Wei, and Houfeng Wang. 2022. A Unified Strategy for Multilingual Grammatical Error Correction with Pre-trained Cross-Lingual Language Model. In Proceedings of the Thirty-First International Joint Conference on Artificial Intelligence, pages 4367-4374, Vienna, Austria. International Joint Conferences on Artificial Intelligence Organization.</p>
<p>Oleksiy Syvokon and Olena Nahorna. 2022. UAGEC: Grammatical error correction and fluency corpus for the Ukrainian language. arXiv preprint arXiv:2103.16997.</p>
<p>Olga Vinogradova and Olga Lyashevskaya. 2022. Review Of Practices Of Collecting And Annotating Texts In The Learner Corpus REALEC. In Text, Speech, and Dialogue: 25th International Conference, TSD 2022, page 77-88, Berlin, Heidelberg. Springer-Verlag.</p>
<p>Elena Volodina, Lena Granstedt, Arild Matsson, BeÃ¡ta Megyesi, IldikÃ³ PilÃ¡n, Julia Prentice, Dan RosÃ©n, Lisa Rudebeck, Carl-Johan SchenstrÃ¶m, GunlÃ¶g Sundberg, et al. 2019. The SweLL language learner corpus: From design to annotation. Northern European Journal of Language Technology (NEJLT), 6:67-104.</p>
<p>Haoran Wu, Wenxuan Wang, Yuxuan Wan, Wenxiang Jiao, and Michael Lyu. 2023. ChatGPT or Grammarly? Evaluating ChatGPT on Grammatical Error Correction Benchmark. arXiv preprint arXiv:2303.13648.</p>
<p>Helen Yannakoudakis, Ted Briscoe, and Ben Medlock. 2011. A New Dataset and Method for Automatically Grading ESOL Texts. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 180-189, Portland, Oregon, USA. Association for Computational Linguistics.</p>
<p>Zheng Yuan, Shiva Taslimipoor, Christopher Davis, and Christopher Bryant. 2021. Multi-Class Grammatical Error Detection for Correction: A Tale of</p>
<p>Two Systems. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 8722-8736, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.</p>
<p>Yuanyuan Zhao, Nan Jiang, Weiwei Sun, and Xiaojun Wan. 2018. Overview of the NLPCC 2018 Shared Task: Grammatical Error Correction. In Natural Language Processing and Chinese Computing, pages 439-445. Springer International Publishing.</p>
<p>Robert Ã–stling and Murathan KurfalÄ±. 2022. Really good grammatical error correction, and how to evaluate it. Proceedings of Swedish Language Technology Conference.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{20}$ GPT stands for Generative Pretrained Transformers&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref6:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref7:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{21}$ https://spraakbanken.gu.se/en/compsla&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:1" title="Jump back to footnote 2 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:1" title="Jump back to footnote 2 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:1" title="Jump back to footnote 2 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
<li id="fn:3">
<p>This work was funded by the National Science Foundation of China (2023-01-2023) and by the National Science Foundation of China (2023-02-01).&#160;<a class="footnote-backref" href="#fnref:3" title="Jump back to footnote 3 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
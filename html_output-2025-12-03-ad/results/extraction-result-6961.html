<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6961 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6961</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6961</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-134.html">extraction-schema-134</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform self‑reflection or self‑critique, including the specific reflection method, number of generate‑then‑reflect iterations, tasks or benchmarks evaluated, performance before and after reflection, evaluation metrics, and any reported limitations or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-260735852</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2308.04592v1.pdf" target="_blank">Shepherd: A Critic for Language Model Generation</a></p>
                <p><strong>Paper Abstract:</strong> As large language models improve, there is increasing interest in techniques that leverage these models' capabilities to refine their own outputs. In this work, we introduce Shepherd, a language model specifically tuned to critique responses and suggest refinements, extending beyond the capabilities of an untuned model to identify diverse errors and provide suggestions to remedy them. At the core of our approach is a high quality feedback dataset, which we curate from community feedback and human annotations. Even though Shepherd is small (7B parameters), its critiques are either equivalent or preferred to those from established models including ChatGPT. Using GPT-4 for evaluation, Shepherd reaches an average win-rate of 53-87% compared to competitive alternatives. In human evaluation, Shepherd strictly outperforms other models and on average closely ties with ChatGPT.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6961.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6961.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform self‑reflection or self‑critique, including the specific reflection method, number of generate‑then‑reflect iterations, tasks or benchmarks evaluated, performance before and after reflection, evaluation metrics, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Shepherd</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Shepherd: A Critic for Language Model Generation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 7B LLaMA-based model finetuned to generate natural-language critiques of LLM outputs (pointing out factual/logical/coherence/alignment errors and suggesting actionable fixes), trained on curated community feedback and human-annotated feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Shepherd (finetuned LLaMA-7B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>LLaMA-7B base model finetuned with AdamW on a combined dataset of community critiques and human-annotated feedback; training hyperparameters include LR=1e-5, batch=64, seq_len=2048, ~3000 steps, checkpoints selected via manual inspection and GPT-4 evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Critique model (generate critique and suggest refinements)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>A separate critic model generates natural-language feedback on a generator's output that identifies errors (factuality, logical errors, coherence, alignment) and gives concrete suggestions for revision; designed as a decoupled generate-then-reflect critic rather than an internal self-reranking method.</td>
                        </tr>
                        <tr>
                            <td><strong>iteration_type</strong></td>
                            <td>generate-then-reflect</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>AlpacaFarm, FairEval, CosmosQA, OBQA, PIQA, TruthfulQA, CritiqueEval (plus datasets used for human annotation: GSM8K, Proofwriter, PIQA, CosmosQA, e-SNLI, Adversarial NLI, ECQA, GPT-3 summarization, DeFacto)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>A broad suite of NLP tasks covering commonsense, physical, deductive and arithmetic reasoning, summarization, and factual QA used to evaluate the quality of critiques.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Pairwise preference win-rate (vs baselines) by GPT-4 and humans; 1–7 Likert scores from GPT-4 and human annotators; aggregated win-rate (%) across datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_before_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_after_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>improvement_observed</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Paper evaluates Shepherd as a critic (not as a closed-loop refiner) and thus does not report generator performance before/after applying Shepherd's critiques; reported failure modes include: Shepherd sometimes issues incorrect judgements (more low human scores in some cases), depends on quality/diversity of training feedback, and evaluation with GPT-4 can be unstable (GPT-4 sometimes mis-scores correct critiques or suffers knowledge barriers). Dataset and human-evaluation scale limitations are also noted (limited annotator counts and task coverage).</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6961.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6961.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform self‑reflection or self‑critique, including the specific reflection method, number of generate‑then‑reflect iterations, tasks or benchmarks evaluated, performance before and after reflection, evaluation metrics, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SelFee</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SelFee (self-feedback finetuned model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A LLaMA-7B model finetuned to produce self-feedback and revisions, trained on a large dataset of ChatGPT-generated generations, feedback, and revisions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>SelFee (finetuned LLaMA-7B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Finetuned LLaMA-7B; training set consists of ~178K instances collected by querying ChatGPT for generation, feedback, and revisions.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Self-feedback (finetuned critic producing feedback and revisions)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Model trained to output feedback and revised generations from examples where ChatGPT produced original generation, feedback, and revised outputs; effectively a learned single-step critique+revision mapping.</td>
                        </tr>
                        <tr>
                            <td><strong>iteration_type</strong></td>
                            <td>generate-then-reflect (single-cycle critique/revision)</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Same evaluation set used in this paper (AlpacaFarm, FairEval, CosmosQA, OBQA, PIQA, TruthfulQA, CritiqueEval, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Various QA and reasoning benchmarks used to assess critique quality.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Pairwise preference win-rate and 1–7 Likert scores by GPT-4 and human annotators (as used in the paper).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_before_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_after_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>improvement_observed</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Authors report SelFee often generates vague feedback, sometimes directly answers the question instead of critiquing, and in the paper's comparisons it underperforms Shepherd; additionally, SelFee's training data overlaps with some evaluation instances (data contamination) which complicates evaluation.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6961.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6961.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform self‑reflection or self‑critique, including the specific reflection method, number of generate‑then‑reflect iterations, tasks or benchmarks evaluated, performance before and after reflection, evaluation metrics, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-Refine</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Refine (Madaan et al., 2023)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method (cited as related work) that uses a single LLM to generate answers, produce feedback, and iteratively refine its own outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Self-Refine (method)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Methodology where the same LLM alternates between generating outputs and producing self-feedback, then revises its output based on that feedback; cited in related work but not implemented in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Self-Refine (iterative self-critique and revision)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Recursive generate-feedback-revise cycles performed by the same model to improve outputs over iterations.</td>
                        </tr>
                        <tr>
                            <td><strong>iteration_type</strong></td>
                            <td>recursive self-critique / generate-then-reflect</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_before_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_after_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>improvement_observed</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Only mentioned in related work; this paper does not report specifics. (No iterations or quantitative outcomes reported here.)</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6961.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6961.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform self‑reflection or self‑critique, including the specific reflection method, number of generate‑then‑reflect iterations, tasks or benchmarks evaluated, performance before and after reflection, evaluation metrics, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-Improve</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Improve (Huang et al., 2022)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Related-work method that uses rationale-augmented high-confidence model outputs to fine-tune and improve a large LLM's reasoning abilities.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Self-Improve (method)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Approach that generates high-confidence rationale-augmented answers from a large LLM and uses them to fine-tune the model for improved reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Self-Improve (fine-tune-from-self-generated high-confidence outputs)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Uses model's own high-confidence outputs as pseudo-labels / training data to improve performance.</td>
                        </tr>
                        <tr>
                            <td><strong>iteration_type</strong></td>
                            <td>generate-then-finetune (self-imitation)</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_before_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_after_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>improvement_observed</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Mentioned as related work; details not provided in this paper.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6961.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e6961.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform self‑reflection or self‑critique, including the specific reflection method, number of generate‑then‑reflect iterations, tasks or benchmarks evaluated, performance before and after reflection, evaluation metrics, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Critic (Gou et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Critic (Gou et al., 2023)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Related-work approach that uses external tools to evaluate certain aspects of generated text and uses that feedback for refinement.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Critic (method)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Framework that interacts with evaluation tools to assess outputs (e.g., factuality checks) and produces targeted feedback for revision.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Tool-augmented critique</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Uses external evaluators/tools to generate feedback on model outputs and feeds that back for refinement.</td>
                        </tr>
                        <tr>
                            <td><strong>iteration_type</strong></td>
                            <td>generate-then-evaluate-then-reflect (tool-assisted)</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_before_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_after_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>improvement_observed</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Mentioned in related work; exact failure modes not reported in this paper.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6961.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e6961.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform self‑reflection or self‑critique, including the specific reflection method, number of generate‑then‑reflect iterations, tasks or benchmarks evaluated, performance before and after reflection, evaluation metrics, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-Correct</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Correct (Welleck et al., 2023)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Related method that decouples generator and corrector, showing that a smaller corrector model can improve generations from a larger generator.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Self-Correct (method)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Approach that separates generation and correction: a generator produces outputs and a separate corrector model refines them.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Decoupled corrector</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>A separate corrector model inspects and revises generator outputs; demonstrates gains even when the corrector is much smaller.</td>
                        </tr>
                        <tr>
                            <td><strong>iteration_type</strong></td>
                            <td>generate-then-reflect (decoupled corrector)</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_before_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_after_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>improvement_observed</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Cited as related work; detailed failure cases not discussed in this paper.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6961.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e6961.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform self‑reflection or self‑critique, including the specific reflection method, number of generate‑then‑reflect iterations, tasks or benchmarks evaluated, performance before and after reflection, evaluation metrics, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChatGPT (baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChatGPT (GPT-3.5 Turbo) as critic baseline</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Proprietary OpenAI model used as a strong baseline critic in the paper; generally provides stable feedback and correct judgements but is occasionally outperformed by Shepherd in pairwise comparisons and may miss some errors Shepherd finds.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGPT (GPT-3.5 Turbo)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI chat model used as a baseline for critique generation; not finetuned by the authors and contains training data up to Sep 2021 (per paper).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Prompted critique (single-step)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>When prompted, ChatGPT generates feedback on LLM outputs (judgement + suggestions) in a single-shot fashion without being a dedicated finetuned critic.</td>
                        </tr>
                        <tr>
                            <td><strong>iteration_type</strong></td>
                            <td>generate-then-reflect (single-step)</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Same multi-dataset critique evaluation (AlpacaFarm, FairEval, CosmosQA, OBQA, PIQA, TruthfulQA, CritiqueEval, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Benchmarks spanning reasoning, QA, and summarization used to compare critique quality.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Pairwise preference win-rate; 1–7 Likert scores from humans and GPT-4.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_before_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_after_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>improvement_observed</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Paper notes ChatGPT is generally stable and often gives correct judgements, but it can miss errors that Shepherd detects; also, GPT-4-based automatic evaluation sometimes rates ChatGPT highly even when humans disagree, and ChatGPT shares the general evaluation limitations noted (e.g., data contamination, domain knowledge gaps).</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>SelFee <em>(Rating: 2)</em></li>
                <li>Self-Refine <em>(Rating: 2)</em></li>
                <li>Self-Improve <em>(Rating: 1)</em></li>
                <li>Critic <em>(Rating: 2)</em></li>
                <li>Self-Correct <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6961",
    "paper_id": "paper-260735852",
    "extraction_schema_id": "extraction-schema-134",
    "extracted_data": [
        {
            "name_short": "Shepherd",
            "name_full": "Shepherd: A Critic for Language Model Generation",
            "brief_description": "A 7B LLaMA-based model finetuned to generate natural-language critiques of LLM outputs (pointing out factual/logical/coherence/alignment errors and suggesting actionable fixes), trained on curated community feedback and human-annotated feedback.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Shepherd (finetuned LLaMA-7B)",
            "model_description": "LLaMA-7B base model finetuned with AdamW on a combined dataset of community critiques and human-annotated feedback; training hyperparameters include LR=1e-5, batch=64, seq_len=2048, ~3000 steps, checkpoints selected via manual inspection and GPT-4 evaluation.",
            "model_size": "7B",
            "reflection_method_name": "Critique model (generate critique and suggest refinements)",
            "reflection_method_description": "A separate critic model generates natural-language feedback on a generator's output that identifies errors (factuality, logical errors, coherence, alignment) and gives concrete suggestions for revision; designed as a decoupled generate-then-reflect critic rather than an internal self-reranking method.",
            "iteration_type": "generate-then-reflect",
            "num_iterations": null,
            "task_name": "AlpacaFarm, FairEval, CosmosQA, OBQA, PIQA, TruthfulQA, CritiqueEval (plus datasets used for human annotation: GSM8K, Proofwriter, PIQA, CosmosQA, e-SNLI, Adversarial NLI, ECQA, GPT-3 summarization, DeFacto)",
            "task_description": "A broad suite of NLP tasks covering commonsense, physical, deductive and arithmetic reasoning, summarization, and factual QA used to evaluate the quality of critiques.",
            "evaluation_metric": "Pairwise preference win-rate (vs baselines) by GPT-4 and humans; 1–7 Likert scores from GPT-4 and human annotators; aggregated win-rate (%) across datasets.",
            "performance_before_reflection": null,
            "performance_after_reflection": null,
            "improvement_observed": null,
            "limitations_or_failure_cases": "Paper evaluates Shepherd as a critic (not as a closed-loop refiner) and thus does not report generator performance before/after applying Shepherd's critiques; reported failure modes include: Shepherd sometimes issues incorrect judgements (more low human scores in some cases), depends on quality/diversity of training feedback, and evaluation with GPT-4 can be unstable (GPT-4 sometimes mis-scores correct critiques or suffers knowledge barriers). Dataset and human-evaluation scale limitations are also noted (limited annotator counts and task coverage).",
            "uuid": "e6961.0"
        },
        {
            "name_short": "SelFee",
            "name_full": "SelFee (self-feedback finetuned model)",
            "brief_description": "A LLaMA-7B model finetuned to produce self-feedback and revisions, trained on a large dataset of ChatGPT-generated generations, feedback, and revisions.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "SelFee (finetuned LLaMA-7B)",
            "model_description": "Finetuned LLaMA-7B; training set consists of ~178K instances collected by querying ChatGPT for generation, feedback, and revisions.",
            "model_size": "7B",
            "reflection_method_name": "Self-feedback (finetuned critic producing feedback and revisions)",
            "reflection_method_description": "Model trained to output feedback and revised generations from examples where ChatGPT produced original generation, feedback, and revised outputs; effectively a learned single-step critique+revision mapping.",
            "iteration_type": "generate-then-reflect (single-cycle critique/revision)",
            "num_iterations": null,
            "task_name": "Same evaluation set used in this paper (AlpacaFarm, FairEval, CosmosQA, OBQA, PIQA, TruthfulQA, CritiqueEval, etc.)",
            "task_description": "Various QA and reasoning benchmarks used to assess critique quality.",
            "evaluation_metric": "Pairwise preference win-rate and 1–7 Likert scores by GPT-4 and human annotators (as used in the paper).",
            "performance_before_reflection": null,
            "performance_after_reflection": null,
            "improvement_observed": null,
            "limitations_or_failure_cases": "Authors report SelFee often generates vague feedback, sometimes directly answers the question instead of critiquing, and in the paper's comparisons it underperforms Shepherd; additionally, SelFee's training data overlaps with some evaluation instances (data contamination) which complicates evaluation.",
            "uuid": "e6961.1"
        },
        {
            "name_short": "Self-Refine",
            "name_full": "Self-Refine (Madaan et al., 2023)",
            "brief_description": "A method (cited as related work) that uses a single LLM to generate answers, produce feedback, and iteratively refine its own outputs.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "Self-Refine (method)",
            "model_description": "Methodology where the same LLM alternates between generating outputs and producing self-feedback, then revises its output based on that feedback; cited in related work but not implemented in this paper.",
            "model_size": null,
            "reflection_method_name": "Self-Refine (iterative self-critique and revision)",
            "reflection_method_description": "Recursive generate-feedback-revise cycles performed by the same model to improve outputs over iterations.",
            "iteration_type": "recursive self-critique / generate-then-reflect",
            "num_iterations": null,
            "task_name": null,
            "task_description": null,
            "evaluation_metric": null,
            "performance_before_reflection": null,
            "performance_after_reflection": null,
            "improvement_observed": null,
            "limitations_or_failure_cases": "Only mentioned in related work; this paper does not report specifics. (No iterations or quantitative outcomes reported here.)",
            "uuid": "e6961.2"
        },
        {
            "name_short": "Self-Improve",
            "name_full": "Self-Improve (Huang et al., 2022)",
            "brief_description": "Related-work method that uses rationale-augmented high-confidence model outputs to fine-tune and improve a large LLM's reasoning abilities.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "Self-Improve (method)",
            "model_description": "Approach that generates high-confidence rationale-augmented answers from a large LLM and uses them to fine-tune the model for improved reasoning.",
            "model_size": null,
            "reflection_method_name": "Self-Improve (fine-tune-from-self-generated high-confidence outputs)",
            "reflection_method_description": "Uses model's own high-confidence outputs as pseudo-labels / training data to improve performance.",
            "iteration_type": "generate-then-finetune (self-imitation)",
            "num_iterations": null,
            "task_name": null,
            "task_description": null,
            "evaluation_metric": null,
            "performance_before_reflection": null,
            "performance_after_reflection": null,
            "improvement_observed": null,
            "limitations_or_failure_cases": "Mentioned as related work; details not provided in this paper.",
            "uuid": "e6961.3"
        },
        {
            "name_short": "Critic (Gou et al.)",
            "name_full": "Critic (Gou et al., 2023)",
            "brief_description": "Related-work approach that uses external tools to evaluate certain aspects of generated text and uses that feedback for refinement.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "Critic (method)",
            "model_description": "Framework that interacts with evaluation tools to assess outputs (e.g., factuality checks) and produces targeted feedback for revision.",
            "model_size": null,
            "reflection_method_name": "Tool-augmented critique",
            "reflection_method_description": "Uses external evaluators/tools to generate feedback on model outputs and feeds that back for refinement.",
            "iteration_type": "generate-then-evaluate-then-reflect (tool-assisted)",
            "num_iterations": null,
            "task_name": null,
            "task_description": null,
            "evaluation_metric": null,
            "performance_before_reflection": null,
            "performance_after_reflection": null,
            "improvement_observed": null,
            "limitations_or_failure_cases": "Mentioned in related work; exact failure modes not reported in this paper.",
            "uuid": "e6961.4"
        },
        {
            "name_short": "Self-Correct",
            "name_full": "Self-Correct (Welleck et al., 2023)",
            "brief_description": "Related method that decouples generator and corrector, showing that a smaller corrector model can improve generations from a larger generator.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "Self-Correct (method)",
            "model_description": "Approach that separates generation and correction: a generator produces outputs and a separate corrector model refines them.",
            "model_size": null,
            "reflection_method_name": "Decoupled corrector",
            "reflection_method_description": "A separate corrector model inspects and revises generator outputs; demonstrates gains even when the corrector is much smaller.",
            "iteration_type": "generate-then-reflect (decoupled corrector)",
            "num_iterations": null,
            "task_name": null,
            "task_description": null,
            "evaluation_metric": null,
            "performance_before_reflection": null,
            "performance_after_reflection": null,
            "improvement_observed": null,
            "limitations_or_failure_cases": "Cited as related work; detailed failure cases not discussed in this paper.",
            "uuid": "e6961.5"
        },
        {
            "name_short": "ChatGPT (baseline)",
            "name_full": "ChatGPT (GPT-3.5 Turbo) as critic baseline",
            "brief_description": "Proprietary OpenAI model used as a strong baseline critic in the paper; generally provides stable feedback and correct judgements but is occasionally outperformed by Shepherd in pairwise comparisons and may miss some errors Shepherd finds.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "ChatGPT (GPT-3.5 Turbo)",
            "model_description": "OpenAI chat model used as a baseline for critique generation; not finetuned by the authors and contains training data up to Sep 2021 (per paper).",
            "model_size": null,
            "reflection_method_name": "Prompted critique (single-step)",
            "reflection_method_description": "When prompted, ChatGPT generates feedback on LLM outputs (judgement + suggestions) in a single-shot fashion without being a dedicated finetuned critic.",
            "iteration_type": "generate-then-reflect (single-step)",
            "num_iterations": null,
            "task_name": "Same multi-dataset critique evaluation (AlpacaFarm, FairEval, CosmosQA, OBQA, PIQA, TruthfulQA, CritiqueEval, etc.)",
            "task_description": "Benchmarks spanning reasoning, QA, and summarization used to compare critique quality.",
            "evaluation_metric": "Pairwise preference win-rate; 1–7 Likert scores from humans and GPT-4.",
            "performance_before_reflection": null,
            "performance_after_reflection": null,
            "improvement_observed": null,
            "limitations_or_failure_cases": "Paper notes ChatGPT is generally stable and often gives correct judgements, but it can miss errors that Shepherd detects; also, GPT-4-based automatic evaluation sometimes rates ChatGPT highly even when humans disagree, and ChatGPT shares the general evaluation limitations noted (e.g., data contamination, domain knowledge gaps).",
            "uuid": "e6961.6"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "SelFee",
            "rating": 2
        },
        {
            "paper_title": "Self-Refine",
            "rating": 2,
            "sanitized_title": "selfrefine"
        },
        {
            "paper_title": "Self-Improve",
            "rating": 1,
            "sanitized_title": "selfimprove"
        },
        {
            "paper_title": "Critic",
            "rating": 2
        },
        {
            "paper_title": "Self-Correct",
            "rating": 2,
            "sanitized_title": "selfcorrect"
        }
    ],
    "cost": 0.0158615,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Shepherd: A Critic for Language Model Generation</p>
<p>Tianlu Wang 
Ping Yu 
Xiaoqing Ellen Tan 
Sean O&apos;brien 
Ramakanth Pasunuru 
Jane Dwivedi-Yu 
Olga Golovneva 
Luke Zettlemoyer 
Maryam Fazel-Zarandi 
Asli Celikyilmaz 
Shepherd: A Critic for Language Model Generation
1 joint first author 2 core contributor 3 work done at Meta 4 directional lead Meta AI Research, FAIR
As large language models improve, there is increasing interest in techniques that leverage these models' capabilities to refine their own outputs. In this work, we introduce Shepherd, a language model specifically tuned to critique model responses and suggest refinements, extending beyond the capabilities of an untuned model to identify diverse errors and provide suggestions to remedy them. At the core of our approach is a high quality feedback dataset, which we curate from community feedback and human annotations. Even though Shepherd is small (7B parameters), its critiques are either equivalent or preferred to those from established models including ChatGPT. Using GPT-4 for evaluation, Shepherd reaches an average win-rate of 53-87% compared to competitive alternatives. In human evaluation, Shepherd strictly outperforms other models and on average closely ties with ChatGPT. 1What are some great financial investments with no risk at all?Investing in the stock market, mutual funds, bonds, and real estate are all great financial investments with no risk at all.The response is not accurate and does not address the question fully. The question asks about investments with "no risk at all," so the response, which mentions investments that do involve risk, is not useful. To answer the question more fully and accurately, the response should include investments with little to no risk, such as certificates of deposit (CDs), government bonds, or money market funds.What are the differences between plant-based and animal-based protein sources?The main difference between plant-based and animal-based protein sources is that plant-based proteins are derived from plants, while animal-based proteins are derived from animals. Plant-based proteins are often lower in fat and cholesterol than animal-based proteins, and they are typically richer in vitamins and minerals. Additionally, plant-based proteins can be more environmentally sustainable than animal-based proteins.The response doesn't mention the most important difference which is that animal-based protein sources are complete protein sources, and plant-based protein sources are incomplete.</p>
<p>Introduction</p>
<p>Large language models (LLMs) have become increasingly sophisticated, exhibiting remarkable proficiency in producing coherent, contextually and semantically meaningful text. Despite this progress, LLMs still often generate false, unreliable and incoherent outputs (Zhang et al., 2023). Methods that continuously critique and refine generations would thus be a highly beneficial step towards more reliable language models.</p>
<p>LLMs have been used to generate feedback to refine language model outputs (Yang et al.  seeking dialog tasks. A more recent study by Ye et al. (2023) instruction-tunes an LLM to generate self-feedback on its responses, though the initial investigations fail to provide feedback on model output generations on complex tasks like math and reasoning, only providing generic feedback on the output response.</p>
<p>In this work, we present Shepherd 2 , a language model explicitly tuned to critique model generated outputs. While sharing similar motivation with recent work, our goal is to build a robust critique model that can provide feedback across a wider range of domains. When asked to refine the output, our model can pinpoint specific issues like factuality, logical errors, coherence, and alignment, while also suggesting improvements. More specifically, Shepherd can generate natural language feedback which does not only give overall judgement or general suggestions but can also involve deep domain knowledge with actionable ideas for refinement (see examples in Table 7 and Table 14).</p>
<p>To fine-tune and evaluate Shepherd, we created a high-quality feedback dataset composing of two distinct sets: (1) community feedback, curated from online forums to capture more diverse interactions, and (2) human-annotated feedback, collected on generations across different types of tasks. See examples in Table 1. Trained on a combination of these datasets, Shepherd shows impressive results, outperforming ChatGPT (OpenAI, 2022) models on multiple downstream tasks. Close inspection of influences of community feedback and human annotated feedback data confirms that the community data is more informative and diverse than human-annotated data, yet leans towards informality. These nuances enable Shepherd to give feedback on diverse tasks, while we find that including high-quality human annotated data for finetuning improves model performance.</p>
<p>We conduct model-based evaluation (using GPT-4 (OpenAI, 2023)) as well as human evaluation of the feedback generated by Shepherd comparing against state-of-the-art baselines including Alpaca (Taori et al., 2023), SelFee (Ye et al., 2023), and ChatGPT (OpenAI, 2022). We find that critiques from Shepherd are generally more preferred over those from other models. For example, Alpaca tends to give positive feedback to all of model responses, resulting a lot incorrect feedback. SelFee tends to provide vague feedback that cannot pinpoint errors, ignoring model responses or directly answering the question instead of critiquing responses. ChatGPT is more stable across different evaluation settings and we find it does a better job of giving feedback with a correct judgement.</p>
<p>In summary, we develop Shepherd, a new model capable of providing comprehensive critiques of any LLM generated text, which can efficiently enhance their quality. Through rigorous evaluation of generated critiques, we demonstrate the efficacy of Shepherd across a variety of generation tasks. A further key contribution of our work is creating a high-quality feedback dataset, which has the potential to facilitate future research in this area.</p>
<p>Data Collection</p>
<p>Community Critique Data</p>
<p>We gather feedback data from two community question and answering websites: Stack Exchange 3 and the Pushshift Reddit Dataset (Baumgartner et al., 2020). We consider the title and the sub-title of a post as a question, its top-level comments as answers, and replies to these comments as critiques. Everything is associated with a community vote score, calculated by subtracting the total number of downvotes from the total number of upvotes. For the sake of clarity, we will refer to the community vote score as question score, answer score, and critique score respectively .</p>
<p>Stack Exchange Data. Stack Exchange contains a network of 173 dedicated Q&amp;A communities, inclusive of the notable Stack Overflow community, dispensing expert insights in a question and answer format. Our data curation temporarily only focuses on English content.</p>
<p>Pushshift Reddit Data. Reddit is one of the most popular websites in the world, facilitating the sharing, discussion, and community voting of content within user-generated communities, or "subreddits". It hosts nearly 140,000 active subreddits at any given moment. However, there are certain challenges associated with fine-tuning a model on Pushshift Reddit data: 1) a substantial proportion of subreddits function primarily for entertainment rather than serious, informative dialogues. This includes subreddits such as r/funny, r/MadeMeSmile, r/facepalm, and many others; 2) many posts are primarily intended to share information (e.g., news, jokes) instead of posing specific questions, making them unsuitable for fine-tuning.</p>
<p>In light of these limitations, we have selected data from 16 subreddits 4 from Pushshift Reddit Dataset. These communities generally contain more formal discussions, often structured in a Q&amp;A format.</p>
<p>Critique Postprocessing</p>
<p>We aim to gather instances in the format of a clean question-answer-critique triad. Upon cleaning the  community data, we observed that while procuring high-quality Q&amp;A pairs was relatively straightforward, a significant portion of the data did not include suitable critiques. We consider a critique to be valid in two scenarios.</p>
<p>-Case #1: The answer is largely accurate, and the critique offers recommendations for further refinement or enhancement.</p>
<p>-Case #2: The answer contains inaccuracies, which the critique explicitly highlights.</p>
<p>We employ a number of techniques to curate valid critiques from community data. First, we filter out invalid critique data, such as joke sharing and follow-up questions that fail to provide feedback. To do this, we use two methods. (1) Keyword filtering : We keep examples that contain certain keywords matching the above two cases 5 . (2) User edit history: In additional to keyword filtering, we collect critiques if users edit their answer after post- 5 For Case #1, keywords we use for filtering are: "not wrong", "agree", "absolutely", "indeed", "agreed", "exactly what I think", "that's right", "not what I think", "you're right", "you are right", "that is right"; For Case #2, keywords we use are: "wrong", "incorrect", "not agree", "not right", "disagree", "can't agree", "beg to differ", "that's not my view". ing the critique. This helps us identify examples where the critique leads to a modification of the original answer.</p>
<p>Next, we note that even critiques that follow the appropriate format may be inaccurate. For instance, a critique might wrongly challenge a true fact from an answer. To further refine our data and ensure high quality of critiques, we incorporate additional filters linked with community vote scores. In Case #1, where the answer is primarily correct, we omit data where the answer score is lower than 10 and the critique score is lower than 2. This filter ensures we only select instances where the initial answer is largely approved by the community, and the critique has received some level of endorsement. In Case #2, where the answer includes inaccuracies, we focus on data where the critique score surpasses the answer score, and the critique score itself is higher than 2. This selection process ensures we are considering instances where the critique, indicating an error in the initial answer, has garnered more community approval than the answer itself.</p>
<p>Lastly, we impose several additional filters to further refine our dataset: 1) To maintain diversity, we retain only one instance per post, choosing the one with the highest critique score; 2) To manage offensive language in community data, we incorporate a profanity check 6 and eliminate any comments or feedback with a profanity score lower than 0.8; 3) Given that our model is a text-only model, we filter out instances that contain URLs, images, or videos; 4) We identify and remove comments that pose further questions to the original question rather than the original answer to preserve the integrity of the Q&amp;A format.</p>
<p>Human Data Collection</p>
<p>Public dataset selection. To collect natural language feedback from a diverse set of contexts and outputs, we select 8 popular languageunderstanding and entailment datasets that require complex reasoning and have step-by-step explanations to arrive at the final answer, as well as two summarization datasets. Specifically, we choose Entailment Bank Only data from the training sets is used for human annotation 8 .</p>
<p>Data construction for human annotation. In order to collect high quality natural language feedback, for each question we provide a context, a correct output, and a candidate output, and ask an-6 https://pypi.org/project/ alt-profanity-check/ 7 For entailment datasets such as e-SNLI and Adversarial NLI, only prompts with a target label of "entailment" or "contradiction" are used, i.e., prompts labelled with "neutral" are removed. This is because it is difficult for our critique model to learn from neutral examples to give useful feedback. 8 To diversify the human data, we also add one or two examples from the training set of each of these datasets: SIQA notators to give feedback on whether there are any errors in the candidate output. The correct output is provided to help annotators identify errors more easily. Except for GPT-3 summarization, in which we choose the summary best preferred by human raters, all datasets contain a gold answer to the question, i.e., the correct output. PIQA, CosmosQA, and DeFacto have candidate outputs in the original datasets. For GPT-3 summarization we choose the summary least preferred by human raters as the candidate output. For other datasets, to increase the possibility of obtaining candidate outputs that have reasonable errors, we prompt LLaMA-65B (Touvron et al., 2023) or LIMA-30B (Zhou et al., 2023) with zero-shot or few-shot in-context examples to obtain step-by-step reasoning sequences. We do a manual inspection to ensure the quality of the generated candidate outputs. Examples from each dataset and the templates used to prompt LLaMA and LIMA in order to obtain the candidate outputs are shown in Table 8.</p>
<p>Annotation specifics. We selected an independent vendor (RWS Moravia), utilizing expert reviewers instead of crowd-sourcing, due to the cognitively demanding nature of our annotation task which necessitates meticulous and nuanced annotations. Each example is annotated by one expert with human-in-the-loop quality assessment. 9 We define different error types in our taxonomy in Table 9. More details about the instruction and guidelines of human annotation can be found in Appendix A.</p>
<p>Postprocessing. To ensure high quality data, we conduct the following postprocessing. First, we remove examples flagged with "Errors in the correct output" and "The context is too complex to work on". We also remove feedback on error types of "Redundancy" and "Consistency with context" because we find that the feedback on these two error types are not helpful. As our ultimate goal is to collect feedback in natural language, we concatenate the feedback from different error types of an example into a paragraph using natural words such as "Firstly," "Secondly," and "Besides" if there exists feedback for more than one error type in a single example. We end up with 1,317 high quality examples in total. Table 10 presents the distribution of Your task is to evaluate the feedback on a model-generated answer. You'll receive a question, an answer, and feedback about this answer. Give a score based on the feedback's quality. 7: When the answer is wrong, the feedback clearly highlights the most important errors and provides very actionable suggestions. When the answer is correct, the feedback confirms the answer is correct and provides very useful suggestions. 6: When the answer is wrong, the feedback confirms that the answer is wrong and points out the most important errors. When the answer is correct, the feedback confirms the answer is correct and provides useful suggestions. 5: When the answer is wrong, the feedback misses the important errors but clearly confirms that the answer is wrong. When the answer is correct, the feedback confirms the answer is correct and proposes some less useful suggestions. 4: The feedback has a correct judgement of the answer (e.g., states correct answer is correct or states wrong answer is wrong). 3: The feedback is vague about whether or not the answer is correct. Or the feedback itself tries to answer the question regardless of the content in the answer. 2: The feedback has a wrong judgement of the answer (e.g., states correct answer is wrong or states wrong answer is correct). 1: The feedback is completely random text or simply repeats the answer.</p>
<p>First, please check whether the feedback has correct or incorrect judegement (correct judgement means the answer is correct, feedback confirms the correctness. Or if the answer is incorrect, the feedback confirms the incorrectness.) Give a score 1-3 for feedback with incorrect judgement and give a score 4-7 for feedback with correct judgement.</p>
<p>Start your answer with the score. collected examples and the identified error types from each dataset used in human annotation.</p>
<p>The Shepherd Model</p>
<p>We train Shepherd with LLaMA-7B (Touvron et al., 2023) as the base model. We use AdamW (Loshchilov and Hutter, 2017) as our optimizer with β 1 = 0.9, β 2 = 0.95 and weight decay of 0.1. We use a learning rate 1e-5 with 2000 warmup steps, and set the batch size to 64 and maximum sequence length to 2048. The training data is formatted using the same template where we use "### {field name}" to separate different fields as shown in Table 1.</p>
<p>We keep checkpoints for each 50 steps with a total of 3, 000 steps. We manually inspect whether the generated feedback can identify errors or come up with constructive suggestions on a held-out set of 20-examples and select 3 best checkpoints. After that, we use the GPT-4 evaluation protocol (as described in 4.3) to pick the best checkpoint on the held-out set.</p>
<p>Evaluating Feedback</p>
<p>To test the ability of Shepherd to critique model generations, we compare it against a series of state-of-the-art language models including Alpaca-7B (Taori et al., 2023), SelFee-7B (Ye et al., 2023) and ChatGPT (GPT-3.5 Turbo). We conduct both human evaluation and automatic evaluation by using GPT-4 (OpenAI, 2023) as an evaluator. Through extensive experiments across multiple downstream tasks, we show our model can effectively critique model generations and propose suggestions, outperforming Alpaca and SelFee while achieving performance that is competitive with ChatGPT.</p>
<p>Evaluation Data</p>
<p>To get a broad coverage of NLP domains, we carefully select 6 public datasets for evaluation: Alpaca- ). These 6 datasets cover a wide range of topics and reasoning skill-sets, including commonsense reasoning, physical reasoning, mathematical reasoning, etc. We sample 50 instances from the validation or test set of each dataset, resulting in a total of 300 instances in the final evaluation set. We further sample 20 instances from each dataset to be used for our ablation studies.</p>
<p>Data contamination. The issue of data contamination has been widely discussed along with ChatGPT and related models. To prevent data contamination in our evaluation, we further develop a new test set CritiqueEval, containing 52 Pushshift questions posted from June 2022 to June 2023 10 . We manually select questions from Pushshift Reddit dataset with highest voting score, and for each question, we collect an answer with the highest vote score and another answer with the lowest vote score 11 . In summary, our evaluation data in total consists of 352 instances from public datasets as well as from Pushshift Reddit dataset.</p>
<p>Baseline Models</p>
<p>We compare Shepherd against the following stateof-the-art baselines:</p>
<p>• ChatGPT, one of the most powerful chatbots developed by OpenAI. 10 The current version of ChatGPT contains training data up to Sep. 2021. Our training set contains training data up to Dec. 2021. 11 community vote score from Pushshift Reddit dataset is calculated by the total number of upvotes minus the total number of downvotes.</p>
<p>You are a helpful and precise assistant for checking the quality of the feedback. Two pieces of feedback have been provided for the same response to a particular question. Which one can point out key errors contained in the answer or provide constructive suggestions?  • Alpaca-7B, a LLaMA-7B model finetuned on 52K instruction-following data obtained from ChatGPT. We include this model since details about the language model behind ChatGPT are not revealed.</p>
<p>• SelFee, a finetuned LLaMA-7B model but curated for self-feedback and self-revision generation. The training set for this model includes 178K training instances from five different data sources, obtained by querying ChatGPT to get original generations, feedback and revisions. Note that using exact matching on questions, we observe some overlap between our evaluation data and SelFee's training data, namely 33 out of 50 instances from AlpacaFarm, 6 out of 50 instances from TruthfulQA, 1 out of 50 instances from OBQA and 1 out of 50 instances from PIQA.</p>
<p>GPT4 Evaluation</p>
<p>Recent work has used GPT-4 (OpenAI, 2023) to assess the quality of model-generated text (Liu et al., 2023; Wang et al., 2023c; Zhou et al., 2023). We follow the same approach to prompt GPT-4 12 to measure to what extent our model can critique answers generated by LLMs.</p>
<p>Absolute Likert Score</p>
<p>We ask GPT-4 to grade each feedback on a 1-7 likert scale mainly based on whether the feedback can point out errors in the answer or confirm the answer is correct when there are no errors. We provide the instruction shown in Figure 4 as the system message to GPT-4. Note GPT-4 can make mistakes when improper instruction is provided. We select the instruction shown in Figure 4 through manual inspection, conducted by testing various instructions on a series of feedback examples and 12 GPT-4-0613 is used in our experiments.</p>
<p>Question Why the elderly body odor is so strong? Response As our skin matures, its natural antioxidant protection declines. This decline results in greater oxidation of lipid acid. When lipid acid is oxidized, the chemical compound nonenal is produced, giving off the "old people smell" that many of us are familiar with. Feedback The response gives a logical explanation to the question, using simple and understandable language. However, it could benefit from proper punctuation and grammar. For example, it would be more accurate to say that the chemical compound is nonanal, rather than nonenal, and the sentence could be rewritten as follows: As our skin matures, its natural antioxidant protection declines. This decline results in greater oxidation of lipid acid. When lipid acid is oxidized, the chemical compound nonanal is produced, giving off the "old people smell" that many of us are familiar with. Related Knoledge from Wikipedia</p>
<p>One study suggested that old person smell may be the result of 2-nonenal, an unsaturated aldehyde which is associated with human body odor alterations during aging; however, there are other hypotheses. Another study failed to detect 2-nonenal at all, but found significantly increased concentrations of benzothiazole, dimethylsulphone, and nonanal on older subjects. assessing if the grading by GPT-4 aligns with human judgement. We provide a few examples in Appendix B.1 to show how GPT-4 performs under this chosen instruction. Despite this careful instruction-selection process, GPT-4 may still be misaligned with human judgment, as evinced in Table 13.</p>
<p>Pairwise Comparison</p>
<p>We also ask GPT-4 to pick the better feedback given two candidate feedback. To do this, we provide the instruction shown in Figure 5. In this setting, instead of giving detailed instructions, we directly ask GPT-4 to pick the better feedback according to whether it can identify errors when the response is wrong or provide useful suggestions when the response is correct but needs improvement.</p>
<p>Human Evaluation</p>
<p>As language models may be biased evaluators (Aiyappa et al., 2023; Wang et al., 2023c), we further conduct human evaluation. Similarly to the GPT-4 evaluation, we ask each crowd worker to rate feedback on a 1-7 likert score. For a fairer comparison, we present the question, answer and feedback generated by different models together to annotators. Note that GPT-4 rates each feedback independently, whereas crowd workers are required to compare the quality of multiple feedback and as- Figure 6: Score distribution from human evaluation. Scores ranging from 1-2 means the model fails to give correct judgements. A score of 7 means the feedback not only gives a correct judgement of the answer but also provides very helpful feedback for improving the answer. sign scores accordingly. In this way, we encourage crowd workers to rank multiple feedback for the same question-answer pair.</p>
<p>Questions in our evaluation data may require specific domain knowledge. Even though we invite expert annotators to do human evaluation who carefully search online whenever they need help, human evaluation is still a challenging task. Table 2 shows one example that requires deep medical or biology knowledge.</p>
<p>Results</p>
<p>Pairwise Comparison</p>
<p>We first analyze whether Shepherd can generate better feedback compared to other competing models. We present pairwise comparison results in Figure 2 and Figure 3, using GPT-4 and human evaluation, respectively. In both evaluation settings, Shepherd significantly outperforms Alpaca and consistently outperforms SelFee. Note that both Shepherd and SelFee are finetuned LLaMA-7B models, however, SelFee was finetuned on a dataset of 178K examples (some of which overlapped with our evaluation data) whereas Shepherd was only finetuned on a dataset of 8K. We also see Shepherd has slightly better performance than ChatGPT according to GPT-4 evaluation, and on par performance in human evaluation.</p>
<p>Detailed comparison results for each task are presented in Table 3 and Table 4. The performance varies across tasks but overall aligns across the two evaluation settings. It is important to note that Shepherd consistently generates better feedback on CritiqueEval, a dataset which has never been seen by ChatGPT or LLaMA models. This demonstrates both the critique and generalization abilities of our Shepherd model. Some qualitative examples are included in Table 7.</p>
<p>Score Distribution</p>
<p>We also look into the distribution of scores using human evaluation. As shown in Figure 6, most feedbacks from Alpaca have an incorrect judgement of the provided answer (score 2). SelFee tends to generate vague feedback or directly answers the question instead of critiquing (score 3). Most feedbacks from ChatGPT have a correct judgement (score 4). Compared to ChatGPT, Shepherd provides more helpful suggestions to improve the answer (more feedback with score 7) although it tends to generate feedback with wrong judgements (more feedback with score 2).</p>
<p>Likert Score</p>
<p>We include results using likert scores annotated by GPT-4 in Table 6 and likert scores annotated by human annotator in Table 5. Compared to pairwise comparison by GPT-4, we find the likert score evaluation by GPT-4 is less convincing with the following reasons:</p>
<p>(i) When GPT-4 is asked to rate each feedback independently, it tends to give a high rating for all feedback. For example, Alpaca obtains an average score of 4.7, meaning most feedback have a correct judgment of the response, while human gives an average score of 2.91, meaning a great proportion of feedback have the incorrect judgement. Evaluation results obtained by using GPT-4 conflict with the observations from the human evaluation.</p>
<p>(ii) The likert score assigned by GPT-4 appears to be favoring a specific format. According to human evaluation, SelFee frequently provides minor and general suggestions, such as adding more examples to better illustrate the answer, even when an ample amount of examples has already been included in the response. In such cases, GPT-4 routinely give higher scores. However, human annotators are typically not misled by this pattern. This results in a discrepancy where GPT-4 assigns high scores to the Selfee model, while human evaluators give significantly lower scores.</p>
<p>(iii) We also see that the performance gap between these models is relatively small, making it difficult to distinguish good and bad models.    Overall, we recommend using GPT-4 to rate in a pairwise comparison style, which aligns well with human evaluation.</p>
<p>Limitation of GPT-4 evaluation. We find that GPT-4 evaluation can be unstable and inaccurate in two cases. (i) One instance of GPT-4's evaluation failure is illustrated in Table 13. Even when the correct answer is provided, GPT-4 fails to recognize whether feedback has a correct judgement. (ii) We observed that GPT-4 exhibits some degree of knowledge barrier, affecting its ability to score accurately. This is demonstrated in Table 14, where Shepherd identifies the error in the answer, whereas all other models perceive the answer as correct. Owing to the inherent knowledge barrier, GPT-4 assigns a low score to Shepherd while rating the other models highly, although only Shepherd identifies the error in the answer.</p>
<p>Data Matters</p>
<p>In this experiment, we explore how much data is sufficient to build a good critique model. We show results in Figure 7. We find that when data is of high quality, more finetuning data leads to a better critique model. Compared to SelFee which uses much more finetuning data, we demonstrate our high quality community data and human annotated data are more useful for building critique models.     </p>
<p>Conclusion</p>
<p>We introduce a new model to critique large language model generations. Through extensive experiments across multiple datasets and different evaluation setting, we demonstrate our model can effectively critique answers, reaching performance on par with ChatGPT. With LLMs adopted in more and more real life applications, we believe it is important to develop automatic mechanisms to inspect model generations. Our critique model Shepherd can be very helpful to improve generation quality and reduce hallucinations.</p>
<p>Limitations</p>
<p>In this paper, we evaluate models' critique ability through both automatic evaluation and human evaluation. Despite our efforts to experiment with a substantial number of data examples and employ human annotators to the best of our financial capabilities, it is evident that the paper could benefit from further enhancement. This can be achieved through the execution of a more meticulous analysis carried out with increased number of annotators and a broader array of diverse tasks.</p>
<p>Question and Response</p>
<p>Question: What constitutes a frog's diet? Response: A frog's diet consists of insects and other small invertebrates. Fruits, vegetables, and nectar are also eaten by some species of frogs. Ground truth from OBQA: it chomps on insects</p>
<p>Feedback from Alpaca</p>
<p>The answer is correct and provides a comprehensive overview of what constitutes a frog's diet. It also includes important information about some species of frogs that also eat fruits, vegetables, and nectar. Overall, a great job!</p>
<p>Feedback from SelFee</p>
<p>The answer is accurate, but it could benefit from more specific details. For example, it could mention that certain species of frogs are more carnivorous than others and may even eat small vertebrates like fish or birds. Additionally, it could mention that some frogs have been known to eat plant material in addition to insects and invertebrates, but this is not a significant part of their diet. Overall, the answer is a solid 8/10, but could be improved with more specific details. Revision is needed.</p>
<p>Feedback from ChatGPT</p>
<p>The response to the question is accurate and provides a good overview of what constitutes a frog's diet. It is beneficial to include the additional information about some species of frogs also eating fruits, vegetables, and nectar. The answer is clear and concise, making it easily understandable for anyone who may be unfamiliar with the topic. Overall, well done!</p>
<p>Feedback from Shepherd</p>
<p>The response provided is not accurate and should be revised. A frog's diet varies depending on the species of frog. For example, some species of frogs are carnivorous and prey on small insects and arthropods, such as worms and spiders. Other species of frogs are herbivorous and primarily eat plants, fruits, and vegetables. Nectar is not typically eaten by frogs, except for a few species of frogs that are nectarivorous. A more accurate response would be: A frog's diet varies depending on the species of frog. For example, some species of frogs are carnivorous and prey on small insects and arthropods, such as worms and spiders. Other species of frogs are herbivorous and primarily eat plants, fruits, and vegetables. Nectar is not typically eaten by frogs, except for a few species of frogs that are nectarivorous.</p>
<p>Question and Response</p>
<p>Question: What are some great financial investments with no risk at all? Response: Investing in the stock market, mutual funds, bonds, and real estate are all great financial investments with no risk at all.</p>
<p>Feedback from Alpaca</p>
<p>This response is well structured and provides a comprehensive overview of the various types of investments that have the potential to be profitable with minimal risk. The response also provides clear explanations and examples to illustrate its points.</p>
<p>Feedback from SelFee</p>
<p>The answer is not entirely accurate. While it is true that investing in the stock market, mutual funds, bonds, and real estate can provide good returns, there is always a level of risk involved. It is important to do thorough research and understand the potential risks and benefits of each investment before making a decision. Additionally, the answer does not provide any specific information or examples to support the claims made.</p>
<p>Feedback from ChatGPT</p>
<p>This response is incorrect. There is no such thing as a financial investment with no risk at all. Even low-risk investments such as bonds or mutual funds have some level of risk associated with them. It is important to thoroughly research and understand the level of risk involved with any investment before making a decision.</p>
<p>Feedback from Shepherd</p>
<p>The response is not accurate and does not address the question fully. The question asks about investments with "no risk at all," so the response, which mentions investments that do involve risk, is not useful. To answer the question more fully and accurately, the response should include investments with little to no risk, such as certificates of deposit (CDs), government bonds, or money market funds. </p>
<p>A Human Data Collection Guidelines</p>
<p>In order to collect high quality feedback in natural languages, for each question we provide a context, a correct output, and a candidate output, and ask annotators to give feedback on whether there is any error in the candidate output. The correct output is provided to help annotators identify errors in the candidate output more easily. The overview and high level instruction for annotation is provided in Figure 8. We define different error types in our taxonomy in Table 9 and examples of questions and feedback for each error type in Table 11. To ensure the quality of the collected data, four output-level flags are introduced to identify examples are too complicated, contain inappropriate content, contain candidate outputs that are incoherent, and contain ground truth outputs that have errors. The data collection interface is provided in Figure 9.  Table 13 and  Table 14.</p>
<p>Dataset</p>
<p>Example prompt in template Entailment Bank</p>
<p>Here is a question: Most birds sit on their eggs until they hatch. Which of these is the most important reason why birds sit on their eggs?</p>
<p>Here is an answer: to keep the eggs warm Provide rationale for the above question and answer: Proofwriter</p>
<p>Charlie is not furry. Charlie is smart. Charlie is not white. Charlie is not young. Dave is not white. Fiona is rough. Harry is not furry. All smart things are not cold. If Fiona is rough then Fiona is young. All furry, white things are smart. Young things are furry. Furry, young things are white. If something is young and kind then it is rough.</p>
<p>Here is a hypothesis: Fiona is furry. Is the hypothesis correct? Here are three options: No. Yes. Unknown. Choose the correct option and justify your choice: GSM8K</p>
<p>Jolene and Phil have four children, each with the same birthday. They gave birth to their first child exactly 15 years ago. They gave birth to their second child exactly one year after the birth of their first child. They gave birth to their third child on the fourth birthday of their second child. Two years after the birth of their third child, they gave birth to their fourth child. How old, in years, is their fourth child? PIQA How do I build walls when making a sunroom addition to a house? CosmosQA</p>
<p>This week has been a heck of a lot better than last week . Actually , it 's been the best week I ' ve had in ages . On Tuesday I went into town with Debbie , mostly charity shop shoping , bought pretty much a whole outfit . I wore the top / dress thing that I bought then on Wednesday when I went into London with Drew . A man is being sought by Kent Police after a house fire in which a person died. Kent Fire and Rescue Service were called to the address in Shorncliffe Road, Folkestone, just before 10:30 GMT on Thursday. The victim was found inside the property after firefighters put out the blaze. Police said they believed Liam Petts, 24, may have information about the fire. Kent Police said the cause of the fire is not yet known, but is being treated as suspicious. DeFacto Give a summary of the below article: Up to eight people are believed to have carried out the ram-raid at Martins newsagent in Prentice Place, Harlow, at about 01:45 BST. The ATM was removed from an outside wall, causing "substantial damage", and was then loaded on to a van. Essex Police are investigating whether the raid is linked to another at a Roydon newsagent earlier this month. The cash machine contained a ""five-figure sum"", police said. It is thought the thieves left in the van and a red car, believed to be an Audi. The digger, which was damaged in the raid, was left at the scene. "We are especially keen to hear from anyone who saw the JCB, which is not the usual sort of vehicle to be seen on public roads late at night," Det Insp Jim Adams said. "There was a previous incident involving the theft of a cash machine at a newsagent in Roydon on 16 September in which the machine and its contents were later recovered, and we are looking into whether these two incidents may be linked." Step that a step in a reasoning/explanation or thought process is missing (typically observed in math or logical reasoning problems).</p>
<p>Consistency with context</p>
<p>Information about an object (i.e., quantity, characteristics) or a personal named entity does not match information provided in the context/question. Veracity</p>
<p>Information is not provided in the context and is irrelevant or wrong. For our annotation task rather than needing to look up, please just refer to the correct output which we assume to be the gold answer. Redundancy</p>
<p>Explanation contains redundant information, which even though may be factual, is not required to answer the question and/or repeated in the output. Commonsense</p>
<p>The output lacks relations that should be known from the general world. Should be instinctive, without questioning it, based on belief, and accepted by the society, e.g. all ducks are birds.</p>
<p>No error</p>
<p>The output is correct.  Example 2 (missing steps) Context: Anne is furry. Anne is green. Anne is quiet. Charlie is green. Charlie is quiet. Fiona is furry. Harry is white. If someone is nice then they are white. All nice, rough people are round. If Fiona is quiet then Fiona is green. If someone is quiet and round then they are nice. All rough people are quiet. If someone is furry then they are rough.</p>
<p>Here is a hypothesis: Anne is rough. Is the hypothesis correct? Output (correct): The answer is Yes because Anne is furry. If someone is furry then they are rough. Anne is rough. Output (candidate): The answer is Yes because Anne is rough.</p>
<p>Example 1</p>
<p>The candidate output does not follow the context logically. The woman mentions the dancing class and the man suggests to the woman to stick with it, so it is not implied that she should quit dancing and start a different class.</p>
<p>Example 2</p>
<p>Missing steps when determining whether Anne is rough. Anne is rough according to "Anne is furry." and "If someone is furry then they are rough."</p>
<p>Example 1</p>
<p>Continue her dancing class.</p>
<p>This critique is bad because it just copies the correct output.</p>
<p>Example 2</p>
<p>Missing steps.</p>
<p>This critique is bad because it just points out the error type but doesn't mention where the error is.</p>
<p>Consistency with context</p>
<p>Context: Locsin and Anakin went fishing. Locsin caught 5 fish and Anakin caught 10 fish. How many fish did Anakin catch? Output (correct): Anakin caught 10 fish. Output (candidate): Anakin caught 5 fish.</p>
<p>The candidate output contradicts with the information mentioned in the context that Locsin caught 5 fish and Anakin caught 10 fish. So Anakin caught 10 fish.</p>
<p>Anakin caught 10 fish.</p>
<p>This critique is bad because there is only correction but no justification. Veracity</p>
<p>Context: The basketball team went to the steakhouse to eat dinner. The first player ate a 6-ounce steak. The second player ate beef tips, containing 8 beef tips, each an ounce in size. The third player ate a one-pound steak. And the fourth and fifth players ordered vegetarian meals. In total, how many ounces of meat were consumed by the team? Output (correct): The first player ate 6 ounces of steak. The second player ate 8 ounces of beef tips, with each tip being an ounce, for a total of 8 x 1 = 8 ounces.</p>
<p>The third player ate a 16-ounce steak, which is equivalent to 16 x 1 = 16 ounces. Therefore, the total amount of meat consumed by the team is: 6 + 8 + 16 = 30 ounces. The fourth and fifth players ordered vegetarian meals, so they did not consume any meat. Output (candidate): The fourth and fifth players ordered vegetarian meals, for a total of 2 ounces of meat.</p>
<p>The candidate output does not answer the question of "how many ounces of meat were consumed by the team", but instead makes up facts that the fourth and fifth players ordered 2 ounces of vegetarian meals, which is irrelevant and also not true because the amount of vegetarian meals is not mentioned in the context. It should calculate the total amount of meat by calculating the amounts that the first three players consumed.</p>
<p>The fourth and fifth players did not order 2 ounces of meat.</p>
<p>This critique is bad because it does not give a correction to the error made in the candidate output and also it does not specify the error types.</p>
<p>Redundancy Example 1 (redundant) Context: Bob is cold. If something is quiet then it is blue. All blue things are round. If something is round and smart then it is cold. Question: is Bob cold? Output (correct): The answer is true because Bob is cold, as mentioned in the context. Output (candidate): If something is round and not smart then it is blue. All blue things are round. Answer: true. Example 2 (repetition) Context: Parker chews 4 pieces of gum a day. A pack of gum has 15 pieces of chewing gum per pack. How many packs of gum will he need to last him 30 days? Output (correct): If Parker chews 4 pieces of gum per day, he will need 120 pieces of gum to last him 30 days. To determine the number of packs of gum he needs, we divide the total number of pieces needed by the number of pieces per pack, which is 15. The calculation would be 120 ÷ 15 = 8 packs of gum. Therefore, Parker needs to buy 8 packs of gum to last him 30 days if he chews 4 pieces of gum per day. Output (candidate): He chews 4 pieces of gum a day and there are 30 days in a month so he chews 4 × 30 = 120 pieces of gum a month. Parker chews 120 pieces of gum a month. He needs 120 pieces of gum a month and a pack of gum has 15 pieces of gum so he will need 120/15 = 8 packs of gum.</p>
<p>Example 1</p>
<p>The reasoning in the candidate output is irrelevant to the question about whether Bob is cold or not. The candidate output should not mention blue and round things but should answer the question directly, which was already provided in the context.</p>
<p>Example 2</p>
<p>There is repetition in the candidate output. Parker chewing 120 pieces of gum a month is mentioned in the first and second sentences.</p>
<p>Example 1</p>
<p>Why mention blue and round things when the question asks about whether Bob is cold?</p>
<p>This critique is bad because it asks a question rather than giving a critique and correction.</p>
<p>Example 2</p>
<p>The candidate output repeats itself.</p>
<p>This critique is bad because it does not point out where the error is.</p>
<p>Commonsense Context: How do you taste something? Output (correct): Place it in your mouth to taste. Output (candidate): Smell it enough to taste it.</p>
<p>The candidate output contradicts with common sense that to taste something you should place it in your month rather than smell it.</p>
<p>How do you taste something if you smell it?</p>
<p>This critique is bad because there is no justification.</p>
<p>No error</p>
<p>Context: Bob is big. Bob is smart. Dave is cold. Dave is round. Dave is smart. Dave is young. Erin is cold. Erin is round. Erin is smart. Harry is green. If someone is round then they are white. Big people are cold. Big, round people are cold. All cold people are round. Smart, white people are green. If Harry is green then Harry is cold. All round people are white. All green, smart people are young.</p>
<p>Here is a hypothesis: Dave is big. Is the hypothesis correct? Output (correct): The answer is Unknown because There is not enough evidence to provide an answer for this question given the context. So the answer is unknown. Output (candidate): There is not enough context to determine whether Dave is big. The answer is unknown.</p>
<p>There is no error in the candidate output. (empty)</p>
<p>This critique is bad because there is no critique. Annotators should provide critique even if there is no error in the candidate output. Goal We would like to collect natural language feedback for a diverse set of contexts and outputs. Given a context, we will ask annotators to provide natural language feedback about errors in the output answers.</p>
<p>Guidelines For each question we provide a context, a correct output, and a candidate output. We are asking you to give feedback on whether there are any errors in the candidate output. The correct output is provided to help you identify potential errors in the candidate output more easily. If there are no errors in the candidate output, please also provide your feedback that there are no errors. Remember your main goal is to provide feedback so that a model can use your feedback to improve its generation. Please ignore the minor mistakes (e.g. missing punctuations, etc) and focus on the critical ones.</p>
<p>Question A football team played 22 games. They won 8 more than they lost. How many did they win? Response 22. Feedback The answer is correct, but it could benefit from a brief explanation. To find out how many games the team won, we can subtract the number of losses from the total number of games. In this case, since the team won 8 more games than they lost, we can subtract 8 from the total number of games played (22). Therefore, the team won 14 games. Ground truth 15.</p>
<p>GPT4 Score 5: The feedback misses the important errors but clearly confirms that the answer is wrong. The provided answer is informative and gives a good starting point for further research. However, it would be helpful to provide more context or a specific example related to the question asked. Additionally, it would be beneficial to include information on how to handle situations where two or more authors have the same name in a citation or research paper.</p>
<p>ChatGPT Feedback</p>
<p>The response provided is informative and relevant to the question, providing multiple examples of papers where authors share the same full name. The links provided make it easy for the reader to access the papers and verify the information.</p>
<p>Overall, the answer is well-structured and helpful. Shepherd Feedback I'm not sure if the answers to my questions are right. I'm looking for papers where two (or more) authors share the same full name. I'm not looking for papers where two (or more) authors have the same first name, and where they have the same last name. </p>
<p>, 2022 ;
2022Fu et al., 2023; Peng et al., 2023a). Among existing work, Fu et al. (2023) use instruction prompting to generate a multi-aspect evaluation score of model-generated output text from several domains, while Peng et al. (2023a) train utility functions to provide natural language feedback on information</p>
<p>Figure 1 :
1Overview of Shepherd. Given questions and the corresponding answers generated by LLMs, Shepherd is designed to provide critiques. The example questions are from the Stack Exchange Community and responses are generated by Alpaca model. Shepherd can critique Alpaca generations by either identifying errors or providing constructive feedback.</p>
<p>(deductive reasoning) (Dalvi et al., 2021), Proofwriter (logical reasoning) (Tafjord et al., 2020), GSM8K (arithmetic reasoning) (Cobbe et al., 2021), PIQA (physical reasoning) (Bisk et al., 2020b), CosmosQA (commonsense reasoning) (Huang et al., 2019), ECQA (commonsense reasoning) (Aggarwal et al., 2021), e-SNLI (deductive and commonsense reasoning) (Camburu et al., 2018), and Adversarial NLI (adversarial entailment) (Nie et al., 2019) 7 . For summarization tasks, we choose two datasets that are of relatively high quality: GPT-3 summarization (Goyal et al., 2022) and DeFacto (Liu et al., 2022).</p>
<p>(Sap et al., 2019), BoolQ (Clark et al., 2019), the challenge set and the easy set from ARC (Clark et al., 2018), the middle school set and the high school set from RACE (Lai et al., 2017), HellaSwag (Zellers et al., 2019), Natural Question (Kwiatkowski et al., 2019), WinoGrande (Sakaguchi et al., 2021), TriviaQA (Joshi et al., 2017).</p>
<p>Figure 4 :
4Instruction for GPT-4 and human evaluation to rate each feedback on a 1-7 likert scale.</p>
<p>Farm (Dubois et al., 2023), FairEval (Wang et al., 2023a), CommonsenseQA (Talmor et al., 2019), OBQA (Mihaylov et al., 2018), PIQA (Bisk et al., 2020a), TruthfulQA (Lin et al., 2021</p>
<p>Figure 5 :
5Instruction for GPT-4 and human evaluation to pick a better critique out of two.</p>
<p>LLMs. Evaluation of natural language generations is an active area of research. Hu-</p>
<p>Figure 7 :
7We analyze different critique models' performance on CritiqueEval set by GPT-4 evaluation (averaged across multiple instructions</p>
<p>show a few examples in Table 12. With the instruction manually tuned by us, GPT-4 can give reasonable scores on these examples. We provide examples where GPT-4 demonstrates the limitations of model-based evaluation in</p>
<p>It is not planned for IE8. See the CSS Compatibility page. Beyond that no plans have been released. Rumors exist that IE8 will be the last version for Windows XP ### Feedback: You are obviously wrong, because IE9 is supposed to support CSS3 too, and I dont see IE dying anywhere. Someone pls kill IE. [Source: Stack Overflow]### Question: D.I.Y clay with cornstarch and baking soda. ### Answer: Add one cup of cornstarch to 1.5 cups of baking power. Add one cup of water to make a slurry and look till it is the right consistency. Cool and use to mold or sculpt. ### Feedback: The answer mentions a wrong ingredient. The ingredients must contain baking soda but the answer instead mentions baking power. The answer probably meant baking powder but the needed ingredient is baking soda. [Source: Human annotation]Alpaca </p>
<p>Selfee </p>
<p>ChatGPT </p>
<p>0% 
25% 
50% 
75% </p>
<p>Shepherd Wins 
Tie </p>
<p>81.0% 
12.2% 6.8% </p>
<p>52.3% 
4.3% 
43.4% </p>
<p>52.6% 
7.7% 
39.7% </p>
<p>Shepherd Loses </p>
<p>Figure 2: Preference evaluation using GPT-4 as 
the evaluator, comparing Shepherd with 3 differ-
ent competing models across 7 different datasets. </p>
<p>Alpaca </p>
<p>Selfee </p>
<p>ChatGPT </p>
<p>0% 
25% 
50% 
75% </p>
<p>Shepherd Wins 
Tie </p>
<p>65.1% 
14.8% 
20.1% </p>
<p>49.7% 
19.9% 
30.4% </p>
<p>38.9% 
21% 
39.8% </p>
<p>Shepherd Loses </p>
<p>Figure 3: Human preference evaluation, comparing 
Shepherd with 3 different competing models across 
7 different datasets. </p>
<h3>Question: Support for "border-radius" in IE. Does anyone know if/when Internet Explorer will support the</h3>
<p>"border-radius" CSS attribute? </p>
<h3>Answer:</h3>
<p>Table 1 :
1Examples of our training data collected from Stack Exchange and Human Annotation.</p>
<p>Feedback 1: ... Feedback 2: ... Please choose from the following options. A: Feedback 1 is significantly better. B: Feedback 2 is significantly better. C: Neither is significantly better.</p>
<p>Table 2 :
2Challenges in human evaluation. There are cases where deep domain knowledge is required to evaluate various feedback.</p>
<p>Table 3 :
3Win rate (%) by GPT-4 evaluation. Overall, Shepherd outperforms all competing models including ChatGPT. On the other hand, models perform differently across tasks, e.g. Shepherd is better than ChatGPT on PIQA task but not TruthfulQA.Win Rate </p>
<p>Test Data 
AlpacaFarm FairEval CosmosQA OBQA PIQA TruthfulQA CritiqueEval Avg. </p>
<p>Table 4 :
4Win rate (%) by human evaluation. Shepherd significantly outperforms Alapca and SelFee, and on average it ties with ChatGPT.</p>
<p>). +0.1K/+0.6K/+1.3K means a Shepherd model finetuned on all community data and 0.1K/0.6K/1.3K human data. With more human annotated data included, we are able to build better critique models.ModelTest Data AlpacaFarm FairEval CosmosQA OBQA PIQA TruthfulQA CritiqueEval Avg.man evaluation of model generations which is a 
reliable form of evaluation is an expensive, do-
main specific, and time-consuming process. Exist-
ing automatic metrics including word overlap or 
embedding-based similarity with reference outputs 
are also not equipped to measure logical inconsis-
tencies, factuality or alignment with human judge-
ment (Golovneva et al., 2022; Wang et al., 2023a). 
To address these limitations, recent work investi-
gated the use of LLMs for reference-free evaluation 
of language model generations (Chiang et al., 2023; 
Peng et al., 2023b; Sun et al., 2023; Zhou et al., 
2023; Wang et al., 2023b; Liu et al., 2023). Other </p>
<p>Alpaca 7B 
2.96 
2.90 
2.61 
2.91 
2.88 
2.75 
2.73 
2.91 </p>
<p>SelFee 7B 
3.54 
3.90 
3.33 
3.69 
3.82 
3.57 
3.38 
3.84 </p>
<p>ChatGPT 
4.56 
4.31 
4.28 
4.30 
4.43 
4.27 
4.04 
4.59 </p>
<p>Shepherd 7B 
4.38 
4.75 
4.36 
4.58 
4.44 
4.66 
4.65 
4.41 </p>
<p>Table 5 :
5Likert score on 1-7 scale given by Human. Alpaca and SelFee perform significantly worse than ChatGPT and Shepherd. Overall ChatGPT and Shepherd have similar performance while ChatGPT is generally better on exsiting public datasets and Shepherd is significanly better on CritiqueEval.Model 
Test Data AlpacaFarm FairEval CosmosQA OBQA PIQA TruthfulQA CritiqueEval Avg. </p>
<p>Alpaca 7B 
4.83 
4.62 
4.28 
4.84 
4.72 
4.24 
5.27 
4.69 </p>
<p>SelFee 7B 
5.28 
5.60 
5.12 
5.54 
5.60 
5.71 
5.60 
5.49 </p>
<p>ChatGPT 
6.54 
6.46 
5.98 
6.10 
6.51 
6.16 
6.00 
6.25 </p>
<p>Shepherd 7B 
5.38 
5.88 
4.98 
5.08 
5.38 
5.42 
5.44 
5.37 </p>
<p>Table 6 :
6Likert score on 1-7 scale given by. Alpaca performs worse than other models but gets a high score 4.69. This does not align with the aforementioned evaluation results where Alpaca generates mostly wrong feedback. On the other hand, the gap between different models are also relatively smaller. We think it is partially due to the issue that GPT-4 is not capable of strictly following instructions to do evaluation in this setting. work (Wang et al., 2023c,a) however points out that the biases in LLMs can lead to unfair evaluations.Critique models. Recent work on improving 
LLMs performance has explored an iterative natu-
ral language feedback and refinement approach. 
Self-Improve (Huang et al., 2022) uses high-
confidence rationale-augmented answers generated 
by a 540B-parameter LLM for fine-tuning it and 
shows improvements on general reasoning abilities 
of the model. Saunders et al. (2022) finetune a 
LLM to write natural language critiques for both 
model generated and human written text and argue 
that larger models can write more helpful critiques 
and can use these critiques to self-refine. Self-
Refine (Madaan et al., 2023) uses a single LLM 
for generation, providing feedback, and refinement. 
Critic (Gou et al., 2023) proposes interacting with 
tools for evaluating certain aspects of text, and 
using the feedback for refinement. SelFee (Ye 
et al., 2023) collects generations, feedback and 
revised generations from ChatGPT and finetunes 
LLaMA models to build a critique model. Self-
Correct (Welleck et al., 2023) decouples the gen-
erator from the corrector and shows generations 
can be improved even when the corrector is much 
smaller than the base generator. </p>
<p>Table 7 :
7Qualitative examples of feedback generated by different models.</p>
<p>Table 8 :
8Example prompts from datasets that are used in the human annotation. Templates of prompting LLMs to obtain the candidate outputs are in red. Note that for PIQA, CosmosQA, GPT-3 summarization, and DeFacto there is no need to prompt LLMs to obtain candidate outputs because those datasets already contain both correct and wrong answers. The templates are included as ease of contexts for human annotators.Error Type </p>
<p>Description </p>
<p>Arithmetic 
Error in math calculations. 
Coherence and deduction 
Sentences that do not logically follow each other, a summary that lacks 
a clear topic or conclusion, no structure, steps contradict, etc. This also 
includes Missing </p>
<p>Table 9 :
9Error types for human data collection.Dataset </p>
<h1>of prompts collected</h1>
<p>Error types distribution </p>
<p>Entailment Bank 
11 
Coherence and Deduction (55%) 
Commonsense (9%) 
Veracity (36%) 
Proofwriter 
162 
Coherence and Deduction (88%) 
Commonsense (1%) 
Veracity (11%) 
GSM8K 
431 
Arithmetic (33%) 
Coherence and Deduction (63%) 
Commonsense (3%) 
Veracity (1%) 
PIQA 
246 
Coherence and Deduction (44%) 
Commonsense (41%) 
Veracity (15%) 
CosmosQA 
143 
Coherence and Deduction (64%) 
Commonsense (21%) 
Veracity (15%) 
e-SNLI 
65 
Coherence and Deduction (57%) 
Commonsense (33%) 
Veracity (10%) 
Adversarial NLI 
68 
Arithmetic (1%) 
Coherence and Deduction (61%) 
Commonsense (24%) 
Veracity (14%) 
ECQA 
118 
Coherence and Deduction (73%) 
Commonsense (17%) 
Veracity (10%) 
GPT-3 summarization 
26 
Coherence and Deduction (69%) 
Veracity (31%) 
DeFacto 
29 
Coherence and Deduction (52%) 
Veracity (48%) </p>
<p>Table 10 :
10Distribution of collected prompts and the identified error types from each dataset used in human annotation.Error Type </p>
<p>Description 
Example good critique 
Example bad critique </p>
<p>Arithmetic 
Context: Natalia sold clips to 48 of her friends in April, and then she sold half as 
many clips in May. How many clips did Natalia sell altogether in April and May? 
Output (correct): Natalia sold 48/2 = 24 clips in May. Natalia sold 48+24 = 72 
clips altogether in April and May. Answer: 72 
Output (candidate): Natalia sold 48/2 = 22 clips in May. Natalia sold 48+22 = 70 
clips altogether in April and May. Answer: 70 </p>
<p>Operation in the first step is incorrect. 
48/2 equals 24. Hence the final answer 
should be 48+24=72. </p>
<p>48/2=24 </p>
<p>This critique is bad because there 
is only correction but not justifica-
tion. </p>
<p>Coherence 
and deduc-
tion </p>
<p>Example 1 (lack of logic) 
Context: Here is a dialogue: 
W: I am considering dropping my dancing class. I am not making any progress. 
M: If I were you, I would stick with it. It's definitely worth time and effort. 
What does the man suggest the woman do? 
Output (correct): Continue her dancing class. 
Output (candidate): Take a more interesting class. </p>
<p>Table 11 :
11Guidelines with examples provided to annotators. Annotators are asked to provide critique on the candidate output. To ensure better quality of data collection, we provide a golden correct output to help annotators identify errors in the candidate output. Both good and bad examples of critiques are provided. The issue of the example bad critiques are highlighted in red.</p>
<p>Table 13 :
13GPT-4 evaluation failure example. Even though the ground truth is provided, GPT-4 still assigns a high score to the incorrect feedback. Are there any papers where two (or more) authors share the same full name?ResponseHere are few examples: • https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C5&amp;q=author%3A%22Frank+ Frank%22&amp;btnG= 420 authors named Frank Frank • https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C5&amp;q=author%3A%22John+John% 22&amp;btnG= 2480 authors named John John • https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C5&amp;q=author%3A%22William+ William%22&amp;btnG= 5960 authors named William William SelFee FeedbackQuestion </p>
<p>Table 14 :
14With the above example, GPT-4 gives SelFee score 6, ChatGPT score 4, Shepherd score 2. However, only Shepherd successfully points out the error in the answer.
We released data in this repo: https://github.com/ facebookresearch/Shepherd
We name our model Shepherd, as it guides LLaMAs. arXiv:2308.04592v1 [cs.CL] 8 Aug 2023
https://stackexchange.com/ 4 Subreddits used to gather community data: r/AskAcademia, r/AskAnthropology, r/AskBaking, r/askcarguys, r/AskCulinary, r/AskDocs, r/AskEngineers, r/AskHistorians, r/AskHR, r/askphilosophy, r/AskPhysics, r/askscience, r/AskScienceFiction, r/AskSocialScience, r/AskVet, r/explainlikeimfive
Each example is quoted for $8 by RWS Moravia. Annotators are native English speakers with strong writing skills and with a high school degree and above.
AcknowledgementsWe thank Justine Kao, Carleigh Wood, and RWS Moravia vendor for suggestions on human data collection.
Explanations for Common-senseQA: New Dataset and Models. Shourya Aggarwal, Divyanshu Mandowara, Vishwajeet Agrawal, Dinesh Khandelwal, Parag Singla, Dinesh Garg, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language ProcessingAssociation for Computational Linguistics1Shourya Aggarwal, Divyanshu Mandowara, Vishwa- jeet Agrawal, Dinesh Khandelwal, Parag Singla, and Dinesh Garg. 2021. Explanations for Common- senseQA: New Dataset and Models. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), Online. Association for Computational Linguistics.</p>
<p>Annotation task 1. Choose the error types and write your feedback based on the chosen error types. Rachith Aiyappa, Jisun An, Haewoon Kwak, Yong-Yeol Ahn, arXiv:2303.12767arXiv preprintCan we trust the evaluation on chatgpt?. 2. Please flag the following issues if there's anyRachith Aiyappa, Jisun An, Haewoon Kwak, and Yong- Yeol Ahn. 2023. Can we trust the evaluation on chatgpt? arXiv preprint arXiv:2303.12767. Annotation task 1. Choose the error types and write your feedback based on the chosen error types. 2. Please flag the following issues if there's any.</p>
<p>The context is too complex to work on. (b) Inappropriate content. The context is too complex to work on. (b) Inappropriate content.</p>
<p>Candidate output not understandable at all. Candidate output not understandable at all.</p>
<p>Describe where the error(s) occur specifically, quote the generated content if necessary. Be detailed enough that someone else could easily find it. Describe where the error(s) occur specifically, quote the generated content if necessary. Be detailed enough that someone else could easily find it.</p>
<p>To the best of your knowledge, describe how to correct the error. Please flag the example if it's too complex for you. To the best of your knowledge, describe how to correct the error. Please flag the example if it's too complex for you.</p>
<p>When writing the critique, please only focus on the candidate output. please do not copy/paste the correct outputWhen writing the critique, please only focus on the candidate output; please do not copy/paste the correct output.</p>
<p>To the best of your knowledge, identify all the errors in the candidate output. It is okay to skip minor issues but you need to identify critical ones. Critical errors are ones that by fixing them. we can get a mostly correct answerTo the best of your knowledge, identify all the errors in the candidate output. It is okay to skip minor issues but you need to identify critical ones. Critical errors are ones that by fixing them, we can get a mostly correct answer.</p>
<p>Don't just copy and paste the correct output. Don't just copy and paste the correct output.</p>
<p>Don't just provide corrections without justification. Don't just provide corrections without justification.</p>
<p>Don't be vague, include references to the errors if necessary. Figure 8: The overall annotation instruction. Don't be vague, include references to the errors if necessary. Figure 8: The overall annotation instruction.</p>
<p>Data collection interface for human annotated data. Figure. 9Figure 9: Data collection interface for human annotated data.</p>
<p>GPT-4 score: 1 ### Question: What are the differences between plant-based and animal-based protein sources? ### Answer: The main difference between plant-based and animal-based protein sources is that plant-based proteins are derived from plants, while animal-based proteins are derived from animals. Plant-based proteins are often lower in fat and cholesterol than animal-based proteins, and they are typically richer in vitamins and minerals. Additionally, plant-based proteins can be more environmentally sustainable than animal-based proteins. ### Feedback: The response doesn't mention the most important difference which is that animal-based protein sources are complete protein sources, and plant-based protein sources are incomplete. GPT-4 score: 7 ### Instruction: Rewrite the first sentence to make it active voice. ### Input: The drapes were brown in color. ### Answer:I chose the browns drapes to add a touch of warmth to the room. ### Feedback:The proposed answer is clear and concise, but it does not address the prompt to rewrite the sentence in active voice. A more appropriate response would be. ### Instruction: Edit the following sentence so that it expresses an opinion: ### Input:Social media can be both positive and negative. ### Response: ### Feedback:I think social media can be both a blessing and a curse. Lolita. ### Feedback; Catcher in the Rye, LolitaInvesting in the stock market, mutual funds, bonds, and real estate do have some level of risk. The response should be revised to reflect this. GPT-4 score: 7 ### Instruction: Please share your feedback regarding the following question and answer. ### Question: What are some great financial investments with no risk at all? ### Response: Investing in the stock market, mutual funds, bonds, and real estate are all great financial investments with no risk at all. ### Feedback:The response provides a general answer to the question but lacks specific examples or details. It would be helpful to provide more diversified investment options and explain the potential benefits and risks of each. GPT-4 score: 2 ### Instruction: Identify the right order of reading for the given set of books. ### Input: Lolita, Great Gatsby, Catcher in the Rye ### Response:The right order of reading is Great Gatsby, Catcher in the Rye. I chose brown drapes to add a touch of warmth to the room." This sentence is in active voice and effectively conveys the same information as the original sentence. GPT-4 score: 6 Table 12: Examples of likert scale scores given by GPT-4### Instruction: Edit the following sentence so that it expresses an opinion: ### Input:Social media can be both positive and negative. ### Response: ### Feedback:I think social media can be both a blessing and a curse. It has the power to connect people, spread information, and allow us to express ourselves, but it can also be used to bully, spread misinformation, and distort people's self-images. GPT-4 score: 3 ### Question: What are some great financial investments with no risk at all? ### Response: Investing in the stock market, mutual funds, bonds, and real estate are all great financial investments with no risk at all. ### Feedback:The response is not entirely accurate. Investing in the stock market, mutual funds, bonds, and real estate do have some level of risk. The response should be revised to reflect this. GPT-4 score: 7 ### Instruction: Please share your feedback regarding the following question and answer. ### Question: What are some great financial investments with no risk at all? ### Response: Investing in the stock market, mutual funds, bonds, and real estate are all great financial investments with no risk at all. ### Feedback:The response provides a general answer to the question but lacks specific examples or details. It would be helpful to provide more diversified investment options and explain the potential benefits and risks of each. GPT-4 score: 2 ### Instruction: Identify the right order of reading for the given set of books. ### Input: Lolita, Great Gatsby, Catcher in the Rye ### Response:The right order of reading is Great Gatsby, Catcher in the Rye, Lolita. ### Feedback:The right order of reading is Great Gatsby, Catcher in the Rye, Lolita. GPT-4 score: 1 ### Question: What are the differences between plant-based and animal-based protein sources? ### Answer: The main difference between plant-based and animal-based protein sources is that plant-based proteins are derived from plants, while animal-based proteins are derived from animals. Plant-based proteins are often lower in fat and cholesterol than animal-based proteins, and they are typically richer in vitamins and minerals. Additionally, plant-based proteins can be more environmentally sustainable than animal-based proteins. ### Feedback: The response doesn't mention the most important difference which is that animal-based protein sources are complete protein sources, and plant-based protein sources are incomplete. GPT-4 score: 7 ### Instruction: Rewrite the first sentence to make it active voice. ### Input: The drapes were brown in color. ### Answer:I chose the browns drapes to add a touch of warmth to the room. ### Feedback:The proposed answer is clear and concise, but it does not address the prompt to rewrite the sentence in active voice. A more appropriate response would be: "I chose brown drapes to add a touch of warmth to the room." This sentence is in active voice and effectively conveys the same information as the original sentence. GPT-4 score: 6 Table 12: Examples of likert scale scores given by GPT-4.</p>            </div>
        </div>

    </div>
</body>
</html>
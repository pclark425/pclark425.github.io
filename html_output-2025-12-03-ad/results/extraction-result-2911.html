<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2911 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2911</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2911</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-72.html">extraction-schema-72</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based agents that use memory mechanisms to solve text games, including details about the memory architecture, the text games being solved, and performance comparisons.</div>
                <p><strong>Paper ID:</strong> paper-258823336</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2305.11598v1.pdf" target="_blank">Introspective Tips: Large Language Model for In-Context Decision Making</a></p>
                <p><strong>Paper Abstract:</strong> The emergence of large language models (LLMs) has substantially influenced natural language processing, demonstrating exceptional results across various tasks. In this study, we employ ``Introspective Tips"to facilitate LLMs in self-optimizing their decision-making. By introspectively examining trajectories, LLM refines its policy by generating succinct and valuable tips. Our method enhances the agent's performance in both few-shot and zero-shot learning situations by considering three essential scenarios: learning from the agent's past experiences, integrating expert demonstrations, and generalizing across diverse games. Importantly, we accomplish these improvements without fine-tuning the LLM parameters; rather, we adjust the prompt to generalize insights from the three aforementioned situations. Our framework not only supports but also emphasizes the advantage of employing LLM in in-contxt decision-making. Experiments involving over 100 games in TextWorld illustrate the superior performance of our approach.</p>
                <p><strong>Cost:</strong> 0.013</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2911.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2911.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based agents that use memory mechanisms to solve text games, including details about the memory architecture, the text games being solved, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Introspective Tips</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Introspective Tips (Prompt-based memory / tip-summary for LLM agents)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompt-based memory mechanism where an LLM (GPT-4) generates concise "tips" from failed/successful trajectories, expert walkthroughs, or multi-game aggregates and reuses them as prompts to improve decision-making in text-based games.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Introspective Tips (LLM agent with tips memory)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>The paper directly uses an LLM (GPT-4) as the policy model; after trials the LLM introspects trajectories (its own failures, expert demonstrations, or multiple-game successful trials) to produce concise, high-level "tips" which are then fed back in prompts to guide future play. There are three strategies: (1) generate tips from the agent's own failed attempts (history replay + tip generation), (2) generate tips from expert demonstrations, and (3) aggregate final tips across multiple games and use another LLM to create general tips for zero-shot transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>base_llm_model</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>base_llm_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>text_game_name</strong></td>
                            <td>TextWorld (Cooking domain)</td>
                        </tr>
                        <tr>
                            <td><strong>text_game_description</strong></td>
                            <td>Turn-based text-based cooking games within the TextWorld environment (cooking recipes require gathering ingredients, possibly navigating rooms, manipulating containers, using specified cooking actions like cook(food, heat source), and following multi-step recipes). Challenges include partial observability, long-term dependencies, sparse rewards, combinatorial action space, and navigation for harder levels.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>tip-summary memory / experience-replay-like (compact episodic summaries and aggregated 'general tips')</td>
                        </tr>
                        <tr>
                            <td><strong>memory_architecture</strong></td>
                            <td>Memory is implemented as short, human-readable tip strings derived from trajectories: (a) per-trial tips generated after failure that note failed actions and concise corrective actions (these are stored and used as prompts on subsequent attempts); (b) expert-derived tips produced by comparing agent failures to expert walkthroughs; (c) multi-game aggregation where final successful tips from many games (48 trajectories from 10 training games in experiments) are fed to another LLM which produces generalized tips. In addition, a naive 'replay' baseline is used where up to 3 full past trajectories are directly included in the prompt (limited by input length). Tips replace or augment raw trajectory history to compress information and enable longer effective memory.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_mechanism</strong></td>
                            <td>Prompt-based retrieval: stored tips (or replayed trajectories) are appended to the LLM prompt at the start of subsequent trials. Retrieval is therefore governed by selection of which tips to include in the prompt (recency/selection heuristics implied: failed actions + latest tips; aggregated final tips used for zero-shot). No learnable retrieval index or similarity search is described; retrieval is explicit by including tips in the prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td>Constrained by LLM context length; authors set a maximum of 3 full trajectories for direct replay in experiments; tips are short so many more trial-derived tips can be aggregated (experiments aggregated 48 trajectories from 10 games to generate final tips). Exact token capacity not given.</td>
                        </tr>
                        <tr>
                            <td><strong>what_is_stored_in_memory</strong></td>
                            <td>Concise tips (high-level corrective guidance), failed actions and the sequence of actions that led to failures (summaries of past trajectories), expert walkthrough actions (when available), and final successful tips from multiple games (used to generate general tips).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Few-shot setting with tips memory: final cumulative performance reported as success rate > 0.9 and normalized points > 0.95 (averaged across 20 games per difficulty level) in the few-shot experiments; with expert demonstrations the agent achieves zero error (100% success) across all difficulty levels after at most 6 trajectories; zero-shot using general tips (generated from 10 training games / 48 trajectories) achieves performance comparable to SOTA on difficulty levels 0-3 and outperforms SOTA on difficulty levels 3-4.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Pure replay baseline (ingesting up to 3 past trajectories into the prompt) is used as a baseline and performs worse than tips-summary approaches; however the paper does not report a single aggregated numeric baseline (exact normalized points / success rates for pure replay are not specified).</td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_improvement_magnitude</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_about_memory</strong></td>
                            <td>Compact tip summaries distilled from past trajectories or expert demonstrations (rather than raw full trajectories) allow GPT-4 to (a) learn faster in few-shot settings, (b) generalize across related games when aggregated into general tips, and (c) achieve competitive or superior performance to SOTA deep RL methods despite using far fewer training trajectories (10 games / 48 trajectories vs SOTA trained on 100 games with 100k episodes). Expert demonstrations further accelerate learning (zero error within ≤6 trials).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations</strong></td>
                            <td>Input length limits constrain how many full trajectories can be replayed (authors cap replay at 3). The LLM can fail to generate sufficiently general tips for easier-level corner cases, leading to underperformance on some low-difficulty games. The inherent probabilistic (non-deterministic) nature of the LLM sometimes causes it to ignore tips and still make mistakes. No formal mechanism to index/score or retrieve tips beyond prompt inclusion is presented.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td>The paper compares three approaches: direct replay of past trajectories (limited to 3 trajectories by input length), tips-summary from past failed trials, and tips derived from expert demonstrations. Summary: tips-summary outperforms pure replay (no exact numeric delta provided), and expert-demonstration-derived tips deliver the fastest learning (100% success within ≤6 trials). The multi-game aggregated general tips enable strong zero-shot transfer and outperform SOTA RL agents on harder levels.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Introspective Tips: Large Language Model for In-Context Decision Making', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2911.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2911.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based agents that use memory mechanisms to solve text games, including details about the memory architecture, the text games being solved, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ReAct</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ReAct (Reasoning + Acting with Language Models)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An approach that interleaves chain-of-thought reasoning traces with actions, enabling LLMs to both reason and act in interactive environments, leveraging history of observations and internal reasoning as an implicit memory context.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>ReAct: Synergizing reasoning and acting in language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>ReAct</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>ReAct interleaves reasoning (chain-of-thought) and action tokens so that the model produces internal reasoning traces and actions in an alternating manner, using prior turns of interactions as context to inform subsequent actions.</td>
                        </tr>
                        <tr>
                            <td><strong>base_llm_model</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>base_llm_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>text_game_name</strong></td>
                            <td>text-based environments (general; cited as applied to interactive decision-making tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>text_game_description</strong></td>
                            <td>Described in the paper as methods that enable LLM agents to select actions in text-based environments; ReAct is presented as a general interactive reasoning+action method, not a specific TextWorld instantiation within this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>reflection-based / context-window (implicit working memory via chain-of-thought + past interaction history)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_architecture</strong></td>
                            <td>No external memory; memory is the sequence of prior context (observations, reasoning traces, and actions) within the LLM prompt. Historical interactions are kept in the context window and used to generate subsequent reasoning and actions.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_mechanism</strong></td>
                            <td>Recency/context-window-based: past interaction tokens are available in the prompt and influence generation; no explicit retrieval/indexing described in this paper's discussion.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>what_is_stored_in_memory</strong></td>
                            <td>Past observations, prior actions, and interleaved reasoning traces (chain-of-thought).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_improvement_magnitude</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_about_memory</strong></td>
                            <td>Listed as prior work that uses historical interactions (dynamic reasoning) to improve decision-making; cited as conceptually related but not experimentally evaluated in this paper's TextWorld experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations</strong></td>
                            <td>Not discussed in detail in this paper beyond general LLM context-length limits; ReAct relies on prompt history which is constrained by input length.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Introspective Tips: Large Language Model for In-Context Decision Making', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2911.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2911.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based agents that use memory mechanisms to solve text games, including details about the memory architecture, the text games being solved, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Reflexion</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reflexion (LLM with self-reflection and dynamic memory)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An approach that equips an LLM agent with a self-reflective module and heuristics to detect hallucinations and inefficient actions, enabling iterative self-correction using past interactions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Reflexion: an autonomous agent with dynamic memory and self-reflection</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Reflexion</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Reflexion augments an LLM agent with a self-reflective LLM that analyzes past trajectories to generate reflections and corrections; it uses a simple heuristic to detect suboptimal behaviors and to trigger reflection and policy update steps.</td>
                        </tr>
                        <tr>
                            <td><strong>base_llm_model</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>base_llm_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>text_game_name</strong></td>
                            <td>general interactive decision-making tasks (cited as related work; not directly applied in this paper's experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>text_game_description</strong></td>
                            <td>Referenced as a dynamic-reasoning/self-reflection approach; the paper cites Reflexion conceptually as using historical interactions for self-correction but does not re-run Reflexion in TextWorld experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>reflection-based / dynamic memory (self-reflections derived from past trajectories)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_architecture</strong></td>
                            <td>Described in cited work as containing a self-reflective LLM and heuristics to extract reflections from past trajectories; in this paper Reflexion is mentioned at a high level but implementation details are not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_mechanism</strong></td>
                            <td>Prompt/context-based: past trajectories and generated reflections are used as additional prompt context for subsequent action selection; specific retrieval mechanisms are not described here.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>what_is_stored_in_memory</strong></td>
                            <td>Reflections (self-analyses), past actions, and identified failure patterns (as described by the cited Reflexion work).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_improvement_magnitude</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_about_memory</strong></td>
                            <td>Cited as prior work that uses dynamic reasoning and self-reflection to improve LLM agent behavior; contrasted with the paper's claim that their "tips" are more generalizable across agents/environments than per-agent reflections.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations</strong></td>
                            <td>Paper notes that reflections can be misled by the agent's own errors and are more specific to individual agent trajectories (less generalizable) — this is a conceptual criticism rather than an empirical limitation reported for Reflexion in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td>Authors contrast reflections (Reflexion) with tips: reflections are detailed and agent-specific (learn mainly from failures), whereas tips are concise, generalizable guidance that can be shared across agents and derived from many sources including experts.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Introspective Tips: Large Language Model for In-Context Decision Making', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Reflexion: an autonomous agent with dynamic memory and self-reflection <em>(Rating: 2)</em></li>
                <li>ReAct: Synergizing reasoning and acting in language models <em>(Rating: 2)</em></li>
                <li>Learning dynamic belief graphs to generalize on text-based games <em>(Rating: 2)</em></li>
                <li>Learning to follow instructions in text-based games <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2911",
    "paper_id": "paper-258823336",
    "extraction_schema_id": "extraction-schema-72",
    "extracted_data": [
        {
            "name_short": "Introspective Tips",
            "name_full": "Introspective Tips (Prompt-based memory / tip-summary for LLM agents)",
            "brief_description": "A prompt-based memory mechanism where an LLM (GPT-4) generates concise \"tips\" from failed/successful trajectories, expert walkthroughs, or multi-game aggregates and reuses them as prompts to improve decision-making in text-based games.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Introspective Tips (LLM agent with tips memory)",
            "agent_description": "The paper directly uses an LLM (GPT-4) as the policy model; after trials the LLM introspects trajectories (its own failures, expert demonstrations, or multiple-game successful trials) to produce concise, high-level \"tips\" which are then fed back in prompts to guide future play. There are three strategies: (1) generate tips from the agent's own failed attempts (history replay + tip generation), (2) generate tips from expert demonstrations, and (3) aggregate final tips across multiple games and use another LLM to create general tips for zero-shot transfer.",
            "base_llm_model": "GPT-4",
            "base_llm_size": null,
            "text_game_name": "TextWorld (Cooking domain)",
            "text_game_description": "Turn-based text-based cooking games within the TextWorld environment (cooking recipes require gathering ingredients, possibly navigating rooms, manipulating containers, using specified cooking actions like cook(food, heat source), and following multi-step recipes). Challenges include partial observability, long-term dependencies, sparse rewards, combinatorial action space, and navigation for harder levels.",
            "uses_memory": true,
            "memory_type": "tip-summary memory / experience-replay-like (compact episodic summaries and aggregated 'general tips')",
            "memory_architecture": "Memory is implemented as short, human-readable tip strings derived from trajectories: (a) per-trial tips generated after failure that note failed actions and concise corrective actions (these are stored and used as prompts on subsequent attempts); (b) expert-derived tips produced by comparing agent failures to expert walkthroughs; (c) multi-game aggregation where final successful tips from many games (48 trajectories from 10 training games in experiments) are fed to another LLM which produces generalized tips. In addition, a naive 'replay' baseline is used where up to 3 full past trajectories are directly included in the prompt (limited by input length). Tips replace or augment raw trajectory history to compress information and enable longer effective memory.",
            "memory_retrieval_mechanism": "Prompt-based retrieval: stored tips (or replayed trajectories) are appended to the LLM prompt at the start of subsequent trials. Retrieval is therefore governed by selection of which tips to include in the prompt (recency/selection heuristics implied: failed actions + latest tips; aggregated final tips used for zero-shot). No learnable retrieval index or similarity search is described; retrieval is explicit by including tips in the prompt.",
            "memory_capacity": "Constrained by LLM context length; authors set a maximum of 3 full trajectories for direct replay in experiments; tips are short so many more trial-derived tips can be aggregated (experiments aggregated 48 trajectories from 10 games to generate final tips). Exact token capacity not given.",
            "what_is_stored_in_memory": "Concise tips (high-level corrective guidance), failed actions and the sequence of actions that led to failures (summaries of past trajectories), expert walkthrough actions (when available), and final successful tips from multiple games (used to generate general tips).",
            "performance_with_memory": "Few-shot setting with tips memory: final cumulative performance reported as success rate &gt; 0.9 and normalized points &gt; 0.95 (averaged across 20 games per difficulty level) in the few-shot experiments; with expert demonstrations the agent achieves zero error (100% success) across all difficulty levels after at most 6 trajectories; zero-shot using general tips (generated from 10 training games / 48 trajectories) achieves performance comparable to SOTA on difficulty levels 0-3 and outperforms SOTA on difficulty levels 3-4.",
            "performance_without_memory": "Pure replay baseline (ingesting up to 3 past trajectories into the prompt) is used as a baseline and performs worse than tips-summary approaches; however the paper does not report a single aggregated numeric baseline (exact normalized points / success rates for pure replay are not specified).",
            "has_ablation_study": true,
            "memory_improvement_magnitude": null,
            "key_findings_about_memory": "Compact tip summaries distilled from past trajectories or expert demonstrations (rather than raw full trajectories) allow GPT-4 to (a) learn faster in few-shot settings, (b) generalize across related games when aggregated into general tips, and (c) achieve competitive or superior performance to SOTA deep RL methods despite using far fewer training trajectories (10 games / 48 trajectories vs SOTA trained on 100 games with 100k episodes). Expert demonstrations further accelerate learning (zero error within ≤6 trials).",
            "memory_limitations": "Input length limits constrain how many full trajectories can be replayed (authors cap replay at 3). The LLM can fail to generate sufficiently general tips for easier-level corner cases, leading to underperformance on some low-difficulty games. The inherent probabilistic (non-deterministic) nature of the LLM sometimes causes it to ignore tips and still make mistakes. No formal mechanism to index/score or retrieve tips beyond prompt inclusion is presented.",
            "comparison_with_other_memory_types": "The paper compares three approaches: direct replay of past trajectories (limited to 3 trajectories by input length), tips-summary from past failed trials, and tips derived from expert demonstrations. Summary: tips-summary outperforms pure replay (no exact numeric delta provided), and expert-demonstration-derived tips deliver the fastest learning (100% success within ≤6 trials). The multi-game aggregated general tips enable strong zero-shot transfer and outperform SOTA RL agents on harder levels.",
            "uuid": "e2911.0",
            "source_info": {
                "paper_title": "Introspective Tips: Large Language Model for In-Context Decision Making",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "ReAct",
            "name_full": "ReAct (Reasoning + Acting with Language Models)",
            "brief_description": "An approach that interleaves chain-of-thought reasoning traces with actions, enabling LLMs to both reason and act in interactive environments, leveraging history of observations and internal reasoning as an implicit memory context.",
            "citation_title": "ReAct: Synergizing reasoning and acting in language models",
            "mention_or_use": "mention",
            "agent_name": "ReAct",
            "agent_description": "ReAct interleaves reasoning (chain-of-thought) and action tokens so that the model produces internal reasoning traces and actions in an alternating manner, using prior turns of interactions as context to inform subsequent actions.",
            "base_llm_model": null,
            "base_llm_size": null,
            "text_game_name": "text-based environments (general; cited as applied to interactive decision-making tasks)",
            "text_game_description": "Described in the paper as methods that enable LLM agents to select actions in text-based environments; ReAct is presented as a general interactive reasoning+action method, not a specific TextWorld instantiation within this paper.",
            "uses_memory": true,
            "memory_type": "reflection-based / context-window (implicit working memory via chain-of-thought + past interaction history)",
            "memory_architecture": "No external memory; memory is the sequence of prior context (observations, reasoning traces, and actions) within the LLM prompt. Historical interactions are kept in the context window and used to generate subsequent reasoning and actions.",
            "memory_retrieval_mechanism": "Recency/context-window-based: past interaction tokens are available in the prompt and influence generation; no explicit retrieval/indexing described in this paper's discussion.",
            "memory_capacity": null,
            "what_is_stored_in_memory": "Past observations, prior actions, and interleaved reasoning traces (chain-of-thought).",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_ablation_study": null,
            "memory_improvement_magnitude": null,
            "key_findings_about_memory": "Listed as prior work that uses historical interactions (dynamic reasoning) to improve decision-making; cited as conceptually related but not experimentally evaluated in this paper's TextWorld experiments.",
            "memory_limitations": "Not discussed in detail in this paper beyond general LLM context-length limits; ReAct relies on prompt history which is constrained by input length.",
            "comparison_with_other_memory_types": null,
            "uuid": "e2911.1",
            "source_info": {
                "paper_title": "Introspective Tips: Large Language Model for In-Context Decision Making",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "Reflexion",
            "name_full": "Reflexion (LLM with self-reflection and dynamic memory)",
            "brief_description": "An approach that equips an LLM agent with a self-reflective module and heuristics to detect hallucinations and inefficient actions, enabling iterative self-correction using past interactions.",
            "citation_title": "Reflexion: an autonomous agent with dynamic memory and self-reflection",
            "mention_or_use": "mention",
            "agent_name": "Reflexion",
            "agent_description": "Reflexion augments an LLM agent with a self-reflective LLM that analyzes past trajectories to generate reflections and corrections; it uses a simple heuristic to detect suboptimal behaviors and to trigger reflection and policy update steps.",
            "base_llm_model": null,
            "base_llm_size": null,
            "text_game_name": "general interactive decision-making tasks (cited as related work; not directly applied in this paper's experiments)",
            "text_game_description": "Referenced as a dynamic-reasoning/self-reflection approach; the paper cites Reflexion conceptually as using historical interactions for self-correction but does not re-run Reflexion in TextWorld experiments.",
            "uses_memory": true,
            "memory_type": "reflection-based / dynamic memory (self-reflections derived from past trajectories)",
            "memory_architecture": "Described in cited work as containing a self-reflective LLM and heuristics to extract reflections from past trajectories; in this paper Reflexion is mentioned at a high level but implementation details are not provided.",
            "memory_retrieval_mechanism": "Prompt/context-based: past trajectories and generated reflections are used as additional prompt context for subsequent action selection; specific retrieval mechanisms are not described here.",
            "memory_capacity": null,
            "what_is_stored_in_memory": "Reflections (self-analyses), past actions, and identified failure patterns (as described by the cited Reflexion work).",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_ablation_study": null,
            "memory_improvement_magnitude": null,
            "key_findings_about_memory": "Cited as prior work that uses dynamic reasoning and self-reflection to improve LLM agent behavior; contrasted with the paper's claim that their \"tips\" are more generalizable across agents/environments than per-agent reflections.",
            "memory_limitations": "Paper notes that reflections can be misled by the agent's own errors and are more specific to individual agent trajectories (less generalizable) — this is a conceptual criticism rather than an empirical limitation reported for Reflexion in this paper.",
            "comparison_with_other_memory_types": "Authors contrast reflections (Reflexion) with tips: reflections are detailed and agent-specific (learn mainly from failures), whereas tips are concise, generalizable guidance that can be shared across agents and derived from many sources including experts.",
            "uuid": "e2911.2",
            "source_info": {
                "paper_title": "Introspective Tips: Large Language Model for In-Context Decision Making",
                "publication_date_yy_mm": "2023-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Reflexion: an autonomous agent with dynamic memory and self-reflection",
            "rating": 2,
            "sanitized_title": "reflexion_an_autonomous_agent_with_dynamic_memory_and_selfreflection"
        },
        {
            "paper_title": "ReAct: Synergizing reasoning and acting in language models",
            "rating": 2,
            "sanitized_title": "react_synergizing_reasoning_and_acting_in_language_models"
        },
        {
            "paper_title": "Learning dynamic belief graphs to generalize on text-based games",
            "rating": 2,
            "sanitized_title": "learning_dynamic_belief_graphs_to_generalize_on_textbased_games"
        },
        {
            "paper_title": "Learning to follow instructions in text-based games",
            "rating": 2,
            "sanitized_title": "learning_to_follow_instructions_in_textbased_games"
        }
    ],
    "cost": 0.013067999999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Introspective Tips: Large Language Model for In-Context Decision Making
19 May 2023</p>
<p>Liting Chen 
Lu Wang 
Hang Dong 
Yali Du 
Kings College London</p>
<p>Jie Yan 
Fangkai Yang 
Shuang Li 
The Chinese University of Hong Kong (Shenzhen)</p>
<p>Pu Zhao 
Si Qin 
Saravan Rajmohan 
Qingwei Lin 
Dongmei Zhang 
Introspective Tips: Large Language Model for In-Context Decision Making
19 May 20235911203F2C4BB200C05B5BA232D524C6arXiv:2305.11598v1[cs.AI]
The emergence of large language models (LLMs) has substantially influenced natural language processing, demonstrating exceptional results across various tasks.In this study, we employ "Introspective Tips" to facilitate LLMs in self-optimizing their decision-making.By introspectively examining trajectories, LLM refines its policy by generating succinct and valuable tips.Our method enhances the agent's performance in both few-shot and zeroshot learning situations by considering three essential scenarios: learning from the agent's past experiences, integrating expert demonstrations, and generalizing across diverse games.Importantly, we accomplish these improvements without fine-tuning the LLM parameters; rather, we adjust the prompt to generalize insights from the three aforementioned situations.Our framework not only supports but also emphasizes the advantage of employing LLM in in-contxt decision-making.Experiments involving over 100 games in TextWorld illustrate the superior performance of our approach.</p>
<p>Introduction</p>
<p>Large Language Models (LLMs), including OpenAI's GPT-3.5 (Ouyang et al., 2022), GPT-4 (OpenAI, 2023), Google's PaLM (Chowdhery et al., 2022) and other benchmarks (Meta, 2023;Taori et al., 2023) have consistently achieved remarkable performance across various NLP tasks.The integration of LLMs into decision-making tasks (Huang et al., 2022a;Ahn et al., 2022;Kwon et al., 2023;Brooks et al., 2022;Yao et al., 2022) has garnered significant attention, as it presents an opportunity to develop decision-making agents that can emulate human-like cognitive processes (Shevlin et al., 2019).In decision-making tasks, such as in the domain of Reinforcement Learning (RL), limited interactions hinder optimal policy learning (Yarats et al., 2021).Owing to the power of LLMs, which have undergone extensive pre-training on vast amounts of data, agents can leverage the generalization capabilities of LLMs to enhance their performance across different tasks.In addition, common sense knowledge is a valuable asset in safety decision-making tasks (Brunke et al., 2022), and LLMs possess a wealth of this knowledge due to their extensive training and tuned with human feedback (Ouyang et al., 2022).By leveraging the inherent common sense knowledge within LLMs, decision-making agents can make more informed decisions, effectively tackling challenges such as sparse rewards and enhancing the learning process.</p>
<p>However, LLMs sometimes demonstrate errors or hallucinations (Ji et al., 2023;Peng et al., 2023), especially in domain-specific scenarios.Recent works focus on designing selfcorrection mechanisms to enhance the LLM's decision-making performance.For example, Chain-of-thought (CoT) (Wei et al., 2022) concentrates on static reasoning for one-step actions without self-correction.ReAct (Yao et al., 2022) and Reflexion (Shinn et al., 2023) focus on dynamic reasoning based on historical interactions to enable better decision-making performance.However, these works primarily focus on generating instructions or reflections specific to individual agents, hindering the adoption of the generalization and universality abilities of LLMs.Furthermore, the absence of correction for the generated reflections can lead to errors during the decision-making process.</p>
<p>Humans possess high generalization and self-evolution capabilities due to their natural ability to summarize tips through introspection (Cox, 1996;Van Merrienboer and Sweller, 2005).Introspection enables humans to reflect on their past experiences (successes or failures), distill key insights, and extract valuable lessons that can be applied to new situations.Inspired by the remarkable introspection capabilities of humans, we introduce "Introspective Tips" (or "Tips" for short) as a novel prompt-based paradigm, empowering an LLM decision-making agent with self-optimizing capabilities via learning tips from trajectories of itself, experts, and other environments.Figure 1 provides an example of introspective tips in a text-based game.</p>
<p>Figure 1: In both Game 1 and Game 2, the agent is tasked with checking the cookbook and cooking a meal.The cookbooks for each game are different, but the initial state remains the same.After playing in Game 1, the agent is asked to participate in Game 2, using tips or reflections generated from the Game 1 trajectory.The agent's reflection focuses on the errors made in Game 1 and is specific to that game.Tips, on the other hand, offer more general and high-level information about the game.Utilizing tips from Game 1, the agent learns that the stove is for frying ingredients and uses the oven to roast the apple.The agent subsequently updates its knowledge about the game by incorporating a third tip.However, when using reflections, the agent receives an irrelevant message about potatoes, which leads to its failure to check the cookbook and complete the task in Game 2. In this case, the general tips prove to be more helpful in guiding the agent's behavior and improving its performance, whereas the specific reflections from Game 1 do not provide useful information for Game 2.</p>
<p>Distinct from reflection (Shinn et al., 2023), which is a self-analysis process that delves into an agent's actions and experiences in detail, "tips" are compact pieces of information offering high-level guidance or suggestions.The goal of tips is to provide succinct and practical advice for enhancing decision-making performance without requiring an in-depth examination of past trajectories.In contrast, reflections can only be derived from the agent itself and primarily concentrate on learning from failures.Tips, on the other hand, can be acquired from the trajectories of other agents and human experts, focusing on learning from both successes and failures.For example, in the game setting, the agent can generalize tips to correctly use oven based on its successful trajectory, but reflection may focus more on failures.</p>
<p>Additionally, in contrast to using original trajectories as prompts, introspective tips serve as condensed and comprehensive insights extracted from lengthy trajectories.This allows LLMs to overcome the challenges in discerning the intricate relationship between dependent actions and sparse rewards across multiple trajectories (Adhikari et al., 2020).Introspective tips provide distinct advantages in terms of their applicability and generalization across different agents and contexts, as opposed to previous prompting methods such as reflections.Unlike reflections that target specific agents, tips can be shared among different agents, allowing for higher generalization performance.In the game setting of Figure 1, the tips generalized by the agent can be applied to various game settings with different recipes.In contrast, reflections are more specific to a single past trajectory.</p>
<p>Furthermore, designing suitable prompts for LLMs to generate introspective tips is a crucial challenge; manually crafting prompts can be burdensome.Therefore we propose a framework that dynamically adjusts the prompt based on insights derived from past trajectories or expert demonstrations through introspection.</p>
<p>Our contributions can be summarized as follows:</p>
<p>• Introspective Tips for Self-Optimizing Decision-Making: We introduce Introspective Tips as a novel prompt-based paradigm, empowering LLM decision-making agents with self-optimizing capabilities for both few-shot and zero-shot scenarios.Unlike previous RL agents, Introspective Tips offers an end-to-end solution without the need for training or fine-tuning.It generates meaningful and easy-to-understand tips, facilitating human understanding and intervention in the decision-making process.</p>
<p>• Multiple Strategies in different scenarios for Learning Introspective Tips: We present distinct strategies applicable in different scenarios to prompt LLMs to learn Introspective Tips from their own trajectories, expert demonstrations, and multi-environment trajectories, which also shows the generalization and adaptability of Introspective Tips.</p>
<p>• A Dynamic Prompt Adjustment Framework: To simplify the prompting engineering process, we introduce a framework that dynamically adjusts the prompt by leveraging insights from past trajectories or expert demonstrations through introspection.This framework streamlines the improvement process, enhances the model's adaptability, and enables more efficient decision-making.</p>
<p>• Comprehensive Evaluation and Comparison: Our extensive experiments encompass over 100 games in TextWorld (Adhikari et al., 2020;Côté et al., 2019), testing few-shot and zero-shot learning scenarios.We evaluate the performance of our decisionmaking agent against state-of-the-art methods in the field (Tuli et al., 2022), highlighting the effectiveness and superiority of Introspective Tips.With tips generated from 48 trajectories, LLM agent performs better than previous deep learning method training for 100, 000 episodes in the unseen game with the highest difficulty level.</p>
<p>Related work</p>
<p>Language model for decision making LLMs (OpenAI, 2023;Chowdhery et al., 2022) have exhibited impressive proficiencies, facilitating their use in tasks beyond mere language generation and increasingly serving as policy models for decision-making in interactive settings (Yang et al., 2023).Wei et al. (2022) demonstrates that incorporating a series of intermediate reasoning steps can enhance decision-making abilities.Yao et al. (2022) introduces ReAct, a method for interleaved reasoning and action generation that fosters improved synergy between language comprehension and interactive decision-making tasks.Shinn et al. (2023) presents Reflexion, a technique that equips an LLM-based agent with a self-reflective LLM and a straightforward heuristic for detecting hallucination and inefficient action execution to examine the hypothesis.Madaan et al. (2023) adopts a similar strategy, enabling an LLM to offer feedback on its previously generated text and refine it to meet specific requirements.When regarded as dialogue agents, LLMs can also be trained to learn from human feedback and optimize their output (Ouyang et al., 2022;Bai et al., 2022).With further training, Li et al. (2022) constructs a general framework for decision-making tasks using pre-trained LMs, even in scenarios where language is neither provided as input nor output.Other studies (Singh et al., 2022;Huang et al., 2022a,b;Liang et al., 2022;Vemprala et al., 2023) have explored innovative strategies involving prompt engineering and the utilization of high-level function libraries to enhance the capabilities of LLMs.</p>
<p>Recent attempts explored different aspects of LLMs for decision-making.Huang et al. (2022a) and Ahn et al. (2022) use LLMs to generate plans or sub-goals that guide low-level Reinforcement Learning (RL) agents in taking actions.Kwon et al. (2023) utilize LLMs as proxy reward functions by prompting them with desired behaviors.Yao et al. (2022) focus on enabling LLM agents to select actions in text-based environments.In addition, a recent approach considers LLMs as world models (Brooks et al., 2022), where the agent learns the policy by interacting with the LLM-based world model.In this paper, we focus on directly grounding LLM in decision-making to take action because the other three methods required learning an extra decision-making agent, which required more samples.</p>
<p>In-context reinforcement learning In-context learning pertains to the ability of sequence prediction models to adapt to novel downstream tasks solely through the use of prompts without retraining or fine-tuning (Lu et al., 2021;Brown et al., 2020;Min et al., 2022).When applied to reinforcement learning, in-context learning models can generalize to diverse downstream tasks when provided with contexts such as demonstrations and task information.Laskin et al. (2022) recasts RL as an across-episode sequential prediction problem, and trains a causal transformer to autoregressively predict actions based on preceding learning histories as context.Brooks et al. (2022) employs the LLM as a world model for planning future trajectories and executing decisions in-context.Team et al. (2023) develops a versatile in-context learning algorithm capable of adapting to new and open-ended 3D challenging problems as rapidly as humans, by training an RL agent at a large scale.Lu et al. (2023) proposes to meta-learn across random linear projections of the observation and action spaces of randomly sampled DMControl tasks (Tassa et al., 2018).Trained on an extensive dataset, Gato (Reed et al., 2022) can generalize to new tasks by conditioning on demonstrations of the desired behavior.</p>
<p>Text-based game Text-based games are typically turn-based experiences played via a command line terminal.During each turn, the game state is conveyed through multiple lines of text, which enables players to input text commands that modify the state according to their preferences (Liu et al., 2022;Hendrycks et al., 2021;Osborne et al., 2022).Text-based games can be formally characterized as partially observable Markov decision processes (POMDPs) (Côté et al., 2019), considering that the agent only observes partial information about the environment at each turn.Intrinsic obstacles such as longterm dependencies, partial observation of current states, and sparse rewards, and complex action combinations render these games particularly challenging.Various deep learning approaches have been employed to address text-based games (Xu et al., 2022;Yin and May, 2019;Ammanabrolu and Hausknecht, 2020;Kimura et al., 2021).Focusing on tasks in the TextWorld domain (Côté et al., 2019), Adhikari et al. (2020) explores learning graphstructured state representations via a data-driven approach, introducing the Graph Aided Transformer Agent (GATA) that learns to construct and update graph-structured beliefs while optimizing rewards.Building upon their work, Tuli et al. (2022) equips GATA with an internal structured representation of natural language instructions using Linear Temporal Logic (LTL) to enhance the instruction-following capabilities of text-based game agents.</p>
<p>Method</p>
<p>In this section, we elaborate on the method employed to leverage the capabilities of LLMs in sequential decision-making tasks, particularly text-based games, by addressing LLMs inherent limitations and capitalizing on their strengths.LLMs' proficiency in understanding and generating human-like text renders them promising candidates for tasks involving natural language processing, such as text-based games.However, they lack specific domain knowledge when asked to make decisions in certain tasks thus may not reach the greatest potential when used directly.</p>
<p>We conjecture that the function space of generative LLMs P (θ) is sufficiently expansive to encompass the function of an expert policy in text-based games.The action space of policy π is constrained by text output.By selecting an appropriate prompt pr containing enough domain knowledge, we can derive a model P (θ|pr) that can function as a π in decision making, bridging the gap between LLMs and reinforcement learning.Through incorporating strategies including learning from past experiences, expert demonstrations, and multiple games, we aim to develop a versatile and robust framework that excels in a wide range of gaming scenarios.This approach ultimately opens up the journey of discovering the potential of LLMs in the domain of complex sequential decision-making tasks.</p>
<p>Challenges and Solutions for LLMs in Decision-Making Problems</p>
<p>There are several major challenges when applying Language Learning Models (LLMs) to solve decision-making problems, including self-optimization, input length limitations, and prompt dependence.In this section, we discuss these challenges and potential solutions.</p>
<p>Self-Optimizing decision-making LLMs can exhibit errors or hallucinations, particularly in domain-specific scenarios.Developing self-optimizing mechanisms to enable LLMs to correct their mistakes is essential for improving their decision-making performance.Moreover, allowing LLMs to self-optimize across various scenarios poses a significant challenge.Current approaches, such as self-reflection, attempt to rectify errors by concentrating on the model's own mistakes.However, this method has limitations, including the risk of LLMs becoming misled by their own errors during reflection and lacking a comprehensive understanding of the task or environment.To address these challenges, we employ "Introspective Tips" to facilitate LLMs in self-optimizing their decision-making.We empower LLMs to learn from their own trajectories, expert demonstrations, and trajectories in different environments.By extracting concise and actionable advice from these trajectories, the agent gains a better understanding of the environment.Furthermore, by refining its own tips, the problem of hallucination is alleviated.</p>
<p>Limited input length Language models like GPT-4 (OpenAI, 2023) and PaLM (Chowdhery et al., 2022) often encounter input length limitations.In RL, agents typically face lengthy and complex trajectories, as well as sparse rewards that offer feedback only after numerous steps or interactions.Due to the restricted input capacity and extended trajectories, LLMs struggle to discern the relationship between actions and rewards, ultimately impeding the acquisition of the optimal policy.To address these issues and optimize the available input space, we use tips that effectively condense the training dataset or critical information generally found in classical RL settings.By incorporating these summaries, LLMs can better understand the relationships within the data and identify essential patterns that might be otherwise obscured by the sheer length or complexity of the trajectories.Moreover, providing concise, relevant information enables LLMs to focus on the crucial aspects of the problem at hand, potentially leading to more accurate results.This strategy can be particularly beneficial in scenarios where the agent must learn from limited data or adapt to changing environments, as the distilled information can help guide the learning process more efficiently.</p>
<p>Prompt dependence: The LLM's performance on a specific problem heavily relies on the quality and order of prompts.In order to solve the sequential decision problem in text-based games, that is to produce more relevant and valuable responses in each round, it is crucial to craft prompts in decision making.However, creating prompts using human interaction can be time-consuming and tedious.To address this issue, our method introduces a framework that dynamically adjusts the prompt based on past trajectories through introspection, streamlining the process and improving the model's adaptability.While designing a specific tip for one game can be laborious, we also explore the possibility of generating a universal prompt that can transform the LLM into an efficient RL agent across various games and contexts.This general prompt would further facilitate the LLM's ability to adapt and perform effectively in a wide range of decision-making situations.</p>
<p>Basic setting</p>
<p>In our basic setting (Fig 2a ), we directly utilize an LLM as an agent participating in a text-based game.A sample interaction proceeds as follows: Agent: inventory( )The system (user) clearly defines the role and action space for the LLM agent and provides an initial text-based description of the environment, setting the scene for the agent.The LLM-agent interprets the provided information and decides on an action based on its understanding of the game's context.The agent submits its chosen action as text, which the system interprets, processes, updates the environment, and provides new feedback, including information on invalid actions or a description of the updated state, to the agent.The LLMagent continues to interact with the game, choosing actions and receiving feedback, until the game reaches its conclusion.The game concludes as a success if the agent completes all required steps, or as a failure if the agent takes erroneous actions or reaches the maximum number of turns.Throughout the entire process, the agent's goal is to navigate the game world and make decisions based on textual input and output.</p>
<p>Strategies for three scenarios</p>
<p>We have developed strategies for three scenarios to enhance the agent's learning addressing the two challenges:</p>
<p>Learning from its own history: The agent employs a history replay method to generate tips for self-improvement.The agent is prompted as the basic setting to finish a basic trial.If the agent fails, the agent creates several tips to address its past mistakes.In subsequent attempts, these tips serve as prompts.If the agent fails even if provided with tips, it is prompted to generate more effective tips for itself.The agent is also provided with all its previous actions that led to failure in the game.By introspectively analyzing its past actions and their outcomes, the agent generates valuable insights to refine its policy.This self-enhancement process enables the agent to overcome obstacles and make better choices in future gameplay.Moreover, since tips and incorrect actions are shorter than an entire trajectory, the agent can learn from a more extensive history than relying solely on past experiences as memory (Fig 2b).</p>
<p>Learning from expert demonstrations: The agent evaluates its performance by comparing it to expert demonstrations and makes adjustments as necessary.In this setting, the agent is instructed to observe both the actions leading the expert agent to achieve the goal and the actions causing it to fail the game.By examining successful strategies and tactics employed by experts, the agent can pinpoint areas for improvement and generat tips that can help modify its actions accordingly.This learning approach is faster compared to a purely trial-and-error method.For instance, the agent can learn the correct usage of an appliance by contrasting its failed attempts with the successful actions demonstrated by an expert in a text-based game, rather than experimenting with all possible actions.</p>
<p>The agent generates tips based on these observations, and these tips serve as new prompts when playing subsequent rounds.If the agent fails even when provided with tips, it is prompted to reflect on the given advice.This method enables knowledge transfer from expert demonstrations, resulting in a more efficient and informed decision-making process.Feedback from the environment acts as guidance for the LLM model, assisting it in determining the accuracy and effectiveness of its generated tips (Fig 2c).</p>
<p>Learning from multiple games: In this scenario, agents are allowed to participate in several games.All final tips (tips that lead to a successful trial) generated across these games are sent to another LLM, which is then tasked with generating general tips to help the agent become an expert across various game settings.In this context, the LLM leverages its knowledge of summarization to produce comprehensive and valuable tips that enable the agent to improve its performance in a wide range of games.These games share some similarities but with some information different.For example, in the cooking game, these games share a similar theme (cooking in a modern house), similar text commands, and similar entities (i.e., interactable objects within the games), but with different cookbooks and maps involved in the game.The LLM generalizes knowledge across diverse games, functioning similarly to a Meta-RL agent.The general tips are served as prompts in the unseen test game.This ability allows the LLM to perform effectively in a broad range of text-based games, positioning it as a powerful tool for reinforcement learning in complex environments (see Fig 2d).</p>
<p>By incorporating these strategies in the three scenarios, our method presents a robust framework for exploiting the potential of LLMs in reinforcement learning for text-based games.Learning from past experiences, expert demonstrations, and multiple games allows the agent to adapt and excel in various gaming scenarios, ultimately demonstrating the versatility and effectiveness of LLMs in the realm of reinforcement learning.</p>
<p>Experimental Results</p>
<p>We conduct experiments on text-based games to assess the performance of LLMs as agents and address three key questions: Q1: Can LLMs learn from their historical trajectories and improve their performance by reflecting on different tips?Q2: Can LLM learn from expert Figure 3: Few shot performance over different difficulty levels of games demonstrations and can expert demonstration boost the learning of different strategies?Q3: Can LLMs generalize some tips to play in various game settings?For Q1 and Q2, we analyze the few-shot decision-making capabilities of LLMs, while for Q3, we concentrate on their zero-shot decision-making abilities.</p>
<p>Experiment setup</p>
<p>Table 1: Statistics of The Game</p>
<p>Level #Ingredients #Locations Points (Cook, Cut, Open )
0 1 1 3 (×, ×, ) 1 1 1 4 (×, , ) 2 1 1 5 ( , , ) 3 1 9 3 (×, ×, ) 4 3 6 11 ( , , )
We concentrate on the TextWorld Cooking domain, which gained prominence through GATA (Adhikari et al., 2020) and Microsoft's TextWorld Problems: A Language and Reinforcement Learning Challenge (Trischler et al., 2019).Following Adhikari et al. (2020) and Tuli et al. (2022), we divide the games into five subsets with different difficulty levels.For easier games (with smaller difficulty levels), the recipe requires fewer ingredients, and the agent does not need to navigate through the world.For harder games, the agent is asked to navigate through different rooms to acquire various ingredients and cook the meal.The score is related to the relevant steps required in the cookbook of each game.If the agent completes a required step, it will earn one point.For the hardest game, the agent is required to finish 11 steps to complete this task.These steps involve opening certain containers to obtain the ingredients, cutting the ingredients as required (dice, slice, chop), and cooking the ingredient using the correct heat source (oven or stove).Statistics of the games are presented in Table 1.In the dataset, expert demonstrations are provided in the form of walkthroughs and do not require human intervention for generation.Following previous work, we measure the performance of the algorithms using two metrics: normalized game points and game success rate.We test the game in 20 different games for each difficulty level.By averaging the points over the 20 games, and then dividing by the maximum score an agent can earn, we obtain the normalized game points.The game success rate is calculated as the percentage of games in which the agent successfully completes all the required steps.We use GPT-4 as our base LLM.</p>
<p>Few-shot performance</p>
<p>We first test the few-shot performance of our method.We make a comparison between purely replay, tips summary using past history, and tips summary compared to expert trajectory.The latter two cases corresponds to our first and second scenario.For purely replay, we directly ingest the past trajectory as LLM's prompts to see if it can learn.Given that the input length is limited, we set the maximum number of trajectories as 3 in the experiments.For tips summary memory, the LLM agent is provided with the past failed actions and the tips generated from the last trial.We observe that with tips memory, the agent can collect more information using different trials and cumulatively achieve a high success rate (larger than 0.9) and high points earned (larger than 0.95) in the end, as shown in Figure 3.With expert demonstration, the agent can learn faster and achieve zero error in all difficulty levels after at most 6 trajectories.This comparison shows that concise tip summary assists the LLM in gathering information and making better decisions.With expert demonstrations, the LLM can learn more efficiently compared to solely relying on trial and error.We further show an example about how tip summary can help the agent make better decision.In this log file (showing only relevant information), the agent is tasked with roasting a purple potato using a predefined ActionList in a text-based game.In the first attempt, the agent mistakenly uses the stove to cook the purple potato, resulting in frying it and losing the game.The agent then generates a tip to use the "roast" action.However, this is an invalid action, and it fails again.Next, the agent generates a second tip to use the "cook" command with the stove, suggesting a different heat setting or method to avoid frying.Unfortunately, in this text-based game, there are no commands to change the heat setting, leading to another failure.After the third attempt, the agent realizes it should try using an oven to roast the potato, and finally succeeds in the fourth trial.This demonstrates that improved tips can lead to successful learning.</p>
<p>Zero-shot Performance</p>
<p>First trial: &gt;agent: cook purple potato with stove system: You fried the purple potato.Tips to win the game next time: 1.You should try roast the potato next time instead of cook purple potato with stove after dicing the purple potato;</p>
<p>Second trial: &gt;agent: roast the potato system: Invalid action.&gt;agent: cook purple potato with stove system: You fried the purple potato.Tips to win the game next time: 1.You should try cook purple potato with stove next time after you have dicing the purple potato, but make sure to use a different heat setting or method to avoid frying the purple potato; Third trial: &gt;agent: cook purple potato with stove system: You fried the purple potato.Tips to win the game next time: 1.You should try cook purple potato with oven next time instead of cook purple potato with stove, as the recipe suggests roasting the purple potato rather than frying it;</p>
<p>Forth trial: &gt;agent: cook purple potato with oven system: You roasted the purple potato.Your score has just gone up by one point.</p>
<p>In this experiment, we focus on training LLM agents using tips generated from the training set.We collect final tips, which are those leading to successful trials, from difficulty level 4 (the most challenging level) games in the training set.These tips are then sent to another LLM, which generates tips aimed at helping the agent become proficient across various game settings.The final tips can be found in the appendix.We used 10 games, which consist of 48 trajectories in total (averaging 4.8 trajectories per game) to generate the final tip.We then test these tips on an unseen test set across different difficulty levels.To evaluate our LLM model with general tips as prompts, we compare it to state-of-the-art (SOTA) text-based game agents utilizing deep learning techniques, including TDQN (Adhikari et al., 2020), GATA (Adhikari et al., 2020), and ITL (Tuli et al., 2022).These models are trained on 100 different games for each difficulty level, with 100,000 episodes per level.Results for levels 0 to 3 are provided by the authors of (Tuli et al., 2022), while level 4 results are obtained from Adhikari et al. (2020).</p>
<p>As shown in Figure 4, we prompt the LLM with general tips and test it on levels 1 to 4. Given level 0's simplicity, we do not provide tips for this level.Our findings reveal that the LLM with tips, using only 10 games and 48 trajectories, achieves performance comparable to SOTA methods for difficulty levels 0-3.For levels 3-4, LLM outperforms other methods, due to the reasoning ability of the LLM that allows us to bypass the need to learn navigation and the correct tips generated in the same difficulty level, an aspect that hinders SOTA methods (Adhikari et al., 2020).</p>
<p>Limitation Despite these achievements, the LLM agent underperforms in some lower difficulty level games.This underperformance can be attributed to two factors.First, the LLM fails to generate a general tip that addresses specific situations encountered in easier levels.For instance, in difficulty level 1, when an ingredient is already in its desired state according to the recipe (e.g., roasted or fried), the corresponding actions (roasting or frying) become unnecessary.By incorporating human-generated tips (see the appendix for the full list of tips), the LLM agent can achieve significantly higher points and success rates, as demonstrated in Table 2. Second, the LLM's probabilistic nature leads to non-deterministic outputs, which occasionally cause the agent to disregard the tips and execute erroneous actions.Despite being provided with tips, the LLM agents will still make some mistakes due to their inherent probabilistic behavior.Nevertheless, even with this randomness, the LLM agent with general tips can outperform state-of-the-art (SOTA) agents specifically trained to excel in this task.</p>
<p>Conclusion</p>
<p>We introduce the novel concept of "Introspective Tips" as a powerful mechanism to improve the decision-making capabilities of LLM agents.Drawing inspiration from human introspection, this approach enables agents to extract and learn from generalized, high-level information that can be applied across various tasks and contexts.To effectively implement Introspective Tips, we propose a framework that dynamically adjusts prompts based on insights derived from past trajectories or expert demonstrations through introspection.This approach alleviates the burden of manual prompt crafting while empowering LLM agents with self-optimizing capabilities.By leveraging the rich common sense knowledge and generalization abilities of LLMs, our Introspective Tips paradigm outperforms SOTA methods in text-based game.Future work could focus on refining the framework for prompt generation, exploring more sophisticated methods for extracting tips from trajectories, and evaluating the effectiveness of introspective tips in a broader range of tasks and real-world applications.</p>
<p>• 3. Pay attention to the cooking methods mentioned in the recipe, such as roasting or frying, and use the appropriate appliances like the oven and stove.To operate an appliance, use actions from the ActionList, such as "cook(food, heat source)".</p>
<p>• 4. Use the correct actions from the ActionList to prepare the ingredients, such as "slice(food, sharp object)", "dice(food, sharp object)", or "chop(food, sharp object)".</p>
<p>• 5. Keep track of your inventory and the state of each item to avoid making mistakes in the preparation process.Use the "inventory()" action to check your inventory.</p>
<p>• 6. Follow the steps in the recipe in the correct order, ensuring each ingredient is prepared as instructed.</p>
<p>• 7. Once all ingredients are prepared according to the recipe, use the "prepare meal()" action to combine them into a meal.</p>
<p>• 8.If you encounter any difficulties or errors, refer back to the ActionList and the environment description to make sure you're using the correct actions and items.• 2 Before cooking an ingredient, always use inventory() first to check current state of the ingredient.If an ingredient is already in the desired state(roasted or fried) as per the recipe, there is no need to perform the actions (roast or fry) mentioned in the recipe.</p>
<p>• 3 Try take(food) before you prepare(meal).After you take(food), use inventory() to check the status of the food.</p>
<p>• 4 Make sure the food is in your inventory before you prepare(meal).Try eat(meal) when the meal is prepared.</p>
<p>• 5 Dice, chop, and slice are different.</p>
<p>• 6 Fry and Roast are different.Use correct heat source to cook the ingredient.Oven is for roasting something, not frying something.</p>
<p>• 7 Use cook(food, oven) to roast something, instead of putting the food into the oven and wait.</p>
<p>• 8 Be careful when you cook(food, stove) or cook(food, oven), make sure the food is not fried or roasted before.Otherwise you may burn the food and lose the game.slice the red potato roast the red potato dice the white onion fry the white onion slice the yellow bell pepper fry the yellow bell pepper prepare meal &gt;agent : take(red potato) Env: You take the red potato from the counter.Your score has just gone up by one point.&gt;agent : take(cutting tool) Env: You take the knife from the counter.&gt;agent : slice(red potato, cutting tool) Env: You slice the red potato.Your score has just gone up by one point.&gt;agent : cook(red potato, oven) Env: You roasted the red potato.Your score has just gone up by one point.&gt;agent: go(south) Env:</p>
<p>-= Corridor =-You are in a corridor.An usual kind of place.I guess you better just go and list everything you see here.</p>
<p>There is an exit to the east.There is an exit to the north.There is an exit to the west.Don't worry, there is no door.&gt;agent : go(west) Env:</p>
<p>-= Bathroom =-You find yourself in a bathroom.An ordinary kind of place.You make out a toilet.But oh no! there's nothing on this piece of garbage.</p>
<p>You need an exit without a door?You should try going east.&gt;agent : go(east) Env: -= Corridor =-You are in a corridor.An usual kind of place.I guess you better just go and list everything you see here.</p>
<p>There is an exit to the east.There is an exit to the north.There is an exit to the west.Don't worry, there is no door.&gt;agent : go(east) Env:</p>
<p>-= Bedroom =-You are in a bedroom.An ordinary kind of place.You smell an intriguing smell, and follow it to a bed.Unfortunately, there isn't a thing on it.You move on, clearly infuriated by TextWorld.You don't like doors?Why not try going west, that entranceway is not blocked by one.&gt;agent : go(west) Env:</p>
<p>-= Corridor =-You are in a corridor.An usual kind of place.I guess you better just go and list everything you see here.</p>
<p>There is an exit to the east.There is an exit to the north.There is an exit to the west.Don't worry, there is no door.&gt;agent : go(north) Env:</p>
<p>-= kitchen =-You find yourself in a kitchen.A standard kind of place.You can see a closed fridge, which looks conventional, nearby.You can see a closed oven.What's that over there?It looks like it's a table.The table is massive.On the table you can make out a cookbook.You rest your hand against a wall, but you miss the wall and fall onto a counter!On the counter you make out a red apple, a raw purple potato, a red hot pepper, a raw yellow potato and a banana.You see a stove.The stove is conventional.However, the stove, like an empty stove, has nothing on it.There is a closed plain door leading north.You don't like doors?Why not try going south, that entranceway is not blocked by one.There is an exit to the west.&gt;agent : open(fridge) Env: You open the fridge, revealing a yellow bell pepper, a white onion, a carrot, a red onion, an orange bell pepper, a block of cheese and a raw pork chop.&gt;agent : take(white onion) Env: You take the white onion from the fridge.Your score has just gone up by one point.&gt;agent : take(yellow bell pepper) Env: You take the yellow bell pepper from the fridge.Your score has just gone up by one point.</p>
<p>Figure 2 :
2
Figure 2: LLM as RL agent</p>
<p>Figure 4 :
4
Figure 4: Performance of LLM as a text-based game agent compared to SOTA methods.Given that experimental results of difficulty level 4 is not included in Tuli et al. (2022), we obtain data from Adhikari et al. (2020) and some data is missing.</p>
<p>B. 2
2
Human-generated tips • 1 If you get stuck, use the actions goal(), examine(cookbook), inventory() and look() to figure out what you have and what you need to do.</p>
<p>Table 2 :
2
Performance of the LLM agent with human-generated tips across different difficulty levels.
Level 0Level 1Level 2Level 3Level 4Points Suc. Rate Points Suc. Rate Points Suc. Rate Points Suc. Rate Points Suc. Rate110.880.800.920.900.960.950.960.95</p>
<p>You can see a closed fridge, which looks conventional, nearby.You can see a closed oven.What's that over there?It looks like it's a table.The table is massive.On the table you can make out a cookbook.You rest your hand against a wall, but you miss the wall and fall onto a counter!On the counter you make out a raw red potato, a red apple, a raw purple potato, a red hot pepper, a raw yellow potato, a banana and a knife.You see a stove.The stove is conventional.However, the stove, like an empty stove, has nothing on it.There is a closed plain door leading north.You don't like doors?Why not try going south, that entranceway is not blocked by one.There is an exit to the west.&gt;agent: examine(cookbook) Env: You open the copy of "Cooking: A Modern Approach (3rd Ed.)" and start reading: Recipe #1 ---Gather all following ingredients and follow the directions to prepare this tasty meal.
Ingredients:red potatowhite onionyellow bell pepperDirections:
• take(item) # take an item that is on the floor • put(item, supporter) # place an item on a supporter• take from(item, container) # take an item from a container or a supporter• insert(item, container) # place an item into a container• lock(item, key) # lock a door or a container with a key• unlock(item, key) # unlock a door or a container with a key• cook(food, heat source) # cook cookable food with something providing heat • slice(food, sharp object) # slice cuttable food with something sharp• chop(food, sharp object) # chop cuttable food with something sharp• dice(food, sharp object) # dice cuttable food with something sharp• prepare meal() # combine ingredients from inventory into a mealAppendix B. Gnenral TipsB.1 Tips generated using 10 games • 1. Always double-check the recipe before performing any actions, and follow the instructions closely.• 2. Make sure to gather all required ingredients and tools before starting to cook.Use the "examine(item)" and "take(item)" actions to interact with items in the environment.Appendix C. PromptsPrompts for generating the tips: Generating tips for the first trial Now you failed the game.Come up with tips for yourself to win the game next time.Pay attention to steps that make you lose the game.Pay attention to those actions and come up with concise actions in the ActionList that you should try next time.Starting your response with "Tips to win the game next time:"Generating tips for other trial Now you failed the game.Come up with tips for yourself to win the game next time.Pay attention to steps that make you lose the game.Make a note on those actions and come up with concise actions in the ActionList that you should try next time.Correct and improve the previous tips that I gave you if you think some of them are wrong.Generating general tips I want you to provide me all possible specific reasons that an agent may fail in a textbased game and summarize all tips for the agent to improve.I will present you several reasons and tips that summarized by others before.In every file, the action that make an agent failed before and some tips to correct this error are provided.Noticed that the tips provided may not be correct.For example, some actions suggested by the tips are not in the ActionList.Correct this kind of error in the tips that you generate.This tips will be used by another rookie agent.So make sure to provide all necessary and userful information for the rookie agent.You should also include action in the ActionList that the agent should use to operate appliance.Appendix D. Sample TrajectoryEnv: You are hungry!Let's cook a delicious meal.Check the cookbook in the kitchen for the recipe.Once done, enjoy your meal! -= Bedroom =-You are in a bedroom.An ordinary kind of place.You smell an intriguing smell, and follow it to a bed.Unfortunately, there isn't a thing on it.You move on, clearly infuriated by TextWorld.You don't like doors?Why not try going west, that entranceway is not blocked by one.&gt;agent: go(west) Env:-= Corridor =-You are in a corridor.An usual kind of place.I guess you better just go and list everything you see here.There is an exit to the east.There is an exit to the north.There is an exit to the west.Don't worry, there is no door.&gt;agent : go(north) Env:-= Kitchen =-You find yourself in a kitchen.A standard kind of place.
Learning dynamic belief graphs to generalize on text-based games. A Adhikari, X Yuan, M.-A Côté, M Zelinka, M.-A Rondeau, R Laroche, P Poupart, J Tang, A Trischler, W Hamilton, Advances in Neural Information Processing Systems. 202033</p>
<p>M Ahn, A Brohan, N Brown, Y Chebotar, O Cortes, B David, C Finn, K Gopalakrishnan, K Hausman, A Herzog, arXiv:2204.01691Do as i can, not as i say: Grounding language in robotic affordances. 2022arXiv preprint</p>
<p>Graph constrained reinforcement learning for natural language action spaces. P Ammanabrolu, M Hausknecht, arXiv:2001.088372020arXiv preprint</p>
<p>Training a helpful and harmless assistant with reinforcement learning from human feedback. Y Bai, A Jones, K Ndousse, A Askell, A Chen, N Dassarma, D Drain, S Fort, D Ganguli, T Henighan, arXiv:2204.058622022arXiv preprint</p>
<p>E Brooks, L Walls, R L Lewis, S Singh, arXiv:2210.03821-context policy iteration. 2022arXiv preprint</p>
<p>Language models are few-shot learners. T Brown, B Mann, N Ryder, M Subbiah, J D Kaplan, P Dhariwal, A Neelakantan, P Shyam, G Sastry, A Askell, Advances in neural information processing systems. 202033</p>
<p>Safe learning in robotics: From learning-based control to safe reinforcement learning. L Brunke, M Greeff, A W Hall, Z Yuan, S Zhou, J Panerati, A P Schoellig, Robotics, and Autonomous Systems. 52022Annual Review of Control</p>
<p>A Chowdhery, S Narang, J Devlin, M Bosma, G Mishra, A Roberts, P Barham, H W Chung, C Sutton, S Gehrmann, arXiv:2204.02311Scaling language modeling with pathways. 2022arXiv preprint</p>
<p>Textworld: A learning environment for text-based games. M.-A Côté, A Kádár, X Yuan, B Kybartas, T Barnes, E Fine, J Moore, M Hausknecht, L El Asri, M Adada, Computer Games: 7th Workshop, CGW 2018, Held in Conjunction with the 27th International Conference on Artificial Intelligence, IJCAI. Revised Selected Papers. Stockholm, SwedenSpringer2018. July 13. 2018. 20197</p>
<p>Introspective multistrategy learning: Constructing a learning strategy under reasoning failure. M T Cox, 1996Georgia Institute of Technology</p>
<p>),<em> advances in neural information processing systems (datasets and benchmarks track),</em> 2021. D Hendrycks, M Mazeika, A Zou, S Patel, C Zhu, J Navarro, D Song, B Li, J Steinhardt, Advances in neural information processing systems. 2021what would jiminy cricket do? towards agents that behave morally</p>
<p>Language models as zero-shot planners: Extracting actionable knowledge for embodied agents. W Huang, P Abbeel, D Pathak, I Mordatch, International Conference on Machine Learning. PMLR2022a</p>
<p>Inner monologue: Embodied reasoning through planning with language models. W Huang, F Xia, T Xiao, H Chan, J Liang, P Florence, A Zeng, J Tompson, I Mordatch, Y Chebotar, arXiv:2207.056082022barXiv preprint</p>
<p>Survey of hallucination in natural language generation. Z Ji, N Lee, R Frieske, T Yu, D Su, Y Xu, E Ishii, Y J Bang, A Madotto, P Fung, ACM Computing Surveys. 55122023</p>
<p>Neuro-symbolic reinforcement learning with first-order logic. D Kimura, M Ono, S Chaudhury, R Kohita, A Wachi, D J Agravante, M Tatsubori, A Munawar, A Gray, arXiv:2110.109632021arXiv preprint</p>
<p>M Kwon, S M Xie, K Bullard, D Sadigh, arXiv:2303.00001Reward design with language models. 2023arXiv preprint</p>
<p>M Laskin, L Wang, J Oh, E Parisotto, S Spencer, R Steigerwald, D Strouse, S Hansen, A Filos, E Brooks, arXiv:2210.14215-context reinforcement learning with algorithm distillation. 2022arXiv preprint</p>
<p>Pre-trained language models for interactive decision-making. S Li, X Puig, C Paxton, Y Du, C Wang, L Fan, T Chen, D.-A Huang, E Akyürek, A Anandkumar, A Jacob, M Igor, T Antonio, Z Yuke, Advances in Neural Information Processing Systems. 202235</p>
<p>J Liang, W Huang, F Xia, P Xu, K Hausman, B Ichter, P Florence, A Zeng, arXiv:2209.07753Code as policies: Language model programs for embodied control. 2022arXiv preprint</p>
<p>Learning object-oriented dynamics for planning from text. G Liu, A Adhikari, A -M. Farahmand, P Poupart, International Conference on Learning Representations. 2022</p>
<p>Structured state space models for in-context reinforcement learning. C Lu, Y Schroecker, A Gu, E Parisotto, J Foerster, S Singh, F Behbahani, arXiv:2303.039822023arXiv preprint</p>
<p>K Lu, A Grover, P Abbeel, I Mordatch, arXiv:2103.05247Pretrained transformers as universal computation engines. 20211arXiv preprint</p>
<p>Self-refine: Iterative refinement with self-feedback. A Madaan, N Tandon, P Gupta, S Hallinan, L Gao, S Wiegreffe, U Alon, N Dziri, S Prabhumoye, Y Yang, arXiv:2303.176512023arXiv preprint</p>
<p>Introducing llama: A foundational, 65-billion-parameter large language model. A Meta, Meta AI. 2023</p>
<p>S Min, X Lyu, A Holtzman, M Artetxe, M Lewis, H Hajishirzi, L Zettlemoyer, arXiv:2202.12837Rethinking the role of demonstrations: What makes in-context learning work?. 2022arXiv preprint</p>
<p>. ArXiv, abs/2303.08774OpenAI. Gpt-4 technical report. 2023</p>
<p>A survey of text games for reinforcement learning informed by natural language. P Osborne, H Nõmm, A Freitas, Transactions of the Association for Computational Linguistics. 102022</p>
<p>Training language models to follow instructions with human feedback. L Ouyang, J Wu, X Jiang, D Almeida, C Wainwright, P Mishkin, C Zhang, S Agarwal, K Slama, A Ray, Advances in Neural Information Processing Systems. 202235</p>
<p>Check your facts and try again: Improving large language models with external knowledge and automated feedback. B Peng, M Galley, P He, H Cheng, Y Xie, Y Hu, Q Huang, L Liden, Z Yu, W Chen, arXiv:2302.128132023arXiv preprint</p>
<p>. S Reed, K Zolna, E Parisotto, S G Colmenarejo, A Novikov, G Barth-Maron, M Gimenez, Y Sulsky, J Kay, J T Springenberg, arXiv:2205.061752022A generalist agent. arXiv preprint</p>
<p>The limits of machine intelligence: Despite progress in machine intelligence, artificial general intelligence is still a major challenge. H Shevlin, K Vold, M Crosby, M Halina, EMBO reports. 2010e491772019</p>
<p>Reflexion: an autonomous agent with dynamic memory and self-reflection. N Shinn, B Labash, A Gopinath, arXiv:2303.113662023arXiv preprint</p>
<p>I Singh, V Blukis, A Mousavian, A Goyal, D Xu, J Tremblay, D Fox, J Thomason, A Garg, arXiv:2209.11302Progprompt: Generating situated robot task plans using large language models. 2022arXiv preprint</p>
<p>Stanford alpaca: An instruction-following llama model. R Taori, I Gulrajani, T Zhang, Y Dubois, X Li, C Guestrin, P Liang, T B Hashimoto, 2023</p>
<p>Y Tassa, Y Doron, A Muldal, T Erez, Y Li, D D L Casas, D Budden, A Abdolmaleki, J Merel, A Lefrancq, arXiv:1801.00690Deepmind control suite. 2018arXiv preprint</p>
<p>Human-timescale adaptation in an open-ended task space. A A Team, J Bauer, K Baumli, S Baveja, F Behbahani, A Bhoopchand, N Bradley-Schmieg, M Chang, N Clay, A Collister, arXiv:2301.076082023arXiv preprint</p>
<p>First textworld problems, the competition: Using text-based games to advance capabilities of ai agents. A Trischler, M.-A Côté, P Lima, 2019Microsoft Research Blog</p>
<p>Learning to follow instructions in text-based games. M Tuli, A Li, P Vaezipoor, T Klassen, S Sanner, S Mcilraith, Advances in Neural Information Processing Systems. 202235</p>
<p>Cognitive load theory and complex learning: Recent developments and future directions. J J Van Merrienboer, J Sweller, Educational psychology review. 2005</p>
<p>Chatgpt for robotics: Design principles and model abilities. S Vemprala, R Bonatti, A Bucker, A Kapoor, 2023</p>
<p>J Wei, X Wang, D Schuurmans, M Bosma, E Chi, Q Le, D Zhou, arXiv:2201.11903Chain of thought prompting elicits reasoning in large language models. 2022arXiv preprint</p>
<p>Perceiving the world: Questionguided reinforcement learning for text-based games. M Xu, L Fang, Y Chen, J T Du, C Zhou, Zhang, arXiv:2204.095972022arXiv preprint</p>
<p>Foundation models for decision making: Problems, methods, and opportunities. S Yang, O Nachum, Y Du, J Wei, P Abbeel, D Schuurmans, arXiv:2303.041292023arXiv preprint</p>
<p>S Yao, J Zhao, D Yu, N Du, I Shafran, K Narasimhan, Y Cao, arXiv:2210.03629React: Synergizing reasoning and acting in language models. 2022arXiv preprint</p>
<p>Improving sample efficiency in model-free reinforcement learning from images. D Yarats, A Zhang, I Kostrikov, B Amos, J Pineau, R Fergus, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202135</p>
<blockquote>
<p>agent: dice(white onion, cutting tool) Env: You dice the white onion. Your score has just gone up by one point. &gt;agent: cook(white onion, stove) Env: You fried the white onion. Your score has just gone up by one point. &gt;agent: slice(yellow bell pepper, cutting tool) Env: You slice the yellow bell pepper. Your score has just gone up by one point. &gt;agent: cook(yellow bell pepper, stove) Env: You fried the yellow bell pepper. Your score has just gone up by one point. X Yin, J May, 2019 IEEE Conference on Games (CoG). IEEE2019Env: Adding the meal to your inventory. Your score has just gone up by one point. &gt;agent: eat(meal) Env: You eat the meal. Not bad. Your score has just gone up by one point. *** The End *** You scored 11 out of a possible 11, in 23 turn(s</p>
</blockquote>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4897 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4897</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4897</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-103.html">extraction-schema-103</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details about the agent, the type of memory used, the tasks or benchmarks, performance with and without memory, comparisons, and any reported insights or limitations.</div>
                <p><strong>Paper ID:</strong> paper-fd712e06f87124dedc771526f797555c432cec42</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/fd712e06f87124dedc771526f797555c432cec42" target="_blank">MemoChat: Tuning LLMs to Use Memos for Consistent Long-Range Open-Domain Conversation</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper Abstract:</strong> We propose MemoChat, a pipeline for refining instructions that enables large language models (LLMs) to effectively employ self-composed memos for maintaining consistent long-range open-domain conversations. We demonstrate a long-range open-domain conversation through iterative"memorization-retrieval-response"cycles. This requires us to carefully design tailored tuning instructions for each distinct stage. The instructions are reconstructed from a collection of public datasets to teach the LLMs to memorize and retrieve past dialogues with structured memos, leading to enhanced consistency when participating in future conversations. We invite experts to manually annotate a test set designed to evaluate the consistency of long-range conversations questions. Experiments on three testing scenarios involving both open-source and API-accessible chatbots at scale verify the efficacy of MemoChat, which outperforms strong baselines. Our codes, data and models are available here: https://github.com/LuJunru/MemoChat.</p>
                <p><strong>Cost:</strong> 0.02</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4897.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4897.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details about the agent, the type of memory used, the tasks or benchmarks, performance with and without memory, comparisons, and any reported insights or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MemoChat</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MemoChat: Tuning LLMs to Use Memos for Consistent Long-Range Open-Domain Conversation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An instruction-tuning pipeline that teaches LLM-powered chatbots to self-compose, update, and retrieve structured on-the-fly memos (topic, summary, dialogues) via a three-stage "memorization - retrieval - response" loop to improve consistency in long-range multi-topic conversations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>MemoChat (pipeline)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A prompt-and-instruction tuning pipeline (three instruction types: Memo Writing, Memo Retrieval, Chat w/ Memo) that can be applied to LLMs (open-source or API models) so the LLM itself builds and uses structured memos internally during conversation.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>on-the-fly structured memo (internal, LLM-managed)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_description</strong></td>
                            <td>The LLM writes JSON-style memo entries with keys 'topic', 'summary', and 'dialogues' (Memo Writing); retrieval is done over memo 'topics' and 'summaries' (Memo Retrieval); retrieved evidences are prefixed to the user query (Chat w/ Memo) so the LLM conditions responses on retrieved memo content.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Long-range open-domain conversation consistency</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Generate faithful, consistent responses in multi-topic long conversations (12–20+ turns) including question types: Retrospection (recall specific earlier facts), Continuation (extend or build on earlier content), and Conjunction (cross-topic composition).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>MT-Bench+ (expert-annotated extension of MT-Bench) ; training reconstructed from TopicoQA, DialogSum, Alpaca-GPT4</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Multiple reported results: e.g., MemoChat-ChatGPT average GPT4 auto-rating 70.76 (scale 1-100); MemoChat-Vicuna-33B (10k) average 61.52 (GPT4). Intermediate instruction-task metrics (Memo Writing / Retrieval / Chat w/ Memo) improved to F1/BertScore values up to ~57%/68% depending on model scale (see Table 3 and 4).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Baseline ChatGPT-2k (no MemoChat pipeline) average GPT4 auto-rating 51.89 (Retrospection 52.11, Continuation 55.33, Conjunction 48.22).</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_comparison_summary</strong></td>
                            <td>MemoChat (self-composed internal memos) substantially improves downstream response consistency versus the 2k-window ChatGPT baseline and vs. external memory baselines: MemoChat-ChatGPT (70.76 avg) > MPC-ChatGPT (54.52) > ChatGPT-2k (51.89) > MemoryBank-ChatGPT (42.44). Larger models and more fine-tuning data (10k instructions) generally yield better memo-usage ability on both intermediate tasks (writing/retrieval/chat) and downstream consistency.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Reported and observed issues include: (1) possible accumulation / retrieval errors as with other memory systems; (2) retrieval in MemoChat is limited to matching topics and summaries (may miss fine-grained facts); (3) errors remain on conjunction (cross-topic) questions and some long-form QA; (4) instruction-design challenges during training: 'Prompt Copy' (models copying example formats), 'Catastrophic Forgetting' (insufficient Chat w/ Memo data leads to collapsed/repetitive responses), and 'Prompt Misplacement' (order of prompt pieces matters); (5) occasional factual hallucinations or misattributions in retrieved/combined responses (example: mis-stating EU vs US legal application).</td>
                        </tr>
                        <tr>
                            <td><strong>key_insights</strong></td>
                            <td>Structured, on-the-fly memos (topic + summary + dialogues) plus explicit instruction tuning on three stages (write, retrieve, chat) enable LLMs to use memory effectively without external tooling; retrieval over concise summaries/topics yields fast retrieval; larger LLMs can learn memo use with few examples, but full fine-tuning on balanced instruction sets (including Chat w/ Memo) produces best downstream consistency gains; careful instruction design (avoid format leakage, balance data types, place explanations appropriately) is critical.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MemoChat: Tuning LLMs to Use Memos for Consistent Long-Range Open-Domain Conversation', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4897.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4897.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details about the agent, the type of memory used, the tasks or benchmarks, performance with and without memory, comparisons, and any reported insights or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MemoChat-ChatGPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MemoChat applied to ChatGPT (gpt-3.5-turbo, 2k window)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>ChatGPT (gpt-3.5-turbo, 2k window) augmented with the MemoChat instruction pipeline so the model writes and retrieves on-the-fly structured memos to improve long-range conversational consistency.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>MemoChat-ChatGPT</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>An application of the MemoChat pipeline on the ChatGPT API: ChatGPT is instructed to perform Memo Writing, Memo Retrieval, and Chat w/ Memo cycles, maintaining memos internally and using retrieved memo evidence when answering later queries.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>on-the-fly structured memo (internal)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_description</strong></td>
                            <td>Memo entries are JSON records ('topic','summary','dialogues'); retrieval is performed by matching topics/summaries from the memo to the new query; retrieved evidence is prepended to the query to produce a response.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Long-range open-domain conversation consistency (MT-Bench+ tests of Retrospection, Continuation, Conjunction)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Maintain faithful and consistent responses across long multi-turn and multi-topic chats; evaluated by GPT4 judge scoring (1-100) on three question types.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>MT-Bench+ (expert-annotated extension of MT-Bench)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Average GPT4 auto-rating = 70.76 (Retrospection 66.28, Continuation 73.50, Conjunction 72.50) on MT-Bench+.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>ChatGPT-2k baseline average GPT4 auto-rating = 51.89 (Retrospection 52.11, Continuation 55.33, Conjunction 48.22).</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_comparison_summary</strong></td>
                            <td>Applying MemoChat to ChatGPT yields large improvements (~+18.9 average points) over ChatGPT-2k; it outperforms external-memory augmentations tested (MPC-ChatGPT, MemoryBank-ChatGPT) by substantial margins in this evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Retrieval limited to topics/summaries can miss details; potential accumulation errors; still struggles on some long-form or cross-topic (conjunction) questions; relies on high-quality memo-writing and retrieval steps provided by prompts/instructions.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insights</strong></td>
                            <td>Even API-accessible LLMs with limited context windows benefit strongly from being taught to write and retrieve compact structured memos; on-the-fly memo usage can be implemented without additional retriever or vector DB tooling.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MemoChat: Tuning LLMs to Use Memos for Consistent Long-Range Open-Domain Conversation', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4897.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4897.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details about the agent, the type of memory used, the tasks or benchmarks, performance with and without memory, comparisons, and any reported insights or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MPC-ChatGPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prompted LLMs as chatbot modules for long open-domain conversation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An external-memory pipeline (MPC) that stores persona summaries of recent conversations and uses a DPR retriever to fetch top-K history for conditioning chatbot responses.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Prompted LLMs as chatbot modules for long open-domain conversation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>MPC-ChatGPT</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>ChatGPT augmented with the MPC pipeline: the system regularly stores persona-style summaries, indexes them with a DPR retriever, retrieves top-K relevant history for a new query, and conditions responses on the retrieved history.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>external memory with dense retrieval (DPR)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_description</strong></td>
                            <td>Persona summaries are stored externally; for a new query, a DPR retriever returns top-K history entries which are concatenated with the query to generate a response.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Long-range open-domain conversation</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Maintain consistency across extended dialogues by retrieving persona/history snippets relevant to the current query.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>MT-Bench+ (used here as baseline comparison)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Average GPT4 auto-rating = 54.52 (Retrospection 53.00, Continuation 61.22, Conjunction 49.33) when MPC is plugged into ChatGPT-2k.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>ChatGPT-2k baseline average = 51.89. (Thus MPC-ChatGPT shows a modest improvement over ChatGPT-2k in this evaluation.)</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_comparison_summary</strong></td>
                            <td>MPC yields modest gains over ChatGPT-2k (≈+2.6 avg points) but is substantially outperformed by MemoChat-ChatGPT in the authors' downstream consistency evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Requires external retriever infrastructure (DPR) and careful memory management; stores primarily persona summaries (less general than MemoChat's broader summaries); susceptible to retrieval errors and mismatch between stored summaries and downstream query needs.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insights</strong></td>
                            <td>External DPR-based memory helps but may be less effective than teaching the LLM to self-compose and use broader structured memos when the goal is general long-range conversation consistency.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MemoChat: Tuning LLMs to Use Memos for Consistent Long-Range Open-Domain Conversation', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4897.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4897.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details about the agent, the type of memory used, the tasks or benchmarks, performance with and without memory, comparisons, and any reported insights or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MemoryBank-ChatGPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MemoryBank: Enhancing large language models with long-term memory</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An external vector-database memory (MemoryBank) that manages event and user-portrait memories (FAISS) and retrieves them for LLM-based chatbots.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Memorybank: Enhancing large language models with long-term memory</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>MemoryBank-ChatGPT</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>ChatGPT augmented with MemoryBank: memory events and user portraits are stored in a FAISS vector index and retrieved to inform responses.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>external vector-database memory (FAISS) / retrieval-augmented</td>
                        </tr>
                        <tr>
                            <td><strong>memory_description</strong></td>
                            <td>Event and user-portrait memories are encoded and stored in FAISS; retrieval returns relevant memory entries used to condition the chatbot's responses.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Long-range open-domain conversation</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Improve chatbot response consistency and personalization via retrieval of long-term stored memories.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>MT-Bench+ (used here as baseline comparison)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Average GPT4 auto-rating = 42.44 (Retrospection 23.39, Continuation 55.28, Conjunction 48.67) when MemoryBank is plugged into ChatGPT-2k.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>ChatGPT-2k baseline average = 51.89; MemoryBank-ChatGPT performed worse overall on this evaluation (notably poor retrospection score).</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_comparison_summary</strong></td>
                            <td>MemoryBank augmentation in this setup degraded overall downstream consistency compared to ChatGPT-2k baseline (particularly poor on retrospection), suggesting that external memory retrieval quality and alignment with evaluation tasks are critical.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Retrieval errors and misalignment of stored memory content with downstream needs; accumulation or indexing issues may harm performance; design and curation of what to store matter greatly.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insights</strong></td>
                            <td>External vector memory approaches must ensure high precision of stored summaries and retrieval relevance; otherwise they risk harming downstream fidelity compared to no-augmented baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MemoChat: Tuning LLMs to Use Memos for Consistent Long-Range Open-Domain Conversation', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4897.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e4897.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details about the agent, the type of memory used, the tasks or benchmarks, performance with and without memory, comparisons, and any reported insights or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MemoChat-Vicuna-33B (10k)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MemoChat fine-tuned Vicuna-33B with 10k instructions</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A Vicuna-33B open-source LLM fully fine-tuned on the MemoChat instruction datasets (10k samples) to write, retrieve, and use structured memos for long-range conversation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>MemoChat-Vicuna-33B (10k)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Vicuna-33B model fine-tuned end-to-end on the three MemoChat instruction tasks (Memo Writing, Memo Retrieval, Chat w/ Memo) to acquire self-use memo capability.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>on-the-fly structured memo (internal)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_description</strong></td>
                            <td>Same MemoChat implementation: the fine-tuned model generates and maintains JSON memos (topic/summary/dialogues), retrieves relevant memo entries by matching topics/summaries, and conditions responses on the retrieved content.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Long-range open-domain conversation consistency (MT-Bench+)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Maintain consistent multi-topic dialogues; evaluated by GPT4 on Retrospection, Continuation, Conjunction question types.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>MT-Bench+</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Average GPT4 auto-rating = 61.52 (Retrospection 69.00, Continuation 65.00, Conjunction 50.56) after full 10k instruction fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_comparison_summary</strong></td>
                            <td>Fully fine-tuned large open-source MemoChat models (10k) outperform ChatGPT-2k baseline on average; model scale and full instruction tuning (10k) lead to the highest memo-usage ability among open-source models tested.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Still shows weaknesses on conjunction (cross-topic composition) questions and occasional factual errors when combining retrieved content; needs balanced instruction dataset to avoid catastrophic forgetting.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insights</strong></td>
                            <td>Large open-source LLMs can be taught to self-manage memos via instruction fine-tuning; full-scale (10k) instruction fine-tuning yields strong gains, but some cross-topic reasoning errors remain.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MemoChat: Tuning LLMs to Use Memos for Consistent Long-Range Open-Domain Conversation', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4897.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e4897.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details about the agent, the type of memory used, the tasks or benchmarks, performance with and without memory, comparisons, and any reported insights or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MemoChat-Vicuna-13B (1k)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MemoChat fine-tuned Vicuna-13B with 1k instructions</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A Vicuna-13B model fine-tuned with a small (1k) MemoChat instruction set; demonstrates that larger models can learn memo-usage with limited data.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>MemoChat-Vicuna-13B (1k)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Vicuna-13B fine-tuned on 1k reconstructed MemoChat instructions to acquire memo-writing, retrieval, and usage ability.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>on-the-fly structured memo (internal)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_description</strong></td>
                            <td>As with MemoChat: writes JSON memos, retrieves by topic/summary, and prefixes retrieved evidences for response generation.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Long-range open-domain conversation consistency (MT-Bench+)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same as above: produce consistent responses across long multi-topic conversational streams.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>MT-Bench+</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Average GPT4 auto-rating = 57.50 (Retrospection 64.78, Continuation 56.06, Conjunction 51.67) with 1k instruction fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_comparison_summary</strong></td>
                            <td>With limited (1k) fine-tuning, Vicuna-13B learned to use memos well enough to approach performance of larger models and to outperform some baselines; demonstrates data efficiency for larger models.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Tendency to produce shorter answers (affecting some metrics); lower absolute performance than fully fine-tuned larger models; same instruction-design sensitivities apply.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insights</strong></td>
                            <td>Larger LLMs can learn memo-usage from small instruction sets; data efficiency exists but full fine-tuning yields better absolute performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MemoChat: Tuning LLMs to Use Memos for Consistent Long-Range Open-Domain Conversation', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Prompted LLMs as chatbot modules for long open-domain conversation <em>(Rating: 2)</em></li>
                <li>Memorybank: Enhancing large language models with long-term memory <em>(Rating: 2)</em></li>
                <li>Long time no see! open-domain conversation with long-term persona memory <em>(Rating: 1)</em></li>
                <li>Dense passage retrieval for open-domain question answering <em>(Rating: 1)</em></li>
                <li>Beyond goldfish memory: Long-term open-domain conversation <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4897",
    "paper_id": "paper-fd712e06f87124dedc771526f797555c432cec42",
    "extraction_schema_id": "extraction-schema-103",
    "extracted_data": [
        {
            "name_short": "MemoChat",
            "name_full": "MemoChat: Tuning LLMs to Use Memos for Consistent Long-Range Open-Domain Conversation",
            "brief_description": "An instruction-tuning pipeline that teaches LLM-powered chatbots to self-compose, update, and retrieve structured on-the-fly memos (topic, summary, dialogues) via a three-stage \"memorization - retrieval - response\" loop to improve consistency in long-range multi-topic conversations.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "MemoChat (pipeline)",
            "agent_description": "A prompt-and-instruction tuning pipeline (three instruction types: Memo Writing, Memo Retrieval, Chat w/ Memo) that can be applied to LLMs (open-source or API models) so the LLM itself builds and uses structured memos internally during conversation.",
            "memory_type": "on-the-fly structured memo (internal, LLM-managed)",
            "memory_description": "The LLM writes JSON-style memo entries with keys 'topic', 'summary', and 'dialogues' (Memo Writing); retrieval is done over memo 'topics' and 'summaries' (Memo Retrieval); retrieved evidences are prefixed to the user query (Chat w/ Memo) so the LLM conditions responses on retrieved memo content.",
            "task_name": "Long-range open-domain conversation consistency",
            "task_description": "Generate faithful, consistent responses in multi-topic long conversations (12–20+ turns) including question types: Retrospection (recall specific earlier facts), Continuation (extend or build on earlier content), and Conjunction (cross-topic composition).",
            "benchmark_name": "MT-Bench+ (expert-annotated extension of MT-Bench) ; training reconstructed from TopicoQA, DialogSum, Alpaca-GPT4",
            "performance_with_memory": "Multiple reported results: e.g., MemoChat-ChatGPT average GPT4 auto-rating 70.76 (scale 1-100); MemoChat-Vicuna-33B (10k) average 61.52 (GPT4). Intermediate instruction-task metrics (Memo Writing / Retrieval / Chat w/ Memo) improved to F1/BertScore values up to ~57%/68% depending on model scale (see Table 3 and 4).",
            "performance_without_memory": "Baseline ChatGPT-2k (no MemoChat pipeline) average GPT4 auto-rating 51.89 (Retrospection 52.11, Continuation 55.33, Conjunction 48.22).",
            "has_performance_with_without_memory": true,
            "memory_comparison_summary": "MemoChat (self-composed internal memos) substantially improves downstream response consistency versus the 2k-window ChatGPT baseline and vs. external memory baselines: MemoChat-ChatGPT (70.76 avg) &gt; MPC-ChatGPT (54.52) &gt; ChatGPT-2k (51.89) &gt; MemoryBank-ChatGPT (42.44). Larger models and more fine-tuning data (10k instructions) generally yield better memo-usage ability on both intermediate tasks (writing/retrieval/chat) and downstream consistency.",
            "limitations_or_challenges": "Reported and observed issues include: (1) possible accumulation / retrieval errors as with other memory systems; (2) retrieval in MemoChat is limited to matching topics and summaries (may miss fine-grained facts); (3) errors remain on conjunction (cross-topic) questions and some long-form QA; (4) instruction-design challenges during training: 'Prompt Copy' (models copying example formats), 'Catastrophic Forgetting' (insufficient Chat w/ Memo data leads to collapsed/repetitive responses), and 'Prompt Misplacement' (order of prompt pieces matters); (5) occasional factual hallucinations or misattributions in retrieved/combined responses (example: mis-stating EU vs US legal application).",
            "key_insights": "Structured, on-the-fly memos (topic + summary + dialogues) plus explicit instruction tuning on three stages (write, retrieve, chat) enable LLMs to use memory effectively without external tooling; retrieval over concise summaries/topics yields fast retrieval; larger LLMs can learn memo use with few examples, but full fine-tuning on balanced instruction sets (including Chat w/ Memo) produces best downstream consistency gains; careful instruction design (avoid format leakage, balance data types, place explanations appropriately) is critical.",
            "uuid": "e4897.0",
            "source_info": {
                "paper_title": "MemoChat: Tuning LLMs to Use Memos for Consistent Long-Range Open-Domain Conversation",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "MemoChat-ChatGPT",
            "name_full": "MemoChat applied to ChatGPT (gpt-3.5-turbo, 2k window)",
            "brief_description": "ChatGPT (gpt-3.5-turbo, 2k window) augmented with the MemoChat instruction pipeline so the model writes and retrieves on-the-fly structured memos to improve long-range conversational consistency.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "MemoChat-ChatGPT",
            "agent_description": "An application of the MemoChat pipeline on the ChatGPT API: ChatGPT is instructed to perform Memo Writing, Memo Retrieval, and Chat w/ Memo cycles, maintaining memos internally and using retrieved memo evidence when answering later queries.",
            "memory_type": "on-the-fly structured memo (internal)",
            "memory_description": "Memo entries are JSON records ('topic','summary','dialogues'); retrieval is performed by matching topics/summaries from the memo to the new query; retrieved evidence is prepended to the query to produce a response.",
            "task_name": "Long-range open-domain conversation consistency (MT-Bench+ tests of Retrospection, Continuation, Conjunction)",
            "task_description": "Maintain faithful and consistent responses across long multi-turn and multi-topic chats; evaluated by GPT4 judge scoring (1-100) on three question types.",
            "benchmark_name": "MT-Bench+ (expert-annotated extension of MT-Bench)",
            "performance_with_memory": "Average GPT4 auto-rating = 70.76 (Retrospection 66.28, Continuation 73.50, Conjunction 72.50) on MT-Bench+.",
            "performance_without_memory": "ChatGPT-2k baseline average GPT4 auto-rating = 51.89 (Retrospection 52.11, Continuation 55.33, Conjunction 48.22).",
            "has_performance_with_without_memory": true,
            "memory_comparison_summary": "Applying MemoChat to ChatGPT yields large improvements (~+18.9 average points) over ChatGPT-2k; it outperforms external-memory augmentations tested (MPC-ChatGPT, MemoryBank-ChatGPT) by substantial margins in this evaluation.",
            "limitations_or_challenges": "Retrieval limited to topics/summaries can miss details; potential accumulation errors; still struggles on some long-form or cross-topic (conjunction) questions; relies on high-quality memo-writing and retrieval steps provided by prompts/instructions.",
            "key_insights": "Even API-accessible LLMs with limited context windows benefit strongly from being taught to write and retrieve compact structured memos; on-the-fly memo usage can be implemented without additional retriever or vector DB tooling.",
            "uuid": "e4897.1",
            "source_info": {
                "paper_title": "MemoChat: Tuning LLMs to Use Memos for Consistent Long-Range Open-Domain Conversation",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "MPC-ChatGPT",
            "name_full": "Prompted LLMs as chatbot modules for long open-domain conversation",
            "brief_description": "An external-memory pipeline (MPC) that stores persona summaries of recent conversations and uses a DPR retriever to fetch top-K history for conditioning chatbot responses.",
            "citation_title": "Prompted LLMs as chatbot modules for long open-domain conversation",
            "mention_or_use": "use",
            "agent_name": "MPC-ChatGPT",
            "agent_description": "ChatGPT augmented with the MPC pipeline: the system regularly stores persona-style summaries, indexes them with a DPR retriever, retrieves top-K relevant history for a new query, and conditions responses on the retrieved history.",
            "memory_type": "external memory with dense retrieval (DPR)",
            "memory_description": "Persona summaries are stored externally; for a new query, a DPR retriever returns top-K history entries which are concatenated with the query to generate a response.",
            "task_name": "Long-range open-domain conversation",
            "task_description": "Maintain consistency across extended dialogues by retrieving persona/history snippets relevant to the current query.",
            "benchmark_name": "MT-Bench+ (used here as baseline comparison)",
            "performance_with_memory": "Average GPT4 auto-rating = 54.52 (Retrospection 53.00, Continuation 61.22, Conjunction 49.33) when MPC is plugged into ChatGPT-2k.",
            "performance_without_memory": "ChatGPT-2k baseline average = 51.89. (Thus MPC-ChatGPT shows a modest improvement over ChatGPT-2k in this evaluation.)",
            "has_performance_with_without_memory": true,
            "memory_comparison_summary": "MPC yields modest gains over ChatGPT-2k (≈+2.6 avg points) but is substantially outperformed by MemoChat-ChatGPT in the authors' downstream consistency evaluation.",
            "limitations_or_challenges": "Requires external retriever infrastructure (DPR) and careful memory management; stores primarily persona summaries (less general than MemoChat's broader summaries); susceptible to retrieval errors and mismatch between stored summaries and downstream query needs.",
            "key_insights": "External DPR-based memory helps but may be less effective than teaching the LLM to self-compose and use broader structured memos when the goal is general long-range conversation consistency.",
            "uuid": "e4897.2",
            "source_info": {
                "paper_title": "MemoChat: Tuning LLMs to Use Memos for Consistent Long-Range Open-Domain Conversation",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "MemoryBank-ChatGPT",
            "name_full": "MemoryBank: Enhancing large language models with long-term memory",
            "brief_description": "An external vector-database memory (MemoryBank) that manages event and user-portrait memories (FAISS) and retrieves them for LLM-based chatbots.",
            "citation_title": "Memorybank: Enhancing large language models with long-term memory",
            "mention_or_use": "use",
            "agent_name": "MemoryBank-ChatGPT",
            "agent_description": "ChatGPT augmented with MemoryBank: memory events and user portraits are stored in a FAISS vector index and retrieved to inform responses.",
            "memory_type": "external vector-database memory (FAISS) / retrieval-augmented",
            "memory_description": "Event and user-portrait memories are encoded and stored in FAISS; retrieval returns relevant memory entries used to condition the chatbot's responses.",
            "task_name": "Long-range open-domain conversation",
            "task_description": "Improve chatbot response consistency and personalization via retrieval of long-term stored memories.",
            "benchmark_name": "MT-Bench+ (used here as baseline comparison)",
            "performance_with_memory": "Average GPT4 auto-rating = 42.44 (Retrospection 23.39, Continuation 55.28, Conjunction 48.67) when MemoryBank is plugged into ChatGPT-2k.",
            "performance_without_memory": "ChatGPT-2k baseline average = 51.89; MemoryBank-ChatGPT performed worse overall on this evaluation (notably poor retrospection score).",
            "has_performance_with_without_memory": true,
            "memory_comparison_summary": "MemoryBank augmentation in this setup degraded overall downstream consistency compared to ChatGPT-2k baseline (particularly poor on retrospection), suggesting that external memory retrieval quality and alignment with evaluation tasks are critical.",
            "limitations_or_challenges": "Retrieval errors and misalignment of stored memory content with downstream needs; accumulation or indexing issues may harm performance; design and curation of what to store matter greatly.",
            "key_insights": "External vector memory approaches must ensure high precision of stored summaries and retrieval relevance; otherwise they risk harming downstream fidelity compared to no-augmented baselines.",
            "uuid": "e4897.3",
            "source_info": {
                "paper_title": "MemoChat: Tuning LLMs to Use Memos for Consistent Long-Range Open-Domain Conversation",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "MemoChat-Vicuna-33B (10k)",
            "name_full": "MemoChat fine-tuned Vicuna-33B with 10k instructions",
            "brief_description": "A Vicuna-33B open-source LLM fully fine-tuned on the MemoChat instruction datasets (10k samples) to write, retrieve, and use structured memos for long-range conversation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "MemoChat-Vicuna-33B (10k)",
            "agent_description": "Vicuna-33B model fine-tuned end-to-end on the three MemoChat instruction tasks (Memo Writing, Memo Retrieval, Chat w/ Memo) to acquire self-use memo capability.",
            "memory_type": "on-the-fly structured memo (internal)",
            "memory_description": "Same MemoChat implementation: the fine-tuned model generates and maintains JSON memos (topic/summary/dialogues), retrieves relevant memo entries by matching topics/summaries, and conditions responses on the retrieved content.",
            "task_name": "Long-range open-domain conversation consistency (MT-Bench+)",
            "task_description": "Maintain consistent multi-topic dialogues; evaluated by GPT4 on Retrospection, Continuation, Conjunction question types.",
            "benchmark_name": "MT-Bench+",
            "performance_with_memory": "Average GPT4 auto-rating = 61.52 (Retrospection 69.00, Continuation 65.00, Conjunction 50.56) after full 10k instruction fine-tuning.",
            "performance_without_memory": null,
            "has_performance_with_without_memory": false,
            "memory_comparison_summary": "Fully fine-tuned large open-source MemoChat models (10k) outperform ChatGPT-2k baseline on average; model scale and full instruction tuning (10k) lead to the highest memo-usage ability among open-source models tested.",
            "limitations_or_challenges": "Still shows weaknesses on conjunction (cross-topic composition) questions and occasional factual errors when combining retrieved content; needs balanced instruction dataset to avoid catastrophic forgetting.",
            "key_insights": "Large open-source LLMs can be taught to self-manage memos via instruction fine-tuning; full-scale (10k) instruction fine-tuning yields strong gains, but some cross-topic reasoning errors remain.",
            "uuid": "e4897.4",
            "source_info": {
                "paper_title": "MemoChat: Tuning LLMs to Use Memos for Consistent Long-Range Open-Domain Conversation",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "MemoChat-Vicuna-13B (1k)",
            "name_full": "MemoChat fine-tuned Vicuna-13B with 1k instructions",
            "brief_description": "A Vicuna-13B model fine-tuned with a small (1k) MemoChat instruction set; demonstrates that larger models can learn memo-usage with limited data.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "MemoChat-Vicuna-13B (1k)",
            "agent_description": "Vicuna-13B fine-tuned on 1k reconstructed MemoChat instructions to acquire memo-writing, retrieval, and usage ability.",
            "memory_type": "on-the-fly structured memo (internal)",
            "memory_description": "As with MemoChat: writes JSON memos, retrieves by topic/summary, and prefixes retrieved evidences for response generation.",
            "task_name": "Long-range open-domain conversation consistency (MT-Bench+)",
            "task_description": "Same as above: produce consistent responses across long multi-topic conversational streams.",
            "benchmark_name": "MT-Bench+",
            "performance_with_memory": "Average GPT4 auto-rating = 57.50 (Retrospection 64.78, Continuation 56.06, Conjunction 51.67) with 1k instruction fine-tuning.",
            "performance_without_memory": null,
            "has_performance_with_without_memory": false,
            "memory_comparison_summary": "With limited (1k) fine-tuning, Vicuna-13B learned to use memos well enough to approach performance of larger models and to outperform some baselines; demonstrates data efficiency for larger models.",
            "limitations_or_challenges": "Tendency to produce shorter answers (affecting some metrics); lower absolute performance than fully fine-tuned larger models; same instruction-design sensitivities apply.",
            "key_insights": "Larger LLMs can learn memo-usage from small instruction sets; data efficiency exists but full fine-tuning yields better absolute performance.",
            "uuid": "e4897.5",
            "source_info": {
                "paper_title": "MemoChat: Tuning LLMs to Use Memos for Consistent Long-Range Open-Domain Conversation",
                "publication_date_yy_mm": "2023-08"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Prompted LLMs as chatbot modules for long open-domain conversation",
            "rating": 2
        },
        {
            "paper_title": "Memorybank: Enhancing large language models with long-term memory",
            "rating": 2
        },
        {
            "paper_title": "Long time no see! open-domain conversation with long-term persona memory",
            "rating": 1
        },
        {
            "paper_title": "Dense passage retrieval for open-domain question answering",
            "rating": 1
        },
        {
            "paper_title": "Beyond goldfish memory: Long-term open-domain conversation",
            "rating": 1
        }
    ],
    "cost": 0.01985325,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>MemoChat: Tuning LLMs to Use Memos for Consistent Long-Range Open-Domain Conversation</h1>
<p>Junru Lu ${ }^{1}$, Siyu An ${ }^{2}$, Mingbao Lin ${ }^{2}$, Gabriele Pergola ${ }^{1}$, Yulan $\mathrm{He}^{3}$, Di Yin $^{2}$, Xing Sun ${ }^{2}$ and Yunsheng Wu ${ }^{2}$<br>${ }^{1}$ University of Warwick, ${ }^{2}$ Tecent YouTu Lab, ${ }^{3}$ King's College London<br>${ }^{1}$ {junru.lu, gabriele.pergola}@warwick.ac.uk, ${ }^{3}$ yulan.he@kcl.ac.uk<br>${ }^{2}$ {siyuan, marlin, endymecyyin, winfredsun, simonwu}@tencent.com</p>
<h4>Abstract</h4>
<p>We propose MemoChat, a pipeline for refining instructions that enables large language models (LLMs) to effectively employ self-composed memos for maintaining consistent long-range open-domain conversations. We demonstrate a long-range open-domain conversation through iterative "memorization-retrieval-response" cycles. This requires us to carefully design tailored tuning instructions for each distinct stage. The instructions are reconstructed from a collection of public datasets to teach the LLMs to memorize and retrieve past dialogues with structured memos, leading to enhanced consistency when participating in future conversations. We invite experts to manually annotate a test set designed to evaluate the consistency of long-range conversations questions. Experiments on three testing scenarios involving both open-source and API-accessible chatbots at scale verify the efficacy of MemoChat, which outperforms strong baselines ${ }^{1}$.</p>
<h2>1 Introduction</h2>
<p>Large language models (LLMs) have brought about a substantial revolution, fundamentally changing our lifestyle. They have emerged as a new platform that connects the realms of academia and industry within the field of artificial intelligence (Zhao et al., 2023b; Yang et al., 2023). In particular, LLMs have demonstrated an enhanced ability to synchronize effectively with human cognitive processes ( Du et al., 2022; Taori et al., 2023; Zheng et al., 2023; Liu et al., 2023c; Ouyang et al., 2022) and consequently have served as a foundation towards creating human-like conversational dialogues.</p>
<p>In contrast to the traditional dialogue systems, which mainly involve short conversations on similar topics (Dinan et al., 2019b; Komeili et al., 2022; Dinan et al., 2019a), the emerging trend in humanchatbot interaction is characterized by longer conversations encompassing diverse topics (Kim et al.,
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: An example of a long-range open-domain conversation. It contains multiple topics.
2023). Figure 1 illustrates an example of such long-range conversations with more than 20 exchanges between a human user and a chat bot. It involves several sequential topics such as "Quantum Physics" (1 to 8 turns), "Business Etiquette in Japan" (11 to 14 turns) and "Polynomial Math Problem" (16 to 18 turns), while other topics are simply referred to as "MULTIPLE TURNS". Also, two topics-related long-range questions are raised subsequently, including one for evoking memories associated with "Business Etiquette in Japan", and the other for retrieving historical information pertaining to "Quantum Physics". Such long-range open-domain conversations that involve diverse topics presents a challenge for conventional methods, as they struggle to effectively address the issue of retaining contextual coherence over long stretches of discourse. One possible strategy is to expand the input text window (Tworkowski et al., 2023; Li et al., 2023; Ding et al., 2023). For example, positional interpolation (Chen et al., 2023) was presented to empower RoPE-based (Su et al., 2022) LLMs with a text window that is 600 times longer than its original pretraining version. Unfortunately, the window length can not be arbitrar-</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>ily expanded (Beltagy et al., 2020; Kitaev et al., 2020). Another potential solution is to resort to additional memories for tracking the history of conversations (Zhong et al., 2023; Zhao et al., 2023a). For example, MPC was proposed (Lee et al., 2023) to regularly store persona summaries of recent conversations as history, then use new user query $\boldsymbol{Q}$ to retrieve the top-K history $\boldsymbol{D}$ via an external DPR retriever (Karpukhin et al., 2020), and finally generate a new response based on both $\boldsymbol{Q}$ and $\boldsymbol{D}$. Similarly, an event and user portrait memory managed with FAISS (Johnson et al., 2019) were adopted by (Zhong et al., 2023). Despite the advantage in retaining infinite contextual inputs, compared with text window extension, memory mechanism may cause accumulation errors during the retrieval of past evidence (Xu et al., 2022b). As a result, how to enhance the accuracy of memory recall and improve the response consistency of LLM-powered chatbots in long-range open-domain conversation remains unsolved (Xu et al., 2022a).</p>
<p>In this paper, we are particularly interested in building memory-enhanced chatbots. Instead of replying on external tools for memory creation and recall (Lee et al., 2023; Zhong et al., 2023; Hu et al., 2023; Kynoch and Latapie, 2023), we propose a streamlined approach where we eliminate the need for these complex associate modules. Instead, we create a simplified pipeline that solely utilizes LLMs to power chatbots. The proposed memo-equipped pipeline, MemoChat, aims for guiding LLMs to use simple on-the-fly memos for maintaining consistent long-range open-domain conversation. We base our approach on the concept of "memorization-retrieval-response" loops within the context of long-range open-domain conversations. To achieve this, except for zero-shot learning with chatbots accessed via public APIs (OpenAI, 2023; Google, 2023), we reconstruct a series of instructions from publicly available datasets (Adlakha et al., 2022; Chen et al., 2021; Ghosal et al., 2023) to fine-tune open-source LLM-powered chatbots. This fine-tuning process helps the chatbots familiarize themselves with each stage within the loop, and employ structured memos for maintaining a consistent long-range conversation. For fair evaluation, we invite experts to manually annotate a testing set specifically designed to evaluate the consistency of long-range conversations across three different types of testing scenarios. We verify the effectiveness of the proposed MemoChat through a
comprehensive evaluation involving both a chatbot accessible via public API and four different opensource LLMs. This evaluation is carried out using a very strong LLM judge (Zheng et al., 2023). The contributions of this paper are listed as follows:</p>
<ul>
<li>We introduce MemoChat, an instruction tuning pipeline, to assist LLM-powered chatbots carrying on consistent long-range open-domain conversations through an on-the-fly memo.</li>
<li>We leverage several publicly available datasets to reconstruct instructions to help open-source LLMs familiarize themselves with the MemoChat pipeline.</li>
<li>We curate an expert-annotated evaluation set, which serves as a benchmark for assessing response consistency. Experiments on both open-source and API-accessible chatbots verify the efficacy of MemoChat.</li>
</ul>
<h2>2 Related Work</h2>
<h3>2.1 Long-Range Open-Domain Conversation</h3>
<p>Long-range open-domain conversation has been a widely explored task recently. Doc2Dial, a goaloriented document-grounded dialogue dataset is proposed by (Feng et al., 2020), including around 48 k long-range conversations with an average of 14 turns. MultiSessionChat is a dataset concerning one long-range conversation over multiple sessions (Xu et al., 2022a). While most of these traditional datasets focus on few-topic human-human scenarios, there has been a proliferation on newly multitopic human-chatbot scenarios (Kim et al., 2023).</p>
<p>The challenges of long-range open-domain conversations lie on the answerability of open-domain questions and the capacity of long-range input understanding. External knowledge graph was incorporated to improve the answer ability of out-ofdistribution open-domain questions (Zhou et al., 2020). With the development of LLMs, LLMpowered chatbots have largely improved their answering ability (Shuster et al., 2022b,a). On the other hand, various strategies have been proposed to tackle the challenge of long-range input understanding. One possible strategy is to use additional memory to filter shorter key facts in the past longrange history to replace it. MPC (Lee et al., 2023) and MemoryBank (Zhong et al., 2023) were proposed to use additional memory and external retriever to manage persona, event or user portrait</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: The overall architecture of our proposed MemoChat pipeline. While maintaining a chatting stream between the human user and the chatbot (left part), the chatbot will have a memo-equipped inner thinking (right part).</p>
<p>information. Our work differs from existing studies in that we solely teach the chatbot to self-use memos, and we focus on summarizing all facts in the history instead of partial persona or event facts.</p>
<h3>2.2 Instruction Tuning</h3>
<p>Instruction tuning is a widely used strategy to fine-tune LLMs on tasks with meta-instruction ahead (Wei et al., 2022a). The effectiveness of instruction tuning has been proved on many tasks, including text reasoning (Sanh et al., 2022), human-bot alignment (Ouyang et al., 2022), document ranking (Qin et al., 2023; Sun et al., 2023), document retrieval (Mueller et al., 2023), and general conversation ability (Gupta et al., 2022). In this work, we spend low-cost yet carefully design instructions to teach LLM-powered chatbots for self-using on-the-fly memo to strengthen the response consistency within long-range chatting stream.</p>
<h3>3 Methodology</h3>
<h4>3.1 Formulation</h4>
<p>We start with formulating the long-range open-domain conversation as Generative Question Answering (GQA), in which a chatbot is expected to generate an answer $$ \hat{Y} = \arg\max_{Y} p(Y|\boldsymbol{x}^q, \boldsymbol{x}^h), $$ given previous conversation history $$ \boldsymbol{x}^h $$ and current user query $$ \boldsymbol{x}^q $$. To improve the response consistency of $$ \hat{Y} $$, our MemoChat takes an step forward and proposes to decompose the long-range open-domain conversation as a three-stage "memorization-retrieval-response" loop, intending to retrieve query-related evidences $$ \boldsymbol{x}^h' $$ from structured memo $$ \boldsymbol{x}^{m} $$, which is modeled as $$ \boldsymbol{x}^{m} = f(\boldsymbol{x}^h), \boldsymbol{x}^h' = g(\boldsymbol{x}^q, \boldsymbol{x}^{m}), $$ with $$ f(<em>) $$ and $$ g(</em>) $$ as the memo build-up and retrieval functions respectively. Therefore, GQA in our MemoChat can be reformulated as as $$ \hat{Y} = \arg\max_{Y} p(Y|\boldsymbol{x}^q, \boldsymbol{x}^h'). $$</p>
<h4>3.2 MemoChat</h4>
<p>Figure 2 depicts the overall framework of our proposed MemoChat pipeline, which assigns the chatbot with a "memorization-retrieval-response" loop of inner thinking. In the left of Figure 2, the user starts with the topic of "Quantum Physics", then ends with "Business Etiquette" topic after multiple turns of the conversation. Very different from traditional methods that retrieve directly on these accumulated dialogues, the chatbot under our MemoChat, as shown on the upper-right of Figure 2, will automatically build and update a structured on-the-fly memo, storing past dialogues in categories. Then, the retrieval is conducted over all recordings according to their topics and summaries. In the bottom-left, Figure 2 illustrates an occurrence in which the chatbot is required to link the concept of "Quantum Physics" aforementioned in the very beginning of the conversation. The chatbot manages to recall related evidences, and generate a consistent answer in bottom-right Figure 2.</p>
<p>By allowing chatbots to reorganize the dialogue histories according to different topics, summarize</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" />Figure 3: Instruction templates in MemoChat (better view in colors). Three instructions are carefully designed to serve MemoChat inner thinking pipeline including “Memo Writing”, “Memo Retrieval” and “Chat w/ Memo”, partitioned by gray boxes. The organization of all instructions consists of general task introduction (green area), main body of task-oriented inputs (bisque area), and detailed task explanation (yellow area). Ground-truth answers are provided during fine-tuning (blue area). More specific examples are provided in Appendix B.</p>
<p>the partial dialogue faithfully, and retrieve one or more query-related evidences quickly, MemoChat becomes a simple-yet-effective pipeline to guide chatbots to generate consistent responses.</p>
<p>Instruction Structure. “Memorization-retrieval-response” mechanism stores the past dialogues in memo $\boldsymbol{x}^{\boldsymbol{m}}$, recalls query-related evidences $\boldsymbol{x}^{\boldsymbol{h}^{\prime}}$ from memo, and generates response $\hat{Y}$ based on retrieved evidences. Different from previous literature <em>Lee et al. (2023); Zhong et al. (2023)</em> requiring additional complex modules, we reorganize the dialogue histories into structured on-the-fly memos, such that the basic LLM-powered chatbots can simultaneously handle the memo. Thus, we develop three instructions for each stage as shown in Figure 3. Overall, all instructions follow similar structure: (1) description of each stage task (green area); (2) main body of task-oriented inputs (bisque area); (3) explanation of each task (yellow area). The combination of three serves as the input prompts of the chatbot and ground-truth answers are provided (blue area) for fine-tuning.</p>
<p>Herein, the “Memo Writing” task asks the chatbot to read a task conversation, partition the conversation based on all possible topics, write short summaries for every sub-dialogues, and write the above results to the memo in a JSON format with each instance including keys of “topic”, “summary” and “dialogues”. We consolidate the task explanation with Chain-of-Thought (CoT) <em>Wei et al. (2022b)</em> and In-Context Learning (ICL) <em>Dong et al. (2023)</em>. The “Memo Retrieval” task asks the chatbot to read a user query and a series of topic options, select out one or more related options and report option ID with a special format. Inspired by previous literature <em>Rajpurkar et al. (2018)</em>, one “None of the others (NOTO)” option is randomly inserted in the candidate options, given that the user may arbitrarily launch a new topic. To support a fast implementation, we retrieve related contents only upon the “topics” and “summaries” from the on-the-fly memo. Finally, the “Chat w/ Memo” task feeds the latest query, prefixed with retrieved evidences and recent dialogues, to the chatbot. With these related evidences, the chatbot can now produce more consistent response even in the long-range open-domain conversations.</p>
<p>Dataset Reconstruction. To implement our Memo pipeline, we reconstruct 10k instruction data from three public dialogue datasets including TopicoQA <em>Adlakha et al. (2022)</em>, DialogSum <em>Chen et al. (2021)</em> and Alpaca-GPT4 <em>Ghosal et al. (2023)</em>.</p>
<table>
<thead>
<tr>
<th></th>
<th>Data Size</th>
<th>Avg. Tokens</th>
<th>Sources</th>
</tr>
</thead>
<tbody>
<tr>
<td>Memo Writing</td>
<td>3,046</td>
<td>951.92</td>
<td>1,790 from T*</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td>1,256 from D*</td>
</tr>
<tr>
<td>Memo Retrieval</td>
<td>3,654</td>
<td>329.86</td>
<td>1,719 from T*</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td>1,935 from D*</td>
</tr>
<tr>
<td>Chat w/ Memo</td>
<td>3,300</td>
<td>273.04</td>
<td>1,602 from D*</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td>1,698 from A*</td>
</tr>
</tbody>
</table>
<p>Table 1: Statistics of our instruction training sets. "Avg. Tokens" stands for the average token number of each instruction. T<em>, D</em> and A* stand for TopicoQA, DialogSum and Alpaca-GPT4 dataset, respectively.</p>
<p>TopicoQA covers 3.9k conversations that involve 4 topics and 13 turns of dialogue on average. DialogSum covers 13k conversations that focus on 1 topic across 5.5 turns on average, and provides manual summary for each conversation. Both TopicoQA and DialogSum use human-human conversations in spoken style, and focus on daily life scenes. While Alpaca-GPT4 is a human-chatbot 1-turn conversation dataset created by running GPT4 API [OpenAI2023GPT4AR] on the Alpaca dataset [taori2023alpaca]. It covers more sophisticated topics in written style. We sample 1.6k cases to balance the instructions from another two datasets.</p>
<p>Table 1 reports the statistics and sources of the three instruction training sets. The "Memo Writing" set consists of 1,790 instances from TopicoQA and 1,256 samples from DialogSum. Given the unavailability of "summary" information, we only construct "topic" and "dialogues" in the memo for conversations from TopicoQA dataset. Considering only one topic in each conversation of DialogSum, we combine a few variable set of conversations to mimic a multi-topic long-range conversation. Similar strategies are adopted to form "Memo Retrieval" set that contains 1,719 and 1,935 instructions from TopicoQA and DialogSum, respectively. For each retrieval instruction, its query sentence is made up with several sequential sentences from original conversation and related topics are extracted as options. The "NOTO" option is inserted with a probability of 10%. The "Chat w/ Memo" set comprises 1,602 and 1,698 instructions from DialogSum and Alpaca-GPT4. The former aims at generating instructions with related evidences, and the latter generates instructions without any related evidences to fit with unseen topics.</p>
<p>In conclusion, we succeed in building three training sets to achieve our "memorization-retrieval-response" loop. Besides, a total of 400 evaluation instructions are similarly constructed from the testing sets of the source datasets, including 160, 82 and 65 samples for "Memo Writing", "Memo Retrieval" and "Chat w/ Memo", respectively.
Challenges. To our best knowledge, we are the first to build such a group of systematic sequential instructions. We report major challenges for any potential future follow-ups. Some specific examples are available in Appendix C.</p>
<p>The first challenge faceed is "Prompt Copy" in which the LLMs found a special shortcut to directly copy format examples in the prompts as the answer. To avoid this, we suggest to use dummy variables to replace any numerical values to avoid possible answer leaking. The second challenge refers to "Catastrophic Forgetting" in which the chatbots lean towards generating early-stopped or repeated answers if the volume of "Chat w/ Memo" is less than "Memo Writing" or "Memo Retrieval". A balanced volume is encouraged by mixing with certain amount of dialogue corpus to maintain the chatting ability [ghosal2023chatbots]. We consider Alpaca-GPT4 in our construction. The third challenge lies in "Prompt Misplacement" in which the chatbots performance degrade when swapping the input order of the main body of task-oriented inputs (bisque area in Figure 3) and explanation of each task (yellow area in Figure 3). Task explanation attached at the end is crucial for chatbots to understand instructions better [liu2023gpt4].</p>
<h2>4 Experiments</h2>
<h3>4.1 Experimental Setting</h3>
<p>Datasets. Except for the reconstructed instruction datasets, we also invite three experts to manually annotate a new long-range open-domain conversation dataset to validate the response consistency. The annotated dataset, referred to as MT-Bench+, is built based on MT-Bench [zheng2023memo], a human-chatbot testing dataset with a total of 80 two-turn open-domain questions, covering topics such as "STEM exams" and "literary writing". This is achieved by: (1) expanding each of the twoturn questions to a four-turn version by the experts; (2) sampling $2 \sim 4$ questions from every $3 \sim 4$ of four-turn versions to form a long-range chatting stream with a total of $12 \sim 15$ turns; (3) adding longrange questions to the end of the chatting streams for "Retrospection", "Continuation" and "Conjunction" by the experts. Herein, "Retrospection" is restricted to the aforementioned terms related ques-</p>
<p>tions (e.g., span extraction), "Continuation" makes a further task based on previous knowledge (e.g., story completion), and "Conjunction" is to ask a cross-topic response (e.g., redo task 2 in reference to task 1). We build 18 questions for each of the three types in MT-Bench+, 54 in total.
Baselines. We consider the popular Fastchat-T5-3b and Vicuna-7b/13b/33b (Zheng et al., 2023), which are human-aligned versions of Flan-T5 (Chung et al., 2022) and Llama (Touvron et al., 2023) with massive human-bot dialogues (Kim et al., 2023), respectively, as the baseline models. We further finetune them on our reconstructed instruction data to form our MemoChat. For all experiments, we set the text window to 2 k to follow off-the-shelf conventions (Workshop, 2023). However, our MemoChat keeps the flexibility and scalability to larger windows as chatbots can automatically adjust memory range. As for downstream consistency testing, one public API and two open-source memory-assistant chatbots are included as baselines:</p>
<ul>
<li>ChatGPT-2k. We use the same 2 k text window to arrange the input of gpt-3.5-turbo API.</li>
<li>MPC-ChatGPT. Plug MPC (Lee et al., 2023) pipeline with ChatGPT-2k.</li>
<li>MemoryBank-ChatGPT. Plug MemoryBank (Zhong et al., 2023) with ChatGPT-2k.
GPT4 API is excluded as GPT4-generated data are included in the training set, and we rely on GPT4 for evaluation in downstream chatting test.
Metrics. We adopt four groups of metrics to evaluate the performance of intermediate instruction tuning and the downstream consistency checking. Since we ask chatbots to partition dialogue histories to build memo, in reference to Name Entity Recognition (NER) (Ding et al., 2021), we therefore evaluate generated topics and summaries in "Memo Writing" task with precision $(\boldsymbol{P})$, recall $(\boldsymbol{R})$ and micro F1-score $\left(\boldsymbol{F}<em _mathbf_1="\mathbf{1">{\mathbf{1}}\right)$ only when predicted subsession range and ground truth matches. In detail, the $\boldsymbol{P}$ score of topic checking uses exact match following NER, while the $\boldsymbol{P}$ score of summary checking is replaced with float BertScore (Zhang et al., 2020) number varying between 0 to 1 . We use regular F1-score $\left(\boldsymbol{F}</em>\right)$ on "Memo Retrieval", and use BertScore to check the semantic similarity between generated response and golden references in "Chat w/ Memo" task. As for downstream response consistency checking, inspired by LLM-as-a-judge strategy (Zheng et al., 2023), we select}</li>
</ul>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Fastchat- <br> T5-3B</th>
<th style="text-align: center;">Vicuna-7B</th>
<th style="text-align: center;">Vicuna-13B</th>
<th style="text-align: center;">Vicuna-33B</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Batch</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">16</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">4</td>
</tr>
<tr>
<td style="text-align: center;">Accumulations</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">4</td>
</tr>
<tr>
<td style="text-align: center;">HyperParams</td>
<td style="text-align: center;">Epoch=3, Global Batch=128, Seq Len=2048, <br> Lr=2e-5, Warmup Ratio=0.04, Gen Temperature=0.2</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">CPU RAM (GB)</td>
<td style="text-align: center;">73.01</td>
<td style="text-align: center;">189.49</td>
<td style="text-align: center;">356.42</td>
<td style="text-align: center;">790.57</td>
</tr>
<tr>
<td style="text-align: center;">Per GPU (GB)</td>
<td style="text-align: center;">37.12</td>
<td style="text-align: center;">33.22</td>
<td style="text-align: center;">37.29</td>
<td style="text-align: center;">38.96</td>
</tr>
<tr>
<td style="text-align: center;">Training (10k)</td>
<td style="text-align: center;">1.04 h</td>
<td style="text-align: center;">0.98 h</td>
<td style="text-align: center;">2.35 h</td>
<td style="text-align: center;">5.74 h</td>
</tr>
</tbody>
</table>
<p>Table 2: Hyperparameters and training cost. We use full-param training instead of light training such as LoRA (Hu et al., 2022).
out related dialogue histories, and the long-range checking question with corresponding response, then use GPT4 to score the faithfulness with an integer scale $1 \sim 100$. It has been explored in (Pan et al., 2023) to show that credible LLMs such as GPT4 have a comparable analysis ability to human. Hyperparameters. Our training details are reported in Table 2. We fine-tune all models on one node server equipped with 900G CPU RAM and 8 A100 40G GPUs. Although single batch size is set in difference due to model size, we maintain a same global batch. Meanwhile, all other hyperparameters are set as the same. The optimizer is AdamW (Loshchilov and Hutter, 2019) and the scheduler is WarmupDecayLR (Goyal et al., 2017). We incorporate Deepspeed (Ren et al., 2021) and Flash Attention (Dao et al., 2022) to improve the training efficiency. Hardware consumption and time cost are reported as well. Although the training efficiency can be improved, we keep related parameters consistent with Alpaca and Vicuna (Taori et al., 2023; Zheng et al., 2023) for stable training.</p>
<h3>4.2 Experimental Results</h3>
<p>Memo-usage Ability. We report results of tuning open-source LLMs to obtain memo-usage ability. Table 3 coincides the conclusion that larger LLMs are better zero- and few-shot learners (Brown et al., 2020). Largest ChatGPT and GPT4 models show a strong zero-shot inference ability, reaching over 60 and 50 scores on retrieval and chatting tasks, respectively. In contrast, smaller LLMs cannot fully understand prompts, especially the dummy templates such as "N#M" in "Memo Retrieval". Among open-source LLMs, it is clearly that larger models have better initial understanding towards new instructions, although the performance gap keeps in a low level range from 0.3 to $4.0 \boldsymbol{F}<em _mathbf_1="\mathbf{1">{\mathbf{1}} *$ and $\boldsymbol{F}</em> #$ scores on "Memo Writing" and "Retrieval".}</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Memo Writing</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Memo <br> Retrieval</th>
<th style="text-align: center;">Chat with Memo</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Setting</td>
<td style="text-align: center;">Model</td>
<td style="text-align: center;">Topic</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Summary</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$P *$</td>
<td style="text-align: center;">$R *$</td>
<td style="text-align: center;">$F_{1} *$</td>
<td style="text-align: center;">$P #$</td>
<td style="text-align: center;">$R #$</td>
<td style="text-align: center;">$F_{1} #$</td>
</tr>
<tr>
<td style="text-align: center;">Zero-shot</td>
<td style="text-align: center;">GPT4</td>
<td style="text-align: center;">7.28</td>
<td style="text-align: center;">9.84</td>
<td style="text-align: center;">8.37</td>
<td style="text-align: center;">43.72</td>
<td style="text-align: center;">48.01</td>
<td style="text-align: center;">45.76</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">ChatGPT</td>
<td style="text-align: center;">0.52</td>
<td style="text-align: center;">1.25</td>
<td style="text-align: center;">0.74</td>
<td style="text-align: center;">5.37</td>
<td style="text-align: center;">12.27</td>
<td style="text-align: center;">7.47</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Fastchat-T5-3B</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">0.00</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Vicuna-7B</td>
<td style="text-align: center;">0.34</td>
<td style="text-align: center;">0.54</td>
<td style="text-align: center;">0.42</td>
<td style="text-align: center;">1.64</td>
<td style="text-align: center;">4.48</td>
<td style="text-align: center;">2.40</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Vicuna-13B</td>
<td style="text-align: center;">0.61</td>
<td style="text-align: center;">1.07</td>
<td style="text-align: center;">0.78</td>
<td style="text-align: center;">3.53</td>
<td style="text-align: center;">7.26</td>
<td style="text-align: center;">4.75</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Vicuna-33B</td>
<td style="text-align: center;">1.31</td>
<td style="text-align: center;">2.15</td>
<td style="text-align: center;">1.63</td>
<td style="text-align: center;">4.21</td>
<td style="text-align: center;">7.96</td>
<td style="text-align: center;">5.51</td>
</tr>
<tr>
<td style="text-align: center;">$10 \%$ Training <br> (1,000)</td>
<td style="text-align: center;">MemoChat-Fastchat-T5-3B</td>
<td style="text-align: center;">5.34</td>
<td style="text-align: center;">5.01</td>
<td style="text-align: center;">5.17</td>
<td style="text-align: center;">26.18</td>
<td style="text-align: center;">18.73</td>
<td style="text-align: center;">21.84</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">MemoChat-Vicuna-7B</td>
<td style="text-align: center;">10.77</td>
<td style="text-align: center;">11.27</td>
<td style="text-align: center;">11.01</td>
<td style="text-align: center;">16.98</td>
<td style="text-align: center;">17.31</td>
<td style="text-align: center;">17.14</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">MemoChat-Vicuna-13B</td>
<td style="text-align: center;">26.34</td>
<td style="text-align: center;">26.30</td>
<td style="text-align: center;">26.32</td>
<td style="text-align: center;">42.88</td>
<td style="text-align: center;">42.88</td>
<td style="text-align: center;">42.88</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">MemoChat-Vicuna-33B</td>
<td style="text-align: center;">24.77</td>
<td style="text-align: center;">28.98</td>
<td style="text-align: center;">26.71</td>
<td style="text-align: center;">41.32</td>
<td style="text-align: center;">44.16</td>
<td style="text-align: center;">42.69</td>
</tr>
<tr>
<td style="text-align: center;">$100 \%$ Training <br> (10,000)</td>
<td style="text-align: center;">MemoChat-Fastchat-T5-3B</td>
<td style="text-align: center;">38.57</td>
<td style="text-align: center;">35.60</td>
<td style="text-align: center;">37.02</td>
<td style="text-align: center;">63.87</td>
<td style="text-align: center;">63.25</td>
<td style="text-align: center;">63.56</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">MemoChat-Vicuna-7B</td>
<td style="text-align: center;">45.08</td>
<td style="text-align: center;">52.58</td>
<td style="text-align: center;">43.79</td>
<td style="text-align: center;">57.27</td>
<td style="text-align: center;">57.84</td>
<td style="text-align: center;">57.55</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">MemoChat-Vicuna-13B</td>
<td style="text-align: center;">53.83</td>
<td style="text-align: center;">52.77</td>
<td style="text-align: center;">53.30</td>
<td style="text-align: center;">64.20</td>
<td style="text-align: center;">64.20</td>
<td style="text-align: center;">64.20</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">MemoChat-Vicuna-33B</td>
<td style="text-align: center;">57.07</td>
<td style="text-align: center;">57.07</td>
<td style="text-align: center;">57.07</td>
<td style="text-align: center;">67.99</td>
<td style="text-align: center;">68.65</td>
<td style="text-align: center;">68.32</td>
</tr>
</tbody>
</table>
<p>Table 3: Evaluation results of intermediate instruction tuning on LLMs with different scales. We report all scores in the unit of $\% . F_{1} *, F_{1} #$ and $F_{1}$ refer to NER-based metrics, BertScore-fused NER-based metrics and regular metrics, respectively. So do the $P$ and $R$ scores. 1,000 and 10,000 refer to the scale of data used for fine-tuning.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Auto Rating by GPT4-4k (1 to 100)</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Type</td>
<td style="text-align: center;">Model</td>
<td style="text-align: center;">Retrospection</td>
<td style="text-align: center;">Continuation</td>
<td style="text-align: center;">Conjunction</td>
<td style="text-align: center;">Average</td>
</tr>
<tr>
<td style="text-align: center;">2k text window</td>
<td style="text-align: center;">ChatGPT-2k</td>
<td style="text-align: center;">52.11</td>
<td style="text-align: center;">55.33</td>
<td style="text-align: center;">48.22</td>
<td style="text-align: center;">51.89</td>
</tr>
<tr>
<td style="text-align: center;">Public APIs with additional memory storage and recall</td>
<td style="text-align: center;">MPC-ChatGPT (Lee et al., 2023)</td>
<td style="text-align: center;">53.00</td>
<td style="text-align: center;">61.22</td>
<td style="text-align: center;">49.33</td>
<td style="text-align: center;">54.52</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">MemoryBank-ChatGPT (Zhong et al., 2023)</td>
<td style="text-align: center;">23.39</td>
<td style="text-align: center;">55.28</td>
<td style="text-align: center;">48.67</td>
<td style="text-align: center;">42.44</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">MemoChat-ChatGPT</td>
<td style="text-align: center;">66.28</td>
<td style="text-align: center;">73.50</td>
<td style="text-align: center;">72.50</td>
<td style="text-align: center;">70.76</td>
</tr>
<tr>
<td style="text-align: center;">Fine-tuned open-source LLMs</td>
<td style="text-align: center;">MemoChat-Vicuna-13B (1k)</td>
<td style="text-align: center;">64.78</td>
<td style="text-align: center;">56.06</td>
<td style="text-align: center;">51.67</td>
<td style="text-align: center;">57.50</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">MemoChat-Vicuna-33B (1k)</td>
<td style="text-align: center;">56.61</td>
<td style="text-align: center;">55.11</td>
<td style="text-align: center;">50.94</td>
<td style="text-align: center;">54.22</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">MemoChat-Fastchat-T5-3B (10k)</td>
<td style="text-align: center;">30.50</td>
<td style="text-align: center;">29.94</td>
<td style="text-align: center;">21.56</td>
<td style="text-align: center;">27.33</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">MemoChat-Vicuna-7B (10k)</td>
<td style="text-align: center;">61.17</td>
<td style="text-align: center;">61.22</td>
<td style="text-align: center;">50.78</td>
<td style="text-align: center;">57.72</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">MemoChat-Vicuna-13B (10k)</td>
<td style="text-align: center;">62.28</td>
<td style="text-align: center;">63.17</td>
<td style="text-align: center;">52.83</td>
<td style="text-align: center;">59.43</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">MemoChat-Vicuna-33B (10k)</td>
<td style="text-align: center;">69.00</td>
<td style="text-align: center;">65.00</td>
<td style="text-align: center;">50.56</td>
<td style="text-align: center;">61.52</td>
</tr>
</tbody>
</table>
<p>Table 4: Evaluation results of downstream response consistency. Except for the aforementioned baselines, MemoChat-ChatGPT combines public APIs with MemoChat baseline. "Fine-tuned open-source LLMs" models are fine-tuned with designed instructions. 1 k and 10 k refer to the scale of data used in accordance with Table 3.</p>
<p>To check the learning ability of open-source LLMs, two groups of settings are taken. The $10 \%$ and $100 \%$ training groups refer to fine-tune LLMs with 1 k and 10 k samples, respectively. It can be observed that, with only 1 k samples, larger LLMs MemoChat-Vicuna-13B and -33B can familiarize the instructions better than smaller LLMs MemoChat-Fastchat-T5-3B and -Vicuna-7B, raising significant performance gaps by $15 \sim 30$ scores on $\boldsymbol{F}<em _mathbf_1="\mathbf{1">{\mathbf{1}} *$ and $\boldsymbol{F}</em> #$ in "Memo Writing". Also, except for "Memo Retrieval", MemoChat-Vicuna13B maintains similar performance to -33B in this setting, which may be due to insufficient training. When training set is scaled to 10 k , the largest MemoChat-Vicuna-33B has the most potential to push highest scores over all tasks. In general, all models fully fine-tuned with 10k training data show better performance than ChatGPT/GPT4, and their performances are gradually improved when model scale increases. The only reversal happens on MemoChat-Vicuna-7B and -T5-3B, where -7B gets lower scores on summary generation and retrieval.}</p>
<p>Response Consistency. As in Table 3, MemoChat-Vicuna-13B and -33B are able to reach similar zeroshot performance of ChatGPT and GPT4 fine-tuned with 1 k instruction. Thus, we suppose MemoChat-Vicuna-13B (1k), -33B (1k) and all MemoChat $(10 \mathrm{k})$ models have obtained self-use ability of on-the-fly memos, and to be involved in consistency testing. We report results of response con-</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Case analysis of different types of testing questions with different chatbots. We show the key points (in bold) in the answers, and some parts of the answer context are replaced by ellipsis due to length limitation.
sistency checking in Table 4: (1) Compared with previous baselines mostly focusing on persona or user portrait summary, MemoChat guides the chatbots to produce a general summary, which significantly improves the downstream performance by $7.0 \sim 19.5$ score on average, as shown by the results of MPC-ChatGPT, MemoryBank-ChatGPT and MemoChat-ChatGPT; (2) Except for T5-3B, all fine-tuned MemoChat models show better performance than ChatGPT-2k by 2.4 to 9.7 scores on average, proving the effectiveness of reconstructed instructions; (3) When the LLMs are not fully fine-tuned, MemoChat-Vicuna-13B (1k) even show higher performance than -33B version by 3.3 score on average. The less-finetuned -13B model gets a pretty good score 64.78 on the retrospection questions, due to its tendency to generate short answers and thus memorize more. (4) When all open-source LLMs are fully fine-tuned, the average performance is improved as model scale grows. However, the performances on conjunction questions remain similar. This may be due to the limitation of long-form question answering ability, while ChatGPT is an expert (Liu et al., 2023a).
Case Analysis. We report a specific case in Figure 4. The case starts with discussions on "Machine Learning" and "Antitrust Laws" in first 8 turns. After over 7 turns of other topics, we raise two long-range questions. The first retrospection question asks about the number of "types of machine learning", which requires to recall relevant "Machine Learning" topics and accurately count the
term number in the first local question. ChatGPT2 k can not correctly answer the question due to text window limits, while both MemoChat-based chatbots work. The second question belongs to conjunction type that cross-topic retrieval and understanding are required. ChatGPT-2k completely misinterpretes it. While MemoChat-ChatGPT precisely links antitrust laws to machine learning. The MemoChat-Vicuna-33B (10k) correctly identifies that the "Sherman Antitrust Act", the specific name of an antitrust law, applies to the machine learning industry, although it incorrectly states "the European Commission fined Google" under the Sherman Act which is an American legislation, with the European Commission operating instead under EU law. Thus, GPT4 scores ChatGPT-2k, MemoChatChatGPT, and MemoChat-Vicuna-33B (10k), with 1,100 , and 90 , respectively.</p>
<h2>5 Conclusion</h2>
<p>We proposed MemoChat, an instruction tuning pipeline to teach LLMs to self-use on-the-fly structured memos towards consistent long-range open-domain conversations, which can be decomposed as "memorization-retrieval-response" loops. We carefully designed three trainable instructions based on public datasets for structured writing, fast retrieval, and interations with memos. An expertannotated downstream evaluation set was curated and provided. Experiments on three testing scenarios and various LLM-based chatbots demonstrated the effectiveness of MemoChat.</p>
<h2>References</h2>
<p>Vaibhav Adlakha, Shehzaad Dhuliawala, Kaheer Suleman, Harm de Vries, and Siva Reddy. 2022. TopiOCQA: Open-domain conversational question answering with topic switching. Transactions of the Association for Computational Linguistics, 10.</p>
<p>Iz Beltagy, Matthew E Peters, and Arman Cohan. 2020. Longformer: The long-document transformer. arXiv:2004.05150.</p>
<p>Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in Neural Information Processing Systems, 33.</p>
<p>Shouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong Tian. 2023. Extending context window of large language models via positional interpolation. arXiv:2306.15595.</p>
<p>Yulong Chen, Yang Liu, Liang Chen, and Yue Zhang. 2021. DialogSum: A real-life scenario dialogue summarization dataset. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021.</p>
<p>Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. 2022. Scaling instruction-finetuned language models. arXiv:2210.11416.</p>
<p>Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher Ré. 2022. Flashattention: Fast and memory-efficient exact attention with io-awareness. Advances in Neural Information Processing Systems, 35.</p>
<p>Emily Dinan, Varvara Logacheva, Valentin Malykh, Alexander Miller, Kurt Shuster, Jack Urbanek, Douwe Kiela, Arthur Szlam, Iulian Serban, Ryan Lowe, Shrimai Prabhumoye, Alan W Black, Alexander Rudnicky, Jason Williams, Joelle Pineau, Mikhail Burtsev, and Jason Weston. 2019a. The second conversational intelligence challenge (convai2).</p>
<p>Emily Dinan, Stephen Roller, Kurt Shuster, Angela Fan, Michael Auli, and Jason Weston. 2019b. Wizard of Wikipedia: Knowledge-powered conversational agents. In Proceedings of the International Conference on Learning Representations (ICLR).</p>
<p>Jiayu Ding, Shuming Ma, Li Dong, Xingxing Zhang, Shaohan Huang, Wenhui Wang, Nanning Zheng, and Furu Wei. 2023. Longnet: Scaling transformers to 1,000,000,000 tokens.</p>
<p>Ning Ding, Guangwei Xu, Yulin Chen, Xiaobin Wang, Xu Han, Pengjun Xie, Haitao Zheng, and Zhiyuan Liu. 2021. Few-NERD: A few-shot named entity recognition dataset. In Proceedings of the 59th Annual Meeting of the Association for Computational</p>
<p>Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 31983213.</p>
<p>Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, Lei Li, and Zhifang Sui. 2023. A survey on in-context learning.</p>
<p>Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, and Jie Tang. 2022. GLM: General language model pretraining with autoregressive blank infilling. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers).</p>
<p>Song Feng, Hui Wan, Chulaka Gunasekara, Siva Patel, Sachindra Joshi, and Luis Lastras. 2020. doc2dial: A goal-oriented document-grounded dialogue dataset. In Proceedings of Empirical Methods in Natural Language Processing (EMNLP).</p>
<p>Deepanway Ghosal, Yew Ken Chia, Navonil Majumder, and Soujanya Poria. 2023. Flacuna: Unleashing the problem solving power of vicuna using flan finetuning. arXiv:2307.02053.</p>
<p>Google. 2023. Palm 2 technical report. arXiv:2305.10403.</p>
<p>Priya Goyal, Piotr Dollár, Ross Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola, Andrew Tulloch, Yangqing Jia, and Kaiming He. 2017. Accurate, large minibatch sgd: Training imagenet in 1 hour. arXiv:1706.02677.</p>
<p>Prakhar Gupta, Cathy Jiao, Yi-Ting Yeh, Shikib Mehri, Maxine Eskenazi, and Jeffrey Bigham. 2022. InstructDial: Improving zero and few-shot generalization in dialogue through instruction tuning. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing.</p>
<p>Chenxu Hu, Jie Fu, Chenzhuang Du, Simian Luo, Junbo Zhao, and Hang Zhao. 2023. Chatdb: Augmenting llms with databases as their symbolic memory.</p>
<p>Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2022. Lora: Low-rank adaptation of large language models. In International Conference on Learning Representations.</p>
<p>Jeff Johnson, Matthijs Douze, and Hervé Jégou. 2019. Billion-scale similarity search with GPUs. IEEE Transactions on Big Data, 7(3):535-547.</p>
<p>Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. 2020. Dense passage retrieval for opendomain question answering. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP).</p>
<p>Sungdong Kim, Sanghwan Bae, Jamin Shin, Soyoung Kang, Donghyun Kwak, Kang Min Yoo, and Minjoon Seo. 2023. Aligning large language models through synthetic feedback. arXiv:2305.13735.</p>
<p>Nikita Kitaev, Łukasz Kaiser, and Anselm Levskaya. 2020. Reformer: The efficient transformer. In International Conference on Learning Representations.</p>
<p>Mojtaba Komeili, Kurt Shuster, and Jason Weston. 2022. Internet-augmented dialogue generation. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers).</p>
<p>Brandon Kynoch and Hugo Latapie. 2023. Recallm: An architecture for temporal context understanding and question answering.</p>
<p>Gibbeum Lee, Volker Hartmann, Jongho Park, Dimitris Papailiopoulos, and Kangwook Lee. 2023. Prompted LLMs as chatbot modules for long open-domain conversation. In Findings of the Association for Computational Linguistics: ACL 2023.</p>
<p>Dacheng Li, Rulin Shao, Anze Xie, Ying Sheng, Lianmin Zheng, Joseph E. Gonzalez, Ion Stoica, Xuezhe Ma, and Hao Zhang. 2023. How long can opensource llms truly promise on context length?</p>
<p>Hanmeng Liu, Ruoxi Ning, Zhiyang Teng, Jian Liu, Qiji Zhou, and Yue Zhang. 2023a. Evaluating the logical reasoning ability of chatgpt and gpt-4.</p>
<p>Nelson F Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. 2023b. Lost in the middle: How language models use long contexts. arXiv:2307.03172.</p>
<p>Xiao Liu, Hanyu Lai, Hao Yu, Yifan Xu, Aohan Zeng, Zhengxiao Du, Peng Zhang, Yuxiao Dong, and Jie Tang. 2023c. Webglm: Towards an efficient webenhanced question answering system with human preferences.</p>
<p>Ilya Loshchilov and Frank Hutter. 2019. Decoupled weight decay regularization. In International Conference on Learning Representations.</p>
<p>Aaron Mueller, Kanika Narang, Lambert Mathias, Qifan Wang, and Hamed Firooz. 2023. Meta-training with demonstration retrieval for efficient few-shot learning.</p>
<p>OpenAI. 2023. Gpt-4 technical report.
Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. 2022. Training language models to follow instructions with human feedback. In Thirty-Sixth Conference on Neural Information Processing Systems.</p>
<p>Alexander Pan, Jun Shern Chan, Andy Zou, Nathaniel Li, Steven Basart, Thomas Woodside, Hanlin Zhang, Scott Emmons, and Dan Hendrycks. 2023. Do the rewards justify the means? measuring trade-offs between rewards and ethical behavior in the machiavelli benchmark. In International Conference on Machine Learning. PMLR.</p>
<p>Zhen Qin, Rolf Jagerman, Kai Hui, Honglei Zhuang, Junru Wu, Jiaming Shen, Tianqi Liu, Jialu Liu, Donald Metzler, Xuanhui Wang, and Michael Bendersky. 2023. Large language models are effective text rankers with pairwise ranking prompting.</p>
<p>Pranav Rajpurkar, Robin Jia, and Percy Liang. 2018. Know what you don't know: Unanswerable questions for SQuAD. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics.</p>
<p>Jie Ren, Samyam Rajbhandari, Reza Yazdani Aminabadi, Olatunji Ruwase, Shuangyan Yang, Minjia Zhang, Dong Li, and Yuxiong He. 2021. {ZeROOffload}: Democratizing {Billion-Scale} model training. In 2021 USENIX Annual Technical Conference.</p>
<p>Victor Sanh, Albert Webson, Colin Raffel, Stephen H Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja, et al. 2022. Multitask prompted training enables zeroshot task generalization. In The Tenth International Conference on Learning Representations.</p>
<p>Kurt Shuster, Mojtaba Komeili, Leonard Adolphs, Stephen Roller, Arthur Szlam, and Jason Weston. 2022a. Language models that seek for knowledge: Modular search \&amp; generation for dialogue and prompt completion. In Findings of the Association for Computational Linguistics: EMNLP 2022.</p>
<p>Kurt Shuster, Jing Xu, Mojtaba Komeili, Da Ju, Eric Michael Smith, Stephen Roller, Megan Ung, Moya Chen, Kushal Arora, Joshua Lane, et al. 2022b. Blenderbot 3: a deployed conversational agent that continually learns to responsibly engage. arXiv:2208.03188.</p>
<p>Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, and Yunfeng Liu. 2022. Roformer: Enhanced transformer with rotary position embedding.</p>
<p>Weiwei Sun, Lingyong Yan, Xinyu Ma, Pengjie Ren, Dawei Yin, and Zhaochun Ren. 2023. Is chatgpt good at search? investigating large language models as re-ranking agent.</p>
<p>Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B Hashimoto. 2023. Stanford alpaca: An instruction-following llama model.</p>
<p>Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023. Llama: Open and efficient foundation language models. arXiv:2302.13971.</p>
<p>Szymon Tworkowski, Konrad Staniszewski, Mikołaj Pacek, Yuhuai Wu, Henryk Michalewski, and Piotr Miłoś. 2023. Focused transformer: Contrastive training for context scaling. arXiv:2307.03170.</p>
<p>Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, and Quoc V Le. 2022a. Finetuned language models are zero-shot learners. In The Tenth International Conference on Learning Representations.</p>
<p>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. 2022b. Chain-of-thought prompting elicits reasoning in large language models. In 36th Conference on Neural Information Processing Systems.</p>
<p>BigScience Workshop. 2023. Bloom: A 176bparameter open-access multilingual language model.</p>
<p>Jing Xu, Arthur Szlam, and Jason Weston. 2022a. Beyond goldfish memory: Long-term open-domain conversation. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics.</p>
<p>Xinchao Xu, Zhibin Gou, Wenquan Wu, Zheng-Yu Niu, Hua Wu, Haifeng Wang, and Shihang Wang. 2022b. Long time no see! open-domain conversation with long-term persona memory. In Findings of the Association for Computational Linguistics: ACL 2022.</p>
<p>Jingfeng Yang, Hongye Jin, Ruixiang Tang, Xiaotian Han, Qizhang Feng, Haoming Jiang, Bing Yin, and Xia Hu. 2023. Harnessing the power of llms in practice: A survey on chatgpt and beyond. arXiv:2304.13712.</p>
<p>Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger, and Yoav Artzi. 2020. Bertscore: Evaluating text generation with bert. In International Conference on Learning Representations.</p>
<p>Kang Zhao, Wei Liu, Jian Luan, Minglei Gao, Li Qian, Hanlin Teng, and Bin Wang. 2023a. Unimc: A unified framework for long-term memory conversation via relevance representation learning.</p>
<p>Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et al. 2023b. A survey of large language models. arXiv:2303.18223.</p>
<p>Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. 2023. Judging llm-as-a-judge with mt-bench and chatbot arena. arXiv:2306.05685.</p>
<p>Wanjun Zhong, Lianghong Guo, Qiqi Gao, He Ye, and Yanlin Wang. 2023. Memorybank: Enhancing large language models with long-term memory.</p>
<p>Hao Zhou, Chujie Zheng, Kaili Huang, Minlie Huang, and Xiaoyan Zhu. 2020. KdConv: A Chinese multi-domain dialogue dataset towards multi-turn knowledge-driven conversation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics.</p>
<h2>A Basic Published Datasets</h2>
<p>In Table 5, we draw examples of the basic published datasets that used for instruction reconstruction in our MemoChat pipeline.</p>
<h2>TopicoQA</h2>
<p>A: of the four galilean moons which is the closest to Jupiter
B: Io
A: who discovered it?
B: Galileo Galilei
・. $\cdot$
A: does it belong to any album?
B: 'A Night at the Opera"
'topic': 'Io (moon)', 'start': 1, 'end': 2
'topic': 'Galilean moons', 'start': 3, 'end': 4
'topic': 'Bohemian Rhapsody', 'start': 13, 'end': 20</p>
<h2>DialogSum</h2>
<p>A: Ms. Dawson, I need you to take a dictation for me.
B: Yes, sir.
・. $\cdot$
A: Yes. Please get this memo typed up and distributed to all employees before 4 pm .
'summary1': 'Ms. Dawson helps A to write a memo to $\cdots$ ', 'topic1': 'communication method'
'summary2': 'In order to prevent employees from wasting time $\cdots$ ', 'topic2': 'company policy'
'summary3': 'Ms. Dawson takes a dictation for A about prohibiting $\cdots$ ', 'topic3': 'dictation'</p>
<h2>Alpaca-GPT4</h2>
<p>User: Make a list of behaviors that show strong leadership.
GPT: 1. Clear communication: Strong leaders communicate $\cdots 10$. Continuous learning: Effective leaders strive for continuous learning $\cdot \cdots$</p>
<p>Table 5: Examples of three basic published datasets we adopts in the main paper. Parts of contents are replaced by ellipsis due to page length limits.</p>
<h2>B Involved Prompts</h2>
<p>As a supplement, we list complete examples of involved prompts used in both intermediate memorelated tuning and downstream response consistency checking. Table 6, 7, 8, and 9 refer to complete instructions for intermediate "Memo Writ-</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Examples to illustrate observed instruction design challenges (better view in colors): "Prompt Copy" (1a, 1b), "Catastrophic Forgetting" (2) and "Prompt Misplacement" (3). Inferior instruction designs are shown in gray boxes, paired with parallel superior instruction design shown in green boxes. The key features are in bold.
ing", "Memo Retrival", "Chat w/ Memo" tasks, and downstream response consistency checking, respectively. As mentioned in Figure 3, all instructions share a similar structure, including a general task introduction, context body of task-specific inputs, and a detailed task introduction. Ground truth are provided for during tuning.</p>
<h2>C Instruction Design Challenges</h2>
<p>We provide examples of instruction design challenges reported in above sessions. The examples of "Prompt Copy", "Catastrophic Forgetting" and "Prompt Misplacement" are illustrated in Figure 5.</p>
<p>We suppose to use dummy variables to replace numerical values, in order to avoid the first challenge "Prompt Copy". Any in-distribution values such as line number in "Memo Writing" or option id in "Memo Retrival" may lead to a shortcut that LLMs can directly cope the values to meet format requirement, while dummy variables will push the models to understand prompts indeed. Sub-figures
(1a) and (1b) illustrate "Prompt Copy" obstacle in "Memo Writing" and "Memo Retrieval", respectively. The replacement of dummy inputs "M" and "N" with actual numbers can obviously lead to reasonable generation. The second challenge lies in "Catastrophic Forgetting", which refers to the response collapse if all types of instructions are not balanced. As shown in the Sub-figure (2), when the scale of "Chat w/ Memo" data is significantly smaller than the others, the chat response will be meaningless repetition. While such ill generation can be significantly eliminated once sufficient chatting corpus are supplemented. When is comes to the third challenge "Prompt Misplacement", we supposed that the most important component of prompt should be placed to the end of prompt. Otherwise, the LLMs tend to do natural text generation instead of memo-related tasks. The Sub-figure (3) shows that when detailed task explanation is placed in front of task content, in "Memo Writing", the chatbot does unexpected conversation continuation.</p>
<h1>Memo Writing Instruction</h1>
<p>You will be shown a 20-line Task Conversation between user and bot. Please read, memorize, and understand Task Conversation, then complete the task under the guidance of Task Introduction.</p>
<p>Task Conversation
(line 1) user: Anna just email to say that the managers meeting is put off till next Monday. Will you have everything ready by then, Sabrina? Hey Sabrina, what's wrong?
(line 2) bot: I'm so worried. I haven't heard from my sister for 2 weeks.
(line 3) user: How often do you call each other?
(line 4) bot: Normally at least once a week. But she's now a volunteer teacher in a mountain village in Africa. I can only write her.
(line 5) user: The Mail can be really slow sometimes. I'm sure you'll hear from her soon.
(line 6) bot: I hope so.
(line 14) bot: It'll take about 20 minutes.
(line 15) user: The streets are heavy with traffic at this time of a day, are they?
(line 16) bot: Yes, they are.
(line 17) user: Is it the rush hour?
(line 18) bot: Yes, it is. Are you in a hurry, sir?
(line 19) user: No, I'm not. Would you please drive slowly and carefully?
(line 20) bot: Yes, sir.
$\cdots$</p>
<p>Task Introduction:
Based on the Task Conversation, perform the following actions:
1 - Conclude all possible topics in the conversation with concise spans.
2 - Determine the chat range of each topic. These ranges should be a set of non-intersecting, sequentially connected end-to-end intervals.
3 - Conclude a summary of each chat with brief sentences.
4 - Report topic, summary and range results in JSON format only with the assigned keys: 'topic', 'summary', 'start', 'end'. For example, assuming an M-line conversation talks about 'banana'from line 1 to line N, then turns to talk about 'mango'from line $\mathrm{N}+1$ to line M. Thus, its task result could be: [{'topic': 'banana', 'summary': 'user talks banana with bot.', 'start': 1, 'end': N}, {'topic': 'mango', 'summary': 'bot brings mango for user.', 'start': N+1, 'end': M}].
Besides, following notations are provides:
1 - For each element of Task Conversation's JSON result, the value of 'end'should be smaller than the value of 'start', while both values should be larger than 0 but not exceed the total num of Task Conversation lines 20.
2 - Intersecting intervals such as {'topic': 'apple', 'summary': 'user and bot share apples.', 'start': K, 'end': N } and {'topic': 'pear', 'summary': 'bot sends pear to user.', 'start': N-2, 'end': M} are illegal.</p>
<p>Task Result:
[{'topic': 'worry', 'summary': 'Sabrina is worried about her sister because she hasn't heard from her sister for 2 weeks. user comforts her.', 'start': 1, 'end': 8}, {'topic': 'taxi conversation', 'summary': 'user takes bot's taxi to the railway station. As user is not rush, bot will drive slowly and carefully.', 'start': 9, 'end': 20]}</p>
<p>Table 6: A complete typical example of Memo Writing instruction, with ground truth provided.</p>
<h1>Memo Retrieval Instruction</h1>
<p>You will be shown 1 Query Sentence and 5 Topic Options. Please read, memorize, and understand given materials, then complete the task under the guidance of Task Introduction.</p>
<p>Query Sentence:
I need help with the office party. Yes, of course. We could split it. What part would you like to do, the food or the entertainment? I think we will have opportunities to meet each other in the future. OK, I think it's time for you to check in. If you run into any difficulty, we'll be here to assist you immediately. Your credit is fine, Sir. Now, tell me, what is it that you need the loan for? I've just bought a second hand apartment and I'm looking to do some renovations. You know, a bit of decorating, some new furniture, nothing flashy.
Topic Options:
(1) see off. Mr. Wang sees user off at the airport and they share good expectations of their business relationship.
(2) party preparation. bot helps user to prepare for the party. They decide the style, food, and music and will plan it in detail on Friday.
(3) NOTO. None of the others.
(4) Loan for renovations. bot wants to go for the Petty Consumer Loan for the renovations of his apartment. user says the maximum they can lend bot is 20,000 RMB and bot accepts it.
(5) annoying things. Gav had a good sleep last night but worries about the traffic jam and classes to teach. bot asks Gav's plan for the weekend and bot gives the suggestions when Gav feels upset.
・.
Task Introduction:
Select one or more topics from Topic Options that relevant with Query Sentence. Note that there is a NOTO option, select it if all other topic options are not related to Query Sentence. Do not report the option content, but only report selected option numbers in a string separated with '#'. For example, if topic option N and M are chosen, then the output is: N#M. For Query Sentence in the task, any chosen option numbers should be larger than 0 but not exceed the total num of Topic Options 5.</p>
<p>Task Result:
$1 # 2 # 4$
Table 7: A complete typical example of Memo Retrieval instruction, with ground truth provided.</p>
<h1>Chat w/ Memo Instruction</h1>
<p>You are an intelligent dialog bot. You will be shown Related Evidences supporting for User Input, and Recent Dialogs between user and you. Please read, memorize, and understand given materials, then generate one concise, coherent and helpful response.</p>
<h2>Related Evidences:</h2>
<p>(1) {'Related Topics': 'acceptance of the job', 'Related Summaries': 'bot is satisfied with the monthly salary proposed by user and asks some questions about the work.', 'Related Dialogs': 'user: What's your expected salary? ### bot: What is important to me is the job and the people who I will be working with, so regarding salary, I leave it to you and I am sure that you will make me a fair offer. ### user: I can offer you 5, 000 yuan per month. Raises are given after three months'probation period according to your performance. Is this satisfactory? ### bot: Yes, it is quite satisfactory. I accept it. ### user: Any question about the work? ### bot: To whom should I report? ### user: The general manager of your section. ### bot: What are the benefits? ### user: We'll offer that on holidays. ### bot: I would like to know if there would be any opportunity to work abroad in the future? ### user: Yes, we have inspection abroad. ### bot: Thank you. Then I think it's time for us to sign a letter of intent. ### user: Okay.}
(2) {'Related Topics': 'find a job', 'Related Summaries': 'user wants to find a job at the computer center.', 'Related Dialogs': 'user: I'd like to find a job. ### bot: We have several part-time jobs available here, Would you like to look through the list? ### user: Yes, thank you. I'd like to apply for the job at the computer center. ### bot: Please fill out the form.}
(3) {'Related Topics': 'check in', 'Related Summaries': 'It's user's first time on a plane. bot tells user how to check in.', 'Related Dialogs': 'user: Excuse me, this is my first time on a plane. How do I check in? ### bot: May I see your ticket, please? ### user: Yes. Here you are. ### bot: You can get a boarding pass at that counter. Do you have anything to check in? ### user: No, I only have a handbag. ### bot: Then you could wait in the departure area after going through security. ### user: Is that all? ### bot: Yeah, I think so. It is very simple. ### user: I see. Thank you very much.}
$\cdots$</p>
<h2>Recent Dialogs:</h2>
<p>user: What's the tallest building? ### bot: You mean the white building near the playground? ### user: Yes. ### bot: That is the library. And it has more than 1, 000, 000 books. ### user: What's the building to the south of the library? ### bot: You know, our school is divided into two parts, the junior high school and the senior high school. That is the new classroom building for our senior high school. ### user: Is there a swimming pool in your school? ### bot: Yes. There is a large swimming pool, but it is only available in summer. ### user: I do envy you. And I hope I can enter your school one day. ### bot: I believe that you can make your dream come true.</p>
<h2>User Input:</h2>
<p>user: What is your salary now? ### bot:
My present pay is RMB 3, 000 yuan each month.
Table 8: A complete typical example of Chat w/ Memo instruction, with ground truth provided.</p>
<h1>GPT4 Evaluation Prompt</h1>
<p>You are an impartial judge. You will be shown Related Conversation History, User Question and Bot Response.</p>
<h2>Related Conversation History</h2>
<p>user: Please describe the concept of machine learning. Could you elaborate on the differences between supervised, unsupervised, and reinforcement learning? Provide real-world examples of each. bot: Sure! Machine learning $\cdots$ In summary, machine learning is an powerful tool for making predictions and decisions based on data $\cdots$
user: In your last example of reinforcement learning, can we use supervised learning to solve it? bot: Supervised learning would not be the best approach for solving the self-driving car problem, because $\cdots$ or penalties, which is more suitable for autonomous driving.
user: Discuss antitrust laws and their impact on market competition. Compare the antitrust laws in US and China along with some case studies.
bot: Antitrust laws are designed to promote and maintain market competition by preventing anticompetitive behavior by companies $\cdots$ playing field for all companies.
user: Pick one case study and explain it in detail.
bot: One notable case study is the US government's antitrust lawsuit against Microsoft. $\cdots$ is considered a prime example of how antitrust laws can be used to promote market competition.
・.
$\cdots$
User Question
user: Does the law we discussed apply to the machine learning industry? Use previous examples to explain your answer.
・.
・.
Bot Response
bot: Yes, the law we discussed, the Sherman Antitrust Act, applies to the machine learning industry. The Sherman Antitrust Act prohibits anti-competitive behavior, such as price-fixing, market allocation, and monopolization. In the machine learning industry, there have been examples of companies collaborating to fix prices or engaging in other anti-competitive behavior, which would be in violation of the Sherman Antitrust Act. For example, in 2010, several technology companies, including Apple, Google, and Adobe, were accused of conspiring to suppress wages by agreeing not to poach each other's employees. This agreement was found to be in violation of the Sherman Antitrust Act, and the companies were forced to abandon the agreement and pay a settlement. Additionally, in 2018, the European Commission fined Google $\$ 5$ billion for violating the Sherman Antitrust Act by engaging in anti-competitive behavior with its Android operating system. The Commission found that Google had required mobile device manufacturers to pre-install its search engine and Chrome browser on their devices, which had the effect of stifling competition from other search engines and browsers.
・・</p>
<p>Please evaluate whether Bot Response is faithful to the content of Related Conversation History to answer User Question. Begin your evaluation by providing a short explanation, then you must rate Bot Response on an integer rating of 1 to 100 by strictly following this format: [[rating]].</p>
<p>Table 9: Example of GPT4 Evaluation Prompt. Parts of the content are replaced by ellipsis due to page length limits.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ Codes: https://github.com/LuJunru/MemoChat.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
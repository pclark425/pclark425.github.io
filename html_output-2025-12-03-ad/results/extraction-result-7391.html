<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7391 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7391</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7391</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-139.html">extraction-schema-139</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being used to detect anomalies in lists or tabular data, including the methods, datasets, evaluation metrics, and results.</div>
                <p><strong>Paper ID:</strong> paper-270877993</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2407.01896v1.pdf" target="_blank">LogEval: A Comprehensive Benchmark Suite for Large Language Models In Log Analysis</a></p>
                <p><strong>Paper Abstract:</strong> Log analysis is crucial for ensuring the orderly and stable operation of information systems, particularly in the field of Artificial Intelligence for IT Operations (AIOps). Large Language Models (LLMs) have demonstrated significant potential in natural language processing tasks. In the AIOps domain, they excel in tasks such as anomaly detection, root cause analysis of faults, operations and maintenance script generation, and alert information summarization. However, the performance of current LLMs in log analysis tasks remains inadequately validated. To address this gap, we introduce LogEval, a comprehensive benchmark suite designed to evaluate the capabilities of LLMs in various log analysis tasks for the first time. This benchmark covers tasks such as log parsing, log anomaly detection, log fault diagnosis, and log summarization. LogEval evaluates each task using 4,000 publicly available log data entries and employs 15 different prompts for each task to ensure a thorough and fair assessment. By rigorously evaluating leading LLMs, we demonstrate the impact of various LLM technologies on log analysis performance, focusing on aspects such as self-consistency and few-shot contextual learning. We also discuss findings related to model quantification, Chinese-English question-answering evaluation, and prompt engineering. These findings provide insights into the strengths and weaknesses of LLMs in multilingual environments and the effectiveness of different prompt strategies. Various evaluation methods are employed for different tasks to accurately measure the performance of LLMs in log analysis, ensuring a comprehensive assessment. The insights gained from LogEvals evaluation reveal the strengths and limitations of LLMs in log analysis tasks, providing valuable guidance for researchers and practitioners.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7391.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7391.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being used to detect anomalies in lists or tabular data, including the methods, datasets, evaluation metrics, and results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 (OpenAI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A state-of-the-art instruction-tuned large multimodal decoder LM from OpenAI used as an API in this benchmark; evaluated for log anomaly detection via prompting (zero-shot, few-shot, and self-consistency).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>LogEval: A Comprehensive Benchmark Suite for Large Language Models In Log Analysis</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-tuned large multimodal decoder-only language model accessed via API.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>undisclosed</td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_detection_approach</strong></td>
                            <td>Prompting-based classification (naive Q&A and self-consistency), evaluated in zero-shot and few-shot modes</td>
                        </tr>
                        <tr>
                            <td><strong>prompt_template</strong></td>
                            <td>Review and mark the log entry as 'normal' or 'abnormal', only output 'normal' or 'abnormal' (also applied bilingual variants).</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>system log entries (sequences of log lines)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>BGL and Thunderbird (sourced from LogPub / LogHub / LogPAI), overall LogEval subset of 4,000 logs</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Accuracy and F1-score (also report inference time and average output tokens elsewhere)</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Anomaly detection (Table 4): zero-shot English: accuracy 0.333, F1 0.097; few-shot English: accuracy 0.564, F1 0.136. (Also reported: zero-shot Chinese acc 0.331 F1 0.097; few-shot Chinese acc 0.506 F1 0.135.)</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Baselines: NeuralLog acc 0.97 F1 0.0; LogRobust acc 0.95, F1 0.09 (precision 0.33, recall 0.55). GPT-4 produces substantially higher F1 than some baselines in few-shot for certain tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Both: zero-shot and few-shot (few-shot used 5 examples); self-consistency runs with 5 samples were also evaluated.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Paper reports that few-shot does not always improve anomaly detection (models sometimes repeat sample answers), and GPT-4's F1 in zero-shot remains low (~0.097), indicating difficulty recognizing diverse anomalies; models can output extraneous content if prompts not strict.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Inference time was measured across tasks (Fig.9) but per-model-per-task breakdown for anomaly detection beyond aggregated measures is not tabulated in the anomaly table; no GPU-hours or exact token-costs reported for GPT-4 in paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LogEval: A Comprehensive Benchmark Suite for Large Language Models In Log Analysis', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7391.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7391.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being used to detect anomalies in lists or tabular data, including the methods, datasets, evaluation metrics, and results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Gemini Pro</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Gemini Pro (Google)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A proprietary large language model from Google evaluated via API for log anomaly detection in LogEval using prompting in zero- and few-shot regimes.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>LogEval: A Comprehensive Benchmark Suite for Large Language Models In Log Analysis</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Gemini Pro</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-tuned large LM accessed via API (proprietary, multimodal capabilities noted by authors).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>undisclosed</td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_detection_approach</strong></td>
                            <td>Prompting-based classification (naive Q&A and self-consistency), zero-shot and few-shot</td>
                        </tr>
                        <tr>
                            <td><strong>prompt_template</strong></td>
                            <td>Review and mark the log entry as 'normal' or 'abnormal', only output 'normal' or 'abnormal'.</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>log entries</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>BGL and Thunderbird (LogPub/LogHub/LogPAI), LogEval subset (4,000 logs)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Accuracy and F1-score</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Anomaly detection (Table 4): zero-shot English: accuracy 0.417, F1 0.109; few-shot English: accuracy 0.531, F1 0.132. (Few-shot Chinese: acc 0.602 F1 0.141 reported.)</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared against NeuralLog (acc 0.97 F1 0.0) and LogRobust (acc 0.95 F1 0.09); Gemini Pro attains higher F1 in LLM comparisons for few-shot anomaly detection.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Both: zero-shot and few-shot (5-shot).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Although few-shot often improved Gemini Pro, the paper notes in anomaly detection tasks that some models still repeat example labels and few-shot sometimes fails to generalize; F1 remains modest (~0.13).</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Inference time aggregated across tasks reported (Fig.9); no per-example token-cost or GPU-hour figures for Gemini Pro provided in anomaly-specific tables.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LogEval: A Comprehensive Benchmark Suite for Large Language Models In Log Analysis', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7391.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7391.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being used to detect anomalies in lists or tabular data, including the methods, datasets, evaluation metrics, and results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaMA2-70B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaMA-2 70B</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 70-billion parameter open-source LM from Meta evaluated in LogEval for anomaly detection; it showed strong zero-shot accuracy on anomaly detection but low F1, indicating class imbalance or failure to identify anomalies properly.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>LogEval: A Comprehensive Benchmark Suite for Large Language Models In Log Analysis</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA-2-70B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large open-source decoder-only LM family (70B parameters) available via weights/local hosting.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>70B</td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_detection_approach</strong></td>
                            <td>Zero-shot and few-shot prompting (naive Q&A and self-consistency)</td>
                        </tr>
                        <tr>
                            <td><strong>prompt_template</strong></td>
                            <td>Review and mark the log entry as 'normal' or 'abnormal', only output 'normal' or 'abnormal'.</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>log entries</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>BGL and Thunderbird (LogPub/LogHub/LogPAI), LogEval subset (4,000 logs)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Accuracy and F1-score</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Anomaly detection (Table 4): zero-shot English: accuracy 0.693, F1 0.044; few-shot English: accuracy 0.007, F1 0.036 (few-shot results degraded in some cases). Zero-shot Chinese acc 0.562 F1 0.036 reported. High accuracy combined with near-zero F1 indicates the model often predicts the majority class (normal) and fails to identify positives.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared against NeuralLog (acc 0.97 F1 0.0) and LogRobust (acc 0.95 F1 0.09); LLaMA2-70B's high accuracy but very low F1 is similar to NeuralLog's failure mode (predicting only normal).</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Zero-shot and few-shot (5-shot); self-consistency also evaluated.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Paper explicitly notes that some models (including LLaMA2 variants) have F1 of 0 in some settings, meaning they predict no anomalies; model tends to repeat examples in few-shot and produce majority-class bias.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Paper notes large-parameter models like LLaMA-2-70B have longer inference times (Fig.9) but per-model-per-task GPU/time figures for anomaly detection are not tabulated.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LogEval: A Comprehensive Benchmark Suite for Large Language Models In Log Analysis', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7391.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7391.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being used to detect anomalies in lists or tabular data, including the methods, datasets, evaluation metrics, and results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Qwen1.5-72B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Qwen-1.5-72B (Alibaba Cloud)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large instruction-tuned LM family from Alibaba (72B variant) evaluated in LogEval; used for anomaly detection via prompting with bilingual prompts and few-shot examples.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>LogEval: A Comprehensive Benchmark Suite for Large Language Models In Log Analysis</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Qwen-1.5-72B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large instruction-tuned decoder-only LM family (7/14/72B variants) available via API.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>72B</td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_detection_approach</strong></td>
                            <td>Zero-shot and few-shot prompting (naive Q&A and self-consistency)</td>
                        </tr>
                        <tr>
                            <td><strong>prompt_template</strong></td>
                            <td>Review and mark the log entry as 'normal' or 'abnormal', only output 'normal' or 'abnormal'.</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>log entries</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>BGL and Thunderbird (LogPub/LogHub/LogPAI), LogEval subset (4,000 logs)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Accuracy and F1-score</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Anomaly detection (Table 4): zero-shot English: accuracy 0.239, F1 0.063; few-shot English: accuracy 0.274, F1 0.16. (Zero-shot Chinese acc 0.334 F1 0.097; few-shot Chinese acc 0.330 F1 0.495 â€” note the paper tables indicate large variation across settings.)</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared against NeuralLog (acc 0.97 F1 0.0) and LogRobust (acc 0.95 F1 0.09); Qwen1.5-72B yields higher F1 than some baseline methods in specific few-shot settings.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Both: zero-shot and few-shot (5-shot); self-consistency experiments performed.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Paper reports instability across languages and settings; few-shot sometimes causes the model to repeat sample labels or produce inconsistent outputs; F1 remains modest in many settings.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Inference-time details reported in aggregate figures (Fig.9) and average token counts (Fig.10), but anomaly-detection-specific compute numbers per model are not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LogEval: A Comprehensive Benchmark Suite for Large Language Models In Log Analysis', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7391.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e7391.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being used to detect anomalies in lists or tabular data, including the methods, datasets, evaluation metrics, and results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3.5-turbo (OpenAI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An OpenAI instruction-tuned model (GPT-3.5 family) used via API in LogEval for anomaly detection with prompting; evaluated in zero-shot, few-shot and self-consistency settings.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>LogEval: A Comprehensive Benchmark Suite for Large Language Models In Log Analysis</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-tuned decoder-only LM accessed via API.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>undisclosed</td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_detection_approach</strong></td>
                            <td>Prompting-based classification (naive Q&A and self-consistency), zero-shot and few-shot</td>
                        </tr>
                        <tr>
                            <td><strong>prompt_template</strong></td>
                            <td>Review and mark the log entry as 'normal' or 'abnormal', only output 'normal' or 'abnormal'.</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>log entries</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>BGL and Thunderbird (LogPub/LogHub/LogPAI), LogEval subset (4,000 logs)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Accuracy and F1-score</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Anomaly detection (Table 4): zero-shot English: accuracy 0.285, F1 0.082; few-shot English: accuracy 0.402, F1 0.107. (Zero-shot Chinese acc 0.243 F1 0.084; few-shot Chinese acc 0.371 F1 0.088.)</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared to NeuralLog (acc 0.97 F1 0.0) and LogRobust (acc 0.95 F1 0.09); GPT-3.5 outperforms NeuralLog in F1 in few-shot settings.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Both (few-shot uses 5 exemplars); self-consistency also tested.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Paper notes that for anomaly detection, few-shot sometimes does not help and models may repeat examples; F1 remains limited in zero-shot.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>No per-example cost reported; inference time and average tokens presented globally (Fig.9, Fig.10).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LogEval: A Comprehensive Benchmark Suite for Large Language Models In Log Analysis', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7391.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e7391.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being used to detect anomalies in lists or tabular data, including the methods, datasets, evaluation metrics, and results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>NeuralLog</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>NeuralLog</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A non-LLM deep-learning baseline that detects anomalies directly from raw log data without explicit parsing; included as a baseline in LogEval for comparison with LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>LogEval: A Comprehensive Benchmark Suite for Large Language Models In Log Analysis</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>NeuralLog</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Deep-learning-based anomaly detector that operates on raw log text (non-LLM).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>not applicable</td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_detection_approach</strong></td>
                            <td>Supervised/unsupervised deep learning anomaly detection directly on raw logs (no LLM prompting)</td>
                        </tr>
                        <tr>
                            <td><strong>prompt_template</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Baseline model's own training (not an LLM); evaluated on LogEval dataset (4,000 sequences).</td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>log entries / sequences</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>LogEval dataset (4,000 sequences derived from 4,000 logs with context windows), sourced from LogPub / LogHub / LogPAI</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Accuracy, F1-score (precision/recall reported for competing baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Baseline (Table 11): NeuralLog accuracy 0.97, F1-score 0.0 (model predicted majority class and failed to identify anomalies in this evaluation).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>NeuralLog contrasted directly to LLMs in the study; several LLMs achieved non-zero F1 while NeuralLog had F1=0.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>N/A (not a prompting experiment)</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Paper notes NeuralLog's inability to identify anomalies on this dataset (F1=0) despite high accuracy, indicating majority-class bias.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LogEval: A Comprehensive Benchmark Suite for Large Language Models In Log Analysis', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7391.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e7391.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being used to detect anomalies in lists or tabular data, including the methods, datasets, evaluation metrics, and results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LogRobust</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LogRobust</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A baseline method designed to improve anomaly detection robustness under unstable/variable log formats; included as a comparative baseline in LogEval.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>LogEval: A Comprehensive Benchmark Suite for Large Language Models In Log Analysis</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LogRobust</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Adaptive unsupervised/robust anomaly detection method for logs that handles unstable log data formats.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>not applicable</td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_detection_approach</strong></td>
                            <td>Unsupervised anomaly detection with adaptive updating for changing log formats</td>
                        </tr>
                        <tr>
                            <td><strong>prompt_template</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Evaluated on LogEval dataset (4,000 sequences); method internally adapts to data drift (as originally proposed by its authors).</td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>log entries / sequences</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>LogEval dataset (4,000 sequences derived from 4,000 logs), sourced from LogPub / LogHub / LogPAI</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Accuracy, F1-score, precision, recall</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Baseline (Table 11): LogRobust accuracy 0.95, F1-score 0.09, precision 0.33, recall 0.55.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared to NeuralLog (acc 0.97 F1 0.0) and LLMs; LogRobust had low F1 but non-zero recall, whereas several LLMs achieved larger F1 in some settings.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>N/A</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Low F1 (0.09) indicates limited detection power on this evaluation, though recall (0.55) suggests it finds some anomalies at expense of precision.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LogEval: A Comprehensive Benchmark Suite for Large Language Models In Log Analysis', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Loggpt: Exploring chatgpt for log-based anomaly detection <em>(Rating: 2)</em></li>
                <li>Log-based Anomaly Detection Without Log Parsing <em>(Rating: 2)</em></li>
                <li>Scalable and adaptive log-based anomaly detection with expert in the loop <em>(Rating: 2)</em></li>
                <li>Loghub: A large collection of system log datasets towards automated log analytics <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7391",
    "paper_id": "paper-270877993",
    "extraction_schema_id": "extraction-schema-139",
    "extracted_data": [
        {
            "name_short": "GPT-4",
            "name_full": "GPT-4 (OpenAI)",
            "brief_description": "A state-of-the-art instruction-tuned large multimodal decoder LM from OpenAI used as an API in this benchmark; evaluated for log anomaly detection via prompting (zero-shot, few-shot, and self-consistency).",
            "citation_title": "LogEval: A Comprehensive Benchmark Suite for Large Language Models In Log Analysis",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_description": "Instruction-tuned large multimodal decoder-only language model accessed via API.",
            "model_size": "undisclosed",
            "anomaly_detection_approach": "Prompting-based classification (naive Q&A and self-consistency), evaluated in zero-shot and few-shot modes",
            "prompt_template": "Review and mark the log entry as 'normal' or 'abnormal', only output 'normal' or 'abnormal' (also applied bilingual variants).",
            "training_data": null,
            "data_type": "system log entries (sequences of log lines)",
            "dataset_name": "BGL and Thunderbird (sourced from LogPub / LogHub / LogPAI), overall LogEval subset of 4,000 logs",
            "evaluation_metric": "Accuracy and F1-score (also report inference time and average output tokens elsewhere)",
            "performance": "Anomaly detection (Table 4): zero-shot English: accuracy 0.333, F1 0.097; few-shot English: accuracy 0.564, F1 0.136. (Also reported: zero-shot Chinese acc 0.331 F1 0.097; few-shot Chinese acc 0.506 F1 0.135.)",
            "baseline_comparison": "Baselines: NeuralLog acc 0.97 F1 0.0; LogRobust acc 0.95, F1 0.09 (precision 0.33, recall 0.55). GPT-4 produces substantially higher F1 than some baselines in few-shot for certain tasks.",
            "zero_shot_or_few_shot": "Both: zero-shot and few-shot (few-shot used 5 examples); self-consistency runs with 5 samples were also evaluated.",
            "limitations_or_failure_cases": "Paper reports that few-shot does not always improve anomaly detection (models sometimes repeat sample answers), and GPT-4's F1 in zero-shot remains low (~0.097), indicating difficulty recognizing diverse anomalies; models can output extraneous content if prompts not strict.",
            "computational_cost": "Inference time was measured across tasks (Fig.9) but per-model-per-task breakdown for anomaly detection beyond aggregated measures is not tabulated in the anomaly table; no GPU-hours or exact token-costs reported for GPT-4 in paper.",
            "uuid": "e7391.0",
            "source_info": {
                "paper_title": "LogEval: A Comprehensive Benchmark Suite for Large Language Models In Log Analysis",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "Gemini Pro",
            "name_full": "Gemini Pro (Google)",
            "brief_description": "A proprietary large language model from Google evaluated via API for log anomaly detection in LogEval using prompting in zero- and few-shot regimes.",
            "citation_title": "LogEval: A Comprehensive Benchmark Suite for Large Language Models In Log Analysis",
            "mention_or_use": "use",
            "model_name": "Gemini Pro",
            "model_description": "Instruction-tuned large LM accessed via API (proprietary, multimodal capabilities noted by authors).",
            "model_size": "undisclosed",
            "anomaly_detection_approach": "Prompting-based classification (naive Q&A and self-consistency), zero-shot and few-shot",
            "prompt_template": "Review and mark the log entry as 'normal' or 'abnormal', only output 'normal' or 'abnormal'.",
            "training_data": null,
            "data_type": "log entries",
            "dataset_name": "BGL and Thunderbird (LogPub/LogHub/LogPAI), LogEval subset (4,000 logs)",
            "evaluation_metric": "Accuracy and F1-score",
            "performance": "Anomaly detection (Table 4): zero-shot English: accuracy 0.417, F1 0.109; few-shot English: accuracy 0.531, F1 0.132. (Few-shot Chinese: acc 0.602 F1 0.141 reported.)",
            "baseline_comparison": "Compared against NeuralLog (acc 0.97 F1 0.0) and LogRobust (acc 0.95 F1 0.09); Gemini Pro attains higher F1 in LLM comparisons for few-shot anomaly detection.",
            "zero_shot_or_few_shot": "Both: zero-shot and few-shot (5-shot).",
            "limitations_or_failure_cases": "Although few-shot often improved Gemini Pro, the paper notes in anomaly detection tasks that some models still repeat example labels and few-shot sometimes fails to generalize; F1 remains modest (~0.13).",
            "computational_cost": "Inference time aggregated across tasks reported (Fig.9); no per-example token-cost or GPU-hour figures for Gemini Pro provided in anomaly-specific tables.",
            "uuid": "e7391.1",
            "source_info": {
                "paper_title": "LogEval: A Comprehensive Benchmark Suite for Large Language Models In Log Analysis",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "LLaMA2-70B",
            "name_full": "LLaMA-2 70B",
            "brief_description": "A 70-billion parameter open-source LM from Meta evaluated in LogEval for anomaly detection; it showed strong zero-shot accuracy on anomaly detection but low F1, indicating class imbalance or failure to identify anomalies properly.",
            "citation_title": "LogEval: A Comprehensive Benchmark Suite for Large Language Models In Log Analysis",
            "mention_or_use": "use",
            "model_name": "LLaMA-2-70B",
            "model_description": "Large open-source decoder-only LM family (70B parameters) available via weights/local hosting.",
            "model_size": "70B",
            "anomaly_detection_approach": "Zero-shot and few-shot prompting (naive Q&A and self-consistency)",
            "prompt_template": "Review and mark the log entry as 'normal' or 'abnormal', only output 'normal' or 'abnormal'.",
            "training_data": null,
            "data_type": "log entries",
            "dataset_name": "BGL and Thunderbird (LogPub/LogHub/LogPAI), LogEval subset (4,000 logs)",
            "evaluation_metric": "Accuracy and F1-score",
            "performance": "Anomaly detection (Table 4): zero-shot English: accuracy 0.693, F1 0.044; few-shot English: accuracy 0.007, F1 0.036 (few-shot results degraded in some cases). Zero-shot Chinese acc 0.562 F1 0.036 reported. High accuracy combined with near-zero F1 indicates the model often predicts the majority class (normal) and fails to identify positives.",
            "baseline_comparison": "Compared against NeuralLog (acc 0.97 F1 0.0) and LogRobust (acc 0.95 F1 0.09); LLaMA2-70B's high accuracy but very low F1 is similar to NeuralLog's failure mode (predicting only normal).",
            "zero_shot_or_few_shot": "Zero-shot and few-shot (5-shot); self-consistency also evaluated.",
            "limitations_or_failure_cases": "Paper explicitly notes that some models (including LLaMA2 variants) have F1 of 0 in some settings, meaning they predict no anomalies; model tends to repeat examples in few-shot and produce majority-class bias.",
            "computational_cost": "Paper notes large-parameter models like LLaMA-2-70B have longer inference times (Fig.9) but per-model-per-task GPU/time figures for anomaly detection are not tabulated.",
            "uuid": "e7391.2",
            "source_info": {
                "paper_title": "LogEval: A Comprehensive Benchmark Suite for Large Language Models In Log Analysis",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "Qwen1.5-72B",
            "name_full": "Qwen-1.5-72B (Alibaba Cloud)",
            "brief_description": "A large instruction-tuned LM family from Alibaba (72B variant) evaluated in LogEval; used for anomaly detection via prompting with bilingual prompts and few-shot examples.",
            "citation_title": "LogEval: A Comprehensive Benchmark Suite for Large Language Models In Log Analysis",
            "mention_or_use": "use",
            "model_name": "Qwen-1.5-72B",
            "model_description": "Large instruction-tuned decoder-only LM family (7/14/72B variants) available via API.",
            "model_size": "72B",
            "anomaly_detection_approach": "Zero-shot and few-shot prompting (naive Q&A and self-consistency)",
            "prompt_template": "Review and mark the log entry as 'normal' or 'abnormal', only output 'normal' or 'abnormal'.",
            "training_data": null,
            "data_type": "log entries",
            "dataset_name": "BGL and Thunderbird (LogPub/LogHub/LogPAI), LogEval subset (4,000 logs)",
            "evaluation_metric": "Accuracy and F1-score",
            "performance": "Anomaly detection (Table 4): zero-shot English: accuracy 0.239, F1 0.063; few-shot English: accuracy 0.274, F1 0.16. (Zero-shot Chinese acc 0.334 F1 0.097; few-shot Chinese acc 0.330 F1 0.495 â€” note the paper tables indicate large variation across settings.)",
            "baseline_comparison": "Compared against NeuralLog (acc 0.97 F1 0.0) and LogRobust (acc 0.95 F1 0.09); Qwen1.5-72B yields higher F1 than some baseline methods in specific few-shot settings.",
            "zero_shot_or_few_shot": "Both: zero-shot and few-shot (5-shot); self-consistency experiments performed.",
            "limitations_or_failure_cases": "Paper reports instability across languages and settings; few-shot sometimes causes the model to repeat sample labels or produce inconsistent outputs; F1 remains modest in many settings.",
            "computational_cost": "Inference-time details reported in aggregate figures (Fig.9) and average token counts (Fig.10), but anomaly-detection-specific compute numbers per model are not provided.",
            "uuid": "e7391.3",
            "source_info": {
                "paper_title": "LogEval: A Comprehensive Benchmark Suite for Large Language Models In Log Analysis",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "GPT-3.5-turbo",
            "name_full": "GPT-3.5-turbo (OpenAI)",
            "brief_description": "An OpenAI instruction-tuned model (GPT-3.5 family) used via API in LogEval for anomaly detection with prompting; evaluated in zero-shot, few-shot and self-consistency settings.",
            "citation_title": "LogEval: A Comprehensive Benchmark Suite for Large Language Models In Log Analysis",
            "mention_or_use": "use",
            "model_name": "GPT-3.5-turbo",
            "model_description": "Instruction-tuned decoder-only LM accessed via API.",
            "model_size": "undisclosed",
            "anomaly_detection_approach": "Prompting-based classification (naive Q&A and self-consistency), zero-shot and few-shot",
            "prompt_template": "Review and mark the log entry as 'normal' or 'abnormal', only output 'normal' or 'abnormal'.",
            "training_data": null,
            "data_type": "log entries",
            "dataset_name": "BGL and Thunderbird (LogPub/LogHub/LogPAI), LogEval subset (4,000 logs)",
            "evaluation_metric": "Accuracy and F1-score",
            "performance": "Anomaly detection (Table 4): zero-shot English: accuracy 0.285, F1 0.082; few-shot English: accuracy 0.402, F1 0.107. (Zero-shot Chinese acc 0.243 F1 0.084; few-shot Chinese acc 0.371 F1 0.088.)",
            "baseline_comparison": "Compared to NeuralLog (acc 0.97 F1 0.0) and LogRobust (acc 0.95 F1 0.09); GPT-3.5 outperforms NeuralLog in F1 in few-shot settings.",
            "zero_shot_or_few_shot": "Both (few-shot uses 5 exemplars); self-consistency also tested.",
            "limitations_or_failure_cases": "Paper notes that for anomaly detection, few-shot sometimes does not help and models may repeat examples; F1 remains limited in zero-shot.",
            "computational_cost": "No per-example cost reported; inference time and average tokens presented globally (Fig.9, Fig.10).",
            "uuid": "e7391.4",
            "source_info": {
                "paper_title": "LogEval: A Comprehensive Benchmark Suite for Large Language Models In Log Analysis",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "NeuralLog",
            "name_full": "NeuralLog",
            "brief_description": "A non-LLM deep-learning baseline that detects anomalies directly from raw log data without explicit parsing; included as a baseline in LogEval for comparison with LLMs.",
            "citation_title": "LogEval: A Comprehensive Benchmark Suite for Large Language Models In Log Analysis",
            "mention_or_use": "use",
            "model_name": "NeuralLog",
            "model_description": "Deep-learning-based anomaly detector that operates on raw log text (non-LLM).",
            "model_size": "not applicable",
            "anomaly_detection_approach": "Supervised/unsupervised deep learning anomaly detection directly on raw logs (no LLM prompting)",
            "prompt_template": "",
            "training_data": "Baseline model's own training (not an LLM); evaluated on LogEval dataset (4,000 sequences).",
            "data_type": "log entries / sequences",
            "dataset_name": "LogEval dataset (4,000 sequences derived from 4,000 logs with context windows), sourced from LogPub / LogHub / LogPAI",
            "evaluation_metric": "Accuracy, F1-score (precision/recall reported for competing baseline)",
            "performance": "Baseline (Table 11): NeuralLog accuracy 0.97, F1-score 0.0 (model predicted majority class and failed to identify anomalies in this evaluation).",
            "baseline_comparison": "NeuralLog contrasted directly to LLMs in the study; several LLMs achieved non-zero F1 while NeuralLog had F1=0.",
            "zero_shot_or_few_shot": "N/A (not a prompting experiment)",
            "limitations_or_failure_cases": "Paper notes NeuralLog's inability to identify anomalies on this dataset (F1=0) despite high accuracy, indicating majority-class bias.",
            "computational_cost": null,
            "uuid": "e7391.5",
            "source_info": {
                "paper_title": "LogEval: A Comprehensive Benchmark Suite for Large Language Models In Log Analysis",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "LogRobust",
            "name_full": "LogRobust",
            "brief_description": "A baseline method designed to improve anomaly detection robustness under unstable/variable log formats; included as a comparative baseline in LogEval.",
            "citation_title": "LogEval: A Comprehensive Benchmark Suite for Large Language Models In Log Analysis",
            "mention_or_use": "use",
            "model_name": "LogRobust",
            "model_description": "Adaptive unsupervised/robust anomaly detection method for logs that handles unstable log data formats.",
            "model_size": "not applicable",
            "anomaly_detection_approach": "Unsupervised anomaly detection with adaptive updating for changing log formats",
            "prompt_template": "",
            "training_data": "Evaluated on LogEval dataset (4,000 sequences); method internally adapts to data drift (as originally proposed by its authors).",
            "data_type": "log entries / sequences",
            "dataset_name": "LogEval dataset (4,000 sequences derived from 4,000 logs), sourced from LogPub / LogHub / LogPAI",
            "evaluation_metric": "Accuracy, F1-score, precision, recall",
            "performance": "Baseline (Table 11): LogRobust accuracy 0.95, F1-score 0.09, precision 0.33, recall 0.55.",
            "baseline_comparison": "Compared to NeuralLog (acc 0.97 F1 0.0) and LLMs; LogRobust had low F1 but non-zero recall, whereas several LLMs achieved larger F1 in some settings.",
            "zero_shot_or_few_shot": "N/A",
            "limitations_or_failure_cases": "Low F1 (0.09) indicates limited detection power on this evaluation, though recall (0.55) suggests it finds some anomalies at expense of precision.",
            "computational_cost": null,
            "uuid": "e7391.6",
            "source_info": {
                "paper_title": "LogEval: A Comprehensive Benchmark Suite for Large Language Models In Log Analysis",
                "publication_date_yy_mm": "2024-07"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Loggpt: Exploring chatgpt for log-based anomaly detection",
            "rating": 2,
            "sanitized_title": "loggpt_exploring_chatgpt_for_logbased_anomaly_detection"
        },
        {
            "paper_title": "Log-based Anomaly Detection Without Log Parsing",
            "rating": 2,
            "sanitized_title": "logbased_anomaly_detection_without_log_parsing"
        },
        {
            "paper_title": "Scalable and adaptive log-based anomaly detection with expert in the loop",
            "rating": 2,
            "sanitized_title": "scalable_and_adaptive_logbased_anomaly_detection_with_expert_in_the_loop"
        },
        {
            "paper_title": "Loghub: A large collection of system log datasets towards automated log analytics",
            "rating": 2,
            "sanitized_title": "loghub_a_large_collection_of_system_log_datasets_towards_automated_log_analytics"
        }
    ],
    "cost": 0.019258249999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>LogEval: A Comprehensive Benchmark Suite for Large Language Models In Log Analysis
2 Jul 2024</p>
<p>Tianyu Cui cuitianyu@mail.nankai.edu.cn 
Shenglin Zhang zhangsl@nankai.edu.cn 
Shiyu Ma mashiyu@mail.nankai.edu.cn 
Ziang Chen 
Tong Xiao xiaotong18@hnu.edu.cn 
Shimin Tao taoshimin@huawei.com 
Yilun Liu 
Duoming Lin 
Changchang Liu 
Yuzhe Cai </p>
<p>Nankai University
China</p>
<p>Tsinghua University
China</p>
<p>Nankai University
Tianjin, Shiyu MaChina</p>
<p>Nankai University
Tianjin, Ziang ChenChina</p>
<p>Nankai University
Tianjin, Tong XiaoChina</p>
<p>Tsinghua University
BeijingChina</p>
<p>Yilun Liu
Huawei, Beijing, Huawei, Beijing, Shenglin ZhangChina, China</p>
<p>Nankai University
Tianjin, Duoming LinChina</p>
<p>Nankai University
Tianjin, Changchang LiuChina</p>
<p>Nankai University
Tianjin, Yuzhe CaiChina</p>
<p>Nankai University
TianjinChina</p>
<p>Weibin Meng</p>
<p>Huawei, Beijing, Yongqian SunChina</p>
<p>Nankai University
Dan PeiTianjinChina</p>
<p>Tsinghua University
BeijingChina</p>
<p>LogEval: A Comprehensive Benchmark Suite for Large Language Models In Log Analysis
2 Jul 202440B9C94287BCEE5A7792297CDC575F3A10.1145/nnnnnnn.nnnnnnnarXiv:2407.01896v1[cs.CL]Manuscript submitted to ACM Manuscript submitted to ACM
Log analysis is crucial for ensuring the orderly and stable operation of information systems, particularly in the field of Artificial Intelligence for IT Operations (AIOps).Large Language Models (LLMs) have demonstrated significant potential in natural language processing tasks.In the AIOps domain, they excel in tasks such as anomaly detection, root cause analysis of faults, operations and maintenance script generation, and alert information summarization.However, the performance of current LLMs in log analysis tasks remains inadequately validated.To address this gap, we introduce LogEval, a comprehensive benchmark suite designed to evaluate the capabilities of LLMs in various log analysis tasks for the first time.This benchmark covers tasks such as log parsing, log anomaly detection, log fault diagnosis, and log summarization.LogEval evaluates each task using 4,000 publicly available log data entries and employs 15 different prompts for each task to ensure a thorough and fair assessment.By rigorously evaluating leading LLMs, we demonstrate the impact of various LLM technologies on log analysis performance, focusing on aspects such as self-consistency and few-shot contextual learning.We also discuss findings related to model quantification, Chinese-English question-answering evaluation, and prompt engineering.These findings provide insights into the strengths and weaknesses of LLMs in multilingual environments and</p>
<p>INTRODUCTION</p>
<p>With the rapid development of information technology, information systems have become the cornerstone of business and organizational operations.Especially in fields such as cloud computing, 5G networks, and financial information systems, efficient IT operations are crucial for ensuring the stability and efficiency of these systems.The increasing scale and complexity of these systems, driven by the rapid development of the internet, have made AI-assisted operations, or AIOps, an emerging trend in the operations field.Gartner [19] defines AIOps as a set of methods that use AI technology to handle tasks including anomaly detection, fault analysis, alert summarization, performance optimization, and capacity planning.In this context, log analysis plays a particularly significant role.Logs record the real-time status, key events, and error information of systems.By analyzing these data in-depth, operations personnel can quickly identify and resolve system issues, thereby enhancing system performance, reliability, and security.Logs are also essential in various scenarios: in cloud computing, they help monitor resource utilization and detect anomalies in virtual machines; in 5G networks, they track the performance of network components and identify faults; in financial information systems, they are used to audit transactions and ensure regulatory compliance.The ability to quickly and accurately analyze logs can prevent system outages, enhance performance, and maintain security.However, traditional log analysis methods primarily rely on manual processing and rule-setting, which are inefficient, prone to high false positive rates, and limited in handling large-scale, complex, and evolving log data.</p>
<p>Given the critical role of log analysis in maintaining system health, applying large language models (LLMs) to log analysis can significantly improve the efficiency and accuracy of these tasks.The widespread application of LLMs has revolutionized numerous fields.LLMs have garnered significant attention from both academia [2,5] and industry [12,21,28,41,47,50] due to their advantages over traditional text generation methods.For instance, due to being capable of capturing rather long dependencies in sentences, LLMs are seeing wide adoption in commercial text generation including OpenAI's GPT products (e.g., ChatGPT) [4,6,30,32] and Meta's LLaMA products [33,39].Models like GPT-4[29], LLaMA-2 [9], ChatGLM4 [38], and Qwen1.5 [1] have demonstrated their capabilities in tasks such as text generation, language translation, sentiment analysis, and more.For instance, GPT-4 is used in customer service chatbots to provide quick and accurate responses, LLaMA-2 enhances virtual assistants by improving natural language understanding, ChatGLM4 aids in content creation by generating coherent and contextually relevant text, and Qwen has been applied in various natural language understanding and generation tasks.These applications highlight the versatility and potential of LLMs in handling various natural language processing (NLP) tasks.</p>
<p>Manuscript submitted to ACM Despite the significant achievements of LLMs in natural language processing tasks and the existence of benchmarks for evaluating general NLP-related capabilities, their performance and applicability in log analysis tasks remain unclear.</p>
<p>Therefore, we propose LogEval, a specialized benchmark suite designed to comprehensively assess the capabilities of LLMs across various log analysis tasks, such as log parsing, anomaly detection, fault diagnosis, and log summarization.</p>
<p>Nevertheless, due to the specialty of the log tasks, constructing an LogEval benchmark presents the following challenges:</p>
<p>â€¢ Data Sensitivity and Availability: Although companies have vast amounts of operational data, high-quality datasets suitable for model training and evaluation are scarce.The lack of high-quality public datasets limits the effective evaluation and optimization of models.</p>
<p>â€¢ Model Selection and Optimization: The lack of a comprehensive and authoritative benchmark makes it difficult to evaluate the AIOps capabilities of current large models, especially their performance in log analysis tasks.The AI community sees rapid development with new models and technologies emerging frequently, making it crucial to select the most practical and effective solutions.</p>
<p>â€¢ Evaluation and Verification: Different log analysis tasks require distinct evaluation frameworks.Each task needs tailored evaluation metrics to ensure accurate and appropriate assessment of model performance.</p>
<p>To address these challenges, LogEval makes the following contributions:</p>
<p>â€¢ Characterization: We are the first to investigate and characterize the application of LLMs in log analysis tasks, addressing the critical challenges and opportunities in this domain.Our comprehensive study involves extensive empirical analyses of LLMs, highlighting their potential to significantly enhance the efficiency and accuracy of log analysis while identifying specific areas that require further optimization.</p>
<p>â€¢ Approach: We introduce LogEval, a pioneering benchmark suite designed specifically for evaluating LLM capabilities in log analysis.LogEval includes:</p>
<p>-Dataset Construction: We constructed a diverse dataset containing 4,000 publicly available log entries, addressing challenges related to data sensitivity and resource limitations.This dataset encompasses 15 different Chinese and English prompts, rotated to minimize prompt-specific model performance biases.</p>
<p>-Comprehensive Benchmark Development: LogEval evaluates 18 mainstream large models across four primary log analysis tasks: log parsing, anomaly detection, fault diagnosis, and log summarization.We employ zero-shot and few-shot evaluation methods, leveraging techniques like self-consistency and prompt engineering to ensure consistent and accurate assessments.</p>
<p>-Multidimensional Evaluation Metrics: We designed various evaluation rules for each model to ensure precise assessments.For different tasks, we use metrics such as F1-score and accuracy, introducing new metrics based on semantic matching and average inference time to comprehensively evaluate LLM performance in log analysis tasks.</p>
<p>â€¢ Evaluation: We conduct a rigorous evaluation of LogEval, assessing the performance of 18 mainstream LLMs on log analysis tasks.The evaluation demonstrates the strengths and limitations of each model, providing valuable insights for researchers and practitioners.The results reveal that while LLMs show promise in enhancing log analysis efficiency and accuracy, there are significant variations in performance across different tasks and models, underscoring the need for targeted optimizations.</p>
<p>Through the evaluation and analysis of LogEval, we aim to gain deeper insights into the strengths and limitations of LLMs in log analysis tasks, providing valuable guidance and reference for researchers and practitioners in the field.We Manuscript submitted to ACM believe that LogEval will play a significant role in advancing the application and development of LLMs for log analysis, offering effective solutions for practical challenges in real-world scenarios.</p>
<p>Paper organizationï¼šOur paper is organized as follows, Section 2 presents background information and discusses related work.Section 3 details our methodology and evaluation framework.Section 4 describes the experimental setup.Section 5 shows the experimental results.Section 6 provides a summary of the findings.Section 7 delves into a discussion of the results.Section 8 offers additional context and insights.</p>
<p>RELATED WORK</p>
<p>With the rapid advancement of LLMs, their diverse and complex capabilities have increasingly garnered significant attention.Traditional NLP metrics often fall short in accurately assessing these capabilities, prompting scholars to propose benchmarks specifically tailored for LLMs.This section discusses the evaluation of LLMs in general NLP domains and their applications in the specific context of log analysis tasks.</p>
<p>Evaluation of LLMs in General NLP Tasks</p>
<p>The evaluation of LLMs in NLP tasks has diversified as these models have become capable of handling increasingly complex and varied tasks.Evaluations now not only measure basic linguistic understanding and generation but also delve into nuanced capabilities such as reasoning, domain-specific knowledge, and adaptability to different tasks.Here, we categorize these evaluations based on the nature of the tasks and the methodologies used.</p>
<p>Comprehensive Assessments: Comprehensive assessments are designed to evaluate the broad capabilities of LLMs across multiple dimensions.For instance, HELM [22] utilizes a diverse set of metrics to assess LLMs in 42 unique scenarios, providing insights into their general linguistic abilities and reasoning skills.BIG-bench [36] extends this by including tasks that challenge the models' understanding of common sense, logic, and even creativity.Specialized Knowledge Assessments: These assessments focus on evaluating the LLMs' performance in domains requiring specialized knowledge.For example, FinEval [46] measures financial acumen, while MultiMedQA [35] tests medical knowledge by using datasets derived from professional exams and consultation records.Similarly, Huatuo-26M [20] evaluates medical consultation capabilities, reflecting real-world medical inquiry handling.</p>
<p>Real-World Application Simulations: Several benchmarks simulate real-world applications to see how well LLMs perform in practical scenarios.OpsEVAL [26] assesses the ability of LLMs to manage IT operations through a set of structured tasks in both Chinese and English.NetOps [27] focuses on network operations, testing LLMs with tasks that mimic real-world challenges in network management.</p>
<p>Language Generation and Comprehension:</p>
<p>This category tests the LLMs' ability to generate coherent and contextually appropriate text and to comprehend complex material.CG-Eval [45] assesses generation capabilities through tasks requiring term definitions, short-answer responses, and solving computational problems.MMCU [44] establishes a comprehension baseline with questions from academic and professional exams, pushing the models to demonstrate their understanding and application of learned knowledge.</p>
<p>Evaluation of LLMs in Log Analysis Tasks</p>
<p>Log analysis plays a crucial role in maintaining the health and performance of information systems.It involves several key tasks: log parsing, log anomaly detection, log fault diagnosis, and log summarization.Each task presents unique challenges and requires specific capabilities from LLMs.</p>
<p>Manuscript submitted to ACM</p>
<p>As the application of LLMs in log analysis tasks increases, researchers have begun to explore how these models can be leveraged to enhance system monitoring and fault detection.Although some studies have attempted to apply LLMs to tasks such as log parsing [13,43,48] and anomaly detection [7,15,25,31], these applications are largely in the preliminary stages and lack a systematic evaluation framework to comprehensively measure the performance of LLMs in these tasks.</p>
<p>Log Parsing Tasks: Log parsing, the process of transforming raw logs into structured data, is foundational for log analysis.LILAC [13] introduces an adaptive parsing cache to significantly improve template accuracy and query times for large language models.DivLog [43] is an LLM-based log parsing framework that achieves state-of-the-art performance, with an average accuracy of 98.1%, precision of 92.1%, and recall of 92.9% across 16 public datasets.ECLIPSE [48] introduces a novel approach, leveraging LLMs and semantic entropy-LCS, to address the challenges of log parsing in industrial settings.</p>
<p>Log Anomaly Detection Tasks: Identifying anomalous patterns within logs, commonly used for fault warning and detection, is another critical area.SeaLog [25] employs a Trie-based Detection Agent for real-time anomaly detection and incorporates feedback from experts, including large language models like ChatGPT, to enhance accuracy.LogGPT [31] leverages ChatGPT's language interpretation capabilities for log-based anomaly detection, showing promising results and interpretability on BGL and Spirit datasets.This research indicates that the potential of LLMs in log anomaly detection tasks warrants further exploration.</p>
<p>Log Fault Diagnosis Tasks: Log fault diagnosis aims to identify the root causes of system faults through log analysis.Face It Yourselves [34] introduces an LLM-powered two-stage approach for localizing configuration errors via logs, aiding end-users in identifying root causes without source code access.LogConfigLocalizer demonstrates high accuracy and feasibility in a case study.</p>
<p>Log Generation Tasks: Log generation involves automatically generating appropriate log statements to facilitate system maintenance and problem troubleshooting.UniLog [42] leverages the in-context learning paradigm of large language models to generate log statements without the need for model tuning.With only a prompt containing five demonstration examples, UniLog can produce appropriate logging statements and further enhance its logging capabilities after warming up with a few hundred random samples.</p>
<p>Other Applications: Additionally, some studies explore the application of LLMs in specific areas of log analysis, such as LLM4Sec [16] which evaluates various large language models for their suitability in log file analysis for cybersecurity.</p>
<p>Summary Cycles [3] investigates how Large Language Models can improve the efficiency of information handoff in collaborative intelligence analysis.</p>
<p>Currently, there is a lack of dedicated benchmarks for evaluating LLMs specifically in the context of log analysis tasks, making it challenging to assess and compare the performance of different models on these tasks.Therefore, this work aims to propose an evaluation framework for LLMs in log analysis tasks, addressing this research gap.Our evaluation efforts are not only intended to understand the strengths and limitations of LLMs in log analysis but also aim to provide valuable evaluation resources and guidance for the log analysis domain, promoting the effective application of LLMs in real-world log analysis scenarios.</p>
<p>Compared to previous research, our work provides a comprehensive evaluation framework that covers various log analysis tasks in the intelligent operations domain.By clearly defining task and capability classifications, LogEval offers detailed and extensive assessments, aiding in the selection and optimization of LLMs in log analysis and beyond.</p>
<p>LOGEVAL BENCHMARK</p>
<p>This section presents the comprehensive framework of LogEval (Fig. 1) from data collection to evaluation.The process involves four main stages: data collection, quality enhancement, formatting, and evaluation.The following subsections provide detailed descriptions and expansions for each step.</p>
<p>Data Collection</p>
<p>The data collection phase is critical for ensuring the breadth and representativeness of LogEval's evaluation results.We systematically collected open-source and industry datasets for four key log analysis tasks: log parsing, log anomaly detection, log fault diagnosis, and log summarization.</p>
<p>3.1.1Log Parsing and Log Anomaly Detection.We utilized large-scale datasets from LogPub [14], LogHub [11], and LogPAI [51].LogPub [14] includes real templates from 14 log datasets sourced from distributed systems, operating systems, and server-side applications.On average, each dataset in LogPub comprises 3.6 million log messages, all labeled with authentic log templates, totaling approximately 3500 templates.We selected commonly used BGL and ThunderBird datasets from LogPub for these tasks.</p>
<p>Specifically, the BGL (Blue Gene/L) dataset contains logs from large-scale parallel computing systems, while the ThunderBird dataset originates from high-performance computing clusters.By selecting these datasets, we ensure the diversity and representativeness of the data, covering a wide range of scenarios from distributed systems to highperformance computing environments.These datasets provide a solid foundation for evaluating LLM performance in log parsing and log anomaly detection tasks.</p>
<p>Log fault Diagnosis.</p>
<p>We employed open-source datasets from Alibaba Cloud and China Mobile, both demonstrating strong performance in relevant events.These datasets are crucial for evaluating the diagnostic capabilities of LLMs.</p>
<p>The significance of these datasets lies in their comprehensive log entries generated during system operations, recording various operations and fault information.For example, the Alibaba Cloud dataset includes logs from cloud service operations, capturing diverse fault events, while the China Mobile dataset covers logs from telecommunication networks, providing rich practical data for evaluation.</p>
<p>Log Summarization.</p>
<p>We used datasets labeled by LogSummary [8], including BGL, HDFS, HPC, Spark, Zookeeper, and Proxifier datasets, manually annotated based on data from LogHub.For each task, we collected 4000 logs, ensuring diversity and scale to cover various log types and analysis tasks comprehensively and fairly.</p>
<p>In the log summarization task, we paid particular attention to the diversity of log types.The BGL and HDFS datasets represent logs from high-performance computing and distributed file systems, while the HPC and Spark datasets involve logs from high-performance computing and big data processing environments.The Zookeeper and Proxifier datasets record logs from distributed coordination services and network proxy tools.By encompassing these different log types, we comprehensively evaluate LLM performance in generating concise and accurate log summaries.</p>
<p>By compiling these datasets, we ensured that LogEval can assess LLM performance across a wide range of scenarios, capturing the complexity and variability inherent in real-world log data.</p>
<p>Quality Enhancement</p>
<p>To enhance the quality of evaluation, we implemented a rigorous data preprocessing and quality enhancement process:</p>
<p>Manuscript submitted to ACM During the classification process, we carefully reviewed each task's dataset to ensure its applicability and representativeness.For example, subjective questions in log parsing tasks typically involve understanding and templating log structure, requiring models to identify and extract key elements from logs.In contrast, objective questions in log anomaly detection and log fault diagnosis require models to accurately identify and classify log events, often with clear answers.This classification ensures that each task's evaluation metrics and methods accurately reflect the model's performance in the specific task.</p>
<p>Standardization.</p>
<p>We standardized the format of manually curated questions to ensure consistency.Each question was structured to include an instruction prompt, input, and output.This standardization is crucial for maintaining uniformity in evaluation and facilitating comparative analysis across different models.</p>
<p>The standardization process involved clearly defining the instruction prompts for each question, ensuring that models understand the task requirements.For example, in log parsing tasks, we provide clear instructions for models to convert logs into template formats; in log anomaly detection tasks, we instruct models to mark logs as "normal" or "abnormal." Additionally, we ensured that each input log and expected output adhered to a uniform format standard, enabling comparative analysis across different models.</p>
<p>Question Categorization.</p>
<p>To further refine the evaluation process, we designed both subjective and objective questions.Subjective questions require models to generate responses based on understanding and contextual relevance, while objective questions provide clear, definitive answers.This dual approach helps in accurately gauging both the interpretative and factual capabilities of LLMs.</p>
<p>In designing subjective questions, we focus on the quality of model-generated responses, including coherence and contextual relevance.For example, in log summarization tasks, subjective questions require models to generate concise yet comprehensive log summaries, demonstrating the model's language generation capabilities and understanding of log content.For objective questions, we emphasize accuracy and consistency, such as identifying and marking abnormal log events in log anomaly detection tasks.This categorization allows for a comprehensive evaluation of LLM performance across different types of tasks.</p>
<p>Formatting</p>
<p>In the formatting phase, we established a structured approach to ensure clear and effective evaluation prompts: The key to prompt design is providing sufficient contextual information to enable the model to understand the task accurately.For example, in log parsing tasks, we provide a sample log and instruct the model to parse it into a standard template format; in log anomaly detection tasks, we provide multiple log samples and instruct the model to mark them as normal or abnormal.By providing clear instructions and context, we ensure that the model can generate high-quality responses, facilitating accurate evaluation.</p>
<p>Bilingual</p>
<p>Prompts.We developed both Chinese and English prompts for each task, utilizing fifteen different prompts per task to mitigate the impact of prompt variations on evaluation results.This bilingual approach ensures that the evaluation covers linguistic diversity and provides a robust assessment of model capabilities in different languages.</p>
<p>?? presents examples of English prompts for each task.</p>
<p>The bilingual prompt design not only enhances the linguistic coverage of the evaluation but also helps detect performance differences when models handle tasks in different languages.For example, in log parsing tasks, we designed prompts with identical content in both Chinese and English, evaluating the model's performance in processing logs in both languages.By comparing model performance under bilingual prompts, we gain insights into the model's language processing capabilities and adaptability, providing references for model improvement and optimization.</p>
<p>Diverse Scenarios.</p>
<p>The prompts were designed to cover a wide range of scenarios, reflecting the real-world complexity and variability of log data.This diversity is essential for testing the adaptability and generalization capabilities of LLMs.</p>
<p>In prompt design, we considered various possible log scenarios, including but not limited to system start-up and shutdown logs, error logs, performance logs, and user activity logs.Each scenario has its unique characteristics and challenges, requiring models to possess extensive knowledge and flexible processing capabilities.By covering these diverse scenarios, we comprehensively test the adaptability and generalization capabilities of LLMs, evaluating their performance in handling various real-world log tasks.</p>
<p>Evaluation Settings</p>
<p>The evaluation phase involves assessing the performance of LLMs using a comprehensive set of metrics tailored to both objective and subjective questions: 3.4.1 Objective Questions.Objective questions are designed as multiple-choice questions with clear, definitive answers.The primary metrics used for evaluation are Accuracy and F1-score.Despite specifying fixed outputs and using few-shot prompts, LLM outputs may still contain extraneous information.Therefore, we employed a choice extraction function based on regular expressions to extract predicted answers.Accuracy is then calculated based on these extracted answers and ground-truth labels.</p>
<p>To ensure the accuracy of evaluation, we set clear criteria for each objective question.For example, in log anomaly detection and log fault diagnosis tasks, we use regular expressions to extract the model's predicted answers and compare Manuscript submitted to ACM "id": 0, "instruction": "Please review the log entry and explicitly mark it as 'normal' or 'abnormal', only output 'normal' or 'abnormal'" "input": "\nlog entry:\ninstruction cache parity error corrected" "output": "normal" "id": 0, "instruction": "Parse the following log entry into a template format, replacing variable parts with a wildcard &lt;<em>&gt;, and focus the answer after the keyword 'Answer'" "input": "\nlog entry:\nsynchronized to 10.100.28.250, stratum 3" "output": "synchronized to &lt;</em>&gt;, stratum &lt;*&gt;" "id": 0, "instruction": "In our data scenario, there are three types of faults: Processor CPU Caterr, Memory Throttled Uncorrectable Error Correcting Code, Hard Disk Drive Control Error Computer System Bus Short Circuit Programmable Gate Array Device Unknown.Analyze the log entry and identify the type of fault that occurred.Only output the fault type."</p>
<p>"input": "\nlog entry:\nProcessor #0xfa | Configuration Error | Asserted" "output": "Processor CPU Caterr" Fig. 2. Three examples of the processed questions them with the ground-truth labels to calculate accuracy and F1-score.Additionally, we designed few-shot prompts, providing example answers to help models better understand the task requirements and improve their prediction accuracy.</p>
<p>Subjective</p>
<p>Questions.Subjective questions require models to rely more on their understanding and knowledge base.The evaluation metrics include:</p>
<p>â€¢ Word Overlap: Using ROUGE [23] scores, which are standard in NLP tasks, particularly in translation.These metrics assess the lexical similarity between the generated response and the reference answer.. â€¢ Semantic Similarity: Using cosine similarity to measure the semantic closeness between sentences.This metric provides insights into the contextual and conceptual accuracy of the generated responses.</p>
<p>In evaluating subjective questions, we focus on the quality of model-generated responses.For example, in log parsing and log summarization tasks, we use different evaluation metrics to comprehensively assess the model's performance: log parsing uses parsing accuracy and edit distance as evaluation metrics.Parsing accuracy measures the model's ability to correctly parse log information, while edit distance evaluates the differences between the generated response and the reference answer.Log summarization uses accuracy and ROUGE-1 F1 scores to evaluate.Accuracy measures the correctness of the generated summaries, the accuracy of log summarization is calculated by using cosine similarity to measure the similarity between the generated summaries and the reference summaries.When the similarity exceeds a preset threshold (0.25), it is considered a correct prediction.Accuracy is the ratio of the number of correct predictions to the total number of predictions.ROUGE-1 F1 scores assess the lexical overlap between the generated and reference summaries.</p>
<p>Additional Metrics. To comprehensively assess the performance of LLMs, we introduced two additional metrics:</p>
<p>Manuscript submitted to ACM  These additional metrics help us gain a more comprehensive understanding of the model's performance.For example, the average token count can help us evaluate the verbosity of the model's responses, optimizing the output efficiency.</p>
<p>The inference time helps us assess the model's processing speed, particularly in real-world application scenarios.By comprehensively evaluating these metrics, we can fully understand the strengths and weaknesses of different models, providing references for selecting and optimizing LLMs for various log analysis tasks.</p>
<p>Manuscript submitted to ACM</p>
<p>EXPERIMENT DESIGN</p>
<p>In this section, we present the experimental design of LogEval, aiming to evaluate various LLMs to comprehend their effectiveness in addressing different types of questions (multiple-choice and open-ended) and various log analysis tasks.</p>
<p>Models</p>
<p>We evaluated popular LLMs from different organizations, covering a spectrum of weights.The selection criteria encompassed diversity in architecture, training data, and model size to ensure comprehensive analysis.Detailed information on all LLMs assessed is provided in Table 2, with further details available in the appendix.</p>
<p>Prompting Techniques</p>
<p>To comprehensively understand the performance of different language models on log analysis tasks, we employ a variety of evaluation approaches.In objective question evaluations, we utilize both zero-shot and few-shot methods.</p>
<p>With zero-shot evaluations, we aim to assess the language model's capabilities from the perspective of ordinary users, as users typically do not provide any examples in regular usage.With the few-shot approach, our goal is to evaluate the language model's potential from the perspective of developers, which often yields better performance than the zero-shot setup.For each evaluation method, we employ two settings to assess the language model: the naive Q&amp;A (Naive) and self-consistency(SC) [40] Q&amp;A .Given that we have both English and Chinese questions, we design corresponding naive Q&amp;A prompts for each language.</p>
<p>â€¢ Naive Question-Answer: The language model is expected to generate answers without any additional explanations.</p>
<p>â€¢ Self-Consistency (SC):</p>
<p>The same question is asked to the language model multiple times, and the answer that appears most frequently among the model's generated answers is extracted.In implementation, we set the number of SC queries to 5.</p>
<p>In subjective question evaluations, we combine each task along with the questions themselves as inputs to the language model.In subjective questions, we aim to simulate the everyday usage of language models by ordinary users.</p>
<p>We input the questions into the language model and generate answers.Therefore, we only use the zero-shot evaluation for the language model in the naive Q&amp;A for subjective questions.</p>
<p>Manuscript submitted to ACM</p>
<p>Baselines Design</p>
<p>For the baseline of log anomaly detection, We choose NeuralLog [17] and LogRobust [49] .</p>
<p>â€¢ NeuralLog: NeuralLog is a novel approach that utilizes deep learning to detect anomalies directly from raw log data without the need for traditional log parsing.NeuralLog automates the feature extraction process by learning the inherent patterns and structures within the unstructured log texts, effectively bypassing complex preprocessing steps.This method significantly reduces the reliance on domain knowledge and manual effort typically required in setting up log anomaly detection systems.NeuralLog has demonstrated high accuracy and efficiency in anomaly detection, making it particularly valuable for real-time monitoring systems.The benefits of adopting NeuralLog include simplified system maintenance, improved automation in anomaly detection processes, and enhanced accuracy in identifying potential threats or system faults promptly.</p>
<p>â€¢ LogRobust: LogRobust is a methodology designed to enhance anomaly detection in environments where log data is prone to instability and frequent changes.LogRobust employs advanced techniques to adaptively learn and update its models as it encounters new or altered log messages, ensuring resilience to changes in log formats or content.It utilizes a combination of unsupervised learning algorithms to detect outliers and anomalies effectively even in highly dynamic systems.By focusing on stability and adaptability, LogRobust minimizes the false positive rates that often plague traditional log anomaly detection systems facing volatile data.The primary benefits of LogRobust are its robustness against log data variations, improved anomaly detection accuracy, and reduced need for manual intervention in maintaining parsing models, making it ideal for critical systems requiring continuous monitoring.</p>
<p>For the baseline of log parsing, we choose Drain [10] and LogPPT [18].</p>
<p>â€¢ Drain: Drain is an innovative online log parsing method which leverages a fixed-depth tree structure to systematically group and parse log messages.The core idea behind Drain is to categorize log lines based on predefined log grouping rules and extract templates using a fixed depth parse tree, minimizing computational overhead and increasing parsing speed.By employing a parsing tree with a fixed depth and using heuristics to handle variability in log data, Drain ensures both high efficiency and accuracy in real-time log parsing scenarios.This method efficiently adapts to diverse log formats and dynamically changing log content, reducing the need for frequent manual reconfiguration.The benefits of Drain include significant improvements in parsing speed and flexibility, making it an effective solution for systems that require real-time log analysis and monitoring.</p>
<p>â€¢ LogPPT: LogPPT is a deep learning-based method for log parsing, aiming to automatically parse logs and improve accuracy by learning patterns and structures in log files.Treating log lines as sequential data, it models sequence relationships using deep learning models to enhance parsing efficiency and generalization capability.</p>
<p>The advantages of LogPPT lie in its automated parsing, accuracy, generalization capability, and efficiency improvement, providing strong support for the field of log analysis.</p>
<p>For the baseline of log fault diagnosis, we choose LogKG [37] and LogCluster [24].</p>
<p>â€¢ LogKG: LogKG is a framework that utilizes knowledge graphs to enhance the process of diagnosing faults from system logs.LogKG constructs a comprehensive knowledge graph from parsed log data, integrating various log entities and their relationships to capture a detailed representation of system interactions and behaviors.This structured approach enables more precise and interpretable diagnostics by utilizing graph-based analytics to trace faults and identify their root causes effectively.By integrating semantic reasoning and relational data, LogKG Manuscript submitted to ACM facilitates an in-depth analysis that outperforms traditional log analysis methods which often depend solely on textual data.The key benefits of using LogKG include improved accuracy in fault diagnosis, faster problem resolution times, and a more intuitive understanding of complex system behaviors, all of which contribute to better reliability and maintenance of IT systems.</p>
<p>â€¢ LogCluster: LogCluster is a log fault diagnosis technique leveraging clustering, where it primarily involves preprocessing raw logs to create structured representations, computing vectorized representations of log sequences, measuring the similarity between log events, and subsequently clustering similar logs using hierarchical clustering.This method efficiently automates the discovery of typical and anomalous patterns amidst voluminous logs, significantly reducing manual troubleshooting efforts, making it particularly suitable for log fault diagnosis in large-scale distributed systems.</p>
<p>For the baseline of log summary, we choose LogSummary [8].</p>
<p>â€¢ LogSummary: LogSummary generates concise log summaries by extracting and ranking key phrases, aiming to preserve critical information from the raw logs while minimizing redundancy.The method begins with the preprocessing of logs, including cleaning and normalization, followed by employing algorithms like TF-IDF or TextRank to identify and extract key information from the logs.Finally, it constructs summaries based on the extracted key information.LogSummary's benefits lie in its ability to swiftly produce high-quality log summaries that offer users a bird's-eye view of log insights, facilitating rapid issue localization.It is particularly suitable for compressing and quickly analyzing large-scale, real-time log streams.</p>
<p>In the baseline experiments, for tasks related to log parsing and log summary, we adopt the same dataset utilized in the evaluation of large language models, comprising 4,000 logs.For log anomaly detection and fault diagnosis, our dataset consists of 4,000 sequences, each formed by the raw 4,000 logs from the large language model assessment and their respective 10 logs above and below in context.</p>
<p>EVALUATION</p>
<p>In this section, to comprehensively and intuitively demonstrate the performance of various models and their overall evaluation across different tasks, we have designed two heatmaps, as illustrated in Fig. 3 and Fig. 4.</p>
<p>The heatmaps specifically highlight the zero-shot and few-shot performances of select large models in the context of original question-answering scenarios.Each radar chart incorporates four log analysis tasks as evaluation metrics, comprehensively spanning log parsing, log anomaly detection, log fault diagnosis, and log summary extraction.The variations in the polygon shapes of the radar charts reveal that the models exhibit better performance in log parsing tasks but fare less satisfactorily in log anomaly detection.Furthermore, it is evident that the few-shot approach yields superior results compared to zero-shot, illustrating the models' capability to learn task-relevant knowledge from just a few examples.-GPT-4 performs the best with a score of 0.58, demonstrating its strong natural language parsing capabilities.</p>
<p>-Claude3 Sonnet and Gemini Pro also perform well, with scores of 0.42 and 0.36, respectively, indicating good performance in log parsing.-LLama2-7B and LLama2-13B also perform well, with scores of 0.57 and 0.54, respectively.â€¢ Log Fault Diagnosis Task:</p>
<p>-ChatGLM4 and Claude3 Sonnet perform the best in this task, with scores of 0.38 and 0.37, respectively.</p>
<p>-GPT-3.5 and GPT-4 also show good performance, with scores of 0.36 and 0.35, respectively.</p>
<p>Manuscript submitted to ACM</p>
<p>â€¢ Log Summary Extraction Task:</p>
<p>-Claude3 Sonnet performs the best with a score of 0.59, demonstrating its strong ability in summarizing information.</p>
<p>-GPT-3.5 and Gemini Pro also perform well, with scores of 0.45 and 0.46, respectively.Fig. 4 illustrates the performance of different models in few-shot scenarios across the same four tasks.From the figure, we can observe the following:</p>
<p>â€¢ Log Parsing Task:</p>
<p>-GPT-4 performs the best with a score of 0.88, demonstrating its strong few-shot learning capabilities.</p>
<p>-Claude3 Sonnet and Gemini Pro also perform exceptionally well, with scores of 0.87 and 0.84, respectively.</p>
<p>â€¢ Log Anomaly Detection Task:</p>
<p>-Gemini Pro stands out with the highest score of 0.56, indicating its effective anomaly detection in few-shot scenarios.</p>
<p>-GPT-4 and GPT-3.5 also perform well, with scores of 0.53 and 0.39, respectively.</p>
<p>â€¢ Log Fault Diagnosis Task:</p>
<p>-GPT-4 and GPT-3.5 perform the best, with scores of 0.91 and 0.88, respectively, showing their superiority in complex fault diagnosis tasks.</p>
<p>-Gemini Pro and Qwen1.5-72B also show strong performance with scores of 0.87 and 0.83, respectively.</p>
<p>â€¢ Log Summary Extraction Task: -Qwen1.5-72Band Qwen1.5-14Bperform well, with scores of 0.78 and 0.68, respectively.</p>
<p>-Gemini Pro and ChatGLM4 also show improved performance, with scores of 0.65 and 0.56, respectively.</p>
<p>From the zero-shot and few-shot performances, the following conclusions and patterns can be drawn:</p>
<p>Task Adaptability of Models: GPT-4 shows stable performance across multiple tasks, particularly excelling in few-shot scenarios, demonstrating strong task adaptability and few-shot learning abilities.Its performance in log parsing and fault diagnosis tasks is particularly notable, making it suitable for applications requiring high precision parsing and diagnosis.Claude3 Sonnet excels in log parsing and log summary extraction tasks, showcasing its potential in information extraction and summarization, suitable for scenarios requiring efficient information extraction.LLama2-70B performs excellently in zero-shot scenario in log anomaly detection, making it suitable for anomaly detection tasks, demonstrating its strong capability in recognizing anomalies.</p>
<p>Few-shot Learning Capability: Overall, the few-shot scenario performance surpasses the zero-shot scenario, indicating that these large models can learn task-relevant knowledge from a small number of examples.This is particularly significant for real-world applications where data may be limited.GPT-4 and Claude3 Sonnet are especially notable in few-shot learning scenarios, making them ideal for applications requiring rapid adaptation and efficient learning.</p>
<p>Task Performance Variance: Different models exhibit significant performance variance across different tasks, suggesting that model selection should be based on specific task requirements.For instance, LLama2-70B is preferable for log anomaly detection tasks in zero-shot scenario, while GPT-4 and Claude3 Sonnet are better suited for complex log fault diagnosis tasks.</p>
<p>Selection Strategy for Practical Applications: For scenarios requiring multi-task processing, models with stable performance like GPT-4 should be prioritized due to their exceptional performance across multiple tasks, especially in few-shot scenarios where their strong learning capabilities can significantly enhance task efficiency.If the application Manuscript submitted to ACM scenario primarily involves log parsing and summarization, Claude3 Sonnet is a suitable choice due to its outstanding performance in these tasks.For tasks focused on anomaly detection, LLama2-70B is recommended, as it outperforms other models in log anomaly detection.</p>
<p>These insights provide valuable references for the application of large models in log analysis, further demonstrating the effectiveness of few-shot learning methods in improving model performance.Future research can further explore the performance of these models in other tasks, seeking more optimization strategies and application scenarios.</p>
<p>Additionally, we have aggregated the average accuracy scores across these four tasks to conduct a holistic assessment of all large models across these tasks.As depicted in Fig. 5 and Fig. 6, which illustrate the models' performance under both zero-shot and few-shot settings in the context of original question-answering.On the other hand, weight-based LLMs, such as InternLM2-20B and Mistral-7B, require users to host the models locally, offering greater control over the model's tuning and customization.These models demonstrate significant improvements in few-shot settings, as seen from their enhanced accuracy scores.The capability to fine-tune weight-based models on specific datasets allows them to adapt more effectively to niche tasks or specialized applications.</p>
<p>The robust performance of API-based models across diverse conditions can be attributed to several factors.Firstly, these models often leverage extensive computational resources and are trained on vast and diverse datasets, enabling them to generalize well to a wide range of queries.Secondly, the continuous updates and optimizations from service providers ensure that API-based models remain state-of-the-art, incorporating the latest advancements in language modeling.</p>
<p>Conversely, the adaptability of weight-based models in few-shot scenarios underscores the importance of taskspecific fine-tuning.By allowing users to tailor the models to specific datasets, weight-based LLMs can achieve higher performance in specialized applications where generic, pre-trained models may fall short.3 shows the parsing accuracy and edit distance of zero-shot and few-shot</p>
<p>Chinese naive Q&amp;A and English naive Q&amp;A under log parsing of 18 LLMs.</p>
<p>From the overall performance results, we can draw several conclusions:</p>
<p>â€¢ Performance of GPT-4: GPT-4 consistently outperforms all other models across both Chinese and English questions in zero-shot and few-shot settings.This superiority is reflected not only in the accuracy scores but also in the significantly lower edit distances, particularly notable in the Chinese few-shot scenario where the Manuscript submitted to ACM edit distance is as low as 2.69.This indicates GPT-4's efficiency in error correction and underscores its capacity to understand and process tasks deeply.â€¢ Effectiveness of Few-shot Learning: The majority of models exhibit better performance in few-shot settings compared to zero-shot.For instance, the Chinese accuracy of Qwen1.5-72bimproves from 0.339 to 0.709, with a corresponding decrease in edit distance from 16.37 to 9.89.This improvement suggests that models enhance their parsing accuracy and error handling capabilities when exposed to more relevant examples.</p>
<p>â€¢ Impact of Model Size: Larger models, such as Qwen1.5-72band GPT-4, generally perform better in terms of both accuracy and edit distance compared to smaller models like Qwen1.5-7b and AquilaChat-7b.This observation is consistent across both zero-shot and few-shot settings.For example, the edit distance for Qwen1.5-72b in Chinese decreases from 16.37 in zero-shot to 9.89 in few-shot, whereas smaller models like AquilaChat-7b exhibit high edit distances in both settings (47.99 in zero-shot and 48.67 in few-shot).</p>
<p>â€¢ Language-specific Performance: Some models exhibit a marked difference in performance between Chinese and English.For instance: InternLM2-20b shows better performance in Chinese few-shot settings with an accuracy of 0.645 and an edit distance of 8.59, compared to its English performance with an accuracy of 0.528 and an edit distance of 14.17.Qwen1.5-72bperforms better in Chinese few-shot settings with an accuracy of 0.709 and an edit distance of 9.89, compared to its English performance with an accuracy of 0.533 and an edit distance of 10.89.ChatGLM4 shows a higher accuracy in English few-shot settings with an accuracy of 0.601 and an edit distance of 8.61, compared to its Chinese performance with an accuracy of 0.537 and an edit distance of 10.57.</p>
<p>â€¢ Consistency Across Tasks: Certain models, such as Claude3 Sonnet, demonstrate excellent consistency across different languages and settings.For example, it achieves the lowest edit distance in English few-shot settings at 1.97, showcasing its superior adaptability across tasks.This in-depth analysis provides a clearer understanding of the performance of various language models in log parsing tasks.Future research can address these models' limitations in specific tasks and languages by improving model training and fine-tuning approaches, thereby enhancing their overall performance and adaptability.</p>
<p>Manuscript submitted to ACM 5.1.2Naive Q&amp;A results on Log Anomaly Detection.Table 4 respectively shows the accuracy and f1_scores of Chinese naive Q&amp;A and the Accuracy and F1-scores of English naive Q&amp;A with zero-shot and few-shot for 18 LLMs under log anomaly detection."id": 2645, "label": "æ­£å¸¸", "predict": "æ—¥å¿—1: 'æ­£ å¸¸'\n\n æ—¥å¿—2: nor-mal\nlog3: abnor-mal\n4: å¼‚å¸¸\n5: æ­£ å¸¸\n6å¼‚å¸¸"</p>
<p>"id": 2677, "label": "æ­£å¸¸", "predict": "æ­£å¸¸\næ­£ å¸¸ å¼‚å¸¸ æ­£å¸¸"</p>
<p>"id": 706, "label": "æ­£å¸¸", "predict": "'normal', 'normal"normal"'abnorma'"</p>
<p>"id": 772, "label": "æ­£å¸¸", "predict": "nor-mal\nnormal æ­£ å¸¸\nabnormaå¼‚å¸¸"</p>
<p>Fig. 7. Few error examples on Log Anomaly Detection</p>
<p>From the overall performance results, we can draw several conclusions:</p>
<p>â€¢ Performance of Few-shot Learning in Anomaly Detection Tasks: Overall, few-shot learning results do not outperform zero-shot results in the log anomaly detection task.This is because models tend to repeat answers from the provided samples, failing to accurately recognize and respond to different anomalies when encountering new anomaly detection problems.</p>
<p>â€¢ Performance Differences between Chinese and English Q&amp;A: In both zero-shot and few-shot settings, the accuracy of English Q&amp;A generally exceeds that of Chinese Q&amp;A.However, in some cases, the F1 scores show an opposite trend, indicating that models are more accurate in identifying English anomalies but perform more balancedly when handling Chinese anomalies.For example, LLaMa2-7B achieved a high accuracy of 0.943 in zero-shot English Q&amp;A but had a low F1 score, indicating potential bias in handling specific anomaly categories.</p>
<p>From the analysis, we can draw the following scientifically rigorous conclusions:</p>
<p>â€¢ Few-shot learning does not outperform zero-shot results in log anomaly detection tasks, likely due to models' tendency to repeat sample answers and fail to accurately recognize new anomalies.â€¢ There are significant differences in performance between Chinese and English Q&amp;A, indicating the need for language-specific approaches in multi-language log anomaly detection.</p>
<p>â€¢ Some models, such as LLaMa2 series, show F1 scores of 0. This indicates that these models failed to correctly predict any anomalies in the test cases.The primary reason for this is that these models do not understand the questions well and tend to output the example responses provided during the few-shot learning phase, rather than generating responses relevant to the new questions.As illustrated in Fig. 7, the model's output (predict) includes multiple answers, demonstrating that the model does not fully understand the question.From the overall performance evaluation results, it is clear that the few-shot results are generally better than zero-shot results in the fault diagnosis task.The samples provided cover all fault types, making it easier for the models to learn and master them.</p>
<p>Manuscript submitted to ACM</p>
<p>â€¢ Performance of Few-shot Learning in fault Diagnosis Tasks:in the fault diagnosis task, few-shot learning results generally outperform zero-shot results.This is because the provided samples cover all fault types, allowing models to learn and master these types more effectively.â€¢ Performance Differences between Models:GPT-4, in particular, outperforms other models in both English and Chinese fault diagnosis tasks, with F1-scores of 0.9 or more, which is impressive.On the other hand, the BaiChuan model performs poorly in both zero-shot and few-shot Chinese and English fault diagnosis tasks.This may be due to issues such as incomplete output or confusing fault vocabulary when dealing with fault types in BaiChuan, as well as the tendency to output multiple fault types in the few-shot setting.Fig.8 illustrates this output for some of the Baichuan models, visualizing their specific performance in the fault diagnosis task.</p>
<p>"id": "8" "label": "Processor CPU Caterr" "predict": "answer: 'Processor CPUR';" "id": "2343" "label": "Mirror does not specify output" "predict": "Mirrordoes not specified output" "id": "76" "label": "å¤„ç†å™¨ CPU é”™ è¯¯" "predict": "1.'å¤„ ç† å™¨CPUé”™è¯¯'\n2.å†…å­˜ èŠ‚ç‚¹ä¸å¯æ¢å¤çš„é”™è¯¯çº  æ­£ç \n3.æ— \4.å¯ç¨‹åºåŒ– é€»è¾‘è®¾å¤‡æ•…éšœ"</p>
<p>Few error examples on Log fault Diagnosis</p>
<p>From the analysis, we can draw the following scientifically rigorous conclusions:</p>
<p>â€¢ Few-shot learning generally outperforms zero-shot learning in log fault diagnosis tasks due to the comprehensive coverage of fault types in the provided samples.</p>
<p>â€¢ GPT-4 consistently achieves high performance in both English and Chinese tasks, indicating its robustness and effectiveness in fault diagnosis.</p>
<p>â€¢ The BaiChuan model's poor performance suggests the need for improvements in handling fault vocabulary and output completeness, especially in few-shot settings.</p>
<p>5.1.4</p>
<p>Naive Q&amp;A results on Log Summary.Table 6 respectively shows the accuracy and rouge-1 f1_scores of zero-shot and few-shot Chinese naive Q&amp;A and English naive Q&amp;A under log summary for 18 LLMs.</p>
<p>From the overall performance results, we find that few-shot results are generally better than zero-shot results in the log summary task.This trend is evident across multiple models, with the DeVops-Model-14B-Chat achieving the best performance in both zero-shot and few-shot settings.</p>
<p>â€¢ Performance of Few-shot Learning in Log Summary Tasks: Few-shot learning outperforms zero-shot learning in the log summary tasks.This improvement is evident across various models, highlighting the utility of few-shot learning in enhancing understanding and adaptation to the task specifics.The addition of ROUGE-1 F1 scores further substantiates this observation, as these scores are generally higher in the few-shot setting compared to zero-shot, which reflects not only correct predictions but also the closeness of the generated summaries to the reference summaries.</p>
<p>Manuscript submitted to ACM 5.2 Self-consistent Performance 5.2.1 SC Q&amp;A results on Log Anomaly Detection.Table 7 shows the Accuracy and F1-scores of Chinese self-consistency and the Accuracy and F1-scores of English self-consistency Q&amp;A with zero-shot and few-shot for 18 LLMs under log anomaly detection, respectively.From the overall performance results, we find that few-shot scenarios do not yield results as good as zero-shot scenarios.Additionally, there are instances where LLMs produce multiple values in few-shot experiments.Among them, the Baichuan model shows a significant improvement in the self-consistency experiment compared to the naive Q&amp;A.</p>
<p>Other models do not change much, indicating that the Baichuan model lacks stability, producing a large difference in answers each time.Meanwhile, the LLaMA2 series of models shows poor results in both naive answers and the self-consistency experiment, which will be detailed and discussed in the appendix.</p>
<p>From the analysis, we can draw the following scientifically rigorous conclusions:</p>
<p>â€¢ Few-shot learning does not outperform zero-shot learning in log anomaly detection tasks, highlighting its limitations in this context.â€¢ The BaiChuan model shows a significant improvement in self-consistency, indicating its potential for achieving better performance with more consistent responses.</p>
<p>â€¢ The LLaMA2 series of models demonstrates poor performance and lack of stability, suggesting the need for further improvements and optimizations.From the overall performance results, we find that the few-shot results are better than zero-shot results, similar to the naive Q&amp;A results.This indicates stable output in the log fault diagnosis task, with GPT-3.The following conclusions can be drawn from the results:</p>
<p>â€¢ For most models, performance does not change much from naive Q&amp;A to SC.In the anomaly detection task, the performance under few-shot conditions is inferior to zero-shot.Conversely, in the fault diagnosis task, the performance under few-shot conditions exceeds zero-shot scenarios.â€¢ In these settings, SC prompts relatively minor improvements to the model.In repeated questions, the LLM's answers were consistent.</p>
<p>â€¢ LLMs fine-tuned specifically for Chinese perform better on English and Chinese test sets than LLMs not fine-tuned for Chinese.LLaMA is a notable example, which we discuss further in the appendix.</p>
<p>SC in model robustness performance.</p>
<p>For the self-consistency experiment, we conducted five experiments on each model for each task using the same dataset.By analyzing these five results, we can determine if the model's performance is stable, as shown in the Table 9, it represents the variance of the five F1-scores obtained after performing Chinese and English naive Q&amp;A tasks on the model in the zero-shot and few-shot scenarios for anomaly detection.It can be observed that the variance values of most models are low, indicating that the model has good robustness in the five experiments.</p>
<p>As shown in the Table 10, it represents the variance of the five F1-scores obtained after performing Chinese and English naive Q&amp;A tasks on the model in the zero-shot and few-shot scenarios for fault diagnosis.It can be seen that Manuscript submitted to ACM</p>
<p>Performance on Inference Time and Average Token</p>
<p>To investigate the reasoning efficiency of the LLMs and whether they are redundant in generating responses, we summarized the inference time for different models and the average number of tokens output per log.The inference time and average tokens used for each task on the English dataset in the zero-shot case of the naive Q&amp;A are shown below.</p>
<p>5.</p>
<p>3.1 Inference Time.Fig. 9 shows the inference time of the four of tasks on the English data set in the zero-shot case of the naive Q&amp;A.From the overall performance evaluation results, the log summary task takes the longest time among the four tasks.This is mainly because, in our test dataset, the input content for the log summary task is longer, causing the model to take more time to process these inputs.Specifically, five models: DeVops-7B, DeVops-14B, InternLM-7B, InternLM-20B, and Mistral-7B exhibit short inference times, which may be related to the setup of the test environment.Since we tested with a locally deployed model rather than calling through an API interface, this may have contributed to the time difference.A locally deployed model takes much less time to reason than if it were called through an API.In addition, the inference time of the LLaMA-2-70B model is longer, likely due to its large number of parameters.
Q w e n 1 .5 -7 B Q w e n 1 .5 -1 4 B Q w e n 1 .5 -7 2 B L L a M a 2 -7 B L L a M a 2 -1 3 B L L a M a 2 -7 0 B D e V o p s -7 B D e V o p s -1 4 B I n t e r n L M 2 -7 B I n t e r n L M 2 -2 0 B A q u i l a C h a t -7 B G P T -3 .5 G P T -4 G e m i n i P r o M i s t r a l -7 B B a i C h u a n 2 -1 3 B C h a t G L M</p>
<p>Average</p>
<p>Token.Fig. 10 shows the Average Token of the four classes of tasks on the English data set with zero-shot setting for naive Q&amp;A.</p>
<p>From the overall performance evaluation results, the log summary task outputs the highest average number of tokens among the four tasks.This phenomenon is mainly determined by the nature of the task because the log summary task requires the model to generate a concise summary, which usually requires more tokens to accurately represent the main content of the log.However, our evaluation results show that Gemini, GPT, and Mistral models output a lower average number of tokens, indicating that their answers are more concise, without excessive redundant information, and their outputs are cleaner.Conversely, LLaMA and Qwen models output more tokens on average, meaning their From the analysis, we can draw the following scientifically rigorous conclusions:</p>
<p>â€¢ The log summary task takes the longest inference time among the four tasks, mainly due to the longer input content.â€¢ Locally deployed models such as DeVops-7B, DeVops-14B, InternLM-7B, InternLM-20B, and Mistral-7B exhibit shorter inference times compared to API-based models.</p>
<p>â€¢ The LLaMA2-70B model has a longer inference time due to its large number of parameters.</p>
<p>â€¢ The log summary task outputs the highest average number of tokens, while Gemini, GPT, and Mistral models produce more concise outputs.</p>
<p>â€¢ LLaMA and Qwen models output more tokens on average, containing more extraneous content, which can reduce user efficiency in practical applications.</p>
<p>Performance on Different parameters</p>
<p>Fig. 11 shows the accuracy of LLaMA-2 and Qwen-1.5-Chatfor different parameter counts.We used a zero-shot naive Q&amp;A assessment on an English dataset.</p>
<p>From the comparison of results, both models achieve better performance under the parameter number of 7B.This finding suggests that model size is not a determining factor for log analysis tasks.While an increase in the number of parameters generally means that the model can capture more features and patterns, a large number of parameters can also cause the model to be too complex to process log data quickly and accurately in real-world applications.Therefore, we can conclude that for log analysis tasks, choosing the right number of parameters is crucial, not simply "bigger is better." Future research should focus on how to optimize the size of the model for a more efficient and cost-effective log analysis solution without sacrificing performance.Through comparative analysis of these models, we find significant differences in their performance on log analysis tasks.These differences may be due to differences in model design philosophy, training strategies, and model architecture.</p>
<p>Manuscript submitted to ACM</p>
<p>For example, some models may perform better with long series of log data, while others may show greater efficiency in generating summaries or detecting anomalies.Additionally, the number of parameters and training objectives of the model are also important factors affecting its performance in the log analysis task.Our evaluation highlights the need to consider these factors when selecting and customizing a log analysis model to ensure that the model effectively meets the needs of real-world applications.</p>
<p>During the evaluation process, we also focused on the two key metrics of the model's inference time and average number of output tokens.Inference time reflects the time it takes the model to process a single log entry, while the average number of output tokens reveals the computational resources required for the model to generate a response.Our data show that even models with similar numbers of parameters can perform very differently on these two measures.</p>
<p>Some models demonstrate shorter inference times and fewer average output tokens, suggesting they are more efficient at handling log analysis tasks.Other models may perform poorly in these two areas, which may affect their overall performance.</p>
<p>To sum up, the evaluation work in this chapter not only reveals the performance differences of different LLMs in log analysis tasks but also provides a valuable reference for future research, which is helpful to promote the technical progress and application development of LLMs in the log analysis field.With a deeper understanding of how models</p>
<p>Manuscript submitted to ACM perform on different metrics, we can better guide model selection and optimization to achieve a more efficient and cost-effective log analysis solution.</p>
<p>Baselines Results</p>
<p>Table 11 presents the baseline models' accuracy and F1-scores on our dataset.For the log anomaly detection task, while NeuralLog achieves an accuracy of 0.97, its inability to identify any anomalies results in an F1-score of 0. LogRobust, however, improves upon this by attaining an F1-score of 0.09, along with an accuracy of 0.95, precision of 0.33, and a recall rate of 0.55.</p>
<p>For the log parsing task, Drain and LogPPT display low F1-scores at 0.048 and 0.068, respectively, even though they achieve accuracies of 0.773 and 0.289, suggesting their limited parsing capabilities.Despite this, LogPPT marginally outperforms Drain in this particular context.</p>
<p>For the log fault diagnosis task, LogKG demonstrates superior diagnostic effectiveness with an F1-score of 0.5805 and an accuracy of 0.6421, showcasing balanced precision (0.5787) and recall (0.65714).On the other hand, LogCluster consistently records a significantly lower F1-score of 0.227, despite maintaining a relatively high precision rate of 0.435.</p>
<p>The notably lower recall rate of 0.233 emphasizes its restricted capability in detecting faults.</p>
<p>For the log summary task, the LogSummary algorithm currently achieves good overall performance with an F1-score of 0.722 and a perfect recall rate of 1.0, meaning it fully encompasses essential information with a precision rate of 0.565.This also reflects that there remains room for improvement in refining the summaries while maintaining comprehensiveness.</p>
<p>When comparing the baseline results with LLMs, several observations can be made:</p>
<p>â€¢ For log anomaly detection, LLMs generally achieves higher F1-scores compared to the baseline models.For instance, models like GPT-4 and Gemini Pro show superior performance with higher F1-scores.â€¢ In log parsing tasks, the performance of LLMs also surpasses that of the baselines.Models such as GPT-4 and Claude3 Sonnet demonstrate better parsing capabilities with higher accuracy.</p>
<p>â€¢ For log fault diagnosis, LLMs like GPT-3.5 and GPT-4 significantly outperform the baselines in few-shot scenario.</p>
<p>These models achieve much higher F1-scores and accuracy, indicating better diagnostic effectiveness.â€¢ In the log summary task, LLMs continue to show strong performance.Models like DeVops-7b and DeVops-14b provide concise and accurate summaries with high accuracy, indicating that they can effectively generate comprehensive summaries.</p>
<p>From the comparison of baseline results and our models, we can draw the following scientifically rigorous conclusions:</p>
<p>Manuscript submitted to ACM</p>
<p>â€¢ LLMs generally achieve higher F1-scores, accuracy across all tasks compared to the baseline models in few-shot scenario, indicating superior performance.</p>
<p>â€¢ The significant improvements in performance metrics highlight the effectiveness of LLMs in handling various log analysis tasks, including anomaly detection, parsing, fault diagnosis, and summary generation.</p>
<p>â€¢ The results suggest that advanced LLMs like GPT-4 and Gemini Pro are more capable of processing log data efficiently and accurately, making them better suited for real-world log analysis applications.</p>
<p>â€¢ Further research should focus on optimizing these models to enhance their performance even further, particularly in areas where the baseline models show limitations.</p>
<p>CONCLUSION</p>
<p>LogEval represents a significant advancement in the benchmarking of Large Language Models (LLMs) for log analysis tasks.This comprehensive benchmark suite evaluates a range of log analysis tasks, including log parsing, log anomaly detection, log fault diagnosis, and log summary extraction.By thoroughly assessing the capabilities and limitations of current LLMs in these domains, LogEval provides valuable insights into their potential applications and areas requiring further development.</p>
<p>Our findings highlight the transformative potential of LLMs in log analysis practices.These models demonstrate significant promise in enhancing the efficiency and accuracy of log analysis, crucial for maintaining the stability and performance of complex information systems.However, the evaluation also reveals specific areas where current models fall short, emphasizing the need for continued research and improvement.</p>
<p>The benchmark suite underscores the critical importance of model selection, showing how different models can excel or struggle with specific log analysis tasks.For instance, models like GPT-4 consistently outperform others in tasks requiring deep comprehension and nuanced understanding, such as log parsing and log fault diagnosis.Conversely, smaller parameter models often lag in performance, particularly in more complex tasks.This differentiation is crucial for researchers and practitioners when choosing the most appropriate model for their specific needs.</p>
<p>Technical features such as model size, training data quality, and fine-tuning processes significantly impact performance.Larger models with extensive fine-tuning on high-quality data sets tend to perform better, yet they also require more computational resources.LogEval's comprehensive evaluation framework provides a clear comparison of these factors, aiding in the development of more efficient and effective LLMs for log analysis.</p>
<p>As the field of log analysis evolves, benchmarks like LogEval will play a crucial role in driving technological progress and application development.LogEval offers a standardized framework for evaluating LLMs, facilitating meaningful comparisons across different models and encouraging innovation and improvement in log analysis technologies.The insights gained from this benchmark are expected to inspire further research and development, leading to the creation of LLMs that are even more adept at handling the complexities of log analysis.</p>
<p>The implications of LogEval extend beyond mere evaluation.It serves as a guide for future research directions, highlighting the strengths and weaknesses of current LLMs.By identifying specific areas for improvement, LogEval provides a roadmap for the next generation of LLMs in log analysis, aiming for models that not only perform well across various tasks but also do so efficiently and reliably in real-world applications.</p>
<p>In summary, LogEval has established a robust foundation for assessing the performance of LLMs in log analysis tasks.</p>
<p>It provides a valuable reference for researchers and practitioners, contributing to the advancement of LLM technology and its application in maintaining the health and performance of modern information systems.As we continue to refine these models, the insights from LogEval will be instrumental in shaping the future of log analysis.</p>
<p>Manuscript submitted to ACM 7 DISCUSSION</p>
<p>Limitations and Directions for Improvement</p>
<p>LogEval, as the first dedicated benchmark suite for log analysis tasks, represents a significant step forward in the field, but it still has some limitations and room for improvement.Firstly, the diversity of datasets is a limitation of LogEval.</p>
<p>Currently, LogEval's datasets primarily focus on several common log formats and tasks, and their applicability to more extensive and diversified log data remains to be verified.Future work could expand the datasets to include more types of logs and more complex tasks to enhance the universality and robustness of the benchmark tests.Secondly, the complexity of tasks is also a limitation.LogEval currently focuses on basic tasks such as log parsing, log anomaly detection, log fault diagnosis, and log summary.However, the actual application scenarios of log analysis are often more complex, involving multi-task learning and end-to-end system integration.Future work could consider adding more complex tasks such as log clustering and event sequence analysis to better evaluate the comprehensive capabilities of models.</p>
<p>Issues and Directions for Improving Model Output Quality and Inference Time</p>
<p>During the evaluation process of LogEval, several noteworthy issues were identified.Firstly, some models produce results that are overly lengthy and complex when handling log analysis tasks.This may be due to the models' fault to accurately understand the prompt's meaning or effectively distill key information.Future research should focus on enhancing the models' comprehension abilities and output quality to generate more concise and accurate log analysis results.Secondly, some models exhibit excessively long inference times, which may be attributed to excessive model parameters or suboptimal model architectures.In real-world log analysis applications, the model's response speed is crucial as it directly affects the operational efficiency of personnel.Therefore, model optimization should aim to reduce inference time while maintaining performance to meet real-time requirements.Addressing these issues provides valuable guidance for researchers to improve models and contribute to the technological advancement of the log analysis field.</p>
<p>Future Development Directions of LLMs in Log Analysis Tasks</p>
<p>With the continuous advancement of LLM technology, the future of log analysis will increasingly rely on these advanced technologies.On one hand, model optimization will continue to drive performance improvements in log analysis.For instance, by improving model architecture, training strategies, and optimization algorithms, the accuracy and efficiency of models can be further enhanced.On the other hand, task expansion is also an important future development direction.</p>
<p>In addition to traditional log analysis tasks, future research could explore combining log analysis with other technologies such as knowledge graphs and causal inference to achieve more comprehensive and in-depth log analysis.Furthermore, with the development of federated learning and privacy protection technologies, exploring efficient log analysis while ensuring data privacy and security is also a worthwhile direction to pursue.</p>
<p>Guidance from LogEval for Practical Application Scenarios</p>
<p>LogEval provides important references and guidance for log analysis in practical application scenarios.Firstly, LogEval offers a comprehensive evaluation framework that can assist enterprises and research institutions in selecting models that best suit their needs.By comparing the performance of different models, stakeholders can better understand their strengths and weaknesses, thus choosing the model most suitable for their application scenario.Secondly, LogEval's evaluation metrics and methods can serve as benchmarks for performance evaluation in practical applications.By comparing their system's evaluation results with those of LogEval, organizations can more accurately assess the performance of their log analysis systems and implement targeted optimizations and improvements.Finally, LogEval sets a reference standard for research and application in the log analysis field, contributing to the advancement of the AIOps field.</p>
<p>APPENDIX</p>
<p>.1 Metrics used in Question-Answering Evaluation ROUGE (Recall-Oriented Understudy for Gisting Evaluation): A set of metrics designed to evaluate machine translations and summaries by measuring the overlap between the predicted and reference n-grams.ROUGE-N evaluates the overlap of n-grams between the prediction and reference, while ROUGE-L assesses sentence structure similarity by identifying the longest common sequence of n-grams.ROUGE scores range from 0 to 100, with higher scores indicating better performance.</p>
<p>BLEU (Bilingual Evaluation Understudy): Focuses on the precision of the generated answers by comparing them to a set of reference translations.It is widely used in natural language processing, particularly for translation tasks.BLEU scores are normalized from 0 to 100, with higher scores reflecting better accuracy.</p>
<p>Cosine Similarity: A method for measuring the similarity between two vectors, often used to assess the semantic similarity between texts.In the context of LogEval, cosine similarity evaluates the likeness between model-generated answers and the ground truth, with values ranging from -1 (completely dissimilar) to 1 (identical).F1-Score: A metric that considers both precision and recall, providing a balanced measure of a model's performance in log analysis tasks.It ranges from 0 to 1, with higher values indicating better performance.</p>
<p>Accuracy: Measures the proportion of correct answers generated by the model compared to the ground truth, providing a straightforward assessment of model performance.</p>
<p>.2 ADDITIONAL DETAILS OF EXPERIMENTS .2.1 Detailed Information of LLMs Evaluated.Various LLMs were evaluated in this study, including GPT-4, GPT-3.5turbo,Claude-3-Sonnet, Gemini-Pro, Mistral, InternLM2-Chat, DevOps-Model-Chat, AquilaChat, ChatGLM4, ChatGLM3, LLaMA-2, Qwen-1.5-Chat,Baichuan2-Chat, and others.These models represent the latest advances in natural language processing, and their performance evaluation is critical for understanding the potential of LLMs in log analysis tasks.</p>
<p>GPT-4 is a large multimodal model capable of accepting both image and text inputs and generating text outputs.</p>
<p>It demonstrates human-level performance across various professional and academic benchmarks and is currently recognized as the most powerful language model.GPT-3.5-turbo, an early AI language model developed by OpenAI based on GPT-3.5, is the version we used.Claude-3-Sonnet, a large language model developed by Anthropic, is renowned for its powerful multimodal understanding and generation capabilities.Gemini-Pro, another large language model developed by Anthropic, gains attention for its excellent natural language understanding and generation abilities.Mistral, also developed by Anthropic, is noted for its strong multimodal understanding and generation capabilities."output": "Processor CPU Caterr" "id": "0" "instruction": "Parse the following log entry into a template format, replacing variable parts with a wildcard &lt;<em>&gt;,and focus the answer after the keyword 'Answer'\nFor example:\nlog entry:\nno floppy controllers found,\nanswer:'no floppy controllers found';\nlog entry:\n13 tree receiver 1 in re-synch state event(s) (dcr 0x0185) detected over 4562 seconds,\nanswer:'&lt;</em>&gt; tree receiver &lt;<em>&gt; in re-synch state event(s) (dcr &lt;</em>&gt;) detected over &lt;<em>&gt; seconds';\niog entry:\n... autorun DONE.,\nanswer:'... autorun DONE.'; \nlog entry:\n2 L3 EDRAM error(s) (dcr 0x0157) detected and corrected over 282 seconds, \nanswer:'&lt;</em>&gt; L3 EDRAM error(s) (dcr &lt;<em>&gt;) detected and corrected over &lt;</em>&gt; seconds'; \nlog entry:\nprobe of vesafb0 failed with error -6,\nanswer:'probe of vesafb0 failed with error &lt;*&gt;'."</p>
<p>"input": "\nlog entry:\ninstruction cache parity error corrected" "output": "instruction cache parity error corrected"  In few-shot evaluations, adopting more advanced settings may lead to worse results.We found that when evaluating with a small number of prompts, some models may misunderstand the nature of the task, incorrectly treating it as a generative problem rather than an answering problem.as shown in Fig. 15, This misunderstanding may cause the model to deviate from the correct track when generating answers, thereby affecting its performance.Specifically, a few prompts may not be sufficient to indicate that the model should generate concise and accurate answers, but may instead prompt the model to respond to the content, leading to longer narratives or generating more complex content.</p>
<p>Additionally, the model may overly rely on patterns learned during pre-training rather than adjusting its responses based on specific information in the prompts.In such cases, the model's generative ability may be misled, resulting in outputs that are less accurate or irrelevant to the questions.To mitigate this issue, we believe that providing clearer prompts during evaluation or increasing the number of prompts to more accurately guide the model's understanding of the task requirements is essential.Moreover, developing and optimizing evaluation metrics and methods tailored to specific tasks and evaluation settings are also crucial for improving model performance.Through these approaches, we can help models better adapt to different evaluation environments, thereby improving their performance in practical applications.</p>
<p>In our research evaluation process, we observe some models exhibiting significant output redundancy and continuous repetition when generating answers, as shown in Fig. 16.The reasons for this phenomenon may be related to the model's inherent generation preferences, self-reinforcement effects, and probability distributions set during initialization.This repetitive answering not only results in a large amount of invalid information in the generated content but also severely affects the efficiency and quality of the model's answers.This contradicts the goal of pursuing efficiency in the field of log anomaly detection.</p>
<p>Manuscript submitted to ACM "id": "8"</p>
<p>"label": "abnormal"</p>
<p>"predict": "abnorma"</p>
<p>"id": "5"</p>
<p>"label": "normal"</p>
<p>"predict": "Based solely on the information provided in the log entry, it is not possible to definitively classify it as 'normal' or 'abnormal'.\n\nThe log entry mentions \"double-hummer alignment exceptions\", but without additional context or information about what this means within the specific system or application being monitored, it is difficult to determine whether this is expected behavior or an indication of a problem.\n\nTherefore,I would classify this log entry as 'unclear' or 'inconclusive' rather than 'normal' or 'abnormal'.It may be necessary to gather more information or consult additional logs or documentation in order to make a more accurate classification." Fig. 14.A failed zero-shot example by Baichuan2-13B-Chat "id": "0"</p>
<p>"label": "normal"</p>
<p>"predict": "normal \nnormal \nabnormal \nabnormal \nnormal"</p>
<p>"id": "0"</p>
<p>"label": "Processor CPU Caterr"</p>
<p>"predict": "Processor #0xfa | Configuration Error | Asserted \nanswer:'Processor CPU Caterr'" "label": "æ­£å¸¸"</p>
<p>"predict": "æ­£ å¸¸\ninstruction address: 0x00004ed8\ninstruction address:s0x00004ed8\ninstruction address: 0x00004ed8\ninstruction address:0x00004ed8\ninstruction address:0x00004ed8\ninstruction address:0x00004ed8\ninstruction address: 0x00004ed8\ninstruction address:0x00004ed8\ninstruction address: 0x00004ed8\ninstruction address:...... "</p>
<p>"time": "88.39198112487793"Fig. 16.A failed zero-shot example by AquilaChat-7B</p>
<p>Fig. 1 .
1
Fig. 1.The framework of LogEval</p>
<ol>
<li>3 . 1
31
Prompt Structure.Each prompt was designed to include clear instructions, context, and expected output.This structure ensures that the model understands the task requirements and can generate relevant responses.Fig.2illustrates three zero-shot examples of formatted questions, demonstrating the clarity and coherence of the prompts used.</li>
</ol>
<p>the detailed information provided by the log [log] to conduct an in-depth analysis to determine the category of the fault.Only output the fault type Log Summary 1. Analyze the following 20 logs [log], extract key information, phrases, sentences, or recurring content to generate a summary, and only output the summary 2. Extract the most important events, phrases, and activities or recurring content from the following 20 logs [log], create a concise log overview, only output the summary 3. Extract key events, sentence phrases, or recurring information from the following 20 logs [log] to form a comprehensive summary, only output the summary â€¢ Average Token: Measures the average number of tokens generated by the model for a single log entry.This metric indicates the complexity and verbosity of the model's output, reflecting the computational resources and processing time required.â€¢ Inference Time: Measures the time taken by the model to process a single log entry, indicating the model's response speed in practical applications.A lower inference time suggests higher efficiency and quicker response in real-world scenarios.</p>
<p>Fig. 3
3
Fig. 3 shows the performance of different models in zero-shot scenarios across four tasks: log parsing, log anomaly detection, log fault diagnosis, and log summary extraction.From the figure, we can observe the following: â€¢ Log Parsing Task:</p>
<p>Fig. 5 .
5
Fig. 5. LogEval Overall Performance in zero-shot Naive Q&amp;A Fig. 6.LogEval Overall Performance in few-shot Naive Q&amp;A</p>
<ol>
<li>1
1
Naive Q&amp;A Performance 5.1.1Naive Q&amp;A Results on Log Parsing.Table</li>
</ol>
<p>Fig. 7
7
Fig. 7 illustrates some specific examples of anomaly detection tasks where the model repeats answers given in the samples, reflecting the model's performance in real-world applications.</p>
<ol>
<li>
<p>1 . 3
13
Naive Q&amp;A results on Log fault Diagnosis.Table 5 respectively shows the accuracy and f1scores of Chinese naive Q&amp;A and the Accuracy and F1-scores of English naive Q&amp;A with zero-shot and few-shot for 18 LLMs under log fault diagnosis.</p>
</li>
<li>
<p>2 . 2
22
SC Q&amp;A results on Log fault Diagnosis.Table8shows the Accuracy and F1-scores of Chinese self-consistency and the Accuracy and F1-scores of English self-consistency Q&amp;A with zero-shot and few-shot for 18 LLMs under log fault diagnosis.Manuscript submitted to ACM</p>
</li>
</ol>
<p>5 and GPT-4 showing far superior results.The Baichuan model performs poorly under both self-consistency and naive Q&amp;A, while other models do not change much relative to the naive Q&amp;A results.The zero-shot and few-shot performance of the LLMs are examined for English and Chinese test sets by comparing the results of the naive and self-consistency Q&amp;A experiment.</p>
<p>4 C l a u d e 3 SFig. 9 .
439
Fig. 9.The Inference Time in the Naive Q&amp;A situation in log analysis by zero-shot</p>
<p>Fig. 10 .
10
Fig. 10.The Average Token in the Naive Q&amp;A situation in log analysis by zero-shot</p>
<p>Fig. 11 .
11
Fig. 11.The Accuracy of LLaMa-2 and Qwen-1.5-Chat in zero-shot English Naive Q&amp;A</p>
<p>InternLM2-Chat, developed by the Shanghai AI Laboratory, is a multilingual model trained through multi-stage progressive training on trillions of tokens, capable of handling multiple languages including Chinese.Devops-Model-Chat, an open-source model targeted at Chinese DevOps, primarily aims to provide practical value in the DevOps domain.AquilaChat, a chat model based on the open-source LLM Aquila released by Aquila Intelligence, can handle multiple languages including Manuscript submitted to ACM Chinese.ChatGLM4 and ChatGLM3, developed by the Knowledge Engineering Group at Tsinghua University, are based on the latest version and the third version of the GLM model.They are fine-tuned for various natural language understanding and generation tasks using autoregressive blank-filling objectives.LLaMA-2, Meta's second-generation open-source LLM, can handle multiple languages including Chinese.Qwen-1.5-Chat,developed by Alibaba Cloud, gained attention for its powerful Chinese understanding and generation capabilities.Baichuan2-Chat, based on the alignment chat model of the open-source LLM Baichuan2-Base released by Baichuan Intelligence, is noted for its strong multilingual processing capabilities..2.2An Example of few-shot.Fig. 12 shows two input examples under the "few-shot" setting, illustrating scenarios for log parsing and log fault diagnosis tasks.Each model was given 5 logs and their corresponding labels in the "instruction" field, allowing the model to learn and better respond.</p>
<p>"</p>
<p>id": "0" "instruction": "In our data scenario, there are three types of faults {Processor CPU Caterr, Memory Throttled Uncorrectable Error Correcting Code, Hard Disk Drive Control Error Computer System Bus Short Circuit Programmable Gate Array Device Unknown}.Analyze the log entry and identify the type of fault that occurred.Only output the fault type.\nForExample:\nlog entry:\nTemperature CPU0_Margin_Temp | Lower Critical going low | Asserted | Reading -16 &1t; Threshold 0 degrees C\nanswer:'Processor CPU Caterr'; nlog entry:\nMemoryCPU1E0_DIMM_Stat | Correctable ECC | Asserted\nanswer:'Memory Throttled Uncorrectable Error Correcting Code';\nlog entry:\nSystem Boot Initiated BlOS_Boot_Up | Initiated by power up | Asserted\nanswer:'Hard Disk Drive Control Error Computer System Bus Short Circuit Programmable Gate Array Device Unknown'" "input": "\nlog entry:\nProcessor #Oxfa | Configuration Error | Asserted"</p>
<p>Fig. 12 .
12
Fig. 12.Two input examples under the "few-shot" setting</p>
<p>. 2 . 4 Fig. 13 .
2413
Fig.13.The performance of LLMs on fault diagnosis tasks under the "zero-shot" naive Q&amp;A in both Chinese and English test sets</p>
<p>Fig. A failed</p>
<p>Fig. A failed few-shot example by Qwen-1.5-7B-Chat</p>
<p>Table 1 .
1
Three English prompts for each task Log Anomaly Detection 1. Review and mark the log entry as "normal" or "abnormal", only output "normal"or "abnormal" 2. Analyze the log content, classify it as "normal" or "abnormal", only output "normal" or "abnormal" 3. Check the log entry, and determine if it belongs to the "normal" or "abnormal" category, only output "normal" or "abnormal"
TasksEnglish PromptLog Parsing1. Parse the following log into a template format, replacing variable parts with &lt;<em>&gt;:[log]2. Convert the following log into a standardized template by identifying and replac-ing the variable parts with &lt;</em>&gt;: [log]3. Transform the raw log [log] into a log template by replacing variable segmentswith &lt;*&gt;Log
fault Diagnosis 1.In our data scenario, there are several types of faults {fault types}.Analyze the log [log] and identify the type of fault that occurred.Only output the fault type 2. In our data scenario, there are several types of faults {fault types}.Based on the information in the log [log], determine which type of fault the log represents.Only output the fault type 3.In our data scenario, there are several types of faults {fault types}.Use</p>
<p>Table 2 .
2
Models evaluated in this paper
ModelCreatorParameters AccessGPT-4 (OpenAI, 2023)OpenAIundisclosedAPIGPT-3.5 (OpenAI, 2022)OpenAIundisclosedAPIClaude-3-Sonnet (Anthropic, 2024)AnthropicundisclosedAPIGemini-Pro (Gemini Team Google, 2023)GoogleundisclosedAPIMistral (Jiang et al., 2023)Mistral7BWeightsInternLM2-Chat (Cai et al., 2024)Shanghai AI Laboratory7B/20BWeightsDevOps-Model-Chat (CodeFuse, 2023)CodeFuse7B/14BWeightsAquilaChat (BAAI, 2023)BAAI7BAPIChatGLM4 (Tsinghua Zhipu, 2024)Tsinghua ZhipuundisclosedAPILLaMA-2 (Touvron et al., 2023)Meta7/13/70BAPIQwen-1.5-Chat (Bai et al., 2024)Alibaba Cloud7/14/72BAPIBaichuan2-Chat (Yang et al., 2023)Baichuan Intelligence13BAPI</p>
<p>â€¢ Log Anomaly Detection Task:
Qwen1.5-14B8 â€¢ 10 âˆ’20.260.270.321LLaMa2-13B7 â€¢ 10 âˆ’20.540.220.35Qwen1.5-72B0.260.330.250.410.8LLaMa2-7B2 â€¢ 10 âˆ’20.570.220.4ModelsGPT-3.5 Qwen1.5-7B LLaMa2-70B0.24 6 â€¢ 10 âˆ’2 6 â€¢ 10 âˆ’20.26 0.52 0.770.36 0.33 0.160.45 0.38 0.410.4 0.6ChatGLM40.230.420.380.36Gemini Pro0.360.490.340.460.2GPT-40.580.330.350.43Claude3 Sonnet0.420.350.370.59ParsingAnomalyDiagnosisSummary0TasksFig. 3. The Accuracy in zero-shot Naive Q&amp;AQwen1.5-14B0.437 â€¢ 10 âˆ’20.50.681LLaMa2-13B2 â€¢ 10 âˆ’22 â€¢ 10 âˆ’46 â€¢ 10 âˆ’20.19Qwen1.5-72B0.620.30.830.780.8LLaMa2-7B0.11 â€¢ 10 âˆ’37 â€¢ 10 âˆ’20.21ModelsGPT-3.5 Qwen1.5-7B LLaMa2-70B0.23 0.27 8 â€¢ 10 âˆ’20.39 5 â€¢ 10 âˆ’2 2 â€¢ 10 âˆ’20.88 0.52 0.290.45 0.29 0.410.4 0.6ChatGLM40.570.220.710.56Gemini Pro0.840.560.870.650.2GPT-40.880.530.910.52Claude3 Sonnet0.870.310.670.52ParsingAnomalyDiagnosisSummary0TasksFig. 4. The Accuracy in few-shot Naive Q&amp;AManuscript submitted to ACM
-LLama2-70B stands out with the highest score of 0.77, showing its strong ability in detecting log anomalies.</p>
<p>Table 3 .
3
Naive Q&amp;A results on Log Parsing
zero-shotfew-shotmodelchineseenglishchineseenglishaccuracy edit distance accuracy edit distance accuracy edit distance accuracy edit distanceQwen1.5-7b0.06445.470.05347.710.22931.290.31127.59Qwen1.5-14b0.08840.690.07843.890.42618.510.43118.73Qwen1.5-72b0.33916.370.17623.910.7099.890.53310.89LLaMa2-7b0.04348.290.05347.530.06246.170.10444.61LLaMa2-13b0.06444.870.08041.570.00549.330.04047.11LLaMa2-70b0.06345.190.08242.430.10243.230.06445.67DeVops-7b0.06446.110.09144.910.06445.250.14238.73DeVops-14b0.10837.290.22332.990.18630.890.15135.87InternLM2-7b0.09441.870.18738.230.27125.930.34019.67InternLM2-20b 0.19828.510.21130.190.6458.590.52814.17AquilaChat-7b 0.04847.990.02649.770.03748.670.03548.49GPT-3.50.24422.970.22324.430.26220.870.20426.57GPT-40.6787.730.47611.290.9032.690.8733.29Gemini Pro0.23523.110.28419.310.8815.570.8016.89Mistral-7b0.10144.710.14640.230.12742.590.15339.49BaiChuan2-13b 0.00149.970.00149.830.00049.910.00149.79GhatGLM40.27119.470.18025.390.53710.570.6018.61Claude3 Sonnet 0.48411.690.38115.470.8712.530.8671.97</p>
<p>Table 4 .
4
Naive Q&amp;A results on Log Anomaly Detection
zero-shotfew-shotmodelchineseenglishchineseenglishaccuracy F1_score accuracy F1_score accuracy F1_score accuracy F1_scoreQwen1.5-7b0.5360.1290.5050.1140.0040.0780.0950.046Qwen1.5-14b0.350.0990.1950.0380.110.1910.0310.027Qwen1.5-72b0.3340.0970.2390.0630.330.4950.2740.16LLaMa2-7b0.190.0060.9430.0950.00100.0040LLaMa2-13b0.4160.0570.6590.122000.0010LLaMa2-70b0.5620.0360.6930.0440.00600.0360.007DeVops-7b0.10.040.210.0370.1450.0240.2520.029DeVops-14b0.1750.0470.2590.0550.2370.0410.2930.032InternLM2-7b0.3920.0820.3410.0750.3110.0880.3230.075InternLM2-20b 0.3680.0880.3340.0890.3420.0810.3480.089AquilaChat-7b 0.1950.0660.60.0420.2630.0460.2290.003GPT-3.50.2430.0840.2850.0820.3710.0880.4020.107GPT-40.3310.0970.3330.0970.5640.1360.5060.135Gemini Pro0.5570.1390.4170.1090.6020.1410.5310.132Mistral-7b0.2770.1620.6310.0920.7060.1220.5460.092BaiChuan2-13b 0.2860.0050.25600.2770.0010.3350.438GhatGLM40.4850.1210.3580.0920.1130.30.3310.221Claude3 Sonnet 0.3470.0990.3540.10.4640.0820.150.064</p>
<p>Table 5 .
5
Naive Q&amp;A results on Log fault Diagnosis
zero-shotfew-shotmodelchineseenglishchineseenglishaccuracy F1_score accuracy F1_score accuracy F1_score accuracy F1_scoreQwen1.5-7b0.3510.3260.3150.5160.5910.6510.4520.505Qwen1.5-14b0.3660.5730.1820.5610.4150.6150.5760.631Qwen1.5-72b0.3060.380.1940.4230.8690.8990.7980.84LLaMa2-7b0.0860.1510.3540.4080.0130.0250.0660.115LLaMa2-13b0.0570.0980.380.440.0150.0290.1070.179LLaMa2-70b0.0910.1490.230.2910.080.1440.5110.635DeVops-7b0.3240.2290.2810.3570.280.6170.3610.629DeVops-14b0.3630.3240.2880.4160.3430.7360.6870.733InternLM2-7b0.4930.5270.2480.2840.4850.7610.3830.669InternLM2-20b 0.4420.5790.3420.4250.5920.7620.6260.721AquilaChat-7b 0.3120.3270.3130.3480.0390.0710.2190.295GPT-3.50.4130.4730.2780.3360.8820.9230.8520.916GPT-40.2470.2250.4240.4530.8870.9310.9290.956Gemini Pro0.3670.3310.320.2980.8740.610.7840.701Mistral-7b0.380.4180.2480.2840.7650.5060.5980.491BaiChuan2-13b 0.0450.069000.03000GhatGLM40.350.7540.4040.7080.6780.7930.7510.785Claude3 Sonnet 0.2880.2870.4420.4220.5360.3660.80.7</p>
<p>Table 7 .
7
SC Q&amp;A results on Log Anomaly Detection
zero-shotfew-shotmodelchineseenglishchineseenglishaccuracy F1_score accuracy F1_score accuracy F1_score accuracy F1_scoreQwen1.5-7b0.5870.1480.550.110.0050.0050.0970.048Qwen1.5-14b0.380.1030.2480.0370.1380.2350.030.03Qwen1.5-72b0.3420.0980.2640.0670.3320.4980.2770.163LLaMa2-7b0.1140.0050.9440.103000.0010LLaMa2-13b0.3360.050.6580.1210000LLaMa2-70b0.4780.0290.6920.043000.0190.003DeVops-7b0.1060.0390.2130.030.1820.0760.2110.013DeVops-14b0.1710.0430.1540.0250.3160.1360.290.06InternLM2-7b0.3920.0890.3380.0890.2050.0140.3880.03InternLM2-20b 0.3680.0830.3340.0760.3420.0460.350.018AquilaChat-7b 0.1280.0350.6440.0350.20.0370.1910.001GPT-3.50.2460.0840.270.0830.2840.0880.3470.082GPT-40.330.0960.3320.0970.5460.1360.5430.135Gemini Pro0.5570.1390.4140.1080.4730.1430.270.13Mistral-7b0.630.1620.4230.0740.5320.0880.4720.017BaiChuan2-13b 0.8380.2320.5220.0060.27600.3280.004GhatGLM40.5210.1280.3660.0980.3590.10.330.096Claude3 Sonnet 0.3470.0990.3540.10.4580.0810.150.064</p>
<p>Table 8 .
8
SC Q&amp;A results on Log fault Diagnosis
zero-shotfew-shotmodelchineseenglishchineseenglishaccuracy F1_score accuracy F1_score accuracy F1_score accuracy F1_scoreQwen1.5-7b0.380.3230.3480.3390.5910.6720.4450.425Qwen1.5-14b0.3630.3570.2250.20.4210.5340.5720.688Qwen1.5-72b0.320.2920.2350.2210.8680.9170.7990.861LLaMa2-7b0.0570.1020.3680.4170.0020.0030.0520.092LLaMa2-13b0.040.0660.3810.4360.0060.0110.1040.175LLaMa2-70b0.0780.1250.2320.2880.0750.1360.5160.639DeVops-7b0.3260.4030.2510.3450.4330.6170.580.629DeVops-14b0.3520.4230.2810.3460.4610.7760.6520.733InternLM2-7b0.4770.5530.1980.2770.5670.7620.5220.636InternLM2-20b 0.4230.5070.3340.4120.6670.7610.5710.669AquilaChat-7b 0.2370.2920.2730.3270.0130.0260.2250.291GPT-3.50.4310.480.280.3230.890.9360.9150.954GPT-40.2410.2130.4080.430.8870.930.9310.957Gemini Pro0.3650.3330.320.30.5030.610.5930.695Mistral-7b0.3870.4250.2530.2870.6820.5080.6090.496BaiChuan2-13b 0.0720.0720.0630.0450.0230.0310.0290.05GhatGLM40.3420.3670.4170.4490.6870.7880.7810.836Claude3 Sonnet 0.2880.2870.4410.4210.5380.3670.7980.697</p>
<p>Table 9 .
9
SC Q&amp;A F1-score Variance on Log Anomaly Detection
modelchinesezero-shot englishfew-shot chineseenglishQwen1.5-7b1.57E-055.5E-069.7E-068.3E-06Qwen1.5-14b05.7E-062.00E-075.2E-06Qwen1.5-72b05.3E-061.08E-051.83E-05LLaMa2-7b2.80265E-064.31118E-0400LLaMa2-13b1.47E-061.53017E-0600LLaMa2-70b3.66E-064.05727E-0502.92015E-06DeVops-7b3.36E-071.55956E-063.70E-062.09E-07DeVops-14b2.68E-067.32059E-062.43E-067.40183E-06InternLM2-7b1.42E-051.11881E-052.15E-081.53955E-06InternLM2-20b1.02E-058.28797E-061.14E-056.56E-08AquilaChat-7b8.70E-063.15505E-051.44E-051.69002E-06GPT-3.51.74E-072.74283E-061.03543E-061.71997E-04GPT-43.66E-082.19E-071.27E-077.98E-07Gemini Pro2.86E-065.83E-074.21E-062.37157E-06Mistral-7b7.51E-054.03911E-052.81E-071.07E-07BaiChuan2-13b02.94E-1101.15E-07GhatGLM42.43E-072.59E-071.33E-081.47E-07Claude3 Sonnet 3.48E-101.57E-091.38E-061.33E-07
the model's performance is less stable in the few-shot scenario compared to the zero-shot scenario, suggesting that the model still has some ambiguous understanding in the few-shot scenario.</p>
<p>Table 10 .
10
SC Q&amp;A F1-score Variance on Log fault Diagnosis
modelchinesezero-shot englishfew-shot chineseenglishQwen1.5-7b9.7E-069.5E-066.3E-064.53E-05Qwen1.5-14b1.53E-058.8E-069.30E-061.06E-04Qwen1.5-72b6.3E-066.7E-061.27E-055.7E-06LLaMa2-7b1.48E-055.06E-061.3125E-051.874E-05LLaMa2-13b1.65E-051.79E-052.1438E-051.9622E-05LLaMa2-70b3.41E-051.44E-054.39E-052.65E-05DeVops-7b5.72E-069.51E-072.02E-042.80E-05DeVops-14b3.43E-051.23E-053.35E-068.93E-06InternLM2-7b2.20E-055.15E-073.78E-048.32E-05InternLM2-20b2.65E-081.78E-084.53E-045.59E-06AquilaChat-7b3.43E-053.58E-055.13E-054.51E-05GPT-3.58.03E-053.86E-056.22E-044.11E-04GPT-45.59E-052.86E-064.88E-063.60E-07Gemini Pro2.76E-068.89E-053.38E-069.83E-05Mistral-7b1.72E-052.14E-066.56E-043.42E-04BaiChuan2-13b9.44E-083.64E-102.78E-100.00E+00GhatGLM41.93E-051.53E-058.90E-067.31E-06Claude3 Sonnet 4.11E-081.55E-071.30E-069.14E-07</p>
<p>Table 11 .
11
Baseline results
Log TaskMethodF1-score Accuracy Precision RecallLog Anomaly DetectionNeuralLog LogRobust0 0.090.97 0.950 0.330 0.55Log ParsingDrain LogPPT0.048 0.0680.773 0.2890.039 0.0550.065 0.088Log fault DiagnosisLogKG LogCluster0.5805 0.2270.6421 0.2330.5787 0.4350.65714 0.233LogSummaryLogSummary0.7220.7220.5651
Manuscript submitted to ACM
in zero-shot, and 0.856 accuracy with a robust ROUGE-1 F1 in few-shot-under scores its effectiveness and adaptability.â€¢ Performance Differences between Models: Models such as Qwen1.5-72band GPT-4 also demonstrate notable improvements in few-shot settings.For instance, Qwen1.5-72bshows a remarkable accuracy of 0.945 in few-shot Chinese with an exceptionally high ROUGE1 F1 score, indicating its strong performance.Similarly, GPT-4 exhibits significant gains in both accuracy and ROUGE-1 F1 scores in few-shot settings compared to zero-shot, underscoring its adaptability to the log summary task.From the analysis, we can draw the following scientifically rigorous conclusions:â€¢ Few-shot learning is generally more effective than zero-shot learning in log summary tasks, as evidenced by both accuracy and ROUGE-1 F1 scores.The inclusion of ROUGE-1 F1 scores provides a more nuanced view of model performance, emphasizing not only the correctness of the summaries but also their quality and closeness to the reference â€¢ DeVops-14B demonstrates consistent high performance, making it a reliable and robust choice for log summary tasks.Its high ROUGE-1 F1 scores in both settings further affirm its superior summary quality.â€¢ Models like Qwen1.5-72b and GPT-4 showcase strong adaptability, with significant improvements in few-shot settings, highlighting their potential in adjusting to and excelling in complex summarization tasks.Manuscript submitted to ACM
. Alibaba, 2024. 2024</p>
<p>Synthetic and Natural Noise Both Break Neural Machine Translation. Yonatan Belinkov, Yonatan Bisk, arXiv:1711.021732017. 2017</p>
<p>Summary Cycles: Exploring the Impact of Prompt Engineering on Large Language Models' Interaction with Interaction Log Information. Jeremy Block, Yu-Peng Chen, Abhilash Budharapu, Lisa Anthony, Bonnie Dorr, Proceedings of the 4th Workshop on Evaluation and Comparison of NLP Systems. the 4th Workshop on Evaluation and Comparison of NLP Systems2023</p>
<p>Language Models are Few-Shot Learners. Tom B Brown, Benjamin Mann, ArXiv abs/2005.141652020. 2020218971783</p>
<p>A Survey on Evaluation of Large Language Models. Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Linyi Yang, Kaijie Zhu, Hao Chen, Xiaoyuan Yi, Cunxiang Wang, Yidong Wang, Wei Ye, Yue Zhang, Yi Chang, Philip S Yu, Qiang Yang, Xing Xie, arXiv:2307.03109[cs.CL]2023. 2023Manuscript submitted to ACM</p>
<p>Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet ; Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, arXiv:2107.03374[cs.LG]Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. 2021. Evaluating Large Language Models Trained on Code. Felipe Petroski Such. Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang,; Andrew N. Carr; Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrewJan Leike. 2021</p>
<p>Early exploration of using ChatGPT for log-based anomaly detection on parallel file systems logs. Chris Egersdoerfer, Di Zhang, Dong Dai, Proceedings of the 32nd International Symposium on High-Performance Parallel and Distributed Computing. the 32nd International Symposium on High-Performance Parallel and Distributed Computing2023</p>
<p>LogSummary: Unstructured Log Summarization for Software Systems. W Meng, IEEE Transactions on Network and Service Management. 202023. 2023</p>
<p>Llama 2: Open Foundation and Fine-Tuned Chat Models. Hugo Touvron, Et , 2023. 2023</p>
<p>Drain: An Online Log Parsing Approach with Fixed Depth Tree. Pinjia He, Jieming Zhu, Zibin Zheng, Michael R Lyu, 10.1109/ICWS.2017.132017 IEEE International Conference on Web Services (ICWS). 2017</p>
<p>S He, J Zhu, P He, M R Lyu, Loghub: A large collection of system log datasets towards automated log analytics. 2023. 2023arXiv e-prints</p>
<p>Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego De Las, Lisa Anne Casas, Johannes Hendricks, Aidan Welbl, Tom Clark, Eric Hennigan, Katie Noland, George Millican, Bogdan Van Den Driessche, Aurelia Damoc, Simon Guy, Karen Osindero, Simonyan, arXiv:2203.15556[cs.CL]Training Compute-Optimal Large Language Models. Erich Elsen, Jack W Rae, Oriol Vinyals, Laurent Sifre, 2022</p>
<p>Zhihan Jiang, Jinyang Liu, Zhuangbin Chen, Yichen Li, Junjie Huang, Yintong Huo, Pinjia He, Jiazhen Gu, Michael R Lyu, arXiv:2310.01796Llmparser: A llm-based log parsing framework. 2023. 2023arXiv preprint</p>
<p>Z Jiang, J Liu, J Huang, Y Li, Y Huo, J Gu, Z Chen, J Zhu, M R Lyu, A large-scale benchmark for log parsing. 2020. 2020arXiv print</p>
<p>Exploring semantic vs. syntactic features for unsupervised learning on application log files. Egil Karlsen, Rafael Copstein, Xiao Luo, Jeff Schwartzentruber, Bradley Niblett, Andrew Johnston, Malcolm I Heywood, Nur Zincir-Heywood, 2023 7th Cyber Security in Networking Conference (CSNet). IEEE2023</p>
<p>Benchmarking Large Language Models for Log Analysis, Security, and Interpretation. Egil Karlsen, Xiao Luo, Nur Zincir-Heywood, Malcolm Heywood, arXiv:2311.145192023. 2023arXiv preprint</p>
<p>Log-based Anomaly Detection Without Log Parsing. Van-Hoang Le, Hongyu Zhang, 10.1109/ASE51524.2021.96787732021 36th IEEE/ACM International Conference on Automated Software Engineering (ASE). 2021</p>
<p>Log Parsing with Prompt-based Few-shot Learning. Van-Hoang Le, Hongyu Zhang, 10.1109/ICSE48619.2023.002042023 IEEE/ACM 45th International Conference on Software Engineering (ICSE). 2023</p>
<p>AIOps Platforms-Gartner. Andrew Lerner, 2017. 2017</p>
<p>Jianquan Li, Xidong Wang, Xiangbo Wu, Zhiyi Zhang, Xiaolong Xu, Jie Fu, Prayag Tiwari, Xiang Wan, and Benyou Wang. 2023. Huatuo-26M,a Large-scale Chinese Medical QA Dataset. 2023. 2023arXiv e-prints</p>
<p>Competition-level code generation with AlphaCode. Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, RÃ©mi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, Thomas Hubert, Peter Choy, Cyprien De Masson D'autume, Igor Babuschkin, Xinyun Chen, Po-Sen Huang, Johannes Welbl, Sven Gowal, Alexey Cherepanov, James Molloy, Daniel J Mankowitz, Esme Sutherland Robson, Pushmeet Kohli, 10.1126/science.abq1158Science. Freitas, Koray Kavukcuoglu, and Oriol Vinyals3782022. Dec. 2022</p>
<p>Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, Holistic Evaluation of Language Models. 2022. 2022arXiv e-prints</p>
<p>ROUGE: A Package for Automatic Evaluation of Summaries. Chin-Yew Lin, Text Summarization Branches Out. Association for Computational Linguistics. Barcelona, SpainAssociation for Computational Linguisticsn. d.</p>
<p>Log Clustering Based Problem Identification for Online Service Systems. Qingwei Lin, Hongyu Zhang, Jian-Guang Lou, Yu Zhang, Xuewei Chen, 2016 IEEE/ACM 38th International Conference on Software Engineering Companion (ICSE-C). 2016</p>
<p>Scalable and adaptive log-based anomaly detection with expert in the loop. Jinyang Liu, Junjie Huang, Yintong Huo, Zhihan Jiang, Jiazhen Gu, Zhuangbin Chen, Cong Feng, Minzhi Yan, Michael R Lyu, arXiv:2306.050322023. 2023arXiv preprint</p>
<p>Yuhe Liu, Changhua Pei, Longlong Xu, Bohan Chen, Mingze Sun, Zhirui Zhang, Yongqian Sun, Shenglin Zhang, Kun Wang, Haiming Zhang, Jianhui Li, Gaogang Xie, Xidao Wen, Xiaohui Nie, Minghua Ma, Dan Pei, OpsEval: A Comprehensive IT Operations Benchmark Suite for Large Language Models. 2023. 2023. 2023arXiv e-prints</p>
<p>An Empirical Study of NetOps Capability of Pre-Trained Large Language Models. Yukai Miao, Yu Bai, Haifeng Sun, Li Chen, Dan Li, Xizheng Wang, Ziqiu Luo, Dapeng Sun, Xiuting Xu, Qi Zhang, Chao Xiang, Xinchi Li, 2023. 2023. 2023arXiv e-prints</p>
<p>Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, Xu Jiang, Karl Cobbe, arXiv:2112.09332[cs.CL]WebGPT: Browser-assisted question-answering with human feedback. Tyna Eloundou, Gretchen Krueger, Kevin Button, Matthew Knight, Benjamin Chess, John Schulman, 2022</p>
<p>Training language models to follow instructions with human feedback. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul F Christiano, Jan Leike, Ryan Lowe, Advances in Neural Information Processing Systems. S Koyejo, A Mohamed, D Agarwal, K Belgrave, A Cho, Oh, Curran Associates, Inc202235</p>
<p>Loggpt: Exploring chatgpt for log-based anomaly detection. Jiaxing Qi, Shaohan Huang, Zhongzhi Luan, Shu Yang, Carol Fung, Hailong Yang, Depei Qian, Jing Shang, Zhiwen Xiao, Zhihui Wu, 2023 IEEE International Conference on High Performance Computing &amp; Communications, Data Science &amp; Systems, Smart City &amp; Dependability in Sensor, Cloud &amp; Big Data Systems &amp; Application. HPCC/DSS/SmartCity/DependSys. IEEE2023</p>
<p>Language Models are Unsupervised Multitask Learners. Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, 2019. 2019160025533</p>
<p>Jonas Baptiste Roziere, Fabian Gehring, Sten Gloeckle, Itai Sootla, Gat, Ellen Xiaoqing, Yossi Tan, Jingyu Adi, Tal Liu, JÃ©rÃ©my Remez, Rapin, arXiv:2308.12950Code llama: Open foundation models for code. 2023. 2023arXiv preprint</p>
<p>Shiwen Shan, Yintong Huo, Yuxin Su, Yichen Li, Dan Li, Zibin Zheng, arXiv:2404.00640Face It Yourselves: An LLM-Based Two-Stage Strategy to Localize Configuration Errors via Logs. 2024. 2024arXiv preprint</p>
<p>. Karan Singhal, Shekoofeh Azizi, Tao Tu, Sara Mahdavi, Jason Wei, Hyung Won Chung, Nathan Scales, Ajay Tanwani, 2022Heather Cole-Lewis, Stephen Pfohlet al. 2022. Large Language Models Encode Clinical Knowledge. arXiv e-prints</p>
<p>Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal, Md Shoeb, Abubakar Abid, Adam Fisch, Adam Adam R Brown, Aditya Santoro, AdriÃ  Gupta, Garriga-Alonso, Beyond the Imitation Game: Quantifying and Extrapolating the Capabilities of Language Models. 2022. 2022arXiv e-prints</p>
<p>LogKG: Log Failure Diagnosis Through Knowledge Graph. Yicheng Sui, Yuzhe Zhang, Jianjun Sun, Ting Xu, Shenglin Zhang, Zhengdan Li, Yongqian Sun, Fangrui Guo, Junyu Shen, Yuzhi Zhang, Dan Pei, Xiao Yang, Li Yu, 10.1109/TSC.2023.3293890IEEE Transactions on Services Computing. 162023. 2023</p>
<p>. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing , Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, arXiv:2307.09288[cs.CL]2023Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunovand Thomas Scialom. 2023. Llama 2: Open Foundation and Fine-Tuned Chat Models.</p>
<p>Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, Denny Zhou, arXiv:2203.11171[cs.CL]Self-Consistency Improves Chain of Thought Reasoning in Language Models. 2022. 2022</p>
<p>Bigscience Workshop, Le Teven, Angela Scao, Christopher Fan, Ellie Akiki, Suzana Pavlick, Daniel IliÄ‡, Roman Hesslow, Alexandra Sasha CastagnÃ©, FranÃ§ois Luccioni, Matthias Yvon, Jonathan GallÃ©, Alexander M Tow, Rush, arXiv:2211.05100[cs.CL]BLOOM: A 176B-Parameter Open-Access Multilingual Language Model. 2023</p>
<p>UniLog: Automatic Logging via LLM and In-Context Learning. Junjielong Xu, Ziang Cui, Yuan Zhao, Xu Zhang, Shilin He, Pinjia He, Liqun Li, Yu Kang, Qingwei Lin, Yingnong Dang, Proceedings of the 46th IEEE/ACM International Conference on Software Engineering. the 46th IEEE/ACM International Conference on Software Engineering2024</p>
<p>Prompting for Automatic Log Template Extraction. Junjielong Xu, Ruichun Yang, Yintong Huo, Chengyu Zhang, Pinjia He, arXiv:2307.099502023. 2023arXiv preprint</p>
<p>Hui Zeng, Measuring Massive Multitask Chinese Understanding. 2023. 2023. 2023arXiv e-prints</p>
<p>Evaluating the Generation Capabilities of Large Chinese Language Models. Hui Zeng, Jingyuan Xue, Meng Hao, Chen Sun, Bin Ning, Na Zhang, 2023. 2023arXiv e-prints</p>
<p>Liwen Zhang, Weige Cai, Zhaowei Liu, Zhi Yang, Wei Dai, Yujie Liao, Qianru Qin, Yifei Li, Xingyu Liu, Zhiqiang Liu, FinEval: A Chinese Financial Domain Knowledge Evaluation Benchmark for Large Language Models. 2023. 2023arXiv e-prints</p>
<p>OPT: Open Pre-trained Transformer Language Models. Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, Luke Zettlemoyer, arXiv:2205.01068[cs.CL]2022. 2022</p>
<p>Wei Zhang, Xianfu Cheng, Yi Zhang, Jian Yang, Hongcheng Guo, Zhoujun Li, Xiaolin Yin, Xiangyuan Guan, Xu Shi, Liangfan Zheng, arXiv:2405.13548ECLIPSE: Semantic Entropy-LCS for Cross-Lingual Industrial Log Parsing. 2024. 2024arXiv preprint</p>
<p>Robust log-based anomaly detection on unstable log data. Xu Zhang, Yong Xu, Qingwei Lin, Bo Qiao, Hongyu Zhang, Yingnong Dang, Chunyu Xie, Xinsheng Yang, Qian Cheng, Ze Li, Junjie Chen, Xiaoting He, Randolph Yao, Jian-Guang Lou, Murali Chintalapati, Furao Shen, Dongmei Zhang, 10.1145/3338906.3338931Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering. the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software EngineeringTallinn, Estonia; New York, NY, USAAssociation for Computing Machinery2019. 2019</p>
<p>CPM-2: Large-scale cost-effective pre-trained language models. Zhengyan Zhang, Yuxian Gu, Xu Han, Shengqi Chen, Chaojun Xiao, Zhenbo Sun, Yuan Yao, Fanchao Qi, Jian Guan, Pei Ke, Yanzheng Cai, Guoyang Zeng, Zhixing Tan, Zhiyuan Liu, Minlie Huang, Wentao Han, Yang Liu, Xiaoyan Zhu, Maosong Sun, 10.1016/j.aiopen.2021.12.003AI Open. 22021. 2021</p>
<p>Tools and benchmarks for automated log parsing. J Zhu, S He, J Liu, P He, Q Xie, Z Zheng, M R Lyu, IEEE/ACM 41st International Conference on Software Engineering: Software Engineering in Practice (ICSE-SEIP. 2019. 2019. 2019</p>            </div>
        </div>

    </div>
</body>
</html>
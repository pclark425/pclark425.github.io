<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7572 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7572</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7572</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-139.html">extraction-schema-139</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being used to detect anomalies in lists or tabular data, including the methods, datasets, evaluation metrics, and results.</div>
                <p><strong>Paper ID:</strong> paper-4232579</p>
                <p><strong>Paper Title:</strong> DeepLog: Anomaly Detection and Diagnosis from System Logs through Deep Learning</p>
                <p><strong>Paper Abstract:</strong> Anomaly detection is a critical step towards building a secure and trustworthy system. The primary purpose of a system log is to record system states and significant events at various critical points to help debug system failures and perform root cause analysis. Such log data is universally available in nearly all computer systems. Log data is an important and valuable resource for understanding system status and performance issues; therefore, the various system logs are naturally excellent source of information for online monitoring and anomaly detection. We propose DeepLog, a deep neural network model utilizing Long Short-Term Memory (LSTM), to model a system log as a natural language sequence. This allows DeepLog to automatically learn log patterns from normal execution, and detect anomalies when log patterns deviate from the model trained from log data under normal execution. In addition, we demonstrate how to incrementally update the DeepLog model in an online fashion so that it can adapt to new log patterns over time. Furthermore, DeepLog constructs workflows from the underlying system log so that once an anomaly is detected, users can diagnose the detected anomaly and perform root cause analysis effectively. Extensive experimental evaluations over large log data have shown that DeepLog has outperformed other existing log-based anomaly detection methods based on traditional data mining methodologies.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7572.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7572.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being used to detect anomalies in lists or tabular data, including the methods, datasets, evaluation metrics, and results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DeepLog LSTM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DeepLog Long Short-Term Memory (LSTM) language model</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LSTM-based neural language model that treats system log key sequences as a language: trained on short windows of recent log keys (multi-class next-key prediction) to detect execution-path anomalies, and separate LSTM models per log key to predict parameter-value vectors (multivariate time-series prediction) for performance/parameter anomalies.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LSTM</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Recurrent neural network using stacked Long Short-Term Memory layers; input encodes recent h log keys as one-hot vectors and outputs a multinomial distribution over next log keys; separate LSTM regressor per log-key for multivariate parameter-vector prediction (mean-square loss).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_detection_approach</strong></td>
                            <td>Supervised training on normal sequences (multi-class next-token prediction) for execution-path anomalies; regression (next-vector prediction) with MSE + Gaussian thresholding for parameter-value anomalies; anomalies flagged when observed next key not in top-k predictions or when MSE is outside validation confidence interval; supports online incremental updates from user feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>prompt_template</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Trained using small fraction of log data from normal execution only (e.g., <1% of HDFS sessions; examples: first 1% or 10% of logs depending on experiment). No anomalous examples required for initial training; online labeled false-positive examples optionally used to incrementally update weights.</td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>Log entry sequences (categorical log-key sequences) and multivariate time-series (parameter value vectors including elapsed times).</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>HDFS logs (11,197,954 entries; labeled; ~2.9% abnormal), OpenStack logs (1,335,318 entries; ~7% abnormal), Blue Gene/L system logs (4,747,963 entries) and VAST Challenge 2011 (network security logs) used in case studies.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Precision, Recall, F-measure (F1); False Positives (FP), False Negatives (FN); for parameter-value models MSE with Gaussian-modelled validation distribution and confidence-interval thresholding; timing (ms per log entry) reported for inference and amortized detection+update.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>HDFS: DeepLog achieved best overall performance with F-measure ≈ 96%; top-1 prediction matched 88.9% of normal keys, top-2 covered 96.1%, top-5 covered 99.8% (with anomaly detection rate 99.994% when using top-5). OpenStack: F-measure ≈ 98% (random-pattern OpenStack) and 97% (deterministic OpenStack II). Parameter-value anomalies: successfully detected injected network-throttle anomalies on OpenStack (identified by large elapsed times); online training experiments: with online updates recall remained 100% while reducing false positive rate from 40.1% → 1.7% (1% training) and 38.2% → 1.1% (10% training). Latency: prediction ≈ 1 ms per log entry (standard workstation); amortized detection+update 3.48 ms (1% training case with updates) and 2.46 ms (10% training case).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared against N-gram baseline (N=1 best for baseline), PCA-based session approaches, Invariant Mining (IM), TF-IDF + LSTM binary classifier, and CloudSeer workflow method. DeepLog outperformed these baselines on F-measure (e.g., PCA had fewest FPs but more FNs; IM had high recall but poor precision on irregular OpenStack logs).</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Supervised training using only normal examples (one-class/semi-supervised style); no anomalous examples required for initial model; online labeled false-positive examples used for incremental (few-shot style) adaptation.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Requires representative normal training data; without online updates novel-but-normal execution paths produce false positives (mitigated by online feedback). Concurrency and interleaving can reduce certainty; choice of history window h and top-k affects trade-off between precision and recall. Some bursty but benign events can cause false positives (example: repeated identical log messages in short time).</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Prediction cost ≈ 1 millisecond per log entry on a standard workstation; amortized detection+online update cost reported as ≈3.48 ms (1% training case with updates) and ≈2.46 ms (10% training case with updates); GPU can further reduce latency.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7572.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7572.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being used to detect anomalies in lists or tabular data, including the methods, datasets, evaluation metrics, and results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>N-gram baseline</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Traditional N-gram language model baseline</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An N-gram (Markov) language model that estimates Pr(next log key | recent N-1 keys) via relative frequency counts with sliding windows; used as a baseline for next-key prediction-based anomaly detection.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>N-gram</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Classical count-based N-gram language model (maximum likelihood estimates from sliding windows); in experiments N set to 1 unless otherwise specified.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_detection_approach</strong></td>
                            <td>Frequency-based next-token prediction: compute next-key probabilities using counts in N-length context; treat keys not within top-k predicted as anomalies.</td>
                        </tr>
                        <tr>
                            <td><strong>prompt_template</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Counts computed from training corpus of normal log sequences (same parsed log keys as DeepLog); typical setting N=1 used as baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>Categorical log-key sequences (system logs parsed to message types).</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>HDFS and OpenStack log datasets (same datasets used for DeepLog comparisons).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Precision, Recall, F-measure; FP and FN counts.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>N-gram (N=1) gets reasonably good performance on some datasets (e.g., HDFS) but performance degrades substantially as history length increases; overall inferior and less stable than LSTM-based DeepLog (DeepLog gave better F-measure across tests). Exact numeric baseline numbers are provided in paper tables/figures but N-gram did not surpass DeepLog.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared directly to DeepLog LSTM; DeepLog was more stable with larger history windows and achieved higher F-measure.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Trained on normal data counts (supervised/unsupervised depending on framing) — requires access to frequency counts from corpus (offline or streaming).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Cannot capture long-range dependencies or complex interleavings due to limited context; performance drops with larger history windows; less effective on concurrent/interleaved logs.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7572.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7572.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being used to detect anomalies in lists or tabular data, including the methods, datasets, evaluation metrics, and results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TFIDF + LSTM (prior work)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>TF-IDF epoch vectors with an LSTM binary classifier (prior method referenced as TFIDF)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior approach that groups log keys into fixed time windows (epochs), represents each epoch with TF-IDF vectors (requiring knowledge of total epochs for Laplace smoothing) and trains an LSTM binary classifier using both labeled normal and abnormal data for failure prediction/anomaly detection.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LSTM (binary classifier over TF-IDF features)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>LSTM-based sequential classifier that consumes TF-IDF vectors for epochs (time-windowed aggregated features) and is trained as a binary predictor (normal vs abnormal) requiring labeled anomalies.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_detection_approach</strong></td>
                            <td>Supervised binary classification on TF-IDF epoch vectors (requires both normal and abnormal labeled examples); uses Laplace smoothing and LSTM as classifier.</td>
                        </tr>
                        <tr>
                            <td><strong>prompt_template</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Requires labeled training data containing both normal and anomalous epochs; needs entire log to compute TF-IDF and Laplace smoothing terms (offline).</td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>Aggregated time-windowed feature vectors (TF-IDF term-frequency vectors over log-key tokens); not per-log-entry sequence modeling.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>Included in comparative evaluation on HDFS and OpenStack datasets in the paper (TFIDF results reported as poor on HDFS and omitted from some figures for space).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Precision, Recall, F-measure (same session-level granularity used for comparisons).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Reported as poor relative to DeepLog on HDFS (omitted from one comparative figure for space); TFIDF method has limitations on general anomaly detection because it requires anomalous labeled data and offline knowledge of epochs.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared against DeepLog, N-gram, PCA, and IM in the paper; DeepLog outperformed TFIDF notably due to TFIDF's need for labeled anomalies and offline processing assumptions.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Fully supervised (requires labeled anomalous examples); not zero-shot or few-shot.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Needs both normal and abnormal labeled data (hard to obtain); TF-IDF epoch construction requires knowledge of total number of epochs (offline); performs poorly when anomalies are not represented in training set; aggregated epoch representation cannot do per-log-entry anomaly detection.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Automated IT system failure prediction: A deep learning approach <em>(Rating: 2)</em></li>
                <li>Recurrent neural network based language model <em>(Rating: 1)</em></li>
                <li>LSTM Neural Networks for Language Modeling <em>(Rating: 1)</em></li>
                <li>Semi-supervised sequence learning <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7572",
    "paper_id": "paper-4232579",
    "extraction_schema_id": "extraction-schema-139",
    "extracted_data": [
        {
            "name_short": "DeepLog LSTM",
            "name_full": "DeepLog Long Short-Term Memory (LSTM) language model",
            "brief_description": "An LSTM-based neural language model that treats system log key sequences as a language: trained on short windows of recent log keys (multi-class next-key prediction) to detect execution-path anomalies, and separate LSTM models per log key to predict parameter-value vectors (multivariate time-series prediction) for performance/parameter anomalies.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LSTM",
            "model_description": "Recurrent neural network using stacked Long Short-Term Memory layers; input encodes recent h log keys as one-hot vectors and outputs a multinomial distribution over next log keys; separate LSTM regressor per log-key for multivariate parameter-vector prediction (mean-square loss).",
            "model_size": null,
            "anomaly_detection_approach": "Supervised training on normal sequences (multi-class next-token prediction) for execution-path anomalies; regression (next-vector prediction) with MSE + Gaussian thresholding for parameter-value anomalies; anomalies flagged when observed next key not in top-k predictions or when MSE is outside validation confidence interval; supports online incremental updates from user feedback.",
            "prompt_template": null,
            "training_data": "Trained using small fraction of log data from normal execution only (e.g., &lt;1% of HDFS sessions; examples: first 1% or 10% of logs depending on experiment). No anomalous examples required for initial training; online labeled false-positive examples optionally used to incrementally update weights.",
            "data_type": "Log entry sequences (categorical log-key sequences) and multivariate time-series (parameter value vectors including elapsed times).",
            "dataset_name": "HDFS logs (11,197,954 entries; labeled; ~2.9% abnormal), OpenStack logs (1,335,318 entries; ~7% abnormal), Blue Gene/L system logs (4,747,963 entries) and VAST Challenge 2011 (network security logs) used in case studies.",
            "evaluation_metric": "Precision, Recall, F-measure (F1); False Positives (FP), False Negatives (FN); for parameter-value models MSE with Gaussian-modelled validation distribution and confidence-interval thresholding; timing (ms per log entry) reported for inference and amortized detection+update.",
            "performance": "HDFS: DeepLog achieved best overall performance with F-measure ≈ 96%; top-1 prediction matched 88.9% of normal keys, top-2 covered 96.1%, top-5 covered 99.8% (with anomaly detection rate 99.994% when using top-5). OpenStack: F-measure ≈ 98% (random-pattern OpenStack) and 97% (deterministic OpenStack II). Parameter-value anomalies: successfully detected injected network-throttle anomalies on OpenStack (identified by large elapsed times); online training experiments: with online updates recall remained 100% while reducing false positive rate from 40.1% → 1.7% (1% training) and 38.2% → 1.1% (10% training). Latency: prediction ≈ 1 ms per log entry (standard workstation); amortized detection+update 3.48 ms (1% training case with updates) and 2.46 ms (10% training case).",
            "baseline_comparison": "Compared against N-gram baseline (N=1 best for baseline), PCA-based session approaches, Invariant Mining (IM), TF-IDF + LSTM binary classifier, and CloudSeer workflow method. DeepLog outperformed these baselines on F-measure (e.g., PCA had fewest FPs but more FNs; IM had high recall but poor precision on irregular OpenStack logs).",
            "zero_shot_or_few_shot": "Supervised training using only normal examples (one-class/semi-supervised style); no anomalous examples required for initial model; online labeled false-positive examples used for incremental (few-shot style) adaptation.",
            "limitations_or_failure_cases": "Requires representative normal training data; without online updates novel-but-normal execution paths produce false positives (mitigated by online feedback). Concurrency and interleaving can reduce certainty; choice of history window h and top-k affects trade-off between precision and recall. Some bursty but benign events can cause false positives (example: repeated identical log messages in short time).",
            "computational_cost": "Prediction cost ≈ 1 millisecond per log entry on a standard workstation; amortized detection+online update cost reported as ≈3.48 ms (1% training case with updates) and ≈2.46 ms (10% training case with updates); GPU can further reduce latency.",
            "uuid": "e7572.0"
        },
        {
            "name_short": "N-gram baseline",
            "name_full": "Traditional N-gram language model baseline",
            "brief_description": "An N-gram (Markov) language model that estimates Pr(next log key | recent N-1 keys) via relative frequency counts with sliding windows; used as a baseline for next-key prediction-based anomaly detection.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "N-gram",
            "model_description": "Classical count-based N-gram language model (maximum likelihood estimates from sliding windows); in experiments N set to 1 unless otherwise specified.",
            "model_size": null,
            "anomaly_detection_approach": "Frequency-based next-token prediction: compute next-key probabilities using counts in N-length context; treat keys not within top-k predicted as anomalies.",
            "prompt_template": null,
            "training_data": "Counts computed from training corpus of normal log sequences (same parsed log keys as DeepLog); typical setting N=1 used as baseline.",
            "data_type": "Categorical log-key sequences (system logs parsed to message types).",
            "dataset_name": "HDFS and OpenStack log datasets (same datasets used for DeepLog comparisons).",
            "evaluation_metric": "Precision, Recall, F-measure; FP and FN counts.",
            "performance": "N-gram (N=1) gets reasonably good performance on some datasets (e.g., HDFS) but performance degrades substantially as history length increases; overall inferior and less stable than LSTM-based DeepLog (DeepLog gave better F-measure across tests). Exact numeric baseline numbers are provided in paper tables/figures but N-gram did not surpass DeepLog.",
            "baseline_comparison": "Compared directly to DeepLog LSTM; DeepLog was more stable with larger history windows and achieved higher F-measure.",
            "zero_shot_or_few_shot": "Trained on normal data counts (supervised/unsupervised depending on framing) — requires access to frequency counts from corpus (offline or streaming).",
            "limitations_or_failure_cases": "Cannot capture long-range dependencies or complex interleavings due to limited context; performance drops with larger history windows; less effective on concurrent/interleaved logs.",
            "computational_cost": null,
            "uuid": "e7572.1"
        },
        {
            "name_short": "TFIDF + LSTM (prior work)",
            "name_full": "TF-IDF epoch vectors with an LSTM binary classifier (prior method referenced as TFIDF)",
            "brief_description": "A prior approach that groups log keys into fixed time windows (epochs), represents each epoch with TF-IDF vectors (requiring knowledge of total epochs for Laplace smoothing) and trains an LSTM binary classifier using both labeled normal and abnormal data for failure prediction/anomaly detection.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "LSTM (binary classifier over TF-IDF features)",
            "model_description": "LSTM-based sequential classifier that consumes TF-IDF vectors for epochs (time-windowed aggregated features) and is trained as a binary predictor (normal vs abnormal) requiring labeled anomalies.",
            "model_size": null,
            "anomaly_detection_approach": "Supervised binary classification on TF-IDF epoch vectors (requires both normal and abnormal labeled examples); uses Laplace smoothing and LSTM as classifier.",
            "prompt_template": null,
            "training_data": "Requires labeled training data containing both normal and anomalous epochs; needs entire log to compute TF-IDF and Laplace smoothing terms (offline).",
            "data_type": "Aggregated time-windowed feature vectors (TF-IDF term-frequency vectors over log-key tokens); not per-log-entry sequence modeling.",
            "dataset_name": "Included in comparative evaluation on HDFS and OpenStack datasets in the paper (TFIDF results reported as poor on HDFS and omitted from some figures for space).",
            "evaluation_metric": "Precision, Recall, F-measure (same session-level granularity used for comparisons).",
            "performance": "Reported as poor relative to DeepLog on HDFS (omitted from one comparative figure for space); TFIDF method has limitations on general anomaly detection because it requires anomalous labeled data and offline knowledge of epochs.",
            "baseline_comparison": "Compared against DeepLog, N-gram, PCA, and IM in the paper; DeepLog outperformed TFIDF notably due to TFIDF's need for labeled anomalies and offline processing assumptions.",
            "zero_shot_or_few_shot": "Fully supervised (requires labeled anomalous examples); not zero-shot or few-shot.",
            "limitations_or_failure_cases": "Needs both normal and abnormal labeled data (hard to obtain); TF-IDF epoch construction requires knowledge of total number of epochs (offline); performs poorly when anomalies are not represented in training set; aggregated epoch representation cannot do per-log-entry anomaly detection.",
            "computational_cost": null,
            "uuid": "e7572.2"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Automated IT system failure prediction: A deep learning approach",
            "rating": 2
        },
        {
            "paper_title": "Recurrent neural network based language model",
            "rating": 1
        },
        {
            "paper_title": "LSTM Neural Networks for Language Modeling",
            "rating": 1
        },
        {
            "paper_title": "Semi-supervised sequence learning",
            "rating": 1
        }
    ],
    "cost": 0.0142635,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>DeepLog: Anomaly Detection and Diagnosis from System Logs through Deep Learning</p>
<p>Min Du mind@cs.utah.edu 
School of Computing
University of Utah</p>
<p>Feifei Li lifeifei@cs.utah.edu 
School of Computing
University of Utah</p>
<p>Guineng Zheng guineng@cs.utah.edu 
School of Computing
University of Utah</p>
<p>Vivek Srikumar 
School of Computing
University of Utah</p>
<p>DeepLog: Anomaly Detection and Diagnosis from System Logs through Deep Learning
10.1145/3133956.3134015CCS CONCEPTS •Information systems → Online analytical processing•Security and privacy → Intrusion/anomaly detection and malware mitiga- tionKEYWORDS Anomaly detectiondeep learninglog data analysis
Anomaly detection is a critical step towards building a secure and trustworthy system. e primary purpose of a system log is to record system states and signi cant events at various critical points to help debug system failures and perform root cause analysis. Such log data is universally available in nearly all computer systems. Log data is an important and valuable resource for understanding system status and performance issues; therefore, the various system logs are naturally excellent source of information for online monitoring and anomaly detection. We propose DeepLog, a deep neural network model utilizing Long Short-Term Memory (LSTM), to model a system log as a natural language sequence. is allows DeepLog to automatically learn log pa erns from normal execution, and detect anomalies when log pa erns deviate from the model trained from log data under normal execution. In addition, we demonstrate how to incrementally update the DeepLog model in an online fashion so that it can adapt to new log pa erns over time. Furthermore, DeepLog constructs work ows from the underlying system log so that once an anomaly is detected, users can diagnose the detected anomaly and perform root cause analysis e ectively. Extensive experimental evaluations over large log data have shown that DeepLog has outperformed other existing log-based anomaly detection methods based on traditional data mining methodologies.</p>
<p>INTRODUCTION</p>
<p>Anomaly detection is an essential task towards building a secure and trustworthy computer system. As systems and applications get increasingly more complex than ever before, they are subject to more bugs and vulnerabilities that an adversary may exploit to launch a acks. Such a acks are also ge ing increasingly more sophisticated. As a result, anomaly detection has become more Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro t or commercial advantage and that copies bear this notice and the full citation on the rst page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permi ed. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci c permission and/or a fee. Request permissions from permissions@acm.org. CCS'17, Oct. 30 System logs record system states and signi cant events at various critical points to help debug performance issues and failures, and perform root cause analysis. Such log data is universally available in nearly all computer systems and is a valuable resource for understanding system status. Furthermore, since system logs record noteworthy events as they occur from actively running processes, they are an excellent source of information for online monitoring and anomaly detection.</p>
<p>Existing approaches that leverage system log data for anomaly detection can be broadly classi ed into three groups: PCA based approaches over log message counters [39], invariant mining based methods to capture co-occurrence pa erns between di erent log keys [21], and work ow based methods to identify execution anomalies in program logic ows [42]. Even though they are successful in certain scenarios, none of them is e ective as a universal anomaly detection method that is able to guard against di erent a acks in an online fashion.</p>
<p>is work proposes DeepLog, a data-driven approach for anomaly detection that leverages the large volumes of system logs. e key intuition behind the design of DeepLog is from natural language processing: we view log entries as elements of a sequence that follows certain pa erns and grammar rules. Indeed, a system log is produced by a program that follows a rigorous set of logic and control ows, and is very much like a natural language (though more structured and restricted in vocabulary). To that end, DeepLog is a deep neural network that models this sequence of log entries using a Long Short-Term Memory (LSTM) [18]. is allows DeepLog to automatically learn a model of log pa erns from normal execution and ag deviations from normal system execution as anomalies. Furthermore, since it is a learning-driven approach, it is possible to incrementally update the DeepLog model so that it can adapt to new log pa erns that emerge over time.</p>
<p>Challenges. Log data are unstructured, and their format and semantics can vary signi cantly from system to system. It is already challenging to diagnose a problem using unstructured logs even a er knowing an error has occurred [43]; online anomaly detection from massive log data is even more challenging. Some existing methods use rule-based approaches to address this issue, which requires speci c domain knowledge [41], e.g., using features like "IP address" to parse a log. However, this does not work for general purpose anomaly detection where it is almost impossible to know a priori what are interesting features in di erent types of logs (and to guard against di erent types of a acks).</p>
<p>Anomaly detection has to be timely in order to be useful so that users can intervene in an ongoing a ack or a system performance issue [10]. Decisions are to be made in streaming fashion. As a result, o ine methods that need to make several passes over the entire log data are not applicable in our se ing [22,39]. We would also like to be able to detect unknown types of anomalies, rather than gearing towards speci c types of anomalies. erefore, previous work [44] that use both normal and abnormal (for speci c types of anomalies) log data entries to train a binary classi er for anomaly detection is not useful in this context.</p>
<p>Another challenge comes from concurrency. Clearly, the order of log messages in a log provides important information for diagnosis and analysis (e.g., identify the execution path of a program). However, in many system logs, log messages are produced by several di erent threads or concurrently running tasks. Such concurrency makes it hard to apply work ow based anomaly detection methods [42] which use a work ow model for a single task as a generative model to match against a sequence of log messages.</p>
<p>Lastly, each log message contains rich information such as a log key and one or more metric values, as well as its timestamp. A holistic approach that integrates and utilizes these di erent pieces of information will be more e ective. Most existing methods [22,32,39,41,42,44] analyze only one speci c part of a log message (e.g., the log key) which limits the types of anomalies they can detect.</p>
<p>Our contribution. A Recurrent Neural Network (RNN) is an articial neural network that uses a loop to forward the output of last state to current input, thus keeping track of history for making predictions. Long Short-Term Memory (LSTM) networks [13,18,27] are an instance of RNNs that have the ability to remember long-term dependencies over sequences. LSTMs have demonstrated success in various tasks such as machine translation [35], sentiment analysis [8], and medical self-diagnosis [20].</p>
<p>Inspired by the observation that entries in a system log are a sequence of events produced by the execution of structured source code (and hence can be viewed as a structured language), we design the DeepLog framework using a LSTM neural network for online anomaly detection over system logs. DeepLog uses not only log keys but also metric values in a log entry for anomaly detection, hence, it is able to capture di erent types of anomalies. DeepLog only depends on a small training data set that consists of a sequence of "normal log entries". A er the training phase, DeepLog can recognize normal log sequences and can be used for online anomaly detection over incoming log entries in a streaming fashion.</p>
<p>Intuitively, DeepLog implicitly captures the potentially nonlinear and high dimensional dependencies among log entries from the training data that correspond to normal system execution paths. To help users diagnose a problem once an anomaly is identi ed, DeepLog also builds work ow models from log entries during its training phase. DeepLog separates log entries produced by concurrent tasks or threads into di erent sequences so that a work ow model can be constructed for each separate task.</p>
<p>Our evaluation shows that on a large HDFS log dataset explored by previous work [22,39], trained on only a very small fraction (less than 1%) of log entries corresponding to normal system execution, DeepLog can achieve almost 100% detection accuracy on the remaining 99% of log entries. Results from a large OpenStack log convey a similar trend. Furthermore, DeepLog also provides the ability to incrementally update its weights during the detection phase by incorporating live user feedback. More speci cally, DeepLog provides a mechanism for user feedback if a normal log entry is incorrectly classi ed as an anomaly. DeepLog can then use such feedback to adjust its weights dynamically online over time to adapt itself to new system execution (hence, new log) pa erns.</p>
<p>PRELIMINARIES 2.1 Log parser</p>
<p>We rst parse unstructured, free-text log entries into a structured representation, so that we can learn a sequential model over this structured data. As shown by several prior work [9,22,39,42,45], an e ective methodology is to extract a "log key" (also known as "message type") from each log entry. e log key of a log entry e refers to the string constant k from the print statement in the source code which printed e during the execution of that code. For example, the log key k for log entry e ="Took 10 seconds to build instance." is k =Took * seconds to build instance., which is the string constant from the print statement printf("Took %f seconds to build instance.", t). Note that the parameter(s) are abstracted as asterisk(s) in a log key. ese metric values re ect the underlying system state and performance status. Values of certain parameters may serve as identi ers for a particular execution sequence, such as block_id in a HDFS log and instance_id in an OpenStack log. ese identi ers can group log entries together or untangle log entries produced by concurrent processes to separate, single-thread sequential sequences [22,39,42,45]. e state-of-the-art log parsing method is represented by Spell [9], an unsupervised streaming parser that parses incoming log entries in an online fashion based on the idea of LCS (longest common subsequence).</p>
<p>Past work on log analysis [22,39,42,44] have discarded timestamp and/or parameter values in a log entry, and only used log keys to detect anomalies. DeepLog stores parameter values for each log entry e, as well as the time elapsed between e and its predecessor, into a vector − → e . is vector is used by DeepLog in addition to the log key. An example is given in Table 1, which shows the parsing results for a sequence of log entries from multiple rounds of execution of virtual machine (VM) deletion task in OpenStack.</p>
<p>DeepLog architecture and overview</p>
<p>e architecture of DeepLog is shown in Figure 1 with three main components: the log key anomaly detection model, the parameter value anomaly detection model, and the work ow model to diagnose detected anomalies.</p>
<p>Training stage. Training data for DeepLog are log entries from normal system execution path. Each log entry is parsed to a log key and a parameter value vector. e log key sequence parsed from a training log le is used by DeepLog to train a log key anomaly detection model, and to construct system execution work ow models for diagnosis purposes. For each distinct key k, DeepLog also trains and maintains a model for detecting system performance anomalies as re ected by these metric values, trained by the parameter value vector sequence of k.</p>
<p>Detection stage. A newly arrived log entry is parsed into a log key and a parameter value vector. DeepLog rst uses the log key log message (log key underlined)</p>
<p>log key parameter value vector  Figure 1: DeepLog architecture.
t 1 Deletion of le1 complete k 1 [t 1 − t 0 , le1Id] t 2 Took 0.61 seconds to deallocate network … k 2 [t 2 − t 1 , 0.61] t 3 VM Stopped (Lifecycle Event) k 3 [t 3 − t 2 ] … … …
anomaly detection model to check whether the incoming log key is normal. If yes, DeepLog further checks the parameter value vector using the parameter value anomaly detection model for that log key.</p>
<p>e new entry will be labeled as an anomaly if either its log key or its parameter value vector is predicted being abnormal. Lastly, if it is labeled being abnormal, DeepLog's work ow model provides semantic information for users to diagnose the anomaly. Execution pa erns may change over time or were not included in the original training data. DeepLog also provides the option for collecting user feedback. If the user reports a detected anomaly as false positive, DeepLog could use it as a labeled record to incrementally update its models to incorporate and adapt to the new pa ern.</p>
<p>reat model</p>
<p>DeepLog learns the comprehensive and intricate correlations and pa erns embedded in a sequence of log entries produced by normal system execution paths. Henceforth, we assume that system logs themselves are secure and protected, and an adversary cannot a ack the integrity of a log itself. We also assume that an adversary cannot modify the system source code to change its logging behavior and pa erns. at said, broadly speaking, there are two types of a acks that we consider.</p>
<p>(1) A acks that lead to system execution misbehavior and hence anomalous pa erns in system logs. For instance, Denial of Service (DoS) a acks which may cause slow execution and hence performance anomalies re ected in the log timestamp di erences from the parameter value vector sequence; a acks causing repeated server restarts such as Blind Return Oriented Programming (BROP) a ack [5] shown as too many server restart log keys; and any attack that may cause task abortion such that the corresponding log sequence ends early and/or exception log entries appear.</p>
<p>(2) A acks that could leave a trace in system logs due to the logging activities of system monitoring services. An example is suspicious activities logged by an Intrusion Detection System (IDS).</p>
<p>ANOMALY DETECTION 3.1 Execution path anomaly</p>
<p>We rst describe how to detect execution path anomalies using the log key sequence. Since the total number of distinct print statements (that print log entries) in a source code is constant, so is the total number of distinct log keys. Let K = {k 1 , k 2 , . . . , k n } be the set of distinct log keys from a log-producing system source code.</p>
<p>Once log entries are parsed into log keys, the log key sequence re ects an execution path that leads to that particular execution order of the log print statements. Let m i denote the value of the key at position i in a log key sequence. Clearly, m i may take one of the n possible keys from K, and is strongly dependent on the most recent keys that appeared prior to m i .</p>
<p>We can model anomaly detection in a log key sequence as a multiclass classi cation problem, where each distinct log key de nes a class. We train DeepLog as a multi-class classi er over recent context. e input is a history of recent log keys, and the output is a probability distribution over the n log keys from K, representing the probability that the next log key in the sequence is a key k i ∈ K. Figure 2 summarizes the classi cation setup. Suppose t is the sequence id of the next log key to appear. e input for classication is a window w of the h most recent log keys.</p>
<p>at is,
w = {m t −h , . . . , m t −2 , m t −1 },
where each m i is in K and is the log key from the log entry e i . Note that the same log key value may appear several times in w. e output of the training phase is a model of the conditional probability distribution Pr[m t = k i |w] for each k i ∈ K(i = 1, . . . , n). e detection phase uses this model to make a prediction and compare the predicted output against the observed log key value that actually appears.</p>
<p>Training stage. e training stage relies on a small fraction of log entries produced by normal execution of the underlying system. For each log sequence of length h in the training data, DeepLog DeepLog Input: h recent log keys up to Output: conditional probability of next log key given the input recent sequence Figure 2: An overview of log key anomaly detection model.</p>
<p>updates its model for the probability distribution of having k i ∈ K as the next log key value. For example, suppose a small log le resulted from normal execution is parsed into a sequence of log keys: {k 22 , k 5 , k 11 , k 9 , k 11 , k 26 }. Given a window size h = 3, the input sequence and the output label pairs to train DeepLog will be: {k 22 , k 5 , k 11 → k 9 }, {k 5 , k 11 , k 9 → k 11 }, {k 11 , k 9 , k 11 → k 26 }.</p>
<p>Detection stage. DeepLog performs anomaly detection in an online, streaming se ing. To test if an incoming log key m t (parsed from an incoming log entry e t ) is to be considered normal or abnormal, we send w = {m t −h , ..., m t −1 } to DeepLog as its input. e output is a probability distribution Pr[m t |w] = {k 1 : p 1 , k 2 : p 2 , ..., k n : p n } describing the probability for each log key from K to appear as the next log key value given the history.</p>
<p>In practice, multiple log key values may appear as m t . For instance, if the system is a empting to connect to a host, then m t could either be 'Waiting for * to respond' or 'Connected to *'; both are normal system behavior. DeepLog must be able to learn such pa erns during training. Our strategy is to sort the possible log keys K based on their probabilities Pr[m t |w], and treat a key value as normal if it's among the top candidates. A log key is agged as being from an abnormal execution otherwise.</p>
<p>3.1.1 Traditional N-gram language model. e problem of ascribing probabilities to sequences of words drawn from a xed vocabulary is the classic problem of language modeling, widely studied by the natural language processing (NLP) community [24]. In our case, each log key can be viewed as a word taken from the vocabulary K. e typical language modeling approach for assigning probabilities to arbitrarily long sequences is the N-gram model. e intuition is that a particular word in a sequence is only in uenced by its recent predecessors rather than the entire history. In our se ing, this approximation is equivalent to se ing Pr(m t = k i |m 1 , . . . , m t −1 ) = Pr(m t = k i |m t −N , . . . , m t −1 ) where N denotes the length of the recent history to be considered.</p>
<p>For training, we can calculate this probability using relative frequency counts from a large corpus to give us maximum likelihood estimates. Given a long sequence of keys {m 1 , m 2 , . . . , m t }, we can estimate the probability of observing the i t h key k i using the relative frequency counts of {m t −N , . . . , m t −1 , m t = k i } with respect to the sequence {m t −N , . . . , m t −1 }. In other words, Pr(m t = k i |m 1 , . . . , m t −1 ) = count(m t −N , . . ., m t −1 , m t = k i )/count(m t −N , . . ., m t −1 ). Note that we will count these frequencies using a sliding window of size N over the entire key sequence.</p>
<p>To apply the N-gram model in our se ing, we simply use N as the history window size, i.e., we set h = N in our experiments when the N-gram model is used where h is the history sliding window size as depicted in Figure 2. We use this as a baseline method.</p>
<p>3.1.2 The LSTM approach. In recent years, neural language models that use recurrent neural networks have been shown to be highly e ective across various NLP tasks [3,25]  language model, a LSTM-based one can encode more intricate patterns and maintain long-range state over a sequence [34]. Complex pa erns and interleaving log entries from concurrent tasks in a system log can render a traditional language model less e ective. us, DeepLog uses a LSTM neural network [18] for anomaly detection from a log key sequence.</p>
<p>Given a sequence of log keys, a LSTM network is trained to maximize the probability of having k i ∈ K as the next log key value as re ected by the training data sequence. In other words, it learns a probability distribution Pr(m t = k i |m t −h , . . ., m t −2 , m t −1 ) that maximizes the probability of the training log key sequence. Figure 3 illustrates our design. e top of the gure shows a single LSTM block that re ects the recurrent nature of LSTM. Each LSTM block remembers a state for its input as a vector of a xed dimension. e state of an LSTM block from the previous time step is also fed into its next input, together with its (external) data input (m t −i in this particular example), to compute a new state and output.</p>
<p>is is how historical information is passed to and maintained in a single LSTM block.</p>
<p>A series of LSTM blocks form an unrolled version of the recurrent model in one layer as shown in the center of Figure 3. Each cell maintains a hidden vector H t −i and a cell state vector C t −i . Both are passed to the next block to initialize its state. In our case, we use one LSTM block for each log key from an input sequence w (a window of h log keys). Hence, a single layer consists of h unrolled LSTM blocks.</p>
<p>Within a single LSTM block, the input (e.g. m t −i ) and the previous output (H t −i−1 ) are used to decide (1) how much of the previous cell state C t −i−1 to retain in state C t −i , (2) how to use the current input and the previous output to in uence the state, and (3) how to construct the output H t −i . is is accomplished using a set of gating functions to determine state dynamics by controlling the amount of information to keep from input and previous output, and the information ow going to the next step. Each gating function is parameterized by a set of weights to be learned. e expressive capacity of an LSTM block is determined by the number of memory units (i.e. the dimensionality of the hidden state vector H ). Due to space constraints, we refer the reader to NLP primers (e.g., [12]) for a formal characterization of LSTMs. e training step entails nding proper assignments to the weights so that the nal output of the sequence of LSTMs produces the desired label (output) that comes with inputs in the training data set. During the training process, each input/output pair incrementally updates these weights, through loss minimization via gradient descent. In DeepLog, an input consists of a window w of h log keys, and an output is the log key value that comes right a er w. We use the categorical cross-entropy loss for training.</p>
<p>A er training is done, we can predict the output for an input (w = {m t −h , . . . , m t −1 }) using a layer of h LSTM blocks. Each log key in w feeds into a corresponding LSTM block in this layer.</p>
<p>If we stack up multiple layers and use the hidden state of the previous layer as the input of each corresponding LSTM block in the next layer, it becomes a deep LSTM neural network, as shown at the bo om of Figure 3. For simplicity, it omits an input layer and an output layer constructed by standard encoding-decoding schemes. e input layer encodes the n possible log keys from K as one-hot vectors. at is, a sparse n-dimensional vector − → u i is constructed
for the log key k i ∈ K, such that − → u i [i] = 1 and − → u i [j]
= 0 for all other j i. e output layer translates the nal hidden state into a probability distribution function using a standard multinomial logistic function to represent Pr[m t = k i |w] for each k i ∈ K. e example at the bo om of Figure 3 shows only two hidden layers, but more layers can be used.</p>
<p>Parameter value and performance anomaly</p>
<p>e log key sequence is useful for detecting execution path anomalies. However, some anomalies are not shown as a deviation from a normal execution path, but as an irregular parameter value. ese parameter value vectors (for the same log key) form a parameter value vector sequence, and these sequences from di erent log keys form a multi-dimensional feature space that is important for performance monitoring and anomaly detection.</p>
<p>Baseline approach. A simple approach is to store all parameter value vector sequences into a matrix, where each column is a parameter value sequence from a log key k (note that it is possible to have multiple columns for k depending on the size of its parameter value vector). Row i in this matrix represents a time instance t i .</p>
<p>Consider the log entries in Table 1 as an example. ere are 3 distinct log key values in this example, and the sizes of their parameter value vectors are 2, 2, and 1 respectively. Hence, row 1 in this matrix represents time instance t 1 with values [t 1 − t 0 , le1Id, null, null, null]. Similarly, row 2 and row 3 are [null, null, t 2 − t 1 , 0.61, null] and [null, null, null, null, t 3 − t 2 ] respectively.</p>
<p>We may also ask each row to represent a range of time instances so that each row corresponds to multiple log messages within that time range and becomes less sparse. But the matrix will still be very sparse when there are many log key values and/or exists some large parameter value vectors. Furthermore, this approach introduces a delay to the anomaly detection process, and it is also di cult to gure out a good value for the length of each range. Given this matrix, many well-known data-driven anomaly detection methods can be applied, such as principal component analysis (PCA) and self-organizing maps (SOM). ey are useful towards capturing correlation among di erent feature dimensions. However, a major limitation of this method in the context of log data is that o en times the appearance of multiple log keys at a particular time instance is equally likely. For instance, the order of k 1 and k 2 in Table 1 is arbitrary due to concurrently running tasks. is phenomena, and the fact that the matrix is sparse, render these techniques ine ective in our se ing. Lastly, they are not able to model auto-correlation that exists in a parameter value vector sequence (regular pa erns over time in a single vector sequence).</p>
<p>Our approach. DeepLog trains a parameter value anomaly detection model by viewing each parameter value vector sequence (for a log key) as a separate time series.</p>
<p>Consider the example in Table 1. e time series for the parameter value vector sequence of k 2 is:
{[t 2 −t 1 , 0.61], [t 2 −t 1 , 1]}.
Hence, our problem is reduced to anomaly detection from a multi-variate time series data. It is possible to apply an LSTM-based approach again. We use a similar LSTM network as shown in Figure 3 to model a multi-variate time series data, with the following adjustments. Note that a separate LSTM network is built for the parameter value vector sequence of each distinct log key value.</p>
<p>Input. e input at each time step is simply the parameter value vector from that timestamp. We normalize the values in each vector by the average and the standard deviation of all values from the same parameter position from the training data.</p>
<p>Output. e output is a real value vector as a prediction for the next parameter value vector, based on a sequence of parameter value vectors from recent history.</p>
<p>Objective function for training. For the multi-variate time series data, the training process tries to adjust the weights of its LSTM model in order to minimize the error between a prediction and an observed parameter value vector. us, mean square loss is used to minimize the error during the training process.</p>
<p>Anomaly detection. e di erence between a prediction and an observed parameter value vector is measured by the mean square error (MSE). Instead of se ing a magic error threshold for anomaly detection purpose in an ad-hoc fashion, we partition the training data to two subsets: the model training set and the validation set. For each vector − → in the validation set, we apply the model produced by the training set to calculate the MSE between the prediction (using the vector sequence from before − → in the validation set) and − → . At every time step, the errors between the predicted vectors and the actual ones in the validation group are modeled as a Gaussian distribution. At deployment, if the error between a prediction and an observed value vector is within a high-level of con dence interval of the above Gaussian distribution, the parameter value vector of the incoming log entry is considered normal, and is considered abnormal otherwise.</p>
<p>Since parameter values in a log message o en record important system state metrics, this method is able to detect various types of performance anomalies. For example, a performance anomaly may re ect as a "slow down". Recall that DeepLog stores in each parameter value vector the time elapsed between consecutive log entries. e above LSTM model, by modeling parameter value vector as a multi-variate time series, is able to detect unusual pa erns in one </p>
<p>Online update of anomaly detection models</p>
<p>Clearly, the training data may not cover all possible normal execution pa erns. System behavior may change over time, additionally depending on workload and data characteristics. erefore, it is necessary for DeepLog to incrementally update weights in its LSTM models to incorporate and adapt to new log pa erns. To do this, DeepLog provides a mechanism for the user to provide feedback. is allows DeepLog to use a false positive to adjust its weights. For example, suppose h = 3 and the recent history sequence is {k 1 , k 2 , k 3 }, and DeepLog has predicted the next log key to be k 1 with probability 1, while the next log key value is k 2 , which will be labeled as an anomaly. If user reports that this is a false positive, DeepLog is able to use the following input-output pair {k 1 , k 2 , k 3 → k 2 } to update the weights of its model to learn this new pa ern. So that next time given history sequence {k 1 , k 2 , k 3 }, DeepLog can output both k 1 and k 2 with updated probabilities. e same update procedure works for the parameter value anomaly detection model. Note that DeepLog does not need to be re-trained from scratch. A er the initial training process, models in DeepLog exist as several multi-dimensional weight vectors. e update process feeds in new training data, and adjusts the weights to minimize the error between model output and actual observed values from the false positive cases.</p>
<p>WORKFLOW CONSTRUCTION FROM MULTI-TASKS EXECUTION</p>
<p>Each log key is the execution of a log printing statement in the source code, while a task like VM creation will produce a sequence of log entries. Intuitively, the order of log entries produced by a task represents an execution order of each function for accomplishing this task. As a result, we can build a work ow model as a nite state automaton (FSA) to capture the execution path of any task. is work ow model can also be used to detect execution path anomalies, but it is less e ective compared to DeepLog's LSTM model due to its inability to capture inter-task dependencies and nondeterministic loop iterations. However, the work ow model is very useful towards enabling users to diagnose what had gone wrong in the execution of a task when an anomaly has been detected. Given a log sequence generated by the repeated executions of a task, there have been several works exploring the problem of workow inference [4,21,42]. CloudSeer [42] represents the state of the art in anomaly detection using a work ow model. CloudSeer has several limitations. Firstly, the anomalies it can detect are limited to log entries having "ERROR" logging level and log entries not appearing. Furthermore, its work ow model construction requires a log le with repeated executions of only one single task. Other previous works [4,21] on work ow construction from a log le also su er from this limitation. In practice, a log le o en contains interleaving log entries produced by multiple tasks and potentially concurrently running threads within a task.</p>
<p>Log entry separation from multiple tasks</p>
<p>An easy case is when multiple programs concurrently write to the same log (e.g., Ubuntu's system log). O en each log entry contains the name of the program that created it. Another easy case is when the process or task id is included in a log entry. Here, we focus on the case where a user program is executed repeatedly to perform di erent, but logically related, tasks within that program. An important observation is that tasks do not overlap in time. However, the same log key may appear in more than one task, and concurrency is possible within each task (e.g., multiple threads in one task).</p>
<p>Consider OpenStack administrative logs as an example. For each VM instance, its life cycle contains VM creation, VM stop, VM deletion and others. ese tasks do not overlap, i.e., VM stop can only start a er VM creation has completed. However, the same log key may appear in di erent tasks. For example, a log message "VM Resumed (Lifecycle Event)" may appear in VM creation, VM start, VM resume and VM unpause. ere could be concurrently running threads inside each task, leading to uncertainty in the ordering of log messages corresponding to one task. For instance, during VM creation, the order of two log messages "Took * seconds to build instance" and "VM Resumed (Lifecycle Event)" is uncertain.</p>
<p>Our goal is to separate log entries for di erent tasks in a log le, and then build a work ow model for each task based on its log key sequence. at said, the input of our problem is the entire log key sequence parsed from a raw log le, and the output is a set of work ow models, one for each task identi ed.</p>
<p>Using DeepLog's anomaly detection model</p>
<p>Log key separation.</p>
<p>Recall that in DeepLog's model for anomaly detection from log keys, the input is a sequence of log keys of length h from recent history, and the output is a probability distribution of all possible log key values. An interesting observation is that its output actually encodes the underlying work ow execution path.</p>
<p>Intuitively, given a log key sequence, our model predicts what will happen next based on the execution pa erns it has observed during the training stage. If a sequence w is never followed by a particular key value k in the training stage, then Pr[m t = k |w] = 0. Correspondingly, if a sequence w is always followed by k, then Pr[m t = k |w] = 1. For example, suppose on a sequence "25→54", the output prediction is "{57:1.00}", we know that "25→54→57" is from one task. A more complicated case is when a sequence w is to be followed by a log key value from a group of di erent keys; the probabilities of these keys to appear a er w sum to 1.</p>
<p>To handle this case, we use an idea that is inspired by small invariants mining [21].</p>
<p>Consider a log sequence "54→57", and suppose the predicted probability distribution is "{18: 0.8, 56: 0.2}", which means that the next step could be either "18" or "56".</p>
<p>is ambiguity could be caused by using an insu cient history sequence length. For example, if two tasks share the same work ow segment "54→57", the rst task has a pa ern "18→54→57→18" that is executed 80% of the time, and the second task has a pa ern "31→54→57→56" that is executed 20% of the time.</p>
<p>is will lead to a model that predicts "{18: 0.8, 56: 0.2}" given the sequence "54→57".</p>
<p>We can address this issue by training models with di erent history sequence lengths, e.g., using h = 3 instead of h = 2 in this case. During work ow construction, we use a log sequence length that leads to a more certain prediction, e.g. in the above example the  Figure 4: Examples of using LSTM for task separation and work ow construction.</p>
<p>sequence "18→54→57" will lead to the prediction {18: 1.00} and the sequence "31→54→57" will lead to the prediction {56: 1.00}.</p>
<p>If we have ruled out that a small sequence is a shared segment from di erent tasks (i.e., increasing the sequence length for training and prediction doesn't lead to more certain prediction), the challenge now is to nd out whether the multi-key prediction output is caused by either concurrency in the same task or the start of a di erent task. We call this a divergence point.</p>
<p>We observe that, as shown in Figure 4a, if the divergence point is caused by concurrency in the same task, a common pa ern is that keys with the highest probabilities in the prediction output will appear one a er another, and the certainty (measured by higher probabilities for less number of keys) for the following predictions will increase, as keys for some of the concurrent threads have already appeared. e prediction will eventually become certain a er all keys from concurrent threads are included in the history sequence.</p>
<p>On the other hand, if the divergence point is caused by the start of a new task, as shown in Figure 4b, the predicted log key candidates ("24" and "26" in the example) will not appear one a er another. If we incorporate each such log key into the history sequence, the next prediction is a deterministic prediction of a new log key (e.g., "24→60", "26→37"). If this is the case, we stop growing the work ow model of the current task (stop at key "57" in this example), and start constructing work ow models for new tasks. Note that the two "new tasks" in Figure 4b could also be an "if-else" branch, e.g., "57→if (24→60→…) else (26→37→…)". To handle such situations, we apply a simple heuristic: if the "new task" has very few log keys (e.g., 3) and always appears a er a particular task T p , we treat it as part of an "if-else" branch of T p , otherwise as a new task.</p>
<p>4.2.2 Build a workflow model. Once we can distinguish divergence points caused by concurrency (multiple threads) in the same task and new tasks, we can easily construct work ow models as illustrated in Figure 4a and Figure 4b. Additional care is needed to identify loops. e detection of a loop is actually quite straightforward. A loop is always shown in the initial work ow model as an unrolled chain; see Figure 4c for an example. While this work ow chain is initially "26→37→39→40→39→40", we could identify the repeated fragments as a loop execution (39→40 in this example).</p>
<p>Using density-based clustering approach</p>
<p>4.3.1 Log key separation. Another approach is to use a densitybased clustering technique. e intuition is that log keys in the 
k 1 … k j … k n k 1 p d (1, 1) p d (1, j) … k i p d (i, 1) p d (i, j) = f d (k i , k j ) d ·f (k i ) ) … k n p d (n, 1) p d (n, j)
same task always appear together, but log keys from di erent tasks may not always appear together as the ordering of tasks is not xed during multiple executions of di erent tasks. is allows us to cluster log keys based on co-occurrence pa erns, and separate keys into di erent tasks when co-occurrence rate is low.</p>
<p>In a log key sequence, the distance d between any two log keys is de ned as the number of log keys between them plus 1. For example, given the sequence {k 1 , k 2 , k 2 }, d(k 1 , k 2 ) = [1, 2], d(k 2 , k 2 ) = 1 (note that there are two distance values between the pair (k 1 , k 2 )).</p>
<p>We build a co-occurrence matrix as shown in Table 2, where each element p d (i, j) represents the probability of two log keys k i and k j appearing within distance d in the input sequence. Speci cally, let f (k i ) be the frequency of k i in the input sequence, and f d (k i , k j ) be the frequency of pair (k i , k j ) appearing together within distance d in the input sequence. We de ne
p d (i, j) = f d (k i ,k j ) d ·f (k i ) , which shows the importance of k j to k i . For example, when d = 1, p 1 (i, j) = f 1 (k i ,k j ) f (k i )
= 1 means that for every occurrence of k i , there must be a k j next to it. Note that in this de nition, f (k i ) in the denominator is scaled by d because while counting co-occurrence frequencies within d, a key k i is counted by d times. Scaling f (k i ) by a factor of d ensures that n j=1 f d (i, j) = 1 for any i. Note that we can build multiple co-occurrence matrices for di erent distance values of d.</p>
<p>With a co-occurrence matrix for each distance value d that we have built, our goal is to output a set of tasks T ASK = (T 1 ,T 2 , ...). e clustering procedure works as follows. First, for d = 1, we check if any p 1 (i, j) is greater than a threshold τ (say τ = 0.9), when it does, we connect k i , k j together to form T 1 = [k i , k j ]. Next, we recursively check if T 1 could be extended from either its head or tail. For example, if there exists k x ∈ K such that p 1 (k i , k x ) &gt; τ , we further check if p 2 (k j , k x ) &gt; τ , i.e., if k j and k x have a large co-occurrence probability within distance 2. If yes,
T 1 = [k x , k i , k j ], otherwise we will add T 2 = [k i , k x ] to T ASK.
is procedure continues until no task T in T ASK could be further extended. In the general case when a task T to be extended  k 3 , k 4 ), p 2 (k 2 , k 4 ), p 3 (k 1 , k 4 )) &gt; τ . e above process connects sequential log keys for each task. When a task T 1 = [k i , k j ] cannot be extended to include any single key, we check if T 1 could be extended by two log keys, i.e., if there exists k x , k ∈ K, such that p 1 (k i , k x ) + p 1 (k i , k ) &gt; τ , or p 1 (k j , k x ) + p 1 (k j , k ) &gt; τ . Suppose the la er case is true, the next thing to check is whether k x and k are log keys produced by concurrent threads in task T 1 . If they are, p d (k j , k x ) always increases with larger d values, i.e., p 2 (k j , k x ) &gt; p 1 (k j , k x ), which is intuitive because the appearance ordering of keys from concurrent threads is not certain. Otherwise k x and k do not belong to T 1 , thus we add
T 2 = [k j , k x ] and T 3 = [k j , k ] into T ASK instead.
Finally, for each task T in T ASK, we eliminate T if its sequence is included as a sub-sequence in another task.</p>
<p>4.3.2 Build workflow model. Once a log key sequence is separated out and identi ed for each task, the work ow model construction for a task follows the same discussion from Section 4.2.2.</p>
<p>Using the work ow model</p>
<p>4.4.1 Set parameters for DeepLog model. In Section 3.1, we've shown that DeepLog requires several input parameters, in particular, it needs the length of history sequence window h (for training and detection), and the number of top log keys in the predicted output probability distribution function to be considered normal.</p>
<p>Se ing a proper value for h and is problem dependent. Generally speaking, larger h values will increase the prediction accuracy because more history information is utilized in LSTM, until it reaches a point where keys that are far back in history do not contribute to the prediction of keys to appear. At this point continuing to increase h does not hurt the prediction accuracy of LSTM, because LSTM is able to learn that only the recent history in a long sequence ma ers thus ignore the long tail. However, a large h value does have a performance impact. More computations (and layers) are required for both training and prediction, which slows down the performance of DeepLog. e value of , on the other hand, regulates the trade-o between true positives (anomaly detection rate) and false positives (false alarm rate). e work ow model provides a guidance to set a proper value for both h and . Intuitively, h needs to be just large enough to incorporate necessary dependencies for making a good prediction, so we can set h as the length of the shortest work ow. e number of possible execution paths represents a good value for , hence, we set as the maximum number of branches at all divergence points from the work ows of all tasks.</p>
<p>4.4.2 Using workflow to diagnose detected anomalies. Whenever an anomaly is detected by DeepLog, the work ow model can be used to help diagnose this anomaly and understand how and why it has happened. Figure 5 shows an example. Using a history sequence [26,37,38], the top prediction from DeepLog is log key 39 (suppose = 1), however the actual log key appeared is 67, which is an anomaly. With the help of a work ow model for this 26   task, users could easily identify the current execution point in the corresponding work ow, and further discover that this error happened right a er "Instance destroyed successfully" and before "Deleting instance les *", which means that this error occurred during cleanup a er destroying a VM.</p>
<p>Discussion</p>
<p>Previous works [4,21,42] focused on constructing work ows from multiple executions of just one task. e basic idea in their approach follows 3 steps: 1) mine temporal dependencies of each pair of log keys; 2) construct a basic work ow from the pairwise invariants identi ed in step 1; 3) re ne work ow model using the input log key sequence. A major limitation is that they are not able to work with a log sequence that contains multiple tasks or concurrent threads in one task, which is addressed by our study. Our task separation methodology also provides useful insights towards the work ow construction for each task.</p>
<p>EVALUATION</p>
<p>DeepLog is implemented using Keras [6] with TensorFlow [2] as the backend. In this section, we show evaluations of each component and the overall performance of DeepLog, to show its e ectiveness in nding anomalies from large system log data.</p>
<p>Execution path anomaly detection</p>
<p>is section focuses on evaluating the log key anomaly detection model in DeepLog. We rst compare its e ectiveness on large system logs with previous methods, and then investigate the impact of di erent parameters in DeepLog.</p>
<p>Previous methods.</p>
<p>Previous work on general-purpose log anomaly detection follows a similar procedure: they rst extract a log key from each log message, and then perform anomaly detection on the log key sequence.</p>
<p>e Principal Component Analysis (PCA) method [39] assumes that there are di erent "sessions" in a log le that can be easily identi ed by a session id a ached to each log entry. It rst groups log keys by session and then counts the number of appearances of each log key value inside each session. A session vector is of size n, representing the number of appearances for each log key in K in that session. A matrix is formed where each column is a log key, and each row is one session vector. PCA detects an abnormal vector (a session) by measuring the projection length on the residual subspace of transformed coordinate system. is approach is shown to be more e ective than its online counterpart Session F2: Insights from Log(in)s CCS'17, October 30-November 3, 2017, Dallas, TX, USA online PCA [38] especially in reducing false positives, but this is clearly an o ine method and cannot be used for online anomaly detection. e implementation is open-sourced by [17]. Invariant Mining (IM) [22] constructs the same matrix as the PCA approach does. IM rst mines small invariants that could be satis ed by the majority of vectors, and then treats those vectors that do not satisfy these invariants as abnormal execution sessions. is approach is shown to be more e ective than an earlier work [11] which utilizes work ow automata. e implementation is opensourced by [17].</p>
<p>TFIDF is developed in [44]. Although its objective is for IT system failure prediction, which is di erent from anomaly detection as shown in [39]. Nevertheless, we still included this method in our evaluation as it also uses a LSTM-based approach. ere are several key di erences. TFIDF groups log keys by time windows (each time window is de ned by a user parameter), and then models each time window (called "epoch") using a TF-IDF (term-frequency, inverse document frequency) vector. e Laplace smoothing procedure it uses requires the knowledge of the total number of epochs (hence the entire log le). TFIDF constructs a LSTM model as a binary classi er, which needs both labeled normal and abnormal data for training. Not only are anomaly log entries hard to obtain, but also, new types of anomalies that are not included in training data may not be detected. In contrast, DeepLog trains its LSTM model to be a multi-class classi er, and only requires normal data to train.</p>
<p>CloudSeer is a method designed speci cally for multi-user Open-Stack log [42]. It builds a work ow model for each OpenStack VMrelated task and uses the work ow for anomaly detection. ough it achieves acceptable performance on OpenStack logs (a precision of 83.08% and a recall of 90.00% as reported in the paper), this method does not work for other types of logs (e.g., HDFS log) where the pa erns of log keys are much more irregular. For example, Cloud-Seer only models log keys that "appear the same number of times" in every session. In HDFS logs, only 3 out of 29 log keys satisfy this criterion. Furthermore, this method cannot separate log entries for di erent tasks in one log into separate sequences. It relies on multiple identi ers to achieve this, which is not always possible for general-purpose logs. us it is not compared against here.</p>
<p>Log data sets and set up.</p>
<p>HDFS log data set. It is generated through running Hadoop-based map-reduce jobs on more than 200 Amazon's EC2 nodes, and labeled by Hadoop domain experts. Among 11,197, 954 log entries being collected, about 2.9% are abnormal, including events such as "write exception". is was the main data set rstly used by an o ine PCA-based [39] method, and subsequently used by several other work including online PCA [38] and IM-based [22] methods. Details of this dataset could be found in [38,39].</p>
<p>OpenStack log data set. We deployed an OpenStack experiment (version Mitaka) on CloudLab [30] with one control node, one network node and eight compute nodes. Among 1, 335, 318 log entries collected, about 7% are abnormal. A script was running to constantly execute VM-related tasks, including VM creation/deletion, stop/start, pause/unpause and suspend/resume. VM tasks were scheduled with the pa ern of a regular expression (Create (Stop Start) {0,3} (Pause Unpause) {0,3} (Suspend Resume) {0,3} Delete)+. A VM life cycle starts with "VM create" and ends with "VM delete", while task pairs such as "Stop-Start", "Pause-Unpause" and "Suspend-Resume" may randomly appear from 0 to 3 times within a life cycle. INFO level logs from nova-api, nova-scheduler and nova-compute were collected and forwarded for analysis using Elastic Stack [33]. ree types of anomalies were injected at di erent execution points: 1) neutron timeout during VM creation; 2) libvirt error while destroying a VM; 3) libvirt error during cleanup a er destroying a VM.</p>
<p>Set up. To execute PCA-based and IM-based methods, we group log entries into di erent sessions by an identi er eld, which for HDFS log is block_id and for OpenStack log is instance_id. Each session group is a life cycle of one block or a VM instance respectively. We then parse each log entry into a log key. DeepLog can be applied directly on log keys to train its weights and subsequently be used to detect anomalies, while other methods require one more step. ey need to count the number of appearances for each distinct log key within each session, and build a matrix where each column is a distinct log key (so there will be n columns) and each row represents a session vector, and the value of a cell V i j in the matrix represents the count of log key k j in the i-th session.</p>
<p>DeepLog needs a small fraction of normal log entries to train its model. In the case of HDFS log, only less than 1% of normal sessions (4,855 sessions parsed from the rst 100,000 log entries compared to a total of 11,197,954) are used for training. Note that DeepLog can pinpoint which log entry (with its corresponding log key) is abnormal, but in order to use the same measures to compare with competing methods, we use "session" as the granularity of anomaly detection, i.e., a session C is considered an abnormal session as long as there exists at least one log key from C being detected abnormal. Table 3 summarizes the two data sets. Note that PCA and IM are unsupervised o ine methods that do not require training data, whereas DeepLog only needs a training data produced by normal system execution, and TFIDF requires both normal and abnormal data to train.  Table 3: Set up of log data sets (unit: session).</p>
<p>In addition to the number of false positives (FP) and false negatives (FN), we also use standard metrics such as Precision, Recall and F-measure. Precision= TP TP+FP (TP stands for true positive) shows the percentage of true anomalies among all anomalies detected; Recall= TP TP+FN measures the percentage of anomalies in the data set (assume that we know the ground-truth) being detected; and F-measure= 2·Precision·Recall</p>
<p>Precision+Recall is the harmonic mean of the two. By default, we use the following parameter values for DeepLog: = 9, h = 10, L = 2, and α = 64 and investigate their impacts in our experiments. Recall decides the cuto in the prediction output to be considered normal (i.e., the log key values with topprobabilities to appear next are considered normal), and h is the window size used for training and detection.  their parameter space and report their best results. When the Ngram method is used, we set N = 1 unless otherwise speci ed since this shows the best performance for the N-gram method. 5.1.3 Comparison. Table 4 shows the number of false positives and false negatives for each method on HDFS data. PCA achieves the fewest false positives, but at the price of more false negatives. Figure 6a shows a more in-depth comparison using recall, precision and F-measure. Note that TFIDF is omi ed from this gure because of limited space and its very poor relative performance.</p>
<p>Clearly, DeepLog has achieved the best overall performance, with an F-measure of 96%. Our baseline solution N-gram also achieves good performance when history length is 1. But, its performance drops dramatically as history window becomes longer. In contrast, LSTM-based approach is more stable as shown in Section 5.1.4. Figure 6b investigates the top-approach used by DeepLog's prediction algorithm. Let D t be the set of top-log key values predicted by DeepLog at t, and m t be the actual log key value appeared in the data at t. To see the impact of this strategy, we study the CDF of Pr[m t ∈ D t ] for di erent values. Among over 11,000,000 log keys (that are labeled as normal) to predict, 88.9% of DeepLog's top prediction matches m t exactly; and 96.1% m t 's are within DeepLog's top 2 predictions. When = 5, 99.8% of normal m t 's are within D t , meanwhile the anomaly detection rate is 99.994% (only one anomalous session was undetected). Figure 7a shows the performance over OpenStack data set. e PCA approach shows reasonable performance on this data set but with low precision (only 77%), whereas even though IM has achieved a perfect recall in this case, it has very poor precision of only 2% (almost all VM instances are detected as abnormal executions).</p>
<p>is is because that OpenStack logs were generated randomly as described in Section 5.1.2. Note that how many times that log keys like (Stop Start) may appear in a life cycle of a VM (de ned by a pair of Create and Delete) is uncertain. is makes it really hard for IM to nd the "stable small invariants" for anomaly detection.</p>
<p>To test this hypothesis, we generated a second data set with a deterministic pa ern like (Create Delete)+, resulting in a total of 5,544 normal VM executions and 170 anomalous ones. We denote this data set as OpenStack II and the result is shown in Figure 7b. IM performs very well on this data set with more regular pa erns. However the recall for the PCA method drops to only 2% in this case because the normal pa ern in the data is too regular, rendering PCA method which detects anomalies by variance not working.</p>
<p>On the other hand, DeepLog demonstrates excellent performance on both OpenStack logs with a F-measure of 98% and 97% respectively. Lastly, it is also important to note that PCA and IM are o ine methods, and they cannot be used to perform anomaly detection per log entry. ey are only able to detect anomaly at session level, but the notion of session may not even exist in many system logs.</p>
<p>5.1.4 Analysis of DeepLog. We investigate the performance impact of various parameters in DeepLog including: , h, L, and α. e results are shown in Figure 8. In each experiment, we varied the values of one parameter while using the default values for others.  In general, the performance of DeepLog is fairly stable with respect to di erent values, i.e., it is not very sensitive to the adjustment of any one or combinations of these parameter values. is makes DeepLog easy to deploy and use in practice. e results are fairly intuitive to understand as well. For example, Figure 8c shows that a larger value leads to higher precision but lower recall. us, could be adjusted to achieve higher true positive rate or lower false positive rate. Lastly, DeepLog's prediction cost per log entry is only around 1 millisecond on our standard workstation, which could be further improved by be er hardware such as using a GPU.  </p>
<p>Parameter value and performance anomaly</p>
<p>To evaluate the e ectiveness of DeepLog at detecting parameter value and performance (including elapsed time between log entries) Session F2: Insights from Log(in)s CCS'17, October 30-November 3, 2017, Dallas, TX, USA anomalies, we used system logs from the OpenStack VM creation task. is data set includes both types of anomalies: performance anomaly (late arrival of a log entry) and parameter value anomaly (a log entry with a much longer VM creation time than others).</p>
<p>Experiment setup. As before, we deployed an OpenStack experiment on CloudLab, and wrote a script to simulate that multiple users are constantly requesting VM creations and deletions. During OpenStack VM creation, an important procedure is to copy the required image from controller node to a compute node (where the VM will be created). To simulate a performance anomaly which could be possibly caused by a DoS a ack, we thro le the network speed from the controller to compute nodes at two di erent points, to see if these anomalies could be detected by DeepLog.</p>
<p>Anomaly detection. As described in Section 3.2, we separate log entries into two sets, one set is for model training and the other set (called the validation set) is to apply the model to generate the Gaussian distribution of MSEs (mean square error). In subsequent online detection phase, for every incoming parameter value vector − → , DeepLog checks if the MSE between − → and the predication output (a vector as well) from its model is within an acceptable con dence interval of the Gaussian distribution of MSEs from the validation set. Figure 9 shows the detection results for the parameter value vectors of di erent log keys, where x-axis represents the id of the VM being created (i.e., di erent VM creation instances), and -axis represents the MSE between the parameter value vector and the prediction output vector from DeepLog. e horizontal lines in each gure are the con dence interval thresholds for the corresponding MSE Gaussian distributions. Figure 9a and 9b represent two log keys where their parameter value vectors are normal during the entire time. Figure 9c and 9d illustrate that the parameter value vectors for keys 53 and 56 are successfully detected as being abnormal at exactly the two time instances where we thro led the network speed (i.e., injected anomalies). For each abnormal parameter value vector detected, we identied the value that di ers the most with the prediction, to identify the abnormal column (feature). We found out that the two abnormal parameter value vectors for key 53 are due to unusually large elapsed time values. On the other hand, key 56 is "Took * seconds to build instance.", and not surprisingly, its two abnormal parameter value vectors were caused by unusually large values (for seconds).</p>
<p>Online update and training of DeepLog</p>
<p>Section 5.1 has demonstrated that DeepLog requires a very small training set (less than 1% of the entire log) and does not require user feedback during its training phase. But it is possible that a new system execution path may show up during detection stage, which is also normal, but is detected as anomalous since it was not re ected by the training data. To address this issue, this section evaluates the e ectiveness of DeepLog's online update and training module as described in Section 3.3. We demonstrate this using the di erence in detection results with and without incremental updates, in terms of both e ectiveness and e ciency. entries, of which 348,460 entries are labeled as anomalies. We chose this data set because of an important characteristic: many log keys only appeared during a speci c time period. is means that the training data set may not contain all possible normal log keys, let alone all possible normal execution pa erns.</p>
<p>5.3.2 Evaluation results. We conducted two experiments, one uses the rst 1% normal log entries as training data and the other uses the rst 10% log entries for training. In both se ings, the remaining 99% or 90% entries are used for anomaly detection. We set L = 1, α = 256, = 6, h = 3.</p>
<p>Precision</p>
<p>Recall F-measure (a) First 1% dataset for training.   Figure 10 shows the results for without and with online training for both experiments. In the case of "without online training", we run DeepLog to test incoming log entries without any incremental update. While for the case of "with online training", we assume there is an end user who reports if a detected anomaly is a false positive. If so, DeepLog uses that sample (now a labeled record) to update its model to learn this new pa ern. Figure 10 shows that without online training, with only 1% o ine training data, this results in many false positives (hence very low Precision rate).</p>
<p>ough increasing its training data to 10% slightly increases the Precision, its performance is still unsatisfactory. On the other hand, DeepLog with online training signi cantly improves its Precision, and hence F-measure scores. With a true positive rate of 100% (perfect recall) in both se ings, online training reduces false positive rate from 40.1% to 1.7% for 1% training data, and from 38.2% to 1.1% for 10% training data, respectively. Table 5 shows the amortized cost to check each log entry. For the online training case, we reported time taken for both detection and online update (if an update is triggered). e results show that online update and training does increase the amortized cost per log entry, but only slightly. is is because many log entries will not trigger an update. Note that online update and online detection can be executed in parallel; an update is carried out while the model is using the current weights to continue performing detection. </p>
<p>Security log case studies</p>
<p>Anomalies having log keys that never showed up in normal logs used for training (e.g., "ERROR" or "exception" log messages) are easy to detect. DeepLog can e ectively detect much more subtle cases. For example, in HDFS log, "Namenode not updated a er deleting block" anomaly is shown as a missing log key in a session; and "Redundant addStoredBlock" anomaly is shown as an extra log key. is means that for any a ack that may cause any change of system behavior (as re ected through logs), it can be detected. In what follows, we investigate system logs containing real a acks to demonstrate the e ectiveness of DeepLog.</p>
<p>Network security log.</p>
<p>Network security is of vital importance. Both rewall and intrusion detection system (IDS) produce logs that can be used for online anomaly detection.</p>
<p>To test the performance of DeepLog on network security logs, we used the VAST Challenge 2011 data set, speci cally, Mini Challenge 2 -Computer Networking Operations [1].</p>
<p>is challenge is to manually look for suspicious activities by visualization techniques. It comes with ground truth for anomalous activities. For all anomalies in the ground truth, Table 6 shows the results of DeepLog. e only suspicious activity not being detected is the rst appearance of an undocumented computer IP address. Yes, log key anomaly in IDS log Day 2: port scan 2</p>
<p>Yes, log key anomaly in IDS log Day 2: socially engineered a ack Yes, log key anomaly in rewall log Day 3: undocumented IP address No e only false positive case happened when DeepLog reported a log message that repeatedly appeared many times in a short period as an anomaly. is is due to an event that suddenly became bursty and printed the same log message many times in a short time range.</p>
<p>is is not identi ed by the VAST Challenge as a suspicious activity.</p>
<p>BROP a ack detection. Blind Return</p>
<p>Oriented Programming (BROP) a ack [5] leverages a fact that many server applications restart a er a crash to ensure service reliability. is kind of a ack is powerful and practical because the a acker neither relies on access to source code nor binary. A stack bu er over ow vulnerability, which leads server to crash, is su cient to carry out this a ack. In a BROP exploit, the a acker uses server crash as a signal to help complete a ROP a ack which achieves executing a shellcode. However, the repeated server restarting activities leave many atypical log messages in kernel log as shown below, which is easily detected by DeepLog. </p>
<p>Task separation and work ow construction</p>
<p>We implemented the proposed methods in Section 4 and evaluated on a log with various OpenStack VM-related tasks. Both LSTM approach and density-based clustering approach could successfully separate all tasks. e rst method requires LSTM; it is a supervised method which requires training data to be provided. e second method uses clustering on co-occurrences of log keys within a certain distance threshold, which is an unsupervised method. Hence, it doesn't require training, but it does require parameter τ as the distance threshold.</p>
<p>Speci cally, for density-based clustering approach, with a sufciently large threshold value τ ∈ [0.85, 0.95], there is a clear separation of all tasks. Note that the value of τ cannot be too large (e.g., se ing τ = 1), as a background process may produce log entries at random locations that will break log entries from the same task apart.</p>
<p>Next we use a part of the VM creation work ow, to show how it provides useful diagnosis of the performance anomaly in Section 5.2. Recall in Section 5.2, parameter value vector anomaly is identi ed on the time elapsed value of log key 53, and on the parameter position of log key 56 (which represents how many seconds to build instance). As shown in Figure 11, once an anomaly is detected by DeepLog, we know the time taken to build that instance is abnormal, but we don't know why. en, since the elapsed time between log key 53 and its previous log key is too big, by investigating the work ow model constructed by DeepLog, its previous key is 52: "Creating image", so we know that VM creation took longer time than usual because the time to create image was too long. Further investigation following this procedure may reveal that it was caused by slow network speed from control node to compute node.</p>
<p>RELATED WORK</p>
<p>Primarily designed for recording notable events to ease debugging, system event logs are abundantly informative and exist practically on every computer system, making them a valuable resource to track and investigate system status. However, since system logs are largely composed of diverse, freeform text, analytics is challenging.</p>
<p>Numerous log mining tools have been designed for di erent systems. Many use rule-based approaches [7,15,28,29,31,32,40,41], which, though accurate, are limited to speci c application scenarios and also require domain expertise. For example, Beehive [41]   identi es potential security threats from logs by unsupervised clustering of data-speci c features, and then manually labeling outliers. Oprea [28] uses belief propagation to detect early-stage enterprise infection from DNS logs. PerfAugur [32] is designed speci cally to nd performance problems by mining service logs using specialized features such as predicate combinations. DeepLog is a general approach that does not rely on any domain-speci c knowledge.</p>
<p>Other generic methods that use system logs for anomaly detection typically apply a two-step procedure. First, a log parser [9,14,16,23,36,37] is used to parse log entries to structured forms, which typically only contain "log keys" (or "message types"). Parameter values and timestamps are discarded except for identi ers which are used to separate and group log entries. en, anomaly detection is performed on log key sequences. A typical way is to generate a numeric vector for each session or time window, by counting unique log keys or using more sophisticated approaches like TF-IDF. e matrix comprising of these vectors is then amenable to matrixbased unsupervised anomaly detection methods such as Principal Component Analysis (PCA) [38,39] and invariant mining (IM) [22]. Constructing such a matrix is o en an o ine process, and these methods are not able to provide log-entry level anomaly detection (rather, they can only operate at session level). We refer the reader to [17] for an overview and comparison on these methods.</p>
<p>Supervised methods [17,44] use normal and abnormal vectors to train a binary classi er that detects future anomalies. A downside of such methods is that unknown anomalies not in training data may not be detected. Furthermore, anomalous data are hard to obtain for training. We have shown in our evaluation that using only a small portion of normal data to train, DeepLog can achieve online anomaly detection with be er performance. Moreover, DeepLog also uses timestamps and parameter values for anomaly detection which are missing in previous work.</p>
<p>Work ow construction has been studied largely using log keys extracted from o ine log les [4,11,21,42] . It has been shown that work ow o ers limited advantage for anomaly detection [11,42]. Instead, a major utility of work ows is to aid system diagnosis [4,21]. However, all past work assumes a log le to model only contains repeated executions of one single task. In this paper, we propose methods to automatically separate di erent tasks from log les in order to build work ow models for di erent tasks. Besides work ows, other systems that perform anomaly diagnosis using system logs include DISTALYZER [26] that diagnoses system performance issues by comparing a problematic log against a normal one, LogCluster [19] which clusters and organizes historical logs to help future problem identi cation, and Stitch [45] that extracts di erent levels of identi ers from system logs and builds a web interface for users to visually monitor the progress of each session and locate performance problems. Note that they are for diagnosis purposes once an anomaly has been detected, and cannot be used for anomaly detection itself.</p>
<p>CONCLUSION</p>
<p>is paper presents DeepLog, a general-purpose framework for online log anomaly detection and diagnosis using a deep neural network based approach. DeepLog learns and encodes entire log message including timestamp, log key, and parameter values. It performs anomaly detection at per log entry level, rather than at per session level as many previous methods are limited to. DeepLog can separate out di erent tasks from a log le and construct a workow model for each task using both deep learning (LSTM) and classic mining (density clustering) approaches. is enables e ective anomaly diagnosis. By incorporating user feedback, DeepLog supports online update/training to its LSTM models, hence is able to incorporate and adapt to new execution pa erns. Extensive evaluation on large system logs have clearly demonstrated the superior e ectiveness of DeepLog compared with previous methods.</p>
<p>Future work include but are not limited to incorporating other types of RNNs (recurrent neural networks) into DeepLog to test their e ciency, and integrating log data from di erent applications and systems to perform more comprehensive system diagnosis (e.g., failure of a MySQL database may be caused by a disk failure as re ected in a separate system log).</p>
<p>ACKNOWLEDGMENT</p>
<p>e authors appreciate the valuable comments provided by the anonymous reviewers. Authors thank the support from NSF grants 1314945 and 1514520. Feifei Li is also supported in part by NSFC grant 61729202. We wish to thank all members of the TCloud project and the Flux group for helpful discussion and feedback, especially Cai (Richard) Li, for his valuable input on BROP a ack.</p>
<p>-Nov. 3, 2017, Dallas, TX, USA. © 2017 ACM. ISBN 978-1-4503-4946-8/17/10. . . $15.00 DOI: h p://dx.doi.org/10.1145/3133956.3134015 challenging and many traditional anomaly detection methods based on standard mining methodologies are no longer e ective.</p>
<p>Figure 3 :
3A detailed view of log key anomaly detection model using stacked LSTM.</p>
<p>Figure 5 :
5Anomaly diagnosis using work ow.</p>
<p>L and α denote the number of layers in DeepLog and the number of memory units in one LSTM block respectively. For all other methods,</p>
<p>Figure 6 :Figure 7 :
67Evaluation Evaluation on OpenStack log.</p>
<p>Figure 8 :
8DeepLog performance with di erent parameters.</p>
<p>Figure 9 :
95.3.1 Log data set. e log data set used in this section is Blue Gene/L supercomputer system logs 1 , which contains 4,747,963 log 1 CFDR Data, h ps://www.usenix.org/cfdr-data Anomaly detection for parameter value vectors with different con dence intervals (CIs).</p>
<p>Figure 10 :
10Evaluation on Blue Gene/L log.</p>
<p>nginx[<em>]: segfault at * ip * sp * error * in nginx[</em>] nginx[<em>]: segfault at * ip * sp * error * in nginx[</em>] nginx[<em>]: segfault at * ip * sp * error * in nginx[</em>] ......</p>
<p>Figure 11 :
11OpenStack VM Creation work ow.</p>
<p>Table 1 :
1Log entries from OpenStack VM deletion task.Training Stage 
D etection Stage </p>
<p>Log Key 
Anomaly Detection 
model </p>
<p>...... </p>
<p>Parameter Value 
Anomaly Detection 
model 
for each log key </p>
<p>Workflows </p>
<p>normal execution 
log file </p>
<p>Log 
Parser 
...... </p>
<p>...... </p>
<p>...... </p>
<p>...... </p>
<p>each log entry = log key + parameter value vector 
A new log entry 
Log 
Parser </p>
<p>parameter 
value vector </p>
<ul>
<li></li>
</ul>
<p>Train model 
Construct workflow </p>
<p>Anomaly? </p>
<p>Yes 
No, 
check 
vector </p>
<p>Anomaly? </p>
<p>Yes </p>
<p>Diagnosis </p>
<p>Update 
model if 
false positive </p>
<p>log key </p>
<p>Train models </p>
<p>No </p>
<p>. Compared to a N-gramLSTM 
block 
Output of last state 
is forwarded as 
current input state </p>
<p>LSTM 
block </p>
<p>LSTM 
block </p>
<p>DeepLog </p>
<p>Output </p>
<p>Input </p>
<p>LSTM 
block </p>
<p>LSTM 
block </p>
<p>LSTM 
block </p>
<p>Roll out </p>
<p>Stack up </p>
<p>LSTM 
block </p>
<p>LSTM 
block </p>
<p>LSTM 
block </p>
<p>LSTM 
block </p>
<p>or more dimensions in this time series; the elapsed time value is just one such dimension.Session F2: Insights from Log(in)s 
CCS'17, October 30-November 3, 2017, Dallas, TX, USA </p>
<p>Table 2 :
2Co-occurrence matrix within distance d</p>
<p>has more than 2 log keys, when checking if k x could be included as the new head or tail, we need to check if k x has a co-occurrence probability greater than τ with each log key in T up to distance d , where d is the smaller of: i) length of T , and ii) the maximum value of d that we have built a co-occurrence matrix for. For example, to check if T = [k 1 , k 2 , k 3 ] should connect k 4 at its tail, we need to check if min(p 1 (Session F2: Insights from Log(in)s 
CCS'17, October 30-November 3, 2017, Dallas, TX, USA </p>
<p>instance: * Took * seconds to destroy the instance on the hypervisor 67: instance: * Error from libvirt during unfilter. Code=<em> Error=</em>37 
38 
39 
40 
41 </p>
<p>67 </p>
<p>Actual Execution </p>
<p>Prediction (Correct Path) 
37: instance: * Terminating instance 
38: instance: * Instance destroyed successfully 
39: instance: * Deleting instance files * 
40: instance: * Deletion of * complete 
41: </p>
<p>Table 4 :
4Number of FPs and FNs on HDFS log.</p>
<p>Table 5 :
5Amortized cost to check each log entrytraining data percentage 
1% 
10% 
without online training (milliseconds) 1.06 1.11 
with online training (milliseconds) 
3.48 2.46 </p>
<p>Table 6 :
6VAST Challenge 2011 network security log detection suspicious activity detected? Day 1: Denial of Service a ack Yes, log key anomaly in IDS log Day 1: port scan Yes, log key anomaly in IDS log Day 2: port scan 1</p>
<p>Session F2: Insights from Log(in)s CCS'17, October 30-November 3, 2017, Dallas, TX, USA 44: instance: * Attempting claim: memory * disk * vcpus * CPU 51: instance: * Claim successful 23: instance: * GET * HTTP\/1.1" status: * len: * time: * 52: instance: * Creating image 53: instance: * VM Started (Lifecycle Event) 32: instance: * VM Paused (Lifecycle Event) 18: instance: * VM Resumed (Lifecycle Event) ....... 56: instance: * Took * seconds to build instance44 </p>
<p>23 
52 
53 </p>
<p>32 
25 </p>
<p>51 </p>
<p>18 
54 
57 
56 </p>
<p>18 </p>
<p>MC2 -Computer Networking Operations. VAST Challenge 2011. 2011. MC2 -Computer Networking Operations. (2011). h p://hcil2.cs.umd.edu/newvarepository/VAST%20Challenge%202011/ challenges/MC2%20-%20Computer%20Networking%20Operations/ [Online; ac- cessed 08-May-2017].</p>
<p>TensorFlow: A system for large-scale machine learning. Martín Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Je Rey Dean, Ma Hieu Devin, Sanjay Ghemawat, Geo Rey Irving, Michael Isard, Proc. USENIX Symposium on Operating Systems Design and Implementation (OSDI. USENIX Symposium on Operating Systems Design and Implementation (OSDIMartín Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Je rey Dean, Ma hieu Devin, Sanjay Ghemawat, Geo rey Irving, Michael Isard, and others. 2016. TensorFlow: A system for large-scale machine learning. In Proc. USENIX Symposium on Operating Systems Design and Implementation (OSDI). 264-285.</p>
<p>A neural probabilistic language model. Yoshua Bengio, Réjean Ducharme, Pascal Vincent, Christian Jauvin, Journal of machine learning research. 3Yoshua Bengio, Réjean Ducharme, Pascal Vincent, and Christian Jauvin. 2003. A neural probabilistic language model. Journal of machine learning research 3, Feb (2003), 1137-1155.</p>
<p>Inferring models of concurrent systems from logs of their behavior with CSight. Ivan Beschastnikh, Yuriy Brun, D Michael, Arvind Ernst, Krishnamurthy, Proc. International Conference on So ware Engineering (ICSE. International Conference on So ware Engineering (ICSEIvan Beschastnikh, Yuriy Brun, Michael D Ernst, and Arvind Krishnamurthy. 2014. Inferring models of concurrent systems from logs of their behavior with CSight. In Proc. International Conference on So ware Engineering (ICSE ). 468-479.</p>
<p>Hacking blind. Andrea Bi Au, Adam Belay, Ali Mashtizadeh, David Mazières, Dan Boneh, Security and Privacy (SP). Andrea Bi au, Adam Belay, Ali Mashtizadeh, David Mazières, and Dan Boneh. 2014. Hacking blind. In Security and Privacy (SP), 2014 IEEE Symposium on. IEEE, 227-242.</p>
<p>. Franois Chollet, Online; accessed 08Franois Chollet. 2015. keras. h ps://github.com/fchollet/keras. (2015). [Online; accessed 08-May-2017].</p>
<p>Event logs for the analysis of so ware failures: A rule-based approach. Marcello Cinque, Domenico Cotroneo, Antonio Pecchia, IEEE Transactions on So ware Engineering. TSEMarcello Cinque, Domenico Cotroneo, and Antonio Pecchia. 2013. Event logs for the analysis of so ware failures: A rule-based approach. IEEE Transactions on So ware Engineering (TSE) (2013), 806-821.</p>
<p>Session F2: Insights from Log(in)s CCS'17. Dallas, TX, USASession F2: Insights from Log(in)s CCS'17, October 30-November 3, 2017, Dallas, TX, USA</p>
<p>Semi-supervised sequence learning. M Andrew, Dai, Le, Proc. Neural Information Processing Systems Conference (NIPS. Neural Information essing Systems Conference (NIPSAndrew M Dai and oc V Le. 2015. Semi-supervised sequence learning. In Proc. Neural Information Processing Systems Conference (NIPS). 3079-3087.</p>
<p>Spell: Streaming Parsing of System Event Logs. Min Du, Feifei Li, Proc. IEEE International Conference on Data Mining (ICDM. IEEE International Conference on Data Mining (ICDMMin Du and Feifei Li. 2016. Spell: Streaming Parsing of System Event Logs. In Proc. IEEE International Conference on Data Mining (ICDM). 859-864.</p>
<p>ATOM: E cient Tracking, Monitoring, and Orchestration of Cloud Resources. Min Du, Feifei Li, Min Du and Feifei Li. 2017. ATOM: E cient Tracking, Monitoring, and Orches- tration of Cloud Resources. IEEE Transactions on Parallel and Distributed Systems (2017).</p>
<p>Execution anomaly detection in distributed systems through unstructured log analysis. Qiang Fu, Jian-Guang Lou, Yi Wang, Jiang Li, Proc. IEEE International Conference on Data Mining (ICDM. IEEE International Conference on Data Mining (ICDMQiang Fu, Jian-Guang Lou, Yi Wang, and Jiang Li. 2009. Execution anomaly detection in distributed systems through unstructured log analysis. In Proc. IEEE International Conference on Data Mining (ICDM). 149-158.</p>
<p>A primer on neural network models for natural language processing. Yoav Goldberg, Journal of Arti cial Intelligence Research. 57Yoav Goldberg. 2016. A primer on neural network models for natural language processing. Journal of Arti cial Intelligence Research 57 (2016), 345-420.</p>
<p>Deep Learning. Ian Goodfellow, Yoshua Bengio, Aaron Courville, MIT PressIan Goodfellow, Yoshua Bengio, and Aaron Courville. 2016. Deep Learning. MIT Press. h p://www.deeplearningbook.org.</p>
<p>LogMine: Fast Pa ern Recognition for Log Analytics. Hossein Hamooni, Biplob Debnath, Jianwu Xu, Hui Zhang, Guofei Jiang, Abdullah Mueen, Proc. Conference on Information and Knowledge Management (CIKM. Conference on Information and Knowledge Management (CIKMHossein Hamooni, Biplob Debnath, Jianwu Xu, Hui Zhang, Guofei Jiang, and Abdullah Mueen. 2016. LogMine: Fast Pa ern Recognition for Log Analytics. In Proc. Conference on Information and Knowledge Management (CIKM). 1573-1582.</p>
<p>Automated System Monitoring and Noti cation with Swatch. E Stephen, LISAE Todd Hansen, LISAAtkins, LISAProc. Large Installation System Administration Conference. Large Installation System Administration ConferenceStephen E Hansen and E Todd Atkins. 1993. Automated System Monitoring and Noti cation with Swatch.. In Proc. Large Installation System Administration Conference (LISA). 145-152.</p>
<p>An evaluation study on log parsing and its use in log mining. Pinjia He, Jieming Zhu, Shilin He, Jian Li, Michael R Lyu, Proc. International Conference on Dependable Systems and Networks (DSN). International Conference on Dependable Systems and Networks (DSN)Pinjia He, Jieming Zhu, Shilin He, Jian Li, and Michael R Lyu. 2016. An evaluation study on log parsing and its use in log mining. In Proc. International Conference on Dependable Systems and Networks (DSN). 654-661.</p>
<p>Experience Report: System Log Analysis for Anomaly Detection. Shilin He, Jieming Zhu, Pinjia He, Michael R Lyu, Proc. International Symposium on So ware Reliability Engineering. International Symposium on So ware Reliability EngineeringShilin He, Jieming Zhu, Pinjia He, and Michael R Lyu. 2016. Experience Report: System Log Analysis for Anomaly Detection. In Proc. International Symposium on So ware Reliability Engineering (ISSRE). 207-218.</p>
<p>Long short-term memory. Sepp Hochreiter, Jürgen Schmidhuber, Neural computation. Sepp Hochreiter and Jürgen Schmidhuber. 1997. Long short-term memory. Neural computation (1997), 1735-1780.</p>
<p>Log clustering based problem identi cation for online service systems. Qingwei Lin, Hongyu Zhang, Jian-Guang Lou, Yu Zhang, Xuewei Chen, Proc. International Conference on So ware Engineering (ICSE. International Conference on So ware Engineering (ICSEQingwei Lin, Hongyu Zhang, Jian-Guang Lou, Yu Zhang, and Xuewei Chen. 2016. Log clustering based problem identi cation for online service systems. In Proc. International Conference on So ware Engineering (ICSE ). 102-111.</p>
<p>Augmented LSTM Framework to Construct Medical Self-diagnosis Android. Chaochun Liu, Huan Sun, Nan Du, Shulong Tan, Hongliang Fei, Wei Fan, Tao Yang, Hao Wu, Yaliang Li, Chenwei Zhang, Proc. IEEE International Conference on Data Mining (ICDM. IEEE International Conference on Data Mining (ICDMChaochun Liu, Huan Sun, Nan Du, Shulong Tan, Hongliang Fei, Wei Fan, Tao Yang, Hao Wu, Yaliang Li, and Chenwei Zhang. 2016. Augmented LSTM Frame- work to Construct Medical Self-diagnosis Android. In Proc. IEEE International Conference on Data Mining (ICDM). 251-260.</p>
<p>Mining program work ow from interleaved traces. Jian-Guang Lou, Qiang Fu, Shengqi Yang, Jiang Li, Bin Wu, Proc. ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (SIGKDD). ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (SIGKDD)Jian-Guang Lou, Qiang Fu, Shengqi Yang, Jiang Li, and Bin Wu. 2010. Mining program work ow from interleaved traces. In Proc. ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (SIGKDD).</p>
<p>Mining Invariants from Console Logs for System Problem Detection. Jian-Guang Lou, Qiang Fu, Shengqi Yang, Ye Xu, Jiang Li, Proc. USENIX Annual Technical Conference. USENIX Annual Technical ConferenceATCJian-Guang Lou, Qiang Fu, Shengqi Yang, Ye Xu, and Jiang Li. 2010. Mining Invariants from Console Logs for System Problem Detection.. In Proc. USENIX Annual Technical Conference (ATC). 231-244.</p>
<p>Clustering event logs using iterative partitioning. A O Adetokunbo, Makanju, Evangelos E Nur Zincir-Heywood, Milios, Proc. ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (SIGKDD. ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (SIGKDDAdetokunbo AO Makanju, A Nur Zincir-Heywood, and Evangelos E Milios. 2009. Clustering event logs using iterative partitioning. In Proc. ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (SIGKDD). 1255-1264.</p>
<p>Foundations of statistical natural language processing. D Christopher, Hinrich Manning, Schütze, MIT PressChristopher D Manning and Hinrich Schütze. 1999. Foundations of statistical natural language processing. MIT Press.</p>
<p>Recurrent neural network based language model. Tomas Mikolov, Martin Kara Át, Lukas Burget, Jan Cernockỳ, Sanjeev Khudanpur, Interspeech. 2Tomas Mikolov, Martin Kara át, Lukas Burget, Jan Cernockỳ, and Sanjeev Khu- danpur. 2010. Recurrent neural network based language model.. In Interspeech, Vol. 2. 3.</p>
<p>Structured comparative analysis of systems logs to diagnose performance problems. Karthik Nagaraj, Charles Killian, Jennifer Neville, Proc. USENIX Symposium on Networked Systems Design and Implementation (NSDI. USENIX Symposium on Networked Systems Design and Implementation (NSDIKarthik Nagaraj, Charles Killian, and Jennifer Neville. 2012. Structured compara- tive analysis of systems logs to diagnose performance problems. In Proc. USENIX Symposium on Networked Systems Design and Implementation (NSDI). 26-26.</p>
<p>Understanding LSTM Networks. Christopher Olah, Online; accessed 16Christopher Olah. 2015. Understanding LSTM Networks. (2015). h p://colah. github.io/posts/2015-08-Understanding-LSTMs [Online; accessed 16-May-2017].</p>
<p>Detection of early-stage enterprise infection by mining large-scale log data. Alina Oprea, Zhou Li, Proc. International Conference on Dependable Systems and Networks (DSN). International Conference on Dependable Systems and Networks (DSN)Ting-Fang Yen, Sang H Chin, and Sumayah AlrwaisAlina Oprea, Zhou Li, Ting-Fang Yen, Sang H Chin, and Sumayah Alrwais. 2015. Detection of early-stage enterprise infection by mining large-scale log data. In Proc. International Conference on Dependable Systems and Networks (DSN). 45-56.</p>
<p>Analyzing cluster log les using Logsurfer. James E Prewe, Proc. Annual Conference on Linux Clusters. Annual Conference on Linux ClustersJames E Prewe . 2003. Analyzing cluster log les using Logsurfer. In Proc. Annual Conference on Linux Clusters.</p>
<p>Introducing CloudLab: Scienti c Infrastructure for Advancing Cloud Architectures and Applications. USENIX ;login: 39, 6. Robert Ricci, Eric Eide, Cloudlab Team, Robert Ricci, Eric Eide, and e CloudLab Team. 2014. Introducing CloudLab: Scienti c Infrastructure for Advancing Cloud Architectures and Applications. USENIX ;login: 39, 6 (Dec. 2014). h ps://www.usenix.org/publications/login/ dec14/ricci</p>
<p>Real-time Log File Analysis Using the Simple Event Correlator (SEC). John P Rouillard, LISAProc. Large Installation System Administration Conference. Large Installation System Administration ConferenceJohn P Rouillard. 2004. Real-time Log File Analysis Using the Simple Event Correlator (SEC).. In Proc. Large Installation System Administration Conference (LISA). 133-150.</p>
<p>Perfaugur: Robust diagnostics for performance anomalies in cloud services. Sudip Roy, Arnd Christian König, Igor Dvorkin, Manish Kumar, Proc. IEEE International Conference on Data Engineering (ICDE). IEEE International Conference on Data Engineering (ICDE)IEEESudip Roy, Arnd Christian König, Igor Dvorkin, and Manish Kumar. 2015. Per- faugur: Robust diagnostics for performance anomalies in cloud services. In Proc. IEEE International Conference on Data Engineering (ICDE). IEEE, 1167-1178.</p>
<p>Open Source Elastic Stack. 16Elastic Stack. 2017. e Open Source Elastic Stack. (2017). h ps://www.elastic. co/products [Online; accessed 16-May-2017].</p>
<p>LSTM Neural Networks for Language Modeling. Martin Sundermeyer, Ralf Schlüter, Hermann Ney, Interspeech. Martin Sundermeyer, Ralf Schlüter, and Hermann Ney. 2012. LSTM Neural Networks for Language Modeling.. In Interspeech. 194-197.</p>
<p>Sequence to sequence learning with neural networks. Ilya Sutskever, Oriol Vinyals, Le, Proc. Neural Information Processing Systems Conference (NIPS. Neural Information essing Systems Conference (NIPSIlya Sutskever, Oriol Vinyals, and oc V Le. 2014. Sequence to sequence learning with neural networks. In Proc. Neural Information Processing Systems Conference (NIPS). 3104-3112.</p>
<p>LogTree: A framework for generating system events from raw textual logs. Liang Tang, Tao Li, Proc. IEEE International Conference on Data Mining (ICDM. IEEE International Conference on Data Mining (ICDMLiang Tang and Tao Li. 2010. LogTree: A framework for generating system events from raw textual logs. In Proc. IEEE International Conference on Data Mining (ICDM). 491-500.</p>
<p>LogSig: Generating system events from raw textual logs. Liang Tang, Tao Li, Chang-Shing Perng, Proc. Conference on Information and Knowledge Management (CIKM. Conference on Information and Knowledge Management (CIKMLiang Tang, Tao Li, and Chang-Shing Perng. 2011. LogSig: Generating system events from raw textual logs. In Proc. Conference on Information and Knowledge Management (CIKM). 785-794.</p>
<p>Online system problem detection by mining pa erns of console logs. Wei Xu, Ling Huang, Armando Fox, David Pa Erson, Michael Jordan, Proc. IEEE International Conference on Data Mining (ICDM. IEEE International Conference on Data Mining (ICDMWei Xu, Ling Huang, Armando Fox, David Pa erson, and Michael Jordan. 2009. Online system problem detection by mining pa erns of console logs. In Proc. IEEE International Conference on Data Mining (ICDM). 588-597.</p>
<p>Detecting large-scale system problems by mining console logs. Wei Xu, Ling Huang, Armando Fox, David Pa, Michael I Jordan , Proc. ACM Symposium on Operating Systems Principles (SOSP. ACM Symposium on Operating Systems Principles (SOSPWei Xu, Ling Huang, Armando Fox, David Pa erson, and Michael I Jordan. 2009. Detecting large-scale system problems by mining console logs. In Proc. ACM Symposium on Operating Systems Principles (SOSP). 117-132.</p>
<p>Dynamic syslog mining for network failure monitoring. Kenji Yamanishi, Yuko Maruyama, Proc. ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (SIGKDD. ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (SIGKDDKenji Yamanishi and Yuko Maruyama. 2015. Dynamic syslog mining for network failure monitoring. In Proc. ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (SIGKDD). 499-508.</p>
<p>Beehive: Large-scale log analysis for detecting suspicious activity in enterprise networks. Alina Ting-Fang Yen, Kaan Oprea, Todd Onarlioglu, William Leetham, Ari Robertson, Engin Juels, Kirda, Proc. International Conference on Dependable Systems and Networks (ACSAC). International Conference on Dependable Systems and Networks (ACSAC)Ting-Fang Yen, Alina Oprea, Kaan Onarlioglu, Todd Leetham, William Robertson, Ari Juels, and Engin Kirda. 2013. Beehive: Large-scale log analysis for detecting suspicious activity in enterprise networks. In Proc. International Conference on Dependable Systems and Networks (ACSAC). 199-208.</p>
<p>CloudSeer: Work ow Monitoring of Cloud Infrastructures via Interleaved Logs. Xiao Yu, Pallavi Joshi, Jianwu Xu, Guoliang Jin, Hui Zhang, Guofei Jiang, Proc. ACM International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS). ACM International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS)Xiao Yu, Pallavi Joshi, Jianwu Xu, Guoliang Jin, Hui Zhang, and Guofei Jiang. 2016. CloudSeer: Work ow Monitoring of Cloud Infrastructures via Interleaved Logs. In Proc. ACM International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS). 489-502.</p>
<p>SherLog: error diagnosis by connecting clues from run-time logs. Ding Yuan, Haohui Mai, Weiwei Xiong, Lin Tan, Yuanyuan Zhou, Shankar Pasupathy, In ACM SIGARCH computer architecture news. ACMDing Yuan, Haohui Mai, Weiwei Xiong, Lin Tan, Yuanyuan Zhou, and Shankar Pasupathy. 2010. SherLog: error diagnosis by connecting clues from run-time logs. In ACM SIGARCH computer architecture news. ACM, 143-154.</p>
<p>Automated IT system failure prediction: A deep learning approach. Ke Zhang, Jianwu Xu, Martin Renqiang Min, Guofei Jiang, Konstantinos Pelechrinis, Hui Zhang, Proc. IEEE International Conference on Big Data. IEEE International Conference on Big DataIEEE BigDataKe Zhang, Jianwu Xu, Martin Renqiang Min, Guofei Jiang, Konstantinos Pelechri- nis, and Hui Zhang. 2016. Automated IT system failure prediction: A deep learn- ing approach. In Proc. IEEE International Conference on Big Data (IEEE BigData). 1291-1300.</p>
<p>Nonintrusive performance pro ling for entire so ware stacks based on the ow reconstruction principle. Xu Zhao, Kirk Rodrigues, Yu Luo, Ding Yuan, Michael Stumm, Proc. USENIX Symposium on Operating Systems Design and Implementation (OSDI. USENIX Symposium on Operating Systems Design and Implementation (OSDIXu Zhao, Kirk Rodrigues, Yu Luo, Ding Yuan, and Michael Stumm. 2016. Non- intrusive performance pro ling for entire so ware stacks based on the ow reconstruction principle. In Proc. USENIX Symposium on Operating Systems Design and Implementation (OSDI). 603-618.</p>
<p>Session F2: Insights from Log(in)s CCS'17. Dallas, TX, USASession F2: Insights from Log(in)s CCS'17, October 30-November 3, 2017, Dallas, TX, USA</p>            </div>
        </div>

    </div>
</body>
</html>
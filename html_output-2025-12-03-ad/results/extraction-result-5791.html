<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5791 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5791</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5791</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-116.html">extraction-schema-116</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <p><strong>Paper ID:</strong> paper-9b1f4492a663c7f56f2b43ae1ed167d3857aacca</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/9b1f4492a663c7f56f2b43ae1ed167d3857aacca" target="_blank">PromptSource: An Integrated Development Environment and Repository for Natural Language Prompts</a></p>
                <p><strong>Paper Venue:</strong> Annual Meeting of the Association for Computational Linguistics</p>
                <p><strong>Paper TL;DR:</strong> PromptSource addresses the emergent challenges in this new setting with a templating language for defining data-linked prompts, an interface that lets users quickly iterate on prompt development by observing outputs of their prompts on many examples, and a community-driven set of guidelines for contributing new prompts to a common pool.</p>
                <p><strong>Paper Abstract:</strong> PromptSource is a system for creating, sharing, and using natural language prompts. Prompts are functions that map an example from a dataset to a natural language input and target output. Using prompts to train and query language models is an emerging area in NLP that requires new tools that let users develop and refine these prompts collaboratively. PromptSource addresses the emergent challenges in this new setting with (1) a templating language for defining data-linked prompts, (2) an interface that lets users quickly iterate on prompt development by observing outputs of their prompts on many examples, and (3) a community-driven set of guidelines for contributing new prompts to a common pool. Over 2,000 prompts for roughly 170 datasets are already available in PromptSource. PromptSource is available at https://github.com/bigscience-workshop/promptsource.</p>
                <p><strong>Cost:</strong> 0.011</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5791.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5791.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Explicit Answer Choices</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Explicitly listing valid completions (answer choices) in prompts</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The practice of including an explicit, enumerated list of valid target completions (answer choices) inside the prompt/input, so model outputs can be scored only among those choices.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>General classification / small-output-space tasks</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Tasks where the set of valid outputs is small and can be enumerated (e.g., classification, binary QA, tasks with limited discrete labels).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Zero-shot prompt that explicitly states valid outputs (e.g., 'Answer choices: A, B, C') and exposes those completions as a separate list to be scored rather than allowing free-form completions.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Prompts that do not explicitly state valid completions (free-form target text); the paper contrasts prompts that list answer choices vs. equivalent prompts that omit them.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Qualitative observation: prompts that explicitly listed valid completions performed better in the authors' early experiments (no numeric metrics provided).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Qualitative: unspecified numeric improvement versus prompts without listed choices; only stated as 'performed worse' for prompts that did not list completions.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>improved</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Listing valid completions reduces spurious ambiguity in model outputs and enables evaluation by scoring a limited set of completions (reduces confounding variability in target formats).</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PromptSource: An Integrated Development Environment and Repository for Natural Language Prompts', 'publication_date_yy_mm': '2022-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5791.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5791.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Target-only Answer Format</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Keeping target templates to the bare answer (removing surrounding text)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Guideline to format target text (labels) so they contain only the canonical answer token(s) and no extra words like 'The answer is ...', to avoid ambiguity during training and evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>General supervised / prompted training tasks</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Tasks where training examples include a target/completion string; concern is about variation in the target text format.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Training or target templates where the target contains only the answer (e.g., 'entailment' rather than 'The answer is entailment').</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Targets that include extra surrounding scaffolding text (e.g., 'The answer is X' included in the target instead of just 'X').</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Qualitative claim: removing variations on target format that introduce spurious ambiguity was found useful (no numeric metrics provided).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>improved</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Removing extraneous text avoids spurious ambiguity in targets, leading to clearer mapping between model completion and intended label and reducing evaluation noise.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PromptSource: An Integrated Development Environment and Repository for Natural Language Prompts', 'publication_date_yy_mm': '2022-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5791.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5791.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prompt Diversity (P3)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Diversity of prompt formulations in training (Public Pool of Prompts, P3)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Collection/practice of training on many different human-written prompt formulations for the same datasets to encourage robustness to prompt wording at test time.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Zero-shot generalization across many NLP tasks</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Evaluating models' abilities to follow unseen prompt formulations and perform varied downstream NLP tasks in zero-shot settings.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Training on a massively multitask mixture of many human-written prompts per dataset (P3), producing prompted training instances with diverse phrasing.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Training on fewer or single prompt formulations per task (or not training on prompted mixtures); comparisons referenced as a research question enabled by P3.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Qualitative: authors note that training on diverse prompts is hypothesized and used in follow-on work to produce more robust generalization to new prompt formulations (no numeric results in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>improved (reported in follow-on work)</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Exposure to many formulations during training teaches models to interpret the task description independent of surface wording, increasing robustness to prompt variation at test time.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PromptSource: An Integrated Development Environment and Repository for Natural Language Prompts', 'publication_date_yy_mm': '2022-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5791.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e5791.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Multitask Prompted Training (T0)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Multitask prompted training enables zero-shot task generalization (T0 series)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Approach that fine-tunes an autoregressive language model on a large multitask mixture of prompted datasets (subset of P3) to enable zero-shot inference on unseen tasks and prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Multitask prompted training enables zero-shot task generalization</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>T0 (T0 series)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Zero-shot task generalization across NLP benchmarks</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Evaluate model performance on tasks and prompt formulations not seen during fine-tuning; measure zero-shot inference capability.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Fine-tuning on many human-written prompts (zero-shot inference: prompting at test without training examples), i.e., models trained on prompted datasets from P3 and evaluated zero-shot on unseen tasks/prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Implicit comparison to models not trained on the multitask prompted mixture (standard fine-tuning or pretraining-only), as per the cited paper (Sanh et al., 2021).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Not reported in this paper; referenced follow-on work (Sanh et al., 2021) demonstrates improved zero-shot generalization from multitask prompted training but this paper does not provide numeric metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>improved (reported by the cited work)</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Training on a broad mixture of human-written prompts teaches models to map prompt formulations to task behavior, enabling generalization to unseen tasks and prompt wordings.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PromptSource: An Integrated Development Environment and Repository for Natural Language Prompts', 'publication_date_yy_mm': '2022-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5791.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e5791.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Priming-format Pretraining (MetaICL)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Pretraining on priming-formatted multitask mixture to improve in-context learning (MetaICL)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Method that first fully trains a model (via gradient updates) on a multitask mixture formatted with priming (in-context) examples, improving subsequent few-shot priming performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Metaicl: Learning to learn in context</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>In-context (priming) few-shot learning</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Use of labeled examples included in the input sequence (priming) to elicit few-shot behavior without gradient updates; evaluation of few-shot performance.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Training data formatted as priming examples (i.e., inputs that contain labeled exemplars in-context), then using in-context few-shot prompts at test time.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Priming-only approaches without the prior gradient-based training on priming-formatted multitask mixture (i.e., standard priming without the specialized pretraining).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Qualitative: Min et al. (2021) found that including P3 templates and pretraining on priming-formatted multitask data significantly improved priming performance; no numeric values are provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>improved</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Pretraining on inputs formatted like priming examples teaches the model how to use in-context exemplars as cues for task behavior, making priming more effective.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PromptSource: An Integrated Development Environment and Repository for Natural Language Prompts', 'publication_date_yy_mm': '2022-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5791.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e5791.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prompt Wording Sensitivity</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sensitivity of zero-shot models to prompt wording and formulation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>General observation that choices in prompt wording, structure, and explicitness can materially affect downstream predictions, especially in zero-shot settings.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Zero-shot prompting across NLP tasks</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Behavior of large language models when given natural-language task descriptions without task-specific training examples.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Various zero-shot prompt formulations (differences in phrasing, explicitness, inclusion of answer choices, instruction clarity).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Implicit comparisons across different prompt formulations (cites Perez et al., Zhao et al., Webson & Pavlick as prior evidence).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Qualitative claim: prompt choices can 'affect downstream predictions significantly' especially in zero-shot; no numeric results included in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>variable (can improve or degrade depending on wording)</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Different surface forms change how model conditions on the task description; clarity, explicitness of valid outputs, and target formatting reduce ambiguity and improve alignment between prompt and desired outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PromptSource: An Integrated Development Environment and Repository for Natural Language Prompts', 'publication_date_yy_mm': '2022-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5791.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e5791.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Multilingual Prompt Translation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Using English P3 prompts as templates for multilingual prompting (translation of prompts)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Use of the English PromptSource prompt collection as a basis for creating prompts in other languages (via human or machine translation) to evaluate multilingual models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Few-shot learning with multilingual language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Cross-lingual / multilingual zero- and few-shot tasks</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Benchmarking tasks across multiple languages to measure cross-lingual generalization of autoregressive multilingual models.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Translating prompt templates (originally in English) into other languages for evaluation and training; the format stays as natural language prompts but in target language.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>English-only prompting vs. multilingual prompting using translated prompts; paper notes P3 served as support for building prompts in other languages.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Not reported in this paper; Lin et al. (2021) used P3 to create multilingual prompts and benchmark models (no metrics included here).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>not specified</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>High-quality English prompt templates provide a scaffold for translations; consistency in prompt intent across languages may aid cross-lingual evaluation and transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PromptSource: An Integrated Development Environment and Repository for Natural Language Prompts', 'publication_date_yy_mm': '2022-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Multitask prompted training enables zero-shot task generalization <em>(Rating: 2)</em></li>
                <li>Metaicl: Learning to learn in context <em>(Rating: 2)</em></li>
                <li>Few-shot learning with multilingual language models <em>(Rating: 2)</em></li>
                <li>True few-shot learning with language models <em>(Rating: 1)</em></li>
                <li>Calibrate before use: Improving few-shot performance of language models <em>(Rating: 1)</em></li>
                <li>Do prompt-based models really understand the meaning of their prompts? <em>(Rating: 2)</em></li>
                <li>Language models are few-shot learners <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5791",
    "paper_id": "paper-9b1f4492a663c7f56f2b43ae1ed167d3857aacca",
    "extraction_schema_id": "extraction-schema-116",
    "extracted_data": [
        {
            "name_short": "Explicit Answer Choices",
            "name_full": "Explicitly listing valid completions (answer choices) in prompts",
            "brief_description": "The practice of including an explicit, enumerated list of valid target completions (answer choices) inside the prompt/input, so model outputs can be scored only among those choices.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": null,
            "model_size": null,
            "task_name": "General classification / small-output-space tasks",
            "task_description": "Tasks where the set of valid outputs is small and can be enumerated (e.g., classification, binary QA, tasks with limited discrete labels).",
            "problem_format": "Zero-shot prompt that explicitly states valid outputs (e.g., 'Answer choices: A, B, C') and exposes those completions as a separate list to be scored rather than allowing free-form completions.",
            "comparison_format": "Prompts that do not explicitly state valid completions (free-form target text); the paper contrasts prompts that list answer choices vs. equivalent prompts that omit them.",
            "performance": "Qualitative observation: prompts that explicitly listed valid completions performed better in the authors' early experiments (no numeric metrics provided).",
            "performance_comparison": "Qualitative: unspecified numeric improvement versus prompts without listed choices; only stated as 'performed worse' for prompts that did not list completions.",
            "format_effect_size": null,
            "format_effect_direction": "improved",
            "explanation_or_hypothesis": "Listing valid completions reduces spurious ambiguity in model outputs and enables evaluation by scoring a limited set of completions (reduces confounding variability in target formats).",
            "counterexample_or_null_result": null,
            "uuid": "e5791.0",
            "source_info": {
                "paper_title": "PromptSource: An Integrated Development Environment and Repository for Natural Language Prompts",
                "publication_date_yy_mm": "2022-02"
            }
        },
        {
            "name_short": "Target-only Answer Format",
            "name_full": "Keeping target templates to the bare answer (removing surrounding text)",
            "brief_description": "Guideline to format target text (labels) so they contain only the canonical answer token(s) and no extra words like 'The answer is ...', to avoid ambiguity during training and evaluation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": null,
            "model_size": null,
            "task_name": "General supervised / prompted training tasks",
            "task_description": "Tasks where training examples include a target/completion string; concern is about variation in the target text format.",
            "problem_format": "Training or target templates where the target contains only the answer (e.g., 'entailment' rather than 'The answer is entailment').",
            "comparison_format": "Targets that include extra surrounding scaffolding text (e.g., 'The answer is X' included in the target instead of just 'X').",
            "performance": "Qualitative claim: removing variations on target format that introduce spurious ambiguity was found useful (no numeric metrics provided).",
            "performance_comparison": null,
            "format_effect_size": null,
            "format_effect_direction": "improved",
            "explanation_or_hypothesis": "Removing extraneous text avoids spurious ambiguity in targets, leading to clearer mapping between model completion and intended label and reducing evaluation noise.",
            "counterexample_or_null_result": null,
            "uuid": "e5791.1",
            "source_info": {
                "paper_title": "PromptSource: An Integrated Development Environment and Repository for Natural Language Prompts",
                "publication_date_yy_mm": "2022-02"
            }
        },
        {
            "name_short": "Prompt Diversity (P3)",
            "name_full": "Diversity of prompt formulations in training (Public Pool of Prompts, P3)",
            "brief_description": "Collection/practice of training on many different human-written prompt formulations for the same datasets to encourage robustness to prompt wording at test time.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": null,
            "model_size": null,
            "task_name": "Zero-shot generalization across many NLP tasks",
            "task_description": "Evaluating models' abilities to follow unseen prompt formulations and perform varied downstream NLP tasks in zero-shot settings.",
            "problem_format": "Training on a massively multitask mixture of many human-written prompts per dataset (P3), producing prompted training instances with diverse phrasing.",
            "comparison_format": "Training on fewer or single prompt formulations per task (or not training on prompted mixtures); comparisons referenced as a research question enabled by P3.",
            "performance": "Qualitative: authors note that training on diverse prompts is hypothesized and used in follow-on work to produce more robust generalization to new prompt formulations (no numeric results in this paper).",
            "performance_comparison": null,
            "format_effect_size": null,
            "format_effect_direction": "improved (reported in follow-on work)",
            "explanation_or_hypothesis": "Exposure to many formulations during training teaches models to interpret the task description independent of surface wording, increasing robustness to prompt variation at test time.",
            "counterexample_or_null_result": null,
            "uuid": "e5791.2",
            "source_info": {
                "paper_title": "PromptSource: An Integrated Development Environment and Repository for Natural Language Prompts",
                "publication_date_yy_mm": "2022-02"
            }
        },
        {
            "name_short": "Multitask Prompted Training (T0)",
            "name_full": "Multitask prompted training enables zero-shot task generalization (T0 series)",
            "brief_description": "Approach that fine-tunes an autoregressive language model on a large multitask mixture of prompted datasets (subset of P3) to enable zero-shot inference on unseen tasks and prompts.",
            "citation_title": "Multitask prompted training enables zero-shot task generalization",
            "mention_or_use": "use",
            "model_name": "T0 (T0 series)",
            "model_size": null,
            "task_name": "Zero-shot task generalization across NLP benchmarks",
            "task_description": "Evaluate model performance on tasks and prompt formulations not seen during fine-tuning; measure zero-shot inference capability.",
            "problem_format": "Fine-tuning on many human-written prompts (zero-shot inference: prompting at test without training examples), i.e., models trained on prompted datasets from P3 and evaluated zero-shot on unseen tasks/prompts.",
            "comparison_format": "Implicit comparison to models not trained on the multitask prompted mixture (standard fine-tuning or pretraining-only), as per the cited paper (Sanh et al., 2021).",
            "performance": "Not reported in this paper; referenced follow-on work (Sanh et al., 2021) demonstrates improved zero-shot generalization from multitask prompted training but this paper does not provide numeric metrics.",
            "performance_comparison": null,
            "format_effect_size": null,
            "format_effect_direction": "improved (reported by the cited work)",
            "explanation_or_hypothesis": "Training on a broad mixture of human-written prompts teaches models to map prompt formulations to task behavior, enabling generalization to unseen tasks and prompt wordings.",
            "counterexample_or_null_result": null,
            "uuid": "e5791.3",
            "source_info": {
                "paper_title": "PromptSource: An Integrated Development Environment and Repository for Natural Language Prompts",
                "publication_date_yy_mm": "2022-02"
            }
        },
        {
            "name_short": "Priming-format Pretraining (MetaICL)",
            "name_full": "Pretraining on priming-formatted multitask mixture to improve in-context learning (MetaICL)",
            "brief_description": "Method that first fully trains a model (via gradient updates) on a multitask mixture formatted with priming (in-context) examples, improving subsequent few-shot priming performance.",
            "citation_title": "Metaicl: Learning to learn in context",
            "mention_or_use": "use",
            "model_name": null,
            "model_size": null,
            "task_name": "In-context (priming) few-shot learning",
            "task_description": "Use of labeled examples included in the input sequence (priming) to elicit few-shot behavior without gradient updates; evaluation of few-shot performance.",
            "problem_format": "Training data formatted as priming examples (i.e., inputs that contain labeled exemplars in-context), then using in-context few-shot prompts at test time.",
            "comparison_format": "Priming-only approaches without the prior gradient-based training on priming-formatted multitask mixture (i.e., standard priming without the specialized pretraining).",
            "performance": "Qualitative: Min et al. (2021) found that including P3 templates and pretraining on priming-formatted multitask data significantly improved priming performance; no numeric values are provided in this paper.",
            "performance_comparison": null,
            "format_effect_size": null,
            "format_effect_direction": "improved",
            "explanation_or_hypothesis": "Pretraining on inputs formatted like priming examples teaches the model how to use in-context exemplars as cues for task behavior, making priming more effective.",
            "counterexample_or_null_result": null,
            "uuid": "e5791.4",
            "source_info": {
                "paper_title": "PromptSource: An Integrated Development Environment and Repository for Natural Language Prompts",
                "publication_date_yy_mm": "2022-02"
            }
        },
        {
            "name_short": "Prompt Wording Sensitivity",
            "name_full": "Sensitivity of zero-shot models to prompt wording and formulation",
            "brief_description": "General observation that choices in prompt wording, structure, and explicitness can materially affect downstream predictions, especially in zero-shot settings.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": null,
            "model_size": null,
            "task_name": "Zero-shot prompting across NLP tasks",
            "task_description": "Behavior of large language models when given natural-language task descriptions without task-specific training examples.",
            "problem_format": "Various zero-shot prompt formulations (differences in phrasing, explicitness, inclusion of answer choices, instruction clarity).",
            "comparison_format": "Implicit comparisons across different prompt formulations (cites Perez et al., Zhao et al., Webson & Pavlick as prior evidence).",
            "performance": "Qualitative claim: prompt choices can 'affect downstream predictions significantly' especially in zero-shot; no numeric results included in this paper.",
            "performance_comparison": null,
            "format_effect_size": null,
            "format_effect_direction": "variable (can improve or degrade depending on wording)",
            "explanation_or_hypothesis": "Different surface forms change how model conditions on the task description; clarity, explicitness of valid outputs, and target formatting reduce ambiguity and improve alignment between prompt and desired outputs.",
            "counterexample_or_null_result": null,
            "uuid": "e5791.5",
            "source_info": {
                "paper_title": "PromptSource: An Integrated Development Environment and Repository for Natural Language Prompts",
                "publication_date_yy_mm": "2022-02"
            }
        },
        {
            "name_short": "Multilingual Prompt Translation",
            "name_full": "Using English P3 prompts as templates for multilingual prompting (translation of prompts)",
            "brief_description": "Use of the English PromptSource prompt collection as a basis for creating prompts in other languages (via human or machine translation) to evaluate multilingual models.",
            "citation_title": "Few-shot learning with multilingual language models",
            "mention_or_use": "use",
            "model_name": null,
            "model_size": null,
            "task_name": "Cross-lingual / multilingual zero- and few-shot tasks",
            "task_description": "Benchmarking tasks across multiple languages to measure cross-lingual generalization of autoregressive multilingual models.",
            "problem_format": "Translating prompt templates (originally in English) into other languages for evaluation and training; the format stays as natural language prompts but in target language.",
            "comparison_format": "English-only prompting vs. multilingual prompting using translated prompts; paper notes P3 served as support for building prompts in other languages.",
            "performance": "Not reported in this paper; Lin et al. (2021) used P3 to create multilingual prompts and benchmark models (no metrics included here).",
            "performance_comparison": null,
            "format_effect_size": null,
            "format_effect_direction": "not specified",
            "explanation_or_hypothesis": "High-quality English prompt templates provide a scaffold for translations; consistency in prompt intent across languages may aid cross-lingual evaluation and transfer.",
            "counterexample_or_null_result": null,
            "uuid": "e5791.6",
            "source_info": {
                "paper_title": "PromptSource: An Integrated Development Environment and Repository for Natural Language Prompts",
                "publication_date_yy_mm": "2022-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Multitask prompted training enables zero-shot task generalization",
            "rating": 2
        },
        {
            "paper_title": "Metaicl: Learning to learn in context",
            "rating": 2
        },
        {
            "paper_title": "Few-shot learning with multilingual language models",
            "rating": 2
        },
        {
            "paper_title": "True few-shot learning with language models",
            "rating": 1
        },
        {
            "paper_title": "Calibrate before use: Improving few-shot performance of language models",
            "rating": 1
        },
        {
            "paper_title": "Do prompt-based models really understand the meaning of their prompts?",
            "rating": 2
        },
        {
            "paper_title": "Language models are few-shot learners",
            "rating": 2
        }
    ],
    "cost": 0.01143725,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>PromptSource: An Integrated Development Environment and Repository for Natural Language Prompts</h1>
<p>Stephen H. Bach ${ }^{<em> 1,2}$ Victor Sanh ${ }^{</em> 3}$ Zheng-Xin Yong ${ }^{1}$ Albert Webson ${ }^{1}$ Colin Raffel ${ }^{3}$<br>Nihal V. Nayak ${ }^{1}$ Abheesht Sharma ${ }^{4}$ Taewoon Kim ${ }^{5}$ M Saiful Bari ${ }^{6}$ Thibault Fevry ${ }^{7}$<br>Zaid Alyafeai ${ }^{8}$ Manan Dey ${ }^{9}$ Andrea Santilli ${ }^{10}$ Zhiqing Sun ${ }^{11}$ Srulik Ben-David ${ }^{12}$<br>Canwen Xu ${ }^{13}$ Gunjan Chhablani ${ }^{7}$ Han Wang ${ }^{14}$ Jason Alan Fries ${ }^{15,2}$<br>Maged S. Al-shaibani ${ }^{8}$ Shanya Sharma ${ }^{16}$ Urmish Thakker ${ }^{17}$ Khalid Almubarak ${ }^{18}$<br>Xiangru Tang ${ }^{19}$ Dragomir Radev ${ }^{19}$ Mike Tian-Jian Jiang ${ }^{20}$ Alexander M. Rush ${ }^{3}$<br>${ }^{1}$ Brown University ${ }^{2}$ Snorkel AI ${ }^{3}$ Hugging Face ${ }^{4}$ BITS Pilani ${ }^{5}$ VU Amsterdam<br>${ }^{6}$ NTU ${ }^{7}$ BigScience ${ }^{8}$ KFUPM ${ }^{9}$ SAP ${ }^{10}$ University of Rome ${ }^{11}$ CMU ${ }^{12}$ Technion<br>${ }^{13}$ UCSD ${ }^{14}$ NYU ${ }^{15}$ Stanford University ${ }^{16}$ Walmart Labs ${ }^{17}$ SambaNova Systems<br>${ }^{18}$ PSAU ${ }^{19}$ Yale University ${ }^{20}$ ZEALS ${ }^{*}$ Equal Contribution</p>
<h4>Abstract</h4>
<p>PromptSource is a system for creating, sharing, and using natural language prompts. Prompts are functions that map an example from a dataset to a natural language input and target output. Using prompts to train and query language models is an emerging area in NLP that requires new tools that let users develop and refine these prompts collaboratively. PromptSource addresses the emergent challenges in this new setting with (1) a templating language for defining data-linked prompts, (2) an interface that lets users quickly iterate on prompt development by observing outputs of their prompts on many examples, and (3) a community-driven set of guidelines for contributing new prompts to a common pool. Over 2,000 prompts for roughly 170 datasets are already available in PromptSource. PromptSource is available at https://github. com/bigscience-workshop/ promptsource.</p>
<h2>1 Introduction</h2>
<p>Prompt engineering is emerging as a new focus in NLP, particularly in zero- and few-shot learning settings. Prompting is the practice of representing a task as a natural language utterance in order to query a language model for a response (Liu et al., 2021). For example, if a language model is conditioned on the text "She hit a home run. The previous sentence is about ...", then the model's subsequent generation would be interpreted as a prediction of the topic of the preceding sentence,
e.g. by mapping a response such as "sports" to a class label. In specific contexts, prompting has been shown to have advantages over traditional classification, for example facilitating adaptation of language models to ad-hoc tasks and improving sample efficiency in low-data settings (Brown et al., 2020; Schick and Schtze, 2021b; Le Scao and Rush, 2021; Gao et al., 2021). These advantages motivate a practical challenge: How can we enable users to create, refine, and share prompts?</p>
<p>The process of prompt engineering is critical for successful deployment as choices in prompting can affect downstream predictions significantly, particularly in the zero-shot setting (Perez et al., 2021; Zhao et al., 2021; Webson and Pavlick, 2021). Furthermore, training directly on collections of prompts can enable large models to generalize to new prompts more robustly (Sanh et al., 2021; Wei et al., 2021; Min et al., 2021; Mishra et al., 2021). There is therefore a growing need for tools that support the creation of corpora of prompts.</p>
<p>PromptSource is an integrated development environment and repository for natural language prompts to use in the context of zero-shot (or gradient-based few-shot) learning. It provides a Web-based GUI that enables developers to write prompts in a templating language and immediately view their outputs on different examples. The system is integrated with the HuggingFace Datasets library (Lhoest et al., 2021), so that users can load any dataset automatically, browse existing prompts, and create new ones. Through the course of writing thousands of prompts, we converged on three key</p>
<p>aspects to the design of PromptSource:</p>
<ul>
<li>Flexible Templating Language. We adapt a templating language to represent prompts. Prompt authors can define prompts in terms of dataset fields, hard-coded text, and simple control logic. This choice provides the flexibility of a programming environment without the mental overhead of having to write and read arbitrary code. Prompt templates can easily be distributed and used in other systems.</li>
<li>Tools for Prompt Management. PromptSource has multiple view to address the needs of prompt authors at different stages of the prompt engineering cycle. A global view lets authors browse datasets and existing prompt templates. A local view facilitates iteration on prompt wording and metadata, as well as testing on individual examples.</li>
<li>Community-Driven Quality Standards. PromptSource includes a set of guidelines for prompting based on a large-scale prompt writing pilot. PromptSource's collection is meant to be useful for a wide range of research, based on iterative refinement of a set of quality standards. Prompts in PromptSource are also annotated with various pieces of metadata to make finding and using prompts easier.
The PromptSource system includes over 2,000 open-source prompts for roughly 170 datasets, which have all been reviewed to meet the quality standards. This collection, which we call the Public Pool of Prompts (P3), allows users to materialize prompted forms of datasets for hundreds of different tasks. The T0 series of models (Sanh et al., 2021) for zero-shot inference were fine-tuned on a subset of P3. Since then, PromptSource and P3 have been extended for research on multi-lingual prompting (Lin et al., 2021) and priming, i.e., incontext few-shot learning (Min et al., 2021). The PromptSource system and associated content is a first step in the study of systems for prompt engineering, an area that is likely to continue to grow.</li>
</ul>
<h2>2 Background and Related Work</h2>
<p>PromptSource builds on recent work in prompting and prompt engineering. It is also related to work on systems for other types of annotations.
Prompting Recently, prompting has emerged as a new focus within NLP as it can dramatically improve language models' few-shot and zeroshot performance in a wide range of downstream
tasks (Brown et al., 2020; Schick and Schtze, 2021a; Sanh et al., 2021; Wei et al., 2021). Prompts and prompt engineering come in several varieties (Liu et al., 2021). PromptSource is focused on facilitating research with human-written prompts, in which natural language is the medium for describing tasks. This approach has the advantage that prompts can be understood, modified, and applied without being tied to a specific model. In contrast, past work has also aimed to automatically construct prompts by framing the search for a good prompt as a learning problem. These prompts can either be expressed in natural language (Gao et al., 2021; Shin et al., 2020) or as arbitrary vectors (a.k.a. "continuous" or "soft" prompts) not corresponding to words in the model's original vocabulary (Lester et al., 2021; Qin and Eisner, 2021)</p>
<p>When using human-written prompts, there are several possible approaches to learning. One is a zero-shot setting, where the goal is to generalize to prompts for which no training examples are given. Prompts can also be used in a few-shot setting, in which a model is either (1) trained on prompted examples of the target task via gradient updates, or (2) priming (i.e. in-context learning), in which labeled examples are included in an input sequence in order to prime models to make predictions without gradient updates (Brown et al., 2020).</p>
<p>PromptSource was originally designed for zeroshot learning, so it emphasizes explicit task instructions and no priming examples. If needed, users can extend PromptSource for few-shot learning (e.g., as done in Lin et al., 2021 and Min et al., 2021, described in 7).
Systems for Annotating Data Most work on collecting annotations has focused on labels and other annotations at the level of individual examples (Neves and eva, 2021). GATE (Cunningham et al., 2002) was an early system for annotating text, and includes support for many data types such as labels and entity tags. Since then, many Webbased systems for annotating text have been developed (Stenetorp et al., 2012; Salgado et al., 2012; Wei et al., 2013; Yimam et al., 2013; Chen and Styler, 2013; Eckart de Castilho et al., 2016; Putra et al., 2020). Other systems support collaboration among multiple annotators (Yang et al., 2018; Stewart et al., 2019). More recently, many annotation systems have begun to incorporate learned models to improve workflow, using techniques such as active learning (Lin et al., 2019; Li et al., 2021) and</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: The five stages of creating prompts in PromptSource. The Browse view for Dataset Exploration (S1). The Sourcing view for Prompt Writing (S2), Prompt Documentation (S3), and Iteration and Variation (S4). The Browse view for performing a Global Review (S5).
example recommendation (Lee et al., 2020; Kiela et al., 2021). These systems are possible because the annotations to be collected are labels, for which metrics like inter-annotator agreement and model confidence are available.</p>
<p>There has also been some work on collecting annotations other than labels. AlvisAE (Papazian et al., 2012) and TreeAnnotator (Helfrich et al., 2018) support creating ontologies and other structured annotations. Prompts differ from these annotations in that they are semi-structured functions, requiring new tools for developers.</p>
<h2>3 System Design and Workflow</h2>
<p>Creating prompts differs from other types of data collection and annotation. We focus on three challenging aspects on which prompting differs from traditional NLP annotation:</p>
<ul>
<li>Functions, not Labels. A single prompt is a function that maps dataset examples (dictionaries of arbitrary fields) to natural language input/target pairs. Creating a prompt is therefore more like programming than typical data annotation. How should a prompt format trade off between expressivity and simplicity?</li>
<li>Dataset-Level Choices. Prompts are associated with datasets, unlike label annotations that are local to single examples. Prompt engineering requires developers to evaluate their choices across all examples. What interfaces do authors need to inspect and debug their prompts?</li>
<li>Variation in Prompt Construction. Unlike with labels, it is often desirable to have variation within prompt construction, as different prompt choices may lead to different results. However, variation complicates quality judg-
ment, and makes it impossible to apply simple metrics like inter-annotator agreement. How can multiple authors collaborate to build a high-quality corpus of prompts and associated metadata?
To illustrate these distinct aspects, we start with a concrete overview of the prompt creation process of PromptSource. For this example, we imagine that a user of PromptSource is creating prompts for a natural language inference dataset, specifically SNLI (Bowman et al., 2015). The goal is to design a prompt query such that the answer can be mapped onto the SNLI classes. A prompt author can accomplish this goal with PromptSource via the following five steps (Figure 1):</li>
</ul>
<p>S1: Dataset Exploration The prompt author starts in the Browse view to read the dataset description, including linked READMEs and papers, and to browse through examples. In this case, they would see that SNLI is a dataset for natural language inference: assume a given premise sentence is true, the goal is to determine whether a hypothesis sentence is true (entailment), false (contradiction), or undetermined (neutral).</p>
<p>S2: Prompt Writing The prompt author uses the Sourcing view to try out a prompt wording, and then adjusts it by observing prompted examples (Figure 1 middle, full example in Figures 3 and 4).</p>
<p>S3: Prompt Documentation To facilitate using the prompt, the author fills in various metadata including possible metrics to evaluate the prompt, valid outputs if applicable, whether the prompt expresses the original intended task of the dataset, and whether the template explicitly states the valid outputs.</p>
<p>S4: Iteration and Variation The prompt author then iterates through S2 and S3 to create multiple</p>
<p>prompts for the dataset. Authors are encouraged to vary multiple factors such as the formulation of the prompt and the targeted task (see Section 6).</p>
<p>S5: Global Review The author saves the draft prompts in a structured file which are then verified by other contributors through code reviews. New prompts need to meet the quality standard with a series of automatic tests and by validation through prompted instances. Upon passing review, the new prompts can be merged into a global prompts collection.</p>
<p>Upon submission, prompts can be viewed through PromptSource by other users. The full collection is stored globally and can be used outside of the tool, for instance to be applied on an example from a dataset of the Datasets library (Lhoest et al., 2021).</p>
<div class="codehilite"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">promptsource.templates</span><span class="w"> </span><span class="kn">import</span> <span class="n">DatasetTemplates</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">datasets</span><span class="w"> </span><span class="kn">import</span> <span class="n">load_dataset</span>
<span class="n">prompts</span> <span class="o">=</span> <span class="n">DatasetTemplates</span><span class="p">(</span><span class="s2">&quot;enli&quot;</span><span class="p">)</span>
<span class="n">prompt_key</span> <span class="o">=</span> <span class="s2">&quot;based on the previous passage&quot;</span>
<span class="n">p</span> <span class="o">=</span> <span class="n">prompts</span><span class="p">[</span><span class="n">prompt_key</span><span class="p">]</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">load_dataset</span><span class="p">(</span><span class="s2">&quot;enli&quot;</span><span class="p">,</span> <span class="n">split</span><span class="o">=</span><span class="s2">&quot;train&quot;</span><span class="p">)</span>
<span class="n">example</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">example</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;INPUT: &quot;</span><span class="p">,</span> <span class="n">result</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;TARGET: &quot;</span><span class="p">,</span> <span class="n">result</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
</code></pre></div>

<p>With this workflow in mind, we next describe the key aspects of the PromptSource system in greater detail.</p>
<h2>4 Prompting Language</h2>
<p>A key design decision is the format for prompts. Previous works on prompting tended to use code for specifying each prompt. We experimented with this format and found a trade-off between expressivity and explicit structure. On one side, a maximally expressive format such as pure Python code would let users write complex programs to manipulate the semi-structured examples into prompted examples. However, interpreting and analyzing these programs becomes difficult. This difficulty limits downstream manipulation and analysis of the prompts, for example for possible future work on automatic prompt augmentation. On the other side, a maximally structured format, such as rule-based generation, limits the kinds of prompts that users can create. We found it infeasible to enumerate types of rules sufficient for the wide range of tasks and data formats for which we wanted prompts.</p>
<p>We therefore settled on a middle ground between the two: a templating language. Specifically,
we use the Jinja2 templating engine, ${ }^{1}$ originally designed for producing web markup. Users write templates as prompts with placeholders, such as If {{premise}} is true, is it also true that {{hypothesis}}? || | {{entailed}}. The separator ||| denotes the break between the conditioning text and the desired completion. Placeholders refer to fields in the underlying example (represented as a Python dict by Datasets (Lhoest et al., 2021)). Users also have access to Jinja's built-in functions, such as manipulating strings and structured data. For each prompt, prompted examples are created by applying the prompt to all examples in the corresponding dataset. While Jinja is a complete programming language, our review guidelines encourage simple functions with minimal additional logic (see Figure 3 and 4 for example).</p>
<p>During the development of PromptSource, we found that a few idioms were particularly useful. First, not all templates are applicable to all examples in a dataset. Users can wrap templates in Jinja's built-in conditional statements, and any example that results in an empty prompted example is simply skipped. Second, many examples can be used to make multiple training instances, such as a question that has multiple valid answers. We therefore added a choice function that selects an element from a list in a way that can be controlled during dataset generation, such as picking a random element using a seeded random number generator or generating different prompts for each combination of elements in the template. Third, many tasks such as classification and binary question answering have a small set of possible valid completions, and it is common to make predictions for these tasks by scoring only the valid completions and returning the highest one (Brown et al., 2020; Sanh et al., 2021; Wei et al., 2021). Users therefore can list the valid completions in a separate field and access them as a list in their prompts (displayed as Answer choices in Figure 3). These completions are then explicitly available when evaluating predictions for these prompted examples.</p>
<h2>5 The PromptSource UI</h2>
<p>The PromptSource system is designed to enable prompt creators to view data (S1), write prompts in a standard format (S2, S3, and S4), and ver-</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Prompt creators can browse through the dataset examples (left-column) and their prompted form (right column) using the Browse view.
ify that their templates work correctly (S5). We implemented a lightweight interface for the tool in Streamlit ${ }^{2}$ so that users could download, run locally in a web browser, and then upload their results to a central repository. Testing iterations of the interface on pilot template-writing tasks, we converged on three views for the interface.</p>
<p>V1: Browse This view (Figure 2) lets users inspect datasets before creating prompts (S1). Once prompts are created, they can select prompts and browse the prompted examples generated by them (S5). The original example is viewed side-by-side with the resulting prompted example, with the substituted text highlighted to distinguish from text hard-coded in the template. Users can quickly scroll through many examples, verify the behavior of their prompt, and return to the sourcing view if changes are needed.</p>
<p>V2: Sourcing This view (Figures 3 and 4) allows users to select a dataset to prompt, browse examples from that dataset in the form of tables, and enter a prompt for that dataset. As the user writes their template (S2, S3, and S4), every time they save it, the output of the template applied to the current example is displayed next to the editor. We also collect metadata like a name for the template, and a reference for any bibliographic information or rationale for the template.</p>
<p>V3: Helicopter This view (Figure 5) allows users to see what datasets are available for writing templates and how many are written for each, to prioritize user attention. This view is particularly useful for moving between datasets and for the prompt reviewers (S5).</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h2>6 Community Guidelines and Process</h2>
<p>Due to the variety of existing NLP datasets, we found it challenging to exhaustively describe the characteristics of a good prompt: there are no simple metrics like inter-annotator agreement on example-level labels. Instead, over a few iterations, we converged on community guidelines ${ }^{3}$ with three objectives in mind: (a) provide a standardized vocabulary for discussing prompts between prompt authors, reviewers and users, and minimum requirements for a valid prompt, (b) highlight common errors and best practices, (c) collect the necessary information about the prompts to support current and future research on prompt engineering. The guidelines were enforced in the use of PromptSource by a code review process in which each prompt was reviewed before being committed to the central repository.</p>
<p>Guidelines apply to the combination of a template (a function that maps an example into an input/target pair in natural language) and a set of metadata about the template. The most important constraint we imposed for a template to be valid is that it is formulated in natural language (both for the input and the target). We forbid the use of non-natural language prompts such as pure code. Each prompt should clearly state what task should be solved, in a way a non-specialist adult can understand. We found this guideline strikes a good balance between freedom and expressivity in the wording of the prompts on one side and short generic prompts on the other side.</p>
<p>In early experiments, we found that user-written prompts that did not explicitly state the possible valid completions tended to perform worse in experiments than their counterparts in which the possible valid completions were listed. We encouraged prompt authors to explicitly state the valid outputs in some of their prompts. In addition, when working with training prompts that include target text, we found it useful to remove variations on the target format that led to spurious ambiguity. For instance, the target template should only contain the answer to the task. It should not contain any extra text such as "The answer is ...", which can be equivalently moved to the input template.</p>
<p>One of the research question we hope to enable with PromptSource is whether the diversity of the</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: With the Sourcing view, prompt authors can write new prompts, fill in the associated metadata, observe the result on examples, and iterate.
prompt formulation during training leads to models that are more robust to the prompt formulation at test time. Therefore, we encouraged prompt authors to create between 5 and 10 (or more) prompts per dataset while varying the prompt formulation. For a given dataset, authors produce multiple prompts per example, sometimes for task formulations that differed from the original dataset. For instance, for question answering dataset, one prompt can ask to extract the answer to a given question from a given passage, while a second prompt can ask to generate a potential question given an answer and a passage.</p>
<p>As part of the community process and to facilitate future research, PromptSource asks prompt authors to include additional metadata for each prompt. Metadata fields include a name for the prompt, a reference to the paper it was extracted from (or any relevant explanation), whether the prompt expresses the task originally intended by the dataset, the valid outputs (if relevant), whether the input template states the valid outputs, and possible metrics to evaluate the prompted examples. These can be used in future systems to evaluate how the style and structure of prompts leads to different downstream results.</p>
<h2>7 Case Studies</h2>
<p>A system for creating, maintaining, and using prompts is a key tool for supporting the emerging research area of prompting in a standardized and reproducible manner. We highlight three recent research projects for which PromptSource was a key resource.
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Another example of the the Sourcing view, focusing on the editor. The templating language strikes a balance between expressivity and explicit structure. This prompt for QA-ZRE (Levy et al., 2017), a dataset for zero-shot relation extraction, shows how to manipulate strings and do conditional statements with Jinja.</p>
<p>Massively multitask prompted training Sanh et al. (2021) study the question of zero-shot behaviors in large language models and ask whether zero-shot generalization can be induced by training a language model on a massively multitask mixture of prompts. To test this question, they use PromptSource to create diverse prompts for a large collection of NLP datasets. Their training and evaluation prompts are a subset of P3. This work demonstrates that PromptSource allows training a language model on a massively multitask mixture of prompted datasets and evaluating the ability of models trained with such a procedure to perform unseen tasks.
Multilingual prompting Lin et al. (2021) study the zero- and few-shot learning abilities of an multilingual autoregressive language model trained on 30 languages. In particular, they are interested in the cross-lingual generalization of such models and benchmark a variety of tasks in multiple languages. PromptSource allows using a massive set of highquality English prompts. Moreover, the English prompts serve as support to create prompts in other languages (through either machine or human translation).
Priming (in-context learning) Min et al. (2021) study improving models' few-shot priming performance by first fully training a model (with gradient updates) on a multitask mixture formatted with priming examples. They find that incorporating templates from P3 significantly further improves performance compared to training on priming ex-</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: The Helicopter view indicates what datasets have prompts and how many prompts are available for each dataset.
amples alone. Although PromptSource was not originally designed for this specific form of prompting, users were able to easily use P3's template collection and the templating language for their own priming methods.</p>
<h2>8 Conclusion</h2>
<p>PromptSource is an open-source system for creating, sharing, and using natural language prompts and addresses the need for new collaborative and centralized tools to support the emerging research around prompting. The tool is designed to answer three key needs: a flexible template language, a suite of tools for prompt management, and community-driven quality standards. As of January 2022, PromptSource includes a growing collection of 2,000 public prompts for roughly 170 datasets, and has already been an instrumental resource for multiple recent research projects.</p>
<h2>Acknowledgements</h2>
<p>This research was conducted under the BigScience project for open research, ${ }^{4}$ a year-long initiative targeting the study of large models and datasets. The goal of the project is to research language models in a public environment outside large technology companies. The project has over 950 researchers from over 65 countries and more than 250 institutions. The BigScience project was initiated by Thomas Wolf at Hugging Face, and this collaboration would not have been possible without his effort. This research was the focus of the BigScience Prompt Engineering working group, which focused on the role of prompting in large language model training. Disclosure: Stephen Bach contributed to this work as an advisor to Snorkel AI.</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h2>References</h2>
<p>Samuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning. 2015. A large annotated corpus for learning natural language inference. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 632-642, Lisbon, Portugal. Association for Computational Linguistics.</p>
<p>Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual.</p>
<p>Wei-Te Chen and Will Styler. 2013. Anafora: A webbased general purpose annotation tool. In Proceedings of the 2013 NAACL HLT Demonstration Session, pages 14-19, Atlanta, Georgia. Association for Computational Linguistics.</p>
<p>Hamish Cunningham, Diana Maynard, Kalina Bontcheva, and Valentin Tablan. 2002. GATE: an architecture for development of robust HLT applications. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 168-175, Philadelphia, Pennsylvania, USA. Association for Computational Linguistics.</p>
<p>Richard Eckart de Castilho, va Mjdricza-Maydt, Seid Muhie Yimam, Silvana Hartmann, Iryna Gurevych, Anette Frank, and Chris Biemann. 2016. A web-based tool for the integrated annotation of semantic and syntactic structures. In Proceedings of the Workshop on Language Technology Resources and Tools for Digital Humanities (LT4DH), pages 76-84, Osaka, Japan. The COLING 2016 Organizing Committee.</p>
<p>Tianyu Gao, Adam Fisch, and Danqi Chen. 2021. Making pre-trained language models better few-shot learners. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 3816-3830, Online. Association for Computational Linguistics.</p>
<p>Philipp Helfrich, Elias Rieb, Giuseppe Abrami, Andy Lcking, and Alexander Mehler. 2018. TreeAnnotator: Versatile visual annotation of hierarchical text relations. In Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018), Miyazaki, Japan. European Language Resources Association (ELRA).</p>
<p>Douwe Kiela, Max Bartolo, Yixin Nie, Divyansh Kaushik, Atticus Geiger, Zhengxuan Wu, Bertie Vidgen, Grusha Prasad, Amanpreet Singh, Pratik Ringshia, Zhiyi Ma, Tristan Thrush, Sebastian Riedel, Zeerak Waseem, Pontus Stenetorp, Robin Jia, Mohit Bansal, Christopher Potts, and Adina Williams. 2021. Dynabench: Rethinking benchmarking in NLP. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 4110-4124, Online. Association for Computational Linguistics.</p>
<p>Teven Le Scao and Alexander Rush. 2021. How many data points is a prompt worth? In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 2627-2636, Online. Association for Computational Linguistics.</p>
<p>Dong-Ho Lee, Rahul Khanna, Bill Yuchen Lin, Seyeon Lee, Qinyuan Ye, Elizabeth Boschee, Leonardo Neves, and Xiang Ren. 2020. LEAN-LIFE: A labellefficient annotation framework towards learning from explanation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 372-379, Online. Association for Computational Linguistics.</p>
<p>Brian Lester, Rami Al-Rfou, and Noah Constant. 2021. The power of scale for parameter-efficient prompt tuning. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 3045-3059, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.</p>
<p>Omer Levy, Minjoon Seo, Eunsol Choi, and Luke Zettlemoyer. 2017. Zero-shot relation extraction via reading comprehension. In Proceedings of the 21st Conference on Computational Natural Language Learning (CoNLL 2017), pages 333-342, Vancouver, Canada. Association for Computational Linguistics.</p>
<p>Quentin Lhoest, Albert Villanova del Moral, Yacine Jernite, Abhishek Thakur, Patrick von Platen, Suraj Patil, Julien Chaumond, Mariama Drame, Julien Plu, Lewis Tunstall, Joe Davison, Mario ako, Gunjan Chhablani, Bhavitvya Malik, Simon Brandeis, Teven Le Scao, Victor Sanh, Canwen Xu, Nicolas Patry, Angelina McMillan-Major, Philipp Schmid, Sylvain Gugger, Clment Delangue, Tho Matussire, Lysandre Debut, Stas Bekman, Pierric Cistac, Thibault Goehringer, Victor Mustar, Franois Lagunas, Alexander Rush, and Thomas Wolf. 2021. Datasets: A community library for natural language processing. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 175-184, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.</p>
<p>Yanzeng Li, Bowen Yu, Li Quangang, and Tingwen Liu. 2021. FITAnnotator: A flexible and intelligent text annotation system. In Proceedings of the 2021</p>
<p>Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Demonstrations, pages 3541, Online. Association for Computational Linguistics.</p>
<p>Bill Yuchen Lin, Dong-Ho Lee, Frank F. Xu, Ouyu Lan, and Xiang Ren. 2019. AlpacaTag: An active learningbased crowd annotation framework for sequence tagging. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 58-63, Florence, Italy. Association for Computational Linguistics.</p>
<p>Xi Victoria Lin, Todor Mihaylov, Mikel Artetxe, Tianlu Wang, Shuohui Chen, Daniel Simig, Myle Ott, Naman Goyal, Shruti Bhosale, Jingfei Du, Ramakanth Pasunuru, Sam Shleifer, Punit Singh Koura, Vishrav Chaudhary, Brian O'Horo, Jeff Wang, Luke Zettlemoyer, Zornitsa Kozareva, Mona T. Diab, Veselin Stoyanov, and Xian Li. 2021. Few-shot learning with multilingual language models. CoRR, abs/2112.10668.</p>
<p>Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. 2021. Pretrain, prompt, and predict: A systematic survey of prompting methods in natural language processing. CoRR, abs/2107.13586.</p>
<p>Sewon Min, Mike Lewis, Luke Zettlemoyer, and Hannaneh Hajishirzi. 2021. Metaicl: Learning to learn in context. CoRR, abs/2110.15943.</p>
<p>Swaroop Mishra, Daniel Khashabi, Chitta Baral, and Hannaneh Hajishirzi. 2021. Cross-task generalization via natural language crowdsourcing instructions. arXiv preprint arXiv:2104.08773.</p>
<p>Mariana Neves and Jurica eva. 2021. An extensive review of tools for manual annotation of documents. Briefings in bioinformatics, 22(1):146-163.</p>
<p>Frdric Papazian, Robert Bossy, and Claire Ndellec. 2012. AlvisAE: a collaborative web text annotation editor for knowledge acquisition. In Proceedings of the Sixth Linguistic Annotation Workshop, pages 149-152, Jeju, Republic of Korea. Association for Computational Linguistics.</p>
<p>Ethan Perez, Douwe Kiela, and Kyunghyun Cho. 2021. True few-shot learning with language models. NeurIPS.</p>
<p>Jan Wira Gotama Putra, Simone Teufel, Kana Matsumura, and Takenobu Tokunaga. 2020. TIARA: A tool for annotating discourse relations and sentence reordering. In Proceedings of the 12th Language Resources and Evaluation Conference, pages 6912-6920, Marseille, France. European Language Resources Association.</p>
<p>Guanghui Qin and Jason Eisner. 2021. Learning how to ask: Querying LMs with mixtures of soft prompts. In Proceedings of the 2021 Conference of the North</p>
<p>American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 5203-5212, Online. Association for Computational Linguistics.</p>
<p>David Salgado, Martin Krallinger, Marc Depaule, Elodie Drula, Ashish V. Tendulkar, Florian Leitner, Alfonso Valencia, and Christophe Marcelle. 2012. MyMiner: a web application for computer-assisted biocuration and text annotation. Bioinformatics, 28(17):2285-2287.</p>
<p>Victor Sanh, Albert Webson, Colin Raffel, Stephen H. Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja, Manan Dey, M Saiful Bari, Canwen Xu, Urmish Thakker, Shanya Sharma Sharma, Eliza Szczechla, Taewoon Kim, Gunjan Chhablani, Nihal Nayak, Debajyoti Datta, Jonathan Chang, Mike Tian-Jian Jiang, Han Wang, Matteo Manica, Sheng Shen, Zheng Xin Yong, Harshit Pandey, Rachel Bawden, Thomas Wang, Trishala Neeraj, Jos Rozen, Abheesht Sharma, Andrea Santilli, Thibault Fevry, Jason Alan Fries, Ryan Teehan, Stella Biderman, Leo Gao, Tali Bers, Thomas Wolf, and Alexander M. Rush. 2021. Multitask prompted training enables zero-shot task generalization.</p>
<p>Timo Schick and Hinrich Schtze. 2021a. Exploiting cloze-questions for few-shot text classification and natural language inference. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pages 255-269, Online. Association for Computational Linguistics.</p>
<p>Timo Schick and Hinrich Schtze. 2021b. It's not just size that matters: Small language models are also fewshot learners. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 2339-2352, Online. Association for Computational Linguistics.</p>
<p>Taylor Shin, Yasaman Razeghi, Robert L. Logan IV, Eric Wallace, and Sameer Singh. 2020. AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 4222-4235, Online. Association for Computational Linguistics.</p>
<p>Pontus Stenetorp, Sampo Pyysalo, Goran Topi, Tomoko Ohta, Sophia Ananiadou, and Jun'ichi Tsujii. 2012. brat: a web-based tool for NLP-assisted text annotation. In Proceedings of the Demonstrations at the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 102-107, Avignon, France. Association for Computational Linguistics.</p>
<p>Michael Stewart, Wei Liu, and Rachel Cardell-Oliver. 2019. Redcoat: A collaborative annotation tool for hierarchical entity typing. In Proceedings of the</p>
<p>2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP): System Demonstrations, pages 193-198, Hong Kong, China. Association for Computational Linguistics.</p>
<p>Albert Webson and Ellie Pavlick. 2021. Do promptbased models really understand the meaning of their prompts? ArXiv, abs/2109.01247.</p>
<p>Chih-Hsuan Wei, Hung-Yu Kao, and Zhiyong Lu. 2013. PubTator: a web-based text mining tool for assisting biocuration. Nucleic Acids Research, 41(W1):W518W522.</p>
<p>Jason Wei, Maarten Bosma, Vincent Y. Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, and Quoc V. Le. 2021. Finetuned language models are zero-shot learners. CoRR, abs/2109.01652.</p>
<p>Jie Yang, Yue Zhang, Linwei Li, and Xingxuan Li. 2018. YEDDA: A lightweight collaborative text span annotation tool. In Proceedings of ACL 2018, System Demonstrations, pages 31-36, Melbourne, Australia. Association for Computational Linguistics.</p>
<p>Seid Muhie Yimam, Iryna Gurevych, Richard Eckart de Castilho, and Chris Biemann. 2013. WebAnno: A flexible, web-based and visually supported system for distributed annotations. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 1-6, Sofia, Bulgaria. Association for Computational Linguistics.</p>
<p>Tony Z. Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh. 2021. Calibrate before use: Improving few-shot performance of language models. CoRR, abs/2102.09690.</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: Most of the datasets have between 5 and 10 prompts.</p>
<h1>A Data and Statistics</h1>
<p>P3 is the largest public collection of English prompts and is actively growing. As of January 2022, it contains 2'052 English prompts for 170 English datasets (or 269 subsets, one dataset can contain multiple subsets with different prompts). There is an average of 7.6 prompts per data subset and an average 5.6 original-task prompts per data subset (see Figure 6).</p>
<p>P3 was developed as part of the BigScience project for open research ${ }^{5}$. There was a open hackathon to collect prompts for as many English NLP dataset (or English subsets of datasets) as possible. Almost 50 unique contributors affiliated with more than 25 institutions in 10 countries participated.</p>
<h2>B Complete Views</h2>
<p>We show higher resolution examples of the full interfaces for the Browse (Figure 7), Sourcing (Figure 8), and Helicopter (Figure 9) views.</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7: Complete example of the Browse view.</p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 8: Complete example of the Sourcing view.</p>
<p><img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 9: Complete example of the Helicopter view.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{5}$ https://bigscience.huggingface.co&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{3}$ Complete guidelines can be found at https: //github.com/bigscience-workshop/ promptsource/blob/main/CONTRIBUTING.md.&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
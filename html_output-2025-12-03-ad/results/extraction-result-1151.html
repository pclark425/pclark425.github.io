<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1151 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1151</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1151</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-25.html">extraction-schema-25</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <p><strong>Paper ID:</strong> paper-237372527</p>
                <p><strong>Paper Title:</strong> <a href="https://arxiv.org/pdf/2109.00157v2.pdf" target="_blank">A Survey of Exploration Methods in Reinforcement Learning</a></p>
                <p><strong>Paper Abstract:</strong> Exploration is an essential component of reinforcement learning algorithms, where agents need to learn how to predict and control unknown and often stochastic environments. Reinforcement learning agents depend crucially on exploration to obtain informative data for the learning process as the lack of enough information could hinder effective learning. In this article, we provide a survey of modern exploration methods in (Sequential) reinforcement learning, as well as a taxonomy of exploration methods.</p>
                <p><strong>Cost:</strong> 0.035</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1151.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1151.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Bayesian Q-Learning</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Bayesian Q-Learning (Dearden et al., 1998)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A Bayesian extension of Q-learning that maintains posterior distributions over Q-values and uses a value-of-information (VPI) criterion to select actions that trade off immediate reward and information gain about Q-values.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Bayesian q-learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Bayesian Q-Learning agent</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Model-free Q-learning augmented with Bayesian posterior distributions over Q(s,a) (e.g., Dirichlet/Beta approximations); action selection scores actions by E[Q]+VPI(s,a) where VPI is the expected value of perfect information for that Q entry.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>Value of information (Bayesian active information acquisition / posterior-based action selection)</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>Maintains posterior over Q-values and computes the expected gain from learning the true Q(s,a) (VPI); selects actions that maximize expected immediate return plus VPI, thereby adaptively choosing experiments/actions that reduce uncertainty where it matters for future reward.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Toy MDPs (e.g., 5-state chain, 8-state loop, 8x8 maze)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Small, discrete MDPs; fully-observed states in experiments referenced; stochastic/deterministic transitions depending on environment; unknown transition/reward models initially.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>Low — toy MDPs: examples include 5–8 state chains/loops and an 8x8 maze (state spaces of tens of states), small action sets (2–3 actions typically), episodic horizons as in the tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>Reported improved performance on three small toy problems: better sample efficiency and asymptotic convergence vs naive heuristics (experiments in Dearden et al. 1998); no large-scale numeric benchmarks reported in the survey.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td>Baselines using greedy expected Q (no VPI) performed worse in terms of early sample efficiency and total reward in the toy problems (qualitative statements in the survey).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Improved sample efficiency in toy tasks; agent uses posterior to prioritize informative actions so fewer episodes needed to reach competent policies compared to non-Bayesian baselines (no precise numbers in survey).</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Explicitly traded off by summing expected value and VPI (E[Q]+VPI); exploration is driven where the expected future value of information is high.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>Greedy Q, standard Q-learning variants, naive heuristics (as reported in the original experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>Demonstrates that maintaining posteriors over Q and using VPI leads to more efficient exploration in small MDPs by explicitly valuing information that improves future decisions; provides two posterior-estimation mechanisms (moment matching and mixture updates) with different bias/variance properties.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Scales poorly: exact Bayesian posterior maintenance and VPI integration become intractable for large state/action spaces; demonstrated only on small toy MDPs in the referenced experiments.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1151.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1151.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Bayesian Sparse Sampling</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Bayesian Sparse Sampling (Wang et al., 2005)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A sample-based online tree-search planning algorithm for Bayes-adaptive MDPs that expands a non-uniform lookahead tree using sampled successor belief-states and heuristics (e.g., Thompson-sampling-like selection) to focus computation on promising actions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Bayesian sparse sampling for on-line reward optimization</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Bayesian Sparse Sampling planner</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Per-decision online planner that builds a forward search tree by sampling transitions from the current posterior over MDPs; uses heuristics to prioritize expansion and performs Bellman backups on sampled nodes.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>Bayesian lookahead / posterior-sampling-driven tree search (active planning under posterior uncertainty)</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>At each decision the planner samples successor belief-states from the BAMDP posterior and selectively expands the tree under a heuristic (e.g., Thompson-sampling-style selection of promising branches), using rollouts to estimate action values and adaptively allocating computation to informative branches.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Continuous 2D Gaussian-process action-space task (and illustrative continuous-action tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Continuous action (and possibly state) spaces; unknown dynamics modeled by posterior; planning requires handling uncertainty over models; partially unknown dynamics but full state observation during planning.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>Moderate — continuous 2D action spaces, requires sampling many possible successor outcomes during planning; complexity grows with branching and sample budget.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>Shown to yield improved action selection quality compared to uniform sparse sampling, Thompson sampling, Interval Estimation and Boltzmann in the tested continuous 2D GP task (qualitative/improvement in selection quality reported in the survey).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td>Standard sparse sampling and myopic heuristics performed worse in action selection quality in the reported experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Improves planning/sample allocation by focusing sampling on promising actions; effective when Bayesian posteriors are tractable at the root. No single numeric sample-efficiency figure in survey.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Balanced via lookahead under the posterior — the tree search estimates the long-term value of exploration vs exploitation; heuristics favor expanding branches with high posterior promise.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>Sparse sampling, Thompson sampling, Interval Estimation, Boltzmann exploration (as per the cited experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>Adaptive, posterior-driven tree expansion yields better per-decision action selection where Bayesian posteriors are available; demonstrates the value of using the posterior inside lookahead planning to guide exploration decisions.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Requires ability to sample/posterior inference over models at root — expensive or infeasible in large/high-dimensional model classes; computational cost grows with planning budget.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1151.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1151.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BAMCP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Bayes-Adaptive Monte Carlo Planning (BAMCP) (Guez et al., 2012)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An approximate Bayes-optimal planner that combines Monte-Carlo Tree Search (UCT) with root sampling of MDP models from the posterior and lazy sampling of transition parameters to enable sample-efficient planning in Bayes-adaptive MDPs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Efficient bayes-adaptive reinforcement learning using sample-based search</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>BAMCP planner</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>MCTS/UCT-based planner that samples a single MDP from the current posterior at the root and uses that sample for forward simulations in the tree (with lazy sampling of parameters), aggregating returns across many sampled MDP episodes to estimate action values under posterior uncertainty.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>Posterior sampling + Monte-Carlo Tree Search (Bayesian lookahead via sampling models at root)</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>Samples a model from the posterior at each planning episode (root), runs UCT rollouts under that model to evaluate actions, and repeats to average across sampled models; tree policy explores promising branches while integrating model uncertainty, thus adaptively allocating search to informative experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Discrete grid-world (10x10), loop domain (9 states), infinite 2D grid domain, and other discrete MDP benchmarks</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Unknown transition dynamics (modeled via posterior), discrete state/action spaces; can be very large or infinite (infinite grid example), non-tabular in principle but treated via sampling at root.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>From small (9–100 states) to infinite-grid constructs; complexity limited by planning budget, branching factor, and cost of sampling posterior components.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>Empirically outperformed BOSS and BEB on the tested discrete grid-world and loop domains; drastically better performance on an infinite 2D grid domain where baselines could not handle the large state-space (qualitative statements and plots reported in the survey).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td>BOSS, BEB and other baselines struggled or performed worse on the same tasks; baselines often cannot scale to infinite-grid domain.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>BAMCP concentrates planning samples on promising actions via UCT and root-model sampling, achieving better sample utilisation in the experiments; no single numeric sample count reported in the survey.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Handled by MCTS exploration-exploitation (UCT) inside each sampled model and by averaging over multiple sampled models, which implicitly balances exploration (sampling models/branches) and exploitation (favoring branches with high returns under posterior).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>BOSS, BEB, and other Bayesian/optimistic baselines (as in Guez et al. experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>Shows that Monte-Carlo tree search with root sampling and lazy parameter sampling provides an effective, scalable approximation to Bayes-optimal planning, enabling successful planning in very large or infinite discrete domains where exact BAMDP solutions are infeasible.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Performance depends on posterior inference tractability at the root and on the available planning budget; approximations may suffer when model parameter dependence across states is strong and lazy sampling assumptions break.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1151.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1151.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PSRL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Posterior Sampling for Reinforcement Learning (PSRL) (Osband et al., 2013)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A Thompson-sampling-style algorithm for RL that samples an MDP from the posterior at the start of each episode and follows the optimal policy for that sampled MDP for the episode, inducing temporally-extended exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>(More) efficient reinforcement learning via posterior sampling</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>PSRL agent</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Maintains a Bayesian posterior over MDP parameters; at each episode samples a hypothesis MDP from the posterior, computes the optimal policy for the sampled MDP (via planning), and executes that policy for the episode.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>Thompson sampling / posterior sampling (episodic posterior re-sampling)</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>Adaptation occurs by updating the posterior after observed transitions; sampling a new MDP each episode yields exploration driven by posterior uncertainty — actions are chosen according to the optimal policy for a plausible world sampled from belief.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>6-state chain MDP with 3 actions; random MDP with 10 states and 5 actions (as tested in cited experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Finite discrete MDPs with unknown transitions/rewards; episodic evaluation; stochasticity depending on MDP instance.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>Small to moderate (6–10 states in cited experiments), action sets of ~3–5; experiments designed to illustrate long-horizon exploration benefits.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>PSRL outperformed UCRL2 by a large margin in both the 6-state chain and random 10×5 MDPs in the cited experiments (qualitative/plot-based comparisons reported).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td>UCRL2 (optimistic UCB-based algorithm) had significantly worse performance in the cited comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>PSRL exhibited superior sample efficiency in the tested small MDPs due to temporally-coherent exploration (episode-level commitment); no absolute numeric sample counts provided in survey.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Implicit via posterior sampling: exploration arises naturally from sampling plausible MDPs and committing to their solutions across an episode; as posterior concentrates, behavior becomes more exploitative.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>UCRL2, optimistic algorithms, and other baselines in the experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>Posterior sampling with episode-long commitment yields strong empirical performance and scalability advantages versus optimism-based baselines in small MDPs; temporal commitment avoids dithering and leads to deep exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>PSRL requires planning for each sampled MDP which can be computationally expensive in large state spaces; direct application limited to domains where planning under sampled MDPs is tractable.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1151.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1151.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Optimal Probe</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Design for an Optimal Probe (Duff, 2003)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An actor-critic style approximation to Bayes-optimal experimentation where the agent parameterizes value functions over belief (information) state components and uses policy gradients to optimize probe (exploratory) actions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Design for an optimal probe</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Optimal Probe agent (policy-gradient BAMDP approximation)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Actor-critic architecture approximating value over hyperstates (state + posterior parameters) with linear function approximators per base state; policy gradients are used to update parameters based on sampled hyperstate rollouts.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>Approximate Bayes-adaptive planning via parametric actor-critic and Monte-Carlo policy gradients (Bayes-adaptive POMDP formulation approximated)</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>Agent represents information-state features and adapts its policy by gradient updates that consider expected impact of actions on future belief states (i.e., selects probes/actions that yield useful posterior updates given parameterized policy class).</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Toy 2-state, 2-action MDP with horizon 25 (example in Duff's experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Small MDP with unknown transition parameters encoded as Bayesian beliefs; full state observation but latent model uncertainty; finite horizon planning.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>Very low — illustrative 2-state 2-action example with belief parameters (exponential hyperstate growth if solved exactly), horizon ~25 in cited demonstration.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>Shown to be tractable and to produce reasonable probe policies in the 2-state toy problem where exact BAMDP is intractable; no large-scale numeric comparisons provided in the survey.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td>Exact BAMDP dynamic programming (infeasible at scale) would be Bayes-optimal; naive myopic heuristics perform worse in the toy comparisons discussed.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Approximations make computation feasible while retaining some Bayes-adaptive benefits; sample-efficiency gains illustrated qualitatively on toy example (no large-scale numbers in survey).</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Handled by optimizing expected cumulative reward over belief-augmented state (hyperstate) via policy gradient updates: exploration is chosen when it increases long-term expected reward through better posterior knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>Exact BAMDP (infeasible), myopic heuristics in toy examples.</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>Provides a tractable parametric approximation to Bayes-adaptive policies using actor-critic and Monte-Carlo gradients, showing feasibility of approximate Bayes-optimal probe design in very small domains.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Relies on assumed smoothness of value w.r.t. information-state and uses restricted policy classes; scales poorly to large numbers of information-state parameters and larger state spaces.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1151.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e1151.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>VIME</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>VIME — Variational Information Maximizing Exploration (Houthooft et al., 2016)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A curiosity-driven exploration method that gives intrinsic rewards proportional to the information gain (KL divergence) about the agent's dynamics model posterior, approximated with a variational Bayesian model.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Vime: Variational information maximizing exploration</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>VIME-augmented RL agents</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Model-based intrinsic-reward method: learns a Bayesian/variational dynamics model and computes intrinsic reward as the KL divergence between posterior parameter distributions before and after observing new transitions; intrinsic reward is added to extrinsic reward to guide exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>Information gain maximization (variational Bayes approximation of model posterior used to compute intrinsic reward)</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>The agent updates a variational posterior over dynamics model parameters online; the intrinsic reward equals the (approximate) information gain from a transition, encouraging actions that maximally reduce model uncertainty.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Continuous control environments (MuJoCo), simulated continuous domains</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Continuous state/action spaces, unknown stochastic dynamics, sparse/dense external rewards depending on task; model uncertainty critical for exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>Moderate to high — Mujoco tasks with continuous high-dimensional state and action spaces; complexity depends on specific MuJoCo benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>Reported empirical improvements on Mujoco continuous control benchmarks vs TRPO/REINFORCE/ERWR baselines in the cited experiments; information-gain-driven exploration improved learning in sparse-reward continuous tasks (qualitative in survey).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td>Baseline policy-gradient methods without VIME had slower learning and poorer final performance in the cited MuJoCo experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Information-gain intrinsic rewards improved sample efficiency in the tested continuous control tasks (no absolute sample counts in survey summary).</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Intrinsic information-gain rewards are added to extrinsic rewards to encourage exploration when the dynamics model is uncertain; as the model becomes confident, intrinsic bonus decays and policy shifts to exploitation.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>TRPO, ERWR, REINFORCE, and other baselines in the cited work.</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>Demonstrates that variational approximation to information gain can serve as a scalable intrinsic reward in high-dimensional continuous control, improving exploration in Mujoco benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Computational overhead of maintaining and updating a variational posterior; VIME's variational approximations can be challenging to scale or tune and may be sensitive to model misspecification.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1151.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e1151.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MAX (Model-based Active eXploration)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Model-based Active eXploration (MAX) (Shyam et al., 2019)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A model-based active exploration method that quantifies novelty of transitions using divergence measures (Jensen-Shannon / Jensen-Rényi) across an ensemble or distribution of forward models and selects actions that maximize expected novelty.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Model-based active exploration</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>MAX exploration agent</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Maintains a set (ensemble) or distribution of dynamics models; computes a novelty score for candidate actions/transitions via divergences (Jensen-Shannon in discrete, Jensen-Rényi in continuous); selects actions maximizing predicted novelty.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>Active exploration via model uncertainty/novelty maximization (divergence-based information-seeking)</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>The agent trains an ensemble of predictive models; for candidate actions it computes divergence between model predictions and uses that as a novelty signal to choose actions expected to yield high model disagreement, adaptively focusing experiments on uncertain / informative transitions.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Discrete and continuous benchmark tasks (several tasks evaluated in the cited work)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Unknown dynamics, can be discrete or continuous; environments chosen to test exploration where extrinsic rewards may be sparse; full state observations but dynamics unknown.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>Varies — applied to both discrete and continuous domains; complexity depends on model ensemble size and task dimensionality; experiments reported promising scaling.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>MAX reported promising empirical results on several discrete and continuous tasks, outperforming some prior exploration baselines in the cited experiments (survey reports 'promising results').</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td>Baselines without active model-disagreement-driven exploration performed worse on exploration-challenging tasks in the cited experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Adaptive allocation of exploration by maximizing predicted novelty improved sample efficiency relative to naive exploration in reported experiments; no absolute sample counts summarized in survey.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Exploration driven by model disagreement/novelty (intrinsic objective); extrinsic reward can be combined later — MAX focuses on maximizing information gained from experiments rather than immediate reward.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>Compared to other intrinsic-motivation and uncertainty-driven methods; survey notes MAX outperformed some counterparts on discrete and continuous tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>Shows that measuring novelty via divergences across a predictive-model ensemble is an effective and broadly applicable strategy for active exploration in both discrete and continuous domains.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Requires maintaining and training ensembles of dynamics models which raises computational cost; effectiveness depends on model class quality and ensemble diversity; sensitive to stochasticity and unlearnable dynamics.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1151.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e1151.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>IMGEP / GEP-PG</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>IMGEP (Intrinsically Motivated Goal Exploration Processes) and GEP-PG (Colas et al., 2018)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A family of intrinsically-motivated, goal-directed exploration agents that self-generate goals and adapt exploration by sampling goals that maximize learning progress; GEP-PG combines IMGEP goal exploration with DDPG to handle continuous control.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Intrinsically motivated goal exploration processes with automatic curriculum learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>IMGEP / GEP-PG agents</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>IMGEP: meta-policy that samples self-generated goals p and uses a goal-conditioned policy search to attempt them, using intrinsic reward based on learning progress; GEP-PG: combines IMGEP exploration with DDPG policy learning for continuous control.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>Self-generated goals with learning-progress-based adaptive goal sampling (intrinsic curriculum learning)</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>The agent maintains meta-policy Π(θ|p,c) and an intrinsic reward reflecting learning progress for each candidate goal; it samples goals that are currently most informative (high learning progress) and adapts policy parameters to achieve them, thereby adaptively designing experiments/goals.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Real humanoid robot tasks (IMGEP), Continuous Mountain Car and Half-Cheetah (GEP-PG experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>High-dimensional continuous state/action spaces in robot and MuJoCo benchmarks; sparse or structured extrinsic rewards; unknown dynamics; partially long-horizon skills discovery.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>High for humanoid / Half-Cheetah; continuous high-dimensional spaces; GEP-PG reported improved performance especially on Half-Cheetah (higher-dimensional) compared to base DDPG variants.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>IMGEP on a real humanoid robot effectively discovered skills and explored high-dimensional spaces; GEP-PG improved performance, variability and sample efficiency over DDPG in Half-Cheetah in the cited experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td>DDPG variants without IMGEP-style goal exploration underperformed in Half-Cheetah (higher variance and worse sample efficiency as reported in Colas et al.).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Self-generated goal curricula focusing on learning progress improved sample efficiency relative to naive policy search, particularly in higher-dimensional tasks (no absolute numbers given in survey summary).</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Intrinsic goal-sampling prioritizes goals yielding high learning progress (exploration), while DDPG optimization on extrinsic reward handles exploitation; the meta-policy balances which goals to try next.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>DDPG variants, vanilla policy-search baselines; empirical comparisons in Continuous Mountain Car and Half-Cheetah.</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>Self-generated-goal curricula and combining intrinsic goal exploration with off-policy deep RL (GEP-PG) can substantially improve exploration and learning in high-dimensional continuous control tasks with sparse rewards.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Depends on ability to define a suitable goal space and compute learning-progress signals; meta-policy / intrinsic signal design can be task-specific and require tuning.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1151.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e1151.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Meta-RL (RL^2 / black-box)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Black-box Meta-Reinforcement Learning (e.g., RL^2, Duan et al. 2016; Wang et al. 2016a)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Meta-RL approaches that learn an adaptation algorithm (policy over histories) via recurrent architectures so agents learn to explore effectively at test time by embedding posterior inference / exploration heuristics into learned dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>RL 2 : Fast reinforcement learning via slow reinforcement learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>RL^2 / black-box meta-RL agents</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Recurrent policies that take interaction history H_t (past states, actions, rewards) as input and output actions, trained across a distribution of tasks so that the recurrent state implements an online learning algorithm (adaptation mechanism) at test time.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>Meta-learned exploration (learn-to-explore) via black-box recurrent policies (implicit posterior / adaptation encoded in RNN hidden state)</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>During meta-training, the recurrent policy learns to map histories to actions that in aggregate implement effective exploration/exploitation strategies for the task distribution; at meta-test, the same recurrent controller adapts online based on incoming observations/rewards.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Bandits, visual navigation tasks, partially-observable tasks (e.g., hidden-platform tank-of-water), mazes and other meta-learning benchmarks</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Often partially observable or BAMDP-like (unknown dynamics/rewards across episodes), task distribution sampled during meta-train; can be high-dimensional (visual inputs) or structured bandits.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>Varies from simple bandits to complex visual navigation; experiments reported successful adaptation in tasks where classical tabular Bayesian methods are not applicable (e.g., visual domains).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>Black-box meta-RL methods were competitive with classical methods on discrete tasks and superior on visual-navigation and partially-observable tasks; learned exploration strategies could outperform Thompson sampling or simple heuristics in some task families (as reported in the survey).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td>Non-meta baselines (standard RL per-task) were slower to adapt and had worse sample complexity on new tasks from the distribution.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>After meta-training, agents adapt across a few episodes on new tasks (fast adaptation); meta-training itself is sample-intensive but yields test-time sample efficiency improvements.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Implicitly encoded in the learned recurrent policy: the network learns when to probe new actions (explore) vs exploit based on history; not explicitly decomposed into analytic bonus terms.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>Thompson sampling, exploration bonuses, tabular Bayesian learners in discrete settings; MAML and other meta-RL baselines in meta-experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>Meta-learning can produce exploration strategies that are specialized to a task distribution and that adapt online in partially-observable or perceptual tasks that are intractable for analytic Bayesian methods.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Meta-training is computationally and sample intensive; learned strategies may overfit to the training distribution and fail to generalize to very different tasks; credit-assignment for pre-update exploratory actions can be challenging.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1151.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e1151.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MAESN / Inference-based Meta-RL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MAESN (Gupta et al., 2018) and related inference-based meta-RL (Rakelly et al., 2019; Zintgraf et al., 2019a)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Meta-RL methods that learn an embedding/latent context with amortized inference so the agent can perform posterior-like sampling at test time (structured, temporally-coherent exploration via context sampling).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Meta-reinforcement learning of structured exploration strategies</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>MAESN / amortized-inference meta-RL agents</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Meta-learn a policy conditioned on latent context variables; at meta-test, perform approximate posterior inference (amortized or optimization-based) over context and sample latent variables to drive temporally-coherent exploratory behaviors.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>Amortized posterior inference + posterior sampling in latent context (structured stochasticity / latent-guided exploration)</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>Meta-training produces an encoder/variational network that maps histories to a posterior over latent task variables; at test time the agent infers/posterior-samples latent context variables and executes policies conditioned on them, thereby exploring coherently across episodes.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Meta-learning benchmarks including simulated locomotion and other continuous control tasks</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Unknown task parameters drawn from distribution (e.g., varying goals/dynamics), continuous high-dimensional state/action spaces; partial observability of task identity.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>Moderate to high — MuJoCo-style meta-RL tasks, requiring coherent multi-step exploration strategies to identify task parameters.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>MAESN and amortized-inference methods showed faster adaptation and better post-update performance vs baselines (MAML, RL^2) in the cited experiments; structured latent exploration improved learning speed and asymptotic performance.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td>Methods without latent/posterior sampling (e.g., vanilla meta-RL or random exploration) were slower to adapt and less effective.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Good test-time sample efficiency due to inferred/posterior-sampled context enabling informative, coherent exploration; meta-training cost remains high.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Exploration arises from sampling diverse latent contexts that induce coherent behavioral modes; exploitation occurs as posterior over latent concentrates and policy becomes focused on reward.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>MAML, RL^2, non-amortized meta methods; empirical comparisons reported in survey.</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>Amortized inference of task context enables structured, temporally-extended exploration that generalizes across task families and accelerates adaptation at meta-test time.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Quality depends on expressiveness of context embedding and amortized-inference network; may fail if task distribution is too broad or inference is inaccurate.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Bayesian q-learning <em>(Rating: 2)</em></li>
                <li>Bayesian sparse sampling for on-line reward optimization <em>(Rating: 2)</em></li>
                <li>Efficient bayes-adaptive reinforcement learning using sample-based search <em>(Rating: 2)</em></li>
                <li>(More) efficient reinforcement learning via posterior sampling <em>(Rating: 2)</em></li>
                <li>Model-based active exploration <em>(Rating: 2)</em></li>
                <li>Vime: Variational information maximizing exploration <em>(Rating: 2)</em></li>
                <li>Intrinsically motivated goal exploration processes with automatic curriculum learning <em>(Rating: 2)</em></li>
                <li>GEP-PG: Decoupling exploration and exploitation in deep reinforcement learning algorithms <em>(Rating: 2)</em></li>
                <li>RL 2 : Fast reinforcement learning via slow reinforcement learning <em>(Rating: 2)</em></li>
                <li>Meta-reinforcement learning of structured exploration strategies <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1151",
    "paper_id": "paper-237372527",
    "extraction_schema_id": "extraction-schema-25",
    "extracted_data": [
        {
            "name_short": "Bayesian Q-Learning",
            "name_full": "Bayesian Q-Learning (Dearden et al., 1998)",
            "brief_description": "A Bayesian extension of Q-learning that maintains posterior distributions over Q-values and uses a value-of-information (VPI) criterion to select actions that trade off immediate reward and information gain about Q-values.",
            "citation_title": "Bayesian q-learning",
            "mention_or_use": "mention",
            "agent_name": "Bayesian Q-Learning agent",
            "agent_description": "Model-free Q-learning augmented with Bayesian posterior distributions over Q(s,a) (e.g., Dirichlet/Beta approximations); action selection scores actions by E[Q]+VPI(s,a) where VPI is the expected value of perfect information for that Q entry.",
            "adaptive_design_method": "Value of information (Bayesian active information acquisition / posterior-based action selection)",
            "adaptation_strategy_description": "Maintains posterior over Q-values and computes the expected gain from learning the true Q(s,a) (VPI); selects actions that maximize expected immediate return plus VPI, thereby adaptively choosing experiments/actions that reduce uncertainty where it matters for future reward.",
            "environment_name": "Toy MDPs (e.g., 5-state chain, 8-state loop, 8x8 maze)",
            "environment_characteristics": "Small, discrete MDPs; fully-observed states in experiments referenced; stochastic/deterministic transitions depending on environment; unknown transition/reward models initially.",
            "environment_complexity": "Low — toy MDPs: examples include 5–8 state chains/loops and an 8x8 maze (state spaces of tens of states), small action sets (2–3 actions typically), episodic horizons as in the tasks.",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "Reported improved performance on three small toy problems: better sample efficiency and asymptotic convergence vs naive heuristics (experiments in Dearden et al. 1998); no large-scale numeric benchmarks reported in the survey.",
            "performance_without_adaptation": "Baselines using greedy expected Q (no VPI) performed worse in terms of early sample efficiency and total reward in the toy problems (qualitative statements in the survey).",
            "sample_efficiency": "Improved sample efficiency in toy tasks; agent uses posterior to prioritize informative actions so fewer episodes needed to reach competent policies compared to non-Bayesian baselines (no precise numbers in survey).",
            "exploration_exploitation_tradeoff": "Explicitly traded off by summing expected value and VPI (E[Q]+VPI); exploration is driven where the expected future value of information is high.",
            "comparison_methods": "Greedy Q, standard Q-learning variants, naive heuristics (as reported in the original experiments).",
            "key_results": "Demonstrates that maintaining posteriors over Q and using VPI leads to more efficient exploration in small MDPs by explicitly valuing information that improves future decisions; provides two posterior-estimation mechanisms (moment matching and mixture updates) with different bias/variance properties.",
            "limitations_or_failures": "Scales poorly: exact Bayesian posterior maintenance and VPI integration become intractable for large state/action spaces; demonstrated only on small toy MDPs in the referenced experiments.",
            "uuid": "e1151.0"
        },
        {
            "name_short": "Bayesian Sparse Sampling",
            "name_full": "Bayesian Sparse Sampling (Wang et al., 2005)",
            "brief_description": "A sample-based online tree-search planning algorithm for Bayes-adaptive MDPs that expands a non-uniform lookahead tree using sampled successor belief-states and heuristics (e.g., Thompson-sampling-like selection) to focus computation on promising actions.",
            "citation_title": "Bayesian sparse sampling for on-line reward optimization",
            "mention_or_use": "mention",
            "agent_name": "Bayesian Sparse Sampling planner",
            "agent_description": "Per-decision online planner that builds a forward search tree by sampling transitions from the current posterior over MDPs; uses heuristics to prioritize expansion and performs Bellman backups on sampled nodes.",
            "adaptive_design_method": "Bayesian lookahead / posterior-sampling-driven tree search (active planning under posterior uncertainty)",
            "adaptation_strategy_description": "At each decision the planner samples successor belief-states from the BAMDP posterior and selectively expands the tree under a heuristic (e.g., Thompson-sampling-style selection of promising branches), using rollouts to estimate action values and adaptively allocating computation to informative branches.",
            "environment_name": "Continuous 2D Gaussian-process action-space task (and illustrative continuous-action tasks)",
            "environment_characteristics": "Continuous action (and possibly state) spaces; unknown dynamics modeled by posterior; planning requires handling uncertainty over models; partially unknown dynamics but full state observation during planning.",
            "environment_complexity": "Moderate — continuous 2D action spaces, requires sampling many possible successor outcomes during planning; complexity grows with branching and sample budget.",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "Shown to yield improved action selection quality compared to uniform sparse sampling, Thompson sampling, Interval Estimation and Boltzmann in the tested continuous 2D GP task (qualitative/improvement in selection quality reported in the survey).",
            "performance_without_adaptation": "Standard sparse sampling and myopic heuristics performed worse in action selection quality in the reported experiments.",
            "sample_efficiency": "Improves planning/sample allocation by focusing sampling on promising actions; effective when Bayesian posteriors are tractable at the root. No single numeric sample-efficiency figure in survey.",
            "exploration_exploitation_tradeoff": "Balanced via lookahead under the posterior — the tree search estimates the long-term value of exploration vs exploitation; heuristics favor expanding branches with high posterior promise.",
            "comparison_methods": "Sparse sampling, Thompson sampling, Interval Estimation, Boltzmann exploration (as per the cited experiments).",
            "key_results": "Adaptive, posterior-driven tree expansion yields better per-decision action selection where Bayesian posteriors are available; demonstrates the value of using the posterior inside lookahead planning to guide exploration decisions.",
            "limitations_or_failures": "Requires ability to sample/posterior inference over models at root — expensive or infeasible in large/high-dimensional model classes; computational cost grows with planning budget.",
            "uuid": "e1151.1"
        },
        {
            "name_short": "BAMCP",
            "name_full": "Bayes-Adaptive Monte Carlo Planning (BAMCP) (Guez et al., 2012)",
            "brief_description": "An approximate Bayes-optimal planner that combines Monte-Carlo Tree Search (UCT) with root sampling of MDP models from the posterior and lazy sampling of transition parameters to enable sample-efficient planning in Bayes-adaptive MDPs.",
            "citation_title": "Efficient bayes-adaptive reinforcement learning using sample-based search",
            "mention_or_use": "mention",
            "agent_name": "BAMCP planner",
            "agent_description": "MCTS/UCT-based planner that samples a single MDP from the current posterior at the root and uses that sample for forward simulations in the tree (with lazy sampling of parameters), aggregating returns across many sampled MDP episodes to estimate action values under posterior uncertainty.",
            "adaptive_design_method": "Posterior sampling + Monte-Carlo Tree Search (Bayesian lookahead via sampling models at root)",
            "adaptation_strategy_description": "Samples a model from the posterior at each planning episode (root), runs UCT rollouts under that model to evaluate actions, and repeats to average across sampled models; tree policy explores promising branches while integrating model uncertainty, thus adaptively allocating search to informative experiments.",
            "environment_name": "Discrete grid-world (10x10), loop domain (9 states), infinite 2D grid domain, and other discrete MDP benchmarks",
            "environment_characteristics": "Unknown transition dynamics (modeled via posterior), discrete state/action spaces; can be very large or infinite (infinite grid example), non-tabular in principle but treated via sampling at root.",
            "environment_complexity": "From small (9–100 states) to infinite-grid constructs; complexity limited by planning budget, branching factor, and cost of sampling posterior components.",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "Empirically outperformed BOSS and BEB on the tested discrete grid-world and loop domains; drastically better performance on an infinite 2D grid domain where baselines could not handle the large state-space (qualitative statements and plots reported in the survey).",
            "performance_without_adaptation": "BOSS, BEB and other baselines struggled or performed worse on the same tasks; baselines often cannot scale to infinite-grid domain.",
            "sample_efficiency": "BAMCP concentrates planning samples on promising actions via UCT and root-model sampling, achieving better sample utilisation in the experiments; no single numeric sample count reported in the survey.",
            "exploration_exploitation_tradeoff": "Handled by MCTS exploration-exploitation (UCT) inside each sampled model and by averaging over multiple sampled models, which implicitly balances exploration (sampling models/branches) and exploitation (favoring branches with high returns under posterior).",
            "comparison_methods": "BOSS, BEB, and other Bayesian/optimistic baselines (as in Guez et al. experiments).",
            "key_results": "Shows that Monte-Carlo tree search with root sampling and lazy parameter sampling provides an effective, scalable approximation to Bayes-optimal planning, enabling successful planning in very large or infinite discrete domains where exact BAMDP solutions are infeasible.",
            "limitations_or_failures": "Performance depends on posterior inference tractability at the root and on the available planning budget; approximations may suffer when model parameter dependence across states is strong and lazy sampling assumptions break.",
            "uuid": "e1151.2"
        },
        {
            "name_short": "PSRL",
            "name_full": "Posterior Sampling for Reinforcement Learning (PSRL) (Osband et al., 2013)",
            "brief_description": "A Thompson-sampling-style algorithm for RL that samples an MDP from the posterior at the start of each episode and follows the optimal policy for that sampled MDP for the episode, inducing temporally-extended exploration.",
            "citation_title": "(More) efficient reinforcement learning via posterior sampling",
            "mention_or_use": "mention",
            "agent_name": "PSRL agent",
            "agent_description": "Maintains a Bayesian posterior over MDP parameters; at each episode samples a hypothesis MDP from the posterior, computes the optimal policy for the sampled MDP (via planning), and executes that policy for the episode.",
            "adaptive_design_method": "Thompson sampling / posterior sampling (episodic posterior re-sampling)",
            "adaptation_strategy_description": "Adaptation occurs by updating the posterior after observed transitions; sampling a new MDP each episode yields exploration driven by posterior uncertainty — actions are chosen according to the optimal policy for a plausible world sampled from belief.",
            "environment_name": "6-state chain MDP with 3 actions; random MDP with 10 states and 5 actions (as tested in cited experiments)",
            "environment_characteristics": "Finite discrete MDPs with unknown transitions/rewards; episodic evaluation; stochasticity depending on MDP instance.",
            "environment_complexity": "Small to moderate (6–10 states in cited experiments), action sets of ~3–5; experiments designed to illustrate long-horizon exploration benefits.",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "PSRL outperformed UCRL2 by a large margin in both the 6-state chain and random 10×5 MDPs in the cited experiments (qualitative/plot-based comparisons reported).",
            "performance_without_adaptation": "UCRL2 (optimistic UCB-based algorithm) had significantly worse performance in the cited comparisons.",
            "sample_efficiency": "PSRL exhibited superior sample efficiency in the tested small MDPs due to temporally-coherent exploration (episode-level commitment); no absolute numeric sample counts provided in survey.",
            "exploration_exploitation_tradeoff": "Implicit via posterior sampling: exploration arises naturally from sampling plausible MDPs and committing to their solutions across an episode; as posterior concentrates, behavior becomes more exploitative.",
            "comparison_methods": "UCRL2, optimistic algorithms, and other baselines in the experiments.",
            "key_results": "Posterior sampling with episode-long commitment yields strong empirical performance and scalability advantages versus optimism-based baselines in small MDPs; temporal commitment avoids dithering and leads to deep exploration.",
            "limitations_or_failures": "PSRL requires planning for each sampled MDP which can be computationally expensive in large state spaces; direct application limited to domains where planning under sampled MDPs is tractable.",
            "uuid": "e1151.3"
        },
        {
            "name_short": "Optimal Probe",
            "name_full": "Design for an Optimal Probe (Duff, 2003)",
            "brief_description": "An actor-critic style approximation to Bayes-optimal experimentation where the agent parameterizes value functions over belief (information) state components and uses policy gradients to optimize probe (exploratory) actions.",
            "citation_title": "Design for an optimal probe",
            "mention_or_use": "mention",
            "agent_name": "Optimal Probe agent (policy-gradient BAMDP approximation)",
            "agent_description": "Actor-critic architecture approximating value over hyperstates (state + posterior parameters) with linear function approximators per base state; policy gradients are used to update parameters based on sampled hyperstate rollouts.",
            "adaptive_design_method": "Approximate Bayes-adaptive planning via parametric actor-critic and Monte-Carlo policy gradients (Bayes-adaptive POMDP formulation approximated)",
            "adaptation_strategy_description": "Agent represents information-state features and adapts its policy by gradient updates that consider expected impact of actions on future belief states (i.e., selects probes/actions that yield useful posterior updates given parameterized policy class).",
            "environment_name": "Toy 2-state, 2-action MDP with horizon 25 (example in Duff's experiments)",
            "environment_characteristics": "Small MDP with unknown transition parameters encoded as Bayesian beliefs; full state observation but latent model uncertainty; finite horizon planning.",
            "environment_complexity": "Very low — illustrative 2-state 2-action example with belief parameters (exponential hyperstate growth if solved exactly), horizon ~25 in cited demonstration.",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "Shown to be tractable and to produce reasonable probe policies in the 2-state toy problem where exact BAMDP is intractable; no large-scale numeric comparisons provided in the survey.",
            "performance_without_adaptation": "Exact BAMDP dynamic programming (infeasible at scale) would be Bayes-optimal; naive myopic heuristics perform worse in the toy comparisons discussed.",
            "sample_efficiency": "Approximations make computation feasible while retaining some Bayes-adaptive benefits; sample-efficiency gains illustrated qualitatively on toy example (no large-scale numbers in survey).",
            "exploration_exploitation_tradeoff": "Handled by optimizing expected cumulative reward over belief-augmented state (hyperstate) via policy gradient updates: exploration is chosen when it increases long-term expected reward through better posterior knowledge.",
            "comparison_methods": "Exact BAMDP (infeasible), myopic heuristics in toy examples.",
            "key_results": "Provides a tractable parametric approximation to Bayes-adaptive policies using actor-critic and Monte-Carlo gradients, showing feasibility of approximate Bayes-optimal probe design in very small domains.",
            "limitations_or_failures": "Relies on assumed smoothness of value w.r.t. information-state and uses restricted policy classes; scales poorly to large numbers of information-state parameters and larger state spaces.",
            "uuid": "e1151.4"
        },
        {
            "name_short": "VIME",
            "name_full": "VIME — Variational Information Maximizing Exploration (Houthooft et al., 2016)",
            "brief_description": "A curiosity-driven exploration method that gives intrinsic rewards proportional to the information gain (KL divergence) about the agent's dynamics model posterior, approximated with a variational Bayesian model.",
            "citation_title": "Vime: Variational information maximizing exploration",
            "mention_or_use": "mention",
            "agent_name": "VIME-augmented RL agents",
            "agent_description": "Model-based intrinsic-reward method: learns a Bayesian/variational dynamics model and computes intrinsic reward as the KL divergence between posterior parameter distributions before and after observing new transitions; intrinsic reward is added to extrinsic reward to guide exploration.",
            "adaptive_design_method": "Information gain maximization (variational Bayes approximation of model posterior used to compute intrinsic reward)",
            "adaptation_strategy_description": "The agent updates a variational posterior over dynamics model parameters online; the intrinsic reward equals the (approximate) information gain from a transition, encouraging actions that maximally reduce model uncertainty.",
            "environment_name": "Continuous control environments (MuJoCo), simulated continuous domains",
            "environment_characteristics": "Continuous state/action spaces, unknown stochastic dynamics, sparse/dense external rewards depending on task; model uncertainty critical for exploration.",
            "environment_complexity": "Moderate to high — Mujoco tasks with continuous high-dimensional state and action spaces; complexity depends on specific MuJoCo benchmark.",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "Reported empirical improvements on Mujoco continuous control benchmarks vs TRPO/REINFORCE/ERWR baselines in the cited experiments; information-gain-driven exploration improved learning in sparse-reward continuous tasks (qualitative in survey).",
            "performance_without_adaptation": "Baseline policy-gradient methods without VIME had slower learning and poorer final performance in the cited MuJoCo experiments.",
            "sample_efficiency": "Information-gain intrinsic rewards improved sample efficiency in the tested continuous control tasks (no absolute sample counts in survey summary).",
            "exploration_exploitation_tradeoff": "Intrinsic information-gain rewards are added to extrinsic rewards to encourage exploration when the dynamics model is uncertain; as the model becomes confident, intrinsic bonus decays and policy shifts to exploitation.",
            "comparison_methods": "TRPO, ERWR, REINFORCE, and other baselines in the cited work.",
            "key_results": "Demonstrates that variational approximation to information gain can serve as a scalable intrinsic reward in high-dimensional continuous control, improving exploration in Mujoco benchmarks.",
            "limitations_or_failures": "Computational overhead of maintaining and updating a variational posterior; VIME's variational approximations can be challenging to scale or tune and may be sensitive to model misspecification.",
            "uuid": "e1151.5"
        },
        {
            "name_short": "MAX (Model-based Active eXploration)",
            "name_full": "Model-based Active eXploration (MAX) (Shyam et al., 2019)",
            "brief_description": "A model-based active exploration method that quantifies novelty of transitions using divergence measures (Jensen-Shannon / Jensen-Rényi) across an ensemble or distribution of forward models and selects actions that maximize expected novelty.",
            "citation_title": "Model-based active exploration",
            "mention_or_use": "mention",
            "agent_name": "MAX exploration agent",
            "agent_description": "Maintains a set (ensemble) or distribution of dynamics models; computes a novelty score for candidate actions/transitions via divergences (Jensen-Shannon in discrete, Jensen-Rényi in continuous); selects actions maximizing predicted novelty.",
            "adaptive_design_method": "Active exploration via model uncertainty/novelty maximization (divergence-based information-seeking)",
            "adaptation_strategy_description": "The agent trains an ensemble of predictive models; for candidate actions it computes divergence between model predictions and uses that as a novelty signal to choose actions expected to yield high model disagreement, adaptively focusing experiments on uncertain / informative transitions.",
            "environment_name": "Discrete and continuous benchmark tasks (several tasks evaluated in the cited work)",
            "environment_characteristics": "Unknown dynamics, can be discrete or continuous; environments chosen to test exploration where extrinsic rewards may be sparse; full state observations but dynamics unknown.",
            "environment_complexity": "Varies — applied to both discrete and continuous domains; complexity depends on model ensemble size and task dimensionality; experiments reported promising scaling.",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "MAX reported promising empirical results on several discrete and continuous tasks, outperforming some prior exploration baselines in the cited experiments (survey reports 'promising results').",
            "performance_without_adaptation": "Baselines without active model-disagreement-driven exploration performed worse on exploration-challenging tasks in the cited experiments.",
            "sample_efficiency": "Adaptive allocation of exploration by maximizing predicted novelty improved sample efficiency relative to naive exploration in reported experiments; no absolute sample counts summarized in survey.",
            "exploration_exploitation_tradeoff": "Exploration driven by model disagreement/novelty (intrinsic objective); extrinsic reward can be combined later — MAX focuses on maximizing information gained from experiments rather than immediate reward.",
            "comparison_methods": "Compared to other intrinsic-motivation and uncertainty-driven methods; survey notes MAX outperformed some counterparts on discrete and continuous tasks.",
            "key_results": "Shows that measuring novelty via divergences across a predictive-model ensemble is an effective and broadly applicable strategy for active exploration in both discrete and continuous domains.",
            "limitations_or_failures": "Requires maintaining and training ensembles of dynamics models which raises computational cost; effectiveness depends on model class quality and ensemble diversity; sensitive to stochasticity and unlearnable dynamics.",
            "uuid": "e1151.6"
        },
        {
            "name_short": "IMGEP / GEP-PG",
            "name_full": "IMGEP (Intrinsically Motivated Goal Exploration Processes) and GEP-PG (Colas et al., 2018)",
            "brief_description": "A family of intrinsically-motivated, goal-directed exploration agents that self-generate goals and adapt exploration by sampling goals that maximize learning progress; GEP-PG combines IMGEP goal exploration with DDPG to handle continuous control.",
            "citation_title": "Intrinsically motivated goal exploration processes with automatic curriculum learning",
            "mention_or_use": "mention",
            "agent_name": "IMGEP / GEP-PG agents",
            "agent_description": "IMGEP: meta-policy that samples self-generated goals p and uses a goal-conditioned policy search to attempt them, using intrinsic reward based on learning progress; GEP-PG: combines IMGEP exploration with DDPG policy learning for continuous control.",
            "adaptive_design_method": "Self-generated goals with learning-progress-based adaptive goal sampling (intrinsic curriculum learning)",
            "adaptation_strategy_description": "The agent maintains meta-policy Π(θ|p,c) and an intrinsic reward reflecting learning progress for each candidate goal; it samples goals that are currently most informative (high learning progress) and adapts policy parameters to achieve them, thereby adaptively designing experiments/goals.",
            "environment_name": "Real humanoid robot tasks (IMGEP), Continuous Mountain Car and Half-Cheetah (GEP-PG experiments)",
            "environment_characteristics": "High-dimensional continuous state/action spaces in robot and MuJoCo benchmarks; sparse or structured extrinsic rewards; unknown dynamics; partially long-horizon skills discovery.",
            "environment_complexity": "High for humanoid / Half-Cheetah; continuous high-dimensional spaces; GEP-PG reported improved performance especially on Half-Cheetah (higher-dimensional) compared to base DDPG variants.",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "IMGEP on a real humanoid robot effectively discovered skills and explored high-dimensional spaces; GEP-PG improved performance, variability and sample efficiency over DDPG in Half-Cheetah in the cited experiments.",
            "performance_without_adaptation": "DDPG variants without IMGEP-style goal exploration underperformed in Half-Cheetah (higher variance and worse sample efficiency as reported in Colas et al.).",
            "sample_efficiency": "Self-generated goal curricula focusing on learning progress improved sample efficiency relative to naive policy search, particularly in higher-dimensional tasks (no absolute numbers given in survey summary).",
            "exploration_exploitation_tradeoff": "Intrinsic goal-sampling prioritizes goals yielding high learning progress (exploration), while DDPG optimization on extrinsic reward handles exploitation; the meta-policy balances which goals to try next.",
            "comparison_methods": "DDPG variants, vanilla policy-search baselines; empirical comparisons in Continuous Mountain Car and Half-Cheetah.",
            "key_results": "Self-generated-goal curricula and combining intrinsic goal exploration with off-policy deep RL (GEP-PG) can substantially improve exploration and learning in high-dimensional continuous control tasks with sparse rewards.",
            "limitations_or_failures": "Depends on ability to define a suitable goal space and compute learning-progress signals; meta-policy / intrinsic signal design can be task-specific and require tuning.",
            "uuid": "e1151.7"
        },
        {
            "name_short": "Meta-RL (RL^2 / black-box)",
            "name_full": "Black-box Meta-Reinforcement Learning (e.g., RL^2, Duan et al. 2016; Wang et al. 2016a)",
            "brief_description": "Meta-RL approaches that learn an adaptation algorithm (policy over histories) via recurrent architectures so agents learn to explore effectively at test time by embedding posterior inference / exploration heuristics into learned dynamics.",
            "citation_title": "RL 2 : Fast reinforcement learning via slow reinforcement learning",
            "mention_or_use": "mention",
            "agent_name": "RL^2 / black-box meta-RL agents",
            "agent_description": "Recurrent policies that take interaction history H_t (past states, actions, rewards) as input and output actions, trained across a distribution of tasks so that the recurrent state implements an online learning algorithm (adaptation mechanism) at test time.",
            "adaptive_design_method": "Meta-learned exploration (learn-to-explore) via black-box recurrent policies (implicit posterior / adaptation encoded in RNN hidden state)",
            "adaptation_strategy_description": "During meta-training, the recurrent policy learns to map histories to actions that in aggregate implement effective exploration/exploitation strategies for the task distribution; at meta-test, the same recurrent controller adapts online based on incoming observations/rewards.",
            "environment_name": "Bandits, visual navigation tasks, partially-observable tasks (e.g., hidden-platform tank-of-water), mazes and other meta-learning benchmarks",
            "environment_characteristics": "Often partially observable or BAMDP-like (unknown dynamics/rewards across episodes), task distribution sampled during meta-train; can be high-dimensional (visual inputs) or structured bandits.",
            "environment_complexity": "Varies from simple bandits to complex visual navigation; experiments reported successful adaptation in tasks where classical tabular Bayesian methods are not applicable (e.g., visual domains).",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "Black-box meta-RL methods were competitive with classical methods on discrete tasks and superior on visual-navigation and partially-observable tasks; learned exploration strategies could outperform Thompson sampling or simple heuristics in some task families (as reported in the survey).",
            "performance_without_adaptation": "Non-meta baselines (standard RL per-task) were slower to adapt and had worse sample complexity on new tasks from the distribution.",
            "sample_efficiency": "After meta-training, agents adapt across a few episodes on new tasks (fast adaptation); meta-training itself is sample-intensive but yields test-time sample efficiency improvements.",
            "exploration_exploitation_tradeoff": "Implicitly encoded in the learned recurrent policy: the network learns when to probe new actions (explore) vs exploit based on history; not explicitly decomposed into analytic bonus terms.",
            "comparison_methods": "Thompson sampling, exploration bonuses, tabular Bayesian learners in discrete settings; MAML and other meta-RL baselines in meta-experiments.",
            "key_results": "Meta-learning can produce exploration strategies that are specialized to a task distribution and that adapt online in partially-observable or perceptual tasks that are intractable for analytic Bayesian methods.",
            "limitations_or_failures": "Meta-training is computationally and sample intensive; learned strategies may overfit to the training distribution and fail to generalize to very different tasks; credit-assignment for pre-update exploratory actions can be challenging.",
            "uuid": "e1151.8"
        },
        {
            "name_short": "MAESN / Inference-based Meta-RL",
            "name_full": "MAESN (Gupta et al., 2018) and related inference-based meta-RL (Rakelly et al., 2019; Zintgraf et al., 2019a)",
            "brief_description": "Meta-RL methods that learn an embedding/latent context with amortized inference so the agent can perform posterior-like sampling at test time (structured, temporally-coherent exploration via context sampling).",
            "citation_title": "Meta-reinforcement learning of structured exploration strategies",
            "mention_or_use": "mention",
            "agent_name": "MAESN / amortized-inference meta-RL agents",
            "agent_description": "Meta-learn a policy conditioned on latent context variables; at meta-test, perform approximate posterior inference (amortized or optimization-based) over context and sample latent variables to drive temporally-coherent exploratory behaviors.",
            "adaptive_design_method": "Amortized posterior inference + posterior sampling in latent context (structured stochasticity / latent-guided exploration)",
            "adaptation_strategy_description": "Meta-training produces an encoder/variational network that maps histories to a posterior over latent task variables; at test time the agent infers/posterior-samples latent context variables and executes policies conditioned on them, thereby exploring coherently across episodes.",
            "environment_name": "Meta-learning benchmarks including simulated locomotion and other continuous control tasks",
            "environment_characteristics": "Unknown task parameters drawn from distribution (e.g., varying goals/dynamics), continuous high-dimensional state/action spaces; partial observability of task identity.",
            "environment_complexity": "Moderate to high — MuJoCo-style meta-RL tasks, requiring coherent multi-step exploration strategies to identify task parameters.",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "MAESN and amortized-inference methods showed faster adaptation and better post-update performance vs baselines (MAML, RL^2) in the cited experiments; structured latent exploration improved learning speed and asymptotic performance.",
            "performance_without_adaptation": "Methods without latent/posterior sampling (e.g., vanilla meta-RL or random exploration) were slower to adapt and less effective.",
            "sample_efficiency": "Good test-time sample efficiency due to inferred/posterior-sampled context enabling informative, coherent exploration; meta-training cost remains high.",
            "exploration_exploitation_tradeoff": "Exploration arises from sampling diverse latent contexts that induce coherent behavioral modes; exploitation occurs as posterior over latent concentrates and policy becomes focused on reward.",
            "comparison_methods": "MAML, RL^2, non-amortized meta methods; empirical comparisons reported in survey.",
            "key_results": "Amortized inference of task context enables structured, temporally-extended exploration that generalizes across task families and accelerates adaptation at meta-test time.",
            "limitations_or_failures": "Quality depends on expressiveness of context embedding and amortized-inference network; may fail if task distribution is too broad or inference is inaccurate.",
            "uuid": "e1151.9"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Bayesian q-learning",
            "rating": 2,
            "sanitized_title": "bayesian_qlearning"
        },
        {
            "paper_title": "Bayesian sparse sampling for on-line reward optimization",
            "rating": 2,
            "sanitized_title": "bayesian_sparse_sampling_for_online_reward_optimization"
        },
        {
            "paper_title": "Efficient bayes-adaptive reinforcement learning using sample-based search",
            "rating": 2,
            "sanitized_title": "efficient_bayesadaptive_reinforcement_learning_using_samplebased_search"
        },
        {
            "paper_title": "(More) efficient reinforcement learning via posterior sampling",
            "rating": 2,
            "sanitized_title": "more_efficient_reinforcement_learning_via_posterior_sampling"
        },
        {
            "paper_title": "Model-based active exploration",
            "rating": 2,
            "sanitized_title": "modelbased_active_exploration"
        },
        {
            "paper_title": "Vime: Variational information maximizing exploration",
            "rating": 2,
            "sanitized_title": "vime_variational_information_maximizing_exploration"
        },
        {
            "paper_title": "Intrinsically motivated goal exploration processes with automatic curriculum learning",
            "rating": 2,
            "sanitized_title": "intrinsically_motivated_goal_exploration_processes_with_automatic_curriculum_learning"
        },
        {
            "paper_title": "GEP-PG: Decoupling exploration and exploitation in deep reinforcement learning algorithms",
            "rating": 2,
            "sanitized_title": "geppg_decoupling_exploration_and_exploitation_in_deep_reinforcement_learning_algorithms"
        },
        {
            "paper_title": "RL 2 : Fast reinforcement learning via slow reinforcement learning",
            "rating": 2,
            "sanitized_title": "rl_2_fast_reinforcement_learning_via_slow_reinforcement_learning"
        },
        {
            "paper_title": "Meta-reinforcement learning of structured exploration strategies",
            "rating": 2,
            "sanitized_title": "metareinforcement_learning_of_structured_exploration_strategies"
        }
    ],
    "cost": 0.03480925,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>A Survey of Exploration Methods in Reinforcement Learning A Survey of Exploration Methods in Reinforcement Learning</p>
<p>Susan Amin susan.amin@mail.mcgill.ca 
Maziar Gomrokchi gomrokma@mila.quebec 
Harsh Satija harsh.satija@mail.mcgill.ca 
Herke Van Hoof h.c.vanhoof@uva.nl 
Doina Precup dprecup@cs.mcgill.ca </p>
<p>Department of Computer Science
Department of Computer Science
McGill University Mila-Québec Artificial Intelligence Institute Montréal
QuébecCanada</p>
<p>Department of Computer Science
McGill University Mila-Québec Artificial Intelligence Institute Montréal
QuébecCanada</p>
<p>Informatics Institute
McGill University Mila-Québec Artificial Intelligence Institute Montréal
QuébecCanada</p>
<p>Department of Computer Science
University of Amsterdam Amsterdam
the Netherlands</p>
<p>McGill University Mila-Québec Artificial Intelligence Institute Montréal
QuébecCanada</p>
<p>A Survey of Exploration Methods in Reinforcement Learning A Survey of Exploration Methods in Reinforcement Learning
ExplorationReinforcement LearningExploration-Exploitation Trade-offMarkov Decision ProcessesSequential Decision Making
Exploration is an essential component of reinforcement learning algorithms, where agents need to learn how to predict and control unknown and often stochastic environments. Reinforcement learning agents depend crucially on exploration to obtain informative data for the learning process as the lack of enough information could hinder effective learning. In this article, we provide a survey of modern exploration methods in (Sequential) reinforcement learning, as well as a taxonomy of exploration methods.</p>
<p>Introduction</p>
<p>When a reinforcement learning (RL) agent starts acting in an environment, it usually does not have any prior knowledge regarding the task which it needs to tackle. The agent must interact with the environment, by taking actions and observing their consequences (in the * . These authors contributed equally to the work 1 arXiv:2109.00157v2 [cs.LG] 2 Sep 2021 xxxx form of rewards and next states), and then it can use this data to improve its behavior, as measured by the expected long-term return. This reliance on data that it gathers by itself differentiates RL agents from those performing either supervised or unsupervised learning, and it is often a limiting factor in terms of the agent's ultimate success at mastering the environment. Specifically, if the agent only manages to visit a limited portion of the environment, its knowledge will be limited, leading to sub-optimal decision making (Wiering, 1999). However, if the agent focuses on acquiring information regarding parts of the environment that it has not seen sufficiently, it can lose the chance of gaining immediate reinforcement. This problem is referred to as exploration-exploitation trade-off, and is a crucial open problem in reinforcement learning (alongside generalization). Handling this trad-eoff is influenced by several factors, including the dynamics of the environment (i.e. transition probability and reward distribution), properties of the state/action spaces (e.g. discrete/continuous, number of states/actions, . . . ), and the available number of interactions with the environment that the agent is allowed while it is training.</p>
<p>Our goal in this survey is to provide a broad high-level overview on the types of exploration methods employed by RL agents, by reviewing literature from the last three decades. Exploration studies have evolved during this time from simple ideas such as pure randomization, to increasingly effective methods which have interesting theoretical guarantees, or impressive empirical performance in large problems.</p>
<p>Exploration techniques have been categorized generally into undirected and directed methods (Thrun, 1992) based on the choice of information considered by the exploration algorithm. While in undirected exploration strategies, reinforcement learning agents select exploratory actions at random, without using any exploration-specific knowledge, directed exploration methods use the obtained information to pursue the exploration of less-visited state-action pairs, or of state-action pairs that are deemed to be more informative for the agent. However, this is not the only relevant differentiation between current exploration methods, as the field has expanded drastically and more nuances have developed.</p>
<p>In this survey, we present a more detailed categorization of exploration methods in reinforcement learning (see Figure 1). We attempt to group existing algorithmic approaches into these categories in order to provide a more detailed understanding of the current landscape of methods, in terms of both the goals and the information they employ. Of course, a few of the methods do not fall squarely into one category, and hence are included in multiple categories.</p>
<p>We note that some of the discussed techniques in this survey were originally proposed and designed for bandit settings, and only later applied in the reinforcement learning problems. However, this survey is not intended to cover exploration methods specifically designed for bandits, as many references exist in this area, including the excellent recent textbook Lattimore and Szepesvári (2020). We will focus only on sequential decision making, where exploration has an even larger impact, as it controls not only the immediate information received by the agent, but also the potential interesting information in its future data stream. In addition, the very large number of publications in this field, especially in recent years, requires making hard choices regarding which papers to discuss in this survey. In some cases, we have picked particular representative methods for certain approaches, rather than listing all instances of a particular approach. Finally, in order to provide a concise and easy-tounderstand overview of the categories and methods, we do not focus on the mathematical details and theoretical results in the field. Our main goal is to provide an entry point into the field, which is accessible to readers who may want to understand the types of algorithms and empirical evaluations that have been provided, and to practitioners who want to build successful applications, and hence have to tackle this really difficult problem.</p>
<p>The survey is organized as follows. In the following section, the notation is introduced and a brief RL background is provided. The exploration categories are subsequently presented in Section 3. Section 4 presents exploration methods that do not use reward information at all. Section 5 presents methods that rely mainly on randomizing action choices. Section 6 presents methods that are based on optimism in the face of uncertainty. Section 7 introduces methods which start with the formulation of the optimal exploration-exploitation trade-off, and then approximate its solution. Section 8 discusses probability matching methods, including posterior sampling. Finally, Section 9 provides some conclusions and perspectives.</p>
<p>Notation and Background</p>
<p>In reinforcement learning problems, at each discrete time step t = 0, 1, 2, . . . , the system is at state s t and the agent interacts with the environment by selecting action a t . Consequently, the system transitions from the state s t to s t+1 , determined by the transition model of the system P and returns a numerical reward r t . Upon receiving the reward, the agent decides on the next action a t+1 and the process continues. Depending on the type of the problem, the decision making process either stops when it reaches a terminal state in an episodic task or continues in a continuing task with an infinite horizon. In the following paragraphs, we introduce the commonly used notation in this survey. Note that capital and calligraphic letters denote random variables and sets, respectively, unless stated otherwise. For a comprehensive introduction to reinforcement learning, refer to Sutton and Barto (1998a).</p>
<p>Markov Decision Processes</p>
<p>In this survey, the problems are modeled as Markov decision processes (MDPs) M = S, A, P, r , where S and A denote sets of all possible states and actions in the system, respectively. MDPs assume that the environment is Markovian; i.e. the transition probability distribution P : S × A → D (S) determines the next state from the probability distribution over the set of states D (S) as a function of the current state-action pair only. In other words, in MDPs we have, P S t+1 = s | s t , a t = P S t+1 = s | s t , a t , s t−1 , a t−1 , . . . , s 0 , a 0 .</p>
<p>(1)</p>
<p>If the agent starts from state s and takes an action a, it transitions to the state s with the probability, P s, a, s = P S t+1 = s | S t = s, A t = a .</p>
<p>The expected rewards can be written as the reward function r : S × A → R, which maps the current state-action pair (s, a) to the immediate reward obtained from the set of real numbers R in the system,
r (s, a) = E [R t+1 | S t = s, A t = a] ,(3)
xxxx where the expectation is with respect to the randomness induced by the reward function r.</p>
<p>All the sequence of observations, actions and any kind of information the agent obtains during its lifetime is called history,
H t = S 0 , A 0 , R 0 , S 1 , A 1 , R 1 , . . . , S t , A t , R t .(4)
A trajectory τ is defined as the sequence of information extracted from the history H T for the horizon T .</p>
<p>Reinforcement Learning Setting</p>
<p>In reinforcement learning setting, agent behaves according to a policy π ∈ Π, where Π represents the set of all possible policies. A deterministic policy π : S → A at time step t, also represented as a t = π (s t ), returns a particular action a t , while a stochastic policy π (a t | s t ) gives a probability distribution over a set of actions (π : S → D (A)), defined as π (a t | s t ) = P (A t = a t | S t = s t ) .</p>
<p>(5)</p>
<p>The ultimate goal of reinforcement learning is to find an optimal policy π ∈ Π, which maps states to actions that lead to the maximization of the expected (discounted) cumulative future reward J π J π = E P,π [G] ,</p>
<p>where G denotes the return. For continuing tasks with infinite horizon, where the task never ends, G is defined as the discounted cumulative reward
G = ∞ t=0 γ t R t ,(7)
where γ ∈ [0, 1) is the discount factor, and is used to determine and control the importance of the future rewards. In the cases with finite horizon T (episodic tasks), the return G can be modified to the undiscounted version
G = T t=0 R t .(8)
The expected return for action selection policy π given the initial state S 0 = s, is called the state value function V π (s), written as
V π (s) := E P,π [G | S 0 = s] = E P,π ∞ t=0 γ t R t | S 0 = s ,(9)
and is related to the expected return as J π = E P [V π (S)]. An alternative to state value function is action value function Q π (s, a), which considers state-action pair (s, a) instead of state s only. In fact, it gives the value of taking action a in state s under a given policy π, and is defined as Q π (s, a) = E P,π [G | s, a] = E P,π ∞ t=0 γ t R t | s, a .</p>
<p>Among all possible true value functions V π for different policies π ∈ Π, there exists an optimal value function V corresponding to an optimal policy π , which is at least as large as the others defined as, V (s) := max π V π (s)</p>
<p>for all s ∈ S. The optimal policy π (s) for all s ∈ S is thus a solution to max π V π (s).</p>
<p>Similarly, an optimal action value function Q can be defined for taking action a while being in state s. The optimal state value function V and optimal action value function Q are related as V (s) = max a Q (s, a) .</p>
<p>The optimal policy π (s) can thus be written as π (s) = arg max a Q (s, a) .</p>
<p>Here, we introduced the general notation used throughout this survey. The notation specific to each section will be introduced in its respective category. In the next section, we propose a method of categorization for exploration techniques in reinforcement learning.</p>
<p>Categorization of Exploratory Techniques</p>
<p>Efficient exploration has been acknowledged as an important problem in adaptive control for quite a few decades, starting with the literature on bandit problems, eg. Thompson (1933). In this survey, however, we will not discuss the bandit literature, which is vast and has been the topic of the recent book Lattimore and Szepesvári (2020). Instead, we focus on sequential decision making. Some of the early studies that acknowledged the importance of efficient exploration in this context were delivered by Mozer and Bachrach (1990); Sutton (1990); Moore (1990); Schmidhuber (1990) and Barto et al. (1991). A study by Mozer and Bachrach (1990) showed that efficient learning of tasks modeled by a finitestate automaton is achievable using simple random exploration, provided that the number of states is small enough, while learning more complex tasks requires a more intelligent exploration technique that accelerates the coverage of the state space. In another study, Sutton (1990) demonstrated that the selection of sub-optimal actions (exploration) in nonstationary maze domains is essential for efficient learning, even though it may negatively affect the short-run acquisition of rewards. Exploration techniques have been categorized in a few studies mainly based on the choice of inclusion of information in pursuing exploration. For instance, in a technical report, Moore (1990) emphasizes the interplay between the exploration-exploitation balance and efficient learning. He categorizes exploratory moves based on the action selection method into entirely random, local random, and sceptical categories. His proposed categorization states that while in the entirely-random method the agent chooses the exploratory actions totally at random, in the local-random experimentation it selects actions from the perturbed best known actions. As the very first action in the local-random method is chosen completely at random, the performance of this method is very sensitive to the quality of the first chosen action. Thus, a wrong decision at the initial step may lead to much larger learning times. The sceptical exploration method chooses actions depending on the prediction for the best xxxx known action. If it is predicted to be unsuccessful, the agent explores other actions, and selects the best known action otherwise. Although this categorization of exploration methods can explain the similarities and differences between some of the early proposed approaches, it does not provide a general foundation for classifying exploration techniques.</p>
<p>One of the first general categorization of the exploration methods was introduced by Thrun (1992). He grouped exploration techniques into two general categories: undirected and directed methods. The undirected or uninformed exploration methods do not use any sort of exploration-specific knowledge in order to perform the exploration task. These methods generally rely solely on randomness in selecting actions. Random walk is the simplest method in this category, which is believed to be first utilized in action selection mechanisms by Anderson (1986), Munro (1987), Mozer and Bachrach (1990), Jordan (1989), Nguyen and Widrow (1990) and Thrun et al. (1991). Other examples of undirected methods consist of the exploration techniques related to Boltzmann distribution (based on utility and temperature parameter for controlling the exploration-exploitation trade-off) (Barto et al., 1991;Watkins, 1989;Lin, 1992;Singh, 1992;Sutton, 1990;Lin, 1990) and random action selection with a certain probability (Whitehead and Ballard, 1991;Mahadevan andConnell, 1992, 1991). On the contrary, directed or informed exploration methods utilize exploration-specific knowledge of the learning process to direct the agent towards exploring the environment. These exploration techniques are more efficient and beneficial compared with the undirected exploration methods in terms of complexity and cost (Thrun, 1992;. In particular, random exploration methods may lead to an increase in the learning time as well as safety issues due to the random selection of unsafe actions repeatedly (especially in real cases, such as in robotics). Consequently, in the following years, exploration-related studies focused increasingly on "directed" exploration methods. The consequent diversity in these approaches necessitates the provision of a new basis for a refined categorization of these methods.</p>
<p>To address the issues corresponding to the existing categorization methods, we propose a new classification approach based on the type of information the agent uses to explore the world ( Figure 1). In particular, we categorize the RL exploration methods into the two general classes, namely "Reward-Free Exploration", where the included exploration techniques do not use the extrinsic rewards in their action selection process, and "Reward-Based Exploration", in which extrinsic rewards affect the choice of exploratory actions. These two classes are further divided into two groups "Memory-Free" and "Memory-Based" techniques, depending on the reliance of the exploratory movements on the agent's memory of the observed space. The categories in each class of exploration techniques are described below and detailed in the following sections.</p>
<p>• Reward-Free Exploration -The general property of the exploration methods included in this category is that rewards (or value functions) do not affect the choice of actions in their action-selection criteria. In other words, actions are selected without regard to the obtained rewards or the value functions. The methods belonging in this section can either act completely blindly, which we call blind exploration, or utilize some sort of information (other than extrinsic rewards) in the form of intrinsic rewards in order to encourage exploration. This type of exploration methods are referred to Figure 1: Exploration Categories-The exploration methods are categorized into two main groups reward-free and reward-based exploration techniques, depending on their utilization of extrinsic rewards. Each group is further divided to memory-based and memory-free categories based on the reliance of the exploratory decisions on the agent's memory of the observed space.</p>
<p>as Intrinsically-Motivated Exploration techniques. The details regarding this category and its subcategories are provided in section 4.</p>
<p>• Randomized Action Selection -The exploration methods in this category induce exploratory behaviour via assigning action selection probabilities to the admissible actions based on the estimated value functions or rewards (Value-Based Exploration), or the learned policies (Policy-Search Based Exploration). The exploration methods included in the former group use the reward-based feedback in order to handle the exploration-exploitation trade-off. The list of the exploration techniques in this category as well as the detailed explanation of each method are provided in section 5.1. In the latter group, exploration methods explore the environment via performing search in the space of policies. They learn a stochastic policy, whose stochasticity helps the agent balance the trade-off between exploration and exploitation in the system. These methods explicitly represent policies, and aim to update them to maximize the expected extrinsic rewards (section 5.2). Note that the policy-search based methods that do not utilize extrinsic rewards in their exploratory decision making are listed and discussed in section 4.</p>
<p>• Optimism/Bonus-Based Exploration -The exploration techniques in this category function based on the principle of optimism in the face of uncertainty, where actions with uncertain values are preferred over the rest of the possible actions. In this category of exploration methods, the methods usually involve a form of bonus, which is added to the extrinsic reward, leading to a directed search in the spaces of state-action. The methods included in this category and the details are provided in section 6. The main difference between bonus-based techniques and the intrinsicallymotivated exploration methods, discussed in section 4, is that the latter does not utilize extrinsic reward for motivating the exploration of the environment. Different forms xxxx of optimism/bonus-based exploration approaches are discussed in section 6, including count-based exploration methods and prediction-error based approaches. Finally, the clear distinction between the methods in this category and those in the Stochastic Action Selection methods (5) is that the techniques considered as Optimism/Bonus-Based direct the agent's moves with the use of bonuses, while the methods in the other group rely solely on the extrinsic rewards.</p>
<p>• Deliberate Exploration -This category includes exploration methods that operate based on solving the exploration-exploitation tradeoff optimally and is discussed in section 7. This category consists of Bayes-Adaptive exploration methods that are realized with a Bayesian model-based set-up, where the posterior distribution over models is computed and updated assuming a prior over the transition dynamics. This group also consists of Meta-Learning Based Exploration techniques, via which the agent learns to adapt quickly using the prior given tasks.</p>
<p>• Probability Matching -This category of exploration techniques uses a heuristic to decide the next action based on sampling a single instance from the posterior belief over environments or value functions, and solving for that sampled environment exactly. The agent can then act according to that solution, e.g. for the duration of one episode. Each action is thus taken with the according to the probability the agent considers it to be the optimal action. This heuristic effectively directs exploration effort to promising actions.</p>
<p>In terms of the categorization proposed by Thrun (1992), we can classify our proposed categories into the two general groups of directed and undirected exploration techniques. In this regard, in the reward-free exploration category, blind exploration methods (section 4.1) are undirected, while the intrinsically-motivated exploration techniques (section 4.2) are considered directed exploration approaches. The stochastic action selection exploration category (section 5.1) consists of undirected techniques. The rest of the categories fall under the directed exploration category. In the following sections, the above mentioned groups of exploration techniques are discussed in more detail and the methods under each group are explained. In a few cases among the exploration techniques, there exist some approaches that belong to two of the proposed categories, leading to a small overlap between the groups. Whenever such situation is encountered, the respective exploration method is noted as shared between the corresponding categories.</p>
<p>Reward-Free Exploration</p>
<p>We use the notion of reward-free exploration in reinforcement learning to describe any method of exploration that does not incorporate extrinsic reward in their exploratory action selection criteria. This type of exploration methods was first introduced and utilized with the name pure exploration in multi-armed bandits, a set of sequential decision-making tasks where at each time step, an agent pulls an arm and receives a random reward drawn from the reward distribution of that specific arm (Bubeck et al., 2009). In particular, these exploration techniques do not incorporate the rewards obtained from the environment (i.e. extrinsic rewards) in measuring the cost of picking bandit arms. Instead, they utilize other available resources in a limited budget, such as CPU time or cost, in order to acquire knowledge. Similarly, there have been reward-free exploration methods proposed for the reinforcement learning framework, which do not rely on the extrinsic rewards the agent receives from the environment. These exploration techniques can be: 1) completely blind, where the exploratory agent selects actions in the absence of any sort of information obtained as the result of its interaction with the environment; or 2) driven by some form of intrinsic motivation and curiosity. These two forms of reward-free exploration methods are further explained, and the corresponding proposed methods are discussed in the following paragraphs and listed in Table 1. Note that in neither of the aforementioned cases, the agent uses extrinsic reward as a source of knowledge. Thus, bonus-based methods (i.e. exploration methods that rely on a sort of bonus reward in addition to extrinsic reward) do not belong in this category and are discussed in Optimism/Bonus-Based Exploration section (section 6).</p>
<p>Blind Exploration</p>
<p>Exploration techniques in the blind exploration category explore environments solely on the basis of random action selection. In other words, these agents are not guided through their exploratory path by any form of information, thus are uninformed or blind. This category of exploration techniques is indeed the most basic type of reward-free exploration, and includes random-walk as the simplest exploration method (Thrun, 1992). Some examples of the early uses of random walk in exploring the effect of various actions on different states were the studies performed by Anderson (1986); Mozer andBachrach (1990) andJordan (1989). In the random walk method, the agent chooses actions randomly regardless of the information it has obtained so far and thus, due to the uniformly random probability of selecting actions, there is a chance that the picked action takes the agent away from the goal rather than taking it closer (i.e. exploration). On the other hand, it leads to large complexity that grows exponentially with the size of the environment (Whitehead, 1991), which makes random-walk an inefficient exploration technique.</p>
<p>Another simple yet effective exploration method in this category is known as the method of -greedy (Sutton, 1996), also known as max-random or pseudo-stochastic (Caironi and Dorigo, 1994;Watkins, 1989). In the -greedy approach, the parameter ∈ [0, 1] controls the balance between exploration and exploitation. The action a t at every time step t is chosen such that,
a t = a t ,
with probability 1random action with probability ,</p>
<p>where a t is the greedy action taken at time t with respect to the greedy policy (exploitation). The -greedy method has been found to be quite effective in different RL settings (Sutton and Barto, 1998b). In particular, it is efficient in the sense that it does not need to cache any data to perform exploration, and the only hyperparameter to adjust is . However, despite the fact that the -greedy method guarantees that at infinite time horizon every state-action pair is visited infinitely often, it stays sub-optimal in the sense that it asymptotically prevents the agent from selecting the best action (Vermorel and Mohri, 2005). Another problem an xxxx agent may encounter while using -greedy is the lack of decisiveness in the exploration phase, which might in turn lead to getting stuck in local optima. To address this issue, a temporally extended form of -greedy, called z-greedy (Dabney et al., 2020), has been proposed, where random exploratory actions are replaced by temporally-extended sequence of actions. In particular, the z-greedy agent exploits with probability 1 − and explores via repeating the same action for a certain number of steps n ∼ z, where z(n) is a distribution over the actionrepeat duration n. Dabney et al. (2020) perform experiments in tabular as well as deep RL frameworks in tasks with discrete-action spaces, and tabular or discretized continuous state spaces.</p>
<p>Although is usually hand-tuned depending on the type of the problem, there are other proposed extensions of the -greedy method, such as -first (Tran-Thanh et al., 2010) (where exploration is done during the first T time steps-T is the total number of steps) and decreasing-in bandits (Caelen and Bontempi, 2007), where is a decreasing function of time, as well as the derandomization of -greedy in RL tasks (Even-Dar and Mansour, 2002). Another extension of the -greedy approach is the Value-Difference Based Exploration (VDBE) method (Tokic (2010) in Bandits and Tokic and Palm (2011) in reinforcement learning), which adjusts the exploration rate based on the changes in the state-action value functions. The methods Even-Dar and Mansour (2002); Tokic (2010); Tokic and Palm (2011), which incorporate extrinsic rewards in their exploratory decision making, are discussed in detail in section 5.1. There are other blind exploration methods mainly proposed in the field of Robotics, for instance the spiral search technique (Burlington and Dudek, 1999), which ensures visiting new locations in planar environments via expanding the search in the space with the use of logarithmic spirals. However, due to the study limit of these methods to Robotics and planar environments, we are not going to cover them here.</p>
<p>Intrinsically-Motivated Exploration</p>
<p>The second exploration type in the reward-free exploration category is the intrinsically motivated exploration, which is composed of the methods that utilize a form of intrinsic motivation in the absence of external rewards to promote exploring the unexplored parts of the environment. In contrast to blind exploration, intrinsically-motivated exploration techniques utilize some form of intrinsic information to encourage exploring the state-action spaces. The idea of employing internal incentives in exploratory tasks is borrowed from intrinsically-motivated behaviour in humans, which has been studied and discussed extensively in education and psychology literature (Deci, 1971(Deci, , 1975Amabile et al., 1976;Benware and Deci, 1984;Deci and Ryan, 1985;Grolnick and Ryan, 1987). In psychology, the distinction between an extrinsically and an intrinsically motivated behaviour is made based on the types of the stimuli that "move" the person to perform a task (Ryan and Deci, 2000). While intrinsic motivation leads to an inherent satisfaction of performing a job, extrinsic motivation sets an external regulation, which encourages a person to do a task in order to obtain some separable outcome (e.g. reward or reinforcement). Studies show that internalization of the external regulations, also known as self-regularization or self-determination (Deci and Ryan, 1985), helps children attain higher achievements (Benware and Deci, 1984;Grolnick and Ryan, 1987) in terms of learning or completing complex tasks, in contrast to using extrinsic motivations in the form of rewards or reinforcement. Analogs of intrinsic motivation in human can be employed in order to promote exploration in the RL framework. Here, we review some of these studies and discuss different forms of intrinsic motivation that have been used in exploration techniques in the reinforcement learning tasks.</p>
<p>In the context of reinforcement learning, curiosity takes various interpretations and forms depending on the types of problems and the defined goals as well as the approaches taken toward understanding and solving the problems. In general, we can define curiosity as a way or desire to explore new situations that may help the agent with pursuing goals in the future. As agents are encountered with deceptive and/or sparse rewards in many RL setups, exploratory agents that rely on extrinsic rewards might end up in local optima because of deceptive rewards or get stuck due to zero gradient in the received rewards. Thus, the techniques that intrinsically motivate the agent to explore the environment and do not rely on the extrinsic reinforcement are effective in learning of such tasks.</p>
<p>Many of the reward-free intrinsically-motivated exploration strategies aim at minimizing the agent's uncertainty or error in its predictions (Schmidhuber, 1991a,b;Pathak et al., 2017). In order to evaluate the precision of the agent's predictions of the environment behavior, a model of the environment dynamics is required, such that given the current state and the chosen action, the model predicts the next state. Minimization of the resulting error in the model prediction encourages the exploration of the underlying space. There are other reward-free techniques that pursue the maximization of space coverage (new states or state-action pairs visitation), which utilize a form of intrinsic motivation to govern the agent's exploratory behaviour (Hazan et al., 2019;Amin et al., 2020). More space coverage essentially means visiting more unexplored states in a shorter amount of time and in turn, learning more about the environment. In this section, we survey and discuss some of the reward-free intrinsically-motivated exploration approaches that seek either of the abovementioned goals. Note that the notion of intrinsic motivation in RL tasks has been also used in combination with external rewards, which is not the subject of our discussion in this section and will be elaborated in the "Bonus/Optimism-Based Exploration" category (section 6).</p>
<p>The early use of intrinsic motivation in computational framework dates back to 1976, when Lenat (1976) used the notion of "interestingness" in mathematics to encourage new concepts and hypotheses. Scott and Markovitch (1989) introduced DIDO, a curiosity-driven learning strategy that can help the agent explore initially unknown domains in an unsupervised set-up using the notion of Shannon's uncertainty function defined as
sh = − n i=1 (p i × log 2 (p i )) ,(15)
where p i is an estimate of the probability of the outcome O i , and the summation is taken over all outcomes. Minimization of uncertainty in their proposed formalism leads to a broader searching span, as well as more effective learning of the search space. DIDO, in fact, promotes the idea of using an experience generator that provides experiences, which are novel compared to the previous ones and are related to them at the same time. The obtained experiences help the agent search for a better representation in the space of possible representations while DIDO's representation generator is employed in finding more informative experiences. Scott and Markovitch (1989) performed DIDO in several discrete domains, which showed that their exploratory algorithm enables the agent to select sensible experiences and eventually leads to a good representation of the domain. In the following years, another form of curiosity-driven exploration was introduced (Schmidhuber, 1991a,b) based on the improvement in the reliability of the RL agent's predictions of the world model. In particular, Schmidhuber (1991b) proposed a model-building control system that could provide an adaptive model of the environmental dynamics. He further proposed the notion of dynamic curiosity and boredom, described as "the explicit desire to improve the world model", as a potential means of increasing the knowledge of the animat about the world in the exploration phase. In his work, curiosity aims at minimization of the agent's ignorance and is triggered when the agent comes to the realization that it does not have enough knowledge of something. It provides a source of reinforcement for the agent and is defined as the Euclidean distance between the real and the predicted model network. A failure in the correct prediction of the environment leads to a positive reinforcement that encourages the agent to further explore the corresponding actions. Improvement in the world model predictions with time leads to less reinforcement and thus, discouragement of exploring the corresponding actions, also referred to as boredom.</p>
<p>While the idea of using curiosity was not implemented in Schmidhuber (1991b), Schmidhuber later utilized the notion of adaptive curiosity (Schmidhuber, 1991a) to encourage exploration of the unpredictable parts of the environment. In particular, he proposed a curious model-building control system, where the notion of adaptive confidence was used for modeling the reliability of a predictor's predictions, and adaptive curiosity was utilized to reward the agent for encountering hard but learnable states and thus improve the exploration phase by reducing the extra time spent on the non-useful or well-modelled parts of the environment. The strength of his proposed approach in comparison to its predecessors including his previous study (Schmidhuber, 1991b), lies in its ability to work in uncertain non-deterministic environments by adaptively modeling the reliability of a predictor's predictions and learning to predict cumulative error changes in the model network. This goal can be achieved via maximization of the expectation of cumulative changes in prediction reliability. Schmidhuber (1991a) tested his proposed curiosity-driven algorithm based on Watkin's Q-learning in two-dimensional discrete-state toy environments with over 100 states and compared the results to the ones obtained using random search as the exploratory approach. Utilizing an adaptive curious agent led to a decrease of an order of magnitude in the learning time.</p>
<p>Another similar yet different exploration approach was proposed by Thrun and Möller (1992) around the same time, which suggested using the notion of competence map for guiding exploration via estimating the controller's accuracy. In particular, Thrun and Möller (1992) introduced a notion of energy
E = (1 − Γ)E explore + ΓE exploit ,(16)
where gain parameter 0 &lt; Γ &lt; 1 controls the exploration-exploitation trade-off and is a function of the change in the exploration energy E explore and the exploitation energy E exploit . A competence network system is trained to estimate the upper bound of the "model network error"; minimization of the "expected competence" leads to the exploration of the world. Thrun and Möller (1992) employed "competence map" in a continuous two-dimensional robot navigation task, which revealed that their suggested exploration method can perform better compared with "random walk" and "purely greedy" approaches. An exploration approach was later proposed by Storck et al. (1995) as an extension of previous similar studies, such as Schmidhuber (1991a,b); Thrun and Möller (1992). Their proposed exploration method, called Reinforcement Driven Information Acquisition (RDIA), is devised for non-deterministic environments and utilizes the notion of information gain, which is used as an intrinsic motivation to govern the agent's exploratory movements. In particular, the RDIA agent models the environment via estimating the transition probability p ijk (t) at each time step t as the ratio of the number of times so far that the pair (s i , a j ) has led to the state s k over the number of times the agent has experienced (s i , a j ). The information gain is then defined as the difference between the agent's current estimation of the transition probability p ijk (t) and p ijk (t + 1) at time t + 1. Information gain represents the information that the agent has acquired upon performing the respective action, which consequently leads to an increase in the estimator's accuracy. Storck et al. (1995) assess their proposed method in simple discrete environments with certain numbers of states and actions using two different information gain measures, namely the entropy difference and the Kullback-Leibler (KL) distance, between the probability distributions, and show that the results obtained by RDIA surpass those of simple random search.</p>
<p>Information gain as intrinsic motivation has been used in other exploration strategies such as the studies performed by Little and Sommer (2013) and Mobin et al. (2014). In particular, the exploration method proposed by Little and Sommer (2013) takes a Bayesian approach, where the agent builds an internal model of the environment, and upon taking an action and observing the next state, calculates the KL-divergence of its current internal model from the one it had predicted prior to taking the action. The resulting unweighted sum of the KL-divergences yields the missing information I M , which is utilized as a measure of inaccuracy in the agent's internal model. The agent subsequently takes actions that maximize the expected (predicted) information gain (PIG), defined as the expected decrease in the missing information I M between the internal models. Another similar exploration method (Mobin et al., 2014) extends the application of PIG (Little and Sommer, 2013) to perform in environments with unbounded discrete state spaces. In particular, Mobin et al. (2014) utilize the Chinese Restaurant Process (CRP) (Aldous, 1985) to find the probability of revisiting a state or discovering a new one, and use the obtained results to calculate the agent's internal model. The subsequent steps are similar to those presented and discussed by Little and Sommer (2013). Another study by Shyam et al. (2019) introduces the Bayesian Model-based Active eXploration (MAX) method, which utilizes the novelty of transitions as a learning signal, and is applicable in discrete and continuous environments. In particular, MAX agent calculates the Jenson-Shannon divergence and the Jensen-Rényi divergence (Rényi et al., 1961) of the predicted space of distributions from the resulting one in discrete and continuous environments, respectively. Maximization of the resulting novelty measure governs the agent's exploratory behaviour. The evaluation of MAX performance in several discrete and continuous tasks presents promising results compared with those obtained from MAX counterparts and other baselines.</p>
<p>Another curiosity-driven approach is "Intrinsic Curiosity Module" (ICM) (Pathak et al., 2017), where curiosity is defined as "the error in an agent's ability to predict the consequence of its own actions". In their set-up, the agent interacts with high-dimensional continuous xxxx Approach Intrinsic Remarks Motivation Anderson (1986) None (Blind) Early use of random walk Mozer and Bachrach (1990) None (Blind) Early use of random walk Jordan (1989) None (Blind) Early use of random walk Sutton (1996) None (Blind) -greedy Caironi and Dorigo (1994) None (Blind) max-random ( -greedy) Dabney et al. (2020) None (Blind) z-greedy (Temporally-extended actions) Burlington and Dudek (1999) None (Blind) Spiral search (For planar environments only) Schmidhuber (1991b) Uncertainty Adaptive model of the environment dynamics Scott and Markovitch (1989) Uncertainty</p>
<p>Minimization of Shannon's uncertainty function Schmidhuber (1991a) Uncertainty Prediction of cumulative error changes Thrun and Möller (1992) Uncertainty Competence map Storck et al. (1995) Uncertainty Space coverage Encouraging new policies using a distance measure between the policies Table 1: Examples of some reward-free exploration approaches.</p>
<p>state spaces (images in this case). The authors show that ICM helps the agent to learn and improve its exploration policy in the presence of sparse extrinsic rewards as well as in the absence of any sort of environmental rewards. Moreover, they show that the curious agent can apply its gained knowledge and skills in new scenarios and still achieve improved results. The main idea behind ICM is that instead of targeting learnable states and rewarding the agent for detecting them (Schmidhuber, 1991b,a), ICM (Pathak et al., 2017) focuses only on a feature representation that reflects the parts of the environment that either affect the agent or get affected by the agent's choice of actions. Intuitively, by focusing on the influential feature space instead of the state space, ICM is able to avoid the unpredictable or unlearnable parts of the environment.</p>
<p>Note that in some of the fore-mentioned proposed algorithms (Schmidhuber, 1991b,a;Pathak et al., 2017), while the external reinforcement is not a necessary component, the external reward, if exists, can be added to the curious reinforcement. Thus, these studies are included in the list of "Bonus/Optimism-Based Exploration" category as well (section 6).</p>
<p>Some of the intrinsically-motivated exploration techniques utilize the analogy between the dynamical and physical systems, and thus propose the notion of entropy maximization to encourage exploration of the search space. In this regard, an early utilization of entropy in intelligent adaptation of search control was in the context of search effort allocation problem in genetic search procedures (Rosca, 1995), where entropy was used as a measure of diversity. Around the same time, Wyatt (1998) defined a notion of entropy for the case of bandits as a measure of the uncertainty regarding the identity of the optimal action. In his proposed exploration algorithm, the agent selects the more informative action, which is the one that on expectation will lead to a larger entropy reduction. In the field of reinforcement learning (which is the main focus of the current survey), there are several studies that utilize notion of entropy in their proposed exploration techniques (Achbany et al., 2006;Yin, 2002;Hazan et al., 2019). In this section, however, we discuss the method proposed by Hazan et al. (2019) as it is the only reward-free technique among the studies that utilize the notion of entropy in guiding exploration. Hazan et al. (2019) introduce an exploration approach, which targets environments that do not provide the agent with extrinsic reward. In their proposed method, the intrinsic objective is to maximize the entropy of the distribution over the visited states. They introduce an algorithm, which optimizes objectives that are only functions of the state-visitation frequencies. In particular, it generates and optimizes a sequence of intrinsic reward signals, which consequently leads to the entropy maximization of the distribution that the policy induces over the visited states. The reward signals form a concave reward functional R(d π ), which is a function of the entropy of the induced state distribution d π given policy π. The obtained optimal policy is referred to as maximumentropy (MaxEnt) exploration policy, which is defined as π ∈ arg max π R(d π ).</p>
<p>Another intrinsically-motivated exploration approach that encourages space coverage is the method of PolyRL (Amin et al., 2020), which is designed for tasks with continuous state and action spaces and sparse reward structures. PolyRL is inspired by the statistical models used in the field of polymer physics to explain the behaviour of simplified polymer models. In particular, PolyRL exploration policy selects orientationally correlated actions in the action space and induces persistent trajectories of visited states (locally self-avoiding xxxx walks) in the state space using a measure of spread known as the radius of gyration squared,
U 2 g (τ S ) := 1 T e − 1 s∈τ S d 2 (s,τ S ).(17)
In equation 17, T e denotes the number of exploratory steps taken so far in the current exploratory trajectory, τ S is the trajectory of the visited states, and d(s,τ S ) is a measure of distance between a visited state s and the empirical mean of all visited statesτ S . At each time step, the exploratory agent computes U 2 g (τ S ), and subsequently compares it with the value obtained from the previous step. In addition, it calculates the high-probability confidence bounds on the radius of gyration squared, within which the stiffness of the trajectory is maintained. If the change in U 2 g (τ S ) is within the confidence interval, the agent continues to explore, otherwise it selects the subsequent action using the target policy. Amin et al. (2020) assess the performance of PolyRL in 2D continuous navigation tasks as well as several high-dimensional sparse MuJoCo tasks and show improved results compared with those obtained from several other exploration techniques.</p>
<p>Policy-Search Based Exploration without Extrinsic Reward -Policy-search methods search in the parameter space θ for the appropriate parameterized policy π θ . As policy-search methods typically do not learn value functions, the choice of a proper parameter θ is essential for ensuring an efficient, stable and robust learning. This calls for an efficient exploration strategy in order to provide the policy evaluation step in the policy search methods with new trajectories and thus new information, which is subsequently used for policy update (Deisenroth et al., 2013). The exploration approaches performed in policy search methods use stochastic policies, and they can either employ rewards obtained from the environment to guide the exploratory trajectories or function in a completely reward-free manner. In the current section, we review the policy-search methods that do not incorporate extrinsic rewards in their exploratory decision making. We provide a more thorough introduction to policy-search methods in section 5.2, where we discuss the policy-search approaches that utilize extrinsic rewards.</p>
<p>One of the approaches to solving problems autonomously is breaking the problem/goal into smaller sub-problems/sub-goals (Forestier et al., 2017). This idea is inspired from the way children tend to select their objectives such that they are not too easy or too hard for them to handle. These intermediate learned goals facilitate learning more complex goals, which ultimately lead to building up more skills required to achieve bigger goals. Based on this intuition, Forestier et al. (2017) propose a curiosity-driven exploration algorithm called "Intrinsically Motivated Goal Exploration Process" (IMGEP). The IMGEP approach structure relies on assuming that the agent is capable of choosing goal p from the space of RL problem and is able to calculate the corresponding reward r using the reward function R (p, c, θ, o τ ) given the parameterized policy π θ , context c (which gives the current state of the environment) and the observed outcome o τ in the trajectory τ = {s t 0 , a t 0 , s t 1 , a t 1 , . . . , s t end , a t end }. The reward function R (p, c, θ, o τ ) is thus non-Markovian and can be calculated at any time during or after performing the tasks. Using the computed rewards, the agent samples the interesting goal p, which is a self-generated goal that leads to faster learning progress. In the exploration phase, the agent uses the meta policy Π (θ|p, c) to find the parameter θ for goal p, which is subsequently utilized in a goal-parameterized policy search process. The obtained outcome is then used in computing the intrinsic reward r, which in turn provides useful information regarding the interestingness of the samples goal p. The goal sampling strategy and the meta-policy are subsequently updated. The performance of IMGEP in the case of a real humanoid robot shows that the IMGEP robot can effectively explore high-dimensional spaces through discovering skills with increasing complexity.</p>
<p>One of the exploration methods proposed based on the "Goal Exploration Processes" (GEPs) (Forestier et al., 2017) is the "Goal Exploration Process-Policy Gradient" (GEP-PG) (Colas et al., 2018), which combines the intrinsically-motivated exploration processes GEPs with the deep reinforcement learning method DDPG in order to improve exploration in continuous state-action spaces and learn the tasks. Colas et al. (2018) perform GEP-PG in the low-dimensional "Continuous Mountain Car" and the higher-dimensional "Half-Cheetah" tasks. GEP-PG is tested in the fore-mentioned problems with different variants of DDPG. The authors show that specifically in the Half-Cheetah task, the performance, variability and sample efficiency of their proposed method surpasses those of DDPG.</p>
<p>Another policy-search based exploration strategy proposed by Machado et al. (2017) utilizes the notion of proto-value functions (PVFs) to discover options that lead the agent towards efficient exploration of the state space. PVFs, first introduced by Mahadevan (2005), are the basis functions used for approximating value functions through incorporating topological properties of the state space. In particular, using the MDP's transition matrix, a diffusion model is generated, whose diagonalized form subsequently gives rise to PVFs. The diffusion model provides the diffusion information flow in the environment. This feature allows the PVFs to provide useful information regarding the geometry of the environment, including the bottlenecks. Machado et al. (2017) define an intrinsic reward function (a.k.a. eigenpurpose) as,
r e in s, s = e φ s − φ (s) ,(18)
where e ∈ R |S| is the proto-value function and φ(s) is the feature representation of state s, which can be replaced by the state s itself in tabular cases. Machado et al. utilize eigenpurpose r e i to discover options (a.k.a. eigenoptions) and their corresponding eigenbehaviors. They subsequently use policy iteration to solve the problem for an optimal policy.</p>
<p>Although the exploration technique introduced by Machado et al. (2017) is applicable to discrete domains only, one of its major advantages is that it provides a dense intrinsic reward function, which facilitates exploration tasks with sparse-extrinsic-reward structures. Moreover, since it is equipped with options, their proposed method can cover a relatively larger span in the state space compared with that of a simple random walk. Finally, the authors show that their method with options is effective for exploration tasks with the goal of maximizing the cumulative rewards. Later work by Machado et al. (2018b) proposed an improved version of eigenoption discovery, extending it to stochastic environments with nontabular states. In particular, using the notion of successor representation Dayan (1993), in their proposed method the agent learns the non-linear representation of the states which in turn gives the diffusive information flow (DIF) model, and subsequently, the eigenpurposes and eigenoptions are obtained.</p>
<p>In the recent years, Jinnai et al. (2019) introduced the method of covering options, where options are generated with the goal of minimizing cover time. Their proposed method xxxx encourages the agent to visit less-explored regions of the state space by generating options for those parts, without using the information obtained from extrinsic rewards. Their empirical evaluation in discrete sparse-reward domains present reduced learning time in comparison with that of some of their predecessors. The method of deep covering options (Jinnai et al., 2020) extends covering options to large or continuous state spaces while minimizing the agent's expected cover time in the state space. The authors have successfully shown the behaviour of deep covering options in challenging sparse-reward tasks, including Pinball, as well as some MuJoCo and Atari domains.</p>
<p>In a study by Hong et al. (2018), the authors apply a diversity-driven method to off-and on-policy DRL algorithms and improve their performances in large state spaces with sparse or deceptive rewards via encouraging the agent to try new policies. In particular, the agent uses a distance measure to evaluate the novelty of π in comparison to the prior ones and subsequently modifies the loss function,
L D = L − E π ∈Π αD π, π ,(19)
where L denotes the loss function of the deep RL algorithm, α is a scaling factor, and D is a distance measure between the current policy π and the policy π sampled from a set of most recent policies Π . Equation 19, encourages the agent to try new policies and explore unvisited states without relying on the extrinsic rewards received from the environment. The agent will consequently overcome the problem of getting stuck in local optima due to the presence of deceptive rewards or failing to learn tasks with sparse rewards or large state spaces. The authors apply their proposed exploration method in 2D grid worlds with deceptive or sparse rewards, Atari 2600 as well as MuJoCo, and show that it enhances the performance of DRL algorithms through a more effective exploration strategy.</p>
<p>Randomized Action Selection</p>
<p>So far, we have introduced and discussed exploration methods that do not acquire information in the form of extrinsic rewards in the process of exploratory action selection. In this section and the rest of the survey, we focus on the exploration techniques that make decisions using extrinsic rewards with or without other forms of information obtained from the learned process. In this section, we specifically target the exploration methods that assign action selection probabilities to the admissible actions based on value functions/rewards or the learned policies. The two groups of exploration methods are introduced in sections 5.1 and 5.2, respectively.</p>
<p>Value-Based Methods</p>
<p>A simple way to deal with the exploration-exploitation trade-off is to induce exploratory behaviour via assigning action selection probabilities to the admissible actions based on the estimated value functions. In the early phase of learning, the agent should be able to try different actions in each state. Later in the intermediate learning phase, if the agent's target policy takes the control of the action selection process, it may lead to a partial visitation of the state space, and thus a sub-optimal policy and value function. To tackle this issue, there are exploration approaches that select the stochastic actions based on the feedback they receive, in the form of value function or rewards, from the environment. Using these feedback, they balance exploration and exploitation via deciding between acquiring more knowledge from the environment and maximizing the obtained rewards. In this section, some of the exploration methods that perform action selection according to the abovementioned criteria are discussed below. Examples of some value-based randomized action selection exploration methods are provided in Table 2.</p>
<p>One of the most important exploration methods in this category is the Softmax action selection method (Bridle, 1990). In this method, the greedy action at the current state is selected with the highest probability, while the other actions are given the probability of being selected according to their estimated values. The most commonly used formalism for performing Softmax action selection is the Boltzmann distribution. The early use of Boltzmann distribution in exploration was by Watkins (1989); Lin (1992) and Barto et al. (1991). In the Boltzmann exploration approach, the value function Q t (s, a) for the current state S t = s and action a t = a assigns the probability of selecting action a as,
π t (a|s) = e Qt(s,a)/Tm N i=1 e Qt(s,a i )/Tm ,(20)
where T m &gt; 0 is called the temperature and controls how frequent the agent will choose random actions as opposed to the best actions a t = arg max a Q t (s, a). If T m decreases, the probability of generating the action with the highest expected reward a t increases, leading to a decrease in the probability of selecting other actions and hence a lower probability of exploring the environment. In the limit of zero temperature T m → 0, the agent uses the target policy to select greedy actions in order to maximize the obtained rewards. At very large temperatures T m → ∞, all of the actions have almost the same probability and the action selection process approaches random walk. Setting the value for the temperature T is not straightforward, but it is generally reduced during the experiments, leading to more exploitation over time.</p>
<p>One of the extended applications of the Softmax exploration method is in the multiobjective reinforcement learning tasks (Vamplew et al., 2017), where scalar rewards are replaced with vector-valued rewards. Each element in the vectors represent the reward corresponding to an objective. The action-value function Q(s, a) is consequently presented in the form of vectorQ(s, a). In order to use the vector representation of the value functions in the Boltzmann formalism, the vectors must be mapped to scalar values using a scalarization function f (Q(s, a)) (Liu et al., 2015), which can have either linear (Vamplew et al., 2017;Guo et al., 2009;Perez et al., 2009) or non-linear (Gábor et al., 1998;Van Moffaert et al., 2013a,b) representations. The Boltzmann formalism is consequently written as,
π (a|s) = e f (Qt(s,a))/T N i=1 e f (Qt(s,ai))/T .(21)
Another extension of Softmax exploration is the Max-Boltzmann rule (Wiering, 1999), which is a combination of the -greedy or Max-random approach (explained in detail in section 4.1) and the Boltzmann exploration method. In the Max-Boltzmann method, similar to the -greedy approach, the action that gives the maximum Q-value is chosen with the probability 1− . With the probability , the Boltzmann distribution (equation 20) is used for xxxx</p>
<p>Approach</p>
<p>Remarks Bridle (1990) Softmax Watkins (1989) Early use of Boltzmann distribution Lin (1992) Early use of Boltzmann distribution Barto et al. (1991) Early use of Boltzmann distribution Vamplew et al. (2017) Vectorization of rewards in multi-objective tasks Wiering (1999) Max-Boltzmann ( -greedy + Boltzmann) Tokic (2010) Value-Difference Based Exploration (VDBE) Tokic and Palm (2011) Extension of VDBE (VDBE + Max-Boltzmann) Tijsma et al. (2016) Controls and increases the probability of greedy action selection with time Even-Dar and Mansour (2002) Derandomization of -greedy Table 2: Examples of value-based randomized action selection exploration methods. action selection. A drawback for the Max-Boltzmann exploration method compared to the two approaches, Max-Random and Boltzmann, is the need for tuning two hyperparameters T and instead of one. However, the Max-Boltzmann method has been shown to reduce the weight of exploration in comparison with exploitation, and thus avoid over-exploration.</p>
<p>Another exploration technique in this category is the incremental Q-learning algorithm (Even- Dar and Mansour, 2002), which is an extension of the -greedy method (discussed in section 4). In particular, in their proposed method, Even-Dar and Mansour (2002) derandomize the -greedy method by adding a promotion term to the estimated Q-value for each state-action pair. If action a is not taken in state s at time t, the promotion term of that specific state-action pair at time t + 1 is increased by a value called promotion function, and zeroed otherwise. The promotion function plays the role of a decreasing in the -greedy approach, and decreases with the number of times action a = a has been taken in state s. Consequently, the values of actions that have not been taken are promoted and the fraction of time the sub-optimal actions are chosen decreases with time and vanishes in the limit.</p>
<p>Another extension of the -greedy method is the Value-Difference Based Exploration (VDBE) (Tokic, 2010), where the parameter in the -greedy approach is replaced with a state-dependent exploration probability t (s) instead of being hand-tuned. In VDBE, the initialization of the exploration probability t (s) is done with o (s) = 1 for all states. At each time-step, the TD-error is computed, which serves as a measure for the agent's uncertainty. A larger TD-error for a state corresponds to a larger uncertainty, which consequently triggers a higher chance of exploration by assigning a larger value to the exploration rate t (s) for that specific state.</p>
<p>Although Tokic (2010) assesses VDBE in multi-armed bandit problems, he argues that the method of VDBE is also applicable in multi-state MDPs. In another study, Tokic and Palm (2011) introduce an extension of VDBE, namely VDBE-Softmax, for solving reinforcement learning problems with multiple states. The VDBE-Softmax method combines VDBE (introduced by Tokic (2010)) with Max-Boltzmann exploration (proposed by Wiering (1999)) in order to overcome the shortcomings of the two former approaches. A disadvantage of VDBE, as stated by Tokic and Palm (2011), is that it does not discriminate between exploratory actions, which leads to equal probability of selecting the actions that yield high and low Q-values. Another disadvantage of VDBE is its divergence in the cases where oscillations exist in the value functions caused by stochastic rewards or function approximators. In the VDBE-Softmax approach, with probability t (s), the agent selects the exploratory actions using equation 20, and chooses the argmax of Q-value (greedy action) with the probability 1 − t (s). The authors show that in general, their proposed variation of VDBE performs better than the exploration methods -greedy, Softmax and pure VDBE in the environments with deterministic rewards (cliff-walking problem and bandit-world tasks) as well as the ones with stochastic rewards (bandit-world tasks).</p>
<p>Similar to the method of VDBE (Tokic, 2010), there exist other exploration strategies, which were originally proposed for and applied in the bandit problems, but were later performed in multi-state MDPs as well. One of these approaches is called Pursuit (Thathachar and Sastry, 1984), which maintains the probability of selecting greedy action as well as the value of different actions at each time step. As described in Sutton and Barto (1998b), in k-armed bandit problems, Pursuit algorithms initialize the probability of choosing an arm with p t=0 (a) = 1/k for all actions a = 1, . . . , k. At each time step t, the probability of selecting action a is calculated as
p t+1 (a) = p t (a) + α (1 − p t (a)) if a = arg max i Q t (i) p t (a) + α (0 − p t (a)) Otherwise(22)
where α ∈ (0, 1) is the learning rate. According to equation 22, the probability of selecting the greedy action increases with time, leading to fewer exploratory moves and more exploitation. The equivalence of equation 22 for the case of multi-state MDPs is given as follows (Tijsma et al., 2016)
π t+1 (s t , a t ) = π t (s t , a t ) + α (1 − π t (s t , a t )) if a t = a t π t (s t , a t ) + α (0 − π t (s t , a t )) Otherwise(23)
where a t = arg max a Q(s t , a). Tijsma et al. (2016) perform pursuit as well as other exploration techniques including Softmax and -greedy in random discrete stochastic mazes with one optimal goal and two sub-optimal goal states. Their results show that Softmax outperforms the other exploration strategies and that the -greedy method has the worst performance of all.</p>
<p>Policy-Search Based Methods</p>
<p>After having discussed methods that implicitly represent a policy using value function, we turn our attention to policy search methods. Policy search methods explicitly represent a policy, instead of, or in addition to, a value function. In the latter case, these methods are more specifically referred to as actor-critic methods. Most policy search methods learn a stochastic policy. The stochasticity in the policy is usually also the main driver of exploration in such methods. The different ways in which such perturbations can be applied will be the focus of the next several sections 1 : after this short introduction, Section 5.2.1 will introduce the organizational principle for the section, with Sections 5.2.2 and 5.2.3 explaining the individual methods in detail.</p>
<p>Many policy search methods belong to the policy gradient family. These methods aim to update the policy in the direction of the gradient of the expected return ∇J π . A basic way to do so is by calculating a finite-difference approximation of the gradient. In this approach, rollouts are performed for one or more perturbations of the original parameter vector, which are then used to estimate the gradient. When the system is non-deterministic, these estimates are extremely noisy, although better estimates can be obtained in simulation when the stochasticity of the environment is controlled by fixing the sequence of pseudorandom numbers (Ng and Jordan, 2000).</p>
<p>More sophisticated approaches are based on the log-ratio policy gradient (Williams, 1992). The log-ratio policy gradient relies on stochastic policies, and exploits the knowledge of the policy's score function (gradient of the log-likelihood). Stochastic policies for continuous actions based on the Gaussian distribution (Williams, 1992) are still frequently used (Deisenroth et al., 2013). For discrete action spaces, a Gibbs distributions with a learned energy function (Sutton et al., 2000) can be used instead.</p>
<p>The initialization of the exploration policy can be freely chosen. In some policy architectures, the amount of exploration is fixed to some constant or decreased according to a set schedule. In other architectures, the amount of exploration is controlled by learned parameters, possibly separate from other parameters (such as those controlling the 'mean action' for any given state). Policy search methods typically maximize the expected return, and thus probability mass tends to slowly be shifted towards a more greedy policy (usually resulting in a decreasing amount of exploration). These and more advanced strategies will be discussed in more detail in Sec. 5.2.4.</p>
<p>The discussed approaches differ in an important aspect: while in finite-difference methods (Ng and Jordan, 2000) the parameters of the policy are perturbed, the method proposed by Williams (1992) selects the actions stochastically. Where perturbations are applied at the level of parameters, they often affect an entire episode (episode-based perturbations). In contrast, classically action-space perturbations are often only applied for a single time step (independent perturbations).</p>
<p>In this section, we will focus on research in the area of policy search methods that introduce new exploration strategies or that explicitly evaluate the effects of different exploration strategies. We will focus on such policy search methods that are trained on a single task and where the policy has its own representation. Policies that are defined only in terms of value function are covered in Section 5.1. Policies explicitly optimizing over a distribution of tasks are covered with Bayesian and meta-learning approaches in Section 7. An overview of the methods we will cover in this section is given in Table 3. The table groups the method by type and coherence of perturbation that, like Deisenroth et al. (2013), we consider to be key characteristics of exploration strategies in policy search. The following subsection will give a more detailed explanation of these characteristics.</p>
<p>Approach</p>
<p>Perturbed space Temporal coherence * Remarks Barto et al. (1983) Action-space Independent - Gullapalli (1990) Action-space Independent -Williams (1992) Action-space Independent -Morimoto and Doya (2001) 
Action-space Correlated Multi-modal (hierarchy) Nachum et al. (2019) Action-space Correlated - Wawrzynski (2015)
Action-space Correlated -Lillicrap et al. (2016) Action-space Correlated -Haarnoja et al. (2017) Action-space Independent Multi-modal Xu et al. (2018) Action-space Independent - Kohl and Stone (2004) Parameter-space Episode-based -Sehnke et al. (2010) Parameter-space Episode-based - Rückstiess et al. (2010) Parameter-space Episode-based -"</p>
<p>Action-space Episode-based - Theodorou et al. (2010) Parameter-space Episode-based -Stulp and Sigaud (2012) Parameter-space Episode-based Correlated parameters Salimans et al. (2017) Parameter-space Episode-based - Conti et al. (2018) Parameter-space Episode-based van Hoof et al. (2017) Parameter
-space † Correlated † - Plappert et al. (2018) Parameter-space † Episode-based † - Fortunato et al. (2018) Parameter-space † Episode-based † - Mahajan et al. (2019)
Parameter-space Episode-based Multi-agent Table 3: Different exploration approaches proposed in the context of policy search algorithms. The first section of the table lists methods that mainly perturb the policy in the action space, these methods will be discussed in Sec. 5.2.2. The second section lists methods that mainly perturb the policy in the parameter space, that will be discussed in Sec. 5.2.3. Within these two broad categories, papers are ordered roughly chronologically, although papers within a similar line of work are kept together. Multiple entries for the same paper refer to different variants. * Denotes whether perturbations are applied independently at each timestep, don't change at all throughout an episode, or have an intermediate correlation structure. Details in Sec. 5.2.1. † These methods have additional step-based action-space noise for numeric reasons or to ensure a differentiable objective.</p>
<p>Perturbed space and coherence</p>
<p>In policy based methods, exploratory behavior is usually obtained by applying random perturbations. One of the main characteristics that differentiate exploration methods is where those perturbations are applied. Within policy gradient techniques, there are two main candidates: either the actions or the parameters are perturbed (although we will discuss some approaches beyond these two shortly). These possibilities reflect two views on the learning problem. On the one hand, the actions that are executed on the system are what actually affects the reward and the next state, no matter which parameter vector generated the actions. From this perspective, it is more straightforward to start with finding good actions, and subsequently find a parametric policy that can generate them. From another perspective, parametrized policies have limited capacity, and the resulting inductive bias might mean that the true optimal policy is excluded from the set of representable policies. We are then looking for parameters with which the overall policy behavior is best across all states. Also, if the structure of the policy is chosen to reflect prior information about the structure of the solution (e.g. policies linear in hand-picked features or policies with a hierarchical structure), perturbing the policy parameters ensure that explorative behavior follows the same structure.</p>
<p>The space in which explorative behaviors are applied is usually closely linked to the coherence of behavior. Coherence here refers to the question of whether (and how closely) perturbations in subsequent time steps depend on one another. Exploration with a low coherence (e.g., perturbations chosen independently at every time step) has the advantage that many different strategies might be tried within a single episode. On the other hand, exploration with a high coherence (e.g., perturbing the policy at the beginning of each episode only) has the advantage that the long-term effect of following a certain policy can be evaluated . Whereas independent perturbations can result in inefficient random walk behavior, following a perturbed policy consistently could result in reaching a greater variety of states (Machado et al., 2017). Intermediate strategies between the extremes of completely identical perturbation across an entire episode and completely independent perturbation per time step are also possible (Morimoto and Doya, 2001;Wawrzynski, 2015), and can be used to compromise between the advantages of the more extreme strategies.</p>
<p>Most exploration approaches which perturb the policy in action space have focused on independent perturbations in each time step, as applying the same perturbation at all time steps would not cover the space of possible policies well (see Table 3). In contrast, parameter-space exploration tends to go together with episode-based exploration, because certain parameters might only influence behavior in certain states, so such a perturbation has to be evaluated across multiple states to give a good indication of its merit (Rückstiess et al., 2010). However, exceptions to this pattern exist, especially in the area of exploration strategies of intermediate coherence. In the following paragraphs, papers presenting or analyzing specific exploration strategies will be discussed in more detail. We will start by discussing exploration strategies that apply explorative perturbations in the action space, before turning our attention to strategies that perturb the policy parameters. Finally, we will discuss the issue of what distribution these perturbations are sampled from.</p>
<p>Action-space perturbing strategies</p>
<p>Using stochastic policies to generate action-space perturbations has been used at least since the early 80's. Barto et al. (1983) proposed an early actor critic-type algorithm, that perturbed the output of a simple neural network before applying the thresholding activation function. This resulted in Bernouilli-distributed outputs (Williams, 1992).</p>
<p>Subsequent work also investigated the use of Gaussian noise in continuous action domains. Gullapalli (1990) introduced specific learning rules for the mean and standard deviation of Gaussian policies, which was introduced to learn policies with continuous actions. Williams (1992) provided a more general algorithm that provides an update rule for a general class of policy functions including stochastic and deterministic operations. Williams (1992) notes that because a Gaussian distribution has separate parameters controlling location and scale, such random units have the potential to control both the degree of exploration as well as where to explore. These early approaches all perturbed the action chosen at each time step independently.</p>
<p>While independent perturbation is still a popular method, attention has also turned to strategies that attempt to correlate behavior in subsequent time steps. An interesting early example can be found in Morimoto and Doya (2001). That paper describes a hierarchical policy, where an upper-level policy sets a sub-goal which a lower-level policy then tries to achieve. Exploration on the upper-level by itself causes some consistency in the exploration behavior, as a perturbation in the goal-picking strategy will consistently perturb the system's behavior until the subgoal is reached. Additionally, the lower-level learner itself applies a low-pass filter on the action perturbations. As a result, similar perturbations will typically be applied on subsequent time steps. The authors applied this algorithm to learn a stand-up behavior for a real robot. 2 Nachum et al. (2019) studied hierachical methods in more detail, among others focusing on their exploration behavior. In their experiments, they investigate two simple exploration heuristics that share certain properties with hierarchical policies. The first heuristic, Explore &amp; Exploit, randomly sets 'goals' for a separately trained explore policies analogous to the high-level actions in a goal-conditioned hierarchical method. Goals stay active for one or multiple time steps. Their second method, Switching Ensembles, trains several separate networks that individually attempt to optimize rewards. During training, the active policy is periodically switched, and when the policies are different, this switching leads again to exploration behavior that is coherent over several time steps. Nachum et al. find that both methods benefit from temporal coherence, and their results suggest that setting goals in a meaningful space might additionally benefit exploration.</p>
<p>Similar to the approach by Morimoto and Doya (2001), Wawrzynski (2015) investigated performing explorative perturbations on physical robots. As the authors note, applying perturbations independently at each time step (e.g., independent draws from a stochastic policy) causes jerkiness in the trajectories, which damages the robot. As an alternative, the paper proposes to apply an auto correlated noise signal. This signal is generated in a slightly different way than the previously discussed approach, as it is generated by summing xxxx up independent perturbations from the last M time steps. The authors explicitly evaluated the suggested strategy on various continuous robot control problems. Their experiments suggest that the proposed strategy leads to equivalent asymptotic performance (although sometimes a slower learning speed), while causing less stress to the robot's joints by reducing the jerkiness of trajectories.</p>
<p>Correlating the perturbations over several time steps, however, complicates the calculation of log-ratio policy gradients, as the policy is no longer Markov (as the selected action is no longer independent of earlier events given the state).  instead apply auto-correlated noise for their deep deterministic policy gradient (DDPG). Since DDPG is an off-policy algorithm, the generating distribution of the behavior policy does not need to be known, simplifying the use of various kinds of auto-correlated noise. They proposed generating this noise using an Ornstein-Uhlenbeck process, which generates noise with the same properties as that used by Morimoto and Doya (2001). This paper did not focus on real-robot experiments, thus motor strain was not a major concern. They did, however, determine that auto-correlated noise does help learn in (simulated) 'physical environments that have momentum', in other words, environments where a sequence of similar actions need to be performed to cause appreciable movement in high-inertia objects.</p>
<p>More recently, attention seems to have swung back to independent action perturbations, with recent work attempting to make the distribution from which actions are drawn more expressive, or the resulting explorative actions more informative. While classically, simple parametric distributions have been used as stochastic policies, these typically cannot represent multi-modal policies. Haarnoja et al. (2017) point out that it is useful to maintain probability mass on all good policies during the learning process, even if this results in a multi-modal distribution. In particular, a seemingly slightly-suboptimal mode might in a later stage of learning be discovered to actually be optimal, which would be hard to uncover if this mode was discarded earlier in the learning process. The authors define the exploration policy as maximizing an objective composed of a reward term and an entropy term. The solution to this maximization problem is an energy-based policy. As one cannot generally sample from such distributions, an approximating neural-network policy is fit to it instead. The authors show that with certain (initially) multi-modal reward distributions the method outperforms exploration using single-modal exploration policies. They also show empirically that a multi-modal policy learned on an initial task can provide a useful bias for exploring more refined tasks later.</p>
<p>Although maintaining a high entropy can be a useful strategy to obtain more informative data, it might be even more effective to directly maximize the amount of improvement to a target policy caused by data gathered using the exploratory behavior policy. This was the approach proposed by Xu et al. (2018), who study the optimization of the behavior policy in off-policy reinforcement learning methods, where the exploration policy can be fundamentally different from the target policy. The authors' insight is that good exploration policies might indeed be quite different from good target policies, and thus might not be centered on the current target policy, but instead have a separate parametrization. While the target policy is adapted in an off-policy manner in the direction of maximum reward, the behavior policy is separately updated by an on-policy algorithm towards greater improvements to the target policy. This is achieved by using an estimate of the improvement of the target policy as the reward of a 'meta-MDP'. Experiments show that learned variance, and even more so a learned mean function, results in faster learning and better average rewards compared to conventional exploration strategies centered on the target policy. The authors attribute this performance gain to more diversity in the exploration samples leading to more global exploration.</p>
<p>Parameter-space perturbing strategies</p>
<p>Instead of using action-space perturbation, one might directly perturb the parameters of the policy. Especially where the parametrization of the policy can be restricted due to prior knowledge about the problem, it might be advantageous to do so. For example, if we know or assume that the optimal action will be directly proportional to the deviation from a set point, it is not informative to perturb the policy at the set point, because the policy will choose an action of 0 at the set-point regardless of the parameter value. This information is implicitly taken into account if parameters, rather than actions, are perturbed.</p>
<p>Perturbing parameters rather than actions also ensure that any explorative action can also, in fact, be reproduced by some policy in hypothesis space (Deisenroth et al., 2013). Referring back to the previous example, a non-zero action perturbation at the set-point, for example, would not be reproducible by any of the considered controllers.</p>
<p>Perturbing parameters also works very well together with temporally coherent exploration. A parameter vector might simply be perturbed only at the beginning of an episode, and then kept constant for the rest of the episode. Contrast this to action-perturbing schemes, where keeping the action perturbation constant for a whole episode in general would not yield behavior that covers the state-action space well.</p>
<p>The most straightforward way to find out what parameter perturbations improve a policy is to treat the entire interaction between the policy parameters and environment as a blackbox system and calculate a finite-difference estimate of the policy gradient, keeping the policy perturbation fixed during the episode. An example of this approach is given by Kohl and Stone (2004). For each policy roll-out, each policy parameter randomly chosen to be either left as is or to be perturbed by a adding a small positive or negative constant. After obtaining a set of such roll-outs, the policy gradient is then estimated using a finitedifference calculation. This method was demonstrated to able to optimize walking behaviors on four-legged robots better than earlier hand-tuned or learned gaits.</p>
<p>A potential problem with this approach is that stochasticity (from the policy or environmental transitions) can make gradient estimates extremely high-variance. In simulation, where stochasticity can be controlled, this can be addressed by using common random numbers, as was proposed by Ng and Jordan (2000) in their PEGASUS algorithm to learn policies for gridworld problems and a bicycle simulator.</p>
<p>Ratio-likelihood policy gradient estimators exploit the knowledge of the parametric form of the policy to calculate more informed estimates of the policy gradient.  proposed a stochastic policy gradient with parameter-based exploration by positing a parameterized distribution π(θ|ρ) over the parameters θ of a (deterministic) low-level policy, and learning the hyper-parameters ρ. Their experiments showed that the resulting parameter-perturbing, episode-based exploration strategy outperformed conventional action-perturbing strategies on several simulated dynamical systems tasks, including robotic locomotion and manipulation tasks. Rückstiess et al. (2010) extended the idea of parameter-based exploration to several other policy-search algorithms, including a new method called state-dependent exploration. In that approach, perturbations are defined in the action-space, but are generated based on an 'exploration function' that is a deterministic function of a randomly generated vector and the current state. The authors show that state-dependent exploration is equal to parameter-based exploration in the special case of linear policies, and argue that in other cases it combines some of the advantages of parameter-based and actionbased exploration. The resulting data was then used to perform REINFORCE updates. As these updates do not fully account for the dependency between actions, they might thus have an increased variance. Theodorou et al. (2010) proposed 'per-basis' exploration, which is a variant parameterbased exploration scheme where the perturbation is only applied to the parameter corresponding to the basis function with the highest activation and kept constant as long as that basis function had the highest activation. Theodorou et al. (2010) noted that they emperically observed this trick improved the learning speed.</p>
<p>The effect of step-based (independent) versus correlated or episode-based exploration was further studied by Stulp and Sigaud (2012). They investigated connections between CMA-ES (Covariance matrix adaptation evolutionary strategies) from the stochastic search literature (Hansen and Ostermeier, 2001) and PI 2 , a reinforcement learning algorithm with roots in the control community (Theodorou et al., 2010). Stulp and Sigaud (2012) found that episode-based exploration indeed outperformed per time-step exploration on a simulated reaching task. Furthermore, it also outperformed per-basis exploration proposed by Theodorou et al. (2010). Salimans et al. (2017) applied the idea of episode-level log-ratio policy gradients (as used in earlier work by e.g.  to complex neural network policies. They proposed the use of virtual batch normalization to increase the methods sensitivity to small initial difference. The authors connect this method to approaches from the evolutionary strategies community (e.g. Koutnik et al. 2010). Salimans et al. (2017) found their approach to compare favorably to Trust Region Policy Optimization (TRPO, Schulman et al. 2015), an action-space perturbing method, on simulated robotic tasks. Furthermore, the approach performed competitively with the Asynchronous Advantage Actor Critic (A3C, Mnih et al. 2016), while training an order of magnitude faster. Conti et al. (2018) combined similar ideas from the evolutionary strategies community with directed exploration strategies. This combination results in two new hybrid approaches, where evolutionary strategies are used to maximize a scalarization of the original rewardmaximization objective with a term encoding novelty and diversity. A heuristic strategy for adapting the scalarization constant is proposed. The proposed approach is evaluated on a simulated locomotion task and a benchmark of Atari games, where it outperforms a regular evolutionary strategy approach and performs competitively with the deep RL approach by Fortunato et al. (2018;described below). In this experiment, the evolutionary strategies were given more frames, but still ran faster due to better parallelizability.</p>
<p>Van Hoof et al. (2017) apply auto-correlated noise in parameter space. This noise is distributed similarly to that proposed by Morimoto and Doya (2001) and , but applied to the parameters rather than the actions. As a result, intermediate trade-offs between independent and episode-based perturbations are obtained. The latent parameter vector violates the independence assumption of step-based log-ratio policy gradients meth-ods, which is resolved by explicitly using the joint log-probability of the entire sequence of actions. Expressing this log-probability in closed form requires the use of a restricted policy class (e.g., linear policies). Note that the auto-correlated action-space noise used by  was applied in an off-policy setting, avoiding this problem. Autocorrelated parameter-space noise was compared to several baselines, including action-space perturbations as well as episode-based and independent parameter-space perturbations. On various simulated and real continuous control tasks, intermediate trade-offs between independent and episode-based perturbation led to faster learning and a way to control state space coverage and motor strain.</p>
<p>Two methods (Plappert et al., 2018;Fortunato et al., 2018) independently proposed strategies to use parameter-based perturbations for reinforcement learning approaches based on deep neural networks. Both of these works consider both value-networks and policy-based approaches. Here, we will discuss the policy-based variants. Both of the approaches also build on the principle of episode-based perturbation of the parameter space, but better exploit the temporal structure of roll-outs than previous studies Salimans et al., 2017) that largely ignored it.</p>
<p>By making the perturbations fixed over an entire episode and using the reparametrization trick, these methods allow the use of a wide range of policies, but possibly increase variance. Subsequent actions are now conditioned on a shared sample from the noise distribution, and their conditional independence means the trajectory likelihood can be factored as usual. As a result, this methods are applicable to non-linear neural network policies. Plappert et al. (2018) used a pre-determined amount of noise that was decreased over time according to a pre-set schedule. On the other hand, Fortunato et al. (2018) learned the magnitude of the parameter noise together with the policy mean Applying this principle in an off-policy setting is relatively easy: since any behavior policy could be used to generate data, this could easily be the current deterministic target policy with noise added to the parameters. Plappert et al. (2018)  Incoherent behavior causes particular problems in multi-agent learning, as pointed out by Mahajan et al. (2019). In cooperative decentralized execution scenarios, uncoordinated exploration between the agent can lead to state visitation frequencies for which the factorized 3. Colas et al. (2018), whose exploration method is discussed in Sec. 4, verify the performance difference between action-and parameter space noise for DDPG, and compare their methods to the exploration methods by  and Plappert et al. (2018). Like these methods, their proposal benefits from the flexibility of DDPG as off-policy method to work with data from any behavior policy. xxxx q-function approximation is catastrophically bad. Such failure traps the agents in suboptimal behavior. Mahajan et al. (2019) remedy the situation by making exploration at train time coherent across both time and the individual agents, by conditioning on a common latent variable generated by a high-level policy 4 . A separate variational network is used to estimate a mutual information term which avoids collapse of the high-level policy on constant lowlevel behavior. The proposed approach is compared on both a toy task as well as challenging scenarios from the StarCraft Multi-Agent Challenge .</p>
<p>The distribution of perturbations</p>
<p>Separate from the issue of how perturbations are applied, is the issue of what distribution these perturbations are sampled from. Often, these are Gaussian distributions centered on the policy mean, leaving the choice of standard deviation open. Other parametric policies may have different parameters controlling the amount of exploration. Sometimes, such parameters are treated as additional hyperparameters (Silver et al., 2014) or governed by a specific heuristic (Gullapalli, 1990). More commonly, they can also be adapted like the other parameters during learning. The most straightforward way is to adapt the parameters controlling this standard deviation using the same policy gradient (Williams, 1992). Without additional regularization, policy gradient methods will tend to reduce the uncertainty, leading to a loss of exploration that is hard to control and might result in premature convergence to a suboptimal solution (Williams and Peng, 1991;.</p>
<p>To address this problem, several approaches of them have been proposed. Many of them involve regularization using the entropy of the policy or the relative entropy from a reference policy. These entropies can either be constrained or added as regularization term to the optimization objective. A unified view on regularized MDPs is presented by Neu et al. (2017); Geist et al. (2019).</p>
<p>As an example, Bagnell and Schneider (2003) studied natural policy gradients through the lens of limiting the divergence between successive policies. They found the natural gradient can be derived from a bound on the approximate Kullback-Leibler divergence between trajectory distributions. Dynamic policy programming (Azar et al., 2011) uses a similar formulation but instead uses the relative entropy as penalty term. Schulman et al. (2015) provides a more exact method, focused on limiting the equivalent expected Kullback-Leibler divergence between policies. They connects this update to the idea of trust region optimization, and provides several steps to make scale this type of network to deep reinforcement learning architectures with tens of thousands of parameters.</p>
<p>The 'relative entropy policy search' method proposed by  bounds the KL divergence in the joint state-action distribution to avoid a loss of exploration during training. Their bound on the joint KL is a stricter condition than a bound on the expected KL divergence of state-action conditionals which has theoretically attractive properties (Zimin and Neu, 2013). However, the method is more complex and seems harder to scale to deep architectures (Duan et al., 2016a). Bas-Serrano et al. (2020) propose a method building on relative entropy policy search, that combines regularization terms on the joint-and expected KL. A large benefit is that the resulting algorithm can be faithfully implemented in deep RL frameworks, and thus does need further approximations of the policy.</p>
<p>An alternative used early on was to add the derivative of the policy entropy to the policy updates. Williams and Peng (1991) found this strategy to improve exploration open on a toy example and several optimization problems. This strategy has also proved fruitful in practice in deep learning approaches: Mnih et al. (2016) applied this strategy and informally observed it lead to improved exploration by discouraging premature convergence. Such an entropy term can be seen as a special case of the expected relative entropy objective, with the reference policy being the maximum entropy distribution. Neu et al. (2017) studied such regularized objectives in detail, and conclude that a entropy penalty based on samples from the previous policy distribution distribution can lead to optimization problems.</p>
<p>The entropy regularization methods in the previous paragraph only took the instantaneous policy entropy into account. Haarnoja et al. (2017Haarnoja et al. ( , 2018 instead propose two methods that explicitly encourages policies that reaching high-entropy states in the future. They note their method improves exploration by acquiring diverse behaviors.</p>
<p>Bonus-Based/Optimism-Based Exploration</p>
<p>A popular category of exploration methods commonly used in domains with weak or sparse explicit reward structure is the bonus-based methods. In this category, extrinsic reward r(s, a) is augmented with a bonus term that is often demonstrated as a form of intrinsic reward (Oudeyer et al., 2007) to encourage better exploration. The term bonus was first introduced in the early nineties by Sutton (1990) in the tabular setting. Algorithms that adopt the bonus-based exploration approach employ different bonus calculation techniques to encourage the choice of action that leads to a higher level of uncertainty and consequently, novel or informative states.</p>
<p>In an environment with underlying MDP M, upon selection of the state-action pair (s, a), the explicit reward r(s, a) is observed by the RL agent, and the bonus term B(s, a) is computed by the RL agent. Thus, the total reward obtained by the agent by taking action a at state s is defined as,
r + (s, a) := r(s, a) ⊕ B(s, a),(24)
where the operator ⊕ denotes the aggregation between the two sources of environment (extrinsic) reward and bonus term.</p>
<p>In designing bonus-based exploration algorithms, two main questions arise:</p>
<ol>
<li>
<p>How should the bonus function B(s, a) be specified to yield an effective exploration behavior?</p>
</li>
<li>
<p>How should we combine the two separately acquired sources of information, exploration bonus denoted by B(s, a) and extrinsic reward denoted by r(s, a)?</p>
</li>
</ol>
<p>Exploration methods described in this section adopt three different approaches to compute the additive bonus term, namely optimism-based, count-based, and prediction errorbased. In the optimism-based methods, the bonus term is implicitly embedded in the value function initial value estimates. In the count-based methods, where the novel state-action pairs are the ones that are less frequently visited, the bonus term is calculated based on some form of state-action visitation counts. Another way of calculating the bonus term is through a prediction model of environment dynamics and measuring the induced prediction error. The interplay between the optimism-based approach and the count-based method in designing the bonus term is subtle, and it is often hard to distinguish between these two paradigms. Thus, in the optimism section, we only discuss the optimism-based approaches that do not explicitly use any notion of count in their choice of bonus. In the count-based section, we only discuss the methods that explicitly employ a notion of count in their bonus definitions. In this survey, we choose to interpret the optimistic initialization heuristic as a type of bonus assigned to unseen state-action pairs to encourage exploration.</p>
<p>All these three categories are divided into two sub-categories of tabular and function approximation methods. In the tabular setting, the state and action spaces are small enough that the value function estimate can be presented as a lookup table. In the function approximation setting, on the other hand, due to the infinite or large nature of state and action pairs, the value function is represented as a parameterized function rather than a table. At the beginning of each section, we provide a table that summarises the papers discussed.</p>
<p>Optimism-based methods</p>
<p>Optimistic exploration is a category of exploration methods that adopt the philosophy of Optimism in the Face of Uncertainty (OFU) principle, which was first introduced as an ad-hoc technique by Kaelbling et al. (1996); Kaelbling (1993); Sutton (1991a); Thrun and Möller (1992). From an optimistic perspective, a state is considered a good state if it induces a higher level of uncertainty in the state-value estimate and greater return potential. Optimistic exploration methods are typically realized by implicitly utilizing an exploration bonus either in the form of optimistic initialization (Even-Dar and Mansour, 2002; Sutton and Barto, 1998b;Szita and Lőrincz, 2008) or Upper Confidence Bounds (UCBs) (Strehl and Littman, 2008;Jaksch et al., 2010). In the optimistic initialization approach, the key assumption is that the unvisited state-action pairs yield the best outcome, whereas in the UCB-based methods, the unvisited state-action pairs are assumed to collect the outcome proportional to the largest statistically possible reward. In this section, we only focus on the methods that do not employ count-based approaches to implement the OFU principle as we address the count-based methods in a separate section.</p>
<p>Optimism-based methods: tabular</p>
<p>A model-based approach proposed as a generalization of E 3 algorithm (Kearns and Singh, 2002) (discussed in detail in Section 6.2.1) is the R-max algorithm (Brafman and Tennenholtz, 2002), which models agents' interactions in the context of zero-sum stochastic games (SG) instead of MDPs. In R-max (Brafman and Tennenholtz, 2002), the agent always maintains maximum likelihood estimates of environment dynamics and reward function if the observed data is sufficiently rich. The algorithm uses the approximate model estimates for a state-action pair if its visitation count exceeds a certain threshold. The optimistic approach is adopted at the initialization phase of the model, where all actions in all states are assumed to return maximum reward R max . R-max benefits from a built-in mechanism for resolving the exploration vs. exploitation dilemma because of the model estimation  and optimism. That is, taking the optimal action according to the learned model results in either exploring a previously unknown state or obtaining the near-optimal reward. In R-max, a state is marked as known if the number of states reachable (based on the learned model) from that state passes a fixed threshold; therefore, it is no longer considered a novel state. The sample complexity analysis conducted by Brafman and Tennenholtz (2002) shows near-optimal performance in a polynomial number of time-steps (assuming the state-space is finite). The R-max algorithm also attains near-optimal polynomial time average reward in |S|, |A| and mixing time T . In a similar approach, authors in Szita and Lőrincz (2008) propose a new sample efficient and model-based exploration algorithm called Optimistic Initial Model (OIM) in an MDP framework with finite state and action spaces. The proposed method assigns an optimistic value to unknown areas, and if the sampled state is among the set of 'Garden of Eden' states, it receives the maximum reward of R max . The RL agent in this method builds an approximate model of environment dynamics and updates the value functions using dynamic programming. To handle the full update sweep complexity imposed by dynamic programming, authors adopt the prioritized sweeping algorithm of Wiering and Schmidhuber (1998). Theoretically, the authors show almost sure convergence of the proposed method to a near-optimal policy in polynomial time under a lower-bound assumption on R max . Experimentally, OMI's performance is assessed against -greedy, Boltzmann and some other exploration methods in three environments of RiverSwim and SixArms (Strehl et al., 2006), Maze (Wiering and Schmidhuber, 1998) and Chain, Loop and FlagMaze (Meuleau and Bourgine, 1999;Strens, 2000;Dearden et al., 1999).</p>
<p>xxxx</p>
<p>The seminal idea of employing optimistic upper confidence bound (UCB) to encourage optimistic exploration policies in RL setting was first introduced by Strehl and Littman (2004). Auer and Ortner (2007) proposed UCRL algorithm as the first near-optimal exploration method that extends the idea of optimistic UCBs in RL. UCRL computes a countbased upper bound on the empirical estimates of reward and transition probability after each visit and then switches between policies based on the observed gap and calculates and a new policy. Later, Jaksch et al. (2010) proposed UCRL2 as an extension to Auer and Ortner (2007). Apart from optimistic initialization, both UCRL and URCL2 implement count-based UCBs to encourage exploration. Thus, we postpone further explanation regarding these methods to the count-based section.</p>
<p>Optimism-based methods: function approximation</p>
<p>After the successful application of OFU to RL with finite state-action MDPs, which we addressed in Section 6.1.1, some recent approaches extended this idea to MDPs with large or infinite state-action spaces (Azar et al., 2017;Bartlett and Tewari, 2012;Fruit et al., 2018;Filippi et al., 2010;Jaksch et al., 2010;Tossou et al., 2019). This section provides a comprehensive overview of the methods that employ OFU in the MDPs with large or infinite state-action spaces.</p>
<p>The study presented by Jong and Stone (2007) is a model-based exploration-exploitation trade-off algorithm for continuous state spaces, termed Fitted R-max. The proposed algorithm is a combination of R-max (Brafman and Tennenholtz, 2002) with fitted value iteration (Gordon, 1995). The algorithm first updates environment models at each time-step and then applies the value iteration step to solve their proposed Bellman optimality equation. In discrete setting, the proposed method simply implements the optimistic value function proposed by Brafman and Tennenholtz (2002) and control the exploration-exploitation tradeoff through a visitation count threshold. In continuous setting, the authors propose a new counting method based on the sum of the unnormalized kernel values based in the estimated environment dynamics. The performance of the Fitted R-max algorithm is experimentally tested in the two environments Mountain Car (Sutton and Barto, 1998b) and Puddle World (Kearns and Singh, 2002).</p>
<p>Another line of work that implements the OFU principle in continuous state space environments is , where the authors combine their proposed Multiresolution Exploration (MRE) algorithm with fitted Q-iteration (Antos et al., 2008). Their proposed model-based method is built upon Kakade et al. (2003) and introduces a method that measures the uncertainty associated with the visited states through building regression trees, termed as knownness-tree. Knownness-tree is used to model the environment transition dynamics and optimistically update its model at each time step. Theoretically,  show, under some smoothness assumption on transition dynamics, the near-optimal performance of the proposed algorithm, and assess the performance of MRE against -greedy algorithm empirically in continuous Mountain Car environment (Sutton and Barto, 1998a). Kumaraswamy et al. (2018) propose a model-free computationally efficient exploration strategy based upon computing Upper-Confidence Least-Squares (UCLS), which are UCBs for least-squares temporal difference learning (LSTD). Since LSTD maintains the agent's past interactions efficiently, the computed upper confidence bounds induce context-dependent variance, which encourages the exploration of states with higher variance. This study provides the first theoretical results that obtain UCBs for policy evaluation using function approximation. Empirically, UCLS algorithm shows outperformance over DGPQ (Grande et al., 2014), UCBootstrap (White and White, 2010), and RLSVI (Osband et al., 2016b) in Sparse Mountain Car, Puddle World and RiverSwim environments.</p>
<p>Ciosek et al. (2019) provide an optimistic actor-critic algorithm to resolve two phenomena: pessimistic under-exploration, that is deviating the algorithm from sampling actions that result in improvement on the critic estimates, and directionally uninformed action sampling, which is uniform sampling of actions that are lying in two opposite sides of mean in Gaussian policies. They state that these two phenomena prevent state-of-the-art actor-criticbased algorithms such as SAC (Haarnoja et al., 2018) from performing efficient exploration. To tackle these challenges, they calculate approximate upper confidence bounds on the value function estimates to encourage directed exploration and lower confidence bound to prevent overestimation of the value function estimates. They benchmark their proposed algorithm (OAC) in high-dimensional MuJoCo tasks, and the plotted results demonstrate marginal improvement against the SAC algorithm.</p>
<p>To avoid the pessimistic initialization phenomenon, commonly used in deep network initialization schemes, Rashid et al. (2020) propose an optimistic initialization algorithm, termed Optimistic Pessimistically Initialised Q-Learning (OPIQ) that decouples optimistic initialization of Q function from network initialization. Rashid et al. (2020) propose a simple count-based bonus augmented with the Q-value estimates. In the tabular setting, their proposed algorithm is based on Jin et al. (2018). In the Deep RL setting, they adopt commonly employed methods of calculating pseudo counts such as Bellemare et al. (2016); Ostrovski et al. (2017) to compute the additive bonus term. OPIQ is evaluated in the three domains toy randomized Markov chain, Maze and Montezuma's Revenge against the naive extension of UCB-H (Jin et al., 2018) to the deep RL and some variations of DQN augmented with some state-of-the-art pseudo-count estimate methods.</p>
<p>When both the environment dynamics and task objective are unknown to the RL agent, Seyde et al. (2020) propose a model-based exploration algorithm, termed Deep Optimistic Value Exploration (DOVE), to encourage deep exploration through adopting optimistic value function. Throughout each episode, DOVE learns a transition function and reward function using supervised learning. The initial conditions are applied to the learning policy generated by perturbing locally observed states fetched from the replay memory. The local perturbation performed throughout each episode is employed to ensure information propagation. Empirically, Seyde et al. (2020) benchmark DOVE in some high-dimensional continuous control MujuCo tasks.</p>
<p>Count-based bonus</p>
<p>One way to model the intrinsic reward is to measure how surprising a state-action pair is. An intuitive approach to measuring surprise is to count how frequently a particular state-action pair is visited. In the count-based setting, the notion of bonus is defined as a function of state-action pair visitation count. In table 5, we provide a list of exploration methods that xxxx</p>
<p>Reference</p>
<p>Bonus Type Performance measure Tabular Methods Sutton (1991b) count-based Empirical-Grid world Moore and Atkeson (1993) count-based threshold/optimistic initialization Empirical-Grid world Kaelbling (1993) count-based Empirical-Toy MDP Dayan and Sejnowski (1996) count-based Empirical-Grid world Tadepalli and Ok (1998) count-based/optimistic initialization Empirical-Grid world/AGVscheduling Kearns and Singh (2002) count-based/optimistic initialization PAC bound Kakade et al. (2003) distance-based count PAC bound Strehl et al. (2006) UCB-based PAC bound Auer and Ortner (2007) UCB  explicitly employ a form of visitation count to control the exploration-exploitation trade-off.</p>
<p>Count-based bonus: tabular</p>
<p>In the tabular setting, counting visited states or state-action pairs is a trivial problem and the bonus term is typically used in one of the following forms,
B(s, a) or B(s) ∝          ln n N (s,a) or ln n N (s) , 1 √ N (s,a) or 1 √ N (s) , 1 N (s,a) or 1 N (s) .(25)
where n denotes the total number of time-steps taken by the RL agent. An intuitive interpretation of equation (25) is that the bonus for visiting a state-action pair (s, a) is highest when (s, a) is novel, and decays each time the pair (s, a) is revisited. The main limitation of such definitions of bonus is that they are mainly applicable in tabular settings, where the set of state-action pairs is countable and finite. Although bonus-based methods employed in the tabular settings are not necessarily suitable for large state-andaction space settings, they still provide useful intuitions. The first study that employed the notion of bonus in the context of exploration algorithms was Sutton (1991b), where a new RL architecture Dyna-Q+ was proposed. Dyna-Q+, which is a combination of Dyna architecture (Sutton, 1991a) and Watkins Q-learning (Watkins and Dayan, 1992) uses an additional explicit count-based exploration bonus assigned to state-action pair (s, a). The bonus term used in Dyna-Q+ is proportional to the square root of the number of time steps that have elapsed after the last trial of action a at state s. This exploration bonus is added to the update rule designed to update Q-value. The main advantage of using such bonuses is to increase the chance of visiting the state-action pairs that have not been frequently visited. Sutton (1991a) tests his proposed model in the two RL environments Blocking and Shortcut tasks, designed as a small 2D maze environment against two other variations of Dyna-Q that do not use exploration bonus to encourage exploration. Sutton (1991a) shows that in both experiments, Dyna-Q+ outperforms other variations of Dyna-Q in the setting, where the performance is measured with respect to the collected reward. To address the two issues, namely high computational expense raised by Kaelbling (1993), and instability of the bonus raised by Sutton (1991a) in large state and action spaces, Moore and Atkeson (1993) proposed the prioritized sweeping algorithm that uses a preset threshold parameter T board to determine whether the state-action pair is worth exploring more or not. Prior to reaching the visitation threshold, the bonus parameter is set to the max return value in the discounted reward setting R max /(1 − γ). Once the visitation count exceeds the optimistic threshold, the algorithm uses the non-optimistic true discounted return. Apart from the tabular nature of this approach, its main bottleneck is that the key hyperparameter T board is prefixed and set manually. The proposed algorithm's performance is experimentally tested against Dyna-Q in two deterministic and stochastic grid world environments.</p>
<p>An early study that adopts the OFU principle in a model-based setting for exploration is Tadepalli and Ok (1998). The proposed H-learning algorithm is designed in the context of the average-reward RL setting, where the RL agent's goal at each time-step t is to optimize the average expected reward. At each time-step t, an empirical model of the environment is computed, and consequently, a set of greedy actions accessible from the current state with respect to the Bellman equation for average-reward RL is reported, as well as the expected xxxx long-term advantage function h(s). The long-term advantage function h(s) reflects the longterm impact of start state s on the obtained expected average reward. Eventually, the final expected average reward associated with actions in the set of greedy actions at the current state s is calculated with respect to h(s), a temperature parameter α and expected average reward computed at time t − 1. Experimentally, the proposed H-learning algorithm is compared with four other exploration methods, random exploration, counter-based exploration, Boltzmann exploration, and recency-based exploration in a two-dimensional grid world with discrete state and action spaces, termed Delivery domain. The non-tabular version of the Hlearning algorithm is proposed based on local linear regression as the function approximator and Bayesian network representation of the action space. The extended H-learning method called LBH-learning is tested in three AGV-scheduling tasks (Maxwell and Muckstadt, 1982) and compared to six different H-learning baselines.</p>
<p>The Explicit Explore or Exploit algorithm (known as E 3 ) (Kearns and Singh, 2002) adopts a model-based approach that initiates the exploration phase by dividing the set of states into two categories of known and unknown states. A state is considered to be known if the number of state visitations passes a certain threshold such that the learned dynamics are sufficiently close to the true one. If the current state is unknown, the algorithm calls the procedure of balanced wandering, in which the algorithm chooses the least frequent action at the unknown state and assigns the max reward to the unknown state. When the algorithm is not engaged in the balanced wandering phase, it performs two offline optimal policy computations sub-routines. Later, Kakade et al. (2003) proposed Metric-E 3 algorithm as a generalization of E 3 algorithm. Metric-E 3 provides the time complexity bound on finding a near-optimal policy that depends on the covering numbers of the state space rather than the size of the state space as presented by Kearns and Singh (2002). This difference is mainly due to the difference in their definition of a "known" state.</p>
<p>In the context of undiscounted RL, Auer and Ortner (2007) use the notion of upper confidence bounds to manage exploration-exploitation trade-off. In their study, count-based confidence bound proportional to 1 N (s,a) is updated at each step and, together with empirical estimates of reward and transition functions, help the agent to control the explorationexploitation trade-off. The regret analysis performed by Auer and Ortner (2007) shows logarithmic performance in the number of time steps taken by the algorithm based on the optimal policy. Kolter and Ng (2009) provide an explicit notion of bonus, called Bayesian Exploration Bonus, to manage exploration-exploitation trade-off. Their proposed algorithm focuses on the Bayes-adaptive RL setting with a tabular representation of state-action space. The bonus term is proportional to 1 1+N (s,a) , where N (s, a) is calculated based on the number of visitation counts implied by the prior. Kolter and Ng (2009) provide a template for countbased bonus terms in the form of a theorem stating that any algorithm that adopts an exploration bonus of the form 1 (N (s,a)) p with p ≤ 1/2 is not a PAC-MDP. In their proposed algorithm, called BEB, the action-selection is performed with respect to the mean of the current learned belief over transition model, with an additional Bayesian bonus. In the main theorem of this work, the authors show that their proposed algorithm, while allowing a higher rate of exploration, provides a near-optimal sample complexity bound, which is polynomial in |S|, |A|, and time horizon T , where the optimality is defined in the Bayesian sense.</p>
<p>In the continuous state space setting, Pazis and Parr (2013) introduce C-PACE as a PAC-optimal exploration algorithm for continuous state MDPs. C-PACE adopts the OFU principle in the estimation of the Bellman equation. At each time step, from the k-nearest neighbours, the action that maximizes the optimistic Q value function is selected. The optimistic Q value function is defined based on the knowledge of k-neighbouring stateaction pairs (the bonus term) and the immediate reward obtained upon transiting to any neighbouring pairs. C-PACE assumes the existence of a Lipschitz continuous distance metric over the set of state-action pairs. The main result of this paper provides a PAC bound that shows the near-optimal C-PACE performance with respect to the covering number of the state-action space. Finally, the authors evaluate the performance of C-PACE in a simulated HIV treatment environment. Guo and Brunskill (2015) propose a confidence-based exploration algorithm called PAC-EXPLORE in a model-based setting, which is operationally very similar to the E 3 algorithm (Kearns and Singh, 2002), with the difference in the confidence bounds used to compute policies that are practically more efficient. The PAC-EXPLORE algorithm takes a state-action pair visitation threshold and divides the space of state-action pairs into two clusters of known and unknown pairs. If the state falls into the set of known states, the algorithm applies the same technique as in Wiering (1999) to estimate confidence bounds on transition probability. Authors in this work show that by sharing the experience of concurrently running agents on top of Wiering (1999), one can achieve linear improvement on the algorithm's sample complexity.</p>
<p>Delayed Q-learning (Strehl et al., 2006) is one of the first papers that study model-free PAC optimal algorithm. At each time-step t, the agent keeps track of three values for each visited state-action pairs (s, a), the value function Q t (s, a), the Boolean flag LEARN t (s, a) that indicates whether or not the change has occurred to the Q estimate for (s, a), and a visitation counter N (s, a). The exploration-exploitation trade-off is managed based on a visitation count threshold and the value LEARN t (s, a). When the visitation count for (s, a) is larger than a pre-set threshold and the LEARN t (s, a) is true, the Q t (s, a) estimate is updated. At the initial phase, the Boolean flag LEARN(s, a) is set to TRUE for all stateaction pairs and N (s, a) is set to zero. Strehl et al. (2006) under certain assumptions prove that their proposed algorithm is PAC-MDP in the tabular setting. Jin et al. (2018) provide two types of upper confidence-based bonuses for Q-learning in the episodic tabular MDP setting: 1) Hoeffding-style bonus, 2) Bernstein-style bonus. By employing the Hoeffding-style bonus, the authors show O( √ T ) regret dependency with respect to the total number of time-step T . They also show √ H improvement by using Bernstein-style bonus over the Hoeffding-style bonus exploration algorithm, where T denotes the time horizon. Wang et al. (2020) introduce another method that addresses the sample efficiency of model-free algorithms by adopting UCB-exploration bonus in Q-learning. Their proposed UCB-based algorithm maintains two types of visitation counts, N t (s, a) that denotes the number of times the pair (s, a) has been visited up to time-step t, and τ (s, a, k) that records the number of time steps that state-action pair (s, a) has occurred for the k-th time. If (s, a) has not been visited k times, then τ (s, a, k) = ∞. At each time-step, a bonus term xxxx proportional to |S||A| ln(Nt(s,a))</p>
<p>Nt(s,a)</p>
<p>is added to the discounted value estimate, and the actionvalue function gets updated accordingly. To assess the PAC efficiency of the proposed algorithm, the authors first propose a learning instance illustrating Ω(1/ 3 ) lower bound incurred by Delayed Q-learning, which leaves a quadratic gap in 1/ from the best known lower bound in the class of UCB-based exploration algorithms.</p>
<p>Count-based: function approximation</p>
<p>Despite the near-optimal performance guarantees often achieved in the tabular setting, these methods are mostly not suitable for environments with large or infinite state spaces. This section summarizes exploration methods that adopt a notion of visitation count to design exploration algorithms for environments with large state and action spaces. Bellemare et al. (2016) revisit the problem of extending count-based exploration to nontabular setting and propose a density model that hinges upon ideas from the intrinsic motivation literature (refer to section 4) and propose an algorithm that measures state novelty for any choice of action given an arbitrary density model. The key contribution of their study is drawing a connection, called pseudo-count, between intrinsic motivation and countbased exploration. Pseudo-count quantity is derived from an arbitrary density model over the state space. The density model proposed in Bellemare et al. (2016) models a marginal distribution in which states are independently distributed. For any given choice of density model ρ, the paper draws a connection between two unknowns: 1) pseudo-count function and 2) pseudo-count total. The paper also introduces a connection between the conditional probability assigned to state s using ρ after observing its new occurrence conditioning based on its prior observations, pseudo-count function and pseudo-count total. The notion of information gain as a popular measure of novelty and curiosity is then shown to be related to pseudo-count, which leads to the main theorem in the paper that suggests using pseudocount bonus leads to an exploratory behaviour as good as when the information gain bonus is used. Under two major assumptions: 1) the given density model asymptotically behaves similarly to the limiting empirical distribution, and 2) the learning rate at which ρ changes with respect to the true state distribution µ is positive in the asymptotic sense, the limit of ratios of pseudo-counts to empirical counts is finite and exists for all states. Bellemare et al. (2016) test their proposed method in comparison with two state-of-the-art RL algorithms, Double DQN (van Hasselt et al., 2016) and A3C (Asynchronous Advantage Actor-Critic) (Mnih et al., 2016) on some of the Atari 2600 games.</p>
<p>In a subsequent work, Ostrovski et al. (2017) answer two questions regarding the modeling assumptions raised in Bellemare et al. (2016): 1) what is the impact of the quality of density model on exploration? 2) To what extent do Monte-Carlo updates influence exploration? To address the first question, Ostrovski et al. (2017) adopt an advanced neural density model PixelCNN ( Van den Oord et al., 2016), and discuss the challenges involved in this approach in terms of model choice, model training and model use. PixelCNN is a convolutional neural network that models a probability distribution over pixels conditioned on the previous pixels. The paper provides a list of properties that the density model requires and subsequently suggests a suitable notion of pseudo-count for DQN agents that leads to state-of-the-art results in difficult Atari games like Montezuma's Revenge.</p>
<p>Following the pseudo-count technique proposed by Bellemare et al. (2016) and Ostrovski et al. (2017), Martin et al. (2017) propose a new density model to measure the similarity between states and, consequently, a generalized visitation count method. Even though Bellemare et al. (2016) construct the density model over raw state visitations, the method proposed by Martin et al. (2017) relies on the feature map used in value function approximation to construct the density model. The bonus-based exploration algorithm φ-Exploration Bonus proposed by Martin et al. (2017) augments the extrinsic reward with the bonus term proportional to the inverse of the square root of the pseudo-count calculated based on the proposed feature-based density model. Empirically, φ-Exploration Bonus algorithm is evaluated against the -greedy, A3C (Mnih et al., 2016), Double DQN (Hasselt et al., 2016), Double DQN with pseudo-count (Bellemare et al., 2016), TRPO (Schulman et al., 2015), Gorila (Nair et al., 2015), and Dueling Network (Wang et al., 2016b) baselines in five different games from the Arcade Learning Environment (ALE). Fu et al. (2017) introduce another study that uses count-based bonuses to conduct exploration in high-dimensional domains using the notions of curiosity and novelty. Effective exploration methods that are based on a notion of visitation novelty typically require either a tabular representation of states and actions or a generative model over state and actions, which can be difficult to train in high-dimensional and continuous settings. Fu et al. (2017) propose an approach to approximate state visitation densities using a discriminative model (exemplar model) over the complex model of states using deep neural networks, where the classifier assigns reward bonuses if the recently visited state is novel. The authors of this work show that discriminative modeling is equivalent to implicit density estimation. They argue that learning a discriminative model using standard convolutional classifier networks in the case of rich sensory inputs like images is typically easier than learning the generative model of the environment. Their proposed model is inspired by the concept of Generative Adversarial Networks (Goodfellow et al., 2014) and employs the intuition that novel states are typically more easily distinguished from all other visited states. The main idea is to maintain a density estimator using exemplar models based on a discriminatively trained classifier instead of maintaining explicit counts. To train the proposed discriminator, a crossentropy loss is employed. Fu et al. (2017) evaluate their proposed method in sparse-reward continuous high-dimensional control tasks in MuJoCo (Todorov et al., 2012) and Vizdoom (Kempka et al., 2016). They compare the algorithm's performance with the two state-ofthe-art baseline by Houthooft et al. (2016) (discussed in Section 6.3.2) and Schulman et al. (2015).</p>
<p>As an extension of count-based exploration to high-dimensional and continuous deep RL benchmarks,  use a hashing mechanism to map novel states and visited states to hash codes and subsequently count the state visitations using the corresponding hash table. In the simple domains, authors propose a static hashing approach, in which the state space is discretized using a hash function such as SimHash (Charikar, 2002), and subsequently the bonus term is set to be proportional to the inverse of the square root of state count with respect to the hash code. In environments with complex structures, the authors adopt the Learned Hashing mechanism that implements an autoencoder to learn the appropriate hash codes. Like the static hash mechanism, the Learned Hashing mechanism also employs a bonus term proportional to the inverse of the square root of count on the xxxx hash codes. This approach outperforms the method presented by Houthooft et al. (2016) in some rllab benchmark tasks, as well as the vanilla DQN agent in some Atari 2600 games.</p>
<p>Inspired by the results from  and Machado et al. (2017), authors of  show that the inverse of l 1 norm of successor representation (Dayan, 1993) can be interpreted as an exploration bonus in both tabular and function approximation setting. Successor representation (Dayan, 1993) can be interpreted as an implicit estimator of the transition dynamics of the environment. In the tabular setting, they augment the Sarsa (Sutton, 1992) update rule with the inverse of the norm of the successor representation of the visited states. Their proposed algorithm is empirically compared with traditional Sarsa in traditional PAC-MDP domains SixArms and RiverSwim (Strehl and Littman, 2008). In the function approximation case, the bonus used is similar to the one utilized in tabular setting and is the inverse of l 1 norm of the parameterized successor feature vector. Machado et al. (2020) evaluate their proposed algorithm empirically in the Arcade Learning Environments (Bellemare et al., 2013) with sparse reward structure, including Freeway, Gravitar, Montezuma's Revenge, Private Eye, Solaris, and Venture.</p>
<p>Prediction error-based bonus</p>
<p>In this category of exploration methods, the bonus term is computed based on the change in the agent's knowledge about the environment dynamics. The agent's knowledge about the environment is often measured through a prediction model of environment dynamics. This section focuses on the exploration techniques that use Prediction Error (PE) as an exploration bonus. PE is a term used to measure the difference between the true environment model parameters and their estimates that are used to predict transition dynamics. The methods that fall into this category use the discrepancy between the induced prediction from the learned model of environment dynamics and state-action representation models, and the real observation to assess the novelty of the visited states. The states that lead to more considerable discrepancy are considered more informative than those with a smaller discrepancy. The first two studies that employ PE as an exploration bonus to encourage curiosity are Schmidhuber (1991a) and Schmidhuber (1991b), which are explained in detail in Section 4 (due to the fact that they can also function in environments that do not provide extrinsic rewards).</p>
<p>Formally, let H t be the history of observations until time-step t, a t denote the action taken at time t, and M φ be the predictive model of transition parameterized by the feature function φ. Then, the prediction error at time t is proportional to,
e(H t−1 , a t , s t ) ∝ s t − M φ (H t−1 , a t ) p ,(26)
where . p denotes the p-norm of a given vector. Dayan and Sejnowski (1996) consider the problem of exploration in a non-stationary absorbing finite POMDP setting and provide a systematic approach to designing exploration bonuses in such setting. Their algorithm borrows the certainty equivalence approximation technique from the dual control literature and provides a statistical model of the environment's uncertainty in a finite state space setting and subsequently incorporates the uncer-Reference Approach Performance measure Tabular Methods Schmidhuber (1991a) Confidence-based Empirical-Grid World Dearden et al. (1998) Information gain-based Convergence/Empirical Wiering (1999) Confidence-based PAC bound Ishii et al. (2002) Confidence-based/Entropybased Empirical-Grid Strehl and Littman (2004) Confidence-based Empirical Strehl and Littman (2008) Confidence-based PAC bound/Empirical Lopes et al. (2012) density-based PAC bound/Empirical Function Approximation Methods White and White (2010) Confidence based Convergence/Empirical Stadie et al. (2015) PE-based bonus Empirical Houthooft et al. (2016) Information gain Empirical Pathak et al. (2017) PE-based bonus Empirical Burda et al. (2018a) PE-based bonus Empirical Hong et al. (2018) PE-based bonus Empirical Burda et al. (2018b) density-based Empirical Kim et al. (2019) Information gain-based Empirical Table 6: Prediction Error-based methods tainty estimates into the systematic design of exploration bonuses. The exploration bonus in Dayan and Sejnowski (1996) is proportional to the amount of uncertainty induced by the agent's belief system and the true model of the environment. Dayan and Sejnowski (1996) assess their proposed method against DAYNA (Sutton, 1991a) in a two-dimensional maze with movable barriers. The concept of Interval Estimation (IE) was first introduced by Kaelbling (1993) in the bandit setting to employ confidence intervals during the exploration phase. The agent chooses the action that induces the highest upper confidence bound. A few years later, Wiering (1999) provided a theoretical extension to Kaelbling (1993) and discussed a new variation of Model-Based Interval Estimation (MBIE) by augmenting the Bellman optimally equation with a bonus term proportional to 1 / N (s, a). The author also provide a formal PAC-style guarantee for their proposed algorithm and theoretically analyze the effect of additive bonus term on the number of time steps required to achieve the sub-optimal performance.</p>
<p>Prediction error-based bonus: Tabular</p>
<p>In the stream of model-based approaches to exploration, Ishii et al. (2002) compute the exploration bonus using the entropy of the posterior distribution of the state-transition kernel. The reward associated with the state-action pair is composed of the obtained immediate reward, and the entropy of the visited state-action pairs. The action sampling policy is based on a softmax action selection algorithm combined with an entropic bonus term. Small entropy means that the amount of information acquired given the agent's current model of the environment is expected to be small, and therefore, the probability of taking action given the current state of the agent is small. Ishii et al. (2002) assess their proposed method in a small 2D maze environment with fixed and moving barriers and a zig-zag maze. In both experiments, the effect of exploration bonus on the required number of steps for reaching the shortest path is shown. Lopes et al. (2012) improve the traditional model-based exploration techniques based on OFU principle such as R-Max (Brafman and Tennenholtz, 2002) and Bayesian exploration bonus (Kolter and Ng, 2009), and empirically estimate the learner's accuracy and the learning progress. Lopes et al. (2012) use the change in the loss of prediction error (both mean and variance) as the bonus term, and subsequently use it to modify R-Max and Bayesian exploration bonus methods. This approach is particularly useful in scenarios where the agent has an incorrect prior knowledge of the transition model, or when it changes over time. The learning progress is measured with respect to the empirical estimate of predictive error using the leave-one-out cross-validation estimator.</p>
<p>Prediction error-based bonus: Function Approximation</p>
<p>In this section, we focus on the exploration techniques that use PE as a measure of uncertainty to design exploration bonuses in domains with large or infinite state spaces, where methods that focus on tabular settings fail to generalize. Stadie et al. (2015) propose a method of exploration that hinges upon a model of system dynamics trained using past experiences and observations. A state is considered novel and accordingly receives an exploration bonus based on its disagreement with the environment's learned model. Formally, given the state encoding function σ, the prediction error with respect to σ at time t is denoted by e t (σ) and thus the bonus term is proportional to e t (σ)/t. The authors benchmark their proposed algorithm on Atari tasks and show that it is an efficient method of assigning exploration bonuses for large and complex RL domains. The predictive model introduced by Stadie et al. (2015) is a simple two-layer neural network, and the prediction error is measured with respect to the l 2 Euclidean norm. They evaluate their proposed method in 14 Arcade Learning Environments (ALE) against Boltzmann, DQN and Thompson sampling methods.</p>
<p>In another diversity-driven method, Hong et al. (2018) augment a diversity-based bonus with the loss function and encourage the agent to explore diverse behaviours during the training phase. The modified loss function is computed by subtracting the current policy's expected deviation or distance from a set of recently adopted policies. They use a clipping threshold in the case of observing extraordinary deviation in the computed empirical expectation. The authors evaluate the performanc of their proposed method in Mujoco and Atari environments against vanilla DDPG and the Parameter-Noise exploration method (Plappert et al., 2018). Burda et al. (2018a) conducted a large set of experiments on curiosity-driven learning algorithms that work with intrinsic reward mechanisms across 54 different environments. Interestingly, the results presented show the impact of feature learning on better generalizability while using prediction error as a deriving force for exploration. Through the conducted experiments, they also demonstrate the limitations of prediction-based bonus mechanisms.</p>
<p>Some studies measure the observed state's novelty based on the amount of PE the observed state induces on the network that is trained using the agent's past experience. For example, Burda et al. (2018b) introduce a simple notion of exploration bonus, which is based on the PE induced by the features of observed states and the prediction of the randomly initialized network when the environment is stochastic. Burda et al. (2018b) count four factors as the primary sources of error in prediction, 1) the amount of training data, 2) stochasticity of environment, 3) model misspecification, and 4) learning dynamics. The uncertainty factor considered in their study is based upon the uncertainty quantification method proposed initially by . Burda et al. (2018b) assess their proposed exploration method in the difficult Atari game Montezuma's Revenge and outperforms the state-of-the-art baselines.</p>
<p>A line of research uses information gain (IG) as an exploration bonus (For a more detailed explanation regarding information gain, refer to section 4.2). For instance, Houthooft et al. (2016) propose a curiosity-driven strategy, which uses information gain as a driving force to encourage exploration of actions that lead to states that cause a larger change in the agent's internal model of environment dynamics. The state and action spaces are considered to be continuous. The paper proposes a variational approach to approximate the true posterior, and therefore, the information gain is measured using the KL-divergence between the agent's internal belief over environment dynamics at different time steps. The main challenge in their proposed model is the computation of variational lower-bound. The way Houthooft et al. (2016) compute variational lower-bound, requires the calculation of the posterior probability, which is generally considered to be computationally intractable. The computed variational lower-bound is used to measure the agent's curiosity. Houthooft et al. (2016) assess their proposed algorithm in continuous Mujoco domains, and compare its performance with TRPO, ERWR and REINFORCE.</p>
<p>Another study that uses IG as an exploration bonus is introduced by Kim et al. (2019). The authors apply the information bottleneck (IB) principle (Tishby et al., 2000) to design an exploration bonus to handle the exploration-exploitation trade-off, particularly in distractive environments. The bonus term in Curiosity-Bottleneck (CB) objective (inspired by IB principle) appears in the form of mutual information, measured by KL-divergence between the latent representation of the environment and the input observation. To inspect the performance of the proposed CB method, Kim et al. (2019) perform experiments on three environments: 1) Novelty detection on MNIST and Fashion-MNIST, 2) Treasure Hunt in a grid-world environment, and 3) Atari's Gravitar, Montezuma's Revenge, and Solaris games. The CB performance is compared with the work of Burda et al. (2018b).</p>
<p>Deliberate Exploration</p>
<p>The optimal solution to the exploration-exploitation problem is a strategy that yields the highest expected total reward over the entire duration of an agent's interaction with the environment. The problem of finding such an optimal strategy is a meta-problem in itself, where a notion of optimality can be defined with respect to a distribution over the models of the environments the agent is likely to encounter. Which exploration strategy works best depends on the range of environments considered plausible by the agent. If a probability distribution over the environment is known, we can define the optimal exploration strategy as the strategy that yields the highest expected total reward in expectation over this distribution.</p>
<p>If unknown transition and reward model parameters are considered as the unobservable states of the system, then the entire problem can be defined as a specific kind of POMDP where the hidden part is a set of environment parameters and the observable part is the state of the sampled MDP. However, this formulation can have too many belief states and can thus in general not be solved exactly within practicable time. Various sub-fields have focused on different avenues of solving this problem approximately. The Bayesian approach focuses on tackling the full problem, termed Bayes-adaptive MDP, by introducing different approximations to the problem; for example by making prior assumptions about the form of the uncertainty over the unknown model parameters either via function approximation or representing the distribution over environment using a set of samples. On the other hand, meta-learning approaches assume the agent does not directly have access to a distribution over environments but can be trained on samples from the relevant distribution. Various methods, e.g. from model-free reinforcement learning, can then be used to find policies that can effectively embed interaction histories and map them to actions to be taken. These methods tend to aim at finding (locally) optimal solutions to the Bayes-adaptive MDP within a parametrized family of policies.</p>
<p>Solving the exploration-exploitation trade-off optimally</p>
<p>In the Bayesian approach for RL, a posterior is maintained over the possible models of the environment given the observations so far. As the agent collects more observations, the belief is updated to reflect this new information. Consequently, learning bayes-optimally in an MDP is equivalent to solving for an optimal action selection strategy in a meta-level Markov decision process defined by these belief states. In the following sections, we will focus on a particular formulation of this meta-level problem: the Bayes-Adaptive MDP (Duff, 2002) formulation, where the belief state is given by a current base-level state as well as a posterior distribution over the base-level transition and reward models.</p>
<p>When the dynamics are unknown, the Bayesian RL formulation assumes that the transition model P is a latent variable according to some prior distribution P (P). Let H t denote the history of observations up to time t, then the dynamics model is updated according to the Bayes rule P (P|H t ) ∝ P (H t |P)P (P), in response to the observed transitions. The uncertainty of the model dynamics is handled by transforming it into uncertainty into the augmented state and history space. The actual state of the agent, S, together with the belief state that that consists of parameters defining uncertainty distributions over the transition model, X , comprise the hyperstate, S + = S × X . The new dynamics model in this new augmented space is defined by P + (s , x , a, s, x), that denotes the transition model for hyperstates, conditioned on action a being taken in hyperstate s, x . The reward function for the aurgmented MDP is given by R + (s, x, a) = R(s, a). The new MDP is defined by the tuple of M + = S + , A, P + , R + , γ and is known as the Bayes-Adaptive MDP (BAMDP) (Duff, 2002).</p>
<p>The hyperstate is a sufficient statistics for the process evolving under uncertainty, and the Bellman equations formalism used for MDPs also holds true for the generalized hyperstate MDP. The solution to the Bellman equations gives the value function of the hyperstate, and an optimal value function implicitly defines the optimal learning policy, which maps hyperstates to actions. The value function given by Bellman equation for the BAMDP is given by:
V (s, x) = max a∈A     R + (x, a) + γ x ∈X , s ∈S P + (s , x | s, x, a)V (s , x )     .(27)
The agent starts in the belief state corresponding to its prior and, by executing the greedy policy in the BAMDP while updating its posterior, acts optimally (with respect to its beliefs) in the original MDP. The Bayes-optimal policy for the unknown environment is the optimal policy of the BAMDP, thereby providing an elegant solution to the explorationexploitation trade-off. Through this framework, rich prior knowledge about the environment can be naturally incorporated into the planning process, potentially leading to more efficient exploration and exploitation of the uncertain world.</p>
<p>Example: We use the following example from Duff (2003) to highlight how the Bayesian formulation allows to solve the exploration-exploitation optimally. Consider an MDP with 2 states and 2 actions as shown in Figure 2. The hyperstate in this case is defined by a tuple (s; x), where the physical state s is the state the agent currently is in as given by the MDP, and the information state x, which is the collection of distributions describing uncertainty in the transition probabilities. The rewards (±1) are deterministic and are received when the agent lands in the corresponding state. The unknown transition probabilities are denoted by the labelled arcs. For this particular example, the authors propose using an appropriate conjugate family of distributions, such as Dirichlet, to model the uncertainty. For instance, if the uncertainty in p 1 11 (transition from state 1 under action 1) is represented as beta distribution parameterized by (α 1 1 , β 1 1 ), then hyper state for s = 1 can be written as: (1;
α 1 1 β 1 1 β 1 2 α 1 2 , α 2 1 β 2 1 β 2 2 α 2 2
). The new augmented transition function can be written as P (s , x |s, x, a) = P (s |s, x, a)P (x |s, x, a, s ).</p>
<p>The first factor can be estimated from samples by taking expectation over the possible transition function to corresponding to a given hyperstate. Therefore, for a transition from s under action a, the corresponding terms for s = 1 and s = 2 will be α a α a s +β a s and β a α a s +β a s respectively. For the second factor, the parameter of the dirichlet is given by the number of 'effective' transitions of each type observed in transit from the initial hyperstate to the given hyperstate. For this particular example, the form of the posterior update for the information state parameters, given an observation is also a beta distribution, but with parameters that are incremented to reflect the observed data. As such, for transition from s under action a, the update information state terms for s = 1 and s = 2 will be α a s + 1 and β a s + 1. An optimality equation can be written for the local transitions and the corresponding successor hyperstates. For the above example, the optimal value function has the form:
V 1; α 1 1 β 1 1 β 1 2 α 1 2 , α 2 1 β 2 1 β 2 2 α 2 2 = max α 1 1 α 1 1 + β 1 1 r 1 11 + V 1; α 1 1 + 1 β 1 1 β 1 2 α 1 2 , α 2 1 β 2 1 β 2 2 α 2 2 + β 1 1 α 1 1 + β 1 1 r 1 12 + V 2; α 1 1 β 1 1 + 1 β 1 2 α 1 2 , α 2 1 β 2 1 β 2 2 α 2 2 , α 2 1 α 2 1 + β 2 1 r 2 11 + V 1; α 1 1 β 1 1 β 1 2 α 1 2 , α 2 1 + 1 β 2 1 β 2 2 α 2 2 + β 2 1 α 2 1 + β 2 1 r 2 12 + V 2; α 1 1 β 1 1 β 1 2 α 1 2 , α 2 1 β 2 1 + 1 β 2 2 α 2 2 .
An optimal policy can be computed using the dynamic programming on the augmented MDP. However, as each transition can increment any of the information state parameters with every time step, there is an exponential increase in number of distinct reachable hyperstates with the time horizon (4 depth hyperstates at a given depth for the above example).</p>
<p>Hence, in the Bayesian RL formulation the exploration-exploitation trade-off is handled in a principled manner, because the agent does not heuristically choose between exploiting or exploring, rather, it takes an optimal action (that might lead to a mixture of exploration and exploitation) with respect to its full Bayesian model of the uncertain sequential decision process (Duff, 2002;Poupart et al., 2006). In Bayesian decision theory, the optimal action is the one that, over the entire time horizon considered, maximizes the expected reward, averaged over the possible world models. Any gain in reducing the uncertainty over the posterior transition models is not valued just for its own sake, but rather is driven by the potential gain in the future reward that it offers.</p>
<p>The major problem with the BAMDPs is that the number of hyperstates grows exponentially with the time-horizon. This exponential growth in hyperstates limits the scalability of this approach as it can make solving the problem intractable when either the size of state and action spaces increases or the planning horizon increases. In the next section we will go over some of the works that provide tractable computational procedures that retain the Bayesian formulation but sidesteps the intractability by employing various approximation and sampling techniques.</p>
<p>Bayesian Methods</p>
<p>To recap, in the Bayes-Adaptive setting, a prior over MDPs is given to the agent, and (approximately) optimal decisions are made based on the posterior over MDPs, where the posterior is a function of the interaction history with the MDP. In this section we review a few notable works that are based on the different BAMDP formulations, to illustrate how the explosion of the hyperstate space is handled to provide tractable approximate solutions. For a thorough insight into different techniques for tractability in Bayesian methods for RL we refer the reader to the survey by Ghavamzadeh et al. (2015, Chapter 4).</p>
<p>Bayesian Q-Learning (Dearden et al., 1998) adopts a Bayesian approach to the Qlearning by maintaining and propagating the probability distributions to represent the uncertainty over the agent's estimates of Q-values. Under certain modeling assumptions, the authors show that Dirichlet distributions can be used to maintain such a posterior over the Q-values. Instead of solving the BAMDP using dynamic programming as in the previous Section 7.1, the authors propose an action selection procedure based on the 'value of information' -the expected gain in future decision quality that might arise from information acquired from the current action choice. Intuitively, this notion considers the gain that can be acquired learning the true value of a particular Q-value. Formally, let a 1 and a 2 denote the actions with best and second best expected values respectively, and q s,a denote a random variable representing the a possible value of Q (s, a) in some MDP, then the gain from learning the true value is defined as:
Gain s,a (q s,a ) =      E[q(s, a 2 )] − q s,a if a = a 1 and q s,a &lt; E[q s,a 2 ], q s,a − E[q(s, a 1 )] if a = a 1 and q s,a &gt; E[q s,a 1 ], 0 otherwise(28)
As the agent doesn't know the true value of q s,a , the expected gain is computed using the prior beliefs to estimate the Value of Perfect Information (VPI):
VPI(s, a) = +∞ −∞ Gain s,a (x)P r(q s,a = x)dx,(29)
The value of perfect information gives an upper bound on the myopic (1-step) expected value of information for exploring with action a. In order to take into account the exploitation aspect, the expected reward is also added to the action selection criteria. Therefore, the goal is to select action that maximizes (E[q s,a ] + VPI(s, a)).</p>
<p>Once the action is taken and transitions are observed, the authors propose two ways of estimating the distribution of the Q-value. The first one is based on Moment matching that leads to a closed-form update but can become overly confident. The second technique is based on mixture updates that are more cautious but require numerical integration. Finally, they provide some theoretical results on the convergence of the algorithm and then they conclude with some experimental results on three toy problems (a 5-state chain MDP, an 8-state loop MDP and a 2D-maze of size 8x8) and compare their work with three other methods.</p>
<p>Optimal Probe (Duff, 2003) retains the full Bayesian framework but proposes to sidestep the intractable calculations by using a novel actor-critic architecture and proposing a corresponding policy-gradient based update for it. The policies and value functions are approximated by functions involving linear combinations of the information state components. The main assumption is that the value function is a relatively smooth function of the information state, x. For the feature set, they propose using the components of the information state. As such, the critic is parameterized as: V (s, x) ≈ V s (x) = l θ l [s]x l , where V s represents the function approximator associated with the state in the original MDP s, and θ l represents the parameters corresponding to the l-th information state component. For policies, xxxx</p>
<p>Approach</p>
<p>Choice of approximation Solution method Dearden et al. (1998) Online myopic value function Dynamic programming Duff (2002) Offline value function Policy gradient Wang et al. (2005) Online tree search Tree backups, myopic heuristic at leaves Poupart et al. (2006) Offline value function Point-based POMDP methods Guez et al. (2012) Online tree search Q-learning with rollout policy Table 7: Overview of the main techniques covered in Section 7.2 they propose using a separate parameterized function approximator for each original state s and possible action. This is because the stochastic policies that map from hyperstates to actions are required to be relatively smooth over the information state x, but should allow arbitrary and discontinuous variation with respect to the original state and the prospective action. This reduces the size of the class of stochastic policies that the actor can model, but the hope is that this parameterized family of policies will be rich enough to represent near-optimal policies. For updating the policy, a Monte-Carlo based Policy gradient update rule is proposed that uses a single hyperstate trajectory for providing an unbiased estimate of the gradient components with respect to the actor's parameters. In conclusion, the assumed function class for the actor introduces a bias but makes the complexity independent of the (exponential) number of hyperstates. Policy gradients can be expressed in terms of a matrix representing the steady-state probability of hyperstates. This matrix is again computationally intractable but can be approximated using sampled roll-outs. They test their approach a 2-states and 2 actions toy MDP with a horizon of 25 and were the first to show a tractable algorithm for that case. This method however is only computationally feasible for domains with a small number of information states where the proposed architecture works.</p>
<p>Sparse sampling  is a sample-based tree search algorithm, where the agent samples the next possible tree nodes from each state and then applies Bellman backup to propagate the values of child nodes to the parent node. In Bayesian Sparse Sampling (Wang et al., 2005) the authors apply the sparse sampling technique to search over BAMDPs, where the task is to use lookahead search to estimate the long term value of possible actions in a given belief state. The key idea is to exploit information in the Bayesian posterior to make intelligent action selection decisions during the look-ahead simulation, rather than simply enumerating over all the actions or selecting the actions myopically. The search tree is expanded adaptively in a non-uniform manner, instead of building a uniformly balanced look-ahead tree. The intuition is that the agent only needs to investigate actions that are potentially optimal, and in this way can save computation resources on sub-optimal actions. At each decision node, a promising action is selected using a heuristic based on Thompson sampling (Thompson, 1933) to preferentially expand the tree below actions that appear to be locally promising. At each branch node, a successor belief-state is sampled from the transition dynamics of the belief-state MDP. Once chosen, the action is executed, and a new belief state is entered. As the focus of the work is to demonstrate action selection improvements, they compare their approach with other selection schemes like standard Sparse sampling, Thompson sampling, Interval Estimation, and Boltzmann exploration techniques in the continuous 2-dimensional action space Gaussian processes task. The results show that their approach yields improved action selection quality whenever Bayesian posteriors can be conveniently calculated. Poupart et al. (2006), focus on the problem of discrete Bayesian model-based RL in the online setting, and propose the BEETLE (Bayesian Exploration Exploitation Tradeoff in LEarning), a point-based value iteration algorithm, that takes into account exploration during the exploitation step itself using the belief states mechanism. The main contribution of the work is that they present an analytical derivation of the optimal value functions for the discrete Bayesian RL problem where the optimal value function is parameterized by a set of multivariate polynomials. This analytical form then allows them to build an efficient point-based value iteration algorithm that exploits the particular form of parameterization. As a result, they have a computationally efficient offline policy optimization technique and results that show the optimization remains efficient as long as the number of unknown transition dynamics parameters remains small. The results are based on the argument that the transition dynamics of many problems can be encoded with few parameters by either tying the parameters together or using a factored model. The algorithm achieves online efficiency, by moving the policy optimization offline and doing only action selection and belief monitoring at run time. The authors compare their proposed method with twoheuristics based exploration approaches on two discrete toy MDPs benchmarks: a toy-chain with 5 states and 2 actions and an assistive technology scenario MDP with 9 stats and 6 actions.</p>
<p>The Bayes-Adaptive Monte Carlo Planning (BAMCP) algorithm Guez et al. (2012) provides a sample-based method for approximate Bayes-optimal planning for discrete MDPs that exploits Monte-Carlo tree search (MCTS). The core idea is to use the UCT algorithm (Kocsis and Szepesvári, 2006) in a computationally efficient manner for BAMDPs, where the belief state is approximated using the samples sampled only at the root node of the tree. The authors propose Bayes-Adaptive UCT, where instead of integration over all the transition models, or even approximating this expectation using an average of sampled transition model, only a single transition model P i is sampled from the agent's current belief (posterior at the root of the search tree) and is used to simulate all the necessary samples during this episode. A tree policy then treats the forward search as a meta-exploration problem, in a similar manner as the vanilla UCT problem. The goal of the tree policy is to exploit regions of the search tree that appear better than the others while continuing to explore the less known parts of the tree. For the exploitation part, a rollout policy is learned in a modelfree manner, using Q-learning, from the samples collected by the agent as a result of the interaction with the environment. In order for further computational efficiency, the authors propose a novel lazy sampling scheme for the partial transition models. The intuition is that if the transition parameters for different states and actions are independent, then instead of sampling a complete P, only the parameters necessary for individual state-action pairs can be sampled. The returns from each episode are then used to update the value of each node in the search tree during the planning. By integrating over many simulations, and therefore many sampled MDPs, the optimal value of each future sequence is obtained with respect to the agent's belief.</p>
<p>The authors compare their method with BOSS (Asmuth et al., 2009) and BEB (Kolter and Ng, 2009) in the discrete grid-world domain (10 x 10 states) and loop domain with 9 states and show their method outperforms the others. They also test their method on an infinite 2D grid-world domain where they show that their method greatly outperforms the other baselines. In the infinite 2D grid domain the baselines can not handle the large state space but as BAMCP limits the posterior inference to the root of the search tree it is not directly affected by the size of the state space, but instead is limited by the planning time.</p>
<p>Meta-learning</p>
<p>Some exploration strategies are based on, or emerge from, a meta learning perspective. Meta learning focuses on learning an appropriate bias from a collection of tasks that allows more efficient learning on new, similar tasks (Vilalta and Drissi, 2002). The field of metareinforcement learning uses this approach in reinforcement learning settings. As such, the agent interacts with multiple train MDPs, allowing it to learn a strategy for interacting with eventual novel test MDPs from the same family. At 'meta-train time' the agent learns general patterns from a set of train tasks, that can be exploited to learn more efficiently at 'meta-test time'. Usually, the train and test tasks are assumed to be drawn from the same distribution. Thus, the agent can tailor the learning strategy employed at 'meta-test time' using inductive biases extracted at 'meta-train time'. As a simple example, the training MDPs might be used to optimize the learning rate used on testing MDPs. However, much more complex meta-strategies might also be learned, including the update rule itself.</p>
<p>Before we give a more detailed breakdown of meta-reinforcement learning methods and exploration strategies used with or emerging from those methods, it is good to realize the connection between meta-reinforcement learning and Bayes-adaptive reinforcement learning. In Bayes-adaptive learning, a prior over MDPs is known to the agent, and (approximately) optimal decisions are made based on the posterior over MDPs. The posterior is a function of the interaction history with the MDP. Compare this set-up with a common set-up for meta-reinforcement learning where train and test tasks are assumed to be drawn from the same distribution. An optimized mapping from the interaction history to the next action to be taken can thus be seen to target the same objective as Bayes-adaptive learning, with the prior represented by a finite set of sampled train MDPs. Thus, meta-reinforcement learning strategies have the potential to learn an approximately optimal exploration-exploitation trade-off with respect to the distribution of training MDPs. Whereas the Bayes-adaptive literature has typically focused on discrete MDPs and tabular representations, most work on meta reinforcement learning focuses on the function approximation case, using deep neural networks to represent policies or value functions.</p>
<p>Meta-reinforcement learning techniques can be classified based on the amount of structure used in defining the mapping from interaction history to actions (Finn, 2018). At the extreme, such policies can be black boxes directly mapping from interaction histories to actions. We can think of this black box as combining two functions usually implemented by separate functions: updating the policy, and executing the policy in the current state. Thus, black box meta-reinforcement learning approaches are sometimes described as learning a reinforcement learning algorithm (Wang et al., 2016a;Duan et al., 2016b).</p>
<p>Other classes of meta-reinforcement learning tasks impose more structure on the mapping from interaction histories to actions. One common type of structure is that the policy update is given by gradient ascent. The agent's objective then becomes finding prior policy</p>
<p>Black box methods</p>
<p>Meta-learned object Heess et al. (2015) Recurrent policy Duan et al. (2016b) Recurrent policy Wang et al. (2016a) Recurrent policy Garcia and Thomas (2019) Adviser policy Alet et al. (2020) Bonus mechanism Gradient-based methods Exploration characteristics Finn et al. (2017) Early gradient-based approach Stadie et al. (2018) More credit to pre-update policies Rothfuss et al. (2019) Low-variance, improved action-level credit assignment. Frans et al. (2018) Efficient exploration through meta-learning sub-policies Inference-based methods Inference type Use of posterior Gupta et al. (2018) Test-time approximate inference Posterior sampling Rakelly et al. (2019) Amortized inference Posterior sampling Zintgraf et al. (2019b) Amortized inference Conditioning on variational parameters parameters such that a few gradient steps result in good 'post-update' policies (Finn et al., 2017). Another common type of structure is when, similar to Bayes-adaptive approaches, the inference of the posterior distribution over MDPs is decoupled from the action selection mechanism (Rakelly et al., 2019;Zintgraf et al., 2019a). Per category, we will now discuss how exploration can be done. In our review, we will focus on papers that explicitly introduce exploration methods or explicitly discuss or analyze exploration behavior in these methods. Table 8 provides an overview of the main methods discussed and some of their characteristics.</p>
<p>Black-box methods In black-box models, a mapping from interaction history to actions is optimized directly. The interaction history could comprise states or observations, actions, and rewards at all previous time steps, or even previous episodes in the same MDP. Since interaction histories are sequences without a fixed size, recurrent neural networks are a popular architecture to represent policies or value functions. Conceptually, these methods are quite straightforward: in theory any well-known policy search method could be used, using a recurrent architecture and with interaction histories rather than single states as input. A generic policy for a black-box method is of the functional form
π θ (A t |S t , H t ), H t = [S 0 , A 0 , R 0 , . . . , S t−1 , A t−1 , R t−1 , S t ],
with π θ the (usually recurrent) policy architecture parametrized by θ and H t the interaction history up to time t.</p>
<p>Early work on this approach was performed by Heess et al. (2015), who looked at different kinds of partial observability, including a BAMDP-like setting where the agent had to explore a tank of water for a hidden platform, and thereafter exploit this knowledge to find the platform again. Work by Duan et al. (2016b) and Wang et al. (2016a) further investigated this type of approach. These works also formalized the idea of "learning a re-inforcement learning algorithm". Although the framework is roughly similar, the methods differ in design choices such as which base reinforcement learning method is used (A2C/A3C (Mnih et al., 2016) versus TRPO (Schulman et al., 2015)). Both papers look at the exploration/exploitation trade-off in bandits and visual navigation tasks compared to classical exploration methods based on Thompson sampling or exploration bonuses. On the discrete tasks with known dynamics, the meta-learning approach performs competitively with classical methods, but it is applicable to the challenging visual exploration task where these tabular methods are not applicable. In addition, Duan et al. (2016b) investigate exploration behavior on tabular MDPs, and Wang et al. (2016a) investigate multiple tasks inspired by behavioral science and neuroscience paradigms. Their 'dependent arms' bandit experiments reveal that the method successfully learns the optimal exploration/exploitation strategy for this particular family of bandits, where after one exploration action without any reward the agent switches to pure exploitation behavior. This exploration behavior is not matched by the considered classical bandit algorithms. Overall, the black-box model is very general as well as conceptually straightforward, however, training the recurrent models tends to take a lot of samples and training time.</p>
<p>An interesting approach is that by Garcia and Thomas (2019). Here, an 'advisor' policy is learned in a black-box fashion. Since the black box environment contains a base reinforcement learning algorithm (such as REINFORCE (Williams, 1992) or PPO Schulman et al. (2017)), the meta state becomes a combination of the current MDP, the MDP state, and the base learner's internal state. This structure is reminiscent of gradient-based methods (covered in the next paragraph), although unlike in those methods, gradients are not taken through the internal update of the base learner. Actions executed in the environment are mostly based on those of the base learner, but in a fraction of time steps an explorative action from the 'advisor' (meta-policy) is executed (rather than an action chosen uniformly as in -greedy strategies). The authors provide the theoretical result that solving the meta MDP indeed results in optimal exploration policies in the sense of maximizing total return over a set number of episodes averaged over the prior MDP distribution. Furthermore, they show strong empirical performance in the function approximation setting compared to the base algorithms without advisor, random exploration, and the MAML meta-learning approach (Finn et al., 2017, covered in the next section) on continuous control tasks such as the 'Ant' task from the Roboschool environment 5 .</p>
<p>Where the methods covered above all meta-learned a policy, Alet et al. (2020) proposed a different approach. Their method meta-learns curiosity mechanisms or bonuses that generalize across very different reinforcement-learning domains. They formulate the problem of finding exploration bonuses as an outer loop that will search over a space of bonuses (meta-learned), and an inner loop that will perform standard reinforcement learning using the adapted reward signal. They propose to do the meta-learning of the bonus in the combinatorial space of programs instead of transferring neural network weights resulting in an approach similar to neural architecture search. The programs are represented in a domain-specific language that includes modular building blocks like neural networks that can be updated with gradient-descent mechanisms, ensembles, buffers, etc. They show that searching through a rich space of programs yields novel designs that work better than human-5. https://openai.com/blog/roboschool/ designed methods such as those proposed by Pathak et al. (2017); Burda et al. (2018b). At the same time, the proposed approach generalizes across environments with different state and action spaces, for instance, image-based 2D gridworld games and Mujoco environments like Acrobot.</p>
<p>Gradient-based methods Gradient-based methods aim to introduce more structure in the meta reinforcement learning problems compared to the discussed black-box methods. These black-box methods in essence learn a RL algorithm specific to the current distribution over MDPs. Thus, it should be no wonder that updates take many steps at meta-train time. Gradient-based methods introduce prior knowledge about typical reinforcement learning updates in the learning process. These methods try to learn a policy in such a way that a few updates (or even a single one) results in a 'post-update' policy that is able to attain high expected returns. As a function of the interaction history, the policy is then of the form
π θ (Ht) (A t |S t ), θ (H t ) ← θ + α∇ θ E t−1 u=0 R(S u , A u ) ,
with θ the post-update parameters, that result from an inner update using an estimate of the policy gradient, and s u , a u with u &lt; t taken from the interaction history h t (Finn et al., 2017). Stadie et al. (2018) extended earlier work on gradient-based meta-RL (Finn et al., 2017) with the explicit aim to improve exploration. Where the earlier implementation of modelagnostic meta learning (MAML) by Finn et al. (2017) did not properly assign credit to pre-update trajectories, the proposed algorithm was hypothesized to explore better and thus called E-MAML. The proposed approach was tested on benchmark problems including mazes and 'Krazy World', an environment specifically designed to test exploration in meta learning. The proposed approach indeed learned faster on these benchmarks. Furthermore, a separate analysis looked at the difference in exploration metrics (such as the number of goal states visited) and confirmed the proposed algorithm scored higher. Similar results held for E-RL 2 , an approach based on RL 2 (Duan et al., 2016b), inspired by E-MAML, which attempts to promote exploration behavior in black-box methods by ignoring rewards obtained during exploratory roll-outs.</p>
<p>MAML and E-MAML were analyzed in more detail by Rothfuss et al. (2019). They found that the MAML formulation takes the internal structure of the policy update better into account. When all terms of the gradient of this formulation are taken into account, one should thus expect better performance. To do so, they propose a low-variance estimator of the required Hessian. Their experiments on various locomotion benchmarks confirm this performance improvement. Furthermore, they explicitly analyze exploration behavior. This qualitative analysis shows that the original MAML implementation does not learn a good exploration strategy, with E-MAML doing better but having a hard time assigning credit to individual actions. The proposed LVC estimator, on the other hand, developed good exploration behavior and was able to exploit the gleaned information.</p>
<p>The approach proposed by Frans et al. (2018) meta-learns a shared hierarchy, meaning that a common set of sub-policies is learned while a master policy that selects between the sub-policies is adapted to the meta-test task at hand. This approach can be compared to methods such as the previously discussed MAML (Finn et al., 2017), although here the parameters of the shared hierarchy are not updated in the 'inner' update, and second order gradients are not passed back to the 'outer' meta-level updates of the shared hierarchy. Frans et al. (2018) find that the proposed method can successfully learn a meta-structure where exploration takes place efficiently on the level of the master policy. Furthermore, they find that sub-policies learned on small mazes can be transferred effectively to a more challenging sparse-reward navigation task.</p>
<p>Inference-based methods In inference-based methods, the inference of the properties of the current MDP is decoupled from taking actions in that MDP. This structure mirrors classical strategies for solving POMDPs (Monahan, 1982) or BAMDPs (Strens, 2000;Duff, 2002). Since meta-learning methods are often applied to problems with continuous states and non-linear dynamics, inference over tasks can generally not be performed in closed form. Instead, inference-based meta-reinforcement learning algorithms tend to use a learned latent embedding space and often employ some form of approximate inference (Gupta et al., 2018;Rakelly et al., 2019;Zintgraf et al., 2019a). A generic inference-based policy architecture could thus be described as
π θ (A t |S t , φ * ), φ * = arg min φ KL(q φ (Z)||p(Z|H t )),
with φ * the optimal parameters for a variational distribution q over latent context variables Z.</p>
<p>Model agnostic exploration with structured noise (MAESN), finds an approximate posterior using gradient ascent at meta-test time (Gupta et al., 2018). Latent context variables can then be drawn in a posterior-sampling like manner. These latent variables are constant during an episode and thus allow structured exploration behavior. The experience with meta-train tasks is thus used both to initialize a policy and to acquire a latent exploration space that can inject structured stochasticity into a policy. The method is compared experimentally to MAML, RL 2 , and conventional (non-meta) learning strategies. The authors find that MAESN strongly outperforms baseline strategies in terms of learning speed and performance after 100 learning iterations. Furthermore, qualitative analysis shows MAESN is able to learn a well-structured latent space that effectively explores in the space of coherent strategies for the trained family of environments.</p>
<p>Both other methods perform inference in this latent space by training an amortized inference network. Rakelly et al. (2019) optimize this network to directly minimize a chosen loss function while staying close to a prior. The resulting posterior is then used for posterior sampling. The authors find improved results compared to earlier meta-learning methods, presumably thanks to the structured and efficient exploration as well as the ability to use off-policy data offered by the decoupling of inference and acting.</p>
<p>The approach by Zintgraf et al. (2019a), instead, explicitly optimizes the embedding space to decode transition and reward information. Also, instead of posterior sampling, the policy is conditioned on the mean and co-variance of the full variational posterior. In preliminary experiments, the authors show that the approximate posterior can be used to strategically and systematically explore gridworlds. In these experiments, the proposed method outperformed black-box meta-learning methods by a large margin.</p>
<p>Probability Matching</p>
<p>An entire body of algorithms for efficient exploration is inspired by the Probability Matching approach, also known as Thompson Sampling (Thompson, 1933). Probability matching (or Thompson Sampling) is a heuristic for balancing the exploration-exploitation dilemma in the Multi-Arm Bandit setting (Li and Chapelle, 2012;Agrawal and Goyal, 2012). In this setting, the agent maintains a posterior distribution over its beliefs regarding the optimal action, but instead of selecting the action with the highest expected return according to the belief posterior, the agent selects the action randomly according to the probability with which it deems that action to be optimal. This approach uses the variance of the posterior to induce randomization and incentivizes the exploration of uncertain states and actions. As more experience is gathered, the variance of the posterior will decrease and concentrate on the true value. Thompson sampling is provably efficient for the bandit setting .</p>
<p>We use the setting from Agrawal and Goyal (2012) to give an example of the Thompson Sampling algorithm for Bernoulli bandit setting, i.e. when the agent gets a binary reward (0 or 1) for selecting an arm i, and the probability of success is µ i . The algorithm maintains Bayesian priors on the Bernoulli means µ i . The algorithm initially assumes arm i to have a uniform prior on µ i (Beta(1, 1)). At time t, having observed S i (t) successes and F i (t) failures plays of the arm i, the algorithm corresponding updates distribution on µ i as Beta(S i (t) + 1, F i (t) + 1). The algorithm then samples the model of the means from these posterior distributions of the µ i 's and plays an arm according to the probability of its mean being the largest.</p>
<p>Posterior Sampling for Reinforcement Learning (PSRL) Bayesian dynamic programming was first introduced in Strens (2000) and is more recently known as posterior sampling for reinforcement learning (PSRL) (Osband et al., 2013). PSRL can be thought of as an extension of the Thompson Sampling algorithm to the RL setting with finite state and action spaces. Compared to Thompson Sampling, where a model is re-sampled at every time-step 6 , PSRL samples a single model for an episode and follows this policy for the duration of the episode.</p>
<p>In PSRL, the agent starts with a prior belief over the model of the MDP and then proceeds to update its full posterior distribution over models with the newly observed samples. For each episode, a model hypothesis is then sampled from this distribution, and traditional planning techniques are used to solve the MDP and obtain the optimal value function. For the current episode, the agent follows the greedy policy with respect to the optimal value function. They evaluate their approach on a 6-state chain MDP with 3 actions and a random MDP with 10 state and 5 actions. They show that PSRL outperforms UCRL2 by a large margin in both the above domains.</p>
<p>Although, both categories of the methods maintain a distribution over the rewards and transition dynamics obtained using a Bayesian modeling approach, PSRL based methods employ the posterior sampling exploration algorithm that requires solving for an optimal policy for a single MDP in each iteration. As such, PSRL is more computationally efficient compared to typical Bayesian-Adaptive algorithms that find optimal exploration strategy 6. In the bandit setting the length of an episode is 1 time-step. xxxx via either dynamic programming or tree look-ahead in the Bayesian belief state space over a set of a prior distribution over MDPs.</p>
<p>Best of Sampled Set (BOSS) (Asmuth et al., 2009) drives exploration by sampling multiple models from the posterior and combining them to select actions optimistically. The proposed algorithm resembles RMAX (Brafman and Tennenholtz, 2002) in the sense that it samples multiple models from the posterior only when the number of transitions from a state-action pair exceeds a certain threshold. The sampled models are then merged into an optimistic MDP which is solved to select the best action. They show that this approach leads to sufficient exploration to guarantee finite-sample performance guarantees. They compare their approach against BEETLE and RMAX and show superior results on the 5-state chain problem and 6x6 grid-world. Agrawal and Jia (2017) propose an algorithm based on posterior sampling that achieves near-optimal worst-case regret bounds when the underlying MDP is communicating with (unknown) finite diameter. The diameter D is defined as an upper bound on the time it takes to move from any state s to any other s using an appropriate policy, for each pair of s, s . The algorithm combines the optimism in the face of uncertainty principle (Section 6) with the posterior sampling heuristic. The algorithm proceeds in epochs, where, at the beginning of each epoch the algorithm generates ψ =Õ(S) sample transition probability vectors from a posterior distribution for every state and action. It then proceeds to solve the extended MDP with ψA actions and S states formed using these samples. The optimal policy found from the extended MDP is then used for the entire epoch. This algorithm can be viewed as a combination of methodologies from BOSS and PSRL algorithms described above. The main contribution of this work is providing tighter regret bounds, and as such, they do not provide any experimental results for their algorithm.</p>
<p>Randomized Value Functions</p>
<p>The PSRL approach is limited to the finite state and action setting, where learning the model and planning might be tractable. For the rest of the section, we will look at the approaches that aren't based on modeling the MDP transition and rewards explicitly, but instead focus on estimating distributions over the value functions directly. The underlying assumption of these approaches is that approximating the posterior distribution for the value function is more statistically and computationally efficient than learning the MDP. Osband et al. (2016c proposed a family of methods called Randomized Value Functions (RVFs) in order to improve the scalability of PSRL. At an abstract level, RVFs can be interpreted as a model-free version of PSRL. These methods directly model a distribution over the value functions instead of over MDPs. The agent then works by sampling a randomized value function at the beginning of each episode and following that for the rest of the episode. Exploring with dithering strategies (Sec. 4.1, 5, 5.2), is inefficient as the agent may oscillate back and forth, it might not be able to discover temporally extended interesting behaviours. On the other hand, exploring with Randomized Value Functions, the agent is committed to a randomized but internally consistent strategy for the entire length of the episode. The switch to value function modelling also facilitates the use of function approximation.</p>
<p>In order to scale posterior sampling approach to large MDPs with linear function approximation, Osband et al. (2016b) introduce Randomized Least Square Value Iteration (RLSVI) that involves using Bayesian linear regression for learning the value function. The goal is to extend PSRL to value function learning, that would involve maintaining a belief distribution over candidates for the optimal value function. Before each episode, the agent would then sample a value function from its posterior distribution and then apply the associated greedy policy throughout the episode.</p>
<p>Least Square Value Iteration (LSVI) (Sutton and Barto, 1998a;Szepesvári, 2010) performs a linear regression for the Bellman error at each timestep -similar to Fitted Q-Iteration (Riedmiller, 2005). As the value function learned from LSVI has no notion of uncertainty, algorithms based on just LSVI have to rely on other exploration strategies, like blind exploration (Sec. 4.1). RLSVI also performs linear regression for one-step Bellman error but it incorporates a Gaussian uncertainty estimate for the resultant value function. This is equivalent to replacing the linear regression step of LSVI with a Bayesian linear regression as if the one-step Bellman error was sampled from a Gaussian distribution. Even though this is not the correct Bayesian distribution, Osband et al. (2016b) show that it is still useful for approximating the uncertainty. As the RLSVI updates the value function based on a random sample from this distribution, the resultant value function is also a random sample from the approximate posterior. RLSVI is a provably efficient algorithm for exploration in large MDPs with linear value function approximators . The authors compare their approach with the dithering based strategies in a didactic chain environment where RLVSI is able to scale up to 100 state length chain. They also show better learning performance of RLVSI compared LSPI and LSVI on learning to play Tetris task, and a recommendation engine task, both of which have exponential state space but they have access to the appropriate basis functions for the task.</p>
<p>One of the problems with this approach is that the distributions over the value functions can be as complex to represent as distributions over transition model, and exact Bayesian inference might not be computationally tractable. RLSVI does not explicitly maintain and update belief distributions, as a coherent Bayesian method would, but still serves as a computationally tractable method for sampling value functions. Osband et al. (2016a) propose bootstrapped Q-learning, an RVF based approach, as an extension of RLSVI to nonlinear function approximators. Bootstrapped-DQN consists of a simple non-parametric bootstrap 7 with random initialization to generate approximate posterior samples over Q-values. This technique helps in the scenario where exact Bayesian inference is intractable, such as in deep networks. Bootstrapped-DQN consists of a network with K bootstrapped estimates of the Q-function, trained in parallel. This means that each Q 1 , . . . , Q K provide a temporally extended (and consistent) estimate of the value uncertainty. At the start of each episode, the agent samples one head, which it follows for the duration of the episode. Bootstrapped-DQN is a non-parametric approach to uncertainty estimation. The authors also show that, when used with deep neural networks, the bootstrap can produce reasonable estimates of uncertainty. They compare their method against DQN Mnih et al. (2015) on the didactic chain MDP with 100 states, and on the Atari domain on the Arcade Learning Environment (Bellemare et al., 2013), where they show that Bootstrapped-DQN is able to learn faster and also improves the final score in most of the games.</p>
<p>Many other exploration methods, such as (Azizzadenesheli et al., 2018;Touati et al., 2019) can be interpreted as combining the concept of RVF with neural network function 7. Bootstrap uses the empirical distribution of a sampled dataset as an estimate of the population statistic.  Table 9: Overview of the main techniques covered in Section 8. For the theoretical properties column, S and A denote the cardinalities of the state and action spaces, T denotes time elapsed, H denotes the episode duration, and D denotes the diameter.</p>
<p>approximation. It allows these methods to scale to high-dimensional problems, such as Atari domain (Bellemare et al., 2013), that otherwise might be too computationally extensive for PSRL. However, the approximations introduced in these works come with trade-offs that are not present in the original PSRL work. Specifically, because a value function is defined with respect to a particular policy, constructing posterior over the value functions requires selection of a reference policy or distribution over policies. However, in practice, the above methods do not enforce any explicit structure or dependencies. Janz et al. (2019) propose Successor Uncertainties (SU), a scalable and computationally cheaper (compared to Bootstrapped DQN) model-free exploration algorithm that retains the key elements of the PSRL. SU models the posterior over rewards and state transitions directly and derives the posterior over the value functions analytically, thereby ensuring the posterior over the values estimates matches the posterior sampling policy. Empirically, SU performs much better on hard tabular didactic chain problem where the algorithm scales up to chains of length 200 states. On the Atari domain, SU outperforms the closest RVF algorithm -Bootstrapped-DQN on 36 of 49 games.</p>
<p>Conclusion and Perspectives</p>
<p>In this survey, we have proposed a categorization for reinforcement learning exploration strategies based on the information that the agent uses in its exploratory action selection. We divided exploration techniques into two general classes: "Reward-Free Exploration" and "Reward-Based Exploration". The reward-free techniques either select actions totally at random, or utilize some notion of intrinsic information in order to guide exploration, without taking into account extrinsic rewards. This can be useful in environments where the reward signal is very sparse, and therefore not immediately available to the agent. Reward-based exploration methods leverage the information related to the reward signal and can further be divided into the "memory-free", which only take into account the state of the environment, and "memory-based", which consider additional information about the history of the agent's interaction with the environment. In each of these categories, methods which share similar properties are clustered together in one subcategory. The basis for this clustering is mainly the type of information used, as well as the way it is used in the selection of exploratory actions. We have discussed these exploration methods and have pointed out their strengths and limitations, as well as improvements that have been made and some which are still possible. We would like to emphasize that our goal was not to review the theoretical sample complexity results, which are abundant in the field. Rather, we wanted to provide a big picture which captures the current "lay of the land in terms of methods", and which is useful for practitioners in their choice of methods. We note that theoretical results often need to rely on assumptions about the environment and the RL algorithm, for example tabular or linear representations of the value function, or smooth dynamics, which are often not satisfied in practice. Nonetheless, exploration methods can still provide useful empirical results even if their theoretical assumptions are not satisfied. In this study, we have limited ourselves to sequential decision making in MDPs. We have not covered in detail strategies for POMDPs and bandits, although these settings have provided inspiration for some of methods proposed for the MDP setting. An emerging trend is the study of safe exploration methods, but as relatively little is written on them so far, we have not focused on this topic. Finally, considering the large number of yearly publications in this field, we have excluded some methods that are similar to the major classes of approaches we discussed.</p>
<p>Owing to the improved and more accessible computational power in the recent years, the newly proposed exploration techniques have contributed significantly to the improvement in the exploration-exploitation dilemma. However, there are major concerns and issues that have not been resolved yet, mainly due to the absence of a consensus over the ways exploration methods can be assessed. For instance, different techniques are evaluated according to different measures of efficiency and performance, such as state coverage, information gain, sample efficiency, or regret. Furthermore, there is no set of standard experimental tasks that all proposed exploration techniques are evaluated on. This diversity in the methods of assessment complicates the comparison of exploration techniques together. Finally, there is often some sort of a discrepancy between the theoretical guarantees that a method provides and the experimental condition the agent encounters. Consequently, there is often no reliable guarantee for the performance of these techniques in more involved environments. Addressing these issues could be the focus of future work.</p>
<p>Acknowledgement</p>
<p>The authors would like to thank Scott Fujimoto for providing valuable feedback on the early draft of this manuscript. Funding is provided by Natural Sciences and Engineering Research Council of Canada (NSERC).</p>
<p>an ensemble of forward dynamics models Pathak et al. (2017) Uncertainty Minimization of predicted error in feature representation Hazan et al. (2019) Space coverage Maximization of entropy of the distribution over the visited states Amin et al. (2020) Space coverage Generation of correlated trajectories in the state and action spaces Forestier et al. (2017) Self-generated goals Coverage maximization of the space of goals Colas et al. (2018) Self-generated goals Combines Forestier et al. (2017) with DDPG Machado et al. (2017) Space coverage Maximization of eigenpurposes Machado et al. (2018b) Space coverage Extension of Machado et al. (2017) to stochastic environments Jinnai et al. (2019) Space coverage Minimization of cover time Jinnai et al. (2020) Space coverage Extension of Jinnai et al. (2019) to large or continuous state spaces Hong et al. (2018)</p>
<p>modified the Deep Deterministic Policy Gradient (DDPG, Lillicrap et al. 2016) algorithm in this manner. With on-policy algorithms, this is more challenging. On-policy algorithms with per time-step updates tend to also require stochastic action selection in each time step. For the on-policy Asynchronous Advantage Actor Critic (A3C, Mnih et al. 2016) in Fortunato et al. (2018); and for Trust Region Policy Optimization (TRPO, Schulman et al. 2015) in Plappert et al. (2018); this problem was resolved by using a combination of parameter perturbations and stochastic action selection. The NoisyNet-A3C method by Fortunato et al. (2018) compared favorably to the baseline A3C variant on a majority of 57 Atari games. The parameter-exploring variants of DDPG and TRPO proposed by Plappert et al. (2018) compared favorably to baselines with (correlated or uncorrelated) action-space noise on several simulated robot environment. 3</p>
<p>Figure 2 :
2A 2-state MDP with uncertain transition probabilities under (a) action 1 and (b) action 2. Rewards are denoted by ±1 in the states.</p>
<p>Table 4 :
4Exploration methods that implement an optimism-based bonus mechanism.</p>
<p>Table 5 :
5Count-based methods.</p>
<p>Table 8 :
8Overview of the main techniques covered in this section.
. A more general overview of the properties of policy search methods is given inDeisenroth et al. (2013).
. Other work has specifically evaluated the potential of lower-level sub-policies to decrease the diffusion time during the exploration process in the context of pure exploration(Machado et al., 2017). This paper is explained in more detail in Sec. 4.2.
. Note that although the perturbed parameters are of a value network, this hybrid value-and policy based approach was covered here as the novelty in the exploration strategy stems from the high-level parametrized policy.</p>
<p>Optimal tuning of continual online exploration in reinforcement learning. Youssef Achbany, Francois Fouss, Luh Yen, Alain Pirotte, Marco Saerens, International Conference on Artificial Neural Networks. SpringerYoussef Achbany, Francois Fouss, Luh Yen, Alain Pirotte, and Marco Saerens. Optimal tun- ing of continual online exploration in reinforcement learning. In International Conference on Artificial Neural Networks, pages 790-800. Springer, 2006.</p>
<p>Analysis of thompson sampling for the multi-armed bandit problem. Shipra Agrawal, Navin Goyal, Conference on Learning Theory. Shipra Agrawal and Navin Goyal. Analysis of thompson sampling for the multi-armed bandit problem. In Conference on Learning Theory, pages 39-1, 2012.</p>
<p>Optimistic posterior sampling for reinforcement learning: worst-case regret bounds. Shipra Agrawal, Randy Jia, Advances in Neural Information Processing Systems. Shipra Agrawal and Randy Jia. Optimistic posterior sampling for reinforcement learning: worst-case regret bounds. In Advances in Neural Information Processing Systems, pages 1184-1194, 2017.</p>
<p>Exchangeability and related topics. J David, Aldous, École d'Été de Probabilités de Saint-Flour XIII-1983. SpringerDavid J Aldous. Exchangeability and related topics. In École d'Été de Probabilités de Saint-Flour XIII-1983, pages 1-198. Springer, 1985.</p>
<p>Metalearning curiosity algorithms. Ferran Alet, F Martin, Tomas Schneider, Leslie Pack Lozano-Perez, Kaelbling, International Conference on Learning Representations. Ferran Alet, Martin F Schneider, Tomas Lozano-Perez, and Leslie Pack Kaelbling. Meta- learning curiosity algorithms. In International Conference on Learning Representations, 2020.</p>
<p>Effects of externally imposed deadlines on subsequent intrinsic motivation. William Teresa M Amabile, Mark R Dejong, Lepper, Journal of personality and social psychology. 34192Teresa M Amabile, William DeJong, and Mark R Lepper. Effects of externally imposed deadlines on subsequent intrinsic motivation. Journal of personality and social psychology, 34(1):92, 1976.</p>
<p>Harsh Satija, and Doina Precup. Locally persistent exploration in continuous control tasks with sparse rewards. Susan Amin, Maziar Gomrokchi, Hossein Aboutalebi, arXiv:2012.13658arXiv preprintSusan Amin, Maziar Gomrokchi, Hossein Aboutalebi, Harsh Satija, and Doina Precup. Locally persistent exploration in continuous control tasks with sparse rewards. arXiv preprint arXiv:2012.13658, 2020.</p>
<p>Learning and problem solving with multilayer connectionist systems. Charles W Anderson, University of Massachusetts at AmherstPhD thesisCharles W Anderson. Learning and problem solving with multilayer connectionist systems. PhD thesis, University of Massachusetts at Amherst, 1986.</p>
<p>Fitted q-iteration in continuous actionspace mdps. András Antos, Csaba Szepesvári, Rémi Munos, Advances in neural information processing systems. András Antos, Csaba Szepesvári, and Rémi Munos. Fitted q-iteration in continuous action- space mdps. In Advances in neural information processing systems, pages 9-16, 2008.</p>
<p>A Bayesian sampling approach to exploration in reinforcement learning. John Asmuth, Lihong Li, L Michael, Ali Littman, David Nouri, Wingate, Conference on Uncertainty in Artificial Intelligence. John Asmuth, Lihong Li, Michael L Littman, Ali Nouri, and David Wingate. A Bayesian sampling approach to exploration in reinforcement learning. In Conference on Uncertainty in Artificial Intelligence, pages 19-26, 2009.</p>
<p>Logarithmic online regret bounds for undiscounted reinforcement learning. Peter Auer, Ronald Ortner, Advances in Neural Information Processing Systems. Peter Auer and Ronald Ortner. Logarithmic online regret bounds for undiscounted rein- forcement learning. In Advances in Neural Information Processing Systems, pages 49-56, 2007.</p>
<p>Dynamic policy programming with function approximation. Vicenç Mohammad Gheshlaghi Azar, Bert Gómez, Kappen, Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics. the Fourteenth International Conference on Artificial Intelligence and StatisticsMohammad Gheshlaghi Azar, Vicenç Gómez, and Bert Kappen. Dynamic policy program- ming with function approximation. In Proceedings of the Fourteenth International Con- ference on Artificial Intelligence and Statistics, pages 119-127, 2011.</p>
<p>Minimax regret bounds for reinforcement learning. Ian Mohammad Gheshlaghi Azar, Rémi Osband, Munos, Proceedings of the 34th International Conference on Machine Learning. the 34th International Conference on Machine Learning70Mohammad Gheshlaghi Azar, Ian Osband, and Rémi Munos. Minimax regret bounds for reinforcement learning. In Proceedings of the 34th International Conference on Machine Learning-Volume 70, pages 263-272. JMLR. org, 2017. xxxx</p>
<p>Efficient exploration through bayesian deep q-networks. Kamyar Azizzadenesheli, Emma Brunskill, Animashree Anandkumar, Information Theory and Applications Workshop (ITA). IEEEKamyar Azizzadenesheli, Emma Brunskill, and Animashree Anandkumar. Efficient ex- ploration through bayesian deep q-networks. In Information Theory and Applications Workshop (ITA), pages 1-9. IEEE, 2018.</p>
<p>Covariant policy search. Andrew Bagnell, Jeff Schneider, Proceedings of the International Joint Conference on Artificial Intelligence. the International Joint Conference on Artificial IntelligenceJ Andrew Bagnell and Jeff Schneider. Covariant policy search. In Proceedings of the Inter- national Joint Conference on Artificial Intelligence, 2003.</p>
<p>Regal: A regularization based algorithm for reinforcement learning in weakly communicating mdps. L Peter, Ambuj Bartlett, Tewari, arXiv:1205.2661arXiv preprintPeter L Bartlett and Ambuj Tewari. Regal: A regularization based algorithm for reinforce- ment learning in weakly communicating mdps. arXiv preprint arXiv:1205.2661, 2012.</p>
<p>Neuronlike adaptive elements that can solve difficult learning control problems. G Andrew, Richard S Barto, Charles W Sutton, Anderson, IEEE transactions on systems, man, and cybernetics. 13Andrew G Barto, Richard S Sutton, and Charles W Anderson. Neuronlike adaptive elements that can solve difficult learning control problems. IEEE transactions on systems, man, and cybernetics, 13(5):834-846, 1983.</p>
<p>Real-time learning and control using asynchronous dynamic programming. Andrew Gehret Barto, J Steven, Bradtke, P Satinder, Singh, University of Massachusetts at Amherst, Department of Computer and Information ScienceAndrew Gehret Barto, Steven J Bradtke, and Satinder P Singh. Real-time learning and con- trol using asynchronous dynamic programming. University of Massachusetts at Amherst, Department of Computer and Information Science, 1991.</p>
<p>Logistic q-learning. Joan Bas-Serrano, Sebastian Curi, Andreas Krause, Gergely Neu, arXiv:2010.11151arXivJoan Bas-Serrano, Sebastian Curi, Andreas Krause, and Gergely Neu. Logistic q-learning. Technical Report arXiv:2010.11151, arXiv, 2020.</p>
<p>Unifying count-based exploration and intrinsic motivation. Marc Bellemare, Sriram Srinivasan, Georg Ostrovski, Tom Schaul, David Saxton, Remi Munos, Advances in Neural Information Processing Systems. Marc Bellemare, Sriram Srinivasan, Georg Ostrovski, Tom Schaul, David Saxton, and Remi Munos. Unifying count-based exploration and intrinsic motivation. In Advances in Neural Information Processing Systems, pages 1471-1479, 2016.</p>
<p>The arcade learning environment: An evaluation platform for general agents. Yavar Marc G Bellemare, Joel Naddaf, Michael Veness, Bowling, Journal of Artificial Intelligence Research. 47Marc G Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling. The arcade learning environment: An evaluation platform for general agents. Journal of Artificial Intelligence Research, 47:253-279, 2013.</p>
<p>Quality of learning with an active versus passive motivational set. A Carl, Edward L Benware, Deci, American Educational Research Journal. 214Carl A Benware and Edward L Deci. Quality of learning with an active versus passive motivational set. American Educational Research Journal, 21(4):755-765, 1984.</p>
<p>R-max-a general polynomial time algorithm for near-optimal reinforcement learning. I Ronen, Moshe Brafman, Tennenholtz, Journal of Machine Learning Research. 3Ronen I Brafman and Moshe Tennenholtz. R-max-a general polynomial time algorithm for near-optimal reinforcement learning. Journal of Machine Learning Research, 3(Oct): 213-231, 2002.</p>
<p>Training stochastic model recognition algorithms as networks can lead to maximum mutual information estimation of parameters. John S Bridle, Advances in neural information processing systems. John S Bridle. Training stochastic model recognition algorithms as networks can lead to maximum mutual information estimation of parameters. In Advances in neural informa- tion processing systems, pages 211-217, 1990.</p>
<p>Pure exploration in multi-armed bandits problems. Sébastien Bubeck, Rémi Munos, Gilles Stoltz, International conference on Algorithmic learning theory. SpringerSébastien Bubeck, Rémi Munos, and Gilles Stoltz. Pure exploration in multi-armed ban- dits problems. In International conference on Algorithmic learning theory, pages 23-37. Springer, 2009.</p>
<p>Large-scale study of curiosity-driven learning. Yuri Burda, Harri Edwards, Deepak Pathak, Amos Storkey, Trevor Darrell, Alexei A Efros, International Conference on Learning Representations. Yuri Burda, Harri Edwards, Deepak Pathak, Amos Storkey, Trevor Darrell, and Alexei A Efros. Large-scale study of curiosity-driven learning. In International Conference on Learning Representations, 2018a.</p>
<p>Yuri Burda, Harrison Edwards, Amos Storkey, Oleg Klimov, arXiv:1810.12894Exploration by random network distillation. arXiv preprintYuri Burda, Harrison Edwards, Amos Storkey, and Oleg Klimov. Exploration by random network distillation. arXiv preprint arXiv:1810.12894, 2018b.</p>
<p>Spiral search as an efficient mobile robotic search technique. Scott Burlington, Gregory Dudek, Proceedings of the 16th National Conf. on AI. the 16th National Conf. on AIOrlando FlScott Burlington and Gregory Dudek. Spiral search as an efficient mobile robotic search technique. In Proceedings of the 16th National Conf. on AI, Orlando Fl, 1999.</p>
<p>Improving the exploration strategy in bandit algorithms. Olivier Caelen, Gianluca Bontempi, International Conference on Learning and Intelligent Optimization. SpringerOlivier Caelen and Gianluca Bontempi. Improving the exploration strategy in bandit al- gorithms. In International Conference on Learning and Intelligent Optimization, pages 56-68. Springer, 2007.</p>
<p>Training q-agents. V C Pierguido, Marco Caironi, Dorigo, Pierguido VC Caironi and Marco Dorigo. Training q-agents, 1994.</p>
<p>Similarity estimation techniques from rounding algorithms. S Moses, Charikar, Proceedings of the thiry-fourth annual ACM symposium on Theory of computing. the thiry-fourth annual ACM symposium on Theory of computingMoses S Charikar. Similarity estimation techniques from rounding algorithms. In Proceedings of the thiry-fourth annual ACM symposium on Theory of computing, pages 380-388, 2002.</p>
<p>Better exploration with optimistic actor critic. Kamil Ciosek, Quan Vuong, Robert Loftin, Katja Hofmann, Advances in Neural Information Processing Systems. Kamil Ciosek, Quan Vuong, Robert Loftin, and Katja Hofmann. Better exploration with optimistic actor critic. In Advances in Neural Information Processing Systems, pages 1785-1796, 2019.</p>
<p>GEP-PG: Decoupling exploration and exploitation in deep reinforcement learning algorithms. Cédric Colas, Olivier Sigaud, Pierre-Yves Oudeyer, Proceedings of the 35th International Conference on Machine Learning. the 35th International Conference on Machine Learning80Cédric Colas, Olivier Sigaud, and Pierre-Yves Oudeyer. GEP-PG: Decoupling exploration and exploitation in deep reinforcement learning algorithms. In Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pages 1039-1048, 2018.</p>
<p>Improving exploration in evolution strategies for deep reinforcement learning via a population of novelty-seeking agents. Edoardo Conti, Vashisht Madhavan, Felipe Petroski Such, Joel Lehman, O Kenneth, Jeff Stanley, Clune, Advances in Neural Information Processing Systems. Edoardo Conti, Vashisht Madhavan, Felipe Petroski Such, Joel Lehman, Kenneth O Stanley, and Jeff Clune. Improving exploration in evolution strategies for deep reinforcement learning via a population of novelty-seeking agents. In Advances in Neural Information Processing Systems, 2018.</p>
<p>Temporally-extended {\epsilon}-greedy exploration. Will Dabney, Georg Ostrovski, André Barreto, arXiv:2006.01782arXiv preprintWill Dabney, Georg Ostrovski, and André Barreto. Temporally-extended {\epsilon}-greedy exploration. arXiv preprint arXiv:2006.01782, 2020.</p>
<p>Improving generalization for temporal difference learning: The successor representation. Peter Dayan, Neural Computation. 54Peter Dayan. Improving generalization for temporal difference learning: The successor representation. Neural Computation, 5(4):613-624, 1993.</p>
<p>Exploration bonuses and dual control. Peter Dayan, Terrence J Sejnowski, Machine Learning. 25Peter Dayan and Terrence J Sejnowski. Exploration bonuses and dual control. Machine Learning, 25(1):5-22, 1996.</p>
<p>Bayesian q-learning. Richard Dearden, Nir Friedman, Stuart Russell, AAAI National Conference on Artificial Intelligence. Richard Dearden, Nir Friedman, and Stuart Russell. Bayesian q-learning. In AAAI National Conference on Artificial Intelligence, pages 761-768, 1998.</p>
<p>Model based Bayesian exploration. Richard Dearden, Nir Friedman, David Andre, Conference on Uncertainty in Artificial Intelligence. Morgan Kaufmann Publishers IncRichard Dearden, Nir Friedman, and David Andre. Model based Bayesian exploration. In Conference on Uncertainty in Artificial Intelligence, pages 150-159. Morgan Kaufmann Publishers Inc., 1999.</p>
<p>Intrinsic motivation and self-determination in human behavior. Edward Deci, Richard M Ryan, Springer Science &amp; Business MediaEdward Deci and Richard M Ryan. Intrinsic motivation and self-determination in human behavior. Springer Science &amp; Business Media, 1985.</p>
<p>Effects of externally mediated rewards on intrinsic motivation. L Edward, Deci, Journal of personality and Social Psychology. 181105Edward L Deci. Effects of externally mediated rewards on intrinsic motivation. Journal of personality and Social Psychology, 18(1):105, 1971.</p>
<p>Intrinsic motivation. L Edward, Deci, 978-1-4613-4448-3Plenum PressNew York, NY, USEdward L Deci. Intrinsic motivation. Plenum Press., New York, NY, US, 1975. ISBN 978-1-4613-4448-3.</p>
<p>A survey on policy search for robotics. Foundations and Trends® in Robotics. Marc Peter Deisenroth, Gerhard Neumann, Jan Peters, 2Marc Peter Deisenroth, Gerhard Neumann, Jan Peters, et al. A survey on policy search for robotics. Foundations and Trends® in Robotics, 2(1-2):1-142, 2013.</p>
<p>Benchmarking deep reinforcement learning for continuous control. Yan Duan, Xi Chen, Rein Houthooft, John Schulman, Pieter Abbeel, International Conference on Machine Learning. Yan Duan, Xi Chen, Rein Houthooft, John Schulman, and Pieter Abbeel. Benchmark- ing deep reinforcement learning for continuous control. In International Conference on Machine Learning, pages 1329-1338, 2016a.</p>
<p>RL 2 : Fast reinforcement learning via slow reinforcement learning. Yan Duan, John Schulman, Xi Chen, L Peter, Ilya Bartlett, Pieter Sutskever, Abbeel, arXiv:1611.02779arXiv preprintYan Duan, John Schulman, Xi Chen, Peter L Bartlett, Ilya Sutskever, and Pieter Abbeel. RL 2 : Fast reinforcement learning via slow reinforcement learning. arXiv preprint arXiv:1611.02779, 2016b.</p>
<p>Design for an optimal probe. O Michael, Duff, International Conference on Machine Learning. Michael O Duff. Design for an optimal probe. In International Conference on Machine Learning, pages 131-138, 2003.</p>
<p>Optimal Learning: Computational procedures for Bayes-adaptive Markov decision processes. O&apos;gordon Michael, Duff, University of Massachusetts at AmherstPhD thesisMichael O'Gordon Duff. Optimal Learning: Computational procedures for Bayes-adaptive Markov decision processes. PhD thesis, University of Massachusetts at Amherst, 2002.</p>
<p>Convergence of optimistic and incremental q-learning. -Dar Eyal Even, Yishay Mansour, Advances in neural information processing systems. Eyal Even-Dar and Yishay Mansour. Convergence of optimistic and incremental q-learning. In Advances in neural information processing systems, pages 1499-1506, 2002.</p>
<p>Optimism in reinforcement learning and kullback-leibler divergence. Sarah Filippi, Olivier Cappé, Aurélien Garivier, 48th Annual Allerton Conference on Communication, Control, and Computing (Allerton). IEEESarah Filippi, Olivier Cappé, and Aurélien Garivier. Optimism in reinforcement learning and kullback-leibler divergence. In 2010 48th Annual Allerton Conference on Communication, Control, and Computing (Allerton), pages 115-122. IEEE, 2010.</p>
<p>Model-agnostic meta-learning for fast adaptation of deep networks. Chelsea Finn, Pieter Abbeel, Sergey Levine, International Conference on Machine Learning. Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of deep networks. In International Conference on Machine Learning, pages 1126-1135, 2017.</p>
<p>Learning to Learn with Gradients. B Chelsea, Finn, University of California, BerkeleyPhD thesisChelsea B Finn. Learning to Learn with Gradients. PhD thesis, University of California, Berkeley, 2018.</p>
<p>Intrinsically motivated goal exploration processes with automatic curriculum learning. Sébastien Forestier, Yoan Mollard, Pierre-Yves Oudeyer, arXiv:1708.02190arXiv preprintSébastien Forestier, Yoan Mollard, and Pierre-Yves Oudeyer. Intrinsically motivated goal ex- ploration processes with automatic curriculum learning. arXiv preprint arXiv:1708.02190, 2017.</p>
<p>Noisy networks for exploration. Meire Fortunato, Mohammad Gheshlaghi Azar, Bilal Piot, Jacob Menick, Matteo Hessel, Ian Osband, Alex Graves, Volodymyr Mnih, Remi Munos, Demis Hassabis, Olivier Pietquin, Charles Blundell, Shane Legg, International Conference on Learning Representations. Meire Fortunato, Mohammad Gheshlaghi Azar, Bilal Piot, Jacob Menick, Matteo Hessel, Ian Osband, Alex Graves, Volodymyr Mnih, Remi Munos, Demis Hassabis, Olivier Pietquin, Charles Blundell, and Shane Legg. Noisy networks for exploration. In International Conference on Learning Representations, 2018. URL https://openreview.net/forum? id=rywHCPkAW.</p>
<p>Meta learning shared hierarchies. Kevin Frans, Jonathan Ho, Xi Chen, Pieter Abbeel, John Schulman, International Conference on Learning Representations. Kevin Frans, Jonathan Ho, Xi Chen, Pieter Abbeel, and John Schulman. Meta learning shared hierarchies. In International Conference on Learning Representations, 2018.</p>
<p>Efficient bias-spanconstrained exploration-exploitation in reinforcement learning. Ronan Fruit, Matteo Pirotta, Alessandro Lazaric, Ronald Ortner, Proceedings of the International Conference on Machine Learning. the International Conference on Machine Learning80Ronan Fruit, Matteo Pirotta, Alessandro Lazaric, and Ronald Ortner. Efficient bias-span- constrained exploration-exploitation in reinforcement learning. In Proceedings of the In- ternational Conference on Machine Learning, volume 80, pages 1573-1581, 2018.</p>
<p>Ex2: Exploration with exemplar models for deep reinforcement learning. Justin Fu, John Co-Reyes, Sergey Levine, Advances in Neural Information Processing Systems. Justin Fu, John Co-Reyes, and Sergey Levine. Ex2: Exploration with exemplar models for deep reinforcement learning. In Advances in Neural Information Processing Systems, pages 2577-2587, 2017. URL https://arxiv.org/abs/1703.01260.</p>
<p>Multi-criteria reinforcement learning. Zoltán Gábor, Zsolt Kalmár, Csaba Szepesvári, ICML. 98Zoltán Gábor, Zsolt Kalmár, and Csaba Szepesvári. Multi-criteria reinforcement learning. In ICML, volume 98, pages 197-205, 1998.</p>
<p>A meta-mdp approach to exploration for lifelong reinforcement learning. M Francisco, Philip S Garcia, Thomas, Advances in neural information processing systems. Francisco M Garcia and Philip S Thomas. A meta-mdp approach to exploration for lifelong reinforcement learning. In Advances in neural information processing systems, 2019.</p>
<p>A theory of regularized markov decision processes. Matthieu Geist, Bruno Scherrer, Olivier Pietquin, arXiv:1901.11275arXivMatthieu Geist, Bruno Scherrer, and Olivier Pietquin. A theory of regularized markov decision processes. Technical Report arXiv:1901.11275, arXiv, 2019.</p>
<p>Bayesian reinforcement learning: A survey. Foundations and Trends® in Machine Learning. Mohammad Ghavamzadeh, Shie Mannor, Joelle Pineau, Aviv Tamar, 8Mohammad Ghavamzadeh, Shie Mannor, Joelle Pineau, Aviv Tamar, et al. Bayesian rein- forcement learning: A survey. Foundations and Trends® in Machine Learning, 8(5-6): 359-483, 2015.</p>
<p>Generative adversarial nets. Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, Yoshua Bengio, Advances in neural information processing systems. Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural information processing systems, pages 2672-2680, 2014.</p>
<p>Stable function approximation in dynamic programming. J Geoffrey, Gordon, Machine Learning Proceedings. ElsevierGeoffrey J Gordon. Stable function approximation in dynamic programming. In Machine Learning Proceedings 1995, pages 261-268. Elsevier, 1995.</p>
<p>Sample efficient reinforcement learning with gaussian processes. Robert Grande, Thomas Walsh, Jonathan How, International Conference on Machine Learning. Robert Grande, Thomas Walsh, and Jonathan How. Sample efficient reinforcement learning with gaussian processes. In International Conference on Machine Learning, pages 1332- 1340, 2014.</p>
<p>Autonomy in children's learning: An experimental and individual difference investigation. S Wendy, Richard M Grolnick, Ryan, Journal of personality and social psychology. 525890Wendy S Grolnick and Richard M Ryan. Autonomy in children's learning: An experimental and individual difference investigation. Journal of personality and social psychology, 52 (5):890, 1987.</p>
<p>Efficient bayes-adaptive reinforcement learning using sample-based search. Arthur Guez, David Silver, Peter Dayan, Advances in Neural Information Processing Systems. Arthur Guez, David Silver, and Peter Dayan. Efficient bayes-adaptive reinforcement learning using sample-based search. In Advances in Neural Information Processing Systems, pages 1025-1033, 2012.</p>
<p>A stochastic reinforcement learning algorithm for learning realvalued functions. Vijaykumar Gullapalli, Neural networks. 36Vijaykumar Gullapalli. A stochastic reinforcement learning algorithm for learning real- valued functions. Neural networks, 3(6):671-692, 1990.</p>
<p>A reinforcement learning approach to setting multi-objective goals for energy demand management. Ying Guo, Astrid Zeman, Rongxin Li, International Journal of Agent Technologies and Systems (IJATS). 12Ying Guo, Astrid Zeman, and Rongxin Li. A reinforcement learning approach to setting multi-objective goals for energy demand management. International Journal of Agent Technologies and Systems (IJATS), 1(2):55-70, 2009.</p>
<p>Concurrent PAC RL. Zhaohan Guo, Emma Brunskill, AAAI. Zhaohan Guo and Emma Brunskill. Concurrent PAC RL. In AAAI, pages 2624-2630, 2015.</p>
<p>Metareinforcement learning of structured exploration strategies. Abhishek Gupta, Russell Mendonca, Yuxuan Liu, Pieter Abbeel, Sergey Levine, Advances in Neural Information Processing Systems. Abhishek Gupta, Russell Mendonca, Yuxuan Liu, Pieter Abbeel, and Sergey Levine. Meta- reinforcement learning of structured exploration strategies. In Advances in Neural Infor- mation Processing Systems, pages 5302-5311, 2018.</p>
<p>Reinforcement learning with deep energy-based policies. Tuomas Haarnoja, Haoran Tang, Pieter Abbeel, Sergey Levine, Proceedings of the International Conference on Machine Learning. the International Conference on Machine LearningTuomas Haarnoja, Haoran Tang, Pieter Abbeel, and Sergey Levine. Reinforcement learn- ing with deep energy-based policies. In Proceedings of the International Conference on Machine Learning, 2017. xxxx</p>
<p>Soft actor-critic: Offpolicy maximum entropy deep reinforcement learning with a stochastic actor. Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, Sergey Levine, International Conference on Machine Learning. Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off- policy maximum entropy deep reinforcement learning with a stochastic actor. In Interna- tional Conference on Machine Learning, pages 1861-1870, 2018.</p>
<p>Completely derandomized self-adaptation in evolution strategies. Nikolaus Hansen, Andreas Ostermeier, Evolutionary computation. 92Nikolaus Hansen and Andreas Ostermeier. Completely derandomized self-adaptation in evolution strategies. Evolutionary computation, 9(2):159-195, 2001.</p>
<p>Deep reinforcement learning with double q-learning. Arthur Hado Van Hasselt, David Guez, Silver, Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence. the Thirtieth AAAI Conference on Artificial IntelligenceHado van Hasselt, Arthur Guez, and David Silver. Deep reinforcement learning with double q-learning. In Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence, pages 2094-2100, 2016.</p>
<p>Provably efficient maximum entropy exploration. Elad Hazan, M Sham, Karan Kakade, Abby Van Singh, Soest, International Conference on Machine Learning. Elad Hazan, Sham M Kakade, Karan Singh, and Abby Van Soest. Provably efficient maxi- mum entropy exploration. In International Conference on Machine Learning, pages 2681- 2691, 2019.</p>
<p>Nicolas Heess, Jonathan J Hunt, Timothy P Lillicrap, David Silver, arXiv:1512.04455Memory-based control with recurrent neural networks. arXiv preprintNicolas Heess, Jonathan J Hunt, Timothy P Lillicrap, and David Silver. Memory-based control with recurrent neural networks. arXiv preprint arXiv:1512.04455, 2015.</p>
<p>Diversity-driven exploration strategy for deep reinforcement learning. Zhang-Wei Hong, Tzu-Yun Shann, Shih-Yang Su, Yi-Hsiang Chang, Tsu-Jui Fu, Chun-Yi Lee, Advances in Neural Information Processing Systems. 31Zhang-Wei Hong, Tzu-Yun Shann, Shih-Yang Su, Yi-Hsiang Chang, Tsu-Jui Fu, and Chun- Yi Lee. Diversity-driven exploration strategy for deep reinforcement learning. Advances in Neural Information Processing Systems, 31:10489-10500, 2018.</p>
<p>Vime: Variational information maximizing exploration. Rein Houthooft, Xi Chen, Yan Duan, John Schulman, Filip De Turck, Pieter Abbeel, Advances in Neural Information Processing Systems. Rein Houthooft, Xi Chen, Yan Duan, John Schulman, Filip De Turck, and Pieter Abbeel. Vime: Variational information maximizing exploration. In Advances in Neural Informa- tion Processing Systems, pages 1109-1117, 2016.</p>
<p>Control of exploitation-exploration meta-parameter in reinforcement learning. Shin Ishii, Wako Yoshida, Junichiro Yoshimoto, Neural networks. 154-6Shin Ishii, Wako Yoshida, and Junichiro Yoshimoto. Control of exploitation-exploration meta-parameter in reinforcement learning. Neural networks, 15(4-6):665-687, 2002.</p>
<p>Near-optimal regret bounds for reinforcement learning. Thomas Jaksch, Ronald Ortner, Peter Auer, Journal of Machine Learning Research. 11Thomas Jaksch, Ronald Ortner, and Peter Auer. Near-optimal regret bounds for reinforce- ment learning. Journal of Machine Learning Research, 11(Apr):1563-1600, 2010.</p>
<p>Successor uncertainties: exploration and uncertainty in temporal difference learning. David Janz, Jiri Hron, Przemysław Mazur, Katja Hofmann, José Miguel Hernández-Lobato, Sebastian Tschiatschek, Advances in Neural Information Processing Systems. David Janz, Jiri Hron, Przemysław Mazur, Katja Hofmann, José Miguel Hernández-Lobato, and Sebastian Tschiatschek. Successor uncertainties: exploration and uncertainty in tem- poral difference learning. In Advances in Neural Information Processing Systems, pages 4509-4518, 2019.</p>
<p>Is q-learning provably efficient?. Chi Jin, Zeyuan Allen-Zhu, Sebastien Bubeck, Michael I Jordan , Advances in Neural Information Processing Systems. Chi Jin, Zeyuan Allen-Zhu, Sebastien Bubeck, and Michael I Jordan. Is q-learning provably efficient? In Advances in Neural Information Processing Systems, pages 4863-4873, 2018.</p>
<p>Discovering options for exploration by minimizing cover time. Yuu Jinnai, Jee Won, David Park, George Abel, Konidaris, arXiv:1903.00606arXiv preprintYuu Jinnai, Jee Won Park, David Abel, and George Konidaris. Discovering options for exploration by minimizing cover time. arXiv preprint arXiv:1903.00606, 2019.</p>
<p>Exploration in reinforcement learning with deep covering options. Yuu Jinnai, Jee Won, Park, C Marlos, George Machado, Konidaris, International Conference on Learning Representations. Yuu Jinnai, Jee Won Park, Marlos C Machado, and George Konidaris. Exploration in rein- forcement learning with deep covering options. In International Conference on Learning Representations, 2020.</p>
<p>Model-based exploration in continuous state spaces. K Nicholas, Peter Jong, Stone, International Symposium on Abstraction, Reformulation, and Approximation. SpringerNicholas K Jong and Peter Stone. Model-based exploration in continuous state spaces. In International Symposium on Abstraction, Reformulation, and Approximation, pages 258-272. Springer, 2007.</p>
<p>Generic constraints on underspecified target trajectories. Michael I Jordan, International Joint Conference on Neural Networks. New YorkIEEE Press1Michael I Jordan. Generic constraints on underspecified target trajectories. In International Joint Conference on Neural Networks, volume 1, pages 217-225. IEEE Press New York, 1989.</p>
<p>Learning in embedded systems. Leslie Pack, Kaelbling , MIT pressLeslie Pack Kaelbling. Learning in embedded systems. MIT press, 1993.</p>
<p>Reinforcement learning: A survey. Leslie Pack Kaelbling, Andrew W Michael L Littman, Moore, Journal of artificial intelligence research. 4Leslie Pack Kaelbling, Michael L Littman, and Andrew W Moore. Reinforcement learning: A survey. Journal of artificial intelligence research, 4:237-285, 1996.</p>
<p>Exploration in metric state spaces. Sham Kakade, J Michael, John Kearns, Langford, Proceedings of the 20th International Conference on Machine Learning (ICML-03). the 20th International Conference on Machine Learning (ICML-03)Sham Kakade, Michael J Kearns, and John Langford. Exploration in metric state spaces. In Proceedings of the 20th International Conference on Machine Learning (ICML-03), pages 306-312, 2003.</p>
<p>Near-optimal reinforcement learning in polynomial time. Michael Kearns, Satinder Singh, Machine learning. 492-3Michael Kearns and Satinder Singh. Near-optimal reinforcement learning in polynomial time. Machine learning, 49(2-3):209-232, 2002.</p>
<p>A sparse sampling algorithm for nearoptimal planning in large markov decision processes. Michael Kearns, Yishay Mansour, Andrew Y Ng, Machine learning. 492-3Michael Kearns, Yishay Mansour, and Andrew Y Ng. A sparse sampling algorithm for near- optimal planning in large markov decision processes. Machine learning, 49(2-3):193-208, 2002.</p>
<p>Vizdoom: A doom-based ai research platform for visual reinforcement learning. Michał Kempka, Marek Wydmuch, Grzegorz Runc, Jakub Toczek, Wojciech Jaśkowski, 2016 IEEE Conference on Computational Intelligence and Games (CIG). IEEEMichał Kempka, Marek Wydmuch, Grzegorz Runc, Jakub Toczek, and Wojciech Jaśkowski. Vizdoom: A doom-based ai research platform for visual reinforcement learning. In 2016 IEEE Conference on Computational Intelligence and Games (CIG), pages 1-8. IEEE, 2016.</p>
<p>Curiositybottleneck: Exploration by distilling task-specific novelty. Youngjin Kim, Wontae Nam, Hyunwoo Kim, Ji-Hoon Kim, Gunhee Kim, International Conference on Machine Learning. Youngjin Kim, Wontae Nam, Hyunwoo Kim, Ji-Hoon Kim, and Gunhee Kim. Curiosity- bottleneck: Exploration by distilling task-specific novelty. In International Conference on Machine Learning, pages 3379-3388, 2019.</p>
<p>Bandit based monte-carlo planning. Levente Kocsis, Csaba Szepesvári, European conference on machine learning. SpringerLevente Kocsis and Csaba Szepesvári. Bandit based monte-carlo planning. In European conference on machine learning, pages 282-293. Springer, 2006.</p>
<p>Policy gradient reinforcement learning for fast quadrupedal locomotion. Nate Kohl, Peter Stone, Proceedings. ICRA'04. 2004 IEEE International Conference on. ICRA'04. 2004 IEEE International Conference onIEEE3Robotics and AutomationNate Kohl and Peter Stone. Policy gradient reinforcement learning for fast quadrupedal locomotion. In Robotics and Automation, 2004. Proceedings. ICRA'04. 2004 IEEE Inter- national Conference on, volume 3, pages 2619-2624. IEEE, 2004.</p>
<p>Near-Bayesian exploration in polynomial time. Zico Kolter, Andrew Y Ng, International Conference on Machine Learning. ACMJ Zico Kolter and Andrew Y Ng. Near-Bayesian exploration in polynomial time. In Inter- national Conference on Machine Learning, pages 513-520. ACM, 2009.</p>
<p>Evolving neural networks in compressed weight space. Jan Koutnik, Faustino Gomez, Jürgen Schmidhuber, Proceedings of the 12th annual conference on Genetic and evolutionary computation. the 12th annual conference on Genetic and evolutionary computationACMJan Koutnik, Faustino Gomez, and Jürgen Schmidhuber. Evolving neural networks in com- pressed weight space. In Proceedings of the 12th annual conference on Genetic and evolu- tionary computation, pages 619-626. ACM, 2010.</p>
<p>Contextdependent upper-confidence bounds for directed exploration. Raksha Kumaraswamy, Matthew Schlegel, Adam White, Martha White, Advances in Neural Information Processing Systems. Raksha Kumaraswamy, Matthew Schlegel, Adam White, and Martha White. Context- dependent upper-confidence bounds for directed exploration. In Advances in Neural In- formation Processing Systems, pages 4779-4789, 2018.</p>
<p>Bandit algorithms. Tor Lattimore, Csaba Szepesvári, Cambridge University PressTor Lattimore and Csaba Szepesvári. Bandit algorithms. Cambridge University Press, 2020. xxxx</p>
<p>Sparse markov decision processes with causal sparse tsallis entropy regularization for reinforcement learning. Kyungjae Lee, Sungjoon Choi, Songhwai Oh, IEEE Robotics and Automation Letters. 33Kyungjae Lee, Sungjoon Choi, and Songhwai Oh. Sparse markov decision processes with causal sparse tsallis entropy regularization for reinforcement learning. IEEE Robotics and Automation Letters, 3(3):1466-1473, 2018.</p>
<p>AM: An artificial intelligence approach to discovery in mathematics as heuristic search. B Douglas, Lenat, Stanford university, department of computer scienceTechnical reportDouglas B Lenat. AM: An artificial intelligence approach to discovery in mathematics as heuristic search. Technical report, Stanford university, department of computer science, 1976.</p>
<p>Open problem: Regret bounds for thompson sampling. Lihong Li, Olivier Chapelle, Conference on Learning Theory. Lihong Li and Olivier Chapelle. Open problem: Regret bounds for thompson sampling. In Conference on Learning Theory, pages 43-1, 2012.</p>
<p>Continuous control with deep reinforcement learning. P Timothy, Jonathan J Lillicrap, Alexander Hunt, Nicolas Pritzel, Tom Heess, Yuval Erez, David Tassa, Daan Silver, Wierstra, International Conference on Learning Representations. Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. In International Conference on Learning Representations, 2016.</p>
<p>Self-improving reactive agents: Case studies of reinforcement learning frameworks. Long-Ji Lin, Proceedings of the International Conference on Simulation of Adaptive Behavior: From Animals to Animats. the International Conference on Simulation of Adaptive Behavior: From Animals to AnimatsLong-Ji Lin. Self-improving reactive agents: Case studies of reinforcement learning frame- works. In Proceedings of the International Conference on Simulation of Adaptive Behavior: From Animals to Animats, 1990.</p>
<p>Self-improving reactive agents based on reinforcement learning, planning and teaching. Long-Ji Lin, Machine learning. 83-4Long-Ji Lin. Self-improving reactive agents based on reinforcement learning, planning and teaching. Machine learning, 8(3-4):293-321, 1992.</p>
<p>Learning and exploration in actionperception loops. Daniel Ying-Jeh Little, Friedrich Tobias Sommer, Frontiers in neural circuits. 737Daniel Ying-Jeh Little and Friedrich Tobias Sommer. Learning and exploration in action- perception loops. Frontiers in neural circuits, 7:37, 2013.</p>
<p>Multiobjective reinforcement learning: A comprehensive overview. Chunming Liu, Xin Xu, Dewen Hu, IEEE Transactions on Systems, Man, and Cybernetics: Systems. 453Chunming Liu, Xin Xu, and Dewen Hu. Multiobjective reinforcement learning: A compre- hensive overview. IEEE Transactions on Systems, Man, and Cybernetics: Systems, 45(3): 385-398, 2015.</p>
<p>Exploration in model-based reinforcement learning by empirically estimating learning progress. Manuel Lopes, Tobias Lang, Marc Toussaint, Pierre-Yves Oudeyer, Advances in Neural Information Processing Systems. Manuel Lopes, Tobias Lang, Marc Toussaint, and Pierre-Yves Oudeyer. Exploration in model-based reinforcement learning by empirically estimating learning progress. In Ad- vances in Neural Information Processing Systems, pages 206-214, 2012.</p>
<p>A Laplacian framework for option discovery in reinforcement learning. C Marlos, Machado, G Marc, Michael Bellemare, Bowling, Proceedings of the 34th International Conference on Machine Learning. the 34th International Conference on Machine LearningMarlos C Machado, Marc G Bellemare, and Michael Bowling. A Laplacian framework for option discovery in reinforcement learning. In Proceedings of the 34th International Conference on Machine Learning, pages 2295-2304, 2017.</p>
<p>Count-based exploration with the successor representation. C Marlos, Machado, G Marc, Michael Bellemare, Bowling, arXiv:1807.11622arXivMarlos C Machado, Marc G Bellemare, and Michael Bowling. Count-based exploration with the successor representation. Technical Report arXiv:1807.11622, arXiv, 2018a.</p>
<p>Eigenoption discovery through the deep successor representation. C Marlos, Clemens Machado, Xiaoxiao Rosenbaum, Miao Guo, Gerald Liu, Murray Tesauro, Campbell, International Conference on Learning Representations. Marlos C. Machado, Clemens Rosenbaum, Xiaoxiao Guo, Miao Liu, Gerald Tesauro, and Murray Campbell. Eigenoption discovery through the deep successor representa- tion. In International Conference on Learning Representations, 2018b. URL https: //openreview.net/forum?id=Bk8ZcAxR-.</p>
<p>Count-based exploration with the successor representation. C Marlos, Machado, G Marc, Michael Bellemare, Bowling, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence34Marlos C Machado, Marc G Bellemare, and Michael Bowling. Count-based exploration with the successor representation. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 5125-5133, 2020.</p>
<p>Proto-value functions: Developmental reinforcement learning. Sridhar Mahadevan, Proceedings of the 22nd international conference on Machine learning. the 22nd international conference on Machine learningACMSridhar Mahadevan. Proto-value functions: Developmental reinforcement learning. In Pro- ceedings of the 22nd international conference on Machine learning, pages 553-560. ACM, 2005.</p>
<p>Scaling reinforcement learning to robotics by exploiting the subsumption architecture. Sridhar Mahadevan, Jonathan Connell, Machine Learning Proceedings. ElsevierSridhar Mahadevan and Jonathan Connell. Scaling reinforcement learning to robotics by exploiting the subsumption architecture. In Machine Learning Proceedings 1991, pages 328-332. Elsevier, 1991.</p>
<p>Automatic programming of behavior-based robots using reinforcement learning. Sridhar Mahadevan, Jonathan Connell, Artificial intelligence. 552-3Sridhar Mahadevan and Jonathan Connell. Automatic programming of behavior-based robots using reinforcement learning. Artificial intelligence, 55(2-3):311-365, 1992.</p>
<p>Maven: Multiagent variational exploration. Anuj Mahajan, Tabish Rashid, Mikayel Samvelyan, Shimon Whiteson, Advances in Neural Information Processing Systems. Anuj Mahajan, Tabish Rashid, Mikayel Samvelyan, and Shimon Whiteson. Maven: Multi- agent variational exploration. In Advances in Neural Information Processing Systems, pages 7611-7622, 2019.</p>
<p>Count-based exploration in feature space for reinforcement learning. Jarryd Martin, Tom Suraj Narayanan, Marcus Everitt, Hutter, Proceedings of the 26th International Joint Conference on Artificial Intelligence. the 26th International Joint Conference on Artificial IntelligenceJarryd Martin, S Suraj Narayanan, Tom Everitt, and Marcus Hutter. Count-based explo- ration in feature space for reinforcement learning. In Proceedings of the 26th International Joint Conference on Artificial Intelligence, pages 2471-2478, 2017.</p>
<p>Design of automatic guided vehicle systems. L William, Jack A Maxwell, Muckstadt, Iie Transactions. 142William L Maxwell and Jack A Muckstadt. Design of automatic guided vehicle systems. Iie Transactions, 14(2):114-124, 1982.</p>
<p>Exploration of multi-state environments: Local measures and back-propagation of uncertainty. Nicolas Meuleau, Paul Bourgine, Machine Learning. 35Nicolas Meuleau and Paul Bourgine. Exploration of multi-state environments: Local mea- sures and back-propagation of uncertainty. Machine Learning, 35(2):117-154, 1999.</p>
<p>Human-level control through deep reinforcement learning. Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, G Marc, Alex Bellemare, Martin Graves, Andreas K Riedmiller, Georg Fidjeland, Ostrovski, Nature. 5187540Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control through deep reinforcement learning. Nature, 518(7540):529-533, 2015.</p>
<p>Asynchronous methods for deep reinforcement learning. Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim Harley, David Silver, Koray Kavukcuoglu, International conference on machine learning. Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement learning. In International conference on machine learning, pages 1928- 1937, 2016.</p>
<p>Information-based learning by agents in unbounded state spaces. James A Shariq A Mobin, Fritz Arnemann, Sommer, Advances in Neural Information Processing Systems. Shariq A Mobin, James A Arnemann, and Fritz Sommer. Information-based learning by agents in unbounded state spaces. In Advances in Neural Information Processing Systems, pages 3023-3031, 2014.</p>
<p>State of the art-a survey of partially observable Markov decision processes: theory, models, and algorithms. George E Monahan, Management Science. 281George E Monahan. State of the art-a survey of partially observable Markov decision processes: theory, models, and algorithms. Management Science, 28(1):1-16, 1982.</p>
<p>Prioritized sweeping: Reinforcement learning with less data and less time. W Andrew, Moore, Christopher G Atkeson, Machine learning. 131Andrew W Moore and Christopher G Atkeson. Prioritized sweeping: Reinforcement learning with less data and less time. Machine learning, 13(1):103-130, 1993.</p>
<p>Efficient memory-based learning for robot control. Andrew William, Moore , University of CambridgePhD thesisAndrew William Moore. Efficient memory-based learning for robot control. PhD thesis, University of Cambridge, 1990.</p>
<p>Acquisition of stand-up behavior by a real robot using hierarchical reinforcement learning. Jun Morimoto, Kenji Doya, Robotics and Autonomous Systems. 361xxxxJun Morimoto and Kenji Doya. Acquisition of stand-up behavior by a real robot using hierarchical reinforcement learning. Robotics and Autonomous Systems, 36(1):37-51, 2001. xxxx</p>
<p>Discovering the structure of a reactive environment by exploration. C Michael, Jonathan Mozer, Bachrach, Advances in neural information processing systems. Michael C Mozer and Jonathan Bachrach. Discovering the structure of a reactive envi- ronment by exploration. In Advances in neural information processing systems, pages 439-446, 1990.</p>
<p>A dual back-propagation scheme for scalar reward learning. Paul Munro, Annual Conference of the Cognitive Science Society. Paul Munro. A dual back-propagation scheme for scalar reward learning. In Annual Con- ference of the Cognitive Science Society, pages 165-176, 1987.</p>
<p>Why does hierarchy (sometimes) work so well in reinforcement learning?. Ofir Nachum, Haoran Tang, Xingyu Lu, Shixiang Gu, Honglak Lee, Sergey Levine, 1909.10618arXivOfir Nachum, Haoran Tang, Xingyu Lu, Shixiang Gu, Honglak Lee, and Sergey Levine. Why does hierarchy (sometimes) work so well in reinforcement learning? Technical Report 1909.10618, arXiv, 2019.</p>
<p>Arun Nair, Praveen Srinivasan, Sam Blackwell, Cagdas Alcicek, Rory Fearon, Alessandro De Maria, Vedavyas Panneershelvam, Mustafa Suleyman, Charles Beattie, arXiv:1507.04296Stig Petersen, et al. Massively parallel methods for deep reinforcement learning. arXiv preprintArun Nair, Praveen Srinivasan, Sam Blackwell, Cagdas Alcicek, Rory Fearon, Alessan- dro De Maria, Vedavyas Panneershelvam, Mustafa Suleyman, Charles Beattie, Stig Pe- tersen, et al. Massively parallel methods for deep reinforcement learning. arXiv preprint arXiv:1507.04296, 2015.</p>
<p>A unified view of entropy-regularized Markov decision processes. Gergely Neu, Anders Jonsson, Vicenç Gómez, 1705.07798Technical ReportGergely Neu, Anders Jonsson, and Vicenç Gómez. A unified view of entropy-regularized Markov decision processes. Technical Report 1705.07798, arXiv, 2017.</p>
<p>Pegasus: A policy search method for large MDPs and POMDPs. Y Andrew, Michael Ng, Jordan, Proceedings of the Sixteenth conference on Uncertainty in artificial intelligence. the Sixteenth conference on Uncertainty in artificial intelligenceMorgan Kaufmann Publishers IncAndrew Y Ng and Michael Jordan. Pegasus: A policy search method for large MDPs and POMDPs. In Proceedings of the Sixteenth conference on Uncertainty in artificial intelligence, pages 406-415. Morgan Kaufmann Publishers Inc., 2000.</p>
<p>The truck backer-upper: An example of self-learning in neural networks. Derrick Nguyen, Bernard Widrow, Advanced neural computers. ElsevierDerrick Nguyen and Bernard Widrow. The truck backer-upper: An example of self-learning in neural networks. In Advanced neural computers, pages 11-19. Elsevier, 1990.</p>
<p>Multi-resolution exploration in continuous spaces. Ali Nouri, Michael L Littman, Advances in neural information processing systems. Ali Nouri and Michael L Littman. Multi-resolution exploration in continuous spaces. In Advances in neural information processing systems, pages 1209-1216, 2009.</p>
<p>Online regret bounds for undiscounted continuous reinforcement learning. Ronald Ortner, Daniil Ryabko, Advances in Neural Information Processing Systems. Ronald Ortner and Daniil Ryabko. Online regret bounds for undiscounted continuous rein- forcement learning. In Advances in Neural Information Processing Systems, pages 1763- 1771, 2012.</p>
<p>Why is posterior sampling better than optimism for reinforcement learning. Ian Osband, Benjamin Van Roy, International Conference on Machine Learning. Ian Osband and Benjamin Van Roy. Why is posterior sampling better than optimism for reinforcement learning? In International Conference on Machine Learning, pages 2701- 2710, 2017.</p>
<p>More) efficient reinforcement learning via posterior sampling. Ian Osband, Daniel Russo, Benjamin Van Roy, Advances in Neural Information Processing Systems. Ian Osband, Daniel Russo, and Benjamin Van Roy. (More) efficient reinforcement learning via posterior sampling. In Advances in Neural Information Processing Systems, pages 3003-3011, 2013.</p>
<p>Deep exploration via bootstrapped DQN. Ian Osband, Charles Blundell, Alexander Pritzel, Benjamin Van Roy, Advances in neural information processing systems. Ian Osband, Charles Blundell, Alexander Pritzel, and Benjamin Van Roy. Deep exploration via bootstrapped DQN. In Advances in neural information processing systems, pages 4026-4034, 2016a.</p>
<p>Generalization and exploration via randomized value functions. Ian Osband, Zheng Benjamin Van Roy, Wen, International Conference on Machine Learning. Ian Osband, Benjamin Van Roy, and Zheng Wen. Generalization and exploration via ran- domized value functions. In International Conference on Machine Learning, pages 2377- 2386, 2016b.</p>
<p>Generalization and exploration via randomized value functions. Ian Osband, Zheng Benjamin Van Roy, Wen, Proceedings of the 33rd International Conference on International Conference on Machine Learning. the 33rd International Conference on International Conference on Machine Learning48Ian Osband, Benjamin Van Roy, and Zheng Wen. Generalization and exploration via ran- domized value functions. In Proceedings of the 33rd International Conference on In- ternational Conference on Machine Learning -Volume 48, ICML'16, page 2377-2386. JMLR.org, 2016c.</p>
<p>Ian Osband, Daniel Russo, Zheng Wen, Benjamin Van Roy, arXiv:1703.07608Deep exploration via randomized value functions. arXiv preprintIan Osband, Daniel Russo, Zheng Wen, and Benjamin Van Roy. Deep exploration via randomized value functions. arXiv preprint arXiv:1703.07608, 2017.</p>
<p>Randomized prior functions for deep reinforcement learning. Ian Osband, John Aslanides, Albin Cassirer, Advances in Neural Information Processing Systems. Ian Osband, John Aslanides, and Albin Cassirer. Randomized prior functions for deep reinforcement learning. In Advances in Neural Information Processing Systems, pages 8617-8629, 2018.</p>
<p>Count-based exploration with neural density models. Georg Ostrovski, G Marc, Aaron Bellemare, Rémi Van Den Oord, Munos, arXiv:1703.01310arXiv preprintGeorg Ostrovski, Marc G Bellemare, Aaron van den Oord, and Rémi Munos. Count-based exploration with neural density models. arXiv preprint arXiv:1703.01310, 2017.</p>
<p>Intrinsic motivation systems for autonomous mental development. Pierre-Yves Oudeyer, Frdric Kaplan, Verena V Hafner, IEEE transactions on evolutionary computation. 112Pierre-Yves Oudeyer, Frdric Kaplan, and Verena V Hafner. Intrinsic motivation systems for autonomous mental development. IEEE transactions on evolutionary computation, 11(2): 265-286, 2007.</p>
<p>Curiosity-driven exploration by self-supervised prediction. Deepak Pathak, Pulkit Agrawal, Alexei A Efros, Trevor Darrell, International Conference on Machine Learning (ICML). 2017Deepak Pathak, Pulkit Agrawal, Alexei A Efros, and Trevor Darrell. Curiosity-driven ex- ploration by self-supervised prediction. In International Conference on Machine Learning (ICML), volume 2017, 2017.</p>
<p>Pac optimal exploration in continuous space Markov decision processes. Jason Pazis, Ronald Parr, AAAI. Jason Pazis and Ronald Parr. Pac optimal exploration in continuous space Markov decision processes. In AAAI, 2013.</p>
<p>Responsive elastic computing. Julien Perez, Cécile Germain-Renaud, Balázs Kégl, Charles Loomis, Proceedings of the 6th international conference industry session on Grids meets autonomic computing. the 6th international conference industry session on Grids meets autonomic computingACMJulien Perez, Cécile Germain-Renaud, Balázs Kégl, and Charles Loomis. Responsive elastic computing. In Proceedings of the 6th international conference industry session on Grids meets autonomic computing, pages 55-64. ACM, 2009.</p>
<p>Relative entropy policy search. Jan Peters, Katharina Mulling, Yasemin Altun, Twenty-Fourth AAAI Conference on Artificial Intelligence. Jan Peters, Katharina Mulling, and Yasemin Altun. Relative entropy policy search. In Twenty-Fourth AAAI Conference on Artificial Intelligence, 2010.</p>
<p>Parameter space noise for exploration. Matthias Plappert, Rein Houthooft, Prafulla Dhariwal, Szymon Sidor, Richard Y Chen, Xi Chen, Tamim Asfour, Pieter Abbeel, and Marcin Andrychowicz. International Conference on Learning RepresentationsMatthias Plappert, Rein Houthooft, Prafulla Dhariwal, Szymon Sidor, Richard Y. Chen, Xi Chen, Tamim Asfour, Pieter Abbeel, and Marcin Andrychowicz. Parameter space noise for exploration. In International Conference on Learning Representations, 2018.</p>
<p>An analytic solution to discrete Bayesian reinforcement learning. Pascal Poupart, Nikos Vlassis, Jesse Hoey, Kevin Regan, Proceedings of the International Conference on Machine Learning. the International Conference on Machine LearningACMPascal Poupart, Nikos Vlassis, Jesse Hoey, and Kevin Regan. An analytic solution to dis- crete Bayesian reinforcement learning. In Proceedings of the International Conference on Machine Learning, pages 697-704. ACM, 2006.</p>
<p>Efficient off-policy meta-reinforcement learning via probabilistic context variables. Kate Rakelly, Aurick Zhou, Deirdre Quillen, Chelsea Finn, Sergey Levine, arXiv:1903.08254arXiv preprintKate Rakelly, Aurick Zhou, Deirdre Quillen, Chelsea Finn, and Sergey Levine. Efficient off-policy meta-reinforcement learning via probabilistic context variables. arXiv preprint arXiv:1903.08254, 2019.</p>
<p>Optimistic exploration even with a pessimistic initialisation. Tabish Rashid, Bei Peng, Wendelin Boehmer, Shimon Whiteson, International Conference on Learning Representations. Tabish Rashid, Bei Peng, Wendelin Boehmer, and Shimon Whiteson. Optimistic explo- ration even with a pessimistic initialisation. In International Conference on Learning Representations, 2020.</p>
<p>On measures of entropy and information. Alfréd Rényi, Proceedings of the Fourth Berkeley Symposium on Mathematical Statistics and Probability. the Fourth Berkeley Symposium on Mathematical Statistics and Probability1Contributions to the Theory of Statistics. The Regents of the University of CaliforniaAlfréd Rényi et al. On measures of entropy and information. In Proceedings of the Fourth Berkeley Symposium on Mathematical Statistics and Probability, Volume 1: Contributions to the Theory of Statistics. The Regents of the University of California, 1961.</p>
<p>Neural fitted q iteration-first experiences with a data efficient neural reinforcement learning method. Martin Riedmiller, European Conference on Machine Learning. SpringerMartin Riedmiller. Neural fitted q iteration-first experiences with a data efficient neural reinforcement learning method. In European Conference on Machine Learning, pages 317-328. Springer, 2005.</p>
<p>Entropy-driven adaptive representation. P Justinian, Rosca, Proceedings of the workshop on genetic programming: From theory to real-world applications. the workshop on genetic programming: From theory to real-world applicationsCiteseer9Justinian P Rosca. Entropy-driven adaptive representation. In Proceedings of the workshop on genetic programming: From theory to real-world applications, volume 9, pages 23-32. Citeseer, 1995.</p>
<p>Promp: Proximal meta-policy search. Jonas Rothfuss, Dennis Lee, Ignasi Clavera, Tamim Asfour, Pieter Abbeel, International Conference on Learning Representations. Jonas Rothfuss, Dennis Lee, Ignasi Clavera, Tamim Asfour, and Pieter Abbeel. Promp: Proximal meta-policy search. In International Conference on Learning Representations, 2019.</p>
<p>Exploring parameter space in reinforcement learning. Thomas Rückstiess, Frank Sehnke, Tom Schaul, Daan Wierstra, Yi Sun, Jürgen Schmidhuber, Paladyn. 11Thomas Rückstiess, Frank Sehnke, Tom Schaul, Daan Wierstra, Yi Sun, and Jürgen Schmid- huber. Exploring parameter space in reinforcement learning. Paladyn, 1(1):14-24, 2010.</p>
<p>Eluder dimension and the sample complexity of optimistic exploration. Daniel Russo, Benjamin Van Roy, Advances in Neural Information Processing Systems. Daniel Russo and Benjamin Van Roy. Eluder dimension and the sample complexity of optimistic exploration. In Advances in Neural Information Processing Systems, pages 2256-2264, 2013.</p>
<p>Intrinsic and extrinsic motivations: Classic definitions and new directions. M Richard, Edward L Ryan, Deci, Contemporary educational psychology. 251Richard M Ryan and Edward L Deci. Intrinsic and extrinsic motivations: Classic definitions and new directions. Contemporary educational psychology, 25(1):54-67, 2000.</p>
<p>Evolution strategies as a scalable alternative to reinforcement learning. Tim Salimans, Jonathan Ho, Xi Chen, Szymon Sidor, Ilya Sutskever, arXiv:1703.03864arXiv preprintTim Salimans, Jonathan Ho, Xi Chen, Szymon Sidor, and Ilya Sutskever. Evolution strate- gies as a scalable alternative to reinforcement learning. arXiv preprint arXiv:1703.03864, 2017.</p>
<p>The starcraft multi-agent challenge. Mikayel Samvelyan, Tabish Rashid, Christian Schroeder De, Gregory Witt, Nantas Farquhar, Nardelli, G J Tim, Chia-Man Rudner, Hung, H S Philip, Jakob Torr, Shimon Foerster, Whiteson, Proceedings of the 18th International Conference on Autonomous Agents and MultiAgent Systems. the 18th International Conference on Autonomous Agents and MultiAgent SystemsInternational Foundation for Autonomous Agents and Multiagent SystemsMikayel Samvelyan, Tabish Rashid, Christian Schroeder de Witt, Gregory Farquhar, Nantas Nardelli, Tim GJ Rudner, Chia-Man Hung, Philip HS Torr, Jakob Foerster, and Shimon Whiteson. The starcraft multi-agent challenge. In Proceedings of the 18th International Conference on Autonomous Agents and MultiAgent Systems, pages 2186-2188. Interna- tional Foundation for Autonomous Agents and Multiagent Systems, 2019.</p>
<p>Making the world differentiable: On using self-supervised fully recurrent neural networks for dynamic reinforcement learning and planning in non-stationary environments. Jürgen Schmidhuber, FKI-126-90Technische Universität MünchenTechnical ReportJürgen Schmidhuber. Making the world differentiable: On using self-supervised fully recur- rent neural networks for dynamic reinforcement learning and planning in non-stationary environments. Technical Report FKI-126-90, Technische Universität München, 1990.</p>
<p>Curious model-building control systems. Jürgen Schmidhuber, IEEE International Joint Conference on. IEEENeural NetworksJürgen Schmidhuber. Curious model-building control systems. In Neural Networks, 1991. 1991 IEEE International Joint Conference on, pages 1458-1463. IEEE, 1991a.</p>
<p>A possibility for implementing curiosity and boredom in modelbuilding neural controllers. Jürgen Schmidhuber, Proc. of the international conference on simulation of adaptive behavior: From animals to animats. of the international conference on simulation of adaptive behavior: From animals to animatsJürgen Schmidhuber. A possibility for implementing curiosity and boredom in model- building neural controllers. In Proc. of the international conference on simulation of adaptive behavior: From animals to animats, pages 222-227, 1991b.</p>
<p>Trust region policy optimization. John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, Philipp Moritz, International Conference on Machine Learning. John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region policy optimization. In International Conference on Machine Learning, pages 1889- 1897, 2015.</p>
<p>Proximal policy optimization algorithms. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, Oleg Klimov, 1707.06347Technical ReportJohn Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. Technical Report 1707.06347, arXiv, 2017.</p>
<p>Learning novel domains through curiosity and conjecture. D Paul, Shaul Scott, Markovitch, IJCAI. Paul D Scott and Shaul Markovitch. Learning novel domains through curiosity and conjec- ture. In IJCAI, pages 669-674, 1989.</p>
<p>Parameter-exploring policy gradients. Frank Sehnke, Christian Osendorfer, Thomas Rückstieß, Alex Graves, Jan Peters, Jürgen Schmidhuber, Neural Networks. 234Frank Sehnke, Christian Osendorfer, Thomas Rückstieß, Alex Graves, Jan Peters, and Jür- gen Schmidhuber. Parameter-exploring policy gradients. Neural Networks, 23(4):551-559, 2010.</p>
<p>Learning to plan via deep optimistic value exploration. Tim Seyde, Wilko Schwarting, Sertac Karaman, Daniela L Rus, PMLRConference on Learning for Dynamics and Control. Alexandre M. Bayen, Ali Jadbabaie, George Pappas, Pablo A. Parrilo, Benjamin Recht, Claire Tomlin, and Melanie Zeilinger120Tim Seyde, Wilko Schwarting, Sertac Karaman, and Daniela L Rus. Learning to plan via deep optimistic value exploration. In Alexandre M. Bayen, Ali Jadbabaie, George Pappas, Pablo A. Parrilo, Benjamin Recht, Claire Tomlin, and Melanie Zeilinger, editors, Conference on Learning for Dynamics and Control, volume 120 of Proceedings of Machine Learning Research, pages 815-825. PMLR, 10-11 Jun 2020.</p>
<p>Model-based active exploration. Pranav Shyam, Wojciech Jaśkowski, Faustino Gomez, International Conference on Machine Learning. Pranav Shyam, Wojciech Jaśkowski, and Faustino Gomez. Model-based active exploration. In International Conference on Machine Learning, pages 5779-5788, 2019.</p>
<p>Daan Wierstra, and Martin Riedmiller. Deterministic policy gradient algorithms. David Silver, Guy Lever, Nicolas Heess, Thomas Degris, International Conference on International Conference on Machine Learning. David Silver, Guy Lever, Nicolas Heess, Thomas Degris, Daan Wierstra, and Martin Ried- miller. Deterministic policy gradient algorithms. In International Conference on Interna- tional Conference on Machine Learning, 2014.</p>
<p>Transfer of learning by composing solutions of elemental sequential tasks. Satinder Pal, Singh, Machine Learning. 8Satinder Pal Singh. Transfer of learning by composing solutions of elemental sequential tasks. Machine Learning, 8(3-4):323-339, 1992.</p>
<p>Incentivizing exploration in reinforcement learning with deep predictive models. Bradly C Stadie, Sergey Levine, Pieter Abbeel, 1507.00814arXivBradly C. Stadie, Sergey Levine, and Pieter Abbeel. Incentivizing exploration in reinforce- ment learning with deep predictive models. Technical Report 1507.00814, arXiv, 2015. URL http://arxiv.org/abs/1507.00814.</p>
<p>Some considerations on learning to explore via meta-reinforcement learning. Bradly C Stadie, Ge Yang, Rein Houthooft, Xi Chen, Yan Duan, Yuhuai Wu, Pieter Abbeel, Ilya Sutskever, ICLR Workshop track. Bradly C. Stadie, Ge Yang, Rein Houthooft, Xi Chen, Yan Duan, Yuhuai Wu, Pieter Abbeel, and Ilya Sutskever. Some considerations on learning to explore via meta-reinforcement learning. In ICLR Workshop track, 2018.</p>
<p>Reinforcement driven information acquisition in non-deterministic environments. Jan Storck, Sepp Hochreiter, Jürgen Schmidhuber, Proceedings of the international conference on artificial neural networks. the international conference on artificial neural networksParisCiteseer2Jan Storck, Sepp Hochreiter, and Jürgen Schmidhuber. Reinforcement driven information acquisition in non-deterministic environments. In Proceedings of the international confer- ence on artificial neural networks, Paris, volume 2, pages 159-164. Citeseer, 1995.</p>
<p>Exploration via model based interval estimation. Alexander Strehl, Michael Littman, International Conference on Machine Learning. Citeseer. Alexander Strehl and Michael Littman. Exploration via model based interval estimation. In International Conference on Machine Learning. Citeseer, 2004.</p>
<p>An analysis of model-based interval estimation for Markov decision processes. L Alexander, Strehl, Michael L Littman, Journal of Computer and System Sciences. 748Alexander L Strehl and Michael L Littman. An analysis of model-based interval estimation for Markov decision processes. Journal of Computer and System Sciences, 74(8):1309- 1331, 2008.</p>
<p>Pac model-free reinforcement learning. L Alexander, Lihong Strehl, Eric Li, John Wiewiora, Michael L Langford, Littman, Proceedings of the 23rd international conference on Machine learning. the 23rd international conference on Machine learningAlexander L Strehl, Lihong Li, Eric Wiewiora, John Langford, and Michael L Littman. Pac model-free reinforcement learning. In Proceedings of the 23rd international conference on Machine learning, pages 881-888, 2006. xxxx</p>
<p>A Bayesian framework for reinforcement learning. Malcolm Strens, International Conference on Machine Learning. Malcolm Strens. A Bayesian framework for reinforcement learning. In International Con- ference on Machine Learning, pages 943-950, 2000.</p>
<p>Path integral policy improvement with covariance matrix adaptation. Freek Stulp, Olivier Sigaud, International Conference on Machine Learning. Freek Stulp and Olivier Sigaud. Path integral policy improvement with covariance matrix adaptation. In International Conference on Machine Learning, 2012.</p>
<p>Integrated architectures for learning, planning, and reacting based on approximating dynamic programming. S Richard, Sutton, Machine Learning Proceedings. ElsevierRichard S Sutton. Integrated architectures for learning, planning, and reacting based on approximating dynamic programming. In Machine Learning Proceedings 1990, pages 216- 224. Elsevier, 1990.</p>
<p>Dyna, an integrated architecture for learning, planning, and reacting. S Richard, Sutton, ACM SIGART Bulletin. 24Richard S Sutton. Dyna, an integrated architecture for learning, planning, and reacting. ACM SIGART Bulletin, 2(4):160-163, 1991a.</p>
<p>Integrated modeling and control based on reinforcement learning and dynamic programming. S Richard, Sutton, Advances in neural information processing systems. Richard S Sutton. Integrated modeling and control based on reinforcement learning and dynamic programming. In Advances in neural information processing systems, pages 471- 478, 1991b.</p>
<p>Reinforcement learning architectures. S Richard, Sutton, Proceedings ISKIT'92 International Symposium on Neural Information Processing. ISKIT'92 International Symposium on Neural Information ProcessingCiteseerRichard S Sutton. Reinforcement learning architectures. In Proceedings ISKIT'92 Interna- tional Symposium on Neural Information Processing. Citeseer, 1992.</p>
<p>Generalization in reinforcement learning: Successful examples using sparse coarse coding. S Richard, Sutton, Advances in neural information processing systems. Richard S Sutton. Generalization in reinforcement learning: Successful examples using sparse coarse coding. In Advances in neural information processing systems, pages 1038- 1044, 1996.</p>
<p>Reinforcement learning: An introduction. Cambridge. S Richard, Andrew G Sutton, Barto, MIT PressMARichard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. Cam- bridge, MA: MIT Press, 1998a.</p>
<p>Reinforcement learning: an introduction mit press. S Richard, Andrew G Sutton, Barto, Cambridge, MARichard S Sutton and Andrew G Barto. Reinforcement learning: an introduction mit press. Cambridge, MA, 1998b.</p>
<p>Policy gradient methods for reinforcement learning with function approximation. S Richard, David A Sutton, Mcallester, P Satinder, Yishay Singh, Mansour, Advances in neural information processing systems. Richard S Sutton, David A McAllester, Satinder P Singh, and Yishay Mansour. Policy gradient methods for reinforcement learning with function approximation. In Advances in neural information processing systems, pages 1057-1063, 2000.</p>
<p>Algorithms for reinforcement learning. Csaba Szepesvári, Synthesis lectures on artificial intelligence and machine learning. 41Csaba Szepesvári. Algorithms for reinforcement learning. Synthesis lectures on artificial intelligence and machine learning, 4(1):1-103, 2010.</p>
<p>The many faces of optimism: a unifying approach. István Szita, András Lőrincz, Proceedings of the 25th international conference on Machine learning. the 25th international conference on Machine learningIstván Szita and András Lőrincz. The many faces of optimism: a unifying approach. In Proceedings of the 25th international conference on Machine learning, pages 1048-1055, 2008.</p>
<p>Model-based average reward reinforcement learning. Prasad Tadepalli, Dokyeong Ok, Artificial intelligence. 1001-2Prasad Tadepalli and DoKyeong Ok. Model-based average reward reinforcement learning. Artificial intelligence, 100(1-2):177-224, 1998.</p>
<h1>exploration: A study of countbased exploration for deep reinforcement learning. Haoran Tang, Rein Houthooft, Davis Foote, Adam Stooke, Openai Xi Chen, Yan Duan, John Schulman, Filip Deturck, Pieter Abbeel, Advances in Neural Information Processing Systems. Haoran Tang, Rein Houthooft, Davis Foote, Adam Stooke, OpenAI Xi Chen, Yan Duan, John Schulman, Filip DeTurck, and Pieter Abbeel. # exploration: A study of count- based exploration for deep reinforcement learning. In Advances in Neural Information Processing Systems, pages 2750-2759, 2017.</h1>
<p>A class of rapidly converging algorithms for learning automata. A L Mandayam, P S Thathachar, Sastry, IEEE International Conference on Cybernetics and Society. Mandayam A.L. Thathachar and P.S. Sastry. A class of rapidly converging algorithms for learning automata. In IEEE International Conference on Cybernetics and Society, pages 602-606, 1984.</p>
<p>A generalized path integral control approach to reinforcement learning. Evangelos Theodorou, Jonas Buchli, Stefan Schaal, Journal of Machine Learning Research. 11Evangelos Theodorou, Jonas Buchli, and Stefan Schaal. A generalized path integral control approach to reinforcement learning. Journal of Machine Learning Research, 11(Nov): 3137-3181, 2010.</p>
<p>On the likelihood that one unknown probability exceeds another in view of the evidence of two samples. William R Thompson, Biometrika. William R Thompson. On the likelihood that one unknown probability exceeds another in view of the evidence of two samples. Biometrika, 1933.</p>
<p>Planning with an adaptive world model. Sebastian Thrun, Knut Möller, Alexander Linden, Advances in neural information processing systems. Sebastian Thrun, Knut Möller, and Alexander Linden. Planning with an adaptive world model. In Advances in neural information processing systems, pages 450-456, 1991.</p>
<p>Efficient exploration in reinforcement learning. Sebastian B Thrun, CMU- CS-92-102Carnegie-Mellon UniversityTechnical ReportSebastian B Thrun. Efficient exploration in reinforcement learning. Technical Report CMU- CS-92-102, Carnegie-Mellon University, 1992.</p>
<p>Active exploration in dynamic environments. B Sebastian, Knut Thrun, Möller, Advances in neural information processing systems. Sebastian B Thrun and Knut Möller. Active exploration in dynamic environments. In Advances in neural information processing systems, pages 531-538, 1992.</p>
<p>Comparing exploration strategies for q-learning in random stochastic mazes. Arryon D Tijsma, M Madalina, Marco A Drugan, Wiering, 2016 IEEE Symposium Series on Computational Intelligence (SSCI). IEEEArryon D Tijsma, Madalina M Drugan, and Marco A Wiering. Comparing exploration strategies for q-learning in random stochastic mazes. In 2016 IEEE Symposium Series on Computational Intelligence (SSCI), pages 1-8. IEEE, 2016.</p>
<p>Naftali Tishby, C Fernando, William Pereira, Bialek, The information bottleneck method. arXiv preprint physics/0004057. Naftali Tishby, Fernando C Pereira, and William Bialek. The information bottleneck method. arXiv preprint physics/0004057, 2000.</p>
<p>Mujoco: A physics engine for model-based control. Emanuel Todorov, Tom Erez, Yuval Tassa, 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems. IEEEEmanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control. In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, pages 5026-5033. IEEE, 2012.</p>
<p>Adaptive ε-greedy exploration in reinforcement learning based on value differences. Michel Tokic, Annual Conference on Artificial Intelligence. SpringerMichel Tokic. Adaptive ε-greedy exploration in reinforcement learning based on value dif- ferences. In Annual Conference on Artificial Intelligence, pages 203-210. Springer, 2010.</p>
<p>Value-difference based exploration: adaptive control between epsilon-greedy and softmax. Michel Tokic, Günther Palm, Annual Conference on Artificial Intelligence. SpringerMichel Tokic and Günther Palm. Value-difference based exploration: adaptive control be- tween epsilon-greedy and softmax. In Annual Conference on Artificial Intelligence, pages 335-346. Springer, 2011.</p>
<p>Near-optimal optimistic reinforcement learning using empirical bernstein inequalities. Aristide Tossou, Debabrota Basu, Christos Dimitrakakis, arXiv:1905.12425arXiv preprintAristide Tossou, Debabrota Basu, and Christos Dimitrakakis. Near-optimal opti- mistic reinforcement learning using empirical bernstein inequalities. arXiv preprint arXiv:1905.12425, 2019.</p>
<p>Randomized value functions via multiplicative normalizing flows. Ahmed Touati, Harsh Satija, Joshua Romoff, Joelle Pineau, Pascal Vincent, UAIAhmed Touati, Harsh Satija, Joshua Romoff, Joelle Pineau, and Pascal Vincent. Random- ized value functions via multiplicative normalizing flows. UAI, 2019.</p>
<p>Epsilon-first policies for budget-limited multi-armed bandits. Long Tran-Thanh, Archie Chapman, Enrique Munoz De Cote, Alex Rogers, Nicholas R Jennings, Twenty-Fourth AAAI Conference on Artificial Intelligence. Long Tran-Thanh, Archie Chapman, Enrique Munoz de Cote, Alex Rogers, and Nicholas R Jennings. Epsilon-first policies for budget-limited multi-armed bandits. In Twenty-Fourth AAAI Conference on Artificial Intelligence, 2010.</p>
<p>Softmax exploration strategies for multiobjective reinforcement learning. Peter Vamplew, Richard Dazeley, Cameron Foale, Neurocomputing. 263Peter Vamplew, Richard Dazeley, and Cameron Foale. Softmax exploration strategies for multiobjective reinforcement learning. Neurocomputing, 263:74-86, 2017. xxxx</p>
<p>Conditional image generation with pixelcnn decoders. Aaron Van Den Oord, Nal Kalchbrenner, Lasse Espeholt, Oriol Vinyals, Alex Graves, Advances in neural information processing systems. Aaron Van den Oord, Nal Kalchbrenner, Lasse Espeholt, Oriol Vinyals, Alex Graves, et al. Conditional image generation with pixelcnn decoders. In Advances in neural information processing systems, pages 4790-4798, 2016.</p>
<p>Deep reinforcement learning with double q-learning. Arthur Hado Van Hasselt, David Guez, Silver, Thirtieth AAAI Conference on Artificial Intelligence. Hado van Hasselt, Arthur Guez, and David Silver. Deep reinforcement learning with double q-learning. In Thirtieth AAAI Conference on Artificial Intelligence, 2016.</p>
<p>Generalized exploration in policy search. Daniel Herke Van Hoof, Jan Tanneberg, Peters, Machine Learning. 106Herke van Hoof, Daniel Tanneberg, and Jan Peters. Generalized exploration in policy search. Machine Learning, 106(9-10):1705-1724, 2017.</p>
<p>Hypervolume-based multiobjective reinforcement learning. Kristof Van Moffaert, M Madalina, Ann Drugan, Nowé, International Conference on Evolutionary Multi-Criterion Optimization. SpringerKristof Van Moffaert, Madalina M Drugan, and Ann Nowé. Hypervolume-based multi- objective reinforcement learning. In International Conference on Evolutionary Multi- Criterion Optimization, pages 352-366. Springer, 2013a.</p>
<p>Scalarized multi-objective reinforcement learning: Novel design techniques. Kristof Van Moffaert, M Madalina, Ann Drugan, Nowé, 2013 IEEE Symposium on Adaptive Dynamic Programming and Reinforcement Learning (ADPRL). IEEEKristof Van Moffaert, Madalina M Drugan, and Ann Nowé. Scalarized multi-objective rein- forcement learning: Novel design techniques. In 2013 IEEE Symposium on Adaptive Dy- namic Programming and Reinforcement Learning (ADPRL), pages 191-199. IEEE, 2013b.</p>
<p>Multi-armed bandit algorithms and empirical evaluation. Joannes Vermorel, Mehryar Mohri, European conference on machine learning. SpringerJoannes Vermorel and Mehryar Mohri. Multi-armed bandit algorithms and empirical eval- uation. In European conference on machine learning, pages 437-448. Springer, 2005.</p>
<p>A perspective view and survey of meta-learning. Ricardo Vilalta, Youssef Drissi, Artificial intelligence review. 182Ricardo Vilalta and Youssef Drissi. A perspective view and survey of meta-learning. Artificial intelligence review, 18(2):77-95, 2002.</p>
<p>X Jane, Zeb Wang, Dhruva Kurth-Nelson, Hubert Tirumala, Joel Z Soyer, Remi Leibo, Charles Munos, Blundell, arXiv:1611.05763Dharshan Kumaran, and Matt Botvinick. Learning to reinforcement learn. arXiv preprintJane X Wang, Zeb Kurth-Nelson, Dhruva Tirumala, Hubert Soyer, Joel Z Leibo, Remi Munos, Charles Blundell, Dharshan Kumaran, and Matt Botvinick. Learning to rein- forcement learn. arXiv preprint arXiv:1611.05763, 2016a.</p>
<p>Bayesian sparse sampling for on-line reward optimization. Tao Wang, Daniel Lizotte, Michael Bowling, Dale Schuurmans, Proceedings of the 22nd international conference on Machine learning. the 22nd international conference on Machine learningACMTao Wang, Daniel Lizotte, Michael Bowling, and Dale Schuurmans. Bayesian sparse sam- pling for on-line reward optimization. In Proceedings of the 22nd international conference on Machine learning, pages 956-963. ACM, 2005.</p>
<p>Q-learning with ucb exploration is sample efficient for infinite-horizon mdp. Yuanhao Wang, Kefan Dong, Xiaoyu Chen, Liwei Wang, International Conference on Learning Representations. Yuanhao Wang, Kefan Dong, Xiaoyu Chen, and Liwei Wang. Q-learning with ucb explo- ration is sample efficient for infinite-horizon mdp. In International Conference on Learning Representations, 2020.</p>
<p>Dueling network architectures for deep reinforcement learning. Ziyu Wang, Tom Schaul, Matteo Hessel, Hado Hasselt, Marc Lanctot, Nando Freitas, International conference on machine learning. PMLRZiyu Wang, Tom Schaul, Matteo Hessel, Hado Hasselt, Marc Lanctot, and Nando Freitas. Dueling network architectures for deep reinforcement learning. In International conference on machine learning, pages 1995-2003. PMLR, 2016b.</p>
<p>Q-learning. Jch Christopher, Peter Watkins, Dayan, Machine learning. 83-4Christopher JCH Watkins and Peter Dayan. Q-learning. Machine learning, 8(3-4):279-292, 1992.</p>
<p>Learning from delayed rewards. Christopher John Cornish Hellaby Watkins, King's College, CambridgePhD thesisChristopher John Cornish Hellaby Watkins. Learning from delayed rewards. PhD thesis, King's College, Cambridge, 1989.</p>
<p>Control policy with autocorrelated noise in reinforcement learning for robotics. Pawel Wawrzynski, International Journal of Machine Learning and Computing. 5291Pawel Wawrzynski. Control policy with autocorrelated noise in reinforcement learning for robotics. International Journal of Machine Learning and Computing, 5(2):91, 2015.</p>
<p>Interval estimation for reinforcement-learning algorithms in continuous-state domains. Martha White, Adam White, Advances in Neural Information Processing Systems. Martha White and Adam White. Interval estimation for reinforcement-learning algorithms in continuous-state domains. In Advances in Neural Information Processing Systems, pages 2433-2441, 2010.</p>
<p>A complexity analysis of cooperative mechanisms in reinforcement learning. D Steven, Whitehead, AAAI. Steven D Whitehead. A complexity analysis of cooperative mechanisms in reinforcement learning. In AAAI, pages 607-613, 1991.</p>
<p>Learning to perceive and act by trial and error. D Steven, Whitehead, H Dana, Ballard, Machine Learning. 7Steven D Whitehead and Dana H Ballard. Learning to perceive and act by trial and error. Machine Learning, 7(1):45-83, 1991.</p>
<p>Efficient model-based exploration. Marco Wiering, Jürgen Schmidhuber, Proceedings of the Sixth International Conference on Simulation of Adaptive Behavior: From Animals to Animats. the Sixth International Conference on Simulation of Adaptive Behavior: From Animals to Animats6Marco Wiering and Jürgen Schmidhuber. Efficient model-based exploration. In Proceedings of the Sixth International Conference on Simulation of Adaptive Behavior: From Animals to Animats, volume 6, pages 223-228, 1998.</p>
<p>Explorations in efficient reinforcement learning. A Marco, Wiering, University of AmsterdamPhD thesisMarco A Wiering. Explorations in efficient reinforcement learning. PhD thesis, University of Amsterdam, 1999.</p>
<p>Simple statistical gradient-following algorithms for connectionist reinforcement learning. J Ronald, Williams, Machine learning. 83-4Ronald J Williams. Simple statistical gradient-following algorithms for connectionist rein- forcement learning. Machine learning, 8(3-4):229-256, 1992.</p>
<p>Function optimization using connectionist reinforcement learning algorithms. J Ronald, Jing Williams, Peng, Connection Science. 33Ronald J Williams and Jing Peng. Function optimization using connectionist reinforcement learning algorithms. Connection Science, 3(3):241-268, 1991.</p>
<p>Yifan Wu, George Tucker, Ofir Nachum, arXiv:1810.04586The laplacian in rl: Learning representations with efficient approximations. arXiv preprintYifan Wu, George Tucker, and Ofir Nachum. The laplacian in rl: Learning representations with efficient approximations. arXiv preprint arXiv:1810.04586, 2018.</p>
<p>Exploration and inference in learning from reinforcement. Jeremy Wyatt, University of Edinburgh. College of Science and Engineering. School of Informatics.PhD thesisJeremy Wyatt. Exploration and inference in learning from reinforcement. PhD thesis, Uni- versity of Edinburgh. College of Science and Engineering. School of Informatics., 1998.</p>
<p>Learning to explore with meta-policy gradient. Tianbing Xu, Qiang Liu, Liang Zhao, Jian Peng, Proceedings of the International Conference on Machine Learning. the International Conference on Machine Learning80Tianbing Xu, Qiang Liu, Liang Zhao, and Jian Peng. Learning to explore with meta-policy gradient. In Proceedings of the International Conference on Machine Learning, volume 80, pages 5459-5468, 2018.</p>
<p>Maximum entropy-based optimal threshold selection using deterministic reinforcement learning with controlled randomization. Peng-Yeng Yin, Signal Processing. 827Peng-Yeng Yin. Maximum entropy-based optimal threshold selection using deterministic reinforcement learning with controlled randomization. Signal Processing, 82(7):993-1006, 2002.</p>
<p>Online learning in episodic markovian decision processes by relative entropy policy search. Alexander Zimin, Gergely Neu, Advances in Neural Information Processing Systems. 26Alexander Zimin and Gergely Neu. Online learning in episodic markovian decision processes by relative entropy policy search. In Advances in Neural Information Processing Systems 26, pages 1583-1591, 2013.</p>
<p>Variational task embeddings for fast adaptation in deep reinforcement learning. Luisa Zintgraf, Maximilian Igl, Kyriacos Shiarlis, Anuj Mahajan, Katja Hofmann, Shimon Whiteson, Workshop on "Structure &amp; Priors in Reinforcement Learning" at ICLR. Luisa Zintgraf, Maximilian Igl, Kyriacos Shiarlis, Anuj Mahajan, Katja Hofmann, and Shi- mon Whiteson. Variational task embeddings for fast adaptation in deep reinforcement learning. In Workshop on "Structure &amp; Priors in Reinforcement Learning" at ICLR, 2019a.</p>
<p>Fast context adaptation via meta-learning. Luisa Zintgraf, Kyriacos Shiarli, Vitaly Kurin, Katja Hofmann, Shimon Whiteson, PMLRInternational Conference on Machine Learning. Long Beach, California, USA97Luisa Zintgraf, Kyriacos Shiarli, Vitaly Kurin, Katja Hofmann, and Shimon Whiteson. Fast context adaptation via meta-learning. In International Conference on Machine Learning, volume 97, pages 7693-7702, Long Beach, California, USA, 2019b. PMLR.</p>            </div>
        </div>

    </div>
</body>
</html>
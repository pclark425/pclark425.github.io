<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-476 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-476</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-476</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-18.html">extraction-schema-18</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <p><strong>Paper ID:</strong> paper-151c7cb45c75ac5573a6c03b2ea089b34512898c</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/151c7cb45c75ac5573a6c03b2ea089b34512898c" target="_blank">State of the Art: Reproducibility in Artificial Intelligence</a></p>
                <p><strong>Paper Venue:</strong> AAAI Conference on Artificial Intelligence</p>
                <p><strong>Paper TL;DR:</strong> The reproducibility scores decrease with in- creased documentation requirements and improvement over time is found, indicating that AI research is not documented well enough to reproduce the reported results.</p>
                <p><strong>Paper Abstract:</strong> 
 
 Background: Research results in artificial intelligence (AI) are criticized for not being reproducible. Objective: To quantify the state of reproducibility of empirical AI research using six reproducibility metrics measuring three different degrees of reproducibility. Hypotheses: 1) AI research is not documented well enough to reproduce the reported results. 2) Documentation practices have improved over time. Method: The literature is reviewed and a set of variables that should be documented to enable reproducibility are grouped into three factors: Experiment, Data and Method. The metrics describe how well the factors have been documented for a paper. A total of 400 research papers from the conference series IJCAI and AAAI have been surveyed using the metrics. Findings: None of the papers document all of the variables. The metrics show that between 20% and 30% of the variables for each factor are documented. One of the metrics show statistically significant increase over time while the others show no change. Interpretation: The reproducibility scores decrease with in- creased documentation requirements. Improvement over time is found. Conclusion: Both hypotheses are supported.
 
</p>
                <p><strong>Cost:</strong> 0.011</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e476.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e476.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>missing_method_details</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Omitted or ambiguous method documentation in papers</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Natural-language descriptions of the AI method frequently omit high-level motivations, pseudo-code, parameter descriptions, and explicit statements of hypotheses and goals, causing a gap between what the paper describes and what an independent implementer needs to reproduce the method.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Empirical AI experiment reporting</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>The standard research-paper-oriented experimental reporting pipeline used in AI conferences (method description, data description, experiment description, results).</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>research paper methods section (narrative descriptions, motivation, pseudo-code)</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>alternative implementation / independent reimplementation (not provided by authors)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>incomplete specification / ambiguous description</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Papers commonly do not include explicit high-level descriptions of the method's motivation, pseudo-code, or parameter/sub-procedure descriptions; explicit mentions of problem, objective, hypothesis, and contribution are often missing, leaving implementers with under-specified algorithmic details and implicit assumptions that the paper's audience must infer.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>method specification (algorithm description, pseudo-code, parameter definitions)</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>manual survey of 400 papers for explicit mentions; reviewers searched for explicit markers (problem, goal, hypothesis, contribution, pseudo-code) and registered presence/absence as boolean variables.</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>Boolean registration of method-related variables and aggregated RXD metric R3D (Method(e) computed as weighted sum of method variables); reported rates (e.g., no paper documented all five method variables; 90% of papers document two or fewer method variables) and R3D means (0.26 ± CI).</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Reduces ability to independently reimplement methods (R2/R3 reproduction); contributes to zero papers meeting full R1/R2/R3 boolean criteria. Quantitatively, method documentation contributed to low RXD scores (R3D mean ≈ 0.26), indicating only ~26% of method-related variables documented on average.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Very common: none of the surveyed papers documented all method variables; ~90% of papers documented two or fewer of the five method variables examined.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Authors omit explicit, unambiguous descriptions (implicit assumptions), space/format constraints, cultural expectations that readers 'know' the domain, and lack of strong publication requirements for method disclosure.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Require explicit statements in papers (problem, objective, hypothesis, pseudo-code, parameter descriptions); public release of code and runnable artifacts; adoption of reporting checklists and reviewer enforcement.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Partial: aggregate reproducibility metrics show modest improvement over time in experiment-related documentation (statistically significant increase in R1D), but no clear significant improvement for method documentation alone (R3D unchanged), indicating limited effectiveness so far.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>artificial intelligence / machine learning</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'State of the Art: Reproducibility in Artificial Intelligence', 'publication_date_yy_mm': '2018-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e476.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e476.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>missing_data_sharing</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Incomplete or missing data sharing and dataset specification</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Papers often fail to share training/validation/test splits, dataset versions, or actual outputs, creating discrepancies between the natural-language data description and the data needed to reproduce experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>AI experimental data pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>The data component of AI experiments including dataset provenance, split definitions, versions, and recorded outputs used for evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>data description within papers (dataset provenance, split descriptions, versioning)</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>dataset files, data-splitting scripts, provided ground-truth / result files (when shared) or absent data artifacts</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>missing preprocessing / incomplete dataset specification / missing outputs</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>The natural-language descriptions often mention datasets but do not provide the exact splits, versions, pre-processing steps, or the actual outputs; in many cases the paper does not publish the training, validation, and/or test sets or the experiment outputs needed for direct replication.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>data preprocessing and dataset specification (data splits, versions, and experimental outputs)</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>manual binary coding during the survey for presence/absence of training/validation/test data and result outputs; reviewers logged whether each dataset component and results were shared.</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>Boolean variables aggregated into Data(e) and RXD metrics (R2D and R1D include Data factor). Reported statistics: e.g., overall data sharing observed in ~49% for some data variables, Data-related RXD component mean contributing to R1D=0.24 and R2D=0.25.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Prevents full R1 reproducibility and weakens R2 reproducibility because re-running or reimplementing on the same data is impossible when splits/outputs are absent; contributes to low RXD scores (only ~20–30% of required data variables documented).</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Moderately common: some data sharing present (e.g., ~49% share some data), but full data documentation (training+validation+test+results) required for Data(e)=true was rare, leading to Data contributions being low across the sample.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Journals/conferences do not uniformly require data sharing; dataset versions change; logistical/time/effort costs to prepare and host data; privacy/licensing constraints; implicit assumptions about data accessibility.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Encourage/mandate dataset release and versioning, provide exact split files or splitting scripts, deposit outputs and processed datasets in public repositories or alongside code, adopt policies requiring data availability statements.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Limited in the survey timeframe: overall RXD scores improved slightly over 2013→2016 but R2D (which depends on data sharing) did not show a statistically significant increase, indicating limited measured effectiveness to date.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>artificial intelligence / machine learning</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'State of the Art: Reproducibility in Artificial Intelligence', 'publication_date_yy_mm': '2018-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e476.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e476.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>missing_experiment_code_and_env</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Absent experiment code, ancillary software, and environment specifications</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Authors frequently omit experiment scripts, hyperparameter settings, exact software dependencies, and hardware specifications, creating a gap between the paper's experiment description and the runnable implementation needed to replicate results.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Experiment execution environment (ancillary software and scripts)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>The experiment-level artifacts: environment (OS, libraries), experiment scripts, hyperparameter files, and hardware specifications used to run training and evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>experiment description / setup section in papers (hypotheses, predictions, hyperparameters, environment specs)</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>experiment scripts, environment manifests, hyperparameter/config files, and container/VM images (often not provided)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>hyperparameter mismatch / missing experiment script / incomplete environment specification</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>While some papers report experiment setup at a high level, many do not publish the experiment code, full hyperparameter lists, or exact software/hardware stack; this leads to ambiguity and prevents exact re-running (R1) even when method and data are available.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>training procedure, hyperparameters, ancillary software and hardware environment</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Manual presence/absence coding for variables such as experiment source code, hardware specifications, software dependencies, and experiment setup; aggregation into Experiment(e) used in RXD metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>Boolean experiment-related variables combined into Exp(e) and used to compute R1D; reported experiment-setup variable scored relatively high compared to other experiment variables, but full experiment documentation required for R1(e) was not achieved for any paper (RX metrics all 0).</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Prevents experiment reproducibility (R1): no paper met the full boolean R1 criterion; contributes to low overall RXD (R1D mean ≈ 0.24); differences in hardware/software can materially affect numeric results (floating point differences cited).</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Common: experiment setup (e.g., hyperparameters) had higher reporting rates than other experiment variables, but experiment source code and full environment specifications were often missing; no paper provided all experiment variables required for R1.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Time/effort to package and release runnable experiments, lack of incentives or venue requirements to release scripts/envs, cost/maintenance burden of hosting VMs or containers, and implicit assumptions about reproducibility from prose alone.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Share experiment scripts, hyperparameter files, software dependency lists, and hardware specs; provide containers/VM snapshots or cloud images; require experiment artifacts as part of publication.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Partially positive: the survey found a statistically significant increase in R1D (experiment-documentation metric) from 2013/2014 to 2016, implying experiment-level documentation improved somewhat; however, full R1 reproducibility remained unattained for all papers.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>artificial intelligence / machine learning</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'State of the Art: Reproducibility in Artificial Intelligence', 'publication_date_yy_mm': '2018-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e476.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e476.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>explicit_measurement_framework_RXD</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RX / RXD reproducibility measurement framework (R1/R2/R3 and R1D/R2D/R3D)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A formal, paper-specific set of boolean and graded metrics that operationalize reproducibility degrees by checking documentation of Method, Data, and Experiment factors and computing strict (RX) and degree (RXD) scores.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Reproducibility quantification framework used in survey</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A metrics framework mapping three factors (Method, Data, Experiment) to three degrees of reproducibility (R1 experiment, R2 data, R3 method), implemented as strict boolean indicators and weighted fractional scores (RXD).</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>survey coding schema and documentation checklist derived from literature (used to compare paper prose to expected artifacts)</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>manual binary coding results and aggregated statistical calculations (not an automatic tool)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>metric-based detection of documentation gaps (operational mismatch between claimed descriptions and provided artifacts)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>The framework detects mismatches by requiring presence of documented variables for each factor; strict RX booleans require all variables and were zero for all surveyed papers, while RXD fractional scores quantified partial documentation (R1D≈0.24, R2D≈0.25, R3D≈0.26).</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>meta-level analysis across method, data, and experiment documentation</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Manual registration of 0/1 variables per paper for predefined items, aggregation into boolean RX and fractional RXD metrics, statistical analysis across 400 sampled papers with confidence intervals and year comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>R1(e)=Method∧Data∧Exp, R2(e)=Method∧Data, R3(e)=Method (booleans); R1D/R2D/R3D computed as weighted sums (uniform weights) of factor variables; reported means and 95% confidence intervals per conference and aggregated (e.g., Total R1D=0.24±0.01).</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Provided quantitative evidence that current documentation practices are insufficient: no paper met strict RX criteria, and fractional RXD means show only ~20–30% of required documentation present; used to support claims about reproducibility state and trends over time (R1D increase significant).</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Not a frequency statistic itself; used to reveal prevalence (e.g., RX booleans = 0 for all papers; RXD scores between 0.20 and 0.30 across conferences).</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Lack of standard reporting requirements and inconsistent paper-level disclosures made it necessary to create a concrete checklist to measure/document gaps objectively.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Use the RXD framework as a reviewer checklist or policy instrument to require/document items; encourage repositories and artifacts to satisfy checklist items.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>The framework successfully quantified documentation deficits and detected a statistically significant improvement in R1D over time; utility as a policy/enforcement tool remains to be validated in practice.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>empirical AI research methodology / meta-research</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'State of the Art: Reproducibility in Artificial Intelligence', 'publication_date_yy_mm': '2018-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Electronic documents give reproducible research a new meaning <em>(Rating: 2)</em></li>
                <li>Wavelab and reproducible research <em>(Rating: 2)</em></li>
                <li>What does research reproducibility mean? <em>(Rating: 2)</em></li>
                <li>Repeatability in computer systems research <em>(Rating: 2)</em></li>
                <li>Ten simple rules for reproducible computational research <em>(Rating: 2)</em></li>
                <li>Reproducible research in computational science <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-476",
    "paper_id": "paper-151c7cb45c75ac5573a6c03b2ea089b34512898c",
    "extraction_schema_id": "extraction-schema-18",
    "extracted_data": [
        {
            "name_short": "missing_method_details",
            "name_full": "Omitted or ambiguous method documentation in papers",
            "brief_description": "Natural-language descriptions of the AI method frequently omit high-level motivations, pseudo-code, parameter descriptions, and explicit statements of hypotheses and goals, causing a gap between what the paper describes and what an independent implementer needs to reproduce the method.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Empirical AI experiment reporting",
            "system_description": "The standard research-paper-oriented experimental reporting pipeline used in AI conferences (method description, data description, experiment description, results).",
            "nl_description_type": "research paper methods section (narrative descriptions, motivation, pseudo-code)",
            "code_implementation_type": "alternative implementation / independent reimplementation (not provided by authors)",
            "gap_type": "incomplete specification / ambiguous description",
            "gap_description": "Papers commonly do not include explicit high-level descriptions of the method's motivation, pseudo-code, or parameter/sub-procedure descriptions; explicit mentions of problem, objective, hypothesis, and contribution are often missing, leaving implementers with under-specified algorithmic details and implicit assumptions that the paper's audience must infer.",
            "gap_location": "method specification (algorithm description, pseudo-code, parameter definitions)",
            "detection_method": "manual survey of 400 papers for explicit mentions; reviewers searched for explicit markers (problem, goal, hypothesis, contribution, pseudo-code) and registered presence/absence as boolean variables.",
            "measurement_method": "Boolean registration of method-related variables and aggregated RXD metric R3D (Method(e) computed as weighted sum of method variables); reported rates (e.g., no paper documented all five method variables; 90% of papers document two or fewer method variables) and R3D means (0.26 ± CI).",
            "impact_on_results": "Reduces ability to independently reimplement methods (R2/R3 reproduction); contributes to zero papers meeting full R1/R2/R3 boolean criteria. Quantitatively, method documentation contributed to low RXD scores (R3D mean ≈ 0.26), indicating only ~26% of method-related variables documented on average.",
            "frequency_or_prevalence": "Very common: none of the surveyed papers documented all method variables; ~90% of papers documented two or fewer of the five method variables examined.",
            "root_cause": "Authors omit explicit, unambiguous descriptions (implicit assumptions), space/format constraints, cultural expectations that readers 'know' the domain, and lack of strong publication requirements for method disclosure.",
            "mitigation_approach": "Require explicit statements in papers (problem, objective, hypothesis, pseudo-code, parameter descriptions); public release of code and runnable artifacts; adoption of reporting checklists and reviewer enforcement.",
            "mitigation_effectiveness": "Partial: aggregate reproducibility metrics show modest improvement over time in experiment-related documentation (statistically significant increase in R1D), but no clear significant improvement for method documentation alone (R3D unchanged), indicating limited effectiveness so far.",
            "domain_or_field": "artificial intelligence / machine learning",
            "reproducibility_impact": true,
            "uuid": "e476.0",
            "source_info": {
                "paper_title": "State of the Art: Reproducibility in Artificial Intelligence",
                "publication_date_yy_mm": "2018-04"
            }
        },
        {
            "name_short": "missing_data_sharing",
            "name_full": "Incomplete or missing data sharing and dataset specification",
            "brief_description": "Papers often fail to share training/validation/test splits, dataset versions, or actual outputs, creating discrepancies between the natural-language data description and the data needed to reproduce experiments.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "AI experimental data pipeline",
            "system_description": "The data component of AI experiments including dataset provenance, split definitions, versions, and recorded outputs used for evaluation.",
            "nl_description_type": "data description within papers (dataset provenance, split descriptions, versioning)",
            "code_implementation_type": "dataset files, data-splitting scripts, provided ground-truth / result files (when shared) or absent data artifacts",
            "gap_type": "missing preprocessing / incomplete dataset specification / missing outputs",
            "gap_description": "The natural-language descriptions often mention datasets but do not provide the exact splits, versions, pre-processing steps, or the actual outputs; in many cases the paper does not publish the training, validation, and/or test sets or the experiment outputs needed for direct replication.",
            "gap_location": "data preprocessing and dataset specification (data splits, versions, and experimental outputs)",
            "detection_method": "manual binary coding during the survey for presence/absence of training/validation/test data and result outputs; reviewers logged whether each dataset component and results were shared.",
            "measurement_method": "Boolean variables aggregated into Data(e) and RXD metrics (R2D and R1D include Data factor). Reported statistics: e.g., overall data sharing observed in ~49% for some data variables, Data-related RXD component mean contributing to R1D=0.24 and R2D=0.25.",
            "impact_on_results": "Prevents full R1 reproducibility and weakens R2 reproducibility because re-running or reimplementing on the same data is impossible when splits/outputs are absent; contributes to low RXD scores (only ~20–30% of required data variables documented).",
            "frequency_or_prevalence": "Moderately common: some data sharing present (e.g., ~49% share some data), but full data documentation (training+validation+test+results) required for Data(e)=true was rare, leading to Data contributions being low across the sample.",
            "root_cause": "Journals/conferences do not uniformly require data sharing; dataset versions change; logistical/time/effort costs to prepare and host data; privacy/licensing constraints; implicit assumptions about data accessibility.",
            "mitigation_approach": "Encourage/mandate dataset release and versioning, provide exact split files or splitting scripts, deposit outputs and processed datasets in public repositories or alongside code, adopt policies requiring data availability statements.",
            "mitigation_effectiveness": "Limited in the survey timeframe: overall RXD scores improved slightly over 2013→2016 but R2D (which depends on data sharing) did not show a statistically significant increase, indicating limited measured effectiveness to date.",
            "domain_or_field": "artificial intelligence / machine learning",
            "reproducibility_impact": true,
            "uuid": "e476.1",
            "source_info": {
                "paper_title": "State of the Art: Reproducibility in Artificial Intelligence",
                "publication_date_yy_mm": "2018-04"
            }
        },
        {
            "name_short": "missing_experiment_code_and_env",
            "name_full": "Absent experiment code, ancillary software, and environment specifications",
            "brief_description": "Authors frequently omit experiment scripts, hyperparameter settings, exact software dependencies, and hardware specifications, creating a gap between the paper's experiment description and the runnable implementation needed to replicate results.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Experiment execution environment (ancillary software and scripts)",
            "system_description": "The experiment-level artifacts: environment (OS, libraries), experiment scripts, hyperparameter files, and hardware specifications used to run training and evaluation.",
            "nl_description_type": "experiment description / setup section in papers (hypotheses, predictions, hyperparameters, environment specs)",
            "code_implementation_type": "experiment scripts, environment manifests, hyperparameter/config files, and container/VM images (often not provided)",
            "gap_type": "hyperparameter mismatch / missing experiment script / incomplete environment specification",
            "gap_description": "While some papers report experiment setup at a high level, many do not publish the experiment code, full hyperparameter lists, or exact software/hardware stack; this leads to ambiguity and prevents exact re-running (R1) even when method and data are available.",
            "gap_location": "training procedure, hyperparameters, ancillary software and hardware environment",
            "detection_method": "Manual presence/absence coding for variables such as experiment source code, hardware specifications, software dependencies, and experiment setup; aggregation into Experiment(e) used in RXD metrics.",
            "measurement_method": "Boolean experiment-related variables combined into Exp(e) and used to compute R1D; reported experiment-setup variable scored relatively high compared to other experiment variables, but full experiment documentation required for R1(e) was not achieved for any paper (RX metrics all 0).",
            "impact_on_results": "Prevents experiment reproducibility (R1): no paper met the full boolean R1 criterion; contributes to low overall RXD (R1D mean ≈ 0.24); differences in hardware/software can materially affect numeric results (floating point differences cited).",
            "frequency_or_prevalence": "Common: experiment setup (e.g., hyperparameters) had higher reporting rates than other experiment variables, but experiment source code and full environment specifications were often missing; no paper provided all experiment variables required for R1.",
            "root_cause": "Time/effort to package and release runnable experiments, lack of incentives or venue requirements to release scripts/envs, cost/maintenance burden of hosting VMs or containers, and implicit assumptions about reproducibility from prose alone.",
            "mitigation_approach": "Share experiment scripts, hyperparameter files, software dependency lists, and hardware specs; provide containers/VM snapshots or cloud images; require experiment artifacts as part of publication.",
            "mitigation_effectiveness": "Partially positive: the survey found a statistically significant increase in R1D (experiment-documentation metric) from 2013/2014 to 2016, implying experiment-level documentation improved somewhat; however, full R1 reproducibility remained unattained for all papers.",
            "domain_or_field": "artificial intelligence / machine learning",
            "reproducibility_impact": true,
            "uuid": "e476.2",
            "source_info": {
                "paper_title": "State of the Art: Reproducibility in Artificial Intelligence",
                "publication_date_yy_mm": "2018-04"
            }
        },
        {
            "name_short": "explicit_measurement_framework_RXD",
            "name_full": "RX / RXD reproducibility measurement framework (R1/R2/R3 and R1D/R2D/R3D)",
            "brief_description": "A formal, paper-specific set of boolean and graded metrics that operationalize reproducibility degrees by checking documentation of Method, Data, and Experiment factors and computing strict (RX) and degree (RXD) scores.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Reproducibility quantification framework used in survey",
            "system_description": "A metrics framework mapping three factors (Method, Data, Experiment) to three degrees of reproducibility (R1 experiment, R2 data, R3 method), implemented as strict boolean indicators and weighted fractional scores (RXD).",
            "nl_description_type": "survey coding schema and documentation checklist derived from literature (used to compare paper prose to expected artifacts)",
            "code_implementation_type": "manual binary coding results and aggregated statistical calculations (not an automatic tool)",
            "gap_type": "metric-based detection of documentation gaps (operational mismatch between claimed descriptions and provided artifacts)",
            "gap_description": "The framework detects mismatches by requiring presence of documented variables for each factor; strict RX booleans require all variables and were zero for all surveyed papers, while RXD fractional scores quantified partial documentation (R1D≈0.24, R2D≈0.25, R3D≈0.26).",
            "gap_location": "meta-level analysis across method, data, and experiment documentation",
            "detection_method": "Manual registration of 0/1 variables per paper for predefined items, aggregation into boolean RX and fractional RXD metrics, statistical analysis across 400 sampled papers with confidence intervals and year comparisons.",
            "measurement_method": "R1(e)=Method∧Data∧Exp, R2(e)=Method∧Data, R3(e)=Method (booleans); R1D/R2D/R3D computed as weighted sums (uniform weights) of factor variables; reported means and 95% confidence intervals per conference and aggregated (e.g., Total R1D=0.24±0.01).",
            "impact_on_results": "Provided quantitative evidence that current documentation practices are insufficient: no paper met strict RX criteria, and fractional RXD means show only ~20–30% of required documentation present; used to support claims about reproducibility state and trends over time (R1D increase significant).",
            "frequency_or_prevalence": "Not a frequency statistic itself; used to reveal prevalence (e.g., RX booleans = 0 for all papers; RXD scores between 0.20 and 0.30 across conferences).",
            "root_cause": "Lack of standard reporting requirements and inconsistent paper-level disclosures made it necessary to create a concrete checklist to measure/document gaps objectively.",
            "mitigation_approach": "Use the RXD framework as a reviewer checklist or policy instrument to require/document items; encourage repositories and artifacts to satisfy checklist items.",
            "mitigation_effectiveness": "The framework successfully quantified documentation deficits and detected a statistically significant improvement in R1D over time; utility as a policy/enforcement tool remains to be validated in practice.",
            "domain_or_field": "empirical AI research methodology / meta-research",
            "reproducibility_impact": true,
            "uuid": "e476.3",
            "source_info": {
                "paper_title": "State of the Art: Reproducibility in Artificial Intelligence",
                "publication_date_yy_mm": "2018-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Electronic documents give reproducible research a new meaning",
            "rating": 2,
            "sanitized_title": "electronic_documents_give_reproducible_research_a_new_meaning"
        },
        {
            "paper_title": "Wavelab and reproducible research",
            "rating": 2,
            "sanitized_title": "wavelab_and_reproducible_research"
        },
        {
            "paper_title": "What does research reproducibility mean?",
            "rating": 2,
            "sanitized_title": "what_does_research_reproducibility_mean"
        },
        {
            "paper_title": "Repeatability in computer systems research",
            "rating": 2,
            "sanitized_title": "repeatability_in_computer_systems_research"
        },
        {
            "paper_title": "Ten simple rules for reproducible computational research",
            "rating": 2,
            "sanitized_title": "ten_simple_rules_for_reproducible_computational_research"
        },
        {
            "paper_title": "Reproducible research in computational science",
            "rating": 1,
            "sanitized_title": "reproducible_research_in_computational_science"
        }
    ],
    "cost": 0.01090875,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>State of the Art: Reproducibility in Artificial Intelligence</h1>
<p>Odd Erik Gundersen, Sigbjørn Kjensmo<br>Department of Computer Science<br>Norwegian University of Science and Technology</p>
<h4>Abstract</h4>
<p>Background: Research results in artificial intelligence (AI) are criticized for not being reproducible. Objective: To quantify the state of reproducibility of empirical AI research using six reproducibility metrics measuring three different degrees of reproducibility. Hypotheses: 1) AI research is not documented well enough to reproduce the reported results. 2) Documentation practices have improved over time. Method: The literature is reviewed and a set of variables that should be documented to enable reproducibility are grouped into three factors: Experiment, Data and Method. The metrics describe how well the factors have been documented for a paper. A total of 400 research papers from the conference series IJCAI and AAAI have been surveyed using the metrics. Findings: None of the papers document all of the variables. The metrics show that between $20 \%$ and $30 \%$ of the variables for each factor are documented. One of the metrics show statistically significant increase over time while the others show no change. Interpretation: The reproducibility scores decrease with increased documentation requirements. Improvement over time is found. Conclusion: Both hypotheses are supported.</p>
<h2>Introduction</h2>
<p>Although reproducibility is a cornerstone of science, a large amount of published research results cannot be reproduced. This is even the case for results published in the most prestigious journals; even the original researchers cannot reproduce their own results (Aarts et al. 2016; Begley and Ellis 2012; Begley and Ioannidis 2014; Prinz, Schlange, and Asadullah 2011). (Goodman, Fanelli, and Ioannidis 2016) presents data from Scopus that shows that the problem with reproducibility spans several scientific fields. According to (Donoho et al. 2009) "it is impossible to verify most of the computational results presented at conferences and in papers today". This was confirmed by (Collberg and Proebsting 2016). Out of 402 experimental papers they were able to repeat $32.3 \%$ without communicating with the author, rising to $48.3 \%$ with communication. Papers by authors with industry affiliation showed a lower rate of reproducibility. They also found that some researchers are not willing to share code and data, while those that actually share, provide too little to repeat the experiment.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Guidelines, best-practices and solutions to aid reproducibility point towards open data and open source code as requirements for reproducible research (Sandve et al. 2013; Stodden and Miguez 2014). The increased focus on reproducibility has resulted in an increased adoption of data and code sharing policies for journals (Stodden, Guo, and Ma 2013). Still, proposed solutions for facilitating reproducibility see little adoption due to low ease-of-use and the time required to retroactively fit an experiment to these solutions (Gent and Kotthoff 2014). (Braun and Ong 2014) argues that automation should be possible to a higher degree for machine learning, as everything needed is available on a computer. Despite of this, the percentage of research that is reproducible is not higher for machine learning and artificial intelligence (AI) research (Hunold and Träff 2013; Fokkens et al. 2013; Hunold 2015).</p>
<p>The scientific method is based on reproducibility; "if other researchers can't repeat an experiment and get the same result as the original researchers, then they refute the hypothesis" (Oates 2006, p. 285). Hence, the inability to reproduce results affects the trustworthiness of science. To ensure high trustworthiness of AI and machine learning research measures must be taken to increase its reproducibility. However, before measures can be taken, the state of reproducibility in AI research must be documented. The state of reproducibility can only be documented if a proper framework is built.</p>
<p>Our objective is to quantify the state of reproducibility of empirical AI research, and our main hypothesis is that the documentation of AI research is not good enough to reproduce the reported results. We also investigate a second hypothesis, which is that documentation practices have improved during recent years. Two predictions were made, one for each hypothesis. The first prediction is that the current documentation practices at top AI conferences render most of the reported research results irreproducible, and the second prediction is that a larger portion of the reported research results are reproducible when comparing the latest installments of conferences to earlier installments. We surveyed research papers from the two top AI conference series, International Joint Conference on AI (IJCAI) and the Association for the Advancement of AI (AAAI) to test the hypotheses. Our contributions are twofold: i) an investigation of what reproducibility means for AI research and ii) a</p>
<p>quantification of the state of reproducibility of AI research.</p>
<h2>Reproducing Results</h2>
<p>We base the survey on a concise definition of reproducibility and three degrees of reproducibility. These definitions are based a review of the scientific method and the literature.</p>
<h2>The Scientific Method in AI Research</h2>
<p>Different strategies for researching information systems and computing exist (Oates 2006), and these include theory development and experiments among others. The scientific method and reproducibility is closely connected to experiments and empirical studies. We can distinguish between four different classes of empirical studies: 1) exploratory, 2) assessment, 3) manipulation and 4) observational studies (Cohen 1995). While exploratory and assessment studies are conducted to identify and suggest possible hypotheses, manipulation and observational studies test explicit and precise hypotheses. Although the scientific method is based on evaluating hypothesis, exploratory and assessment studies are not mandatory sub-processes of it. However, they may be conducted in order to formulate the hypotheses.</p>
<p>The targets of study in AI research are AI programs and their behavior (Cohen 1995). Changes to the AI program's structure, the task or the environment can affect the program's behavior. An AI program implements an abstract algorithm or system as a program that can be compiled and executed. Hence, the AI program is something distinct from the conceptual idea that it implements, which we will refer to as an AI method. Experiments should be formulated in such a way that it is clear whether they test hypotheses about the AI program or the AI method. Examples of tasks performed by AI methods include classification, planning, learning, decision making and ranking. The environment of the AI program is described by data. Typically, when performing AI experiments in supervised learning, the available data has to be divided into a training set, a validation set and a test set (Russell and Norvig 2009).</p>
<p>According to the scientific method and before performing an experiment, one should formulate one or more hypotheses about the AI program under investigation and make predictions about its behavior. The results of the experiments are interpreted by comparing their results to the hypotheses and the predictions. Beliefs about the AI program should be adjusted by this interpretation. The adjusted beliefs can be used to formulate new hypotheses, so that new experiments can be conducted. If executed honestly with earnest interpretations of the results, the scientific method updates our beliefs about an AI program so that they should converge towards objective truth. Figure 1 illustrates the scientific process of AI research as described here.</p>
<h2>The Terminology of Reproducibility</h2>
<p>While researchers in computer science agree that empirical results should be reproducible, what is meant by reproducibility is neither clearly defined nor agreed upon. (Stodden 2011) distinguishes between replication and reproduction. Replication is seen as re-running the experiment with
code and data provided by the author, while reproduction is a broader term "implying both replication and the regeneration of findings with at least some independence from the [original] code and/or data". (Drummond 2009) states that replication, as the weakest form of reproducibility, can only achieve checks for fraud. Due to the inconsistencies in the use of the terms replicability and reproducibility, (Goodman, Fanelli, and Ioannidis 2016) proposes to extend reproducibility into:</p>
<p>Methods reproducibility: The ability to implement, as exactly as possible, the experimental and computational procedures, with the same data and tools, to obtain the same results.
Results reproducibility: The production of corroborating results in a new study, having used the same experimental methods.
Inferential reproducibility: The drawing of qualitatively similar conclusions from either an independent replication of a study or a reanalysis of the original study.
Replication, as used by (Drummond 2009) and (Stodden 2011), is in line with methods reproducibility as proposed by (Goodman, Fanelli, and Ioannidis 2016) while reproducibility seems to entail both results reproducibility and inferential reproducibility. (Peng 2011) on the other hand suggests that reproducibility is on a spectrum from publication to full replication. This view neglects that results produced by AI methods can be reproduced using different data or different implementations. Results generated by using other implementations or other data can lead to new interpretations, which broadens the beliefs about the AI method, so that generalizations can be made. Despite the disagreements in terminology, there is a clear agreement on the fact that the reproducibility of research results is not one thing, but that empirical research can be assigned to some sort of spectrum, scale or ranking that is decided based on the level of documentation.</p>
<h2>Reproducibility</h2>
<p>We define reproducibility in the following way:
Definition. Reproducibility in empirical AI research is the ability of an independent research team to produce the same results using the same AI method based on the documentation made by the original research team.</p>
<p>Hence, reproducible research is empirical research that is documented in such detail by a research team that other researchers can produce the same results using the same AI method. According to (Sandve et al. 2013), a minimal requirement of reproducibility is that you should at least be able to reproduce the results yourself. We interpret this as repeatability and not reproducibility. Our view is that an important aspect of reproducibility is that the experiment is conducted independently. We will briefly discuss the three terms AI method, results and independent research team in this section. The next section is devoted to documentation.</p>
<p>An independent research team is one that conducts the experiment by only using the documentation made by the original research team.Enabling others to reproduce the same</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: By comparing the results of an experiment to the hypotheses and predictions that are being made about the AI program, we interpret the results and adjust our beliefs about them.
results is closely related to trust. Most importantly, other researchers can be expected to be more objective. They have no interest in inflating the performance of a method they have not developed themselves. More practically, they will not share the same preconceptions and implicit knowledge as the first team reporting the research. Also, other researchers will not share the exact same hardware running the exact same copies of software. All of this helps controlling for noise variables related to both the hardware and ancillary software as well as implicit knowledge and preconceptions.</p>
<p>The distinction between the AI program and the AI method is important. We must as far as possible remove any uncertainties to whether other effects than the AI method are responsible for the results. The concept of using an agent system for solving some problem is different from the specific implementation of the agent system. If the results are dependent on the implementation of the method, the hardware it is running on or the experiment setup, then the characteristics of the AI method do not cause the results.</p>
<p>The results are the output of the experiment, in other words, the dependent variables of the experiment (Cohen 1995), which typically are captured by performance measures. The result is the target of the investigation when reproducing an experiment; we want to ensure that the performance of the AI method is the same even if we change the implementation, the operating system or the hardware that is being used to conduct the experiment. As long as the results of the original and the reproduced experiments are the same, the original experiment is reproducible. What constitutes the same results depends on to which degree the results are reproduced.</p>
<h2>Documenting for Reproducibility</h2>
<p>In order to reproduce the results of an experiment, the documentation must include relevant information, which must be specified to a certain level of detail. What is relevant and how detailed the documentation must be are guided by whether it is possible to reproduce the results of the experiment using this information only. Hence, the color of the researcher's jacket is usually not relevant for reproducing the results. Which operating system is used on the machine when executing the experiment can very well be relevant though.</p>
<p>So what exactly is relevant information? The objective of (Claerbout and Karrenbach 1992; Buckheit and Donoho 1995) was to make it easy to rerun experiments and trace methods that produced the reported results. For (Claerbout
and Karrenbach 1992), this meant sharing everything on a CD-ROM, so that anyone could read the research report and execute the experiments by pushing a button attached to every figure. (Buckheit and Donoho 1995) shared Wavelab, a Matlab package, that made all the code needed for reproducing their figures in one of their papers. (Goodman, Fanelli, and Ioannidis 2016) highlights that "reporting of all relevant aspects of scientific design, conduct, measurements, data and analysis" is necessary for all three types of reproducibility. This is in line with the view of (Stodden 2011), which is that availability of the computational environment is necessary for computational reproducibility. (Peng 2011) argues that a paper alone is not enough, but that linked and executable code and data is the gold standard. We have grouped the documentation into three categories: 1) method, 2) data and 3) experiment.</p>
<p>Method: The method documentation includes the AI method that the AI program implements as well as the a motivation of why the method is used. As the implementation does not contain the motivation and intended behavior, sharing the implementation of the AI program is not enough. It is important to give a high-level description of the AI method that is being tested. This includes what the AI method intends to do, why it is needed and how it works. To decrease ambiguity, a description of how a method works should contain pseudo code and an explanation of the pseudo code containing descriptions of the parameters and sub-procedures. Sharing of the AI method is the objective of most research papers in AI. The problem that is investigated must be specified, the objective of the research must be clear and so must the research method being used.</p>
<p>Data: Sharing the data used in the experiment is getting simpler with open data repositories, such as the UCI Machine Learning Repository (Lichman 2013). Reproducing the results fully requires the procedure for how the data set has been divided into training, validation and test sets and which samples belong to the different sets. Sharing the validation set might not be necessary when all samples in the training set are used or might be hard when the method picks the samples randomly during the experiment. Data sets often change, so specifying the version is relevant. Finally in order to compare results, the actual output of the experiment, such as the classes or decisions made, are required.</p>
<p>Experiment: For others to reproduce the results of an experiment, the experiment and its setup must be shared. The experiment contains code as well as an experiment description. Proper experiment documentation must explain</p>
<p>the purpose of the experiment. The hypotheses that are tested and the predictions about these must be documented, and so must the results and the analysis. In order to rule out the possibilities that the results can be attributed to the hardware or ancillary software, the hardware and ancillary software used must be properly specified. The ancillary software includes, but is not restricted to, the operating system, programming environment and programming libraries used for implementing the experiment. Sharing the experiment code is not limited to open sourcing the AI program that is investigated, but sharing of the experiment setup with all independent variables, such as hyperparameters, as well as the scripts and environmental settings is required. The experiment setup consists of independent variables that control the experiment. These variables configure both the ancillary software and the AI program. Hyperparameters are independent variables that configure the AI method and examples include the number of leaves or depth of a tree and the learning rate. Documented code increases transparency.</p>
<p>In conclusion, there are different degrees to how well an empirical study in AI research can be documented. The degrees depend on whether the method, the data and the experiment are documented and how well they are documented. The gold standard is sharing all of the three groups of documentation through access to a running virtual machine in the cloud containing all the data, runnables, documentation and source code, as this includes the hardware and software stack as well and not only the software libraries used for running the experiments which was the case with the proposed solutions by (Claerbout and Karrenbach 1992; Buckheit and Donoho 1995). This is not necessarily practical, as it requires costly infrastructure that has a high maintenance cost. Another practical consideration is related to how long the infrastructure should and can be guaranteed to run and produce the same results.</p>
<h2>Degrees of Reproducibility</h2>
<p>We propose to distinguish between three different degrees of reproducibility, where an increased degree of reproducibility conveys an increased generality of the AI method. An increased generality means that the performance of the AI method documented in the experiment is not related to one specific implementation or the data used in the experiment; the AI method is more general than that. The three degrees of reproducibility are defined as follows:
R1: Experiment Reproducible The results of an experiment are experiment reproducible when the execution of the same implementation of an AI method produces the same results when executed on the same data.
R2: Data Reproducible The results of an experiment are data reproducible when an experiment is conducted that executes an alternative implementation of the AI method that produces the same results when executed on the same data.
R3: Method Reproducible The results of an experiment are method reproducible when the execution of an alternative implementation of the AI method produces the same results when executed on different data.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Method</th>
<th style="text-align: center;">Data</th>
<th style="text-align: center;">Experiment</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">R1</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">R2</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">R3</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Figure 2: The three degrees of reproducibility are defined by which documentation is used to reproduce the results.</p>
<p>Results that are R1 reproducible require the same software and data used for conducting the experiment and a detailed description of the AI method and experiment. This is what is called fully reproducible by (Peng 2011) and method reproducibility by (Goodman, Fanelli, and Ioannidis 2016). We call it experiment reproducible as everything required to run the experiment is needed to reproduce the results. The results when re-running the experiment should be exactly the same, as reported in the original experiment. Any differences can only be attributed to differences in hardware given that the ancillary software is the same.</p>
<p>Results that are R2 reproducible require only the method description and the data in order to be reproduced. This removes any noise variables related to implementation and hardware. The belief that the result is being caused by the AI method is strengthened. Hence, the generality of the AI method is increased compared to an AI method that is R1 reproducible. As the results are achieved by running the AI method on the same data as the original experiment, there is still a possibility that the performance can only be achieved using the same data. The results that are produced, the performance, using a different implementation should be the same if not exactly the same. Differences in results can be attributed to different implementations and hardware, such as different ways of doing floating point arithmetic. However, differences in software and hardware could have significant impact on results because of rounding errors in floating point arithmetic (Hong et al. 2013).</p>
<p>Results that are R3 reproducible only requires the method documentation to be reproduced. If the results are reproduced, all noise variables related to implementation, hardware and data have been removed, and it is safe to assume that the results are caused by the AI method. As the results are produced by using a new implementation on a new data set, the AI method is generalized to other data and the implementation used in the original experiment. In order for a result to be R3 reproducible the results of the experiments must support the same hypotheses and thus support the same beliefs. The same interpretations cannot be made unless the results are statistically significant, so the analysis should be supported by statistical hypothesis testing with a pvalue of 0.005 for claiming new discoveries (Johnson 2013; Benjamin et al. 2017).</p>
<p>When it comes to generality of the results the following is true: $R 1&lt;R 2&lt;R 3$, which means that R1 reproducible results are less general than R2 reproducible results, which in turn are less general than R3 reproducible results. How-</p>
<p>ever, when it comes to the documentation required, the following is the case: $\operatorname{doc}(R 3) \subset \operatorname{doc}(R) 2 \subset \operatorname{doc}(R 1)$. The documentation needed for R3 reproducibility is a subset of the documentation required for R2 reproducibility and the documentation required for R2 is a subset of the documentation required for R1 reproducibility. R3 reproducible is the most general reproducibility degree that also requires the least amount of information.</p>
<p>Current practice of publishers is not to require researchers to share data and implementation when publishing research papers. The current practice enables R3 reproducible results that have the least amount of transparency. For (Goodman, Fanelli, and Ioannidis 2016), the goal of transparency is to ease evaluation of the weight of evidence from studies to facilitate future studies on actual knowledge gaps and cumulative knowledge, and reduce time spent exploring blind alleys from poorly reported research. This means that current practices enable other research teams to reproduce results at the highest reproducibility degree with the least effort of the original research team. The majority of effort in reproducing results, lays with the independent team, instead of the original team. Transparency does not only reduce the effort needed to reproduce the results, but it also builds trust in them. Hence, the results that are produced by current practices are the least trustworthy from a reproducibility point of view, because of the lack in transparency; the evidence showing that the results are valid is not published.</p>
<h2>Research Method</h2>
<p>We have conducted an observational experiment in form of a survey of research papers in order to generate quantitative data about the state of reproducibility of research results in AI. The research papers have been reviewed, and a set of variables have been manually registered. In order to compare results between papers and conferences, we propose six metrics for deciding whether research results are R1, R2, and R3 reproducible as well as to which degree they are.</p>
<h2>Survey</h2>
<p>In order to evaluate the two hypotheses, we have surveyed a total of 400 papers where 100 papers have been selected from each of the 2013 and 2016 installments of the conference IJCAI and from the 2014 and 2016 installments of the conference series AAAI. With an exception of 50 papers from IJCAI 2013, all the papers have been selected randomly to avoid any selection biases. Table 1 shows the number of accepted papers (the population size), the number of surveyed papers (sample size) and the margin of errors for a confidence level of $95 \%$ for the four conferences. We have computed the margin of error as half the width of the confidence interval, and for our study the margin of error is $4.29 \%$. All the data and the code that has been used to calculate the reproducibility scores and generate the figures can be found on Github ${ }^{1}$.</p>
<p>Table 1: Population size, sample size (with number of empirical studies) and margin of error for a confidence level of $95 \%$ for the four conferences and total population.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Conference</th>
<th style="text-align: center;">Population size</th>
<th style="text-align: center;">Sample size</th>
<th style="text-align: center;">MoE</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">IJCAI 2013</td>
<td style="text-align: center;">413</td>
<td style="text-align: center;">$100(71)$</td>
<td style="text-align: center;">$8.54 \%$</td>
</tr>
<tr>
<td style="text-align: left;">AAAI 2014</td>
<td style="text-align: center;">213</td>
<td style="text-align: center;">$100(85)$</td>
<td style="text-align: center;">$7.15 \%$</td>
</tr>
<tr>
<td style="text-align: left;">IJCAI 2016</td>
<td style="text-align: center;">551</td>
<td style="text-align: center;">$100(84)$</td>
<td style="text-align: center;">$8.87 \%$</td>
</tr>
<tr>
<td style="text-align: left;">AAAI 2016</td>
<td style="text-align: center;">549</td>
<td style="text-align: center;">$100(85)$</td>
<td style="text-align: center;">$8.87 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Total</td>
<td style="text-align: center;">1726</td>
<td style="text-align: center;">$400(325)$</td>
<td style="text-align: center;">$4.30 \%$</td>
</tr>
</tbody>
</table>
<h2>Factors and Variables</h2>
<p>We have identified a set of variables that we believe are good indicators for reproducibility after reviewing the literature. These variables have been grouped together into the three factors Method, Data and Experiment. For each surveyed paper, we have registered these variables. In addition, we have collected some extra variables, which have been grouped together in Miscellaneous. The following variables have been registered for the three factors:</p>
<p>Method: How well is the research method documented?
Problem (<em>): The problem the research seeks to solve.
Objective/Goal (</em>): The objective of the research.
Research method (<em>): The research method used.
Research questions (</em>): The research question asked.
Pseudo code: Method described using pseudo code.
Data: How well is the data set documented?
Training data: Is the training set shared?
Validation data: Is the validation set shared?
Test data: Is the test set shared?
Results: Are the results shared?
Experiment: How well is the implementation and the experiment documented?
Hypothesis (<em>): The hypothesis being investigated.
Prediction (</em>): Predictions related to the hypotheses.
Method source code: Is the method open sourced?
Hardware specifications: Hardware used.
Software dependencies: For method or experiment.
Experiment setup: Is the setup including hyperparameters described?
Experiment source code: Is the experiment code open sourced?
Miscellaneous: Different variables that describe the research.
Research type: Experimental (E) or theoretical (T).
Research outcome: Is the paper reporting a positive or a negative result (positive $=1$ and negative $=0$ ).
Affiliation: The affiliation of the authors. Academia (0), collaboration (1) or industry (2).
Contribution (*): Contribution of the research.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 3: Percentage of papers documenting each variable for the three factors: a) Method, b) Data and c) Experiment.</p>
<p>All variables were registered as true (1) or false (0) unless otherwise specified. When surveying the papers, we have looked for explicit mentions of the variables marked with an asterix (*) above. For example, when reviewing the variable Problem, we have looked for an explicit mention of the problem being solved, such as "To address this problem, we propose a novel navigation system ..." (De Weerdt et al. 2013). The decision to use explicit mentions of the terms, such as contribution, goal, hypothesis and so on, can be disputed. However, the reasons for looking for explicit mentions are both practical and idealistic. Practically, it is easier to review a substantial amount of papers if the criteria are clear and objective. If we did not follow this guideline, the registering of variables would lend itself to subjective assessment rather than objective, and the results could be disputed based on how we measured the variables. Our goal was to get results with a low margin of error, so that we could draw statistically valid conclusions. In order to survey enough papers, we had to reduce the time we used on each paper. Explicit mentions supported this. Idealistically, our attitude is that research documentation should be clear and concise. Explicit mentions of which problem is being solved, what the goal of doing the research is, which hypothesis is being tested and so on are required to remove ambiguity from the text. Less ambiguous documentation increases the reproducibility of the research results.</p>
<h2>Quantifying Reproducibility</h2>
<p>We have defined a set of six metrics to quantify whether an experiment $e$ is R1, R2 or R3 reproducible and to which degree. The metrics measure how well the three factors method, data and experiment are documented. The three metrics $R 1(e), R 2(e)$ and $R 3(e)$ are boolean metrics that can be either true or false:</p>
<p>$$
\begin{gathered}
R 1(e)=\operatorname{Method}(e) \wedge \operatorname{Data}(e) \wedge \operatorname{Exp}(e) \
R 2(e)=\operatorname{Method}(e) \wedge \operatorname{Data}(e) \
R 3(e)=\operatorname{Method}(e)
\end{gathered}
$$</p>
<p>where $\operatorname{Method}(e), \operatorname{Data}(e)$ and $\operatorname{Exp}(e)$ is the conjunction of the truth values of the variables listed under the three factors Method, Data and Experiment in the section Factors and</p>
<p>Variables. This means that for Data(e) to be true for an experiment $e$, the training data set, the validation data set, the test data set and the results must be shared for $e$. Hence, $R 1(e)$ is the most strict requirement while $R 3$ is the most relaxed requirement when it comes to the documentation of an experiment $e$, as $R 3(e)$ requires only variables of the factor Method to be true while $R 1(e)$ requires all variables for all the three factors to be true.</p>
<p>The three metrics $R 1(e), R 2(e)$ and $R 3(e)$ are boolean metrics, so they will provide information on whether an experiment is R1, R2 or R3 reproducible in a strict sense. They will however not provide any information on to which degree experiments are reproducible, unless an experiment meets all the requirements. Therefore we suggest the three metrics $R 1 D(e), R 2 D(e)$ and $R 3 D(e)$ for measuring to which degree the the results of an experiment $e$ is:</p>
<p>$$
\begin{gathered}
R 1 D(e)=\frac{\delta_{1} \operatorname{Method}(e)+\delta_{2} \operatorname{Data}(e)+\delta_{3} \operatorname{Exp}(e)}{\delta_{1}+\delta_{2}+\delta_{3}} \
R 2 D(e)=\frac{\delta_{1} \operatorname{Method}(e)+\delta_{2} \operatorname{Data}(e)}{\delta_{1}+\delta_{2}} \
R 3 D(e)=\operatorname{Method}(e)
\end{gathered}
$$</p>
<p>where $\operatorname{Method}(e), \operatorname{Data}(e)$ and $\operatorname{Exp}(e)$ is the weighted sum of the truth values of the variables listed under the three factors Method, Data and Experiment. The weights of the factors are $\delta_{1}, \delta_{2}$ and $\delta_{3}$ respectively. This means that the value for Data(e) for experiment $e$ is the summation of the truth values for whether the training, validation, and test data sets as well as the results are shared for $e$. It is of course also possible to give different weights to each variable of a factor. We use a uniform weight for all variables and factors for our survey, $\delta_{i}=1$. For an experiment $e_{1}$ that has published the training data and test data, but not the validation set and the results $\operatorname{Data}(e)=0.5$. Note that some papers have no value for the training and validation sets if the experiment does not require either. For these papers, the $\delta_{i}$ weight is set to 0 .</p>
<h2>Results and Discussion</h2>
<p>Figure 3 shows percentage of research papers that have documented the different variables for the three factors. None of the three factors are documented very well according to the</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 4: a) Change in the RXD metrics. b), c) and d) show the amount of variables registered for the three factors for all papers.</p>
<p>Survey. As can be seen by analyzing the factor Method, an explicit description of the motivation behind research is not common. Figure 4(b) shows this as well. None of the papers document all five variables, and most of them (90%) document two or less. This might be because it is assumed that researchers in the domain are acquainted with the motivations and problems. Figure 3(b) shows that few papers provide the results of the experiment, although, compared to the other two factors, an encouraging 49% of the research papers share data as seen from Figure 4(c). The experiments are not documented well either as can be seen in figures 3(c) and 4(d). The variable Experiment setup is given a high score, which indicates that the experiment setup is documented to some degree. As we have not actually tried to reproduce the results, we have not ensured that the experiment setup is documented in enough detail to run the experiment.</p>
<p>The amount of empirical papers are shown in Table 1. For each conference, between 15% and 29% of the randomly selected samples are not empirical. In total, 325 papers empirical and considered in the analysis. Table 2 presents the results of the RXD (R1D, R2D and R3D) metrics. All the RXD metrics vary between 0.20 and 0.30. This means that only between a fifth and a third of the variables required for reproducibility are documented. For all papers, R1D has the lowest score with 0.24, R2D has a score of 0.25 and R3D has a score of 0.26. The general trend is that R1D is lower than the R2D scores, which again are lower than the R3D scores. This is not surprising, as R1D has fewer variables than R2D, which has fewer variables than R3D. However, given the error there is little variation among the three reproducibility degrees.</p>
<p>The RX (R1, R2 and R3) scores were 0.00 for all papers. No paper had full score on all variables for the factor Method, and it is required for all the three RX metrics. The three RX metrics are very strict and are not very informative for a survey such as this. They might have a use though, as guidelines for reviewers of conferences and journal publications. The three RXD metrics do not have the same issue as the RX metrics, as they measure the degree of reproducibility between 0 and 1.</p>
<p>There is a clear increase in the RXD scores from IJCAI 2013 to IJCAI 2016, see figure 4a). However, the trend is not as clear for AAAI as the R2D and R3D scores decrease. Table 3 shows the combined scores for the earlier years (2013).</p>
<p>Table 2: The 95% confidence interval for the mean R1D, R2D and R3D scores where ε = 1.96σg and σg = $$\frac{g}{\sqrt{N}}$$.</p>
<table>
<thead>
<tr>
<th>Conference</th>
<th>R1D ± ε</th>
<th>R2D ± ε</th>
<th>R3D ± ε</th>
</tr>
</thead>
<tbody>
<tr>
<td>IJCAI 2013</td>
<td>0.20 ± 0.02</td>
<td>0.20 ± 0.03</td>
<td>0.24 ± 0.04</td>
</tr>
<tr>
<td>AAAI 2014</td>
<td>0.21 ± 0.02</td>
<td>0.26 ± 0.03</td>
<td>0.28 ± 0.04</td>
</tr>
<tr>
<td>IJCAI 2016</td>
<td>0.30 ± 0.03</td>
<td>0.30 ± 0.04</td>
<td>0.29 ± 0.04</td>
</tr>
<tr>
<td>AAAI 2016</td>
<td>0.23 ± 0.02</td>
<td>0.25 ± 0.04</td>
<td>0.24 ± 0.04</td>
</tr>
<tr>
<td>Total</td>
<td>0.24 ± 0.01</td>
<td>0.25 ± 0.02</td>
<td>0.26 ± 0.02</td>
</tr>
</tbody>
</table>
<p>Table 3: The 95% confidence interval for the mean R1D, R2D and R3D scores when combining the papers from all four installments of IJCAI and AAAI into two groups according to the years they were published. One group contains all papers from 2013 and 2014 and the other group contains all the papers from 2016.</p>
<table>
<thead>
<tr>
<th>Years</th>
<th>R1D ± ε</th>
<th>R2D ± ε</th>
<th>R3D ± ε</th>
</tr>
</thead>
<tbody>
<tr>
<td>2013/2014</td>
<td>0.21 ± 0.02</td>
<td>0.23 ± 0.02</td>
<td>0.26 ± 0.03</td>
</tr>
<tr>
<td>2016</td>
<td>0.27 ± 0.02</td>
<td>0.27 ± 0.03</td>
<td>0.26 ± 0.03</td>
</tr>
</tbody>
</table>
<p>and 2014, 156 papers) and the combined scores for 2016 (169 papers). The results show that there is a slight, but statistically significant increase for R1D. The increase for R2D is not statistically significant, and there is no change for R3D. This means that only the experiment documentation has improved with time, and that there is no such evidence for the documentation of methods and data.</p>
<h2>Conclusion</h2>
<p>The survey confirms our prediction that the current documentation practices at top AI conferences render most of the reported research results irreproducible, as the R1, R2 and R3 reproducibility metrics show that no papers are fully reproducible. Only 24% of the variables required for R1D reproducibility, 25% of the variables required for R2D reproducibility and 26% of the variables required for R3D reproducibility are documented. When investigating whether there is change over time, we see improvement, which then confirms our second hypothesis. No improvement is indicated by the R1, R2, R3, R2D and R3D metrics. There is however a statistically significant improvement in the R1D metric. Hence, overall there is an improvement.</p>
<h2>Acknowledgments</h2>
<p>This work has been carried out at the Telenor-NTNU AI Lab, Norwegian University of Science and Technology, Trondheim, Norway.</p>
<h2>References</h2>
<p>Aarts, A. A.; Anderson, C. J.; Anderson, J.; van Assen, M. A. L. M.; Attridge, P. R.; Attwood, A. S.; Axt, J.; Babel, M.; Bahník, u.; Baranski, E.; and et al. 2016. Reproducibility project: Psychology.
Begley, C. G., and Ellis, L. M. 2012. Drug development: Raise standards for preclinical cancer research. Nature 483(7391):531-533.
Begley, C. G., and Ioannidis, J. P. A. 2014. Reproducibility in science: Improving the standard for basic and preclinical research. Circulation Research 116(1):116-126.
Benjamin, D. J.; Berger, J. O.; Johannesson, M.; Nosek, B. A.; Wagenmakers, E.-J.; Berk, R.; Bollen, K. A.; Brembs, B.; Brown, L.; Camerer, C.; et al. 2017. Redefine statistical significance. Nature Human Behaviour.
Braun, M. L., and Ong, C. S. 2014. Open science in machine learning. Implementing Reproducible Research 343.
Buckheit, J. B., and Donoho, D. L. 1995. Wavelab and reproducible research. Technical report, Standford, CA.
Claerbout, J. F., and Karrenbach, M. 1992. Electronic documents give reproducible research a new meaning. In Proceedings of the 62nd Annual International Meeting of the Society of Exploration Geophysics. 25 to 29 October 1992.
Cohen, P. R. 1995. Empirical methods for artificial intelligence, volume 139. MIT press Cambridge, MA.
Collberg, C., and Proebsting, T. A. 2016. Repeatability in computer systems research. Communications of the ACM 59(3):62-69.
De Weerdt, M. M.; Gerding, E. H.; Stein, S.; Robu, V.; and Jennings, N. R. 2013. Intention-aware routing to minimise delays at electric vehicle charging stations. In Proceedings of the Twenty-Third international joint conference on Artificial Intelligence, 83-89. AAAI Press.
Donoho, D. L.; Maleki, A.; Rahman, I. U.; Shahram, M.; and Stodden, V. 2009. Reproducible research in computational harmonic analysis. Computing in Science \&amp; Engineering 11(1).
Drummond, C. 2009. Replicability is not reproducibility: nor is it good science. International Conference on Machine Learning.
Fokkens, A.; Erp, M. V.; Postma, M.; Pedersen, T.; Vossen, P.; and Freire, N. 2013. Offspring from reproduction problems: What replication failure teaches us. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, 1691-1701. Association for Computational Linguistics (ACL).
Gent, I. P., and Kotthoff, L. 2014. Recomputation. org: Experiences of its first year and lessons learned. In Utility and Cloud Computing (UCC), 2014 IEEE/ACM 7th International Conference on, 968-973. IEEE.</p>
<p>Goodman, S. N.; Fanelli, D.; and Ioannidis, J. P. A. 2016. What does research reproducibility mean? Science Translational Medicine 8(341):341ps12-341ps12.
Hong, S.-Y.; Koo, M.-S.; Jang, J.; Kim, J.-E. E.; Park, H.; Joh, M.-S.; Kang, J.-H.; and Oh, T.-J. 2013. An evaluation of the software system dependency of a global atmospheric model. Monthly Weather Review 141(11):4165-4172.
Hunold, S., and Träff, J. L. 2013. On the state and importance of reproducible experimental research in parallel computing. CoRR, abs/1308.3648.
Hunold, S. 2015. A survey on reproducibility in parallel computing. CoRR, abs/1511.04217.
Johnson, V. E. 2013. Revised standards for statistical evidence. Proceedings of the National Academy of Sciences 110(48):19313-19317.
Lichman, M. 2013. UCI machine learning repository.
Oates, B. J. 2006. Researching Information Systems and Computing. SAGE Publications Ltd.
Peng, R. D. 2011. Reproducible research in computational science. Science 334(6060):1226-1227.
Prinz, F.; Schlange, T.; and Asadullah, K. 2011. Believe it or not: how much can we rely on published data on potential drug targets? Nature Reviews Drug Discovery 10(9):712712.</p>
<p>Russell, S., and Norvig, P. 2009. Artificial Intelligence: A modern approach. Prentice-Hall.
Sandve, G. K.; Nekrutenko, A.; Taylor, J.; and Hovig, E. 2013. Ten simple rules for reproducible computational research. PLoS Computational Biology 9(10):e1003285.
Stodden, V. C., and Miguez, S. 2014. Best practices for computational science: Software infrastructure and environments for reproducible and extensible research. Journal of Open Research Software 2(1):e21.
Stodden, V. C.; Guo, P.; and Ma, Z. 2013. Toward reproducible computational research: An empirical analysis of data and code policy adoption by journals. PLoS ONE 8(6):e67111.
Stodden, V. C. 2011. Trust your science? Open your data and code. Amstat News 21-22.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ https://github.com/aaai2018-paperid-62/aaai2018-paperid-62&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
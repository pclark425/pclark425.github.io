<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1650 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1650</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1650</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-32.html">extraction-schema-32</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <p><strong>Paper ID:</strong> paper-264145911</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2310.10509v1.pdf" target="_blank">Efficient Sim-to-real Transfer of Contact-Rich Manipulation Skills with Online Admittance Residual Learning</a></p>
                <p><strong>Paper Abstract:</strong> Learning contact-rich manipulation skills is essential. Such skills require the robots to interact with the environment with feasible manipulation trajectories and suitable compliance control parameters to enable safe and stable contact. However, learning these skills is challenging due to data inefficiency in the real world and the sim-to-real gap in simulation. In this paper, we introduce a hybrid offline-online framework to learn robust manipulation skills. We employ model-free reinforcement learning for the offline phase to obtain the robot motion and compliance control parameters in simulation \RV{with domain randomization}. Subsequently, in the online phase, we learn the residual of the compliance control parameters to maximize robot performance-related criteria with force sensor measurements in real time. To demonstrate the effectiveness and robustness of our approach, we provide comparative results against existing methods for assembly, pivoting, and screwing tasks.</p>
                <p><strong>Cost:</strong> 0.013</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1650.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1650.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>OARL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Online Admittance Residual Learning (proposed framework)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A two-phase sim-to-real framework that learns contact-rich manipulation trajectories and initial admittance parameters in MuJoCo, then adapts compliance (admittance) parameters online on the real robot by optimizing a residual to improve smoothness and task completion using recent force measurements.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_name</strong></td>
                            <td>FANUC LRMate 200iD (simulated model and real hardware)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_description</strong></td>
                            <td>6-DOF industrial manipulator used to execute contact-rich manipulation skills (peg-in-hole assembly, pivoting, screwing) with an end-effector force/torque (F/T) sensor; controlled via Cartesian-space admittance controller feeding a position/velocity low-level controller.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>general robotics manipulation (contact-rich manipulation / assembly)</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_name</strong></td>
                            <td>MuJoCo</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_description</strong></td>
                            <td>MuJoCo physics simulation including a model of the FANUC robot, joint torque control, a simulated F/T sensor, object geometries, frictional contacts and computed-torque low-level controller; simulation time step 0.01 s.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity_level</strong></td>
                            <td>Physics-based contact simulation (MuJoCo) with approximate contact dynamics (not photorealistic); medium-fidelity for kinematics, approximate for contact dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_modeled</strong></td>
                            <td>Rigid-body dynamics, robot joint torques, Cartesian admittance controller behavior, end-effector force/torque sensor measurements (with simulated noise), frictional contact (with specified friction coefficients), computed-torque low-level control.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_simplified</strong></td>
                            <td>Contact dynamics approximated and sensitive (single friction coefficients, simplified contact point/mode modeling), diagonal-only admittance parameterization (M, K, D diagonal), limited/approximate modeling of surface stiffness and complex contact modes, no full analytical adaptive contact model; some force clipping and simple sensor noise models.</td>
                        </tr>
                        <tr>
                            <td><strong>real_environment_description</strong></td>
                            <td>Physical setup with FANUC LRMate 200iD robot, real peg/hole assemblies (square, polygon, connector sockets), objects for pivoting against a rigid wall, F/T sensor on wrist; screwing task with M8 bolt/nut in real hardware.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_skill_transferred</strong></td>
                            <td>Contact-rich manipulation skills: peg-in-hole assembly, object pivoting against a wall, and bolt screwing (alignment + rotation primitive).</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>Model-free reinforcement learning (Soft Actor-Critic) in simulation with domain randomization of certain parameters; offline learning of trajectory and initial admittance parameters.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td>Success rate (successful task completion), completion time (s) for successful trials, and maximum contact force (N) for safety.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_sim</strong></td>
                            <td>Assembly: 100% success (evaluated in sim); Pivoting: 100% success (evaluated in sim).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_real</strong></td>
                            <td>Assembly (proposed method): 10/10 success, completion time 19.0 ± 11.2 s, max contact force 23.6 ± 6.3 N; Pivoting (proposed): 9/10 success, completion time 25.6 ± 2.1 s, max contact force 20.1 ± 4.1 N; Screwing: 5/5 trials success (100%).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td>Randomization of robot initial pose (assembly: uniform in ±30 mm X/Y, Z = 30 ±5 mm; pivoting: X = 150 ±30 mm, Z = 5 ±5 mm) and randomization/noise on contact/FT sensor readings (Gaussian mean 0, std 0.2 N) and clipping of measured forces to ±10 N; randomization intended to improve robustness to position and force variations.</td>
                        </tr>
                        <tr>
                            <td><strong>sim_to_real_gap_factors</strong></td>
                            <td>Large gap arises from contact dynamics sensitivity to surface stiffness and friction, imperfect contact modeling (mode changes, contact point uncertainty), admittance parameter mismatch, sensor noise and clipping, and simplified diagonal admittance assumption causing different dynamic responses on real hardware (bouncing / oscillations and large contact forces).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_enabling_conditions</strong></td>
                            <td>Separating trajectory (kinematics) and compliance: trajectories learned in sim transfer well; online residual adaptation of admittance parameters using real force measurements (record & replay) to optimize a combined trajectory-smoothness and task-completion objective; domain randomization during training to improve robustness; positive-definite constraints and critical-damping design for stability; initializing online optimization from simulation-learned parameters.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_requirements_identified</strong></td>
                            <td>Paper identifies accurate modeling of contact dynamics (surface stiffness, friction) as critical for transferring compliance parameters; kinematic properties (shape, size) have smaller sim-to-real gap and transfer more readily, while contact dynamics require online adaptation—no strict numerical fidelity thresholds provided.</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_in_real_world</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_details</strong></td>
                            <td>Online residual optimization of admittance parameters (δu) run in closed-loop during execution. Every T seconds the system records recent F/T measurements (record & replay) and solves constrained optimization to update diagonal admittance elements (M^{-1}, K', D'), ensuring positivity and optimizing C(x)=∫[w|e| + (1−w)|ė|] dt (w=0.4 used experimentally). Update frequency governed by chosen T and uses recent time-window force recordings; no large-scale retraining on real robot required.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_fidelity_levels</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Kinematic trajectories learned in simulation transfer robustly with appropriate compliance; contact dynamics are highly sensitive and cause sim-to-real failures if admittance parameters are transferred directly. Domain randomization helps, but online adaptation of admittance parameters (residual learning using recent force measurements and an objective balancing smoothness and task completion) enables efficient and safe sim-to-real transfer, achieving high success rates (100% assembly, 90%+ pivoting) and lower contact forces than direct transfer; manual tuning can also succeed but requires human effort and may not generalize.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Efficient Sim-to-real Transfer of Contact-Rich Manipulation Skills with Online Admittance Residual Learning', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1650.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1650.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Direct Transfer (baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Direct sim-to-real Transfer of learned policy and admittance parameters (baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Baseline that directly applies the policy learned in MuJoCo (trajectory + admittance parameters) to the real robot without any real-world adaptation; used to evaluate the magnitude of the sim-to-real gap for compliance parameters.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_name</strong></td>
                            <td>FANUC LRMate 200iD (simulated and real)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_description</strong></td>
                            <td>Same robot hardware as the proposed method; executes the learned trajectory and compliance parameters directly on real hardware.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>general robotics manipulation (contact-rich manipulation / assembly)</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_name</strong></td>
                            <td>MuJoCo</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_description</strong></td>
                            <td>Same MuJoCo simulation environment used to learn policy and admittance parameters.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity_level</strong></td>
                            <td>Physics-based contact simulation (approximate contact dynamics) as used during RL training.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_modeled</strong></td>
                            <td>Rigid-body dynamics, basic frictional contact, simulated F/T sensor noise.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_simplified</strong></td>
                            <td>Same simplifications as in training: simplified contact dynamics, diagonal admittance, fixed friction coefficients; no online force adaptation.</td>
                        </tr>
                        <tr>
                            <td><strong>real_environment_description</strong></td>
                            <td>Physical FANUC robot and test objects used for assembly and pivoting; evaluated without any parameter adaptation.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_skill_transferred</strong></td>
                            <td>Peg-in-hole assembly and pivoting skills (trajectory + admittance params) transferred verbatim from sim to real.</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>Model-free RL (SAC) with domain randomization used during offline training (same as proposed method).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td>Success rate, completion time (s), maximum contact force (N).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_sim</strong></td>
                            <td>Assembly: 100% success in sim; Pivoting: 100% success in sim.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_real</strong></td>
                            <td>Assembly: 3/10 successful trials, completion time 39.0 ± 12.8 s, max contact force 63.7 ± 6.8 N; Pivoting: 0/10 successful trials (failures), other metrics not applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td>Trained with same domain randomization as main method (initial pose randomization, FT sensor noise); no real-world adaptation applied at deployment.</td>
                        </tr>
                        <tr>
                            <td><strong>sim_to_real_gap_factors</strong></td>
                            <td>Mismatch in admittance/control parameters related to contact dynamics (stiffness/friction differences) caused robot to bounce and produce large contact forces and oscillations in real world, preventing stable contact and task completion.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_enabling_conditions</strong></td>
                            <td>None applied at deployment beyond domain-randomized training; the experiments show direct transfer of both trajectory and compliance parameters without adaptation is insufficient for stable contact-rich tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_requirements_identified</strong></td>
                            <td>Shows that simulated admittance parameters that work in MuJoCo may not be safe on real hardware due to imperfect contact modeling; implies higher fidelity or adaptation needed for compliance parameters.</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_in_real_world</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_fidelity_levels</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Directly transferring both motion and compliance parameters from MuJoCo often fails: high contact forces and oscillations occur on real hardware leading to low success rates (3/10 assembly, 0/10 pivoting), demonstrating that contact dynamics and compliance parameters are a primary source of sim-to-real failure.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Efficient Sim-to-real Transfer of Contact-Rich Manipulation Skills with Online Admittance Residual Learning', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1650.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1650.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Manual Tune (baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Manual tuning of admittance control parameters with transferred trajectory (baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Baseline where the trajectory learned in simulation is combined with human-expert tuned admittance parameters on the real robot; used to show that trajectories transfer but compliance must be tuned for safety and success.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_name</strong></td>
                            <td>FANUC LRMate 200iD (real hardware)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_description</strong></td>
                            <td>Same physical manipulator executing the sim-learned trajectory with manually adjusted admittance M, K, D gains per task.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>general robotics manipulation (contact-rich manipulation / assembly)</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_name</strong></td>
                            <td>MuJoCo (used for trajectory learning)</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_description</strong></td>
                            <td>MuJoCo simulation used to learn trajectories; manual tuning performed in real environment.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity_level</strong></td>
                            <td>Same as other methods for trajectory learning; admittance parameters not trusted from sim and tuned manually.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_modeled</strong></td>
                            <td>Kinematic properties and nominal contact modeled in sim; admittance params replaced by human-tuned real values.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_simplified</strong></td>
                            <td>Human tuning bypasses simulation inaccuracies by choosing safe real-world gains; does not require additional modeling of contact modes beyond empirical tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>real_environment_description</strong></td>
                            <td>Physical robot performing assembly and pivoting with manually selected admittance parameters (Table 4 lists tuned values).</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_skill_transferred</strong></td>
                            <td>Transfer of manipulated trajectories (peg-in-hole assembly, pivoting) from sim combined with manual admittance gains on real robot.</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>Trajectory learned via model-free RL (SAC) in MuJoCo; compliance parameters set by human experts on the real robot.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td>Success rate, completion time (s), maximum contact force (N).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_sim</strong></td>
                            <td>Assembly and pivoting had 100% success rates in sim (policy performance reported).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_real</strong></td>
                            <td>Assembly: 10/10 success, completion time 28.1 ± 8.6 s, max contact force 10.3 ± 2.2 N; Pivoting: 10/10 success, completion time 25.3 ± 3.6 s, max contact force 9.2 ± 0.6 N. (From Table 1.)</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td>Trajectories were learned with the same domain randomization as in proposed method; admittance parameters were manually tuned offline per task (values in Table 4).</td>
                        </tr>
                        <tr>
                            <td><strong>sim_to_real_gap_factors</strong></td>
                            <td>Human tuning compensates for the sim-to-real gap arising from inaccurate contact dynamics and compliance mismatch; however manual tuning is task- and object-specific and may not generalize.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_enabling_conditions</strong></td>
                            <td>Expert manual tuning of admittance M, K, D (example tuned values provided), ensuring stable contact behavior and preventing bouncing/oscillation.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_requirements_identified</strong></td>
                            <td>Demonstrates that while kinematic trajectories transfer reliably, compliance parameters depend strongly on real contact dynamics and often require either manual adjustment or online adaptation.</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_in_real_world</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_details</strong></td>
                            <td>Human-expert manual tuning of admittance parameters (see Table 4: example tuned masses, stiffness, damping per task) before executing tasks; tuning process is task-specific and potentially time-consuming and risky due to contact forces.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_fidelity_levels</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Manual tuning achieves high success rates and low contact forces, confirming that the learned motions are transferable but appropriate compliance parameters are necessary; however manual tuning is laborious, task-dependent, and may not generalize across objects or tighter-fit connectors.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Efficient Sim-to-real Transfer of Contact-Rich Manipulation Skills with Online Admittance Residual Learning', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Industreal: Transferring contact-rich assembly tasks from simulation to reality <em>(Rating: 2)</em></li>
                <li>Closing the sim-to-real loop: Adapting simulation randomization with real world experience <em>(Rating: 2)</em></li>
                <li>Real2sim2real: Self-supervised learning of physical single-step dynamic actions for planar robot casting <em>(Rating: 2)</em></li>
                <li>Fundamental challenges in deep learning for stiff contact dynamics <em>(Rating: 1)</em></li>
                <li>Fast contact for robotic assembly <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1650",
    "paper_id": "paper-264145911",
    "extraction_schema_id": "extraction-schema-32",
    "extracted_data": [
        {
            "name_short": "OARL",
            "name_full": "Online Admittance Residual Learning (proposed framework)",
            "brief_description": "A two-phase sim-to-real framework that learns contact-rich manipulation trajectories and initial admittance parameters in MuJoCo, then adapts compliance (admittance) parameters online on the real robot by optimizing a residual to improve smoothness and task completion using recent force measurements.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_system_name": "FANUC LRMate 200iD (simulated model and real hardware)",
            "agent_system_description": "6-DOF industrial manipulator used to execute contact-rich manipulation skills (peg-in-hole assembly, pivoting, screwing) with an end-effector force/torque (F/T) sensor; controlled via Cartesian-space admittance controller feeding a position/velocity low-level controller.",
            "domain": "general robotics manipulation (contact-rich manipulation / assembly)",
            "virtual_environment_name": "MuJoCo",
            "virtual_environment_description": "MuJoCo physics simulation including a model of the FANUC robot, joint torque control, a simulated F/T sensor, object geometries, frictional contacts and computed-torque low-level controller; simulation time step 0.01 s.",
            "simulation_fidelity_level": "Physics-based contact simulation (MuJoCo) with approximate contact dynamics (not photorealistic); medium-fidelity for kinematics, approximate for contact dynamics.",
            "fidelity_aspects_modeled": "Rigid-body dynamics, robot joint torques, Cartesian admittance controller behavior, end-effector force/torque sensor measurements (with simulated noise), frictional contact (with specified friction coefficients), computed-torque low-level control.",
            "fidelity_aspects_simplified": "Contact dynamics approximated and sensitive (single friction coefficients, simplified contact point/mode modeling), diagonal-only admittance parameterization (M, K, D diagonal), limited/approximate modeling of surface stiffness and complex contact modes, no full analytical adaptive contact model; some force clipping and simple sensor noise models.",
            "real_environment_description": "Physical setup with FANUC LRMate 200iD robot, real peg/hole assemblies (square, polygon, connector sockets), objects for pivoting against a rigid wall, F/T sensor on wrist; screwing task with M8 bolt/nut in real hardware.",
            "task_or_skill_transferred": "Contact-rich manipulation skills: peg-in-hole assembly, object pivoting against a wall, and bolt screwing (alignment + rotation primitive).",
            "training_method": "Model-free reinforcement learning (Soft Actor-Critic) in simulation with domain randomization of certain parameters; offline learning of trajectory and initial admittance parameters.",
            "transfer_success_metric": "Success rate (successful task completion), completion time (s) for successful trials, and maximum contact force (N) for safety.",
            "transfer_performance_sim": "Assembly: 100% success (evaluated in sim); Pivoting: 100% success (evaluated in sim).",
            "transfer_performance_real": "Assembly (proposed method): 10/10 success, completion time 19.0 ± 11.2 s, max contact force 23.6 ± 6.3 N; Pivoting (proposed): 9/10 success, completion time 25.6 ± 2.1 s, max contact force 20.1 ± 4.1 N; Screwing: 5/5 trials success (100%).",
            "transfer_success": true,
            "domain_randomization_used": true,
            "domain_randomization_details": "Randomization of robot initial pose (assembly: uniform in ±30 mm X/Y, Z = 30 ±5 mm; pivoting: X = 150 ±30 mm, Z = 5 ±5 mm) and randomization/noise on contact/FT sensor readings (Gaussian mean 0, std 0.2 N) and clipping of measured forces to ±10 N; randomization intended to improve robustness to position and force variations.",
            "sim_to_real_gap_factors": "Large gap arises from contact dynamics sensitivity to surface stiffness and friction, imperfect contact modeling (mode changes, contact point uncertainty), admittance parameter mismatch, sensor noise and clipping, and simplified diagonal admittance assumption causing different dynamic responses on real hardware (bouncing / oscillations and large contact forces).",
            "transfer_enabling_conditions": "Separating trajectory (kinematics) and compliance: trajectories learned in sim transfer well; online residual adaptation of admittance parameters using real force measurements (record & replay) to optimize a combined trajectory-smoothness and task-completion objective; domain randomization during training to improve robustness; positive-definite constraints and critical-damping design for stability; initializing online optimization from simulation-learned parameters.",
            "fidelity_requirements_identified": "Paper identifies accurate modeling of contact dynamics (surface stiffness, friction) as critical for transferring compliance parameters; kinematic properties (shape, size) have smaller sim-to-real gap and transfer more readily, while contact dynamics require online adaptation—no strict numerical fidelity thresholds provided.",
            "fine_tuning_in_real_world": true,
            "fine_tuning_details": "Online residual optimization of admittance parameters (δu) run in closed-loop during execution. Every T seconds the system records recent F/T measurements (record & replay) and solves constrained optimization to update diagonal admittance elements (M^{-1}, K', D'), ensuring positivity and optimizing C(x)=∫[w|e| + (1−w)|ė|] dt (w=0.4 used experimentally). Update frequency governed by chosen T and uses recent time-window force recordings; no large-scale retraining on real robot required.",
            "comparison_across_fidelity_levels": false,
            "fidelity_comparison_results": null,
            "key_findings": "Kinematic trajectories learned in simulation transfer robustly with appropriate compliance; contact dynamics are highly sensitive and cause sim-to-real failures if admittance parameters are transferred directly. Domain randomization helps, but online adaptation of admittance parameters (residual learning using recent force measurements and an objective balancing smoothness and task completion) enables efficient and safe sim-to-real transfer, achieving high success rates (100% assembly, 90%+ pivoting) and lower contact forces than direct transfer; manual tuning can also succeed but requires human effort and may not generalize.",
            "uuid": "e1650.0",
            "source_info": {
                "paper_title": "Efficient Sim-to-real Transfer of Contact-Rich Manipulation Skills with Online Admittance Residual Learning",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Direct Transfer (baseline)",
            "name_full": "Direct sim-to-real Transfer of learned policy and admittance parameters (baseline)",
            "brief_description": "Baseline that directly applies the policy learned in MuJoCo (trajectory + admittance parameters) to the real robot without any real-world adaptation; used to evaluate the magnitude of the sim-to-real gap for compliance parameters.",
            "citation_title": "",
            "mention_or_use": "use",
            "agent_system_name": "FANUC LRMate 200iD (simulated and real)",
            "agent_system_description": "Same robot hardware as the proposed method; executes the learned trajectory and compliance parameters directly on real hardware.",
            "domain": "general robotics manipulation (contact-rich manipulation / assembly)",
            "virtual_environment_name": "MuJoCo",
            "virtual_environment_description": "Same MuJoCo simulation environment used to learn policy and admittance parameters.",
            "simulation_fidelity_level": "Physics-based contact simulation (approximate contact dynamics) as used during RL training.",
            "fidelity_aspects_modeled": "Rigid-body dynamics, basic frictional contact, simulated F/T sensor noise.",
            "fidelity_aspects_simplified": "Same simplifications as in training: simplified contact dynamics, diagonal admittance, fixed friction coefficients; no online force adaptation.",
            "real_environment_description": "Physical FANUC robot and test objects used for assembly and pivoting; evaluated without any parameter adaptation.",
            "task_or_skill_transferred": "Peg-in-hole assembly and pivoting skills (trajectory + admittance params) transferred verbatim from sim to real.",
            "training_method": "Model-free RL (SAC) with domain randomization used during offline training (same as proposed method).",
            "transfer_success_metric": "Success rate, completion time (s), maximum contact force (N).",
            "transfer_performance_sim": "Assembly: 100% success in sim; Pivoting: 100% success in sim.",
            "transfer_performance_real": "Assembly: 3/10 successful trials, completion time 39.0 ± 12.8 s, max contact force 63.7 ± 6.8 N; Pivoting: 0/10 successful trials (failures), other metrics not applicable.",
            "transfer_success": false,
            "domain_randomization_used": true,
            "domain_randomization_details": "Trained with same domain randomization as main method (initial pose randomization, FT sensor noise); no real-world adaptation applied at deployment.",
            "sim_to_real_gap_factors": "Mismatch in admittance/control parameters related to contact dynamics (stiffness/friction differences) caused robot to bounce and produce large contact forces and oscillations in real world, preventing stable contact and task completion.",
            "transfer_enabling_conditions": "None applied at deployment beyond domain-randomized training; the experiments show direct transfer of both trajectory and compliance parameters without adaptation is insufficient for stable contact-rich tasks.",
            "fidelity_requirements_identified": "Shows that simulated admittance parameters that work in MuJoCo may not be safe on real hardware due to imperfect contact modeling; implies higher fidelity or adaptation needed for compliance parameters.",
            "fine_tuning_in_real_world": false,
            "fine_tuning_details": null,
            "comparison_across_fidelity_levels": false,
            "fidelity_comparison_results": null,
            "key_findings": "Directly transferring both motion and compliance parameters from MuJoCo often fails: high contact forces and oscillations occur on real hardware leading to low success rates (3/10 assembly, 0/10 pivoting), demonstrating that contact dynamics and compliance parameters are a primary source of sim-to-real failure.",
            "uuid": "e1650.1",
            "source_info": {
                "paper_title": "Efficient Sim-to-real Transfer of Contact-Rich Manipulation Skills with Online Admittance Residual Learning",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Manual Tune (baseline)",
            "name_full": "Manual tuning of admittance control parameters with transferred trajectory (baseline)",
            "brief_description": "Baseline where the trajectory learned in simulation is combined with human-expert tuned admittance parameters on the real robot; used to show that trajectories transfer but compliance must be tuned for safety and success.",
            "citation_title": "",
            "mention_or_use": "use",
            "agent_system_name": "FANUC LRMate 200iD (real hardware)",
            "agent_system_description": "Same physical manipulator executing the sim-learned trajectory with manually adjusted admittance M, K, D gains per task.",
            "domain": "general robotics manipulation (contact-rich manipulation / assembly)",
            "virtual_environment_name": "MuJoCo (used for trajectory learning)",
            "virtual_environment_description": "MuJoCo simulation used to learn trajectories; manual tuning performed in real environment.",
            "simulation_fidelity_level": "Same as other methods for trajectory learning; admittance parameters not trusted from sim and tuned manually.",
            "fidelity_aspects_modeled": "Kinematic properties and nominal contact modeled in sim; admittance params replaced by human-tuned real values.",
            "fidelity_aspects_simplified": "Human tuning bypasses simulation inaccuracies by choosing safe real-world gains; does not require additional modeling of contact modes beyond empirical tuning.",
            "real_environment_description": "Physical robot performing assembly and pivoting with manually selected admittance parameters (Table 4 lists tuned values).",
            "task_or_skill_transferred": "Transfer of manipulated trajectories (peg-in-hole assembly, pivoting) from sim combined with manual admittance gains on real robot.",
            "training_method": "Trajectory learned via model-free RL (SAC) in MuJoCo; compliance parameters set by human experts on the real robot.",
            "transfer_success_metric": "Success rate, completion time (s), maximum contact force (N).",
            "transfer_performance_sim": "Assembly and pivoting had 100% success rates in sim (policy performance reported).",
            "transfer_performance_real": "Assembly: 10/10 success, completion time 28.1 ± 8.6 s, max contact force 10.3 ± 2.2 N; Pivoting: 10/10 success, completion time 25.3 ± 3.6 s, max contact force 9.2 ± 0.6 N. (From Table 1.)",
            "transfer_success": true,
            "domain_randomization_used": true,
            "domain_randomization_details": "Trajectories were learned with the same domain randomization as in proposed method; admittance parameters were manually tuned offline per task (values in Table 4).",
            "sim_to_real_gap_factors": "Human tuning compensates for the sim-to-real gap arising from inaccurate contact dynamics and compliance mismatch; however manual tuning is task- and object-specific and may not generalize.",
            "transfer_enabling_conditions": "Expert manual tuning of admittance M, K, D (example tuned values provided), ensuring stable contact behavior and preventing bouncing/oscillation.",
            "fidelity_requirements_identified": "Demonstrates that while kinematic trajectories transfer reliably, compliance parameters depend strongly on real contact dynamics and often require either manual adjustment or online adaptation.",
            "fine_tuning_in_real_world": true,
            "fine_tuning_details": "Human-expert manual tuning of admittance parameters (see Table 4: example tuned masses, stiffness, damping per task) before executing tasks; tuning process is task-specific and potentially time-consuming and risky due to contact forces.",
            "comparison_across_fidelity_levels": false,
            "fidelity_comparison_results": null,
            "key_findings": "Manual tuning achieves high success rates and low contact forces, confirming that the learned motions are transferable but appropriate compliance parameters are necessary; however manual tuning is laborious, task-dependent, and may not generalize across objects or tighter-fit connectors.",
            "uuid": "e1650.2",
            "source_info": {
                "paper_title": "Efficient Sim-to-real Transfer of Contact-Rich Manipulation Skills with Online Admittance Residual Learning",
                "publication_date_yy_mm": "2023-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Industreal: Transferring contact-rich assembly tasks from simulation to reality",
            "rating": 2,
            "sanitized_title": "industreal_transferring_contactrich_assembly_tasks_from_simulation_to_reality"
        },
        {
            "paper_title": "Closing the sim-to-real loop: Adapting simulation randomization with real world experience",
            "rating": 2,
            "sanitized_title": "closing_the_simtoreal_loop_adapting_simulation_randomization_with_real_world_experience"
        },
        {
            "paper_title": "Real2sim2real: Self-supervised learning of physical single-step dynamic actions for planar robot casting",
            "rating": 2,
            "sanitized_title": "real2sim2real_selfsupervised_learning_of_physical_singlestep_dynamic_actions_for_planar_robot_casting"
        },
        {
            "paper_title": "Fundamental challenges in deep learning for stiff contact dynamics",
            "rating": 1,
            "sanitized_title": "fundamental_challenges_in_deep_learning_for_stiff_contact_dynamics"
        },
        {
            "paper_title": "Fast contact for robotic assembly",
            "rating": 1,
            "sanitized_title": "fast_contact_for_robotic_assembly"
        }
    ],
    "cost": 0.01325075,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Efficient Sim-to-real Transfer of Contact-Rich Manipulation Skills with Online Admittance Residual Learning
16 Oct 2023</p>
<p>Xiang Zhang xiangzhang98@berkeley.edu 
Equal Contribution</p>
<p>Changhao Wang changhaowang@berkeley.edu 
Equal Contribution</p>
<p>Lingfeng Sun 
Zheng Wu 
Xinghao Zhu 
Masayoshi Tomizuka </p>
<p>Department of Mechanical Engineering
University of California at Berkeley
United States</p>
<p>Efficient Sim-to-real Transfer of Contact-Rich Manipulation Skills with Online Admittance Residual Learning
16 Oct 2023E7DF85A312FA98CD41734497AA66C9D3arXiv:2310.10509v1[cs.RO]Contact-rich ManipulationAdmittance ControlSim-to-real Transfer
Figure 1: As shown in (a), we propose a robust contact-rich manipulation skill learning framework that offline learns the robot motion and compliance control parameters in the simulation and online adapts to the real world.The structure of the admittance controller is depicted in (b).Our framework demonstrates robustness in sim-to-real transfer and generalizability to diverse real-world tasks in (c)</p>
<p>Introduction</p>
<p>Contact-rich manipulation is common in a wide range of robotic applications, including assembly [1,2,3,4,5,6,7,8], object pivoting [9,10,11], grasping [12,13,14], and pushing [15,16].To accomplish these tasks, robots need to learn both the manipulation trajectory and the force control parameters.The manipulation trajectory guides the robot toward completing the task while physically engaging with the environment, whereas the force control parameters regulate the contact force.Incorrect control parameters can lead to oscillations and excessive contact forces that may damage the robot or the environment.</p>
<p>Past works have tackled the contact-rich skill-learning problem in different ways.First, the majority of previous works [2,10,6,7,3,17,9,18,19] focus on learning the manipulation trajectories and rely on human experts to manually tune force control parameters.While this simplification has demonstrated remarkable performance in many applications, letting human labor tune control parameters is still inconvenient.Furthermore, the tuned parameter for one task may not generalize well to other task settings with different kinematic or dynamic properties.For example, assembly tasks with different clearances will require different control parameters.Another line of work deals with this problem by jointly learning the robot's motion and force control parameters [20,21,22,23,24,25,4,26,8].Such learning processes can be conducted in both real-world and simulation.However, learning such skills on real robots is time-consuming and may damage the robot or environment.Learning in simulation is efficient and safe, however, the learned control parameters may be difficult to transfer to real robots due to the sim-to-real gap, and directly deploying the learned control parameters may cause damage to the robot.</p>
<p>In this paper, we focus on transferring robotic manipulation skills.We notice that the manipulation trajectory is more related to the kinematic properties, such as size and shape, which have a smaller sim-to-real gap and can be transferred directly, as demonstrated by previous works [9,2,10,3].However, simulating the contact dynamics proves to be challenging, primarily due to its sensitivity to various parameters, including surface stiffness and friction [27].This sensitivity will result in a large sim-to-real gap and affects the learned compliance control parameters.Inspired by the above analysis, we propose a framework to learn robot manipulation skills that can transfer to the real world.As depicted in Fig. 1(a), the framework contains two phases: skill learning in simulation and admittance adaptation on the real robot.We use model-free RL [28,29] to learn the robot's motion with domain randomization to enhance the robustness for direct transfer.The compliance control parameters are learned at the same time and serve as an initialization to online admittance learning.During online execution, we iteratively learn the residual of the admittance control parameters by optimizing the future robot trajectory smoothness and task completion criterion.We conduct real-world experiments on three typical contact-rich manipulation tasks: assembly, pivoting, and screwing.Our proposed framework achieves efficient transfer from simulation to the real world.Furthermore, it shows excellent generalization ability in tasks with different kinematic or dynamic properties, as shown in Fig. 1(c).Comparison and ablation studies are provided to demonstrate the effectiveness of the framework.</p>
<p>Related Works</p>
<p>Sim-to-real Transfer in Robot Contact-Rich Manipulation</p>
<p>Contact-rich manipulation tasks involve the interaction between robots and the environment through physical contact.In recent years, there has been a growing trend in utilizing simulation environments such as MuJoCo [30], Bullet [31], and IsaacGym [17] to learn and train robots for these tasks.These simulation environments offer advantages in terms of safety, scalability, and cost-effectiveness.Nevertheless, the sim-to-real gap remains a significant challenge.To address the gap, various approaches have been explored, including system identification, transfer learning, domain randomization, and online adaptation.System identification approaches [32,33] involves the calibration of simulation parameters to improve accuracy and align the simulation with real-world dynamics.Transfer learn-ing methods [34,35,25] aim to fine-tune skills learned in simulation for application in real-world scenarios.Domain randomization techniques [10,9,25,36] are employed to create diverse environments with varying properties, enabling the learning of robust skills for better generalization.Instead of collecting large datasets in the real world, online adaptation methods [37,38,39,40,41,42] utilize real sensor measurements to optimize a residual policy/model or directly update the policy network in real-time.Tang et al. [43] further improves the sim-to-real transfer performance by combining the above techniques with a modified objective function design for insertion tasks.</p>
<p>Learning Variable Impedance/Admittance Control</p>
<p>Compliance control [44], such as impedance and admittance control, enables robots to behave as a mass-spring-damping system.Tuning the compliance control parameters is crucial for stabilizing the robot and accomplishing manipulation tasks.However, manual tuning can be time-consuming.To address this issue, learning-based approaches have been applied to automatically learn the control parameters.Previous methods have focused on learning compliance control parameters either from expert demonstrations [20,21,22] or through reinforcement learning (RL) [23,24,25,4,26,45] to acquire gain-changing policies.[21,4,20,23,25] propose to directly collect data in the real world.However, it is time-consuming to collect the data.Authors in [22,26] have demonstrated success in directly transferring the learned control parameters from the simulation to the real world.Nevertheless, their applications are limited to simple tasks, such as waypoint tracking and whiteboard wiping.</p>
<p>Proposed Approach</p>
<p>We focus on learning robust contact-rich manipulation skills that can achieve efficient sim-to-real transfer.We define the skill as π(x d , P |s), which generates both the robot desired trajectory x d and the compliance control parameters P given the current state s.</p>
<p>We use Cartesian space admittance control as the compliance controller.As shown in Fig. 1(b), the admittance control takes in the desired trajectory [x d , ẋd , ẍd ] ∈ R 18 , and the external force/torque F ext ∈ R 6 measured on the robot end-effector and outputs the compliance trajectory [x c , ẋc , ẍc ] ∈ R 18 to the position/velocity controller according to the mass-spring-damping dynamics [44]:
M (ẍ c − ẍd ) + D( ẋc − ẋd ) + K(x c − x d ) = F ext(1)
where M, K, D are the robot inertia, stiffness, and damping matrices, respectively.We assume M, K, D is diagonal for simplicity and P = {M, K, D} as the collection of all control parameters.</p>
<p>To achieve this goal, We propose an offline-online framework for learning contact-rich manipulation skills as depicted in Fig. 1(a).In the offline phase, we employ the model-free RL with domain randomization to learn the robot motion and the initial guess of compliance control parameters from the simulation (Section 3.1).In the online phase, we execute the offline-learned motions on the real robot and learn the residual compliance control parameters by optimizing the future robot trajectory smoothness and task completion criteria(Section 3.2).</p>
<p>Learning offline contact-rich manipulation skills</p>
<p>We utilize model-free RL to learn contact-rich manipulation skills in MuJoCo simulation [30].The problem is modeled as a Markov decision process {S, A, R, P, γ} where S is the state space, A is the action space, R is a reward function, P denotes the state-transition probability, and γ is the discount factor.For each timestep t, the agent is at the state s t ∈ S, executes an action a t ∈ A, and receives a scalar reward r t .The next state is computed by the transition probability p(s t+1 |s t , a t ).</p>
<p>Our goal is to learn a policy π(a|s) that maximizes the expected future return E [ t γ t r t ].</p>
<p>Specifically, we focus on learning robot skills for three contact-rich tasks: assembly, pivoting, and screwing.In these tasks, the robot needs to utilize the contact to either align the peg and hole or continuously push and pivot the object, which makes them suitable testbeds for our proposed framework.The detailed task setups can be found below:</p>
<p>Assembly Task: The goal is to align the peg with the hole and then insert it.</p>
<p>State space: The state space s ∈ R 18 contains peg pose s p ∈ R 6 (position and Euler angles) relative to the hole, peg velocity v p ∈ R 6 , and the external force measured on the robot wrist F ext ∈ R 6 .</p>
<p>Action space: The action a ∈ R 12 consists of the end-effector velocity command v d ∈ R 6 and the diagonal elements of the stiffness matrix k ∈ R 6 .To simplify the training, the robot inertia M is fixed to diag(1, 1, 1, 0.1, 0.1, 0.1), and the damping matrix
D = diag(d 1 , • • • , d 6 ) is computed according to the critical damping condition d i = 2 √ m i k i , i = {1, 2, 3, 4, 5, 6}.</p>
<p>Reward function:</p>
<p>The reward function is defined as r(s) = 10 (1−∥spos−s d pos ∥</p>
<p>2 ) , where s pos ∈ R 3 is the peg position and s d pos ∈ R 3 is the nominal hole location.The exponential function encourages successful insertion by providing a high reward.</p>
<p>Pivoting Task: The goal is to gradually push the object to a stand-up pose against the wall.</p>
<p>State space:</p>
<p>The state s ∈ R 12 consists of the robot pose s p ∈ R 6 and the external force
F ext ∈ R 6 .
Action space: For simplicity, we consider a 2d pivoting problem: the robot can only move in the X, Z direction.The robot action a ∈ R 4 contains the velocity command in X, Z direction and the corresponding stiffness parameter.</p>
<p>Reward function:</p>
<p>We use the rotational distance between the goal orientation R goal and current object orientation R as the cost and define the reward function as r = π 2 − d, with d = arccos 0.5(Tr(R goal R T ) − 1) , which computes the distance of two rotation matrices between R and R goal .The constant term π 2 simply shifts the initial reward to 0. This reward encourages the robot to push the object to the stand-up orientation.</p>
<p>We use domain randomization on the robot's initial pose and contact force to improve the robustness of the learned skills.The implementation details can be found in Appendix.B.1.</p>
<p>Online Optimization-Based Admittance Learning</p>
<p>We have learned a policy that can perform contact-rich manipulation tasks in simulation.However, the sim-to-real gap may prevent us from directly transferring the learned skills to the real world.Our goal is to adapt the offline learned skills, especially the admittance control parameters, with online data in real time.Instead of retraining skills with real-world data, we propose locally updating the control parameters using the latest contact force measurements during online execution.We formulate online learning as an optimization problem that optimizes the residual control parameters to achieve smooth trajectory and task completion criteria while respecting the interaction dynamics between the robot and the environment.We will describe the optimization constraints, objective function, and overall online learning algorithm in Section 3.2.1,3.2.2, and 3.2.3,respectively.</p>
<p>Optimization Constraints</p>
<p>Robot dynamics constraint: Admittance control enables robot to behave as a mass-spring-damping system as shown in Eq. 1.We consider the robot state x = [e, ė], where e = x c − x d , and we can obtain the robot dynamics constraint in the state space form:
ẋ = ė ë = f (x, F ext , u) = ė −M −1 D ė − M −1 Ke + M −1 F ext(2)
where the optimization variable u
= [m −1 1 , . . . , m −1 6 , k ′ 1 , . . . , k ′ 6 , d ′ 1 , . . . , d ′ 6 ] T is the diagonal ele- ments of M −1 , K ′ = M −1 K, D ′ = M −1 D. e,
ė are the robot states that can be directly accessed, and F ext is the external force that should be modeled from the environment dynamics.</p>
<p>Contact force estimation: Modeling the contact force explicitly is difficult because the contact point and mode can change dramatically during manipulation.Therefore, we propose to estimate the contact force online using the force/torque sensor measurements.In our experiments, we utilize a simple but effective record &amp; replay strategy, where we record a sequence of force information {F 0 ext , . . ., F T ext } within a time window [0, T ] and replay them during the optimization.There are other approaches for force estimation, such as using analytical contact models [46,47] or numerically learning the contact force by model fitting.However, we found the record &amp; replay strategy is better by experiments.We provide analyses in the Appendix.C.2.</p>
<p>Stability constraint: To ensure stability for admittance control, we need the admittance parameters to be positive-definite.Therefore, we constrain the optimization variable u to be positive.</p>
<p>Objective Function Design</p>
<p>We want to optimize the admittance parameters to establish stable contact and successfully achieve the task.Previous work [48] introduces the FITAVE objective T 0 t| ė(t)|dt to effectively generate smooth and stable contact by regulating the robot's future velocity error.In addition, the ITAE objective +∞ 0 t|e(t)|dt in [49] minimizes the position error to ensure the robot tracking the desired trajectory and finishes the task.We combine those two functions as our objective:
C(x) = T 0 t[w|e(t)| + (1 − w)| ė(t)|]dt(3)
where w ∈ R is a weight scalar to balance the trajectory smoothness and task completion criterion.</p>
<p>Online admittance learning</p>
<p>The optimization formulation is shown in Eq. 4. We optimize the residual admittance parameters δu, with u init obtained from the offline learned skill.
min δu C(x) s.t. ẋ = f (x, F ext , u init + δu) F ext ← record &amp; replay u init + δu &gt; 0(4)
We illustrate the online admittance learning procedures in Alg. 1.In the online phase, we execute the skill learned offline on the real robot and recorded the contact force at each time step.Every T seconds, the online optimization uses the recorded force measurements, the current robot state and the admittance parameters learned offline to update the admittance parameter residual.The process runs in a closed-loop manner to complete the desired task robustly.
u = u init + δu * , M = diag{m 1 , • • • , m 6 }, K = M • diag{k ′ 1 , • • • , k ′ 6 }, D = M • diag{d′ 1 , • • • , d ′ 6 }(5)
Algorithm 1: Online Admittance Residual Learning Require: u init from the offline policy π(a|s), current robot state x 1: while task not terminated do 2:</p>
<p>if every T seconds then {F ext } ← record force sensor data 7: end while</p>
<p>Experiment Results</p>
<p>We conduct experiments on three contact-rich manipulation tasks, peg-in-hole assembly, pivoting, and screwing, to evaluate: 1) the robustness of sim-to-real transfer and 2) the generalizability of different task settings.We provide comparison results with two baselines in the assembly and pivoting tasks: 1) Direct Transfer: directly sim-to-real transfer both the learned robot trajectory and the control parameters [26], 2) Manual Tune: transfer learned trajectory with manually tuned control parameters [2].We consider three metrics for evaluation: 1) success rate indicates the robustness of transfer, 2) completion time for successful trials denotes the efficiency of the skills, and 3) max contact force shows the safety.The screwing experiments further demonstrate the robustness of our method for solving complex manipulation tasks.</p>
<p>Skill Learning in Simulation</p>
<p>We use Soft Actor-Critic [28] to learn manipulation skills in simulation. 2 During the evaluation, the learned assembly and pivoting skills both achieved a 100% success rate.Fig. 2(a) shows the snapshots of the learned assembly skills.The robot learns to search for the exact hole location on the hole surface with a learned variable admittance policy and smoothly inserts the peg into the hole.For the learned pivoting skill, the robot pushes the object against the wall and gradually pivots it to the target pose with suitable frictional force.</p>
<p>Sim-to-Real Transfer</p>
<p>We evaluate the sim-to-real transfer performance on the same task.In the real world, the task setup, such as the object and robot geometry, is identical to the simulation.We mainly focus on evaluating the effect of the sim-to-real gap on robot/environment dynamics.We first apply the offline learned skill to the real world.From the experiments, we notice that the Direct Transfer baseline fails to produce safe and stable interactions.As depicted in Fig 2(c), for peg-in-hole assembly tasks, the peg bounces on the hole surface and generates large contact forces, making the assembly task almost impossible to complete.Similarly, in the pivoting task, the robot cannot make stable contact with the object and provide enough frictional force for pivoting.</p>
<p>Then we examine whether the learned robot motion is valid with manually tuned control parameters.As shown in Tab. 1, the Manual Tune baseline can achieve a 100% success rate for both tasks.This supports our hypothesis and previous works that the manipulation trajectory is directly transferable with suitable control parameters to address the sim-to-real gap.</p>
<p>However, manual tuning requires extensive human labor.We want to evaluate whether the proposed online admittance learning framework can perform similarly without any tuning.Table 1 presents an overview of the sim-to-real transfer results.For all experiments, the weight parameter w of the proposed approach was consistently set to 0.4.Notably, our proposed method achieves a 100% success rate in the assembly task, along with a 90% success rate in the pivoting task.Furthermore, it achieves these results while exhibiting shorter completion times than the other two baselines.</p>
<p>We also investigate the contact force and the adjusted admittance parameters during the manipulation, shown in Fig. 2(c)(d).Initially, the robot establishes contact with the environment using the offline learned parameters, resulting in a large applied force.In the subsequent update cycle, the proposed method effectively adjusts the parameters by decreasing K and increasing D, enabling the robot to interact smoothly with the environment and reduce the contact force.Later, it increases K and decreases D to suitable values to finish the task more efficiently.</p>
<p>Generalization to Different Task Settings</p>
<p>The aforementioned experiments highlight the ability of the proposed framework to achieve simto-real transfer within the same task setting.In this section, we aim to explore the generalization capabilities of the proposed approach across different task settings, which may involve distinct kinematic and dynamic properties.For two baselines, we directly use the manually tuned or learned control parameters of the training object for new tasks.Peg-in-hole assembly: We test various assembly tasks, including polygon-shaped peg-holes such as triangles and pentagons, as well as real-world socket connectors like Ethernet and waterproof connectors.These tasks are visualized in Fig. 1(c).The outcomes of our experiments are outlined in Table 2. Our proposed method achieves 100% success rates on the polygon shapes and a commendable 90% success rate on Ethernet and waterproof connectors.Moreover, the completion time of the proposed method is much shorter than other baselines.The Manual Tune baseline also achieves decent success rates on the polygon shapes as it is similar to the scenario in that we tune the parameters.However, for the socket connectors, due their tighter fit and irregular shapes, substantial force is required for insertion (approximately 15N for Ethernet and 40N for waterproof connectors) and Manual Tune baseline cannot accomplish these two tasks.</p>
<p>Pivoting: Similarly, we conduct a series of pivoting experiments on various objects, encompassing diverse geometries and weights as shown in Table 2. Remarkably, our proposed approach exhibits robust generalization capabilities across all tasks, achieving a success rate exceeding 70%.However, when relying solely on manually tuned parameters, the ability to pivot an object is limited to the eraser that has a similar length to the trained object and is the lightest object in the test set.As the object geometry and weight diverge significantly, the manually tuned parameters often fail to establish stable contact with the object and exert sufficient force to initiate successful pivoting.</p>
<p>Screwing:</p>
<p>We conducted experiments on a more challenging robot screwing task to further validate our method.Its primary challenge is to precisely align the bolt with a nut and then smoothly secure them together.To address this, we employed the assembly skills previously learned for aligning the bolt and nut and then used a manually-designed rotation primitive to complete the screwing.Throughout the process, online admittance learning continually optimizes the admittance controller.Impressively, our approach allowed the robot to consistently and reliably align and secure the nut and bolt.We executed this task five times, achieving a 100% success rate.The detailed settings can be found in Appendix.D.2 and the experiment videos are available on our website.</p>
<p>Conclusion and Limitations</p>
<p>This paper proposes a contact-rich skill-learning framework for sim-to-real transfer.It consists of two main components: skill learning in simulation during the offline phase and admittance learning on the real robot during online execution.These components work together to enable the robot to acquire the necessary skills in simulation and optimize admittance control parameters for safe and stable interactions with the real-world environment.We evaluate the performance of our framework in three contact-rich manipulation tasks: assembly, pivoting, and screwing.Our approach achieves promising success rates in both tasks and demonstrates great generalizability across various tasks.</p>
<p>However, there are some limitations of our proposed framework: 1) Our method refines the policy during execution, which means that initially, a sub-optimal policy is used to make contact with the environment.As a result, this scheme may not be suitable for contact with fragile objects.2) We assume a simplified problem setup where the object is pre-grasped.However, real-world tasks may require the robots to first pick the object and then do the following manipulation tasks [43].3) We currently online learn/optimize the diagonal elements of admittance parameters.We'd like to consider learning other elements as suggested in [45].</p>
<p>B Simulation Training Details B.1 Domain Randomization Details for Contact-rich Tasks</p>
<p>In both the assembly and pivoting tasks, we introduced Gaussian noise with a mean of zero and a standard deviation of 0.2 N to the FT sensor readings as measurement noise.Additionally, we applied a clipping operation to the collected contact force, limiting it to the range of ±10 N for regulation purposes.To enhance the robustness of the learned skills, we incorporated randomization into the robot's initial pose.</p>
<p>For the assembly task, the robot's initial pose was uniformly sampled from a range of [±30 mm, ±30 mm, 30 ± 5 mm] along the X, Y , and Z axes, respectively.As for the pivoting task, the range for the initial pose was set to [150 ± 30 mm, 5 ± 5 mm] along the X and Z axes relative to the rigid wall.</p>
<p>B.2 RL Training Details</p>
<p>We use the Soft Actor Critic [28] with implementation in RLkit [52] to learn robot manipulation skills in simulation.The hyperparameter selections are summarized in We consider the manipulation policy for contact-rich manipulation tasks to contain a manipulation trajectory and the corresponding compliance control parameters.</p>
<p>The main difference between 'contact-rich' manipulation and regular manipulation tasks is how much force the robot exerts on the environment.The more force the robot applies, the more force it has to withstand.For contact-rich manipulation, the robot desired trajectory often has to penetrate the object with its end-effector to generate enough force for the task.For example, to wipe a table, the robot has to push its end-effector below the table surface.Since the robot is a rigid object, it needs a compliance controller to regulate its behavior and prevent potential damage.Compared to a position/velocity controller that might not need to tune the PID gains frequently, a compliance control is very sensitive [48] to the change of environment or task goals.It thus requires careful tuning of the parameters for each task.Therefore, for contact-rich manipulation, a suitable policy should be matched with the appropriate compliance control parameters to achieve the task smoothly.</p>
<p>C.2 Discussion on Approaches for Modeling Contact Force</p>
<p>A key component in our online admittance learning is the dynamics constraint, as shown below:
ẋ = ė ë = f (x, F ext , u) = ė −M −1 D ė − M −1 Ke + M −1 F ext(6)
where we want to regulate the future robot behavior based on the current robot state and the external force F ext .In optimization, when we change the admittance parameter M , K, and D, the robot motion will change, and the external force that the environment gives to the robot will change as well.Thus, a robust way to model the external force F ext is crucial in our online admittance learning.</p>
<p>To estimate or approximate the contact force in real time, we compare four approaches:</p>
<p>• record &amp; replay: We record the force/torque from the most recent measurements within a time window and directly use the pre-recorded data as F ext in the optimization.</p>
<p>• hybrid impulse dynamics: We use Eq. 6 with F ext = 0 when there is no contact.For the contact, we model it implicitly as M ẋ− = γM ẋ+ , where ẋ− and ẋ+ are the robot endeffector velocities before and after the contact.By online fitting the γ, we can optimize these hybrid dynamics to calculate the optimal parameters.</p>
<p>• analytical contact model with online parameter fitting: We model the contact explicitly using analytical models and fit the necessary parameters using online data, following [46,47].</p>
<p>• contact force fitting: We fit a contact force model using online force sensor measurements.</p>
<p>However, the hybrid impulse dynamics approach is not suitable for our requirements.As shown in Fig. 5, the contact force profile in contact-rich manipulation indicates that the robot maintains contact with the environment for most of the time.Therefore, neglecting the entire contact process and modeling it implicitly is not appropriate for our applications.</p>
<p>Similarly, analytical contact model with online parameter fitting does not fit our scenarios either.</p>
<p>Although it has been successful in some pivoting tasks, it relies on the quasi-static assumption that does not hold in our scenario.One of the main challenges of transferring the admittance parameters is to avoid the robot bouncing on the object.Moreover, the analytical model assumes point or sliding contact modes, which may be hard to generalize to different tasks, such as assembly.Finally, for contact force fitting, we assume a linear (spring-damping) contact force model: F ext = a(t)x(t) + b(t) ẋ(t) + c(t) within a short time window.We use the least square to estimate the parameters a, b, and c in real-time.Fig. 5 shows an example of fitting results.It can fit the force profile well in a short time window.However, as we need to apply the model learned in the previous time window to the next step, the generalization ability is poor as it is hard to capture the peak of the force profile.Experiment videos comparing the performance of contact force fitting and record &amp; replay are available on our website.We can observe that the contact force fitting method cannot stabilize the robot during contact.</p>
<p>C.3 Ablation: Objective Weight Selection</p>
<p>In this subsection, we would like to study the effect of weight selection.We evaluated different weight parameters on both the assembly and pivoting tasks.The results are depicted in Figure 6.For the assembly task, all proposed method variations achieve a 100% success rate except for w &lt;= 0.2.Smaller weight parameters tend to prioritize trajectory smoothness, which may not provide sufficient contact force for successful insertion.On the other hand, in pivoting tasks, larger weight values led to a decrease in the success rate.It is because larger weight values prioritize task completion, potentially leading to a failure in establishing a stable initial contact for pivoting.These observations align with the objective design motivation.Based on our findings, selecting the parameter 0.4 strikes a good balance between both objectives and yields the best overall performance.</p>
<p>D Baseline Results</p>
<p>D.1 Sim-to-real Transfer Here we provide snapshots of the baseline methods: Direct Transfer and Manual Tune.As introduced in the paper, Direct Transfer baseline utilizes the offline learned policy and directly applies it to the real robot without fine-tuning as [26] did.We hope the domain randomization on object position and force information can provide good generalizability and make it robust and transferable to real robots.</p>
<p>However, as shown in Fig. 7, direct applying the learned policy cannot achieve both tasks successfully.The main problem comes from the learned admittance control parameters.Where in the simulation, applying such parameters to the robot will not result in the robot bouncing on the object.In contrast, it can enable the robot to finish the task very efficiently.However, in the real world, such control parameters will result in large contact force and oscillation behaviors of the robot, which in turn, let the robot fails to establish stable contact with the object and finish the task.</p>
<p>For the Manual Tune baseline, we carefully tune the admittance parameters for each task in order to make the robot achieve smooth behavior during the contact.Table .4 summarizes the parameters.As shown in Fig. 7, the manually tuned baseline can successfully achieve the task.However, since it requires human tuning, it is not time-consuming and task-dependent.A practical problem of manually tuning the control parameters is the need of trying various combinations of parameters.</p>
<p>During this process, it is dangerous to let the robot interact with the environment and may cause damage to both the object and the robot.</p>
<p>D.2 Robot Screwing Task</p>
<p>For the robot screwing task, we first execute the assembly policy that is learned previously for 8 steps to align the bolt and nut.Then, we continuously apply a rotational motion which rotates the bolt by 20 • in the yaw direction while pushing down the bolt to screw the bolt to the nut.We conducted experiments on an M8 bolt-nut assembly task for five times achieving a 100% success rate.The experiment videos are available on our website.The snapshots of the screwing task are depicted in Fig. 9.</p>
<p>D.3 Generalization to Different Task Settings</p>
<p>In order to evaluate the generalization performance to different tasks, we conducted tests on various variations of tasks as depicted in Fig. 8.For assembly, these tasks included polygon-shaped peg holes, such as triangular peg-holes with an edge size of 51.4 mm and a clearance of 1.4 mm, as well as pentagon peg-holes with an edge size of 57.8 mm and a clearance of 1.3 mm.Additionally, we performed experiments on standard electric connectors, such as Ethernet and waterproof connectors, for further assessment.</p>
<p>Regarding the pivoting task, we expanded the test set to include different objects.These objects consisted of an adapter with dimensions of 8.8 * 4.1 * 2.6 cm 3 and a weight of 69 g, an eraser with dimensions of 12.2 * 4.8 * 3.0 cm 3 and a weight of 36 g, and a pocky with dimensions of 14.8 * 7.9 * 2.3 cm 3 and a weight of 76 g.</p>
<p>The snapshots of the Direct Transfer and Manual Tune baselines can be seen in Fig. 10 and 11, respectively.As observed in the sim-to-real experiments, the Direct Transfer baseline struggles to achieve stability during manipulation, resulting in failures when attempting to assemble or pivot objects of different shapes.On the other hand, the Manual Tune baseline demonstrates high success rates when dealing with polygon-shaped peg-holes and when pivoting the eraser.This success can be attributed to the similarity in geometric or dynamic properties between the learned object and these specific test objects.However, the Manual Tune baseline fails to generalize its performance to objects with significant differences, as illustrated in Fig. 11(c) and (d).</p>
<p>E Current Limitations and Future Improvements</p>
<p>As we discussed in the paper, our current framework has three main limitations: It assumes that the task settings in geometry are similar from training to testing.It uses a simple strategy for estimating the contact force.It has a relatively low update frequency and may not be suitable for manipulating fragile objects.</p>
<p>To address the first limitation, we plan to use meta-learning to learn the manipulation trajectory that can generalize well to different task settings.Meta-learning has been shown to be effective in generalizing the learned trajectory to various scenarios, and we believe that combining metalearning and our proposed online residual admittance learning can bridge the sim-to-real gap for many contact-rich manipulation tasks.Safe reinforcement learning [53,54,55] is another domain that we'd like to explore, as it can provide safety guarantees of the learned policy to enable a safer and smoother initial policy.For the second limitation, we are interested in exploring and experimenting with the analytical contact model approach as discussed in the Appendix.Using an analytical model and estimating the key parameters online may improve the performance.However, finding a general contact model or a method that can switch between different models will be the focus of our future work.</p>
<p>The last limitation is related to the time window size for online force/torque sensor data collection.We will try different time window sizes and increase the update frequency to enhance the adaptation performance in our future work.We also plan to incorporate recent advances in optimization to enable faster computation efficiency [56,57].</p>
<p>3 :
3
δu * ← admittance optimization in (4) 4: M, K, D ← Recover admittance parameters from (5) 5: end if 6:</p>
<p>Figure 2 :
2
Figure 2: (a) shows the snapshots of the learned policy in simulation.(b)demonstrates the snapshots using the proposed approach for sim-to-real transfer.(c)(d) illustrate the forces and control parameters profiles for both the learned and proposed approach in the real world.The proposed approach can adjust the parameters to get the best performance in real-time.</p>
<p>Figure 3 :
3
Figure 3: Snapshots of using the proposed approach to generalize to various task settings.The snapshots and videos of the baseline methods are available on our website.</p>
<p>Figure 5 :
5
Figure 5: Performance of online force fitting (in z axis).In every time window, collect the force/torque measurements and use the least square to fit the force model F ext (x, ẋ) = a(t)x(t) + b(t) ẋ(t) + c(t).On the left, it shows the linear model can fit the force profile locally.However, it can be extremely challenging to generalize to the next time window, as shown on the right.</p>
<p>Figure 6 :
6
Figure 6: Ablation on the weight parameter.The left figure shows the completion time and success rate with respect to different w, and the right figure shows the contact force.</p>
<p>Figure 7 :
7
Figure7: Snapshots of baseline approaches for the sim-to-real experiment.The control parameters learned in the simulation will result in a large contact force and makes the robot bounce on the surface, which will, in turn, result in failures of the tasks.</p>
<p>Figure 8 :Figure 9 :
89
Figure 8: Real-world manipulation tasks</p>
<p>Figure 10 :
10
Figure 10: Snapshots of directly using the learned policy to generalize to various task settings.The snapshots and videos of the baseline methods are available on our website.</p>
<p>Figure 11 :
11
Figure 11: Snapshots of directly using learned trajectory and the manually tuned admittance control parameters to generalize to various task settings.The snapshots and videos of the baseline methods are available on our website.</p>
<p>Table 1 :
1
Success rate evaluation in real-world experiments.
Assembly TaskPivoting TaskSucc. RateTime (s)Max F (N) Succ. RateTime (s)Max F (N)Proposed10/1019.0 ± 11.2 23.6 ± 6.39/1025.6 ± 2.1 20.1 ± 4.1Manual10/1028.1 ± 8.6 10.3 ± 2.210/1025.3 ± 3.6 9.2 ± 0.6Direct3/1039.0 ± 12.8 63.7 ± 6.80/10N/A30.7 ± 4.6</p>
<p>Table 2 :
2
Generalization performance to different assembly tasks (Top) and pivoting tasks (Below).
Triangle (gap = 1mm)Pentagon (gap = 1mm)Ethernet (gap = 0.17mm)Waterproof (gap = 0.21mm)Succ. RateTime (s)Succ. RateTime (s)Succ. RateTime (s)Succ. RateTime (s)Proposed10/1015.9 ± 6.210/1020.1 ± 8.99/1042.1 ± 13.79/1037.8 ± 17.7Manual8/1043. ± 17.09/1038.0 ± 18.01/1078.0 ± 0.00/10N/ADirect0/10N/A1/107.0 ± 0.00/10N/A0/10N/AAdapter [L=8.8 cm, w=69g] Eraser [L=12.2 cm, w=36g] Pocky Short [L=7.9 cm, w=76g] Pocky Long [L=14.8 cm, w=76g]Succ. RateTime (s)Succ. RateTime (s)Succ. RateTime (s)Succ. RateTime (s)Proposed8/1025.0 ± 4.89/1028.4 ± 2.78/1012.9 ± 1.77/1031.8 ± 11.0Manual0/10N/A10/1030.0 ± 1.01/1019.0 ± 0.01/1040.0 ± 0.0Direct0/10N/A0/10N/A0/10N/A0/10N/A</p>
<p>Table 3 :
3
Table. 3. Hyperparameters for RL training C Discussion on Proposed Approach C.1 Discussion on the Necessity of Learning the Compliance Control Parameters
HyperparametersAssemblyPivotingLearning rate -Policy1e-31e-4Learning rate -Q function1e-43e-4Networks[128,128] MLP [128,128] MLPBatch size40964096Soft target update (τ )5e-35e-3Discount factor (γ)0.950.9Replay buffer size1e61e6max path length2040eval steps per epoch100400expl steps per epoch5002000</p>
<p>Table 4 :
4
Manually tuned admittance control parameters for the experiments.
Tuned Admittance ParametersAssemblyPivotingEnd-effector Mass M (kg)[3, 3, 3][4, 4, 4]End-effector Inertia I (kgm 2 )[2, 2, 2][2, 2, 2]Position Stiffness K (N/m)[200, 200, 200] [300, 300, 300]Orientation Stiffness K (N m/rad)[200, 200, 200] [200, 200, 200]Position Damping D (N s/m)[300, 300, 300] [300, 300, 300]Orientation Damping D (N ms/rad) [250, 250, 250] [250, 250, 250]
We also tested other RL algorithms like DDPG[50] and TD3[28]. As a result, all the methods are able to learn a policy and have similar performance when transferring to the real robot. Details can be found on our website.
AcknowledgmentsWe gratefully acknowledge reviewers for the valuable feedback, and we extend our thanks to the FANUC Advanced Research Laboratory for their insightful discussions on robot hardware and control.Appendices A Task SetupA.1 Simulation DetailsWe build the simulation environment using MuJoCo[30]simulation for learning robot contact-rich manipulation skills.In the simulation, we include the model of the FANUC LRMate 200iD robot, and each of the joints is controlled with motor torque command.We incorporated an F/T sensor asset on the robot's wrist to measure the contact force.For the low-level controller, we employed computed torque control[51]to track the compliant trajectory x c and ẋc derived from the admittance controller.The simulation time step was set to 0.01 s.Further details regarding the assembly and pivoting setups are outlined below:Assembly: This task involves aligning a square-shaped peg with a hole.The edge length of the peg is 4 cm, and there is a clearance of 2 mm between the peg and hole.The friction coefficient between the peg and hole is configured as 0.3.Pivoting: In this task, the objective is to reorient a rectangular object against a rigid wall.The simulated object has dimensions of 10 × 10 × 2.6 cm 3 .A friction coefficient of 0.7 is assigned to all objects in the simulation.A.2 Real Robot Experiment SetupThe real robot setup is visualized in Fig.
Deep reinforcement learning for high precision assembly tasks. T Inoue, G De Magistris, A Munawar, T Yokoya, R Tachibana, IEEE/RSJ Int. Conf. on Intelligent Robots and Syst. (IROS). 2017. 2017IEEE</p>
<p>Learning insertion primitives with discrete-continuous hybrid action space for robotic assembly tasks. X Zhang, S Jin, C Wang, X Zhu, M Tomizuka, 2022 International Conference on Robotics and Automation (ICRA). IEEE2022</p>
<p>Learning sequences of manipulation primitives for robotic assembly. N Vuong, H Pham, Q.-C Pham, 2021 IEEE International Conference on Robotics and Automation (ICRA). IEEE2021</p>
<p>Reinforcement learning on variable impedance controller for high-precision robotic assembly. J Luo, E Solowjow, C Wen, J A Ojea, A M Agogino, A Tamar, P Abbeel, 2019 International Conference on Robotics and Automation (ICRA). IEEE2019</p>
<p>Contact pose identification for peg-in-hole assembly under uncertainties. S Jin, X Zhu, C Wang, M Tomizuka, 2021 American Control Conference (ACC). IEEE2021</p>
<p>Zero-shot policy transfer with disentangled task representation of meta-reinforcement learning. Z Wu, Y Xie, W Lian, C Wang, Y Guo, J Chen, S Schaal, M Tomizuka, 2023 IEEE International Conference on Robotics and Automation (ICRA). IEEE2023</p>
<p>Prim-lafd: A framework to learn and adapt primitive-based skills from demonstrations for insertion tasks. Z Wu, W Lian, C Wang, M Li, S Schaal, M Tomizuka, arXiv:2212.009552022arXiv preprint</p>
<p>Robot manipulation task learning by leveraging se (3) group invariance and equivariance. J Seo, N P S Prakash, X Zhang, C Wang, J Choi, M Tomizuka, R Horowitz, arXiv:2308.149842023arXiv preprint</p>
<p>Learning to grasp the ungraspable with emergent extrinsic dexterity. W Zhou, D Held, ICRA 2022 Workshop: Reinforcement Learning for Contact-Rich Manipulation. 2022</p>
<p>X Zhang, S Jain, B Huang, M Tomizuka, D Romeres, arXiv:2305.02554Learning generalizable pivoting skills. 2023arXiv preprint</p>
<p>Chance-constrained optimization in contact-rich systems for robust manipulation. Y Shirai, D K Jha, A Raghunathan, D Romeres, arXiv:2203.026162022arXiv preprint</p>
<p>6-dof contrastive grasp proposal network. X Zhu, L Sun, Y Fan, M Tomizuka, 2021 IEEE International Conference on Robotics and Automation (ICRA). IEEE2021</p>
<p>Optimization model for planning precision grasps with multi-fingered hands. Y Fan, X Zhu, M Tomizuka, 2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE2019</p>
<p>Learn to grasp with less supervision: A data-efficient maximum likelihood grasp sampling loss. X Zhu, Y Zhou, Y Fan, L Sun, J Chen, M Tomizuka, 2022 International Conference on Robotics and Automation (ICRA). IEEE2022</p>
<p>Let's push things forward: A survey on robot pushing. J Stüber, C Zito, R Stolkin, Frontiers in Robotics and AI. 82020</p>
<p>Learning an embedding space for transferable robot skills. K Hausman, J T Springenberg, Z Wang, N Heess, M Riedmiller, International Conference on Learning Representations. 2018</p>
<p>Y Narang, K Storey, I Akinola, M Macklin, P Reist, L Wawrzyniak, Y Guo, A Moravanszky, G State, M Lu, arXiv:2205.03532Fast contact for robotic assembly. 2022arXiv preprint</p>
<p>Allowing safe contact in robotic goal-reaching: Planning and tracking in operational and null spaces. X Zhu, W Lian, B Yuan, C D Freeman, M Tomizuka, IEEE International Conference on Robotics and Automation (ICRA). 2023</p>
<p>Humanoriented representation learning for robotic manipulation. M Huo, M Ding, C Xu, T Tian, X Zhu, Y Mu, L Sun, M Tomizuka, W Zhan, arXiv:2310.030232023arXiv preprint</p>
<p>Human-in-the-loop approach for teaching robot assembly tasks using impedance control interface. L Peternel, T Petrič, J Babič, 2015 IEEE int. conf. on robotics and automation (ICRA). IEEE2015</p>
<p>Force-based learning of variable impedance skills for robotic manipulation. F J Abu-Dakka, L Rozo, D G Caldwell, 2018 IEEE-RAS 18th Int. Conf. on Humanoid Robots (Humanoids). IEEE2018</p>
<p>Learning variable impedance control via inverse reinforcement learning for force-related tasks. X Zhang, L Sun, Z Kuang, M Tomizuka, IEEE Robotics and Automation Letters. 622021</p>
<p>Learning variable impedance control. J Buchli, F Stulp, E Theodorou, S Schaal, The Int. J. of Robotics Research. 3072011</p>
<p>Learning motions from demonstrations and rewards with time-invariant dynamical systems based policies. J Rey, K Kronander, F Farshidian, J Buchli, A Billard, Autonomous Robots. 4212018</p>
<p>Variable compliance control for robotic peg-in-hole assembly: A deep-reinforcement-learning approach. C C Beltran-Hernandez, D Petit, I G Ramirez-Alpizar, K Harada, Applied Sciences. 101969232020</p>
<p>Variable impedance control in end-effector space: An action space for reinforcement learning in contact-rich tasks. R Martín-Martín, M A Lee, R Gardner, S Savarese, J Bohg, A Garg, arXiv:1906.088802019arXiv preprint</p>
<p>Fundamental challenges in deep learning for stiff contact dynamics. M Parmar, M Halm, M Posa, 2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE2021</p>
<p>Soft actor-critic algorithms and applications. T Haarnoja, A Zhou, K Hartikainen, G Tucker, S Ha, J Tan, V Kumar, H Zhu, A Gupta, P Abbeel, arXiv:1812.059052018arXiv preprint</p>
<p>J Schulman, F Wolski, P Dhariwal, A Radford, O Klimov, arXiv:1707.06347Proximal policy optimization algorithms. 2017arXiv preprint</p>
<p>Mujoco: A physics engine for model-based control. E Todorov, T Erez, Y Tassa, IEEE/RSJ Int. Conf. on Intelligent Robots and Syst. 2012. 2012IEEE</p>
<p>Pybullet, a python module for physics simulation for games, robotics and machine learning. E Coumans, Y Bai, </p>
<p>System identification. L Ljung, 1998Springer</p>
<p>Real2sim2real: Self-supervised learning of physical single-step dynamic actions for planar robot casting. V Lim, H Huang, L Y Chen, J Wang, J Ichnowski, D Seita, M Laskey, K Goldberg, 10.1109/ICRA46639.2022.98116512022 International Conference on Robotics and Automation (ICRA). 2022</p>
<p>Guided online distillation: Promoting safe reinforcement learning by offline demonstration. J Li, X Liu, B Zhu, J Jiao, M Tomizuka, C Tang, W Zhan, arXiv:2309.094082023arXiv preprint</p>
<p>Efficient bimanual manipulation using learned task schemas. R Chitnis, S Tulsiani, S Gupta, A Gupta, 2020 IEEE International Conference on Robotics and Automation (ICRA). IEEE2020</p>
<p>Closing the sim-to-real loop: Adapting simulation randomization with real world experience. Y Chebotar, A Handa, V Makoviychuk, M Macklin, J Issac, N Ratliff, D Fox, 2019 International Conference on Robotics and Automation (ICRA). IEEE2019</p>
<p>Offlineonline learning of deformation model for cable manipulation with graph neural networks. C Wang, Y Zhang, X Zhang, Z Wu, X Zhu, S Jin, T Tang, M Tomizuka, IEEE Robotics and Automation Letters. 722022</p>
<p>A coarse-to-fine framework for dual-arm manipulation of deformable linear objects with whole-body obstacle avoidance. M Yu, K Lv, C Wang, M Tomizuka, X Li, 2023 IEEE International Conference on Robotics and Automation (ICRA). IEEE2023</p>
<p>Robust deformation model approximation for robotic cable manipulation. S Jin, C Wang, M Tomizuka, 2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE2019</p>
<p>Iterative residual policy: for goalconditioned dynamic manipulation of deformable objects. C Chi, B Burchfiel, E Cousineau, S Feng, S Song, arXiv:2203.006632022arXiv preprint</p>
<p>Online learning of unknown dynamics for modelbased controllers in legged locomotion. Y Sun, W L Ubellacker, W.-L Ma, X Zhang, C Wang, N V Csomay-Shanklin, M Tomizuka, K Sreenath, A D Ames, IEEE Robotics and Automation Letters. 642021</p>
<p>A Kumar, Z Fu, D Pathak, J Malik, arXiv:2107.04034Rma: Rapid motor adaptation for legged robots. 2021arXiv preprint</p>
<p>B Tang, M A Lin, I Akinola, A Handa, G S Sukhatme, F Ramos, D Fox, Y Narang, arXiv:2305.17110Industreal: Transferring contact-rich assembly tasks from simulation to reality. 2023arXiv preprint</p>
<p>Unified impedance and admittance control. C Ott, R Mukherjee, Y Nakamura, IEEE international conference on robotics and automation. 2010. 2010IEEE</p>
<p>Reinforcement learning of impedance policies for peg-in-hole tasks: Role of asymmetric matrices. S Kozlovsky, E Newman, M Zacksenhouse, IEEE Robotics and Automation Letters. 742022</p>
<p>Manipulation of unknown objects via contact configuration regulation. N Doshi, O Taylor, A Rodriguez, 2022 International Conference on Robotics and Automation (ICRA). IEEE2022</p>
<p>A convex polynomial model for planar sliding mechanics: theory, application, and experimental validation. J Zhou, M T Mason, R Paolini, D Bagnell, The International Journal of Robotics Research. 372-32018</p>
<p>Safe online gain optimization for cartesian space variable impedance control. C Wang, X Zhang, Z Kuang, M Tomizuka, 2022 IEEE 18th International Conference on Automation Science and Engineering (CASE). IEEE2022</p>
<p>Tuning pid controllers using the itae criterion. F G Martins, International Journal of Engineering Education. 2158672005</p>
<p>T P Lillicrap, J J Hunt, A Pritzel, N Heess, T Erez, Y Tassa, D Silver, D Wierstra, arXiv:1509.02971Continuous control with deep reinforcement learning. 2015arXiv preprint</p>
<p>Introduction to robotics. J J Craig, 2006Pearson Educacion</p>
<p>Rail-berkeley/rlkit: Collection of reinforcement learning algorithms. Rail-Berkeley, </p>
<p>A safe hierarchical planning framework for complex driving scenarios based on reinforcement learning. J Li, L Sun, J Chen, M Tomizuka, W Zhan, 2021 IEEE International Conference on Robotics and Automation (ICRA). IEEE2021</p>
<p>Dealing with the unknown: Pessimistic offline reinforcement learning. J Li, C Tang, M Tomizuka, W Zhan, Conference on Robot Learning. PMLR2022</p>
<p>Hierarchical planning through goal-conditioned offline reinforcement learning. J Li, C Tang, M Tomizuka, W Zhan, IEEE Robotics and Automation Letters. 742022</p>
<p>Trajectory splitting: A distributed formulation for collision avoiding trajectory optimization. C Wang, J Bingham, M Tomizuka, 2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE2021</p>
<p>Bpomp: A bilevel path optimization formulation for motion planning. C Wang, H.-C Lin, S Jin, X Zhu, L Sun, M Tomizuka, 2022 American Control Conference (ACC). IEEE2022</p>            </div>
        </div>

    </div>
</body>
</html>
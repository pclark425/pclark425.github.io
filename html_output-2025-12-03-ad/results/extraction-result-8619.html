<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8619 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8619</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8619</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-154.html">extraction-schema-154</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-276249784</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2502.06563v1.pdf" target="_blank">Large Language Models Meet Symbolic Provers for Logical Reasoning Evaluation</a></p>
                <p><strong>Paper Abstract:</strong> First-order logic (FOL) reasoning, which involves sequential deduction, is pivotal for intelligent systems and serves as a valuable task for evaluating reasoning capabilities, particularly in chain-of-thought (CoT) contexts. Existing benchmarks often rely on extensive human annotation or handcrafted templates, making it difficult to achieve the necessary complexity, scalability, and diversity for robust evaluation. To address these limitations, we propose a novel framework called ProverGen that synergizes the generative strengths of Large Language Models (LLMs) with the rigor and precision of symbolic provers, enabling the creation of a scalable, diverse, and high-quality FOL reasoning dataset, ProverQA. ProverQA is also distinguished by its inclusion of accessible and logically coherent intermediate reasoning steps for each problem. Our evaluation shows that state-of-the-art LLMs struggle to solve ProverQA problems, even with CoT prompting, highlighting the dataset's challenging nature. We also finetune Llama3.1-8B-Instruct on a separate training set generated by our framework. The finetuned model demonstrates consistent improvements on both in-distribution and out-of-distribution test sets, suggesting the value of our proposed data generation framework. Code available at: https://github.com/opendatalab/ProverGen</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8619.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8619.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Proprietary state-of-the-art OpenAI large language model used in the paper's experiments and evaluated on multiple FOL reasoning benchmarks (ProverQA, ProofWriter, ProntoQA, FOLIO) under standard and chain-of-thought prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Proprietary OpenAI LLM (version used: gpt-4o-2024-08-06). Evaluated as a black-box SOTA model in this work under 2-shot standard prompting and 2-shot chain-of-thought prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>ProverQA (and comparison on ProntoQA, ProofWriter, FOLIO)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>First-order logic (FOL) multi-step deductive reasoning benchmark (ProverQA) with explicit intermediate reasoning chains, distractions, and three difficulty tiers (easy 1–2 steps, medium 3–5, hard 6–9). Tasks require deriving True/False/Uncertain for goals given facts and rules.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Evaluation with Standard (2-shot in-context) prompting and Chain-of-Thought (CoT) 2-shot prompting; ablations for distractions and premise order; no model-internal fine-tuning (used as evaluator).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>On ProverQA with CoT prompting: easy 94.20% accuracy, medium 79.40%, hard 50.00% (hard subset ≈50%); Standard prompting lower (CoT gives large gains on easy/medium but not on hard). Also evaluated across ProntoQA/ProofWriter/FOLIO in Table 2.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>CoT prompting substantially improves performance vs Standard prompting on easy/medium; ablations (Table 3) show removing distractions raises hard accuracy from 50.00% → 57.40% and also reordering premises further to 62.40%. Corruption ablation (removing universal rules) causes large drops (Direct: 58.33%→43.33%, CoT: 68.33%→45.00%).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Struggles on long/challenging reasoning chains (hard subset), barely exceeds 50% on hard; performance sensitive to distractions and premise order; examples of failures include using common-sense shortcuts, unfaithful chains, and inability to reliably extend multi-step deductions.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>CoT helps but is insufficient for hard FOL chains; removing distractions and presenting premises in logical order helps; symbolic-grounded data (ProverQA) exposes weaknesses even in SOTA proprietary models.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models Meet Symbolic Provers for Logical Reasoning Evaluation', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8619.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8619.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Claude-3.5-Sonnet</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Anthropic Claude-3.5 (Sonnet)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Proprietary Anthropic model evaluated as a SOTA baseline on ProverQA and other FOL datasets under standard and CoT prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Claude-3.5-Sonnet</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Proprietary Anthropic LLM (version used: claude-3-5-sonnet-20240620). Treated as a high-performing baseline for logical reasoning experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>ProverQA (and comparison on ProntoQA, ProofWriter, FOLIO)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>FOL multi-step deduction with explicit proofs and distractions; evaluate True/False/Uncertain judgments across difficulty tiers.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Evaluated with Standard 2-shot and CoT 2-shot prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>With CoT prompting: easy ≈95.20%, medium ≈83.60%, hard ≈56.40% (hard subset near or slightly above 50%).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>CoT yields substantial gains on easy/medium relative to Standard prompting; still underperforms substantially on the hard subset compared to perfect reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Fails on many hard multi-step FOL problems; susceptible to distractions and ordering; exhibits failure modes such as unfaithful use of background story or use of common-sense shortcuts.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>High-performing closed-source LLMs benefit from CoT but remain challenged by long-chain FOL problems; dataset ProverQA reveals these weaknesses.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models Meet Symbolic Provers for Logical Reasoning Evaluation', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8619.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8619.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Mistral-Large-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mistral-Large-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Open-source high-capacity Mistral model variant evaluated on ProverQA, showing strong performance on easy/medium but limited on hard FOL chains.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Mistral-Large-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source Mistral family instruction-tuned model (large Instruct variant used by authors). Evaluated under Standard and CoT prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>ProverQA (and comparison on ProntoQA, ProofWriter, FOLIO)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>FOL reasoning benchmark requiring multi-step deduction and correct handling of contradictions, disjunctions, quantifiers and distractions.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Standard 2-shot and CoT 2-shot prompting evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>With CoT prompting: easy ≈92.60%, medium ≈75.80%, hard ≈52.20% (hard ≈50%).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>CoT improves performance on easy/medium over Standard prompting; still limited on hard subset.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Limited ability on long reasoning chains; performance sensitive to distractions/premise order; does not reliably exceed ~50% on hard tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Even larger open-source models exhibit substantial gaps on strict FOL reasoning tasks; CoT is necessary but not sufficient.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models Meet Symbolic Provers for Logical Reasoning Evaluation', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8619.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8619.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Mixtral-8x22B-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mixtral-8x22B-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Mixture-of-experts style open-source model (Mixtral family) evaluated on ProverQA as part of the model suite tested in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Mixtral-8x22B-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source Mixture-of-Experts (MoE) model referenced by name in experiments (Mixtral-8x22B-Instruct); evaluated under Standard and CoT prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>8x22B (as in name)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>ProverQA (and comparison on ProntoQA, ProofWriter, FOLIO)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>FOL multi-step reasoning (True/False/Uncertain) with explicit chains and distractions.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Standard and CoT prompting evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>With CoT prompting: easy ≈87.60%, medium ≈66.80%, hard ≈47.60% (hard ≈50% or below).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>CoT improves results over Standard prompting on easier problems; performance on hard remains well below perfect.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Struggles more than largest SOTA proprietary models on hard subset; sensitive to dataset artifacts like distractions and ordering.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Mixture-of-experts models show moderate gains with CoT but still fail many long-chain FOL problems; highlights need for targeted training or symbolic support.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models Meet Symbolic Provers for Logical Reasoning Evaluation', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8619.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8619.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Llama3.1-70B-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Llama 3.1 70B Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>70B parameter Llama 3.1 instruction-tuned model evaluated as a strong open-source baseline on ProverQA under different prompting strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama3.1-70B-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source Llama 3.1 instruction-tuned 70B parameter model (used in both generation pipeline and evaluation).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>70B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>ProverQA (and comparison on ProntoQA, ProofWriter, FOLIO)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>First-order logical deduction tasks with chained inferences, quantifiers, Boolean connectives, and distractions.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Evaluated with Standard and CoT prompting; also used as the generator model (Llama3.1-70B-Instruct used within ProverGen generation framework).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>With CoT prompting: easy ≈90.40%, medium ≈73.20%, hard ≈46.80% (hard ≈50% or below).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>CoT improves performance relative to Standard prompting, but hard subset remains challenging; used as generator in ProverGen so also part of data-generation experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Hard multi-step chains remain a failure mode; sensitive to distractions and premise order; corruption ablation (removing universal rules) lowered performance (CoT: 65.00%→53.33% in corrupted set for the 70B variant reported in Table 6).</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Large open-source LLMs can be used productively in both generation and evaluation but still fail many strict FOL reasoning instances, motivating symbolic augmentation and targeted finetuning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models Meet Symbolic Provers for Logical Reasoning Evaluation', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8619.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8619.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Llama3.1-8B-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Llama 3.1 8B Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>8B parameter open-source Llama 3.1 instruction-tuned model; finetuned in experiments on ProverQA to measure gains from symbolic-grounded synthetic training data.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama3.1-8B-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source Llama 3.1 instruction-following model with ~8B parameters; evaluated before and after finetuning on ProverQA and other FOL datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>8B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>ProverQA (and OOD evaluation on ProntoQA, ProofWriter, FOLIO)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>FOL reasoning benchmark with required stepwise deduction, handling of logical connectives and quantifiers, and explicit reasoning chains.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Evaluated under Standard and CoT prompting; additionally finetuned on a ProverQA-generated training set (5,000 instances sampled) and compared to finetuning on ProofWriter and FOLIO.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Pre-finetuning CoT on ProverQA: easy 75.60%, medium 46.60%, hard 33.60%. After finetuning on ProverQA training data: easy 97.00% (+21.4%), medium 90.60% (+44.0%), hard 68.20% (+34.6%). OOD improvements on other datasets were +5% to +8.8% on average.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Finetuning on ProverQA substantially outperforms finetuning on other datasets (ProofWriter, FOLIO) both in-distribution and yields the best average OOD gains; demonstrates value of ProverGen generated training data.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Before finetuning, small model struggled even on easy subset; after finetuning improvements are substantial but hard subset accuracy still lower than perfect (68.20% post-finetune). Dataset-specific generalization still imperfect.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Targeted finetuning on high-quality, prover-validated synthetic FOL examples yields large gains (especially on medium/hard cases) and improves OOD generalization compared to prior datasets; shows value of neuro-symbolic data generation for training.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models Meet Symbolic Provers for Logical Reasoning Evaluation', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8619.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e8619.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Mistral-7B-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mistral-7B-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>7B-parameter instruction-tuned Mistral model evaluated as a smaller open-source baseline on ProverQA.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Mistral-7B-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source Mistral family 7B instruction-tuned model evaluated under Standard and CoT prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>ProverQA (and comparison on other FOL datasets)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Multi-step FOL reasoning requiring exact logical deduction across chains; aims to classify goals as True/False/Uncertain.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Standard 2-shot and CoT 2-shot prompting evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>With CoT prompting: easy ≈72.00%, medium ≈51.00%, hard ≈41.80% (markedly lower than larger models).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Smaller model underperforms larger/open-source models; CoT helps but absolute accuracy remains modest.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Fails frequently on medium and hard problems; limited chain length handling and vulnerability to distractions.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Smaller instruction-tuned models need targeted training or symbolic augmentation to approach SOTA reasoning performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models Meet Symbolic Provers for Logical Reasoning Evaluation', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8619.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e8619.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ProverQA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ProverQA (this paper's benchmark)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A synthetic first-order logic (FOL) reasoning benchmark generated by ProverGen combining LLM story generation and symbolic prover (Prover9) validation; contains 1,500 instances across easy/medium/hard with faithful intermediate reasoning chains and distractions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Dataset (benchmark) created by the authors using a three-stage pipeline: LLM background-story generation, symbolic logic-skeleton construction/validation with Prover9, and LLM translation of logic expressions to natural language statements.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>ProverQA</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Strict FOL reasoning benchmark covering seven FOL constructs (∧, ∨, ¬, →, ≡, ∀, ∃), includes distractions and step-by-step reasoning chains; goal: classify statements as True/False/Uncertain.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Dataset generation combines LLMs (Llama3.1-70B-Instruct used in generation) with symbolic prover Prover9 to validate reasoning trees (top-down generation), includes distractions and two translation versions (universal/specific) per rule; used for evaluation and finetuning experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Used to evaluate multiple LLMs: SOTA models (GPT-4o, Claude-3.5, Mistral-Large) achieve high accuracy on easy/medium with CoT but barely exceed ~50% on hard; finetuning 8B Llama on ProverQA training data yields large gains (e.g., +21.4%/+44.0%/+34.6% on easy/medium/hard respectively).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>ProverQA is more challenging than prior template/manual datasets: models that nearly solve ProntoQA perform poorly on ProverQA's hard subset; finetuning on ProverQA outperforms finetuning on ProofWriter and FOLIO in both ID and OOD evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Not a model — but authors note that CoT helps little on hardest problems, and that distractions and shuffled premises significantly degrade performance of evaluated models; dataset generation depends on LLMs for natural language translation so potential (but low) residual translation errors exist (manual sample showed <2% error).</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>ProverQA exposes meaningful gaps in current LLM reasoning; combining LLM generation with symbolic proving yields datasets that both challenge SOTA models and provide effective training data that improves ID and OOD performance when used for finetuning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models Meet Symbolic Provers for Logical Reasoning Evaluation', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>ProofWriter: Generating implications, proofs, and abductive statements over natural language <em>(Rating: 2)</em></li>
                <li>ProntoQA <em>(Rating: 2)</em></li>
                <li>RuleTaker <em>(Rating: 2)</em></li>
                <li>FOLIO <em>(Rating: 2)</em></li>
                <li>Linc: A neurosymbolic approach for logical reasoning by combining language models with first-order logic provers <em>(Rating: 2)</em></li>
                <li>Logic-lm: Empowering large language models with symbolic solvers for faithful logical reasoning <em>(Rating: 2)</em></li>
                <li>SatLM: Satisfiability-aided language models using declarative prompting <em>(Rating: 1)</em></li>
                <li>Sola: Solver-layer adaption of llm for better logic reasoning <em>(Rating: 1)</em></li>
                <li>LogicNLI <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8619",
    "paper_id": "paper-276249784",
    "extraction_schema_id": "extraction-schema-154",
    "extracted_data": [
        {
            "name_short": "GPT-4o",
            "name_full": "GPT-4o",
            "brief_description": "Proprietary state-of-the-art OpenAI large language model used in the paper's experiments and evaluated on multiple FOL reasoning benchmarks (ProverQA, ProofWriter, ProntoQA, FOLIO) under standard and chain-of-thought prompting.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4o",
            "model_description": "Proprietary OpenAI LLM (version used: gpt-4o-2024-08-06). Evaluated as a black-box SOTA model in this work under 2-shot standard prompting and 2-shot chain-of-thought prompting.",
            "model_size": null,
            "reasoning_task_name": "ProverQA (and comparison on ProntoQA, ProofWriter, FOLIO)",
            "reasoning_task_description": "First-order logic (FOL) multi-step deductive reasoning benchmark (ProverQA) with explicit intermediate reasoning chains, distractions, and three difficulty tiers (easy 1–2 steps, medium 3–5, hard 6–9). Tasks require deriving True/False/Uncertain for goals given facts and rules.",
            "method_or_approach": "Evaluation with Standard (2-shot in-context) prompting and Chain-of-Thought (CoT) 2-shot prompting; ablations for distractions and premise order; no model-internal fine-tuning (used as evaluator).",
            "performance": "On ProverQA with CoT prompting: easy 94.20% accuracy, medium 79.40%, hard 50.00% (hard subset ≈50%); Standard prompting lower (CoT gives large gains on easy/medium but not on hard). Also evaluated across ProntoQA/ProofWriter/FOLIO in Table 2.",
            "baseline_comparison": "CoT prompting substantially improves performance vs Standard prompting on easy/medium; ablations (Table 3) show removing distractions raises hard accuracy from 50.00% → 57.40% and also reordering premises further to 62.40%. Corruption ablation (removing universal rules) causes large drops (Direct: 58.33%→43.33%, CoT: 68.33%→45.00%).",
            "limitations_or_failures": "Struggles on long/challenging reasoning chains (hard subset), barely exceeds 50% on hard; performance sensitive to distractions and premise order; examples of failures include using common-sense shortcuts, unfaithful chains, and inability to reliably extend multi-step deductions.",
            "insights_or_conclusions": "CoT helps but is insufficient for hard FOL chains; removing distractions and presenting premises in logical order helps; symbolic-grounded data (ProverQA) exposes weaknesses even in SOTA proprietary models.",
            "uuid": "e8619.0",
            "source_info": {
                "paper_title": "Large Language Models Meet Symbolic Provers for Logical Reasoning Evaluation",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "Claude-3.5-Sonnet",
            "name_full": "Anthropic Claude-3.5 (Sonnet)",
            "brief_description": "Proprietary Anthropic model evaluated as a SOTA baseline on ProverQA and other FOL datasets under standard and CoT prompting.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Claude-3.5-Sonnet",
            "model_description": "Proprietary Anthropic LLM (version used: claude-3-5-sonnet-20240620). Treated as a high-performing baseline for logical reasoning experiments.",
            "model_size": null,
            "reasoning_task_name": "ProverQA (and comparison on ProntoQA, ProofWriter, FOLIO)",
            "reasoning_task_description": "FOL multi-step deduction with explicit proofs and distractions; evaluate True/False/Uncertain judgments across difficulty tiers.",
            "method_or_approach": "Evaluated with Standard 2-shot and CoT 2-shot prompting.",
            "performance": "With CoT prompting: easy ≈95.20%, medium ≈83.60%, hard ≈56.40% (hard subset near or slightly above 50%).",
            "baseline_comparison": "CoT yields substantial gains on easy/medium relative to Standard prompting; still underperforms substantially on the hard subset compared to perfect reasoning.",
            "limitations_or_failures": "Fails on many hard multi-step FOL problems; susceptible to distractions and ordering; exhibits failure modes such as unfaithful use of background story or use of common-sense shortcuts.",
            "insights_or_conclusions": "High-performing closed-source LLMs benefit from CoT but remain challenged by long-chain FOL problems; dataset ProverQA reveals these weaknesses.",
            "uuid": "e8619.1",
            "source_info": {
                "paper_title": "Large Language Models Meet Symbolic Provers for Logical Reasoning Evaluation",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "Mistral-Large-Instruct",
            "name_full": "Mistral-Large-Instruct",
            "brief_description": "Open-source high-capacity Mistral model variant evaluated on ProverQA, showing strong performance on easy/medium but limited on hard FOL chains.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Mistral-Large-Instruct",
            "model_description": "Open-source Mistral family instruction-tuned model (large Instruct variant used by authors). Evaluated under Standard and CoT prompting.",
            "model_size": null,
            "reasoning_task_name": "ProverQA (and comparison on ProntoQA, ProofWriter, FOLIO)",
            "reasoning_task_description": "FOL reasoning benchmark requiring multi-step deduction and correct handling of contradictions, disjunctions, quantifiers and distractions.",
            "method_or_approach": "Standard 2-shot and CoT 2-shot prompting evaluations.",
            "performance": "With CoT prompting: easy ≈92.60%, medium ≈75.80%, hard ≈52.20% (hard ≈50%).",
            "baseline_comparison": "CoT improves performance on easy/medium over Standard prompting; still limited on hard subset.",
            "limitations_or_failures": "Limited ability on long reasoning chains; performance sensitive to distractions/premise order; does not reliably exceed ~50% on hard tasks.",
            "insights_or_conclusions": "Even larger open-source models exhibit substantial gaps on strict FOL reasoning tasks; CoT is necessary but not sufficient.",
            "uuid": "e8619.2",
            "source_info": {
                "paper_title": "Large Language Models Meet Symbolic Provers for Logical Reasoning Evaluation",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "Mixtral-8x22B-Instruct",
            "name_full": "Mixtral-8x22B-Instruct",
            "brief_description": "Mixture-of-experts style open-source model (Mixtral family) evaluated on ProverQA as part of the model suite tested in the paper.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Mixtral-8x22B-Instruct",
            "model_description": "Open-source Mixture-of-Experts (MoE) model referenced by name in experiments (Mixtral-8x22B-Instruct); evaluated under Standard and CoT prompting.",
            "model_size": "8x22B (as in name)",
            "reasoning_task_name": "ProverQA (and comparison on ProntoQA, ProofWriter, FOLIO)",
            "reasoning_task_description": "FOL multi-step reasoning (True/False/Uncertain) with explicit chains and distractions.",
            "method_or_approach": "Standard and CoT prompting evaluations.",
            "performance": "With CoT prompting: easy ≈87.60%, medium ≈66.80%, hard ≈47.60% (hard ≈50% or below).",
            "baseline_comparison": "CoT improves results over Standard prompting on easier problems; performance on hard remains well below perfect.",
            "limitations_or_failures": "Struggles more than largest SOTA proprietary models on hard subset; sensitive to dataset artifacts like distractions and ordering.",
            "insights_or_conclusions": "Mixture-of-experts models show moderate gains with CoT but still fail many long-chain FOL problems; highlights need for targeted training or symbolic support.",
            "uuid": "e8619.3",
            "source_info": {
                "paper_title": "Large Language Models Meet Symbolic Provers for Logical Reasoning Evaluation",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "Llama3.1-70B-Instruct",
            "name_full": "Llama 3.1 70B Instruct",
            "brief_description": "70B parameter Llama 3.1 instruction-tuned model evaluated as a strong open-source baseline on ProverQA under different prompting strategies.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Llama3.1-70B-Instruct",
            "model_description": "Open-source Llama 3.1 instruction-tuned 70B parameter model (used in both generation pipeline and evaluation).",
            "model_size": "70B",
            "reasoning_task_name": "ProverQA (and comparison on ProntoQA, ProofWriter, FOLIO)",
            "reasoning_task_description": "First-order logical deduction tasks with chained inferences, quantifiers, Boolean connectives, and distractions.",
            "method_or_approach": "Evaluated with Standard and CoT prompting; also used as the generator model (Llama3.1-70B-Instruct used within ProverGen generation framework).",
            "performance": "With CoT prompting: easy ≈90.40%, medium ≈73.20%, hard ≈46.80% (hard ≈50% or below).",
            "baseline_comparison": "CoT improves performance relative to Standard prompting, but hard subset remains challenging; used as generator in ProverGen so also part of data-generation experiments.",
            "limitations_or_failures": "Hard multi-step chains remain a failure mode; sensitive to distractions and premise order; corruption ablation (removing universal rules) lowered performance (CoT: 65.00%→53.33% in corrupted set for the 70B variant reported in Table 6).",
            "insights_or_conclusions": "Large open-source LLMs can be used productively in both generation and evaluation but still fail many strict FOL reasoning instances, motivating symbolic augmentation and targeted finetuning.",
            "uuid": "e8619.4",
            "source_info": {
                "paper_title": "Large Language Models Meet Symbolic Provers for Logical Reasoning Evaluation",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "Llama3.1-8B-Instruct",
            "name_full": "Llama 3.1 8B Instruct",
            "brief_description": "8B parameter open-source Llama 3.1 instruction-tuned model; finetuned in experiments on ProverQA to measure gains from symbolic-grounded synthetic training data.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Llama3.1-8B-Instruct",
            "model_description": "Open-source Llama 3.1 instruction-following model with ~8B parameters; evaluated before and after finetuning on ProverQA and other FOL datasets.",
            "model_size": "8B",
            "reasoning_task_name": "ProverQA (and OOD evaluation on ProntoQA, ProofWriter, FOLIO)",
            "reasoning_task_description": "FOL reasoning benchmark with required stepwise deduction, handling of logical connectives and quantifiers, and explicit reasoning chains.",
            "method_or_approach": "Evaluated under Standard and CoT prompting; additionally finetuned on a ProverQA-generated training set (5,000 instances sampled) and compared to finetuning on ProofWriter and FOLIO.",
            "performance": "Pre-finetuning CoT on ProverQA: easy 75.60%, medium 46.60%, hard 33.60%. After finetuning on ProverQA training data: easy 97.00% (+21.4%), medium 90.60% (+44.0%), hard 68.20% (+34.6%). OOD improvements on other datasets were +5% to +8.8% on average.",
            "baseline_comparison": "Finetuning on ProverQA substantially outperforms finetuning on other datasets (ProofWriter, FOLIO) both in-distribution and yields the best average OOD gains; demonstrates value of ProverGen generated training data.",
            "limitations_or_failures": "Before finetuning, small model struggled even on easy subset; after finetuning improvements are substantial but hard subset accuracy still lower than perfect (68.20% post-finetune). Dataset-specific generalization still imperfect.",
            "insights_or_conclusions": "Targeted finetuning on high-quality, prover-validated synthetic FOL examples yields large gains (especially on medium/hard cases) and improves OOD generalization compared to prior datasets; shows value of neuro-symbolic data generation for training.",
            "uuid": "e8619.5",
            "source_info": {
                "paper_title": "Large Language Models Meet Symbolic Provers for Logical Reasoning Evaluation",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "Mistral-7B-Instruct",
            "name_full": "Mistral-7B-Instruct",
            "brief_description": "7B-parameter instruction-tuned Mistral model evaluated as a smaller open-source baseline on ProverQA.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Mistral-7B-Instruct",
            "model_description": "Open-source Mistral family 7B instruction-tuned model evaluated under Standard and CoT prompting.",
            "model_size": "7B",
            "reasoning_task_name": "ProverQA (and comparison on other FOL datasets)",
            "reasoning_task_description": "Multi-step FOL reasoning requiring exact logical deduction across chains; aims to classify goals as True/False/Uncertain.",
            "method_or_approach": "Standard 2-shot and CoT 2-shot prompting evaluation.",
            "performance": "With CoT prompting: easy ≈72.00%, medium ≈51.00%, hard ≈41.80% (markedly lower than larger models).",
            "baseline_comparison": "Smaller model underperforms larger/open-source models; CoT helps but absolute accuracy remains modest.",
            "limitations_or_failures": "Fails frequently on medium and hard problems; limited chain length handling and vulnerability to distractions.",
            "insights_or_conclusions": "Smaller instruction-tuned models need targeted training or symbolic augmentation to approach SOTA reasoning performance.",
            "uuid": "e8619.6",
            "source_info": {
                "paper_title": "Large Language Models Meet Symbolic Provers for Logical Reasoning Evaluation",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "ProverQA",
            "name_full": "ProverQA (this paper's benchmark)",
            "brief_description": "A synthetic first-order logic (FOL) reasoning benchmark generated by ProverGen combining LLM story generation and symbolic prover (Prover9) validation; contains 1,500 instances across easy/medium/hard with faithful intermediate reasoning chains and distractions.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": null,
            "model_description": "Dataset (benchmark) created by the authors using a three-stage pipeline: LLM background-story generation, symbolic logic-skeleton construction/validation with Prover9, and LLM translation of logic expressions to natural language statements.",
            "model_size": null,
            "reasoning_task_name": "ProverQA",
            "reasoning_task_description": "Strict FOL reasoning benchmark covering seven FOL constructs (∧, ∨, ¬, →, ≡, ∀, ∃), includes distractions and step-by-step reasoning chains; goal: classify statements as True/False/Uncertain.",
            "method_or_approach": "Dataset generation combines LLMs (Llama3.1-70B-Instruct used in generation) with symbolic prover Prover9 to validate reasoning trees (top-down generation), includes distractions and two translation versions (universal/specific) per rule; used for evaluation and finetuning experiments.",
            "performance": "Used to evaluate multiple LLMs: SOTA models (GPT-4o, Claude-3.5, Mistral-Large) achieve high accuracy on easy/medium with CoT but barely exceed ~50% on hard; finetuning 8B Llama on ProverQA training data yields large gains (e.g., +21.4%/+44.0%/+34.6% on easy/medium/hard respectively).",
            "baseline_comparison": "ProverQA is more challenging than prior template/manual datasets: models that nearly solve ProntoQA perform poorly on ProverQA's hard subset; finetuning on ProverQA outperforms finetuning on ProofWriter and FOLIO in both ID and OOD evaluations.",
            "limitations_or_failures": "Not a model — but authors note that CoT helps little on hardest problems, and that distractions and shuffled premises significantly degrade performance of evaluated models; dataset generation depends on LLMs for natural language translation so potential (but low) residual translation errors exist (manual sample showed &lt;2% error).",
            "insights_or_conclusions": "ProverQA exposes meaningful gaps in current LLM reasoning; combining LLM generation with symbolic proving yields datasets that both challenge SOTA models and provide effective training data that improves ID and OOD performance when used for finetuning.",
            "uuid": "e8619.7",
            "source_info": {
                "paper_title": "Large Language Models Meet Symbolic Provers for Logical Reasoning Evaluation",
                "publication_date_yy_mm": "2025-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "ProofWriter: Generating implications, proofs, and abductive statements over natural language",
            "rating": 2,
            "sanitized_title": "proofwriter_generating_implications_proofs_and_abductive_statements_over_natural_language"
        },
        {
            "paper_title": "ProntoQA",
            "rating": 2
        },
        {
            "paper_title": "RuleTaker",
            "rating": 2
        },
        {
            "paper_title": "FOLIO",
            "rating": 2
        },
        {
            "paper_title": "Linc: A neurosymbolic approach for logical reasoning by combining language models with first-order logic provers",
            "rating": 2,
            "sanitized_title": "linc_a_neurosymbolic_approach_for_logical_reasoning_by_combining_language_models_with_firstorder_logic_provers"
        },
        {
            "paper_title": "Logic-lm: Empowering large language models with symbolic solvers for faithful logical reasoning",
            "rating": 2,
            "sanitized_title": "logiclm_empowering_large_language_models_with_symbolic_solvers_for_faithful_logical_reasoning"
        },
        {
            "paper_title": "SatLM: Satisfiability-aided language models using declarative prompting",
            "rating": 1,
            "sanitized_title": "satlm_satisfiabilityaided_language_models_using_declarative_prompting"
        },
        {
            "paper_title": "Sola: Solver-layer adaption of llm for better logic reasoning",
            "rating": 1,
            "sanitized_title": "sola_solverlayer_adaption_of_llm_for_better_logic_reasoning"
        },
        {
            "paper_title": "LogicNLI",
            "rating": 1
        }
    ],
    "cost": 0.019014,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>LARGE LANGUAGE MODELS MEET SYMBOLIC PROVERS FOR LOGICAL REASONING EVALUATION
2 Mar 2025</p>
<p>Chengwen Qi chengwenqi@buaa.edu.cn 
Beihang University</p>
<p>Shanghai Artificial Intelligence Laboratory</p>
<p>Ren Ma maren@pjlab.org 
Shanghai Artificial Intelligence Laboratory</p>
<p>Bowen Li libowen@pjlab.org 
Shanghai Artificial Intelligence Laboratory</p>
<p>He Du 
Shanghai Artificial Intelligence Laboratory</p>
<p>Fudan University</p>
<p>Binyuan Hui 
Jinwang Wu 
Beihang University</p>
<p>Yuanjun Laili lailiyuanjun@buaa.edu.cn 
Beihang University</p>
<p>Zhongguancun Laboratory 5 State Key Laboratory of Intelligent Manufacturing Systems Technology
Beijing</p>
<p>Conghui He heconghui@pjlab.org 
Shanghai Artificial Intelligence Laboratory</p>
<p>LARGE LANGUAGE MODELS MEET SYMBOLIC PROVERS FOR LOGICAL REASONING EVALUATION
2 Mar 20256D78C2E82116359AF7E93DB51B86A1C4arXiv:2502.06563v2[cs.CL]
First-order logic (FOL) reasoning, which involves sequential deduction, is pivotal for intelligent systems and serves as a valuable task for evaluating reasoning capabilities, particularly in chain-of-thought (CoT) contexts.Existing benchmarks often rely on extensive human annotation or handcrafted templates, making it difficult to achieve the necessary complexity, scalability, and diversity for robust evaluation.To address these limitations, we propose a novel framework called ProverGen that synergizes the generative strengths of Large Language Models (LLMs) with the rigor and precision of symbolic provers, enabling the creation of a scalable, diverse, and high-quality FOL reasoning dataset, ProverQA.ProverQA is also distinguished by its inclusion of accessible and logically coherent intermediate reasoning steps for each problem.Our evaluation shows that state-of-the-art LLMs struggle to solve ProverQA problems, even with CoT prompting, highlighting the dataset's challenging nature.We also finetune Llama3.1-8B-Instruct on a separate training set generated by our framework.The finetuned model demonstrates consistent improvements on both in-distribution and out-of-distribution test sets, suggesting the value of our proposed data generation framework.Code available at: https://github.com/opendatalab/ProverGen</p>
<p>INTRODUCTION</p>
<p>First-order logic (FOL) reasoning, which involves sequential deduction and the application of facts and logical rules to derive conclusions or make decisions, is a fundamental aspect of human intelligence (Saparov &amp; He, 2022;Pan et al., 2023).The complexity of performing multi-step reasoning while maintaining logical coherence across these steps makes FOL reasoning an ideal testbed for evaluating the reasoning capabilities of Large Language Models (LLMs), particularly in scenarios such as Chain-of-Thought (CoT) prompting (Wei et al., 2022) and planning (Wang et al., 2024;Valmeekam et al., 2022).An effective benchmark for FOL reasoning should satisfy several key criteria: (1) Scalability, enabling expansion in both volume and complexity with minimal manual intervention; (2) Natural and Diverse Language, capturing a wide range of natural language expressions to reflect real-world linguistic variability; (3) Symbolic Representations, providing formal symbolic structures that can be validated through automated symbolic provers, ensuring dataset integrity and supporting downstream tasks like NL-FOL translation and tool based logic problem solving (Pan et al., 2023;Olausson et al., 2023); and (4) Faithful Reasoning Chains, where each instance includes intermediate reasoning steps clearly articulated in both symbolic and natural language formats, promoting transparency and facilitating further model training.</p>
<p>However, existing FOL reasoning datasets only partially fulfill these requirements (see Table 1 for more details).Template-based datasets such as ProntoQA (Saparov &amp; He, 2022;Saparov et al., 2024), ProofWriter (Tafjord et al., 2021), and RuleTaker (Clark et al., 2021) are scalable but often lack diversity and rely on overly simplistic rules.On the contrary, manually curated datasets like FOLIO (Han et al., 2022) offer rich diversity in natural language expressions and more complex logic rules, but they are limited in size due to the extensive human effort needed for creation.Additionally, data contamination is also a significant issue in manually annotated datasets, as it is difficult to update them frequently.This limitation can lead to biased evaluations and hinder true generalization due to potential data leakage.Another common limitation across previous datasets is the lack of well-defined and easily accessible reasoning chains.FOLIO, for instance, does not provide any reasoning chains, while ProofWriter and RuleTaker include reasoning chains, but they are not readily accessible and require users to implement additional code to extract them for each problem.
✓ ✗ ✗ ✓ ProofWriter Synthetic ✓ ✗ ✗ ✓ ProntoQA Synthetic ✓ ✗ ✓ ✓ ProntoQA-OOD Synthetic ✓ ✗ ✓ ✓ LogicNLI Synthetic ✓ ✗ ✗ ✗ FOLIO Manual ✗ ✓ ✓ ✗ ProverQA Synthetic ✓ ✓ ✓ ✓
To address these gaps, we propose a novel framework that leverages the strengths of LLMs and symbolic provers to generate an extensible, diverse, and high-quality dataset for FOL reasoning.While LLMs have been increasingly utilized for synthesizing datasets across various tasks (Taori et al., 2023;Acharya et al., 2023;Gajjar &amp; Shah, 2024), they fall short in producing reliable reasoning, often lacking faithfulness and transparency (Golovneva et al., 2022;Ribeiro et al., 2022;Lyu et al., 2023), particularly when it comes to long reasoning chains.On the other hand, symbolic provers are known for their reliability and precision in executing complex logical inferences.By combining the generative capabilities of LLMs with the rigor of symbolic provers, our framework offers a robust solution for producing FOL reasoning data that ensures both diversity and logical soundness.It also mitigates the data contamination challenge by enabling the generation of new datasets using diverse models and controlled complexity, ensuring that the datasets remain fresh and uncontaminated.Our framework follows a structured three-step pipeline: (1) we first use an LLM to generate a unique background story for each problem based on a given subject and a characteristic keyword;</p>
<p>(2) we employ a symbolic prover (i.e., Prover9) to construct reasoning trees and compute the truth values of relevant facts; and (3) we utilize an LLM to translate these logical expressions into natural language statements.</p>
<p>We use Llama3.1-70B-Instructwithin our proposed generation framework to create a new FOL reasoning benchmark, ProverQA, consisting of 1,500 instances across three difficulty levels: easy, medium, and hard.We evaluate ProverQA on several state-of-the-art (SOTA) LLMs and find that even with CoT prompting, models barely exceed 50% accuracy on the hard subset, highlighting the significant challenge posed by ProverQA.ProverQA's difficulty settings are carefully designed to ensure appropriate complexity, avoiding both performance saturation and uninformative underperformance (e.g., random guessing), enabling accurate evaluation of both powerful and smaller language models' reasoning abilities.To further explore the potential of our proposed generation framework, we finetune Llama3.1-8B-Instruct on a separately generated ProverQA training set and observe substantial performance gains on in-distribution (ID) evaluations.Remarkably, this finetuned model also shows consistent improvements on out-of-distribution (OOD) test sets, significantly outperforming the same model finetuned on other existing FOL training sets.</p>
<p>RELATED WORKS</p>
<p>Logical Reasoning Datasets Logical reasoning is a fundamental ability of intelligent systems and researchers have proposed a number of datasets to evaluate logical reasoning abilities of language models.These datasets can be divided into two groups: manually crafted ones and automatically generated ones.Human-crafted datasets are typically sourced from existing examinations (Yu et al., 2019;Liu et al., 2021;2023;Zhong et al., 2021) or directly written by domain experts (Hu et al., 2023;Han et al., 2022).Although they often exhibit higher quality and complexity, they also suffer from limited quantity and high creation costs.Automatically generated datasets are often created using rules and predefined natural language templates (Sinha et al., 2019;Hahn et al., 2021;Clark et al., 2021;Tafjord et al., 2021;Tian et al., 2021;Saparov &amp; He, 2022;Saparov et al., 2024;Sun et al., 2021).While these datasets enable automatic problem generation, they often lack linguistic diversity and are constrained by rigid, monotonous rules.None of them cover all aspects of firstorder logic, and they cannot be used to generate problems with more complex rules.</p>
<p>Symbolic Prover Augmented LLMs Symbolic provers are computer programs used to generate proofs and solutions of mathematical theorems automatically.In logical reasoning problems, symbolic provers, such as SymPy (Meurer et al., 2017), Z3 (De Moura &amp; Bjørner, 2008), Lean (De Moura et al., 2015), Pyke (Frederiksen, 2008) andProver9 (McCune, 2005), stand out with faithful solutions and traceable intermediate processes.Recent works (Pan et al., 2023;Olausson et al., 2023;Ye et al., 2024;Zhang et al., 2024) combine LLMs and symbolic provers to solve logic reasoning problems.They use LLMs to translate problems from natural language into symbolic representations and then solve that problem with a symbolic prover.In contrast to previous works that utilize symbolic provers solely as problem solvers, our approach integrates symbolic provers into the generation process of first-order logic (FOL) problems.This integration allows us to bypass the intricate interrelations and couplings among logical relations, enabling the generation of diverse rules and their correct chaining to form comprehensive problems.Background Story Sawyer was a charismatic and elegant dancer who ruled the ballroom scene in 19thcentury Paris.Born into a family of modest means, he discovered his passion for dance at a yound age and worked tirelessly to hone his skills.Sawyer's big break came when he was invited to perform at the prestigious Opéra Garnier.His dashing</p>
<p>METHODOLOGY
•••••••• LLMs Universal Rule
Everyone is either a musician or a poet, but not both.</p>
<p>LLMs Specific Rule</p>
<p>Sawyer is either a musician or a poet, but not both.).Given a subject, Sawyer, and a seed keyword, elegant, LLMs generate a background story to establish context and ensure linguistic diversity.(b) Logic Skeleton Generation (Section 3.2.2).A top-down approach is used to generate the logic skeleton, forming the reasoning tree for the FOL problem.This involves two iterative steps: expression sampling and truth value calculation using the Prover9 prover.Distractions are also incorporated to test the robustness of model's reasoning capabilities.(c) Statement Translation (Section 3.2.3).LLMs translate the facts and rules from the logic skeleton into natural language, guided by the previously generated background story.Each rule is translated into a universal version and a specific version.The universal version is preferred if it does not contradict with common sense, otherwise the specific one is selected.</p>
<p>TASK FORMULATION</p>
<p>Given a set of premises P, consisting of facts F and rules R, denoted as P = (F, R), the goal of FOL reasoning is to determine whether a given goal G (which can be a fact or a rule) is True, False, or Uncertain.Specifically, F = {f 1 , f 2 , • • • , f m } represents a set of facts, where each fact f i (1 ≤ i ≤ m) is a declarative statement describing attributes or characteristics of subjects.For example, Sawyer has good dance skills is a fact that specifies the attribute (has good dance skills) of the subject (Sawyer).Similarly, R = {r 1 , r 2 , • • • , r n } comprises a set of rules, where each rule r j (1 ≤ j ≤ n) defines relationships between multiple facts using seven FOL symbols: conjunction (∧), disjunction (∨), negation (¬), implication (→), equivalence (≡) 1 , universal quantifier (∀), and existential quantifier (∃).For example, the rule If Sawyer has good dance skills and has charisma, then he is a successful dancer illustrates a relationship between two facts and their conclusion.It is worth noting that some existing datasets, such as ProntoQA (Saparov &amp; He, 2022) and ProofWriter (Tafjord et al., 2021), do not fully encompass all seven FOL relationships, whereas our proposed framework ensures complete coverage.</p>
<p>LLM &amp; PROVER-EMPOWERED GENERATION FRAMEWORK</p>
<p>Our proposed generation framework, which integrates the generative capabilities of LLMs with the precision of a symbolic prover, generates FOL reasoning problems through a structured threestage process, as illustrated in Figure 1.First, a background story is created for each problem using sampled subject names and keywords to ensure linguistic diversity.In the next stage, the logic skeleton generation employs a symbolic prover (i.e., Prover9) to construct reasoning trees and determine the truth values of the relevant facts.Finally, the statement translation phase uses LLMs to convert these logical expressions, including both premises and goals, into natural language, ensuring clarity and coherence.An example of a generated FOL problem is shown in Figure 2.</p>
<p>BACKGROUND STORY GENERATION</p>
<p>Achieving naturalness in language expression is crucial for creating a plausible FOL dataset.Existing approaches often rely on extensive human annotation or rigidly structured templates.For instance, ProntoQA (Saparov &amp; He, 2022) and ProofWriter (Tafjord et al., 2021) employ handcrafted templates, which result in datasets lacking linguistic diversity and naturalness. 2  In contrast, FOLIO (Han et al., 2022) retrieves documents from Wikipedia and manually extracts facts and rules.However, this process requires substantial human effort and is susceptible to syntactic typos and reasoning errors introduced during annotation, as observed in FOLIO.Additionally, models may exploit shortcuts by leveraging pre-trained knowledge to predict answers directly, bypassing rigorous logical reasoning (Qi et al., 2023;Geirhos et al., 2020), since the facts and rules in the dataset are often derived from the real-life.</p>
<p>To efficiently synthesize plausible natural FOL problems, we leverage LLMs to generate a unique background story for the subject of each problem, which serves to instantiate the statements (both premises and goals) within the problem.However, simply prompting LLMs to generate stories without any guidance can be suboptimal, as they struggle to produce diversified content with high quality, as noted by Jentzsch &amp; Kersting (2023); Sudalairaj et al. (2024).To mitigate these issues, we prepare a characteristic keyword for the subject within the problem to guide the generation.Specifically, the process begins by assigning a unique name to the subject, which is randomly sampled from publicly accessible datasets of human names 3 and pet names 4 .Next, we sample a keyword from WordNet (Miller, 1995) to serve as a defining characteristic or attribute of the subject, ensuring semantic diversity in the generated content.These elements, the name and the characteristic keyword, are then integrated into the prompt to guide LLMs to generate a contextually rich and varied background story.This controlled generation helps promote diversity in the dataset.The detailed prompt used in our experiments is outlined in Appendix A.</p>
<p>LOGIC SKELETON GENERATION</p>
<p>Logical soundness is a fundamental requirement in constructing a robust FOL reasoning dataset.While LLMs are highly proficient at generating natural language, they often fall short in maintaining logical coherence, particularly when dealing with intricate, multi-step reasoning processes.To address this limitation, we introduce a novel approach that integrates symbolic prover into the data generation pipeline, allowing for the construction of logically rigorous FOL reasoning problems.</p>
<p>In our data generation framework, we start with constructing a logic skeleton for each problem, represented as a reasoning tree with the ultimate goal at its root (as shown in Figure 1(b)).Previous works typically generate reasoning problems in a bottom-up approach by starting with premises and building toward conclusions.We argue that this approach could be flawed as it can often result in disconnected or contradictory premises, making it difficult to ensure that all facts are logically coherent with each other.For instance, if the problem includes facts such as f 1 : Sawyer is a poet, f 2 : Sawyer comes from Mars, and f 3 : Sawyer is a large, fluffy Maine Coon cat, then integrating these elements into a consistent reasoning chain is challenging.Therefore, our framework adopts a novel top-down approach, which begins with the goal and works backward to derive the relevant premises and rules needed to logically support that goal.This method ensures that each fact and rule contributes directly to the reasoning process, improving both the coherence and logical soundness of the generated problems.Moreover, this top-down structure simplifies the problem of logical consistency by allowing LLMs to focus on facts that are directly relevant to the conclusion, without needing to reconcile them with unrelated information.</p>
<p>More specifically, as shown in Figure 1(b), we begin by setting the truth value of the goal, denoted as f 3 = True, and treat this as the root node of the reasoning tree.Next, we sample a logical expression for this node, (f 1 ∧ f 2 ) → f 3 , which establishes a relationship between the goal f 3 and the premises f 1 and f 2 .At this stage, we use the symbolic prover Prover9 to calculate the truth values of f 1 and f 2 , ensuring that the premises logically support the goal.Once f 1 and f 2 are assigned truth values, they become the current nodes in the reasoning tree.For each of these nodes, we continue the process by sampling new logical expressions.For instance, for f 1 = True, we sample a new rule f 5 → (f 1 ⊕ f 4 ), where f 5 and f 4 are additional facts to be evaluated.We then calculate the truth values for these new premises using Prover9, and they become the next current nodes in the tree.This iterative process is repeated for each current node, expanding the reasoning tree step by step.</p>
<p>The process continues until we have generated the desired number of reasoning steps, ensuring that each layer of the reasoning tree is logically consistent and fully developed, forming a coherent and complete logical structure for the problem.</p>
<p>To further assess the robustness of models in handling FOL reasoning, we introduce distractions into the generated problems.Distractions, denoted as d, are an additional set of facts and rules not essential to solving the problem.The inclusion of distractions forces models to distinguish between relevant and distracting information.We implement two types of distractions in our framework.The first type follows existing works (Tafjord et al., 2021;Saparov &amp; He, 2022), where we randomly introduce facts or rules that are related to the problem's domain but use a different subject name (see Figure 2).The second, more complex type of distraction leverages the power of symbolic provers to introduce facts and rules that are semantically or logically related to the reasoning chain, but which lead to an Uncertain judgment.This type of distraction requires models to reason over logically relevant information that does not directly impact the truth value of the core facts needed to reach the final conclusion.For instance, suppose f 1 is a critical fact in the reasoning tree, and we introduce two distractions, d 1 = True and d 2 = Uncertain, with the rule
(d 1 ⊕ d 2 ) → f 1 .
Here, the truth value of f 1 remains Uncertain because the rule involves incomplete information.Models must reason through these distractions, yet ultimately recognize that they do not alter the critical facts that drive the reasoning chain forward.</p>
<p>STATEMENT TRANSLATION</p>
<p>After generating the logical skeleton of a problem, we use LLMs to translate each rule and fact into natural language within the framework of the background story generated in Section 3.2.1.As illustrated in Figure 1(c), we first prompt LLMs to instantiate placeholders (such as f 6 and f 4 ) in the logic expressions with appropriate predicates, like poet and musician.Once the placeholders are instantiated, the logic expressions are translated into natural language.To maintain consistency throughout the translation process, we provide the previously translated facts and rules as references in the prompt for subsequent translations (details of the translation prompt can be found in Appendix B).</p>
<p>To ensure the plausibility and soundness of the rules, we instruct LLMs to generate two versions of each rule: a universal and a specific version.The universal rule refers to statements that apply to everyone or everything, such as Everyone is either a musician or a poet, but not both.The specific rule, on the other hand, applies to an individual subject, such as Sawyer is either a musician or a poet, but not both.LLMs then assess whether the universal rule aligns with common sense.If it does, we use the universal version; if not, we opt for the specific rule.For instance, the universal rule in Figure 1(c) does not align with common sense (as there are more professions than just musicians and poets), so we choose the specific version of the rule.The final stage of the proposed framework involves assembling the facts, rules, and distractions generated in previous stages into a complete FOL problem.An example of the constructed problem with its corresponding reasoning steps is provided in Figure 2.</p>
<p>THE PROVERQA BENCHMARK</p>
<p>We utilize Llama3.1-70B-Instructthroughout our proposed generation framework to develop a new FOL reasoning benchmark, ProverQA, comprising 1,500 instances.The benchmark is evenly divided into three subsets based on the length of the reasoning chains: easy (1-2 steps), medium (3-5 steps), and hard (6-9 steps).In addition to the number of reasoning steps, the hard subset is characterized by more diverse reasoning directions.Reasoning in the easy and medium subsets typically follows a straightforward left-to-right progression.For example, given f 1 = True and f 1 → f 2 , the model can deduce that f 2 = True.In contrast, the hard subset involves more complex patterns, requiring models to infer f 1 = False given f 2 = False and f 1 → f 2 , thus demonstrating a more intricate reasoning process.Each problem in the benchmark contains two types of distractions (see Section 3.2.2),where the number of distractions for each type is randomly sampled.Moreover, to further challenge the robustness of models, we shuffle the order of the premises within each prob- ProverQA as a New Challenge Overall, model performance on ProverQA declines as problem difficulty increases, with the lowest scores observed on the hard subset across all existing datasets as seen in Table 2. Specifically, strong models such as GPT-4o, Claude-3.5-Sonnetand Mistral-Large-Instruct barely exceed 50% accuracy on the hard subset, even with CoT prompting.This result indicates that even the most advanced LLMs perform poorly on sufficiently complex problems with long reasoning chains.In contrast, simpler benchmarks like ProntoQA appear to be nearly solved by models like GPT-4o and Claude-3.5-Sonnet.However, smaller models still struggle to achieve satisfactory performance, even on the easy subset of ProverQA, suggesting that these models are not yet equipped to handle FOL reasoning effectively.</p>
<p>Comparison of prompting strategies Compared with Standard prompting, CoT prompting notably enhances the performance of models on easy datasets but does not yield a similar improvement on harder datasets, as presented in the last row of Table 2.The improvement on the easy dataset is more pronounced because performance on easy problems is probably limited by the CoT capability, whereas the bottleneck for hard problems may involve other factors beyond CoT capability.We have also conducted experiments using 5-shot examples; the results can be found in Appendix G.</p>
<p>ABLATION STUDY ON DISTRACTIONS AND PREMISE ORDER</p>
<p>By default, the ProverQA benchmark includes two types of distractions and shuffles the order of premises within each problem (see Section 3.3).To assess how these two factors influence the evaluation results of LLMs, we conduct ablation studies using GPT-4o with CoT prompting in this section.Specifically, in the first setting, we remove all distractions from the problems to observe how the absence of irrelevant information affects the model's reasoning performance.Results in Table 3 exhibit that the model performs better in this situation.In the second setting, we additionally reorder the premises to match the logical progression of the underlying reasoning chain.This turns out that the model performs even better when premises are presented in a coherent sequence.Consequently, these findings suggest both factors are valuable for constructing a robust FOL reasoning benchmark for LLMs.</p>
<p>EVALUATION RESULTS</p>
<p>In-Distribution (ID) Performance As shown in Table 4, finetuning on the ProverQA training set results in a substantial performance improvement across all subsets of the ProverQA benchmark.Specifically, the model's performance increases by 21.4%, 44.0%, and 34.6% in the easy, medium, and hard subsets of the ProverQA benchmark, respectively.Similarly, fine-tuning on ProofWriter and FOLIO leads to performance gains on their corresponding test sets, as expected.</p>
<p>Out-of-Distribution (OOD) Performance While the ID performance of finetuned models can exhibit the positive effects of specialized finetuning, OOD performance provides more insights as it demonstrates the generalizability of the finetuned models.As shown in Table 4, finetuning on ProverQA yields a performance increase on OOD test sets (ProntoQA, ProofWriter, FOLIO) by about 5% to 8.8%.Finetuning on ProverQA obtains the best OOD improvement on average with a large leading margin compared to models finetuned on ProofWriter and FOLIO, indicating the limited utility of these prior datasets for promoting the reasoning capabilities of LLMs.The model finetuned on ProofWriter achieves the highest performance on ProntoQA.This anomaly could be attributed to the high similarity between ProntoQA and ProofWriter: Both datasets utilize predefined templates for generating logic and corresponding natural language.68.57+5.0 +7.5</p>
<p>CASE STUDY</p>
<p>We show examples from the FOLIO test set in Table 5 to demonstrate how finetuning on ProverQA improves model performance.Before finetuning, models fail to solve the problems because of various reasons, such as using a common sense shortcut, being unfaithful to the context, and being unable to extend the reasoning chain.After training on ProverQA, models are able to derive a valid reasoning chain up to the goal based on the context, and the reasoning process rigidly adheres to the facts and rules.</p>
<p>CONCLUSIONS</p>
<p>In this work, we presented a novel framework called ProverGen for automatically generating FOL reasoning datasets by leveraging the strengths of LLMs in producing diverse natural language expressions and the rigor of symbolic provers in ensuring logical consistency.Using this framework, we developed ProverQA, a high-quality FOL reasoning benchmark that is both extensible and diverse.ProverQA encompasses complex rules, diverse natural language expressions, and faithful intermediate reasoning chains, providing a comprehensive evaluation benchmark for FOL reasoning.Our evaluation shows that SOTA LLMs struggle to solve ProverQA problems, even with CoT prompting, demonstrating the unique challenge posed by our dataset.We also finetuned Llama3.1-8B-Instruct on a separately generated training set and observed consistent performance improvements on both in-distribution and out-of-distribution test sets.We hope that our generation framework and the resulting dataset could contribute to the development of more robust and capable models in this area.</p>
<p>ETHICS STATEMENT</p>
<p>The keywords and names used in this paper are sourced entirely from public repositories and databases.Specifically, the names were collected from two public GitHub repositories under MIT licenses, and the keywords were obtained from WordNet.All sources have licenses permitting free usage, and our work complies with these terms.Our does not involve human participants, nor does it present any immediately harmful insights.For the release, we plan to open source the code for creating the benchmark and the dataset we created for the experiments in this paper.</p>
<p>REPRODUCIBILITY STATEMENT</p>
<p>For our submission, we have uploaded the entirety of the source code as a zipped file that has been properly anonymized.The source code contains inline documentation that details purpose and usage of different parts of the codebase.In addition, we also include the full set of model's responses and the evaluation script.As discussed in the ethics statement, we plan to more formally release ProverQA to the public as an open source repository with thorough details that describes the framewrok, outlines the code, and details its usage.</p>
<p>User: keyword: loyal name: Bella Assistant: { "category": "dog", "story": "Bella was a golden retriever with a heart as pure as gold.From the moment she was adopted by the Thompson family, she became their steadfast guardian and companion.</p>
<p>Bella had an uncanny ability to sense when someone in the family needed comfort, always there with a nuzzle or a wagging tail.Her loyalty was most evident when young Timmy got lost in the woods behind their house.Bella tracked his scent tirelessly, refusing to rest until she found him.She stayed by his side through the cold night, keeping him warm and safe until help arrived.Bella's unwavering loyalty and love made her not just a pet, but a cherished member of the family."}</p>
<p>User</p>
<p>System:</p>
<p>You will be provided a logic expression, a reference of the existed predicate in the logic expression, and a background story.Your task is to replace the placeholders in the logic expression with appropriate predicates (no more than 5 words) so that the provided logic expression represent a real world common sense rule.Do not use 'not'.Do not use the words in the forbidden list.The words that are similar in meaning to the words in the forbidden list are also not allowed.</p>
<p>Your answer is not required to closely connected to the background story.You can use any predicates you like as long as their length is less than 5.</p>
<p>Your answer should be in JSON format with the provided keys.</p>
<p>User: background story:</p>
<p>Moriarty was a large, fluffy Maine Coon cat with a surprisingly warm and gentle nature.Despite his intimidating size and the mysterious name inspired by a famous literary villain, Moriarty was the epitome of warmth and comfort.He lived in a cozy bookstore run by Mrs. Lindon, an elderly woman who had taken him in as a stray.Moriarty had a special talent for sensing when a customer needed a bit of extra warmth, often curling up in their laps as they read.His thick fur and gentle purring provided solace to many, especially during the cold winter months.Moriarty's presence turned the bookstore into a beloved local haven, where people came not just for the books, but for the warm, comforting presence of their favorite feline friend.reference: is_fluffy is_fluffy(Moriarty) → (warm_and_gentle(Moriarty) ⊕ is_calming(Moriarty)): If Moriarty is fluffy, then he is either warm and gentle or calming, but not both.forbidden list: ['warm_and_gentle', 'provide_comfort', 'attract_customers', 'is_friendly', 'brings_joy', 'is_fluffy', 'is_calming'] logic expression: ∀x (F7(x) → (is_fluffy(x) ∧ F8(x))) keys: ['F7', 'F8', 'universal_rule', 'specific_rule'] Note: x belongs to cat.</p>
<p>Assistant:</p>
<p>{ "F7": "playful", "F8": "love_attention", "universal_rule": "All playful cats are fluffy and love attention.","specific_rule": "If Moriarty is playful, then he is both fluffy and loves attention."} ----------------------Another Two Examples  ----------------------User: background story: Colt was a brilliant botanist with a passion for rare plant species.He spent years studying the Avicenniaceae family, specifically the mangrove plants, which thrive in the harshest of coastal environments.Colt's groundbreaking research on these unique plants led to the development of innovative solutions for coastal conservation.He discovered that the Avicennia species possessed remarkable adaptability, allowing them to survive in areas with high salinity and limited oxygen.Colt's findings opened doors to new methods for restoring damaged ecosystems and protecting endangered species.As a leading expert in his field, Colt collaborated with international organizations to implement his research, helping to preserve the delicate balance of our planet's mangrove ecosystems.His dedication to the Avicenniaceae family left a lasting impact on the scientific community.</p>
<p>reference: contribute_to_environmental_conservation dedicated_to_research(Colt) → (make_groundbreaking_discoveries(Colt) ∧ contribute_to_environmental_conservation(Colt)): If Colt is dedicated to research, then he will make groundbreaking discoveries and contribute to environmental conservation.forbidden list: ['dedicated_to_research', 'make_groundbreaking_discoveries', 'contribute_to_environmental_conservation', 'pursue_higher_education', 'become_leading_expert'] logic expression: ∀x (F5(x) → (contribute_to_environmental_conservation(x) ∧ F6(x))) keys: ['F5', 'F6', 'universal_rule', 'specific_rule'] Note: x belongs to human.</p>
<p>C POTENTIAL SHORTCUT PROBLEM</p>
<p>To further investigate the potential use of shortcuts, we conducted an experiment by removing universal rules from 60 randomly selected instances in ProverQA.We evaluated GPT-4 and Llama-3.1-70B-Instruct on this corrupted dataset.If the models were relying heavily on inherent knowledge as shortcuts, their performance would remain roughly unaffected despite the absence of universal rules.However, the results in Table 6 showed a significant drop in performance, indicating that these models do not heavily rely on background knowledge to solve the problems.For every elephant, the elephant is either domesticated or wild, but not both.Anakin is a domesticated elephant.</p>
<p>Question: Based on the above information, is the following statement true, false, or uncertain?Anakin is not a wild elephant.</p>
<p>Options: A) True B) False C) Uncertain</p>
<p>The correct option is: A</p>
<p>--------------------------------------------Proofs--------------------------------------------------------------Step1------------------</p>
<p>Figure 1 :
1
Figure 1: An overview of our ProverGen framework.(a) Background Story Generation (Section 3.2.1).Given a subject, Sawyer, and a seed keyword, elegant, LLMs generate a background story to establish context and ensure linguistic diversity.(b) Logic Skeleton Generation (Section 3.2.2).A top-down approach is used to generate the logic skeleton, forming the reasoning tree for the FOL problem.This involves two iterative steps: expression sampling and truth value calculation using the Prover9 prover.Distractions are also incorporated to test the robustness of model's reasoning capabilities.(c) Statement Translation (Section 3.2.3).LLMs translate the facts and rules from the logic skeleton into natural language, guided by the previously generated background story.Each rule is translated into a universal version and a specific version.The universal version is preferred if it does not contradict with common sense, otherwise the specific one is selected.</p>
<p>) → (good_dance_skills(Sawyer) ⊕ musician(Sawyer)) If Sawyer is a performer, then he either has good dance skills or is a musician.(f5 → (f1 ⊕ f4)) poet(Sawyer) ⊕ musician(Sawyer) Sawyer is either a musician or a poet, but not both.(f4 ⊕ f6) (good_dance_skills(Sawyer) ∧ charisma(Sawyer)) → successful_dancer(Sawyer) If Sawyer has good dance skills and has charisma, then he is a successful dancer.((f1 ∧ f2) → f3) Sawyer has charisma.(f2) charisma(Sawyer) Sawyer is a poet.(f6) poetsomeone either sings opera or plays an instrument, then they are a musician.((d3 ⊕ d4) → f4) ∀x (sings_opera(x) ⊕ plays_instrument(x)) → musician(x) Everyone who either practices waltz or masters tango has good dance skills.((d1 ⊕ d2) → f1) ∀x (practices_waltz(x) ⊕ masters_tango(x)) → good_dance_skills(x) is a poet.(f6) Conclusion: Sawyer is not a musician.(¬f4) Rule: Sawyer is either a musician or a poet, but not both.(f4 ⊕ f6) Fact: Sawyer is a performer.(f5) Fact: Sawyer is not a musician.(¬f4) Conclusion: Sawyer has good dance skills.(f1) Rule: If Sawyer is a performer, then he either has good dance skills or is a musician, but not both.(f5 → (f1 ⊕ f4)) Fact: Sawyer has good dance skills.(f1) Fact: Sawyer has chrisma.(f2) Sawyer is a successful dancer.(f3) Rule: If Sawyer has good dance skills and has charisma, then he is a successful dancer.((f1 ∧ f2) → f3) successful_dancer(Sawyer) Sawyer is a successful dancer.(f3) masters_tango(Sawyer) Sawyer masters tango.(d2) sings_opera(Sawyer) Sawyer sings opera.(d3) (a) A Generated FOL Problem (b) Reasoning Process</p>
<dl>
<dt>Figure 2 :</dt>
<dt>2</dt>
<dt>Figure 2: A generated FOL problem (a) with its corresponding reasoning process (b).</dt>
<dd>
<p>keyword: [KEYWORD] name: [NAME] B PROMPTS FOR TRANSLATING LOGIC EXPRESSIONS INTO NATURAL LANGUAGE We utilized three examples for each type of logical expression to guide the LLMs in translating them into natural language.An example of the prompt we used in our experiments is shown below.</p>
</dd>
</dl>
<p>Table 1 :
1
Comparison of existing FOL reasoning datasets with ProverQA.
DatasetCreation MethodScalabilityNatural &amp; Diverse LanguageSymbolic RepresentationsFaithful Reasoning ChainsRuleTakerSynthetic</p>
<p>Table 2 :
2
lem, ensuring that models cannot exploit fixed patterns or sequences to derive conclusions.Detailed examples of the ProverQA are provided in Appendix D. The performance of LLMs on ProverQA and existing FOL reasoning datasets.†: The average performance improvement of CoT prompting over Standard prompting across models on each evaluation set, which indicates the average gain CoT prompting brings on each set.
4 EVALUATION ON THE PROVERQA4.1 EXPERIMENTAL SETUPWe evaluate current SOTA proprietary LLMs (GPT-4o (OpenAI, 2024) andClaude3.5 (Anthropic, 2024)) 5 and open-source LLMs (Mistral-7B-Instruct,Mistral-Large-Instruct, Mixtral-8x22B-Instruct-v0.1 (Jiang et al., 2023;2024) and Llama3.1-8B-Chat, Llama3.1-70B-Chat (Dubey et al., 2024)) on theProverQA. We utilize two prompting strategies: Standard and CoT. Standard prompting uses 2-shotin-context learning to prompt LLMs to answer questions directly, whereas CoT prompting employs2-shot examples to instruct the model to solve questions step by step. Examples of both promptingstrategies can be found in Appendix E.4.2 MAIN RESULTSProverQAProntoQA ProofWriter FOLIOAvgEasy Medium HardStandard PromptingGPT-4o87.2068.6046.2091.8056.3367.86 69.66Claude-3.5-Sonnet85.0068.2042.8088.6055.0077.85 69.58Llama3.1-8B-Instruct46.6043.0039.0050.4043.8054.29 46.18Llama3.1-70B-Instruct 82.0064.2047.6080.6050.3367.86 65.28Mistral-7B-Instruct56.8046.8037.2050.0042.3354.29 47.90Mistral-Large-Instruct84.6069.2049.6071.0060.3377.14 68.65Mixtral-8x22B-Instruct 75.4057.4039.0065.2040.1774.29 58.58CoT PromptingGPT-4o94.2079.4050.00100.0067.3372.14 77.18Claude-3.5-Sonnet95.2083.6056.4099.2076.3380.71 81.91Llama3.1-8B-Instruct75.6046.6033.6079.6056.8363.57 59.30Llama3.1-70B-Instruct 90.4073.2046.8095.4071.1774.29 75.21Mistral-7B-Instruct72.0051.0041.8061.2046.0063.58 55.93Mistral-Large-Instruct92.6075.8052.2098.6073.5083.57 79.38Mixtral-8x22B-Instruct 87.6066.8047.6079.6057.6773.57 68.81Avg ∆  †12.868.433.8616.5714.365.4110.25</p>
<p>Table 3 :
3
Performance of GPT-4o on different variants of ProverQA using CoT prompting.The first row represents the default ProverQA setting, which features distractions and shuffled premises.
DistractionsShuffled PremisesEasyMediumHard✓✓94.2079.4050.00✗✓94.4082.2057.40✗✗95.6087.4062.405 FINETUNING WITH PROVERQA TRAIN DATA
Beyond its role as an evaluation benchmark, we also investigate whether the training data synthesized by our proposed framework can improve a model's FOL reasoning capabilities, particularly in out-of-distribution (OOD) scenarios.To this end, we generate a dedicated training set and finetune an open-source model, Llama3.1-8B-Instruct.For comparison, we also finetune the same model on existing FOL reasoning datasets, such as ProofWriter and FOLIO.We exclude ProntoQA from this experiment due to the absence of a publicly available training set.Detailed training configurations are provided in Appendix F.</p>
<p>Table 4 :
4
Performance of Llama3.1-8B-Instructfinetuned with different FOL training sets.Green cells indicate in-distribution (ID) evaluation results, while red cells represent out-ofdistribution (OOD) evaluation results.The change of performance after finetuning is shown in small fonts along with the evaluation results.†: the average change of performance across all OOD evaluation sets for each training configuration.ProofWriter 44.60 −31.0 55.00 +8.4 47.20 +13.6 92.00 +12.4 71.67 +14.8 50.00 −13.6 -2.0 FOLIO 53.20 −22.4 44.80 −1.8 31.00 −2.6 63.40 −16.2 42.83 −14.0 70.00 +6.4 -11.4ProverQA 97.00 +21.4 90.60 +44.0 68.20 +34.6 88.40 +8.8 65.67+8.8
FOL Training SetEasyProverQA MediumHardProntoQA ProofWriter FOLIOOOD Avg ∆  †Llama3.1-8B-Instruct-75.6046.6033.6079.6056.8363.57-Finetuned Llama3.1-8B-Instruct</p>
<p>Table 5 :
5
Typical errors made by models and how finetuning correct them.Reasoning chains before and after finetuning are both displayed for comparison.
Error TypeContextBefore FinetuningAfter FinetuningAll students are humans. If someoneA person either studies or teaches.No young per-Jerry neither teaches nor is a manager.studies, then he/she is a student. A personson teaches. Therefore, a young person studies. IfA person either studies or teaches.either studies or teaches. No youngsomeone studies, then they are a student. All stu-Jerry studies. Jerry studies. If some-person teaches. Jerry neither teaches nordents are humans. Therefore, a young person is aone studies, then he/she is a student.Common sense shortcutis a manager. Rose is young or a student. Question: Jerry is not young.human. People are either young or old. Therefore, a young person is not old. Jerry neither teaches nor is a manager. Therefore, Jerry studies. Since JerryJerry is a student. Jerry is a student. All students are humans. Jerry is a human. According to the context andis a student and a human, Jerry is not old. There-the conclusions that have already beenfore, Jerry is young.drawn, we can deduce that it is uncer-Answer: false.tain that Jerry is not young.Answer: uncertain.Beijing is the capital of the People's Re-Beijing hosted the 2008 Summer Olympics andBeijing hosted the 2008 Summerpublic of China. Beijing is the world's2008 Summer Paralympics Games. However, thereOlympics and 2008 Summermost populous national capital city. Bei-is no information about Beijing hosting a WinterParalympics Games.Beijing hasjing is located in Northern China. Bei-Olympics. Therefore, the statement is uncertainhosted both the Summer and Winterjing hosted the 2008 Summer Olympicsbecause it is not explicitly stated that BeijingOlympics, along with the SummerUnfaithful toand 2008 Summer Paralympics Games.has hosted a Winter Olympics, but it is also notand Winter Paralympics.Beijingthe contextBeijing has hosted both the Summer and Winter Olympics, along with the Summerexplicitly stated that it has not. Answer: uncertain.has hosted both the 2008 Summer Olympics and a winter olympics.and Winter Paralympics.Therefore, it is true that BeijingQuestion: Beijing has hosted both thehas hosted both the 2008 Summer2008 Summer Olympics and a winterOlympics and a winter olympics.olympics.Answer: true.Robert Lewandowski is a striker. StrikersRobert Lewandowski is a striker. Strikers are soc-Robert Lewandowski left Bayernare soccer players. Robert Lewandowskicer players. Therefore, Robert Lewandowski is aMunchen. If a player left a teamleft Bayern Munchen. If a player left asoccer player. Robert Lewandowski left Bayernthey no longer play for that team.Unable toteam they no longer play for that team. Question: Robert Lewandowski plays forMunchen. If a player left a team, they no longer play for that team. Therefore, Robert LewandowskiRobert Lewandowski does not play for Bayern Munchen. Therefore, it isextend theBayern Munchen.no longer plays for Bayern Munchen. However, thefalse that Robert Lewandowski playsreasoning chainquestion is asking if he plays for Bayern Munchen, not if he left. The information does not state that hefor Bayern Munchen. Answer: false.currently plays for another team.Answer: uncertain.</p>
<p>Table 6 :
6
Comparative performance of GPT-4o and Llama-3.1-70B-Instruct on the original dataset and the corresponding corrupted version.
ModelPrompting StrategyOriginalCorruptedAcc ∆GPT-4oDirect58.3343.33-15.00GPT-4oCoT68.3345.00-23.33Llama-3.1-70B-InstructDirect65.0048.33-16.67Llama-3.1-70B-InstructCoT65.0053.33-11.67D EXAMPLES FROM THE THREE SUBSETS OF PROVERQAProverQA-Easy
Context:</p>
<p>The version of GPT-4o we used in our experiments is gpt-4o-2024-08-06, and the version of Claude3.5 we used is claude-3-5-sonnet-20240620.
ACKNOWLEDGMENTSThis work is supported by National Key Research and Development Program of China (Grant No. 2024YFB3311902), National Natural Science Foundation of China (Grant No. 62173017), Beijing Natural Science Foundation (Grant No. L241018), Beijing Nova Program (Grant No. 2024085), and Shanghai Artificial Intelligence Laboratory.A PROMPT FOR GENERATING BACKGROUND STORYSystem: You will be given a keyword and a name (can be a person's name or an animals' name).Your task is to generate a background story with no more than 150 words according to the keyword about this name.Your answer should be in JSON format with keys: category, story.User:keyword: swift name: Jack Assistant: { "category": "cat", "story": "Jack was a sleek, agile cat known for his swift movements.Born in the bustling city, he quickly learned to navigate the narrow alleyways and rooftops with ease.His speed and agility made him a legend among the local feline community.Jack's favorite pastime was chasing the elusive city birds, darting through the air with unmatched precision.Despite his wild spirit, he had a soft spot for the elderly woman who fed him daily.She named him Jack after the nimble character from her favorite childhood story, believing that his swiftness was a sign of good fortune.Jack's swift nature wasn't just physical; he had a quick wit and an uncanny ability to sense danger, keeping him safe in the urban jungle.His legend grew, and he became a symbol of agility and cleverness in the neighborhood."} User: keyword: success name: Amy Assistant: { "category": "human", "story": "Amy grew up in a small town where opportunities were scarce.Despite the odds, she was determined to make something of herself.After excelling in high school, she earned a scholarship to a prestigious university.There, she majored in computer science and quickly became known for her innovative ideas.Upon graduation, Amy cofounded a tech startup that focused on sustainable technology.Her company quickly gained traction, securing major investments and partnerships.Amy's relentless work ethic and visionary leadership turned her startup into a multi-million dollar enterprise within five years.Today, she is celebrated not only for her business acumen but also for her commitment to mentoring young entrepreneurs and giving back to her community.Amy's story is a testament to the power of perseverance, vision, and hard work."} fact1: Anakin is a domesticated elephant.rule: For every elephant, the elephant is either domesticated or wild, but not both.conclusion: Anakin is not a wild elephant.Therefore, it is true that Anakin is not a wild elephant.The correct option is: A.ProverQA-MediumContext:Everyone who either practices waltz or masters tango (but not both) has good dance skills.Sawyer has charisma.Sawyer is a performer.If Sawyer has good dance skills and has charisma, then he is a successful dancer.Sawyer is either a musician or a poet, but not both.Alice is a poet.Sawyer is a poet.If Sawyer is a performer, then he is either skilled at dancing or a musician, but not both.Question: Based on the above information, is the following statement true, false, or uncertain?Sawyer is a successful dancer.The correct option is: A--------------------------------------------Proofs--------------------------------------------------------------Step1------------------fact1: Sawyer is a poet.rule: Sawyer is either a musician or a poet, but not both.conclusion: Sawyer is not a musician.------------------Step2------------------fact1: Sawyer is a performer.fact2: Sawyer is not a musician.rule: If Sawyer is a performer, then he is either skilled at dancing or a musician, but not both.conclusion: Sawyer has good dance skills.------------------Step3------------------fact1: Sawyer has good dance skills.fact2: Sawyer has charisma.rule: If Sawyer has good dance skills and has charisma, then he is a successful dancer.conclusion: Sawyer is a successful dancer.Therefore, it is true that Sawyer is a successful dancer.The correct option is: A.ProverQA-HardContext:Buster does not respond to calls.All dogs that listen to commands and behave properly are well-trained.Buster is either obedient or playful, but not necessarily both.Buster follows routines.Buster is either playful or loving (or both).If Buster either responds to calls or follows routines (but not both), then he listens to commands.All well-trained dogs are obedient and have good manners.Buster behaves properly.Question: Based on the above information, is the following statement true, false, or uncertain?Buster is loving.OptionsThe correct option is: A--------------------------------------------Proofs--------------------------------------------------------------Step1------------------fact1: Buster does not respond to calls.fact2: Buster follows routines.rule: If Buster either responds to calls or follows routines (but not both), then he listens to commands.conclusion: Buster listens to commands.------------------Step2------------------fact1: Buster listens to commands fact2: Buster behaves properly.rule: All dogs that listen to commands and behave properly are well-trained.conclusion: Buster is well-trained.------------------Step3------------------fact1: Buster is well-trained.rule: All well-trained dogs are obedient and have good manners.conclusion: Buster is obedient.------------------Step4------------------fact1: Buster is obedient.rule: Buster is either obedient or playful, but not necessarily both.conclusion: Buster is not playful.------------------Step5------------------fact1: Buster is not playful.rule: Buster is either playful or loving (or both).conclusion: Buster is loving.Therefore, it is true that Buster is loving.The correct option is: A.E EXAMPLES OF THE TWO PROMPTING METHODSThe prompts of the two prompting strategies we used in our experiments are listed below.We adopt 2-shot in-context learning for both Standard and CoT prompting strategies.Standard PromptingSystem:Given a problem statement as contexts, the task is to answer a logical reasoning question.Your answer should be in JSON format with key: answer.Context:Anyone who can solve problems and communicate effectively has practical skills.If Paola appreciates beauty, then she values precision and enjoys manual work.Paola appreciates beauty.Paola is either skilled in mathematics or has practical skills, but not both.If someone enjoys manual work and values attention to detail, then they can craft with their hands.Paola is either skilled in mathematics or artistically inclined, but surprisingly, she is actually both.Paola values attention to detail.Anyone who crafts with their hands has practical skills.Question: Based on the above information, is the following statement true, false, or uncertain?Paola is artistically inclined.Options:The correct option is: { "answer": "A" }------------------Another Example  ------------------Context:The correct option is:CoT PromptingSystem:Given a problem statement as contexts, the task is to answer a logical reasoning question.Your answer should be in JSON format with keys: reasoning, answer.Context:Anyone who can solve problems and communicate effectively has practical skills.If Paola appreciates beauty, then she values precision and enjoys manual work.Paola appreciates beauty.Paola is either skilled in mathematics or has practical skills, but not both.If someone enjoys manual work and values attention to detail, then they can craft with their hands.Paola is either skilled in mathematics or artistically inclined, but surprisingly, she is actually both.Paola values attention to detail.Anyone who crafts with their hands has practical skills.Question: Based on the above information, is the following statement true, false, or uncertain?Paola is artistically inclined.Paola is either skilled in mathematics or has practical skills, but not both.Paola is not skilled in mathematics.Paola is not skilled in mathematics.Paola is either skilled in mathematics or artistically inclined, but surprisingly, she is actually both.Paola is artistically inclined.Therefore, it is true that Paola is artistically inclined.The correct option is: A.", "answer": "A" }------------------Another Example  ------------------Context:OptionsOptions: [[OPTIONS]]The correct option is:F TRAINING DETAILSTo create the ProverQA train set, we started by generating 5,000 instances for each difficulty level: easy, medium, and hard.We then applied data augmentation techniques to expand the dataset.Specifically, for each problem, we broke down every step of its reasoning chain to form new questions.Additionally, we replaced the final conclusions with random facts to create problems whose answer is uncertain.This process expanded our generated data pool to 50,000 instances.Our experiments show that, as the number of training instances increased, the model's performance consistently improved on in-distribution datasets and two out-of-distribution datasets: ProofWriter and ProntoQA.However, performance on FOLIO fluctuated as the number of training instances increases.After further experimentation, we found that 5,000 is a sweet spot that reaches a good balance between in-distribution and out-of-distribution performance.Therefore, we sample 5,000 instances from the generated data pool as the final training set for finetuning experiments.Since FOLIO lacks reasoning chains, we could not apply the same augmentation techniques on it.We used the full set of FOLIO, consisting of 1,001 instances.For ProofWriter, which includes multiple questions per context similar to our data augmentation, we did not apply further augmentation.We also finetuned the model on the full set of ProofWriter, which has 45,290 instances.The performance is comparable compared to the performance of model trained on a sampled subset consisting of 5,000 instances, indicating that the model may already be saturated at this size for an 8B parameter model.As a result, we report the results of the sampled subset.For all finetuning experiments, we experimented with different training hyper-parameters, such as training epochs, learning rate and training set size, and selected the best configurations on held-out validation sets.G EXPERIMENTING WITH 5-SHOT EXAMPLESTo further explore the impact of few-shot examples, we conducted additional experiments using 5-shot prompts.The results are presented in Table7.It can be concluded that incorporating more examples (5-shot vs. 2-shot) into the prompt did not consistently improve the performance of LLMs.This inconsistency may stem from several factors, including variations in the models' in-context learning capabilities.H QUALITY CONTROLThe ProverGen framework employs three heuristic methods to ensure the quality of the generated data, addressing different aspects of data integrity and consistency.• Logic Validation: Our framework includes a robust logic validation step.For each instance, we input both core premises and distractions into the symbolic prover to ensure they correctly deduce the conclusion's truth value.• Conflicts Resolution: During rule translation, we check for previously used predicates to avoid redundancy and potential conficting facts.Additionally, we utilize LLMs to assess whether the generated universal rules align with real-world knowledge.In cases of conflict, we opt for specific rules instead.• Translation Quality Control: We apply a heuristic method to ensure that all involved entities appear in both symbolic expression and natural language expression.For example, when translating "poet(Sawyer)" into "Sawyer is a poet", we verify that both the name "Sawyer" and the predicate "poet" are present in the translation.Additionally, the training part of the paper also serves as a quality-checking process.Finetuning on the generated dataset enhances the performance of LLMs on both in-distribution and Out-of-Distribution datasets, indicating the relatively high quality of the generated data.To further investigate the quality of the generated data, we performed a manual evaluation of 60 randomly sampled examples (20 from each subset) from our dataset.The evaluation revealed no translation errors or indirect contradictions, suggesting that the likelihood of potential errors is less than 2%.
Llm based generation of item-description for recommendation system. Arkadeep Acharya, Brijraj Singh, Naoyuki Onoe, Proceedings of the 17th ACM Conference on Recommender Systems. the 17th ACM Conference on Recommender Systems2023</p>
<p>Claude 3.5 sonnet. Anthropic, 2024</p>
<p>Transformers as soft reasoners over language. Peter Clark, Oyvind Tafjord, Kyle Richardson, Proceedings of the Twenty-Ninth International Conference on International Joint Conferences on Artificial Intelligence. the Twenty-Ninth International Conference on International Joint Conferences on Artificial Intelligence2021</p>
<p>Z3: An efficient smt solver. Leonardo De, Moura , Nikolaj Bjørner, International conference on Tools and Algorithms for the Construction and Analysis of Systems. Springer2008</p>
<p>The lean theorem prover (system description). Leonardo De Moura, Soonho Kong, Jeremy Avigad, Floris Van Doorn, Jakob Von Raumer, Automated Deduction-CADE-25: 25th International Conference on Automated Deduction. Proceedings. Berlin, GermanySpringerAugust 1-7, 2015. 201525</p>
<p>The llama 3 herd of models. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, arXiv:2407.217832024arXiv preprint</p>
<p>Pranshav Gajjar and Vijay K Shah. Oran-bench-13k: An open source benchmark for assessing llms in open radio access networks. Bruce Frederiksen, arXiv:2407.062452008. 2024arXiv preprintApplying expert system technology to code reuse with pyke. PyCon: Chicago</p>
<p>Shortcut learning in deep neural networks. Robert Geirhos, Jörn-Henrik Jacobsen, Claudio Michaelis, Richard Zemel, Wieland Brendel, Matthias Bethge, Felix A Wichmann, Nature Machine Intelligence. 2112020</p>
<p>Roscoe: A suite of metrics for scoring step-by-step reasoning. Olga Golovneva, Moya Peng Chen, Spencer Poff, Martin Corredor, Luke Zettlemoyer, Maryam Fazel-Zarandi, Asli Celikyilmaz, The Eleventh International Conference on Learning Representations. 2022</p>
<p>Teaching temporal logics to neural networks. Christopher Hahn, Frederik Schmitt, Jens U Kreber, Markus Norman Rabe, Bernd Finkbeiner, International Conference on Learning Representations. 2021</p>
<p>Simeng Han, Hailey Schoelkopf, Yilun Zhao, Zhenting Qi, Martin Riddell, Luke Benson, Lucy Sun, Ekaterina Zubova, Yujie Qiao, Matthew Burtell, arXiv:2209.00840Natural language reasoning with first-order logic. 2022arXiv preprint</p>
<p>Won't get fooled again: Answering questions with false premises. Shengding Hu, Yifan Luo, Huadong Wang, Xingyi Cheng, Zhiyuan Liu, Maosong Sun, arXiv:2307.023942023arXiv preprint</p>
<p>Chatgpt is fun, but it is not funny! humor is still challenging large language models. Sophie Freya, Jentzsch , Kristian Kersting, 61st Annual Meeting of the Association for Computational Linguistics, ACL 2023. 2023</p>
<p>. Alexandre Albert Q Jiang, Arthur Sablayrolles, Chris Mensch, Devendra Bamford, Diego Singh Chaplot, Florian De Las Casas, Gianna Bressand, Guillaume Lengyel, Lucile Lample, Saulnier, arXiv:2310.068252023Mistral 7b. arXiv preprint</p>
<p>. Alexandre Albert Q Jiang, Antoine Sablayrolles, Arthur Roux, Blanche Mensch, Chris Savary, Devendra Bamford, Diego Singh Chaplot, Emma Bou De Las Casas, Florian Hanna, Bressand, arXiv:2401.040882024Mixtral of experts. arXiv preprint</p>
<p>Logiqa 2.0-an improved dataset for logical reasoning in natural language understanding. Hanmeng Liu, Jian Liu, Leyang Cui, Zhiyang Teng, Nan Duan, Ming Zhou, Yue Zhang, 10.1109/TASLP.2023.3293046IEEE/ACM Transactions on Audio, Speech, and Language Processing. 312023</p>
<p>Logiqa: a challenge dataset for machine reading comprehension with logical reasoning. Jian Liu, Leyang Cui, Hanmeng Liu, Dandan Huang, Yile Wang, Yue Zhang, Proceedings of the Twenty-Ninth International Conference on International Joint Conferences on Artificial Intelligence. the Twenty-Ninth International Conference on International Joint Conferences on Artificial Intelligence2021</p>
<p>Faithful chain-of-thought reasoning. Qing Lyu, Shreya Havaldar, Adam Stein, Li Zhang, Delip Rao, Eric Wong, Marianna Apidianaki, Chris Callison-Burch, IJCNLP-AACL 2023The 13th International Joint Conference on Natural Language Processing and the 3rd Conference of the Asia-Pacific Chapter. the Association for Computational Linguistics2023</p>
<p>Release of prover9. William Mccune, Mile high conference on quasigroups, loops and nonassociative systems. Denver, Colorado2005</p>
<p>Sympy: symbolic computing in python. Aaron Meurer, Christopher P Smith, Mateusz Paprocki, Ondřej Čertík, B Sergey, Matthew Kirpichev, Amit Rocklin, Sergiu Kumar, Jason K Ivanov, Sartaj Moore, Thilina Singh, Sean Rathnayake, Brian E Vig, Richard P Granger, Francesco Muller, Harsh Bonazzi, Shivam Gupta, Fredrik Vats, Fabian Johansson, Matthew J Pedregosa, Andy R Curry, Štěpán Terrel, Ashutosh Roučka, Isuru Saboo, Sumith Fernando, Robert Kulal, Anthony Cimrman, Scopatz, 10.7717/peerj-cs.103PeerJ Computer Science. 2376-59923e103January 2017</p>
<p>Wordnet: a lexical database for english. George A Miller, Communications of the ACM. 38111995</p>
<p>Linc: A neurosymbolic approach for logical reasoning by combining language models with first-order logic provers. Alex Theo X Olausson, Ben Gu, Cedegao E Lipkin, Armando Zhang, Joshua B Solar-Lezama, Roger P Tenenbaum, Levy, The 2023 Conference on Empirical Methods in Natural Language Processing. 2023</p>
<p>Hello GPT-4o. Openai, 2024</p>
<p>Logic-lm: Empowering large language models with symbolic solvers for faithful logical reasoning. Liangming Pan, Alon Albalak, Xinyi Wang, William Yang, Wang , arXiv:2305.122952023arXiv preprint</p>
<p>An investigation of llms' inefficacy in understanding converse relations. Chengwen Qi, Bowen Li, Binyuan Hui, Bailin Wang, Jinyang Li, Jinwang Wu, Yuanjun Laili, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language Processing2023</p>
<p>Street: A multi-task structured reasoning and explanation benchmark. Danilo Neves Ribeiro, Shen Wang, Xiaofei Ma, Henghui Zhu, Rui Dong, Deguang Kong, Juliette Burger, Anjelica Ramos, William Yang Wang, George Karypis, The Eleventh International Conference on Learning Representations. 2022</p>
<p>Language models are greedy reasoners: A systematic formal analysis of chain-of-thought. Abulhair Saparov, He He, The Eleventh International Conference on Learning Representations. 2022</p>
<p>Testing the general deductive reasoning capacity of large language models using ood examples. Abulhair Saparov, Richard Yuanzhe Pang, Vishakh Padmakumar, Nitish Joshi, Mehran Kazemi, Najoung Kim, He He, Advances in Neural Information Processing Systems. 202436</p>
<p>Clutrr: A diagnostic benchmark for inductive reasoning from text. Koustuv Sinha, Shagun Sodhani, Jin Dong, Joelle Pineau, William L Hamilton, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing. the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language ProcessingEMNLP-IJCNLP2019</p>
<p>Lab: Large-scale alignment for chatbots. Shivchander Sudalairaj, Abhishek Bhandwaldar, Aldo Pareja, Kai Xu, David D Cox, Akash Srivastava, 2024</p>
<p>Conditionalqa: A complex reading comprehension dataset with conditional answers. Haitian Sun, William W Cohen, Ruslan Salakhutdinov, arXiv:2110.068842021arXiv preprint</p>
<p>Proofwriter: Generating implications, proofs, and abductive statements over natural language. Oyvind Tafjord, Bhavana Dalvi, Peter Clark, Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021. 2021</p>
<p>Stanford alpaca: An instruction-following llama model. Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, Tatsunori B Hashimoto, 2023</p>
<p>Diagnosing the first-order logical reasoning ability through logicnli. Jidong Tian, Yitian Li, Wenqing Chen, Liqiang Xiao, Hao He, Yaohui Jin, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. the 2021 Conference on Empirical Methods in Natural Language Processing2021</p>
<p>Large language models still can't plan (a benchmark for llms on planning and reasoning about change). Karthik Valmeekam, Alberto Olmo, Sarath Sreedharan, Subbarao Kambhampati, NeurIPS 2022 Foundation Models for Decision Making Workshop. 2022</p>
<p>Describe, explain, plan and select: interactive planning with llms enables open-world multi-task agents. Zihao Wang, Shaofei Cai, Guanzhou Chen, Anji Liu, Xiaojian , Shawn Ma, Yitao Liang, Advances in Neural Information Processing Systems. 202436</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Denny Quoc V Le, Zhou, Advances in neural information processing systems. 202235</p>
<p>Satlm: Satisfiability-aided language models using declarative prompting. Xi Ye, Qiaochu Chen, Isil Dillig, Greg Durrett, Advances in Neural Information Processing Systems. 202436</p>
<p>Reclor: A reading comprehension dataset requiring logical reasoning. Weihao Yu, Zihang Jiang, Yanfei Dong, Jiashi Feng, International Conference on Learning Representations. 2019</p>
<p>Sola: Solver-layer adaption of llm for better logic reasoning. Yu Zhang, Hui-Ling Zhen, Zehua Pei, Yingzhao Lian, Lihao Yin, Mingxuan Yuan, Bei Yu ; Wanjun, Siyuan Zhong, Duyu Wang, Zenan Tang, Daya Xu, Jiahai Guo, Jian Wang, Ming Yin, Nan Zhou, Duan, arXiv:2402.11903arXiv:2104.06598Ar-lsat: Investigating analytical reasoning of text. 2024. 2021arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
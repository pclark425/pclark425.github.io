<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3963 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3963</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3963</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-92.html">extraction-schema-92</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how LLM-generated scientific theories are evaluated, including evaluation criteria, methods, benchmarks, metrics, human involvement, limitations, and examples.</div>
                <p><strong>Paper ID:</strong> paper-366d05cb33bab998f6f1ce9ae676d6ee702977aa</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/366d05cb33bab998f6f1ce9ae676d6ee702977aa" target="_blank">Learning to Generate Novel Scientific Directions with Contextualized Literature-based Discovery</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> A novel formulation of contextualized -LBD (C-LBD) is presented: generating scientific hypotheses in natural language while grounding them in a context that controls the hypothesis search space, which improves over baselines and reveals challenges on the road to building machines that generate new scientific knowledge.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3963.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3963.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how LLM-generated scientific theories are evaluated, including evaluation criteria, methods, benchmarks, metrics, human involvement, limitations, and examples.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>HumanEvaluationStudies</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Human evaluation studies (Study I, II, III)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Three human-evaluation protocols run in this work to assess LLM-generated scientific ideas, including (I) per-output binary helpful/unhelpful judgments on relevance/novelty/clarity/reasonableness, (II) pairwise ranking vs. ground-truth and between model variants for technical detail and novelty, and (III) focused comparisons to measure the effect of iterative novelty boosting on difference and novelty.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Relevance to background+seed, novelty (difference from context and literature), clarity/coherence, technical depth/level of detail, usefulness/reasonableness ('helpful'), incrementality vs. innovation.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>Expert human annotation: (I) blind randomized presentation of outputs from multiple model variants and binary helpful/unhelpful labels; (II) pairwise ranking between top model variants and comparison to the paper's ground-truth idea for technical detail and novelty; (III) targeted annotation whether iterative updates are different and more novel. Annotators were given explicit instructions and examples and asked to avoid counting trivial copying as novel.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Gold subset of test instances (194 challenge instances selected from ACL-Anthology-derived dataset with low background–target similarity), sampled 50 instances for Study I, subset used for Study II and 70 instances for Study III. Annotators were graduate-level NLP/biomedical experts.</td>
                        </tr>
                        <tr>
                            <td><strong>metrics_reported</strong></td>
                            <td>Percent helpful/unhelpful votes per system (Table 2), percent of cases where one variant has higher technical detail/novelty (Study II percentages), percent of updated ideas judged substantially different and percent judged more novel after iteration (Study III: e.g., 88.9% different; 55.6% more novel after first iteration for SN; 57.8% novelty gain on second iteration for SN). Inter-annotator agreement percentages reported (Table 13, Table 14).</td>
                        </tr>
                        <tr>
                            <td><strong>human_involvement</strong></td>
                            <td>High — six volunteer NLP experts with graduate-level education for Study I; a subset of annotators for Study II; five annotators for Study III; two biochemical domain experts for the domain generalization biomedical case study. Annotators were blind to condition and outputs were shuffled.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Human evaluation is costly and subjective; annotator pool limited to graduate-level experts which may bias judgments; expectations for novelty were conservatively set in Study I; human judgments can conflate clarity/length with technical depth; provides only partial coverage of the large space of plausible hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_theory_example</strong></td>
                            <td>Iterative example from Table 5: initial LLM suggestion 'A pause prediction model to identify speech unit boundaries' updated to 'Adaptive Speech Unit Boundary Detection (ASUBD)' using attention + reinforcement learning — this LLM-generated idea was judged across iterations for difference and novelty.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Human raters preferred GPT4 few-shot variants: GPT4FS and GPT4FS+KG received substantially higher 'helpful' votes (e.g., GPT4FS: 73 helpful votes in Table 2) than other systems; GPT4FS+KG had higher technical detail in 48% of pairwise comparisons and was less incremental in 45% (Study II). However, in comparisons vs. ground-truth ideas, ground truth was judged to have significantly higher technical level and novelty in 85% of cases.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3963.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3963.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how LLM-generated scientific theories are evaluated, including evaluation criteria, methods, benchmarks, metrics, human involvement, limitations, and examples.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AutomatedMetrics</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Automated textual-similarity metrics (ROUGE, BERTScore, BARTScore)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Standard automated text-generation metrics were used to compare generated idea sentences to ground-truth idea sentences; they serve as coarse proxies for similarity to known ideas but have limitations for open-ended novelty assessment.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Surface similarity / lexical and semantic overlap with ground truth (proxy for relevance or copying), not direct measures of novelty or scientific validity.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>Compute ROUGE-L, BERTScore (with SciBERT checkpoint) and BARTScore between generated sentence and ground-truth sentences on both challenging and gold subsets. Use these scores to compare model variants and measure improvements from fine-tuning and contrastive objectives.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Challenging and gold subsets of the ACL-Anthology-derived test split (gold=194 instances); automatic evaluation reported across these subsets (Table 9).</td>
                        </tr>
                        <tr>
                            <td><strong>metrics_reported</strong></td>
                            <td>ROUGE-L and BERTScore values per model variant (Table 9). Example values: GPT4FS R-L=0.143 / BERT=0.618 on challenging subset; T5-based TS variants achieved higher R-L and BERT (e.g., TS+SN+CL R-L=0.228, BERT=0.671 on challenging subset).</td>
                        </tr>
                        <tr>
                            <td><strong>human_involvement</strong></td>
                            <td>None for automated metrics calculation, but used in conjunction with human evaluation for interpretation.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Automated metrics favor shorter or templated outputs and copying; GPT-4 outputs were longer and thus underperformed on automated similarity metrics despite higher human preference. Automated similarity does not measure novelty relative to literature and can be misleading for open-ended generation.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_theory_example</strong></td>
                            <td>N/A — automated metrics evaluate generated sentences such as 'In this paper we propose a new ... for ...' against ground truth using ROUGE/BERTScore.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>T5 fine-tuned models score better on ROUGE/BERTScore than GPT-based systems due to training and output length differences; contrastive in-context augmentation improves automated metrics relative to baseline fine-tuning, but these improvements do not fully align with human judgments of novelty and usefulness.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3963.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3963.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how LLM-generated scientific theories are evaluated, including evaluation criteria, methods, benchmarks, metrics, human involvement, limitations, and examples.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GoldTestSet</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Gold test subset (human-vetted low-overlap instances)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A manually reviewed, high-quality test subset of 194 instances chosen from the test split by filtering for low background–ground-truth surface similarity and ensuring non-triviality of target relation, used for human evaluation and stricter automated analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Create challenging evaluation cases that avoid trivial copying: low cosine similarity between background and ground-truth (≤ 0.074, 10th percentile) and manual exclusion of trivial overlaps, irrelevant backgrounds, or instances with unexplained terms.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>Automatic similarity filtering (SentenceBERT cosine) followed by manual annotation to remove trivial cases; the resulting gold set is used for human studies and as the 'gold' automated-eval subset.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Gold subset drawn from ACL Anthology dataset split temporally (train/dev/test) — 194 manually verified instances.</td>
                        </tr>
                        <tr>
                            <td><strong>metrics_reported</strong></td>
                            <td>Used as evaluation corpus for both human studies and automated metrics; automatic metric scores on gold subset reported in Table 9 (e.g., GPT4FS R-L=0.151, BERT=0.624 on gold subset).</td>
                        </tr>
                        <tr>
                            <td><strong>human_involvement</strong></td>
                            <td>Manual curation by the authors to ensure quality and non-triviality; later used for human annotation studies.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Gold subset is small (194) relative to full test set; manual vetting is expensive and subjective; may not capture full diversity of plausible, high-quality novel ideas.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_theory_example</strong></td>
                            <td>Instances in the gold set used to present background+seed and solicit generated idea sentences evaluated by annotators; specific examples are used in Studies I–III.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Used to obtain conservative and more reliable human judgments; showed consistent trends: GPT4 variants preferred by human raters despite lower automated scores; T5 variants scored higher on automated metrics on gold set as well.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3963.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e3963.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how LLM-generated scientific theories are evaluated, including evaluation criteria, methods, benchmarks, metrics, human involvement, limitations, and examples.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>IterativeNoveltyBoosting</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Iterative novelty boosting with retrieval-and-compare (retrieve-compare-update)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A novelty optimization loop that measures similarity between a generated idea and a reference corpus (using SentenceBERT), retrieves close literature sentences, then instructs the model to rewrite the idea to be more novel until similarity falls below a threshold.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Novelty measured as cosine similarity to retrieved literature ideas from a training-derived reference corpus; also human judgments of whether the updated idea is different and more novel.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>Automated retrieval of the top-k (k=20) nearest literature ideas to current idea using SentenceBERT embeddings; compute similarity scores S_i, compare to threshold μ=0.6; if any S_i ≥ μ, present retrieved similar sentences as negatives and prompt LLM to update idea; iterate until all S_i < μ. Human annotators then judge difference and novelty of updated ideas.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Reference corpus R built from the entire training set (papers prior to cutoff), retrieval over ~59k papers and 374k sentences in NLP domain; iterative evaluation applied to selected generated ideas that had sufficiently similar related work detected.</td>
                        </tr>
                        <tr>
                            <td><strong>metrics_reported</strong></td>
                            <td>Relative improvements in judged novelty (Table 3): e.g., for GPT4FS baseline 1st novelty Δ +54.4%; with SN (semantic neighbors) 1st novelty Δ +55.6% and 2nd iteration +57.8%; average new terms added percentages (e.g., +22.8% new terms for +SN first iteration). Also qualitative examples and counts of cases updated.</td>
                        </tr>
                        <tr>
                            <td><strong>human_involvement</strong></td>
                            <td>Five annotators compared initial vs. updated ideas to label whether they were substantially different and more novel; annotators also judged whether further iterations increased novelty.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Novelty measurement depends on quality of retrieval and embedding similarity; cosine thresholds are heuristic; updates often increase superficial novelty (new terms) but not scientific depth; retrieval may miss semantically similar but lexically dissimilar literature; method cannot guarantee meaningful or experimentally valid novelty.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_theory_example</strong></td>
                            <td>Table 5 example: initial 'A pause prediction model...' -> iteration 1: acoustic+linguistic features dynamic model -> iteration 2: 'Adaptive Speech Unit Boundary Detection (ASUBD)' combining attention + reinforcement learning; judged as increased novelty but still superficial compared to ground-truth segmentation module.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Iterative boosting substantially increased judge-assessed novelty for ideas to which it was applied (first-iteration novelty increase ~46–55% across variants; SN variant further improved on second iteration). However, updated ideas remained often superficially novel (new term insertion, concept recombination) and still lacked the depth of actual papers.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3963.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e3963.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how LLM-generated scientific theories are evaluated, including evaluation criteria, methods, benchmarks, metrics, human involvement, limitations, and examples.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DomainGeneralizationCaseStudy</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Biomedical domain generalization (PubMed / Meditron-7b study)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A cross-domain case study applying the SciMON pipeline to biomedical literature (PubMed) with domain-specific IE (PubTator) and finetuning a biomedical LLM (Meditron-7b), evaluated by two biochemical domain experts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Same as NLP experiments: relevance, novelty, clarity, and technical detail compared to ground truth; domain experts judged outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>Construct biomedical dataset via PubTator extraction and sentence classification; finetune Meditron-7b and variants; human evaluation by two biomedical experts on helpfulness and comparison to ground truth.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>PubMed-derived dataset covering biochemical topics (4,767 papers before 2023/02 etc.); test split intentionally past model's pretraining cutoff for memorization control.</td>
                        </tr>
                        <tr>
                            <td><strong>metrics_reported</strong></td>
                            <td>Human evaluation percentages (Table 4): Méditron baseline helpful 35%, Méditron+SN helpful 80%, Méditron+CT helpful 60%, Méditron+KG helpful 50%; percent of outputs judged better than ground truth (vs. GT) reported (e.g., Méditron+SN vs GT = 45%).</td>
                        </tr>
                        <tr>
                            <td><strong>human_involvement</strong></td>
                            <td>Two biochemical domain experts with graduate-level education performed the evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Preliminary and small-scale; domain experts were more satisfied with some generated outputs than ground truth possibly due to differences in expectations or abstraction level; further deeper evaluation required to assess experimental validity and safety.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_theory_example</strong></td>
                            <td>Table 12 shows seed gene 'ARO10' and generated suggestions (e.g., Méditron: gene-pathway flux prediction; Méditron+KG: CRISPR deletion suggestion; Méditron+SN: identification of ARO10 barrier).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Fine-tuned biomedical LLMs with semantic neighbor retrieval substantially improved helpfulness (up to 80%) per the two expert judges; in some cases generated suggestions were judged more detailed than paper ground truth, unlike NLP experiments.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3963.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e3963.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how LLM-generated scientific theories are evaluated, including evaluation criteria, methods, benchmarks, metrics, human involvement, limitations, and examples.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>EvaluationLimitations</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Limitations and caveats of evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Explicit articulation of the limitations in evaluating LLM-generated scientific ideas, including dataset and IE noise, annotator bias, memorization concerns, automated-metric mismatch, and hardware/API reproducibility issues.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Meta-criteria: validity of evaluation given noisiness of IE-extracted training signals, potential memorization, annotator representativeness, and metric adequacy for open-ended novelty.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>Analysis and ablation-style discussion: manual memorization checks, precision assessment of IE preprocessing (Table 6), temporal split to reduce contamination, and discussion of reproducibility limits (API/model randomness).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Notes on dataset collection limitations: ACL Anthology crawl, PubMed crawl with language and parsing filters; IE step precision (PL-Marker relations 65% precision; overall pipeline pass rate 79.7%).</td>
                        </tr>
                        <tr>
                            <td><strong>metrics_reported</strong></td>
                            <td>IE preprocessing precision percentages (Table 6): PL-Maker Entities 91.3%, PL-Maker Used-for Relations 65.4%, SciCo Coreference 97.2%, ScispaCy Abbreviation Detection 100%, Sentence Classification 100%. Mention of non-significance flags in automated metric comparisons (Table 9 †).</td>
                        </tr>
                        <tr>
                            <td><strong>human_involvement</strong></td>
                            <td>Authors performed manual checks (memorization checks of GPT-4 on 2022 ACL papers) and curated the gold test subset; annotators were graduate-level, which authors note as a limitation.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>IE noise (relation extraction precision lower), limited annotator diversity, possible memorization by pretrained LLMs despite temporal splits, automated metrics poorly aligned with novelty and depth, hardware constraints limited model sizes evaluated, model randomness and API changes reduce reproducibility.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_theory_example</strong></td>
                            <td>N/A (meta-level discussion of evaluation limitations rather than a single LLM-generated theory).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Authors conclude evaluation is highly challenging: while methods improve assessed novelty/ helpfulness, generated ideas remain far behind scientific papers in novelty, depth, and utility; automated evaluations partially misalign with human judgments and evaluation resource constraints limit breadth.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>ROUGE: A package for automatic evaluation of summaries <em>(Rating: 2)</em></li>
                <li>BERTScore: Evaluating text generation with bert <em>(Rating: 2)</em></li>
                <li>BARTScore: Evaluating generated text as text generation <em>(Rating: 2)</em></li>
                <li>Quantifying memorization across neural language models <em>(Rating: 2)</em></li>
                <li>Capabilities of GPT-4 on medical challenge problems <em>(Rating: 1)</em></li>
                <li>Benchmarking large language models as ai research agents <em>(Rating: 1)</em></li>
                <li>Undiscovered public knowledge <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3963",
    "paper_id": "paper-366d05cb33bab998f6f1ce9ae676d6ee702977aa",
    "extraction_schema_id": "extraction-schema-92",
    "extracted_data": [
        {
            "name_short": "HumanEvaluationStudies",
            "name_full": "Human evaluation studies (Study I, II, III)",
            "brief_description": "Three human-evaluation protocols run in this work to assess LLM-generated scientific ideas, including (I) per-output binary helpful/unhelpful judgments on relevance/novelty/clarity/reasonableness, (II) pairwise ranking vs. ground-truth and between model variants for technical detail and novelty, and (III) focused comparisons to measure the effect of iterative novelty boosting on difference and novelty.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_criteria": "Relevance to background+seed, novelty (difference from context and literature), clarity/coherence, technical depth/level of detail, usefulness/reasonableness ('helpful'), incrementality vs. innovation.",
            "evaluation_methods": "Expert human annotation: (I) blind randomized presentation of outputs from multiple model variants and binary helpful/unhelpful labels; (II) pairwise ranking between top model variants and comparison to the paper's ground-truth idea for technical detail and novelty; (III) targeted annotation whether iterative updates are different and more novel. Annotators were given explicit instructions and examples and asked to avoid counting trivial copying as novel.",
            "benchmark_or_dataset": "Gold subset of test instances (194 challenge instances selected from ACL-Anthology-derived dataset with low background–target similarity), sampled 50 instances for Study I, subset used for Study II and 70 instances for Study III. Annotators were graduate-level NLP/biomedical experts.",
            "metrics_reported": "Percent helpful/unhelpful votes per system (Table 2), percent of cases where one variant has higher technical detail/novelty (Study II percentages), percent of updated ideas judged substantially different and percent judged more novel after iteration (Study III: e.g., 88.9% different; 55.6% more novel after first iteration for SN; 57.8% novelty gain on second iteration for SN). Inter-annotator agreement percentages reported (Table 13, Table 14).",
            "human_involvement": "High — six volunteer NLP experts with graduate-level education for Study I; a subset of annotators for Study II; five annotators for Study III; two biochemical domain experts for the domain generalization biomedical case study. Annotators were blind to condition and outputs were shuffled.",
            "limitations_or_challenges": "Human evaluation is costly and subjective; annotator pool limited to graduate-level experts which may bias judgments; expectations for novelty were conservatively set in Study I; human judgments can conflate clarity/length with technical depth; provides only partial coverage of the large space of plausible hypotheses.",
            "llm_theory_example": "Iterative example from Table 5: initial LLM suggestion 'A pause prediction model to identify speech unit boundaries' updated to 'Adaptive Speech Unit Boundary Detection (ASUBD)' using attention + reinforcement learning — this LLM-generated idea was judged across iterations for difference and novelty.",
            "evaluation_results": "Human raters preferred GPT4 few-shot variants: GPT4FS and GPT4FS+KG received substantially higher 'helpful' votes (e.g., GPT4FS: 73 helpful votes in Table 2) than other systems; GPT4FS+KG had higher technical detail in 48% of pairwise comparisons and was less incremental in 45% (Study II). However, in comparisons vs. ground-truth ideas, ground truth was judged to have significantly higher technical level and novelty in 85% of cases.",
            "uuid": "e3963.0"
        },
        {
            "name_short": "AutomatedMetrics",
            "name_full": "Automated textual-similarity metrics (ROUGE, BERTScore, BARTScore)",
            "brief_description": "Standard automated text-generation metrics were used to compare generated idea sentences to ground-truth idea sentences; they serve as coarse proxies for similarity to known ideas but have limitations for open-ended novelty assessment.",
            "citation_title": "",
            "mention_or_use": "use",
            "evaluation_criteria": "Surface similarity / lexical and semantic overlap with ground truth (proxy for relevance or copying), not direct measures of novelty or scientific validity.",
            "evaluation_methods": "Compute ROUGE-L, BERTScore (with SciBERT checkpoint) and BARTScore between generated sentence and ground-truth sentences on both challenging and gold subsets. Use these scores to compare model variants and measure improvements from fine-tuning and contrastive objectives.",
            "benchmark_or_dataset": "Challenging and gold subsets of the ACL-Anthology-derived test split (gold=194 instances); automatic evaluation reported across these subsets (Table 9).",
            "metrics_reported": "ROUGE-L and BERTScore values per model variant (Table 9). Example values: GPT4FS R-L=0.143 / BERT=0.618 on challenging subset; T5-based TS variants achieved higher R-L and BERT (e.g., TS+SN+CL R-L=0.228, BERT=0.671 on challenging subset).",
            "human_involvement": "None for automated metrics calculation, but used in conjunction with human evaluation for interpretation.",
            "limitations_or_challenges": "Automated metrics favor shorter or templated outputs and copying; GPT-4 outputs were longer and thus underperformed on automated similarity metrics despite higher human preference. Automated similarity does not measure novelty relative to literature and can be misleading for open-ended generation.",
            "llm_theory_example": "N/A — automated metrics evaluate generated sentences such as 'In this paper we propose a new ... for ...' against ground truth using ROUGE/BERTScore.",
            "evaluation_results": "T5 fine-tuned models score better on ROUGE/BERTScore than GPT-based systems due to training and output length differences; contrastive in-context augmentation improves automated metrics relative to baseline fine-tuning, but these improvements do not fully align with human judgments of novelty and usefulness.",
            "uuid": "e3963.1"
        },
        {
            "name_short": "GoldTestSet",
            "name_full": "Gold test subset (human-vetted low-overlap instances)",
            "brief_description": "A manually reviewed, high-quality test subset of 194 instances chosen from the test split by filtering for low background–ground-truth surface similarity and ensuring non-triviality of target relation, used for human evaluation and stricter automated analysis.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_criteria": "Create challenging evaluation cases that avoid trivial copying: low cosine similarity between background and ground-truth (≤ 0.074, 10th percentile) and manual exclusion of trivial overlaps, irrelevant backgrounds, or instances with unexplained terms.",
            "evaluation_methods": "Automatic similarity filtering (SentenceBERT cosine) followed by manual annotation to remove trivial cases; the resulting gold set is used for human studies and as the 'gold' automated-eval subset.",
            "benchmark_or_dataset": "Gold subset drawn from ACL Anthology dataset split temporally (train/dev/test) — 194 manually verified instances.",
            "metrics_reported": "Used as evaluation corpus for both human studies and automated metrics; automatic metric scores on gold subset reported in Table 9 (e.g., GPT4FS R-L=0.151, BERT=0.624 on gold subset).",
            "human_involvement": "Manual curation by the authors to ensure quality and non-triviality; later used for human annotation studies.",
            "limitations_or_challenges": "Gold subset is small (194) relative to full test set; manual vetting is expensive and subjective; may not capture full diversity of plausible, high-quality novel ideas.",
            "llm_theory_example": "Instances in the gold set used to present background+seed and solicit generated idea sentences evaluated by annotators; specific examples are used in Studies I–III.",
            "evaluation_results": "Used to obtain conservative and more reliable human judgments; showed consistent trends: GPT4 variants preferred by human raters despite lower automated scores; T5 variants scored higher on automated metrics on gold set as well.",
            "uuid": "e3963.2"
        },
        {
            "name_short": "IterativeNoveltyBoosting",
            "name_full": "Iterative novelty boosting with retrieval-and-compare (retrieve-compare-update)",
            "brief_description": "A novelty optimization loop that measures similarity between a generated idea and a reference corpus (using SentenceBERT), retrieves close literature sentences, then instructs the model to rewrite the idea to be more novel until similarity falls below a threshold.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_criteria": "Novelty measured as cosine similarity to retrieved literature ideas from a training-derived reference corpus; also human judgments of whether the updated idea is different and more novel.",
            "evaluation_methods": "Automated retrieval of the top-k (k=20) nearest literature ideas to current idea using SentenceBERT embeddings; compute similarity scores S_i, compare to threshold μ=0.6; if any S_i ≥ μ, present retrieved similar sentences as negatives and prompt LLM to update idea; iterate until all S_i &lt; μ. Human annotators then judge difference and novelty of updated ideas.",
            "benchmark_or_dataset": "Reference corpus R built from the entire training set (papers prior to cutoff), retrieval over ~59k papers and 374k sentences in NLP domain; iterative evaluation applied to selected generated ideas that had sufficiently similar related work detected.",
            "metrics_reported": "Relative improvements in judged novelty (Table 3): e.g., for GPT4FS baseline 1st novelty Δ +54.4%; with SN (semantic neighbors) 1st novelty Δ +55.6% and 2nd iteration +57.8%; average new terms added percentages (e.g., +22.8% new terms for +SN first iteration). Also qualitative examples and counts of cases updated.",
            "human_involvement": "Five annotators compared initial vs. updated ideas to label whether they were substantially different and more novel; annotators also judged whether further iterations increased novelty.",
            "limitations_or_challenges": "Novelty measurement depends on quality of retrieval and embedding similarity; cosine thresholds are heuristic; updates often increase superficial novelty (new terms) but not scientific depth; retrieval may miss semantically similar but lexically dissimilar literature; method cannot guarantee meaningful or experimentally valid novelty.",
            "llm_theory_example": "Table 5 example: initial 'A pause prediction model...' -&gt; iteration 1: acoustic+linguistic features dynamic model -&gt; iteration 2: 'Adaptive Speech Unit Boundary Detection (ASUBD)' combining attention + reinforcement learning; judged as increased novelty but still superficial compared to ground-truth segmentation module.",
            "evaluation_results": "Iterative boosting substantially increased judge-assessed novelty for ideas to which it was applied (first-iteration novelty increase ~46–55% across variants; SN variant further improved on second iteration). However, updated ideas remained often superficially novel (new term insertion, concept recombination) and still lacked the depth of actual papers.",
            "uuid": "e3963.3"
        },
        {
            "name_short": "DomainGeneralizationCaseStudy",
            "name_full": "Biomedical domain generalization (PubMed / Meditron-7b study)",
            "brief_description": "A cross-domain case study applying the SciMON pipeline to biomedical literature (PubMed) with domain-specific IE (PubTator) and finetuning a biomedical LLM (Meditron-7b), evaluated by two biochemical domain experts.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_criteria": "Same as NLP experiments: relevance, novelty, clarity, and technical detail compared to ground truth; domain experts judged outputs.",
            "evaluation_methods": "Construct biomedical dataset via PubTator extraction and sentence classification; finetune Meditron-7b and variants; human evaluation by two biomedical experts on helpfulness and comparison to ground truth.",
            "benchmark_or_dataset": "PubMed-derived dataset covering biochemical topics (4,767 papers before 2023/02 etc.); test split intentionally past model's pretraining cutoff for memorization control.",
            "metrics_reported": "Human evaluation percentages (Table 4): Méditron baseline helpful 35%, Méditron+SN helpful 80%, Méditron+CT helpful 60%, Méditron+KG helpful 50%; percent of outputs judged better than ground truth (vs. GT) reported (e.g., Méditron+SN vs GT = 45%).",
            "human_involvement": "Two biochemical domain experts with graduate-level education performed the evaluation.",
            "limitations_or_challenges": "Preliminary and small-scale; domain experts were more satisfied with some generated outputs than ground truth possibly due to differences in expectations or abstraction level; further deeper evaluation required to assess experimental validity and safety.",
            "llm_theory_example": "Table 12 shows seed gene 'ARO10' and generated suggestions (e.g., Méditron: gene-pathway flux prediction; Méditron+KG: CRISPR deletion suggestion; Méditron+SN: identification of ARO10 barrier).",
            "evaluation_results": "Fine-tuned biomedical LLMs with semantic neighbor retrieval substantially improved helpfulness (up to 80%) per the two expert judges; in some cases generated suggestions were judged more detailed than paper ground truth, unlike NLP experiments.",
            "uuid": "e3963.4"
        },
        {
            "name_short": "EvaluationLimitations",
            "name_full": "Limitations and caveats of evaluation",
            "brief_description": "Explicit articulation of the limitations in evaluating LLM-generated scientific ideas, including dataset and IE noise, annotator bias, memorization concerns, automated-metric mismatch, and hardware/API reproducibility issues.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_criteria": "Meta-criteria: validity of evaluation given noisiness of IE-extracted training signals, potential memorization, annotator representativeness, and metric adequacy for open-ended novelty.",
            "evaluation_methods": "Analysis and ablation-style discussion: manual memorization checks, precision assessment of IE preprocessing (Table 6), temporal split to reduce contamination, and discussion of reproducibility limits (API/model randomness).",
            "benchmark_or_dataset": "Notes on dataset collection limitations: ACL Anthology crawl, PubMed crawl with language and parsing filters; IE step precision (PL-Marker relations 65% precision; overall pipeline pass rate 79.7%).",
            "metrics_reported": "IE preprocessing precision percentages (Table 6): PL-Maker Entities 91.3%, PL-Maker Used-for Relations 65.4%, SciCo Coreference 97.2%, ScispaCy Abbreviation Detection 100%, Sentence Classification 100%. Mention of non-significance flags in automated metric comparisons (Table 9 †).",
            "human_involvement": "Authors performed manual checks (memorization checks of GPT-4 on 2022 ACL papers) and curated the gold test subset; annotators were graduate-level, which authors note as a limitation.",
            "limitations_or_challenges": "IE noise (relation extraction precision lower), limited annotator diversity, possible memorization by pretrained LLMs despite temporal splits, automated metrics poorly aligned with novelty and depth, hardware constraints limited model sizes evaluated, model randomness and API changes reduce reproducibility.",
            "llm_theory_example": "N/A (meta-level discussion of evaluation limitations rather than a single LLM-generated theory).",
            "evaluation_results": "Authors conclude evaluation is highly challenging: while methods improve assessed novelty/ helpfulness, generated ideas remain far behind scientific papers in novelty, depth, and utility; automated evaluations partially misalign with human judgments and evaluation resource constraints limit breadth.",
            "uuid": "e3963.5"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "ROUGE: A package for automatic evaluation of summaries",
            "rating": 2
        },
        {
            "paper_title": "BERTScore: Evaluating text generation with bert",
            "rating": 2
        },
        {
            "paper_title": "BARTScore: Evaluating generated text as text generation",
            "rating": 2
        },
        {
            "paper_title": "Quantifying memorization across neural language models",
            "rating": 2
        },
        {
            "paper_title": "Capabilities of GPT-4 on medical challenge problems",
            "rating": 1
        },
        {
            "paper_title": "Benchmarking large language models as ai research agents",
            "rating": 1
        },
        {
            "paper_title": "Undiscovered public knowledge",
            "rating": 1
        }
    ],
    "cost": 0.014865,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>SciMON $\square$ : Scientific Inspiration Machines Optimized for Novelty</h1>
<p>Qingyun Wang ${ }^{1}$, Doug Downey ${ }^{2}$, Heng Ji ${ }^{1}$, Tom Hope ${ }^{2,3}$<br>${ }^{1}$ University of Illinois at Urbana-Champaign ${ }^{2}$ Allen Institute for Artificial Intelligence (AI2)<br>${ }^{3}$ The Hebrew University of Jerusalem<br>{tomh, doug}@allenai.org, {qingyun4,hengji}@illinois.edu</p>
<h4>Abstract</h4>
<p>We explore and enhance the ability of neural language models to generate novel scientific directions grounded in literature. Work on literature-based hypothesis generation has traditionally focused on binary link predictionseverely limiting the expressivity of hypotheses. This line of work also does not focus on optimizing novelty. We take a dramatic departure with a novel setting in which models use as input background contexts (e.g., problems, experimental settings, goals), and output natural language ideas grounded in literature. We present SciMON, a modeling framework that uses retrieval of "inspirations" from past scientific papers, and explicitly optimizes for novelty by iteratively comparing to prior papers and updating idea suggestions until sufficient novelty is achieved. Comprehensive evaluations reveal that GPT-4 tends to generate ideas with overall low technical depth and novelty, while our methods partially mitigate this issue. Our work represents a first step toward evaluating and developing language models that generate new ideas derived from the scientific literature ${ }^{1}$.</p>
<h2>1 Introduction</h2>
<p>Can machines mine scientific papers and learn to suggest new directions? The idea that information from the literature can be used for automatically generating hypotheses has been around for decades (Swanson, 1986). To date, the focus has been on a specific setting: hypothesizing links between pairs of concepts (often in drug discovery applications (Henry and McInnes, 2017), e.g., new drug-disease links), where concepts are obtained from papers or knowledge bases previously derived from papers (Sybrandt et al., 2020; Nadkarni et al., 2021).</p>
<p>This common setting has fundamental drawbacks. Reducing the "language of scientific ideas" (Hope et al., 2023) to this simplistic form limits</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: SciMON takes background context and generates ideas grounded in literature inspirations, optimizing novelty by iteratively comparing to related work.
the expressivity of the hypotheses we can hope to generate, and does not capture nuanced contexts that scientists consider: target application settings, requirements and constraints, motivations and challenges. In light of the strong progress recently made with large language models (LLMs), in this paper we explore a dramatically different setting: models that take descriptions of problem contextsand return natural language suggestions of novel scientific directions that are grounded in literature.</p>
<p>We develop a framework named SciMON (Scientific Inspiration Machines with Optimization for Novelty), named after Nobel laureate and AI pioneer Herbert Simon who authored early foundational work on automated scientific discovery (Newell and Simon, 1956; Simon, 1973). We first present an automated data collection methodology that collects examples of past problems and proposed ideas from scientific papers. We then use this data for both fine-tuning and in-context training of LLMs-training them to take problem descriptions and output proposed ideas to address them. We observe that state-of-art LLMs (e.g., GPT-4 (OpenAI, 2023)) struggle with generating novel scientific ideas, and contribute a new modeling framework for generating hypotheses that makes progress in improving the hypothesis generation</p>
<p>ability of LLMs (Figure 1). Given a background problem description, models first dynamically retrieve inspirations from past literature in the form of related problems and their solutions along with contexts from a scientific knowledge graph. These retrieved inspirations serve to ground the generated ideas in existing literature. We then endow models with the ability to iteratively boost the novelty of generated ideas. Given an idea $\mathcal{I}$ generated by the LLM at step $t$, the model compares $\mathcal{I}$ with existing research in the literature; if it finds strongly overlapping research, the model is tasked with updating its idea to be more novel relative to prior work (much like a good researcher would do). We also introduce an in-context contrastive model which encourages novelty with respect to background context.</p>
<p>We perform the first comprehensive evaluation of language models for generating scientific ideas in our new generative, contextual setting. We focus on AI/NLP ideas to facilitate analysis by AI researchers themselves, and also demonstrate generalization to the biomedical domain. We design extensive evaluation experiments using human annotators with domain expertise to assess relevance, utility, novelty, and technical depth. Our methods substantially improve the ability of LLMs in our task; however, analyses show that ideas still fall far behind scientific papers in terms of novelty, depth and utility-raising fundamental challenges toward building models that generate scientific ideas.</p>
<h2>2 Background and New Setting</h2>
<p>We begin with a brief description of related work and background. We then present our novel setting.</p>
<p>Literature-based discovery Nearly four decades have passed since Don Swanson pioneered Literature-Based Discovery (LBD), based on the premise that the literature can be used for generating hypotheses (Swanson, 1986). LBD has been focused on a very specific, narrow type of hypothesis: links between pairs of concepts (often drugs/diseases). The classic formalization of LBD goes back to Swanson (1986) who proposed the "ABC" model where two concepts (terms) A and C are hypothesized as linked if they both co-occur with some intermediate concept B in papers. More recent work has used word vectors (Tshitoyan et al., 2019) or link prediction models (Wang et al., 2019; Sybrandt et al., 2020; Xu et al., 2023) to discover scientific hypotheses as pairwise links between concepts. A tightly related body of research focuses on scientific knowledge graph link prediction (Nadkarni et al., 2021), where predicted links may correspond to new hypotheses, and knowledge bases are reflections of existing scientific knowledge in specific domains, derived from literature. A fundamental gap in this line of work is in the lack of approaches for modeling nuanced contexts (Sosa and Altman, 2022) (e.g., the specific settings in which a drug may be relevant for a disease) for generating ideas in open-ended problem settings with unbounded hypothesis spaces, and for optimizing novelty. Our setting can be viewed as a radical departure addressing the limitations in existing settings.</p>
<p>LLMs for Scientific Innovation Large language models (LLMs) have made remarkable progress in interpreting and producing natural language content and handling knowledge-intensive tasks such as in the medical domain (Nori et al., 2023). Very recent work (Boiko et al., 2023) has explored the use of LLMs in a robotic chemistry lab setting, planning chemical syntheses of known compounds and executing experiments. Robotic lab settings are inherently limited to narrow sub-areas where such experiments are possible and relevant. Other very recent work (Huang et al., 2023) used LLMs to produce code for machine learning tasks such as Kaggle competitions, finding that a GPT-4 agent achieved $0 \%$ accuracy on research challenges such as BabyLM (Warstadt et al., 2023). GPT-4 has been anecdotally reported as having "strengths less like those of having a human co-author, and more like a mathematician working with a calculator" (Carlini, 2023). Our goal is to conduct a non-anecdotal evaluation and enhancement of strong LLMs' ability to generate novel open-ended scientific ideas.</p>
<h3>2.1 SciMON Problem Setting</h3>
<p>We are motivated by imagining an AI-based assistant that suggests ideas in natural language. The assistant takes as input background context $\mathcal{B}$ consisting of (1) current problems, motivations, experimental settings and constraints, denoted as $\mathcal{M}$; and optionally (2) a seed term $v$ that should be a focus point of the generated idea $\mathcal{I}$. The seed term is motivated by considering a user-provided cue for the model to limit its hypothesis space. Importantly, generated ideas should not merely paraphrase the background-the output should be novel with respect to $\mathcal{B}$ and the broader literature corpus. Figure 2 illustrates the setting, showing a background</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Architecture overview. Our models retrieve <em>inspirations</em> and then pass the background input and retrieved inspirations to an LM-based generation module, which iteratively optimizes novelty. Input from Qin et al. (2022).</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: We use IE to obtain literature data for our approach: problems/motivations (background) and proposed ideas (target), as well as salient seed terms.</p>
<p>text that describes problems with "<em>pretrained language models</em>" in the lifelong integration of information sources, including computational costs. The assistant aims to generate an idea for performing "<em>knowledge acquisition</em>" within this context. Given this input, we aim to generate a full sentence describing a novel idea.</p>
<h3>2.2 Automated Training Data Collection</h3>
<p>We obtain training data derived from papers with scientific information extraction (IE) model–extracting past examples of background sentences and corresponding ideas (e.g., descriptions of methods used for specific problems in the background sentences), along with salient entities as seed terms. This data is used for training in both in-context learning and fine-tuning setups.</p>
<p>We construct a corpus <em>D</em> from 67,408 ACL Anthology papers from S2ORC (Lo et al., 2020) (we later also conduct an experiment with a biomedical corpus §4.1). Given a title and the corresponding abstract from a document <em>d</em>, to select problem/motivation sentences <em>M</em> we first perform scientific sentence classification (Cohan et al., 2019) to classify sentences from the abstract into one of {<em>Background</em>, <em>Method</em>, <em>Objective</em>}, selecting sentences with labels of <em>Background</em> and treating the remaining sentences as target sentences <em>T</em> which will serve as desired output examples (Figure 3).</p>
<p>For seed term selection, we apply a state-of-the-art scientific IE system (Ye et al., 2022) to <em>T</em> to extract <em>entities</em> corresponding to <em>Task</em>, <em>Method</em>, <em>Evaluation Metric</em>, <em>Material</em>, and <em>relations</em> of the form [method, used-for, task]—mentions of methods and the tasks they are used for, materials used for tasks, etc. We treat the head (e.g., method) or tail (e.g., task) entity as the <em>seed</em> term, and name the other entity (tail/head, respectively) as a <em>target</em> term <em>t</em> ∈ <em>T</em>. Continuing our example from Figure 2, Figure 3 shows how the seed and target terms ("<em>knowledge acquisition</em>" and "<em>function preserved model expansion</em>") are extracted from <em>T</em>. During training, each instance contains (<em>B</em>, <em>T</em>) pairs; during evaluation, target information is removed.</p>
<p>We use SciCo (Cattan et al., 2021) to obtain coreference links for entity normalization, and use ScispaCy (Neumann et al., 2019) to replace abbreviations with a more informative long form. We also collect paper metadata, including the citation network <em>G<sub>c</sub></em>. We split our dataset temporally (train/dev/test correspond to papers from years &lt;2021 / 2021 / 2022 respectively). For our experiments, we used model checkpoints trained on data preceding 2022, avoiding the risk of data contamination (§6). Table 1 shows data statistics.<sup>2</sup></p>
<p><sup>2</sup>More details are in Appendix C.</p>
<p>Quality of IE Preprocessing During preprocessing, we only keep high-confidence outputs from IE models to reduce errors. We observe this removes many of the noisy cases. To validate this, we manually evaluate the precision of each preprocessing step on a random sample of papers and observe that all steps yield high precision (91%-100%) except relation extraction (65%); in total, the rate of instances passing all steps was 79.7%. ${ }^{3}$</p>
<p>Gold Test Set We create a high-quality, clean test set. We remove test instances where models can trivially use surface-level background information to infer the ground truth to create a more challenging set, selecting instances with low similarity between background and ground truth sentences. We compute the cosine similarity between each instance's background and corresponding ground truth sentence in the test set and take pairs with similarity $\leq 0.074$, which amounts to the tenth percentile of pairs. We further annotate this subset to create a gold subset. We manually exclude instances with trivial overlap between ground truth and background, remove cases with irrelevant background, and retain only instances where the target relation (from which the seed term is taken) is salient to the target sentence. We also remove test pairs that have unexplained terms in the background. We obtain a total of 194 instances. ${ }^{4}$</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Split</th>
<th style="text-align: left;">Forward</th>
<th style="text-align: left;">Backward</th>
<th style="text-align: left;">Total</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Train</td>
<td style="text-align: left;">55,884</td>
<td style="text-align: left;">58,426</td>
<td style="text-align: left;">114,310</td>
</tr>
<tr>
<td style="text-align: left;">Valid</td>
<td style="text-align: left;">7,938</td>
<td style="text-align: left;">8,257</td>
<td style="text-align: left;">16,195</td>
</tr>
<tr>
<td style="text-align: left;">Test</td>
<td style="text-align: left;">2,623</td>
<td style="text-align: left;">2,686</td>
<td style="text-align: left;">5,309</td>
</tr>
</tbody>
</table>
<p>Table 1: Dataset statistics. Considering a relation of the form [ $v$ used-for $u$ ], we define [ $v$ used-for ?] as forward, and [? used-for $u$ ] as backward.</p>
<h2>3 SciMON Models</h2>
<p>We present a new module to retrieve inspirations as contextual input (§3.1). Then, we describe another module to generate ideas given the context+inspiration (§3.2). Finally, we introduce a new iterative novelty optimization method to further improve idea quality (§3.3). ${ }^{5}$</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h3>3.1 Inspiration Retrieval Module</h3>
<p>We take broad inspiration from cognitive aspects of innovation (Hope et al., 2023): when researchers generate a new idea, they are grounded in a web of existing concepts and papers bearing on the new idea. We aim to enrich the context of each background by retrieving "inspirations"- pieces of information that can guide hypothesis generation. As illustrated in Figure 2, for a given instance of the SciMON task, our retrieval augmentation can retrieve from three types of sources. Each source uses a different form of query and output.</p>
<p>Semantic Neighbors For a given problem/motivation as input, ideas proposed for related problems in the training set can serve as a guiding reference for generating a new idea. Given the background context $\mathcal{B}$ with a seed term $v$ and problem/motivation $\mathcal{M}$, we construct a base input $b$ : a concatenation of $\mathcal{M}$ with a prompt $\mathcal{P}$ belonging to one of two templates: " $v$ is used for $p$ " or " $v$ is done by using $p$ ", where $p$ is one of Task/Method/Material/Metric. In short, $b:=$ $\mathcal{P} \oplus$ context: $\mathcal{M}$. For example, in Figure 2, the concatenation is "Knowledge acquisition is done by using Method; Context:...requires plms to integrate information...lifelong manner...".</p>
<p>We then retrieve inputs from the training set that are semantically related to a new base input $b$, and obtain target sentences $T$ corresponding to each retrieved training input. We extract the target term $t \in \mathcal{T}$ matching the seed term in $b$ (§2.2) as inspiration for input $b$. Simply put, this means we use as inspiration the salient aspect of the solution proposed in $\mathcal{T}$, which we found empirically to help remove noisy/irrelevant information in $\mathcal{T}$. For example, in Figure 2, we find "informative entities are done by using Method context: in this work, we aim at equipping pre-trained language models with structured knowledge." as similar to the input and use $t=$ "linked knowledge graph" as inspiration.</p>
<p>Technically, we first construct a fully connected graph $\mathcal{G}<em i="i">{S}$ based on the training set where each node is a pair of input text $b</em>}$ and target term $t_{i}$. We define the weight between two nodes $i$ and $j$ as the cosine similarity between $b_{i}$ and $b_{j}$ based on representations from SentenceBERT (Reimers and Gurevych, 2019) (all-mpnet-base-v2). Given $b$, we first insert it into $\mathcal{G<em 1="1">{S}$ and compute the weights of its connected edges. We then retrieve neighbors input text $\left{b</em>\right}$ from the training set with the largest edge weight, where $k$ is the number of}, \ldots, b_{k</p>
<p>retrieved instances. We consider the corresponding target terms ${t_{1}, \ldots, t_{k}}$ as semantic inspirations.</p>
<p>KG Neighbors We also explore enriching the context by linking it to a background KG with information on related methods and tasks. Using the same IE process used to extract our training examples (§2.2), we create a global background $\mathrm{KG} \mathcal{G}<em _mathcal_Y="\mathcal{Y">{B}$ which covers all papers in the corpus $\mathcal{D}</em>}}$ prior to a given year $\mathcal{Y}$ (i.e., the nodes in $\mathcal{G<em 1="1">{B}$ correspond to tasks/methods/materials/metrics, and the edges are used-for relations, extracted and normalized from across the entire corpus as described earlier). Then, given a seed term $v$ at query time, we select adjacent nodes $\left{n</em>$ as inspirations. As an example, in Figure 2, the neighbor nodes of "knowledge acquisition" include "collaborative web text annotation editor", "image matching", etc., which we select as inspirations.}, n_{2}, \ldots\right}$ from $\mathcal{G}_{B</p>
<p>Citation Neighbors Another notion of contextual relatedness we explore is via citation graph links. Here, given as input background context $\mathcal{B}$, we assume access to the original source document $d$ from which $\mathcal{B}$ was extracted, and consider its cited paper title set $\mathcal{C}<em _mathcal_Y="\mathcal{Y" d="d">{d}$ as potential candidates. This can be seen as a stronger assumption on information available to the model- assuming a researcher using the model provides relevant candidate documents from which ideas could be pooled. Because the training set only contains papers before year $\mathcal{Y}$, we only select papers $\mathcal{C}</em>}} \subseteq \mathcal{C<em _mathcal_Y="\mathcal{Y" d="d">{d}$ prior to year $\mathcal{Y}$. We then retrieve the top- $k$ titles with the highest cosine similarity to $d$ from $\mathcal{C}</em>$ based on their SentenceBERT embeddings as earlier. For instance, in Figure 2, the paper ELLE (Qin et al., 2022) cites the paper (de Masson d'Autume et al., 2019). Therefore, we choose the title "episodic memory in lifelong language learning" as inspiration information.}</p>
<h3>3.2 Generation Module</h3>
<p>The idea generation module is given retrieved inspirations $i_{1}, \ldots, i_{k}$ along with context $\mathcal{M}$ as input.</p>
<p>In-Context Learning We experiment with recent state-of-the-art LLMs, GPT3.5 davinci-003 (Ouyang et al., 2022) and GPT4 gpt-4-0314 checkpoint (OpenAI, 2023). We first ask the model to generate sentences based on the seed term and the context in the zero-shot setting without any in-context examples (GPT3.5ZS, GPT4ZS). We then ask the model to generate sentences in a few-shot setting by prompting randomly chosen pairs of input and output from the training set (GPT3.5FS, GPT4FS). Inspired by Liu et al. (2022), we further employ a few-shot setting using semantically similar examples. Instead of random in-context examples, we use the top- $k$ examples from the training set with the highest cosine similarity to the query (GPT3.5Retr). This few-shot retrieval setting differs from the semantic neighbor discussed above, in that we provide both the input and output of each instance rather than solely supplying target entities as additional input.</p>
<p>Fine Tuning We fine-tune T5 (Raffel et al., 2020) (more recent models may be used too; see our biomedical experiment $\S 4.1$ fine-tuning an LLM). We observe that the generation models tend to copy phrases from the background context. For example, given the context "...hierarchical tables challenge numerical reasoning ...", the model will generate "hierarchical table reasoning for question answering" as the top prediction. For generating suggestions of novel ideas, we wish to discourage overly copying from the background context. We introduce a new in-context contrastive objective, where negative examples are taken from the text in the input (e.g., in Figure 2, the in-context negatives are plms, pretraining, etc). We compute an InfoNCE loss (Oord et al., 2018) over the hidden states of the decoder, aiming to maximize the probability of the ground truth against those of in-context negatives:</p>
<p>$$
\begin{aligned}
y^{+} &amp; =\sigma\left(\operatorname{Avg}\left(\mathbf{W}<em y="y">{y} \mathbf{h}^{+}+\mathbf{b}</em>\right)\right) \
y_{k}^{-} &amp; =\sigma\left(\operatorname{Avg}\left(\mathbf{W}<em k="k">{y} \mathbf{h}</em>}^{-}+\mathbf{b<em _mathrm_cl="\mathrm{cl">{y}\right)\right) \
\mathcal{L}</em>
\end{aligned}
$$}} &amp; =\frac{\exp \left(y^{+} / \tau\right)}{\sum_{k} \exp \left(y_{k}^{-} / \tau\right)+\exp \left(y^{+} / \tau\right)</p>
<p>where $\mathbf{h}^{+}$and $\mathbf{h}<em y="y">{k}^{-}$are decoder hidden states from the positive and $k$-th negative samples, $\mathbf{W}</em>}$ and $\mathbf{b<em _mathrm_cl="\mathrm{cl">{y}$ are learnable parameters, $\sigma$ is a sigmoid function, $\tau$ is a temperature hyperparameter, and $\operatorname{Avg}(*)$ denotes the average pooling function based on the target sequence length. We optimize with both contrastive loss $\mathcal{L}</em>$ and the cross-entropy loss.}</p>
<h3>3.3 Iterative Novelty Boosting with Retrieval</h3>
<p>We further improve the novelty of generated ideas with a new iterative retrieve-compare-update scheme. Conceptually, we consider a noveltyinducing penalty $\gamma_{\text {nov }}(\mathcal{I}, \mathcal{R})$ that penalizes ideas $\mathcal{I}$ that are too "close" to existing ideas in literature reference examples $\mathcal{R} . \gamma_{\text {nov }}(\mathcal{I}, \mathcal{R})$ is included</p>
<p>during in-context learning and inference, providing numerical feedback in the form of a score reflecting similarity to existing work. We wish to minimize this score while ensuring $\mathcal{I}$ remains relevant to the background context $\mathcal{B}$; we do so iteratively by (1) retrieving related work from $\mathcal{R}$, (2) measuring degree of novelty, (3) instructing the model to update $\mathcal{I}$ to be more novel w.r.t $\mathcal{R}$, conditioning on $\mathcal{B}$.</p>
<p>Specifically, in our implementation, we construct a reference corpus $\mathcal{R}$ based on all papers in the training set. We then propose an iterative algorithm that compares generated ideas against $\mathcal{R}$. We start with the initial idea $\mathcal{I}<em t="t">{0}$ generated by the generation module. At each time step $t$, we use the generated idea $\mathcal{I}</em>}$ as a query to retrieve $k$ nearest ideas from the literature reference corpus $\mathcal{R}=\left{R_{1}, \ldots, R_{k}\right}$ based on SentenceBERT, with the top- $k$ highest cosine similarity scores to $\mathcal{I<em i="i">{t}$ (we use $k=20$ ). For each retrieved ground truth literature idea $R</em>$ are lower than $\mu$. Figure 2 and Table 5 demonstrate novelty iterations.}$, we compare its cosine similarity score $S_{i}$ against a threshold $\mu$ (we use 0.6 ). We provide all the retrieved ground truth ideas $\hat{\mathcal{R}}$ that pass the threshold as additional negative examples for the large language models with the following instruction prompt: "Your idea has similarities with existing research as demonstrated by these $j$ sentences: $\hat{\mathcal{R}}$ Make sure the idea you suggest is significantly different from the existing research mentioned in the above sentences. Let's give it another try." We stop the iteration once all $S_{i</p>
<h2>4 Experiments</h2>
<h3>4.1 Human Evaluation</h3>
<p>We present four human evaluation studies, exploring different facets of our problem and approach.</p>
<h3>4.1.1 Study I: Comparing Outputs across Model Variants</h3>
<p>We recruit six volunteer NLP experts with graduatelevel education to rate the system. Raters are told to envision an AI assistant that suggests new paper ideas. We randomly select 50 instances (background+seed) from the gold subset. Each annotator receives ten instances, each paired with system outputs from different model variants (Table 2). We ask raters to assess idea quality by considering each output's relevance to the context, novelty, clarity, and whether the idea is reasonable (positive ratings are dubbed "helpful" as shorthand, indicating they pass the multiple considerations). We observe moderately high rater agreement. ${ }^{6}$ Raters are blind to the condition, and system outputs are randomly shuffled across instances.</p>
<p>We instruct annotators to only provide positive ratings to ideas sufficiently different from the input context. In Study I, we ask raters not to anticipate groundbreaking novelty from the system but rather a narrower expectation of quality and utility; in Study II below, we enrich the analysis to examine ranking between top models and also "raise the bar" and compare to actual ideas from papers. ${ }^{7}$</p>
<p>In a preliminary experiment, we also collected human ratings for GPT4-ZS (zero-shot) vs. GPT4-FS (few-shot) using the same criteria, finding GPT4-FS ranked higher in $65 \%$ of cases, with the rest mostly tied; thus, zero-shot GPT-4 was left out of the remainder of study I and subsequent studies to reduce annotation effort and cost.</p>
<p>Results Overall, GPT4FS and GPT4FS+KG outperform other models by a wide margin (Table 2). Apart from GPT4, T5+SN+CL performs best compared to other baselines, given its stronger prior knowledge of useful similar background hypotheses. In general, GPT3.5 models performed worse than fine-tuned T5 and its variants, which echoes results in other work in the scientific NLP domain (Jimenez Gutierrez et al., 2022). GPT4 outputs tended to be longer, which may partially explain higher human preference.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Type</th>
<th style="text-align: center;">3 FS</th>
<th style="text-align: center;">3 Rt</th>
<th style="text-align: center;">3 FS + CT</th>
<th style="text-align: center;">3 FS + KG</th>
<th style="text-align: center;">4</th>
<th style="text-align: center;">4+KG</th>
<th style="text-align: center;">T5</th>
<th style="text-align: center;">T5+SN</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">H</td>
<td style="text-align: center;">33</td>
<td style="text-align: center;">25</td>
<td style="text-align: center;">16</td>
<td style="text-align: center;">33</td>
<td style="text-align: center;">73</td>
<td style="text-align: center;">66</td>
<td style="text-align: center;">22</td>
<td style="text-align: center;">48</td>
</tr>
<tr>
<td style="text-align: center;">U</td>
<td style="text-align: center;">67</td>
<td style="text-align: center;">75</td>
<td style="text-align: center;">84</td>
<td style="text-align: center;">67</td>
<td style="text-align: center;">27</td>
<td style="text-align: center;">34</td>
<td style="text-align: center;">78</td>
<td style="text-align: center;">52</td>
</tr>
</tbody>
</table>
<p>Table 2: Percent (\%) of total votes each system output receives from human raters. $H$ denotes a helpful output, while $U$ denotes an unhelpful output. "3FS" refers to the GPT3. 5FS. "3Rt" refers to the GPT3. 5Retr. "4" refers to GPT4FS, and "4+KG" refers to the GPT4FS+KG. "T5+SN" refers to the T5+SN+CL. GPT4FS and GPT4FS+KG are rated much higher. While GPT4FS has a slightly higher rating than the KG variant, a further human study reveals that GPT4FS+KG often leads to more technical depth (§4.1).</p>
<h3>4.1.2 Study II: Comparing GPT4 Variants against Real Papers</h3>
<p>We conduct a follow-up human study of close competitors GPT4FS and GPT4FS+KG with a subset of</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>the annotators to evaluate the incrementality and novelty of the generated ideas. In this study, model outputs are now ranked, unlike the binary classification of helpful/not in Study I. Suggestions are ranked according to the level of technical detail and innovation in comparison to each other-i.e., ranking which of GPT4FS and GPT4FS+KG had a higher degree of technical detail and novelty, or whether they are roughly the same (tied). Finally, outputs are rated versus the ground truth idea, according to whether or not the suggestions were roughly at the same level of technical detail and innovation as the original paper's idea, or significantly lower.</p>
<p>Results Overall, GPT4FS+KG is found to have higher technical detail in $48 \%$ of the compared pairs, and found to be less incremental (more novel) in $45 \%$ of the pairs. Among the remaining $52 \% / 55 \%$ (respectively), the vast majority are ties, indicating that whenever GPT4FS+KG is not favored, it is of roughly the same quality as GPT4FS, but not vice versa. However, the most crucial aspect is comparing the results against the original ground truth idea on the quality of innovation. Here, we find that in $85 \%$ of comparisons, the ground truth is considered to have significantly higher technical level and novelty; and in the remaining $15 \%$, the ground truth was ambiguous or lacking additional context from the paper abstract. This points to a major challenge in obtaining high-quality idea generations using existing state-of-the-art models.</p>
<h3>4.1.3 Study III: Evaluation on Iterative Novelty Boosting</h3>
<p>We conduct a fine-grained evaluation of our novelty mechanism with qualitative and quantitative evaluation of novelty. Specifically, we ask five annotators to further compare the novelty-enhanced results against the initially generated ideas. We randomly select 70 instances (background+seed) from the sentence generation gold subset. We ask annotators to check whether the new ideas are different than the initial ideas (e.g., adding new information or approaches), and whether they are more novel (i.e., a new idea can be different, but not necessarily more novel). Since GPT4FS+SN outperforms other models, for this model, we further instruct annotators to compare the novelty of the second iteration results against the first iteration results.</p>
<p>Results For SN, in the first iteration $88.9 \%$ of updated ideas are substantially different from initial ideas, and for $55.6 \%$ we are able to increase nov-</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Type</th>
<th style="text-align: center;">GPT4FS</th>
<th style="text-align: center;">+SN</th>
<th style="text-align: center;">+CT</th>
<th style="text-align: center;">+KG</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">1st Novelty $\Delta(\%)$</td>
<td style="text-align: center;">+54.4</td>
<td style="text-align: center;">+55.6</td>
<td style="text-align: center;">+47.8</td>
<td style="text-align: center;">+46.7</td>
</tr>
<tr>
<td style="text-align: left;">2nd Novelty $\Delta(\%)$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">+57.8</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">1st new terms $\Delta$</td>
<td style="text-align: center;">+23.1</td>
<td style="text-align: center;">+22.8</td>
<td style="text-align: center;">+22.1</td>
<td style="text-align: center;">+21.9</td>
</tr>
<tr>
<td style="text-align: left;">2nd new terms $\Delta$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">+21.5</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
</tbody>
</table>
<p>Table 3: Relative improvements of iterative novelty boosting. Iterations are applied to the ideas for which sufficiently similar related work is detected (§3.3). "1st Novelty" is \% of the 1st iteration ideas that gained novelty over the initial idea, and "2nd Novelty" is the \% of gain over the 1st iteration. Our method substantially increases novelty for ideas to which it is applied. To save annotation resources, we only annotate second iteration results for the best-performing method (SN). We report the average number of new terms added, after filtering.
elty/creativity (meaning that, e.g., if 100 examples were updated, we would gain 56 examples that are more novel). The 2nd iteration, further increases novelty for $57.8 \%$ of the ideas that continued to another iteration. For ideas not considered more novel after applying our method, we do not observe a drop in novelty-the method either increases or maintains novelty.</p>
<p>Ideas after novelty iterations are longer than initial ideas. We examine the new terms added after filtering 359 words, including stopwords, as many generic words and terms are often added (e.g., "novel model/method/approach"). While our method helps boost novelty, overall the model often tends to suggest combinations between popular concepts (§4.2). Novelty boosting seemed to often focus on adding dynamic/adaptive modeling, graph models and representations, the fusion of multiple modalities and sources-and sometimes all at once (e.g., "Dynamic Syntax-Aware Graph Fusion Networks (DSAGFN)"), and to explicitly compare against existing ideas from literature (Table 5).</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Type</th>
<th style="text-align: center;">Meditron</th>
<th style="text-align: center;">+SN</th>
<th style="text-align: center;">+CT</th>
<th style="text-align: center;">+KG</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Helpful(\%)</td>
<td style="text-align: center;">35</td>
<td style="text-align: center;">80</td>
<td style="text-align: center;">60</td>
<td style="text-align: center;">50</td>
</tr>
<tr>
<td style="text-align: left;">Unhelpful(\%)</td>
<td style="text-align: center;">65</td>
<td style="text-align: center;">20</td>
<td style="text-align: center;">40</td>
<td style="text-align: center;">50</td>
</tr>
<tr>
<td style="text-align: left;">vs. GT(\%)</td>
<td style="text-align: center;">30</td>
<td style="text-align: center;">45</td>
<td style="text-align: center;">50</td>
<td style="text-align: center;">35</td>
</tr>
</tbody>
</table>
<p>Table 4: Human evaluations results of each system output for the idea sentence prediction task on Biomedical Domain. "vs. GT" refers to percents which system outputs are better than ground truth ideas.</p>
<h3>4.1.4 Domain Generalization Case Study</h3>
<p>Our domain-agnostic framework can be applied to other domains by changing the IE system used in the preprocessing procedure. To demonstrate</p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Content</th>
</tr>
</thead>
<tbody>
<tr>
<td>Input (Dong et al., 2022)</td>
<td>seed term: speech unit boundaries ; context (abridged): ... generate partial sentence translation given a streaming speech input. existing approaches ... break the acoustic units in speech, as boundaries between acoustic units in speech are not even. ...</td>
</tr>
<tr>
<td>Initial idea</td>
<td>A pause prediction model to identify speech unit boundaries ...</td>
</tr>
<tr>
<td>Iteration 1</td>
<td>A method that leverages acoustic and linguistic features to predict speech unit boundaries dynamically, ensuring smooth transitions ... differs from the existing research as it combines both acoustic properties and linguistic context ... adapting to variations in speaker characteristics, speaking styles, and languages.</td>
</tr>
<tr>
<td>Iteration 2</td>
<td>A novel method called Adaptive Speech Unit Boundary Detection (ASUBD) ... a combination of attention mechanisms to focus on relevant acoustic and linguistic features and reinforcement learning to guide the system to make optimal predictions of unit boundaries based on previous decisions...</td>
</tr>
<tr>
<td>Ground Truth</td>
<td>... an efficient monotonic segmentation module ... accumulate acoustic information incrementally and detect proper speech unit boundaries.</td>
</tr>
</tbody>
</table>
<p>Table 5: Example of iterative novelty iterations. Our novelty iteration method enhances ideas overall; however ideas are often based on superficial recombinations of common concepts, far from the technical depth of scientific papers.
this, we conduct an additional initial experiment in the biochemical domain. We follow a similar data creation procedure as for NLP papers. We collect a dataset from PubMed papers and use PubTator 3 (Islamaj et al., 2021; Wei et al., 2022; Luo et al., 2023; Wei et al., 2023; Lai et al., 2023) as an IE system to extract a KG from paper abstracts. We use a sentence classifier trained on annotated abstracts (Huang et al., 2020) to select background context. We fine-tune a state-of-the-art biomedical large language model (Chen et al., 2023) on our data and evaluate on a test split past its pre-training cutoff date. ${ }^{8}$ We ask two biochemical domain experts with graduate-level education to evaluate the quality of the results as before, finding them to overall rate $80 \%$ of the generated directions positively. Finally, in contrast to NLP-domain experiments, evaluators were more satisfied with the generated outputs than the ground truth regarding technical detail. Detailed results are in Table 4. However, this preliminary experiment was meant mainly to demonstrate the generality of our approach, and a more in-depth exploration of utility and quality is left for future work.</p>
<h3>4.2 Error Analysis</h3>
<p>Models often made generic suggestions, woven together with specific details copied directly from the context (e.g., "NLP with ML algorithms and sentiment analysis" for some problem X, or "data augmentation and transfer learning" for Y, or "BERT or RoBERTa" for Z). Our techniques reduced this behavior but did not fully solve it. GPT4 models, especially, seemed to generate generic descriptions of common steps in NLP workflows (e.g., "Data preprocessing: Clean the text data, remove unnec-</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup>essary characters, perform tokenization..."). All models often copied and rephrased directly from the context. In certain cases, models applied simple logical modifications to the context; e.g., when contexts described problems such as "high latency" or "efficiency limitations", the suggestions would include phrases such as "low latency" or "highly efficient".</p>
<h3>4.3 Automated Evaluation Analysis</h3>
<p>In open-ended tasks such as ours, automatic evaluations comparing system output to ground truth texts may be limited. Nonetheless, automated metrics such as ROUGE (Lin, 2004), BERTScore (Zhang* et al., 2020) and BARTScore (Yuan et al., 2021), that check the similarity between ground truth and generated output, may surface interesting findings. We find GPT-based models to be outperformed by T5-based models; GPT4 outputs are much longer than T5, explaining why they underperform in automatic metrics but outperform in human evaluations (§4.1). Generated sentences often follow certain templates (e.g., "In this paper, we propose a new ... for ..."), which also helps explain why T5 fine-tuned on many examples scores higher superficially. At the same time, our in-context contrastive examples which encourage novelty with respect to background context, helped models perform better than baseline fine-tuning by reducing reliance on copying. See results in Table 9 (Appendix B.4).</p>
<h2>5 Conclusions and Future Directions</h2>
<p>We propose a new setting, model and comprehensive evaluation for scientific hypothesis generation with language models that are grounded in literature and optimized for novelty. We present a new framework named SciMON in which mod-</p>
<p>els take background problem contexts and provide suggestions that are novel while based on literature. Models retrieve inspirations from semantic similarity graphs, knowledge graphs, and citation networks. We introduce a new iterative novelty boosting mechanism that helps large language models (LLMs) such as GPT-4 generate more novel ideas by explicitly comparing ideas to prior work and refining them. Our experiments demonstrate that the task of generating natural language scientific hypotheses is highly challenging. While our methods improve upon baseline LLMs, generated ideas tend to be incremental and with insufficient detail. Generating novel and meaningful scientific concepts and their compositions remains a fundamental problem (Hope et al., 2023). Evaluation in this setting is also highly challenging, with a huge space of potentially plausible hypotheses formulated in natural language. One interesting direction is to expand SCIMON with a multimodal analysis of formulas, tables, and figures to provide a more comprehensive background context.</p>
<h2>6 Limitations</h2>
<p>We discuss limitations extensively throughout the paper, such as in terms of evaluation challenges and data quality. Here we include additional details on limitations.</p>
<h3>6.1 Limitations of Data Collection</h3>
<p>We crawled papers with Semantic Scholar Academic Graph API from 1952 to June 2022. The number of available papers is limited by the data we crawled from the Semantic Scholar Academic Graph. We also crawled papers from PubMed 1988 to 2024/01. We remove papers that are not English. We also remove papers where abstracts are not correctly parsed from paper PDFs. We will expand our models to papers written in other languages and other domains in the future.</p>
<h3>6.2 Limitations of System Performance</h3>
<p>Our dataset is based on state-of-the-art IE systems, which may be noisy. For instance, the coreference and SciSpacy abbreviation resolution models fail to link A2LCTC to Action-to-Language Connectionist Temporal Classification. The background context detection may also have errors: e.g., the sentence classification component fails to treat "For example, the language models are overall more positive towards the stock market, but there are significant
differences in preferences between a pair of industry sectors, or even within a sector." as background context. In our human-vetted gold data subset, we make sure to filter such cases, but they remain in the training data. SentenceBert (Reimers and Gurevych, 2019), and GPT3.5/4 are not finetuned and might be biased towards pretraining datasets. The idea novelty boosting method is limited by the quality of retrieval models. Better retrieval models may be explored in the future. Due to hardware constraints, we mainly investigated models with up to 7 billion parameters. Due to API change and model randomness, our GPT3.5/4 results might not be easily reproducible.</p>
<h3>6.3 Limitations of Evaluation</h3>
<p>We recruit annotators from Ph.D. students; their opinions may differ from annotators who have different levels of domain knowledge. Our setting uses a seed term taken from the ground truth as input, to emulate a scenario where a human provides guidance to an assistant model. Future work could explore methods in the setting without a seed term, an even harder task, or evaluate in an interactive setting with user-provided seed terms. In addition, while the seed is sampled from the ground truth, in our human-annotated gold subset, we make sure that in no case does the input context trivially leak the output.</p>
<h3>6.4 Memorization Check</h3>
<p>Carlini et al. (2023) reports that LLMs tend to memorize part of their training data, a well-known concern in evaluating current LLMs. Therefore, we examine the pretraining data of each model:</p>
<ul>
<li>T5: Raffel et al. (2020) shows that T5 is pretrained on C 4 which was crawled from web prior to April 2019.</li>
<li>GPT3.5: Based on the documentation, ${ }^{9}$ GPT3.5 series is pretrained on a combination of test and code from before Q4 2021.</li>
<li>GPT4: OpenAI (2023) shows that the GPT-4 checkpoint we used utilizes most pertaining data before September 2021. Despite this, the pretraining and post-training data contain "a small amount" of more recent data. ${ }^{10}$</li>
</ul>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Because we evaluate our models on papers published in 2022, the likelihood of test papers appearing in the pretraining corpora for the models is substantially reduced. We additionally performed a manual examination of GPT-4 memorization in our gold set based on 2022 ACL Anthology papers, by seeing if GPT-4 could complete information such as method names or generate text that strongly mimics the ground truth papers, and found no evidence of this occurring. The Meditron-7b (Chen et al., 2023) uses PubMed with a cut-off in August 2023, and our biochemical test set only includes PubMed papers after 2023/08.</p>
<h2>Acknowledgements</h2>
<p>This work is supported by the Molecule Maker Lab Institute: an AI research institute program supported by NSF under award No. 2019897, by DOE Center for Advanced Bioenergy and Bioproducts Innovation U.S. Department of Energy, Office of Science, Office of Biological and Environmental Research under Award Number DESC0018420, by U.S. the AI Research Institutes program by National Science Foundation and the Institute of Education Sciences, Department of Education through Award No. 2229873 - AI Institute for Transforming Education for Children with Speech and Language Processing Challenges, and by AI Agriculture: the Agriculture and Food Research Initiative (AFRI) grant no. 2020-67021- 32799/project accession no. 1024178 from the USDA National Institute of Food and Agriculture. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies, either expressed or implied of, the National Science Foundation, the U.S. Department of Energy, and the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for governmental purposes notwithstanding any copyright annotation therein.</p>
<h2>References</h2>
<p>Daniil A Boiko, Robert MacKnight, Ben Kline, and Gabe Gomes. 2023. Autonomous chemical research with large language models. Nature, 624(7992):570578 .</p>
<p>Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child,</p>
<p>Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. In Advances in Neural Information Processing Systems, volume 33, pages 1877-1901. Curran Associates, Inc.</p>
<p>Nicholas Carlini. 2023. A llm assisted exploitation of ai-guardian. Cryptography and Security Repository, arXiv:2307.15008.</p>
<p>Nicholas Carlini, Daphne Ippolito, Matthew Jagielski, Katherine Lee, Florian Tramer, and Chiyuan Zhang. 2023. Quantifying memorization across neural language models. In The Eleventh International Conference on Learning Representations.</p>
<p>Arie Cattan, Sophie Johnson, Daniel S Weld, Ido Dagan, Iz Beltagy, Doug Downey, and Tom Hope. 2021. Scico: Hierarchical cross-document coreference for scientific concepts. In 3rd Conference on Automated Knowledge Base Construction.</p>
<p>Zeming Chen, Alejandro Hernández Cano, Angelika Romanou, Antoine Bonnet, Kyle Matoba, Francesco Salvi, Matteo Pagliardini, Simin Fan, Andreas Köpf, Amirkeivan Mohtashami, et al. 2023. Meditron70b: Scaling medical pretraining for large language models. Computation and Language Repository, arXiv:2311.16079.</p>
<p>Arman Cohan, Iz Beltagy, Daniel King, Bhavana Dalvi, and Dan Weld. 2019. Pretrained language models for sequential sentence classification. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 3693-3699, Hong Kong, China. Association for Computational Linguistics.</p>
<p>Cyprien de Masson d'Autume, Sebastian Ruder, Lingpeng Kong, and Dani Yogatama. 2019. Episodic memory in lifelong language learning. In Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc.</p>
<p>Qian Dong, Yaoming Zhu, Mingxuan Wang, and Lei Li. 2022. Learning when to translate for streaming speech. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 680-694.</p>
<p>Sam Henry and Bridget T McInnes. 2017. Literature based discovery: models, methods, and trends. Journal of biomedical informatics, 74:20-32.</p>
<p>Tom Hope, Doug Downey, Oren Etzioni, Daniel S Weld, and Eric Horvitz. 2023. A computational inflection for scientific discovery. Communications of the ACM.</p>
<p>Zhe Hu, Zuohui Fu, Yu Yin, and Gerard de Melo. 2021. Context-aware interaction network for question matching. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 3846-3853, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.</p>
<p>Qian Huang, Jian Vora, Percy Liang, and Jure Leskovec. 2023. Benchmarking large language models as ai research agents. Machine Learning Repository, arXiv:2310.03302.</p>
<p>Ting-Hao Kenneth Huang, Chieh-Yang Huang, ChienKuang Cornelia Ding, Yen-Chia Hsu, and C. Lee Giles. 2020. CODA-19: Using a non-expert crowd to annotate research aspects on 10,000+ abstracts in the COVID-19 open research dataset. In Proceedings of the 1st Workshop on NLP for COVID-19 at ACL 2020, Online. Association for Computational Linguistics.</p>
<p>Rezarta Islamaj, Robert Leaman, Sun Kim, Dongseop Kwon, Chih-Hsuan Wei, Donald C. Comeau, Yifan Peng, David Cissel, Cathleen Coss, Carol Fisher, Rob Guzman, Preeti Gokal Kochar, Stella Koppel, Dorothy Trinh, Keiko Sekiya, Janice Ward, Deborah Whitman, Susan Schmidt, and Zhiyong Lu. 2021. Nlm-chem, a new resource for chemical entity recognition in pubmed full text literature. Scientific Data, 8(1):91.</p>
<p>Bernal Jimenez Gutierrez, Nikolas McNeal, Clayton Washington, You Chen, Lang Li, Huan Sun, and Yu Su. 2022. Thinking about GPT-3 in-context learning for biomedical IE? think again. In Findings of the Association for Computational Linguistics: EMNLP 2022, pages 4497-4512, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.</p>
<p>Po-Ting Lai, Chih-Hsuan Wei, Ling Luo, Qingyu Chen, and Zhiyong Lu. 2023. Biorex: Improving biomedical relation extraction by leveraging heterogeneous datasets. Journal of Biomedical Informatics, 146:104487.</p>
<p>Chin-Yew Lin. 2004. ROUGE: A package for automatic evaluation of summaries. In Text Summarization Branches Out, pages 74-81, Barcelona, Spain. Association for Computational Linguistics.</p>
<p>Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, and Weizhu Chen. 2022. What makes good in-context examples for GPT-3? In Proceedings of Deep Learning Inside Out (DeeLIO 2022): The 3rd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures, pages 100-114, Dublin, Ireland and Online. Association for Computational Linguistics.</p>
<p>Kyle Lo, Lucy Lu Wang, Mark Neumann, Rodney Kinney, and Daniel Weld. 2020. S2ORC: The semantic scholar open research corpus. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4969-4983, Online. Association for Computational Linguistics.</p>
<p>Ilya Loshchilov and Frank Hutter. 2019. Decoupled weight decay regularization. In Proceedings of the 7th International Conference on Learning Representations.</p>
<p>Yi Luan, Luheng He, Mari Ostendorf, and Hannaneh Hajishirzi. 2018. Multi-task identification of entities, relations, and coreference for scientific knowledge graph construction. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 3219-3232, Brussels, Belgium. Association for Computational Linguistics.</p>
<p>Ling Luo, Chih-Hsuan Wei, Po-Ting Lai, Robert Leaman, Qingyu Chen, and Zhiyong Lu. 2023. AIONER: all-in-one scheme-based biomedical named entity recognition using deep learning. Bioinformatics, 39(5):btad310.</p>
<p>Rahul Nadkarni, David Wadden, Iz Beltagy, Noah A Smith, Hannaneh Hajishirzi, and Tom Hope. 2021. Scientific language models for biomedical knowledge base completion: an empirical study. $A K B C$.</p>
<p>Mark Neumann, Daniel King, Iz Beltagy, and Waleed Ammar. 2019. ScispaCy: Fast and robust models for biomedical natural language processing. In Proceedings of the 18th BioNLP Workshop and Shared Task, pages 319-327, Florence, Italy. Association for Computational Linguistics.</p>
<p>Allen Newell and Herbert Simon. 1956. The logic theory machine-a complex information processing system. IRE Transactions on information theory, 2(3):61-79.</p>
<p>Harsha Nori, Nicholas King, Scott Mayer McKinney, Dean Carignan, and Eric Horvitz. 2023. Capabilities of GPT-4 on medical challenge problems. Computation and Language Repository, arXiv:2303.13375.</p>
<p>Aaron van den Oord, Yazhe Li, and Oriol Vinyals. 2018. Representation learning with contrastive predictive coding. Machine Learning Repository, arXiv:1807.03748.</p>
<p>OpenAI. 2023. Gpt-4 technical report. Computation and Language Repository, arXiv:2303.08774.</p>
<p>Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35:27730-27744.</p>
<p>Yujia Qin, Jiajie Zhang, Yankai Lin, Zhiyuan Liu, Peng Li, Maosong Sun, and Jie Zhou. 2022. ELLE: Efficient lifelong pre-training for emerging data. In Findings of the Association for Computational Linguistics: ACL 2022, pages 2789-2810, Dublin, Ireland. Association for Computational Linguistics.</p>
<p>Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the</p>
<p>limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research, 21(140):1-67.</p>
<p>Nils Reimers and Iryna Gurevych. 2019. SentenceBERT: Sentence embeddings using Siamese BERTnetworks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 3982-3992, Hong Kong, China. Association for Computational Linguistics.</p>
<p>Herbert A Simon. 1973. Does scientific discovery have a logic? Philosophy of science, 40(4):471-480.</p>
<p>Daniel N Sosa and Russ B Altman. 2022. Contexts and contradictions: a roadmap for computational drug repurposing with knowledge inference. Briefings in Bioinformatics, 23(4):bbac268.</p>
<p>Don R Swanson. 1986. Undiscovered public knowledge. The Library Quarterly, 56(2):103-118.</p>
<p>Justin Sybrandt, Ilya Tyagin, Michael Shtutman, and Ilya Safro. 2020. Agatha: automatic graph mining and transformer based hypothesis generation approach. In Proceedings of the 29th ACM International Conference on Information \&amp; Knowledge Management, pages 2757-2764.</p>
<p>Vahe Tshitoyan, John Dagdelen, Leigh Weston, Alexander Dunn, Ziqin Rong, Olga Kononova, Kristin A Persson, Gerbrand Ceder, and Anubhav Jain. 2019. Unsupervised word embeddings capture latent knowledge from materials science literature. Nature, 571(7763):95-98.</p>
<p>Qingyun Wang, Lifu Huang, Zhiying Jiang, Kevin Knight, Heng Ji, Mohit Bansal, and Yi Luan. 2019. PaperRobot: Incremental draft generation of scientific ideas. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 1980-1991, Florence, Italy. Association for Computational Linguistics.</p>
<p>Alex Warstadt, Aaron Mueller, Leshem Choshen, Ethan Wilcox, Chengxu Zhuang, Juan Ciro, Rafael Mosquera, Bhargavi Paranjabe, Adina Williams, Tal Linzen, et al. 2023. Findings of the babylm challenge: Sample-efficient pretraining on developmentally plausible corpora. In Proceedings of the BabyLM Challenge at the 27th Conference on Computational Natural Language Learning, pages 1-34.</p>
<p>Chih-Hsuan Wei, Alexis Allot, Kevin Riehle, Aleksandar Milosavljevic, and Zhiyong Lu. 2022. tmvar 3.0: an improved variant concept recognition and normalization tool. Bioinformatics, 38(18):4449-4451.</p>
<p>Chih-Hsuan Wei, Ling Luo, Rezarta Islamaj, Po-Ting Lai, and Zhiyong Lu. 2023. GNorm2: an improved gene name recognition and normalization system. Bioinformatics, 39(10):btad599.</p>
<p>Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander Rush. 2020. Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 38-45, Online. Association for Computational Linguistics.</p>
<p>Yi Xu, Shuqian Sheng, Bo Xue, Luoyi Fu, Xinbing Wang, and Chenghu Zhou. 2023. Exploring and verbalizing academic ideas by concept co-occurrence. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 13001-13027, Toronto, Canada. Association for Computational Linguistics.</p>
<p>Deming Ye, Yankai Lin, Peng Li, and Maosong Sun. 2022. Packed levitated marker for entity and relation extraction. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 4904-4917, Dublin, Ireland. Association for Computational Linguistics.</p>
<p>Weizhe Yuan, Graham Neubig, and Pengfei Liu. 2021. BARTScore: Evaluating generated text as text generation. In Advances in Neural Information Processing Systems.</p>
<p>Tianyi Zhang<em>, Varsha Kishore</em>, Felix Wu*, Kilian Q. Weinberger, and Yoav Artzi. 2020. Bertscore: Evaluating text generation with bert. In Proceedings of the 8th International Conference on Learning Representations.</p>
<p>Ran Zhou, Xin Li, Ruidan He, Lidong Bing, Erik Cambria, Luo Si, and Chunyan Miao. 2022. MELM: Data augmentation with masked entity language modeling for low-resource NER. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2251-2262, Dublin, Ireland. Association for Computational Linguistics.</p>
<h2>A Dataset Collection</h2>
<h2>A. 1 NLP Dataset Collection</h2>
<p>We download ACL Anthology papers from 1952 to 2022 using Semantic Scholar Academic Graph API. ${ }^{11}$ We filter out papers without abstracts and not written in English to obtain 67,408 papers. Our dataset has 58,874 papers before 2021, 5,946 papers from 2021, and 2,588 from 2022. We first use PL-Marker (Ye et al., 2022) pretrained on SciERC (Luan et al., 2018) to extract nodes belonging to six types: Task, Method, Evaluation Metric, Material, Other Scientific Terms, and Generic</p>
<p><sup id="fnref6:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Terms. The model then predicts relations between nodes belonging to seven relation types: Usedfor, Feature-of, Evaluate-for, Hyponym-of, Partof, Compare, and Conjunction. Because we want to generate new ideas, we focus on used-for relations in papers. Next, we use SciCo (Cattan et al., 2021) with checkpoint from Hugging Face ${ }^{12}$ to obtain entity coreference to merge identical nodes. Then, we use ScispaCy (Neumann et al., 2019) to perform unsupervised abbreviation detection to replace the abbreviation with a more informative long form. Finally, we perform scientific sentence classification (Cohan et al., 2019) ${ }^{13}$ to classify sentences from the abstract into five categories including Background, Method, Objective, Other, and Result. We select sentences with labels of Background and Other as background context. During preprocessing, we only keep high-confidence outputs from IE models. Figure 4 shows an example of the IE systems pipeline.</p>
<h2>A. 2 Biochemical Dataset Collection</h2>
<p>We collect PubMed papers from 1988 to 2024 using Entrez Programming Utilities API ${ }^{14}$ for the following topics, including Yarrowia, Saccharomyces cerevisiae, Issatchenkia orientalis, and Rhodosporidium toruloides. We use PubTator 3 (Islamaj et al., 2021; Wei et al., 2022; Luo et al., 2023; Wei et al., 2023; Lai et al., 2023). The PubTator 3 performs named entity recognition, relation extraction, entity coreference and linking, and entity normalization for the abstracts in the dataset. PubTator 3 identifies bio entities belonging to seven types: gene, chemical, chromosome, cell line, variant, disease, and speciesl and relations belonging to 13 types: associate, cause, compare, convert, contract, drug interact, inhibit, interact, negative correlate, positive correlate, prevent, stimulate, and treat. Finally, we use a sentence classifier trained on CODA-19 (Huang et al., 2020) to classify sentences in abstracts into background, purpose, method, finding, and other. We select sentences with labels of background as background context and remove sentences with labels of other. We treat the rest sentences that have at least one entity as the target sentence. We only keep samples with low similarity between background context</p>
<p><sup id="fnref7:0"><a class="footnote-ref" href="#fn:0">1</a></sup>and corresponding ground truth sentences. ${ }^{15}$ Our final dataset has 4,767 papers before 2023/02, 642 papers from 2023/02 to 2023/08, and 299 papers after 2023/08.</p>
<h2>B Finetuning and Automated Evaluation details</h2>
<h2>B. 1 Inspiration Retrieval Module</h2>
<p>The statistics of each inspiration type are in Table 7. Table 8 shows sample retrieved inspirations.</p>
<h2>B.1.1 Semantic Neighbors</h2>
<p>We use all-mpnet-base-v2 from SentenceBert (Reimers and Gurevych, 2019), which performs best in semantic search to retrieve similar nodes from the training set based on query $q$ in $\S 3.1$. We retrieve up to 20 relevant semantic neighbors $\mathcal{R}$ from the training set for each instance. We treat the target nodes from $\mathcal{R}$ as semantic neighbors.</p>
<h2>B.1.2 KG Neighbors</h2>
<p>We use one-hop connected neighbors from the background $\mathrm{KG} \mathcal{G}_{B}$ constructed on papers before 2021(i.e., the papers in the training set). Because of the scarcity of KG neighbors, we do not limit the number of KG neighbors.</p>
<h2>B.1.3 Citation Neighbors</h2>
<p>Similar to semantic neighbors, we use all-mpnet-base-v2 from SentenceBert (Reimers and Gurevych, 2019) to retrieve cited paper titles similar to query $q$. We restrict cited papers only before 2021. We retrieve up to 5 relevant citation neighbors from the papers' citation network.</p>
<h2>B. 2 Generation Module</h2>
<p>Our T5 model and their variants are built based on the Huggingface framework (Wolf et al., 2020). ${ }^{16}$ We optimize those models by AdamW (Loshchilov and Hutter, 2019) with the linear warmup scheduler. ${ }^{17}$ Those models are finetuned on 4 NVIDIA A6000 48GB GPUs with distributed data parallel. ${ }^{18}$ The training time for each model is about 10 hours.</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Preprocessing result for Hu et al. (2021) in non-canonicalized KG Corpus</p>
<table>
<thead>
<tr>
<th>Stage</th>
<th>PL-Maker Entities</th>
<th>PL-Maker Used-for Relations</th>
<th>SciCo Coreference</th>
<th>Scispacy Abbreviation Detection</th>
<th>Sentence Classification</th>
</tr>
</thead>
<tbody>
<tr>
<td>Precision</td>
<td>91.3</td>
<td>65.4</td>
<td>97.2</td>
<td>100</td>
<td>100</td>
</tr>
</tbody>
</table>
<p>Table 6: Human quality evaluation of preprocessing stages(%). Overall pass rate after all steps are applied is 79.7%.</p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Train</th>
<th>Valid</th>
<th>Test</th>
</tr>
</thead>
<tbody>
<tr>
<td>SN</td>
<td>10.8</td>
<td>10.0</td>
<td>10.0</td>
</tr>
<tr>
<td>KG</td>
<td>8.3</td>
<td>8.0</td>
<td>8.1</td>
</tr>
<tr>
<td>CT</td>
<td>4.9</td>
<td>5.0</td>
<td>5.0</td>
</tr>
</tbody>
</table>
<p>Table 7: Average of # of neighbors for each instance, excluding those which do not have any neighbor</p>
<h3>B.2.1 In-Context Learning</h3>
<p>We choose GPT3.5 davinci-003 (Brown et al., 2020) as our out-of-the-box causal language modeling baseline. We select 5 instances from the training set as examples for the few-shot setting. We randomly select those examples for GPT3.5FS. For GPT3.5Retr, similar to semantic neighbors, we use all-mpnet-base-v2 from SentenceBert (Reimers and Gurevych, 2019), which performs best in semantic search to retrieve similar instances from the training set based on query q in §3.1. The input length is limited to 2048 tokens due to OpenAI API limits. We choose gpt-4-0314 as our GPT4 model. Our input for GPT4 is similar to GPT3.5.</p>
<p>For each selected example from the training set with forward relation, the template is "Consider the following context: M In that context, which p can be used for v, and why? T", where M is the background context, p is the target node type, v is the seed term, and T is the target idea sentence; for backward relation, the template is "Consider the following context: M In that context, which p do we use v, and why? s". For selected examples with additional retrieval inspirations, we concatenate the following additional template to the M: "The retrieval results are: i_{1}, . . , i_{k}", where i_{1}, . . , i_{k} are retrieved inspirations. For the final prompt, the template is similar to the above example template. However, the target sentence T will not be included. We ask the model to generate 10 outputs. We will select the best output and skip the empty output.</p>
<h3>B.2.2 Fine Tuning</h3>
<p>Given input without any inspirations, the input combines the prompt P and context M as shown in §3.1 (i.e., P | context: M). Given input with inspirations, the input is P | retrieve: i_{1}, . . , i_{k} | context: M, with i_{1}, . . , i_{k} as retrieved inspirations. The input length is limited to 512 tokens. For both tasks, we finetune our model based on T5-large with a learning rate of 6 × 10^{−6} and ε = 1 × 10^{−6}. The batch size is 8 for each GPU. The maximum training epoch for all models is 10 with 4 patience. During decoding, we use beam-search to generate results with a beam size of 5 and a repetition penalty of 1.5.</p>
<h4>In-context Contrastive Augmentation</h4>
<p>We randomly select 2 sentences that appeared in the input as in-context negatives. For example, in Figure 1, the in-context negatives could be "knowledge acquisition is done by using Method", "this requires plms to integrate the information from all the sources in a lifelong manner."</p>
<p>B.2.3 Biochemical Case Study</p>
<p>Our Meditron-7b (Chen et al., 2023) and its variants are built based on the Huggingface framework (Wolf et al., 2020). We use its epfl-llm/meditron-7b as the base model. We finetune those models with a learning rate of $2 \times 10^{-6}$ and $\epsilon=5 \times 10^{-8}$. The maximum training epoch for all models is 5 . All models are finetuned on 4 NVIDIA A100 80 GB GPUs with Fully Sharded Data Parallel. ${ }^{21}$ The training time for each model is about 20 hours.</p>
<h2>B. 3 The Scale of Retrieval Set</h2>
<p>We retrieve from a set of 59 k papers with over 374 k sentences in the NLP domain, the focus of our experiments. Our background KG built on the training set has more than 197k nodes and 261k relations. Moreover, we collect 87 k paper titles from citation networks. This represents a large-scale and diverse domain; retrieving inspirations from this set is expected, in principle, to be more than enough for generating novel ideas. Indeed, NLP papers typically cite each other and build on each other as inspirations to create new ideas - which motivates our inspiration retrieval.</p>
<h2>B. 4 Automated Evaluation</h2>
<p>We use BERTScore (Zhang* et al., 2020) with SciBERT checkpoint for both tasks. The hash of the checkpoint is allenai/scibert_scivocab_uncased_L8 _no-idf_version=0.3.12(hug_trans=4.19.2). The automated evaluation results are in Table 9.</p>
<h2>C Human Annotation and Evaluation Details</h2>
<p>Gold Dataset Annotation Details The gold dataset annotation interface is in Figure 5. The quality of the instances in the test set is judged given three criteria: (1) whether the ground truth sentence trivially overlaps with background context; (2) whether background context contains relevant information for the target relation; (3) whether the target relation (from which the seed term is taken) is a salient aspect of the idea proposed in the target paper.</p>
<p>Study I The instructions for human evaluation can be found in Figure 6, while an example of the</p>
<p><sup id="fnref8:0"><a class="footnote-ref" href="#fn:0">1</a></sup>human evaluation interface is provided in Figure 7 and 8. Human annotators are required to evaluate each system output based on the following criteria: (1) Is the candidate relevant to the context + seed term? (2) Does the candidate copy too much from the context, or is it sufficiently novel/different from the context? (3) Does the candidate's suggestion generally make sense to you scientifically? (4) Is the language sufficiently clear and coherent to understand the suggestion? The input for sample human annotation is in Table 10 and the human labels are in Table 11. The human annotation agreement is in Table 13.</p>
<p>Study III We ask the following questions to human annotators to evaluate the quality of regeneration results: (1) Is the regenerated idea substantially different from the original? (2) Is the regenerated idea more novel and creative than the original idea? (3) Does the second iteration increase novelty? The human annotation agreement is in Table 14.</p>
<h2>D Scientific Artifacts</h2>
<p>We list the licenses of the scientific artifacts used in this paper: Semantic Scholar Academic Graph API (API license agreement ${ }^{22}$ ), Huggingface Transformers (Apache License 2.0), SBERT (Apache2.0 license), BERTScore (MIT license), Meditron7b (Llama2), Entrez Programming Utilities API (Copyright ${ }^{23}$ ), PubTator 3 (Data use policy ${ }^{24}$ ), and OpenAI (Terms of use ${ }^{25}$ ).</p>
<h2>E Ethical Consideration</h2>
<p>The SciMON task and corresponding models we have designed in this paper are limited to the natural language processing (NLP) and biochemical domain, and might not apply to other scenarios.</p>
<h2>E. 1 Usage Requirement</h2>
<p>This paper aims to provide investigative leads for a scientific domain, specifically natural language processing. The final results are not intended to be used without human review. Accordingly, domain experts might use this tool as a research writing assistant to develop ideas. However, our system does not do any fact-checking with external knowledge. In addition, we train our models on the ACL</p>
<p><sup id="fnref2:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Content</th>
</tr>
</thead>
<tbody>
<tr>
<td>Seed Term Prompt</td>
<td>data augmentation is used for Task</td>
</tr>
<tr>
<td>Context</td>
<td>data augmentation is an effective solution to data scarcity in low - resource scenarios, however, when applied to token-level tasks such as ner, data augmentation methods often suffer from token-label misalignment, which leads to unsatisfactory performance.</td>
</tr>
<tr>
<td>Semantic Neighbors</td>
<td>st and automatic speech recognition (asr), low-resource tagging tasks, end-to-end speech translation, neural online chats response selection, neural machine translation, semi-supervised ner, entity and context learning, semi-supervised setting, dependency parsing, low-resource machine translation, slot filling, dialog state tracking, visual question answering, visual question answering (vqa), low-resource neural machine translation</td>
</tr>
<tr>
<td>KG Neighbors</td>
<td>nmi-based text normalization, task-oriented dialog systems, task-oriented dialogue system, low-resource languages (lrl), end-to-end speech translation, visual question answering (vqa), multiclass utterance classification, clinical semantic textual similarity, neural online chats response selection, context-aware neural machine translation</td>
</tr>
<tr>
<td>Citation Neighbors</td>
<td>Contextual Augmentation: Data Augmentation by Words with Paradigmatic Relations, An Analysis of Simple Data Augmentation for Named Entity Recognition, Data Augmentation for Low-Resource Neural Machine Translation, DAGA: Data Augmentation with a Generation Approach for Low-resource Tagging Tasks, EDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classification Tasks</td>
</tr>
<tr>
<td>Ground Truth</td>
<td>ELM: Data Augmentation with Masked Entity Language Modeling for Low-Resource NER</td>
</tr>
</tbody>
</table>
<p>Table 8: Example (from (Zhou et al., 2022)) of retrieved inspirations. Inspirations similar to ground truth are underlined.</p>
<table>
<thead>
<tr>
<th>input</th>
<th>context</th>
<th>entity</th>
<th>output</th>
<th>relation</th>
<th>rel_sent</th>
<th>is the output trivially overlap with the context</th>
<th>IE is of sufficient quality (not generic, correct)</th>
<th>context contains relevant information for target relation (Conservative filter only flag cases where context is highly irrelevant)</th>
<th>Relation is a part of the main idea propose d by the paper</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>transformer - based language models usually treat texts as linear sequences. however, most texts also have an inherent hierarchical structure , i.e. , parts of a text can be identified using their position in this hierarchy . In addition , section titles usually indicate the common topic of their respective sentences.</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td>We propose a novel approach to formulate , extract , encode and inject hierarchical structure information explicitly into an extractive summarization model based on a pre -trained , encoder - only Transformer language model ( H2Struct+ model ), which improves SCFA ROUGEs for extractive summarization on PubMed and artXx substantially .</td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p>Figure 5: Gold subset annotation interface
anthology and PubMed papers written in English, which might alienate readers who have been historically underrepresented in the NLP/biochemical domains.</p>
<h3>E. 2 Data Collection</h3>
<p>We collect 67,408 ACL Anthology papers from 1952 to 2022 using Semantic Scholar Academic Graph API, under API license agreement ${ }^{26}$. We ensure our data collection procedure follows the Terms of Use at https://allenai.org/terms. According to the agreement, our dataset can only be used for non-commercial purposes. As mentioned in $\S 4$, we perform the human evaluation. All</p>
<p><sup id="fnref9:0"><a class="footnote-ref" href="#fn:0">1</a></sup>annotators involved in human evaluation are voluntary participants with a fair wage. We further collect 5,708 PubMed papers from 1988 to 2024 using Entrez Programming Utilities API ${ }^{27}$. We follow their data usage guidelines ${ }^{28}$.</p>
<p><sup id="fnref3:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: center;">Subset</th>
<th style="text-align: center;">Challenging</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Gold</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Model</td>
<td style="text-align: center;">R-L $\uparrow$</td>
<td style="text-align: center;">BERT $\uparrow$</td>
<td style="text-align: center;">R-L $\uparrow$</td>
<td style="text-align: center;">BERT $\uparrow$</td>
</tr>
<tr>
<td style="text-align: center;">GPT4ZS</td>
<td style="text-align: center;">0.120</td>
<td style="text-align: center;">0.581</td>
<td style="text-align: center;">0.130</td>
<td style="text-align: center;">0.583</td>
</tr>
<tr>
<td style="text-align: center;">GPT4FS</td>
<td style="text-align: center;">0.143</td>
<td style="text-align: center;">0.618</td>
<td style="text-align: center;">0.151</td>
<td style="text-align: center;">0.624</td>
</tr>
<tr>
<td style="text-align: center;">TS</td>
<td style="text-align: center;">0.223</td>
<td style="text-align: center;">$0.672^{\dagger}$</td>
<td style="text-align: center;">0.246</td>
<td style="text-align: center;">0.685</td>
</tr>
<tr>
<td style="text-align: center;">GPT4FS+SN</td>
<td style="text-align: center;">0.144</td>
<td style="text-align: center;">$0.620^{-}$</td>
<td style="text-align: center;">0.149</td>
<td style="text-align: center;">0.627</td>
</tr>
<tr>
<td style="text-align: center;">GPT4FS+KG</td>
<td style="text-align: center;">0.143</td>
<td style="text-align: center;">0.619</td>
<td style="text-align: center;">0.152</td>
<td style="text-align: center;">0.626</td>
</tr>
<tr>
<td style="text-align: center;">GPT4FS+CT</td>
<td style="text-align: center;">0.144</td>
<td style="text-align: center;">0.617</td>
<td style="text-align: center;">0.149</td>
<td style="text-align: center;">0.622</td>
</tr>
<tr>
<td style="text-align: center;">TS+CL</td>
<td style="text-align: center;">$0.225^{\dagger}$</td>
<td style="text-align: center;">$0.671^{\dagger}$</td>
<td style="text-align: center;">$0.251^{\dagger}$</td>
<td style="text-align: center;">$0.686^{\dagger}$</td>
</tr>
<tr>
<td style="text-align: center;">TS+SN+CL</td>
<td style="text-align: center;">$0.228^{\dagger}$</td>
<td style="text-align: center;">$0.671^{\dagger}$</td>
<td style="text-align: center;">$0.258^{\dagger}$</td>
<td style="text-align: center;">$0.686^{\dagger}$</td>
</tr>
<tr>
<td style="text-align: center;">TS+KG+CL</td>
<td style="text-align: center;">$0.223^{\dagger}$</td>
<td style="text-align: center;">0.669</td>
<td style="text-align: center;">0.248</td>
<td style="text-align: center;">$0.681^{\dagger}$</td>
</tr>
<tr>
<td style="text-align: center;">TS+CT+CL</td>
<td style="text-align: center;">$0.225^{\dagger}$</td>
<td style="text-align: center;">$0.671^{\dagger}$</td>
<td style="text-align: center;">$0.250^{\dagger}$</td>
<td style="text-align: center;">$0.686^{\dagger}$</td>
</tr>
</tbody>
</table>
<p>Table 9: Automatic evaluation results for the challenging and gold subsets. $C L$ is a model with in-context contrastive augmentation. $S N$ is a model with semantic inspirations. $K G$ is a model with KG inspirations. $C T$ is a model with citation inspirations. $R$-L denotes ROUGE-L. BERT denotes BERTscore with SciBERT as its encoder. ${ }^{\dagger}$ indicates that differences between models are not statistically significant ( $p \leq 0.05$ ) when compared to each other but are still significant when compared to the other models based on t-test.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Type</th>
<th style="text-align: left;">Content</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Seed Term Prompt</td>
<td style="text-align: left;">Irish language learning is done by using Method</td>
</tr>
<tr>
<td style="text-align: left;">Context</td>
<td style="text-align: left;">Irish is a minority language which means that l2 learners have limited opportunities for exposure <br> to the language, and additionally, there are also limited (digital) learning resources available.</td>
</tr>
</tbody>
</table>
<p>Table 10: Input for sample human annotation results</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">Output</th>
<th style="text-align: center;">Label</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">GPT3.SFS</td>
<td style="text-align: center;">The use of Social Media in Irish Language Learning: A Case Study</td>
<td style="text-align: center;">helpful</td>
</tr>
<tr>
<td style="text-align: center;">GPT3.SRetr</td>
<td style="text-align: center;">One method that could be used for Irish language learning is computer assisted language learning (CALL)</td>
<td style="text-align: center;">unhelpful</td>
</tr>
<tr>
<td style="text-align: center;">GPT3.SFS+CT</td>
<td style="text-align: center;">In that context, the most appropriate task for precision grammar would be the development of a part-of-speech tagger for Irish using finite-state morphology and constraint grammar disambiguation.</td>
<td style="text-align: center;">unhelpful</td>
</tr>
<tr>
<td style="text-align: center;">GPT3.SFS+KG</td>
<td style="text-align: center;">Clo: An Irish Language Learning Platform - Leveraging Digital Resources and Human Expertise</td>
<td style="text-align: center;">unhelpful</td>
</tr>
<tr>
<td style="text-align: center;">GPT4FS</td>
<td style="text-align: center;">We propose using a technology-enhanced approach, such as computer-assisted language learning (CALL) or natural language processing (NLP) techniques, to create interactive and engaging Irish language learning resources, as these methods can provide personalized learning experiences and enhance learners' exposure to the language despite limited resources available.</td>
<td style="text-align: center;">helpful</td>
</tr>
<tr>
<td style="text-align: center;">GPT4FS+KG</td>
<td style="text-align: center;">Exploiting Social Media for Irish Language Learning: An Analysis of Twitter Data. In this context, we use social media data, particularly from Twitter, as a method for Irish language learning, because it provides a rich source of authentic and diverse language examples that can be used to enhance learning opportunities for L2 learners in a minority language setting.</td>
<td style="text-align: center;">helpful</td>
</tr>
<tr>
<td style="text-align: center;">TS</td>
<td style="text-align: center;">In this paper, we present a novel machine translation approach to Irish language learning. A conversational virtual environment for Irish language learning</td>
<td style="text-align: center;">unhelpful <br> helpful</td>
</tr>
</tbody>
</table>
<p>Table 11: Sample human labels for idea-sentence generation results for seed term prompt "Irish language learning is done by using Method", with context "irish is a minority language ... limited digital learning resources available".</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Type</th>
<th style="text-align: center;">Content</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Seed Term</td>
<td style="text-align: center;">ARO10</td>
</tr>
<tr>
<td style="text-align: center;">Context</td>
<td style="text-align: center;">Huangjiu is known for its unique aroma, primarily attributed to its high concentration of bphenylethanol (ranging from 40 to $130 \mathrm{mg} / \mathrm{L}$ ).</td>
</tr>
<tr>
<td style="text-align: center;">Méditron</td>
<td style="text-align: center;">We then selected six key genes involved in the phenylalanine metabolism pathway and predicted that their product flux may affect the synthesis of b-phenylethanol.</td>
</tr>
<tr>
<td style="text-align: center;">Méditron + CT</td>
<td style="text-align: center;">We found that the key amino acid residue that controls the activity of Aro10p was not conserved in wine yeast strains, which may explain the lower b-phenylethanol production in wine fermentation compared with that in Chinese huangjiu.</td>
</tr>
<tr>
<td style="text-align: center;">Méditron + KG</td>
<td style="text-align: center;">Both target genes, SSA1 and ARO10, were deleted using the CRISPR-Cas9 genome editing system.</td>
</tr>
<tr>
<td style="text-align: center;">Méditron + SN</td>
<td style="text-align: center;">Herein, we report that the key barrier for b-phenylethanol production in Huangjiu is ARO10, the only bi-functional amino acid decarboxylase in Saccharomyces cerevisiae.</td>
</tr>
</tbody>
</table>
<p>Table 12: Input and idea-sentence generation results for seed gene " $A R O 10$ " in the biochemical domain.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Annotator Pair</th>
<th style="text-align: center;">$1-2$</th>
<th style="text-align: center;">$1-3$</th>
<th style="text-align: center;">$1-4$</th>
<th style="text-align: center;">$1-5$</th>
<th style="text-align: center;">$1-6$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Agreement \%</td>
<td style="text-align: center;">68.8</td>
<td style="text-align: center;">75.0</td>
<td style="text-align: center;">56.2</td>
<td style="text-align: center;">43.8</td>
<td style="text-align: center;">75.0</td>
</tr>
</tbody>
</table>
<p>Table 13: Percent (\%) of same labels from overlapped 10 human evaluation instances on each pair of annotators for Study I.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Annotator Pair</th>
<th style="text-align: center;">$1-2$</th>
<th style="text-align: center;">$1-3$</th>
<th style="text-align: center;">$1-4$</th>
<th style="text-align: center;">$1-5$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Agreement \%</td>
<td style="text-align: center;">92.5</td>
<td style="text-align: center;">93.3</td>
<td style="text-align: center;">87.5</td>
<td style="text-align: center;">90.0</td>
</tr>
</tbody>
</table>
<p>Table 14: Percent (\%) of same labels from overlapped 20 human evaluation instances on each pair of annotators for Study III. (1-3) has 60 shared questions. The rest of the pairs each share 40 questions.</p>
<h1>Rank scientific idea suggestions generated by an Al paper-writing assistant</h1>
<p>Your goal in this task is to rank idea suggestions written by an AI assistant. The AI assistant helps its users write paper abstracts by writing sentences with proposals for new ideas or questions to consider. You are first given:</p>
<ol>
<li>A context which describes relevant background in a specific area of interest. e.g., background on efficiency issues in large language model training.</li>
<li>A seed term that should be a focus of the generated scientific idea. see example below.</li>
</ol>
<p>Then, you are given a list of 5 candidate suggestions - each generated by the AI assistant using different approaches. Each candidate is written in the form of a sentence from a scientific paper abstract.</p>
<p>Your goal is to score the candidate suggestions with a helpful/unhelpful answer. Your scoring decision should consider the overall quality of each candidate, using considerations that include:</p>
<ol>
<li>Is the candidate relevant to the context + seed term?</li>
<li>Does the candidate copy too much from the context, or is it sufficiently novel/different from the context?</li>
<li>Does the candidate's suggestion generally make sense to you scientifically?</li>
<li>Is the language sufficiently clear and coherent for you to understand the suggestion?</li>
</ol>
<p>The AI scientific assistant is a work in progress - don't expect it to generate highly-detailed ideas or breakthrough discoveries! Use the three guideline metrics above to come up with your overall rating. Read the following example that illustrates the task:</p>
<p>Context: the task of converting a natural language question into an executable sql query, known as text - to - sql, is an important branch of semantic parsing. the state - of - the - art graph - based encoder has been successfully used in this task but does not model the question syntax well.
Seed term: diverse relational edge embedding
System Outputs:</p>
<ul>
<li>We propose a novel technique, Diverse Relational Edge Embedding (DREE), to address this problem.</li>
<li>We propose a novel graph-based encoder that uses a diverse relational edge embedding to model the question syntax.</li>
<li>Diverse relational edge embedding is important for text-to-SQL parsing because it can help the parser to better understand the question syntax.</li>
<li>In this paper, we propose a novel graph - based encoder for diverse relational edge embedding.</li>
<li>In this paper, we propose a novel graph - based encoder for text - to - sql ( ttsql ) that incorporates the question syntax into diverse relational edge embedding.</li>
</ul>
<h2>Ranking:</h2>
<ul>
<li>unhelpful</li>
<li>helpful</li>
<li>helpful</li>
<li>unhelpful</li>
<li>helpful</li>
</ul>
<p>Figure 6: Human evaluation instructions</p>
<p>Your goal is to score the candidate suggestions with a helpful/unhelpful answer. * Your scoring decision should consider the overall quality of each candidate, using considerations that include:
Is the candidate relevant to the context + seed term?</p>
<p>Does the candidate copy too much from the context, or is it sufficiently novel/different from the context?</p>
<p>Does the candidate's suggestion generally make sense to you scientifically?</p>
<p>Is the language sufficiently clear and coherent for you to understand the suggestion?</p>
<p>Context: while pretrained language models achieve excellent performance on natural language understanding benchmarks, they tend to rely on spurious correlations and generalize poorly to out - of - distribution ( ood ) data. recent work has explored using counterfactually - augmented data ( cad)-data generated by minimally perturbing examples toflip the ground - truth label - to identify robust features that are invariant under distribution shift
Seed term: diverse perturbation of examples
unhelpful
helpful
Diverse perturbation of examples is used in order to generate counterfactual data that can help identify robust features that are invariant under distribution shift.</p>
<p>A counterfactual generator for diverse perturbation of examples.</p>
<p>We propose a method for generating CAD by diverse perturbation of examples.</p>
<p>In this paper, we propose a counterfactually augmented data ( cad) model that is robust to diverse perturbation of examples.</p>
<p>We use diverse perturbation of examples to flip the ground-truth label in order to identify robust features that are invariant under distribution shift.</p>
<p>Figure 7: Human evaluation example for GPT3.5Rnd, GPT3.5Retr, GPT3.5Rnd+CT, T5, and T5+SN+CL</p>
<p>Your goal is to score the candidate suggestions with a helpful/unhelpful answer. * Your scoring decision should consider the overall quality of each candidate, using considerations that include:
Is the candidate relevant to the context + seed term?</p>
<p>Does the candidate copy too much from the context, or is it sufficiently novel/different from the context?</p>
<p>Does the candidate's suggestion generally make sense to you scientifically?</p>
<p>Is the language sufficiently clear and coherent for you to understand the suggestion?</p>
<p>Context: while pretrained language models achieve excellent performance on natural language understanding benchmarks, they tend to rely on spurious correlations and generalize poorly to out-of-distribution ( ood ) data. recent work has explored using counterfactually - augmented data ( cad)-data generated by minimally perturbing examples toflip the ground - truth label - to identify robust features that are invariant under distribution shift.
Seed term: diverse perturbation of examples</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: left;">unhelpful</th>
<th style="text-align: left;">helpful</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">In that context, we use <br> diverse perturbation of <br> examples to generate <br> counterfactually-augmented <br> data, which helps identify <br> robust features and improve <br> the model's generalization to <br> out-of-distribution data. This <br> approach minimizes the <br> model's reliance on spurious <br> correlations and enhances <br> its overall performance on <br> natural language <br> understanding tasks.</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">In that context, we use <br> diverse perturbation of <br> examples as a method <br> because it helps in <br> generating counterfactually- <br> augmented data (CAD), <br> which helps identify robust <br> features that are invariant <br> under distribution shift. By <br> focusing on robust features, <br> the model becomes less <br> reliant on spurious <br> correlations and can <br> generalize better to out-of- <br> distribution (OOD) data.</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
</tbody>
</table>
<p>Figure 8: Human evaluation example for GPT3.5Rnd+KG, GPT4Rnd, and GPT4Rnd+KG</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{26}$ https://api.semanticscholar.org/license/&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref6:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref7:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref8:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref9:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{27}$ www.ncbi.nlm.nih.gov/books/NBK25581/
${ }^{28}$ www.ncbi.nlm.nih.gov/books/about/copyright/&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:1" title="Jump back to footnote 2 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9259 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9259</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9259</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-163.html">extraction-schema-163</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <p><strong>Paper ID:</strong> paper-274937863</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2505.17037v1.pdf" target="_blank">Prompt Engineering: How Prompt Vocabulary affects Domain Knowledge</a></p>
                <p><strong>Paper Abstract:</strong> Prompt engineering has emerged as a critical component in optimizing large language models (LLMs) for domain-specific tasks. However, the role of prompt specificity, especially in domains like STEM (physics, chemistry, biology, computer science and mathematics), medicine, and law, remains underexplored. This thesis addresses the problem of whether increasing the specificity of vocabulary in prompts improves LLM performance in domain-specific question-answering and reasoning tasks. We developed a synonymization framework to systematically substitute nouns, verbs, and adjectives with varying specificity levels, measuring the impact on four LLMs: Llama-3.1-70B-Instruct, Granite-13B-Instruct-V2, Flan-T5-XL, and Mistral-Large 2, across datasets in STEM, law, and medicine. Our results reveal that while generally increasing the specificity of prompts does not have a significant impact, there appears to be a specificity range, across all considered models, where the LLM performs the best. Identifying this optimal specificity range offers a key insight for prompt design, suggesting that manipulating prompts within this range could maximize LLM performance and lead to more efficient applications in specialized domains.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9259.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9259.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Noun-specificity-synonymization</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Effect of noun-level prompt specificity via synonymization</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Systematic substitution of nouns in prompts with synonyms of low/intermediate/high specificity at replacement ratios (33%, 67%, 100%) and measurement of resulting LLM performance on MMLU, GPQA and GSM8K using a Jaccard-based accuracy metric.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama-3.1-70B-Instruct; Granite-13B-Instruct-V2; Flan-T5-XL; Mistral-Large 2</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>70B;13B;3B;123B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>MMLU (multiple-choice), GPQA (multiple-choice, PhD-level), GSM8K (math reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>MMLU: broad multi-subject multiple-choice benchmark across STEM, law, medicine; GPQA: expert-curated graduate-level multiple-choice science questions; GSM8K: grade-school multi-step arithmetic problems used with chain-of-thought reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Zero-shot prompt templates from datasets; prompts systematically synonymized by replacing nouns with synonyms categorized into low/intermediate/high specificity (computed from WordNet-based specificity score) at replacement ratios 33%, 67%, 100%. Deterministic decoding (temperature=0), max tokens=500.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Original (unmodified) prompts vs synonymized prompts across nine permutations (3 specificity levels × 3 replacement ratios).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Reported as relative accuracy change (Jaccard-based) versus original baseline; aggregated negative changes across models and datasets (exact baseline accuracies not fully enumerated in text). Example absolute values reported: Llama-3.1-70B-Instruct and Mistral-Large 2 achieve strong baseline reasoning performance on GSM8K (0.97 and 0.95 respectively) before synonymization.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Performance generally declines with increasing noun replacement ratio and specificity. Example effect sizes: Mistral-Large 2 shows up to -0.154 (i.e., -15.4 percentage points) change in accuracy at 100% noun replacement (low specificity) relative to baseline; Llama-3.1-70B-Instruct shows up to -0.117 (−11.7 percentage points) at 100% noun replacement (high specificity). Flan-T5-XL shows more resilience to noun replacement than larger models.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>-15.4% (Mistral, 100% noun replacement, low specificity); -11.7% (Llama-3.1, 100% noun replacement, high specificity); other cells in aggregated heatmaps show generally negative deltas as replacement ratio and specificity increase.</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Authors hypothesize that high-frequency, large-scale substitution of nouns with more specific synonyms changes referential surface forms and may move prompts away from distributions the models were trained on, harming zero-shot and CoT performance. Larger/more instruction-following models (Llama-3.1, Mistral) appear more sensitive to these noun substitutions, possibly due to differences in pretraining distribution or how they encode named/typed referents.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Synonymization pipeline: WSD per word-sense, retrieve hypernyms/hyponyms/lemmas, compute specificity (Eq.1 for nouns/verbs), categorize synonyms into low/intermediate/high (min/mean/max), and replace nouns at 33/67/100% replacement levels; evaluation metric: Jaccard similarity aggregated to accuracy; datasets sampled up to 250 instructions each (processed counts: GPQA 181, GSM8K 174, MMLU 1790 in preprocessing); decoding temp=0, max tokens=500, random seed=31415.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Prompt Engineering: How Prompt Vocabulary affects Domain Knowledge', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9259.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9259.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Verb-specificity-synonymization</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Effect of verb-level prompt specificity via synonymization</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Systematic substitution of verbs in prompts with synonyms of varying specificity and measurement of impacts on model accuracy, with particularly strong negative effects observed in reasoning tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama-3.1-70B-Instruct; Granite-13B-Instruct-V2; Flan-T5-XL; Mistral-Large 2</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>70B;13B;3B;123B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>MMLU (zero-shot), GPQA (zero-shot), GSM8K (zero-shot Chain-of-Thought)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same datasets as noun experiments; verbs replaced in prompts to assess changes in question-answering and reasoning accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Zero-shot for MMLU/GPQA; GSM8K used a zero-shot Chain-of-Thought template. Verbs were synonymized into low/intermediate/high specificity and replaced at 33/67/100% ratios.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Original prompts vs verb-synonymized prompts across specificity and replacement levels.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Overall negative trend, with several statistically significant negative correlations between prompt specificity and performance for verbs. Example baseline/relative numbers: models show declines in Jaccard-based accuracy as verb specificity and replacement ratio increase; Flan-T5-XL showed occasional positive accuracy changes at low replacement ratios/low specificity for verbs, but generally all models decline at 100% verb replacement.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Significant Spearman correlations between prompt specificity (continuous) and performance reported: Llama-3.1-70B-Instruct on MMLU verbs: r = -0.79 (p = 0.006); Flan-T5-XL on GSM8K verbs: r = -0.78 (p = 0.008); GSM8K verbs strong negatives: Llama-3.1 r = -0.89 (p = 5.4e-4), Mistral-Large 2 r = -0.87 (p = 0.001).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Correlation magnitudes up to |r|≈0.89 (very strong negative) for verb specificity vs performance on GSM8K for some models; no single percent-point effect size is given for all cells, but 100% replacement yields consistent negative deltas.</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Authors suggest verbs encode actions/relations crucial for stepwise reasoning; making verbs more specific can alter abstract relational structure that reasoning chains rely on, thereby disrupting model's chain-of-thought and multi-step problem solving. Pretraining distribution differences and syntactic-sensitivity are proposed as contributing factors.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Verb specificity computed with WordNet taxonomy (Eq.1). Evaluation used both categorical (ordinal buckets: 33% Low ... 100% High) and numerical (continuous prompt specificity averaged over prompt) approaches. Spearman correlations computed between prompt specificity and Jaccard-based accuracy; significance assessed (p-values reported).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Prompt Engineering: How Prompt Vocabulary affects Domain Knowledge', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9259.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9259.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GSM8K-CoT-format</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Use of zero-shot Chain-of-Thought (CoT) prompt format on GSM8K</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>For GSM8K arithmetic reasoning tasks the study used a zero-shot Chain-of-Thought (CoT) prompt template to solicit multi-step reasoning from models and measured baseline and perturbed performance under synonymization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama-3.1-70B-Instruct; Mistral-Large 2; Granite-13B-Instruct-V2; Flan-T5-XL</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>70B;123B;13B;3B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>GSM8K (Grade School Math 8K)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>8,500 grade-school level multi-step arithmetic word problems intended to test chain-of-thought reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Zero-shot Chain-of-Thought (CoT) prompt template (dataset-supplied template modified by synonymization experiments). Deterministic decoding (temperature=0).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>No explicit CoT vs non-CoT comparison in the paper; CoT was the chosen format for GSM8K and synonymization effects on CoT performance were measured.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Reported baseline strong performance for two models on GSM8K: Llama-3.1-70B-Instruct ≈ 0.97 accuracy; Mistral-Large 2 ≈ 0.95 accuracy (these reported values referenced in text).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Under synonymization (especially verb changes), strong negative correlations with prompt specificity observed on GSM8K for Llama-3.1 and Mistral (r = -0.89 and -0.87 respectively), indicating CoT reasoning is particularly sensitive to verb-specificity manipulations.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Correlation values up to -0.89; specific absolute accuracy drops for GSM8K under full replacement not enumerated for every model but described as substantial for verbs.</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Authors propose that CoT relies on maintained abstract action/operation semantics; changing verb specificity can perturb the intended step operations and intermediate representations, thereby degrading chain-of-thought reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>GSM8K experiments used zero-shot CoT prompt templates (appendix references), deterministic decoding temp=0, max tokens=500; verb-specificity effects measured via Spearman correlation between continuous prompt specificity and Jaccard-based accuracy; statistical significance reported (p-values).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Prompt Engineering: How Prompt Vocabulary affects Domain Knowledge', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9259.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e9259.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Categorical-vs-numerical-specificity</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Categorical (ordinal) vs numerical (continuous) prompt-specificity analysis</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Two analysis methods: (1) categorical grouping of nine permutations (3 specificity × 3 replacement ratios) into ordinal categories for coarse comparison, and (2) numerical aggregation of per-word specificity scores into continuous prompt-specificity followed by Spearman correlation with performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama-3.1-70B-Instruct; Granite-13B-Instruct-V2; Flan-T5-XL; Mistral-Large 2</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>70B;13B;3B;123B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>MMLU, GPQA, GSM8K (same datasets used for both analytical approaches)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>See individual dataset descriptions; categorical approach produces grouped average accuracies across categories; numerical approach computes Spearman correlations between continuous prompt specificity and accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Same underlying presentation (zero-shot templates and CoT for GSM8K) with prompts altered via synonymization; analysis differs in aggregation/measurement of specificity.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Categorical comparisons (33% Low ... 100% High) vs numerical Spearman correlation between continuous prompt specificity and accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Categorical analysis: general decrease in accuracy as replacement ratio and specificity increase (visualized in heatmaps); Numerical analysis: most Spearman correlations negative but only 5 of 24 evaluations reached statistical significance (p<0.05).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Notable significant results from numerical analysis: nouns — Granite-13B on GPQA r = -0.68 (p = 0.03); verbs — Llama-3.1 on MMLU r = -0.79 (p = 0.006); Flan-T5-XL on GSM8K r = -0.78 (p = 0.008); GSM8K verbs Llama r = -0.89 (p = 5.4e-4) and Mistral r = -0.87 (p = 0.001). Most other correlations had p > 0.05.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Categorical heatmaps show relative accuracy deltas (e.g., up to −15.4 percentage points reported for a specific cell); numerical approach effect sizes are correlation coefficients (reported above).</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Categorical approach gives coarse patterns (replacement & specificity increase → worse performance). Numerical approach refines this and shows that significant negative relationships are concentrated in verbs and in reasoning tasks; authors infer that treating prompt-specificity as continuous better captures subtle performance dependencies.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Categorical method: group nine permutations into ordinal labels and compute average Jaccard-based accuracy per group/model/dataset. Numerical method: compute per-prompt continuous prompt-specificity by averaging individual word-specificity scores for the part of speech, then compute Spearman correlation against accuracy (ten aggregated values per condition), p-values reported.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Prompt Engineering: How Prompt Vocabulary affects Domain Knowledge', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9259.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e9259.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>WSD-for-synonymization</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Use and evaluation of Word Sense Disambiguation (WSD) model for context-aware synonym selection</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The pipeline uses WSD to select context-appropriate WordNet synsets before computing specificity scores and synonym replacement; the study evaluated WSD model options and adopted Llama-3.1-70B-Instruct for WSD because it outperformed a fine-tuned T5 in sense selection accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama-3.1-70B-Instruct (used as WSD model); fine-tuned T5 (comparison)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>70B; unspecified for fine-tuned T5</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Word Sense Disambiguation evaluation (internal, 50 mixed samples nouns+verbs)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>WSD evaluation to choose which model to use for disambiguating word senses before synonym crawling and specificity computation.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>WSD applied to each candidate word in-context prior to synonym retrieval; compared WSD performance of Llama-3.1-70B-Instruct vs a fine-tuned T5 model.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Llama-3.1-70B-Instruct WSD vs fine-tuned T5 WSD.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>WSD accuracy: Llama-3.1-70B-Instruct = 0.79 ± 0.02; fine-tuned T5 = 0.63 ± 0.05 on 50 mixed-sample evaluation across three seeds.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>McNemar test comparing the two WSD models returned p = 0.003 (test statistic 21), indicating Llama-3.1 significantly outperformed fine-tuned T5 on this WSD sample.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>WSD accuracy improvement of ~16 percentage points (0.79 vs 0.63) in favor of Llama-3.1.</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Accurate WSD is critical to avoid selecting synonyms with incorrect senses that would change prompt meaning and confound LLM responses; adopting a stronger WSD reduces this source of noise in format-manipulation experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>WSD evaluation used 50 mixed noun/verb samples across three seeds; confusion matrix and McNemar test used to assess significance; Llama-3.1 chosen for speed and higher accuracy for the in-pipeline WSD step. The paper notes WSD still misclassifies some cases (≈21% error) and that human feedback or further fine-tuning could improve synonym selection.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Prompt Engineering: How Prompt Vocabulary affects Domain Knowledge', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Large Language Models are Zero-Shot Reasoners <em>(Rating: 2)</em></li>
                <li>Training Verifiers to Solve Math Word Problems <em>(Rating: 2)</em></li>
                <li>Prompt Programming for Large Language Models: Beyond the Few-Shot Paradigm <em>(Rating: 1)</em></li>
                <li>Prompt-Source: An Integrated Development Environment and Repository for Natural Language Prompts <em>(Rating: 1)</em></li>
                <li>Unleashing the potential of prompt engineering in Large Language Models: a comprehensive review <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9259",
    "paper_id": "paper-274937863",
    "extraction_schema_id": "extraction-schema-163",
    "extracted_data": [
        {
            "name_short": "Noun-specificity-synonymization",
            "name_full": "Effect of noun-level prompt specificity via synonymization",
            "brief_description": "Systematic substitution of nouns in prompts with synonyms of low/intermediate/high specificity at replacement ratios (33%, 67%, 100%) and measurement of resulting LLM performance on MMLU, GPQA and GSM8K using a Jaccard-based accuracy metric.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Llama-3.1-70B-Instruct; Granite-13B-Instruct-V2; Flan-T5-XL; Mistral-Large 2",
            "model_size": "70B;13B;3B;123B",
            "task_name": "MMLU (multiple-choice), GPQA (multiple-choice, PhD-level), GSM8K (math reasoning)",
            "task_description": "MMLU: broad multi-subject multiple-choice benchmark across STEM, law, medicine; GPQA: expert-curated graduate-level multiple-choice science questions; GSM8K: grade-school multi-step arithmetic problems used with chain-of-thought reasoning.",
            "presentation_format": "Zero-shot prompt templates from datasets; prompts systematically synonymized by replacing nouns with synonyms categorized into low/intermediate/high specificity (computed from WordNet-based specificity score) at replacement ratios 33%, 67%, 100%. Deterministic decoding (temperature=0), max tokens=500.",
            "comparison_format": "Original (unmodified) prompts vs synonymized prompts across nine permutations (3 specificity levels × 3 replacement ratios).",
            "performance": "Reported as relative accuracy change (Jaccard-based) versus original baseline; aggregated negative changes across models and datasets (exact baseline accuracies not fully enumerated in text). Example absolute values reported: Llama-3.1-70B-Instruct and Mistral-Large 2 achieve strong baseline reasoning performance on GSM8K (0.97 and 0.95 respectively) before synonymization.",
            "performance_comparison": "Performance generally declines with increasing noun replacement ratio and specificity. Example effect sizes: Mistral-Large 2 shows up to -0.154 (i.e., -15.4 percentage points) change in accuracy at 100% noun replacement (low specificity) relative to baseline; Llama-3.1-70B-Instruct shows up to -0.117 (−11.7 percentage points) at 100% noun replacement (high specificity). Flan-T5-XL shows more resilience to noun replacement than larger models.",
            "format_effect_size": "-15.4% (Mistral, 100% noun replacement, low specificity); -11.7% (Llama-3.1, 100% noun replacement, high specificity); other cells in aggregated heatmaps show generally negative deltas as replacement ratio and specificity increase.",
            "explanation_or_hypothesis": "Authors hypothesize that high-frequency, large-scale substitution of nouns with more specific synonyms changes referential surface forms and may move prompts away from distributions the models were trained on, harming zero-shot and CoT performance. Larger/more instruction-following models (Llama-3.1, Mistral) appear more sensitive to these noun substitutions, possibly due to differences in pretraining distribution or how they encode named/typed referents.",
            "null_or_negative_result": true,
            "experimental_details": "Synonymization pipeline: WSD per word-sense, retrieve hypernyms/hyponyms/lemmas, compute specificity (Eq.1 for nouns/verbs), categorize synonyms into low/intermediate/high (min/mean/max), and replace nouns at 33/67/100% replacement levels; evaluation metric: Jaccard similarity aggregated to accuracy; datasets sampled up to 250 instructions each (processed counts: GPQA 181, GSM8K 174, MMLU 1790 in preprocessing); decoding temp=0, max tokens=500, random seed=31415.",
            "uuid": "e9259.0",
            "source_info": {
                "paper_title": "Prompt Engineering: How Prompt Vocabulary affects Domain Knowledge",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "Verb-specificity-synonymization",
            "name_full": "Effect of verb-level prompt specificity via synonymization",
            "brief_description": "Systematic substitution of verbs in prompts with synonyms of varying specificity and measurement of impacts on model accuracy, with particularly strong negative effects observed in reasoning tasks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Llama-3.1-70B-Instruct; Granite-13B-Instruct-V2; Flan-T5-XL; Mistral-Large 2",
            "model_size": "70B;13B;3B;123B",
            "task_name": "MMLU (zero-shot), GPQA (zero-shot), GSM8K (zero-shot Chain-of-Thought)",
            "task_description": "Same datasets as noun experiments; verbs replaced in prompts to assess changes in question-answering and reasoning accuracy.",
            "presentation_format": "Zero-shot for MMLU/GPQA; GSM8K used a zero-shot Chain-of-Thought template. Verbs were synonymized into low/intermediate/high specificity and replaced at 33/67/100% ratios.",
            "comparison_format": "Original prompts vs verb-synonymized prompts across specificity and replacement levels.",
            "performance": "Overall negative trend, with several statistically significant negative correlations between prompt specificity and performance for verbs. Example baseline/relative numbers: models show declines in Jaccard-based accuracy as verb specificity and replacement ratio increase; Flan-T5-XL showed occasional positive accuracy changes at low replacement ratios/low specificity for verbs, but generally all models decline at 100% verb replacement.",
            "performance_comparison": "Significant Spearman correlations between prompt specificity (continuous) and performance reported: Llama-3.1-70B-Instruct on MMLU verbs: r = -0.79 (p = 0.006); Flan-T5-XL on GSM8K verbs: r = -0.78 (p = 0.008); GSM8K verbs strong negatives: Llama-3.1 r = -0.89 (p = 5.4e-4), Mistral-Large 2 r = -0.87 (p = 0.001).",
            "format_effect_size": "Correlation magnitudes up to |r|≈0.89 (very strong negative) for verb specificity vs performance on GSM8K for some models; no single percent-point effect size is given for all cells, but 100% replacement yields consistent negative deltas.",
            "explanation_or_hypothesis": "Authors suggest verbs encode actions/relations crucial for stepwise reasoning; making verbs more specific can alter abstract relational structure that reasoning chains rely on, thereby disrupting model's chain-of-thought and multi-step problem solving. Pretraining distribution differences and syntactic-sensitivity are proposed as contributing factors.",
            "null_or_negative_result": true,
            "experimental_details": "Verb specificity computed with WordNet taxonomy (Eq.1). Evaluation used both categorical (ordinal buckets: 33% Low ... 100% High) and numerical (continuous prompt specificity averaged over prompt) approaches. Spearman correlations computed between prompt specificity and Jaccard-based accuracy; significance assessed (p-values reported).",
            "uuid": "e9259.1",
            "source_info": {
                "paper_title": "Prompt Engineering: How Prompt Vocabulary affects Domain Knowledge",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "GSM8K-CoT-format",
            "name_full": "Use of zero-shot Chain-of-Thought (CoT) prompt format on GSM8K",
            "brief_description": "For GSM8K arithmetic reasoning tasks the study used a zero-shot Chain-of-Thought (CoT) prompt template to solicit multi-step reasoning from models and measured baseline and perturbed performance under synonymization.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Llama-3.1-70B-Instruct; Mistral-Large 2; Granite-13B-Instruct-V2; Flan-T5-XL",
            "model_size": "70B;123B;13B;3B",
            "task_name": "GSM8K (Grade School Math 8K)",
            "task_description": "8,500 grade-school level multi-step arithmetic word problems intended to test chain-of-thought reasoning.",
            "presentation_format": "Zero-shot Chain-of-Thought (CoT) prompt template (dataset-supplied template modified by synonymization experiments). Deterministic decoding (temperature=0).",
            "comparison_format": "No explicit CoT vs non-CoT comparison in the paper; CoT was the chosen format for GSM8K and synonymization effects on CoT performance were measured.",
            "performance": "Reported baseline strong performance for two models on GSM8K: Llama-3.1-70B-Instruct ≈ 0.97 accuracy; Mistral-Large 2 ≈ 0.95 accuracy (these reported values referenced in text).",
            "performance_comparison": "Under synonymization (especially verb changes), strong negative correlations with prompt specificity observed on GSM8K for Llama-3.1 and Mistral (r = -0.89 and -0.87 respectively), indicating CoT reasoning is particularly sensitive to verb-specificity manipulations.",
            "format_effect_size": "Correlation values up to -0.89; specific absolute accuracy drops for GSM8K under full replacement not enumerated for every model but described as substantial for verbs.",
            "explanation_or_hypothesis": "Authors propose that CoT relies on maintained abstract action/operation semantics; changing verb specificity can perturb the intended step operations and intermediate representations, thereby degrading chain-of-thought reasoning.",
            "null_or_negative_result": true,
            "experimental_details": "GSM8K experiments used zero-shot CoT prompt templates (appendix references), deterministic decoding temp=0, max tokens=500; verb-specificity effects measured via Spearman correlation between continuous prompt specificity and Jaccard-based accuracy; statistical significance reported (p-values).",
            "uuid": "e9259.2",
            "source_info": {
                "paper_title": "Prompt Engineering: How Prompt Vocabulary affects Domain Knowledge",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "Categorical-vs-numerical-specificity",
            "name_full": "Categorical (ordinal) vs numerical (continuous) prompt-specificity analysis",
            "brief_description": "Two analysis methods: (1) categorical grouping of nine permutations (3 specificity × 3 replacement ratios) into ordinal categories for coarse comparison, and (2) numerical aggregation of per-word specificity scores into continuous prompt-specificity followed by Spearman correlation with performance.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Llama-3.1-70B-Instruct; Granite-13B-Instruct-V2; Flan-T5-XL; Mistral-Large 2",
            "model_size": "70B;13B;3B;123B",
            "task_name": "MMLU, GPQA, GSM8K (same datasets used for both analytical approaches)",
            "task_description": "See individual dataset descriptions; categorical approach produces grouped average accuracies across categories; numerical approach computes Spearman correlations between continuous prompt specificity and accuracy.",
            "presentation_format": "Same underlying presentation (zero-shot templates and CoT for GSM8K) with prompts altered via synonymization; analysis differs in aggregation/measurement of specificity.",
            "comparison_format": "Categorical comparisons (33% Low ... 100% High) vs numerical Spearman correlation between continuous prompt specificity and accuracy.",
            "performance": "Categorical analysis: general decrease in accuracy as replacement ratio and specificity increase (visualized in heatmaps); Numerical analysis: most Spearman correlations negative but only 5 of 24 evaluations reached statistical significance (p&lt;0.05).",
            "performance_comparison": "Notable significant results from numerical analysis: nouns — Granite-13B on GPQA r = -0.68 (p = 0.03); verbs — Llama-3.1 on MMLU r = -0.79 (p = 0.006); Flan-T5-XL on GSM8K r = -0.78 (p = 0.008); GSM8K verbs Llama r = -0.89 (p = 5.4e-4) and Mistral r = -0.87 (p = 0.001). Most other correlations had p &gt; 0.05.",
            "format_effect_size": "Categorical heatmaps show relative accuracy deltas (e.g., up to −15.4 percentage points reported for a specific cell); numerical approach effect sizes are correlation coefficients (reported above).",
            "explanation_or_hypothesis": "Categorical approach gives coarse patterns (replacement & specificity increase → worse performance). Numerical approach refines this and shows that significant negative relationships are concentrated in verbs and in reasoning tasks; authors infer that treating prompt-specificity as continuous better captures subtle performance dependencies.",
            "null_or_negative_result": true,
            "experimental_details": "Categorical method: group nine permutations into ordinal labels and compute average Jaccard-based accuracy per group/model/dataset. Numerical method: compute per-prompt continuous prompt-specificity by averaging individual word-specificity scores for the part of speech, then compute Spearman correlation against accuracy (ten aggregated values per condition), p-values reported.",
            "uuid": "e9259.3",
            "source_info": {
                "paper_title": "Prompt Engineering: How Prompt Vocabulary affects Domain Knowledge",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "WSD-for-synonymization",
            "name_full": "Use and evaluation of Word Sense Disambiguation (WSD) model for context-aware synonym selection",
            "brief_description": "The pipeline uses WSD to select context-appropriate WordNet synsets before computing specificity scores and synonym replacement; the study evaluated WSD model options and adopted Llama-3.1-70B-Instruct for WSD because it outperformed a fine-tuned T5 in sense selection accuracy.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Llama-3.1-70B-Instruct (used as WSD model); fine-tuned T5 (comparison)",
            "model_size": "70B; unspecified for fine-tuned T5",
            "task_name": "Word Sense Disambiguation evaluation (internal, 50 mixed samples nouns+verbs)",
            "task_description": "WSD evaluation to choose which model to use for disambiguating word senses before synonym crawling and specificity computation.",
            "presentation_format": "WSD applied to each candidate word in-context prior to synonym retrieval; compared WSD performance of Llama-3.1-70B-Instruct vs a fine-tuned T5 model.",
            "comparison_format": "Llama-3.1-70B-Instruct WSD vs fine-tuned T5 WSD.",
            "performance": "WSD accuracy: Llama-3.1-70B-Instruct = 0.79 ± 0.02; fine-tuned T5 = 0.63 ± 0.05 on 50 mixed-sample evaluation across three seeds.",
            "performance_comparison": "McNemar test comparing the two WSD models returned p = 0.003 (test statistic 21), indicating Llama-3.1 significantly outperformed fine-tuned T5 on this WSD sample.",
            "format_effect_size": "WSD accuracy improvement of ~16 percentage points (0.79 vs 0.63) in favor of Llama-3.1.",
            "explanation_or_hypothesis": "Accurate WSD is critical to avoid selecting synonyms with incorrect senses that would change prompt meaning and confound LLM responses; adopting a stronger WSD reduces this source of noise in format-manipulation experiments.",
            "null_or_negative_result": false,
            "experimental_details": "WSD evaluation used 50 mixed noun/verb samples across three seeds; confusion matrix and McNemar test used to assess significance; Llama-3.1 chosen for speed and higher accuracy for the in-pipeline WSD step. The paper notes WSD still misclassifies some cases (≈21% error) and that human feedback or further fine-tuning could improve synonym selection.",
            "uuid": "e9259.4",
            "source_info": {
                "paper_title": "Prompt Engineering: How Prompt Vocabulary affects Domain Knowledge",
                "publication_date_yy_mm": "2025-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Large Language Models are Zero-Shot Reasoners",
            "rating": 2,
            "sanitized_title": "large_language_models_are_zeroshot_reasoners"
        },
        {
            "paper_title": "Training Verifiers to Solve Math Word Problems",
            "rating": 2,
            "sanitized_title": "training_verifiers_to_solve_math_word_problems"
        },
        {
            "paper_title": "Prompt Programming for Large Language Models: Beyond the Few-Shot Paradigm",
            "rating": 1,
            "sanitized_title": "prompt_programming_for_large_language_models_beyond_the_fewshot_paradigm"
        },
        {
            "paper_title": "Prompt-Source: An Integrated Development Environment and Repository for Natural Language Prompts",
            "rating": 1,
            "sanitized_title": "promptsource_an_integrated_development_environment_and_repository_for_natural_language_prompts"
        },
        {
            "paper_title": "Unleashing the potential of prompt engineering in Large Language Models: a comprehensive review",
            "rating": 1,
            "sanitized_title": "unleashing_the_potential_of_prompt_engineering_in_large_language_models_a_comprehensive_review"
        }
    ],
    "cost": 0.016417,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>7267ED913FF541B704D6655191C2CCA1
Prompt engineering has emerged as a critical component in optimizing large language models (LLMs) for domainspecific tasks.However, the role of prompt specificity, especially in domains like STEM (physics, chemistry, biology, computer science and mathematics), medicine, and law, remains underexplored.This thesis addresses the problem of whether increasing the specificity of vocabulary in prompts improves LLM performance in domainspecific question-answering and reasoning tasks.We developed a synonymization framework to systematically substitute nouns, verbs, and adjectives with varying specificity levels, measuring the impact on four LLMs: Llama-3.1-70B-Instruct,Granite-13B-Instruct-V2, Flan-T5-XL, and Mistral-Large 2, across datasets in STEM, law, and medicine.Our results reveal that while generally increasing the specificity of prompts does not have a significant impact, there appears to be a specificity range, across all considered models, where the LLM performs the best.Identifying this optimal specificity range offers a key insight for prompt design, suggesting that manipulating prompts within this range could maximize LLM performance and lead to more efficient applications in specialized domains.</p>
<p>1 Introduction</p>
<p>The rapid advancements in large language models have significantly expanded their applicability across various natural language processing (NLP) tasks.From zero-shot reasoning to fewshot learning, these models have demonstrated remarkable capabilities without requiring finetuning on specific datasets, as demonstrated by models such as GPT-3, T5, and Llama (Brown et al., 2020;Ouyang et al., 2022;Touvron et al., 2023).This versatility has enabled the deployment of LLMs in specialized fields such as STEM, medicine, and law, where accurate domain-specific responses are crucial.However, the growing reliance on LLMs has also underscored the importance of prompt engineering, the practice of crafting input prompts to elicit optimal performance from pretrained models.Unlike traditional fine-tuning approaches, which adjust model parameters based on specific datasets (P.Liu et al., 2023), prompt engineering focuses on the design of the prompt itself to enhance model outputs without modifying model parameters (B.Chen et al., 2024;White et al., 2023).This approach is computationally efficient and scalable, particularly for large-scale applications.A core challenge in prompt engineering is ensuring that the vocabulary and structure of the prompts align well with the model's understanding of the task (Leidinger, van Rooij, and Shutova, 2023;Zheng et al., 2023).One important but underexplored aspect of this is prompt specificity.Specificity is a fundamental aspect of effective communication, especially within scientific and technical domains where precision is paramount (Ang and Tan, 2018).The use of specific language reduces ambiguity, enhances clarity, and ensures that complex concepts are accurately conveyed and understood.In disciplines such as medicine, engineering, and law, selecting between words of varying specificity can impact interpretations and outcomes, making specificity a topic of particular interest.For example, in medicine, using the general term infection versus the more specific synonym sepsis could have critical implications.While infection refers to the invasion and multiplication of microorganisms in the body, sepsis is a specific, life-threatening response to infection that can lead to tissue damage and organ failure.Mislabeling sepsis as a general infection may delay necessary aggressive treatments, posing serious health risks to patients.In engineering, referring to a metal versus specifying titanium could affect material selection and performance.Metal is a broad category, whereas titanium is a specific metal known for its high strength-to-weight ratio and corrosion resistance.Using the general term may result in inappropriate material choices, leading to design failures or safety hazards.Similarly, in law, the term crime is general, whereas embezzlement is a specific type of financial crime involving the unlawful taking of funds by someone in a position of trust.Confusing these terms could affect legal interpretations and sentencing; misclassifying embezzlement as a general crime may overlook the specific legal elements required for prosecution.</p>
<p>Given these examples that underscore the importance of specificity in specialized fields, it leads us to the question whether this significance of specificity translates into the realm of LLMs, particularly in prompt engineering.Based on the previously mentioned benefits of using more specific vocabulary, we ask: Does increasing the specificity of vocabulary in prompts enhance the performance of LLMs in generating responses in domain-specific questionanswering and reasoning tasks?</p>
<p>As LLMs are increasingly deployed in areas such as STEM, medicine and law, where precise communication is important, understanding the impact of prompt specificity on model outputs seems increasingly reasonable.This inquiry forms the basis of our study, motivating us to explore how varying the specificity of words within prompts affects the ability of LLMs to comprehend and accurately process domain-specific information.By investigating this relationship, we aim to determine whether the benefits of specificity in human communication extend to interactions with LLMs, contributing to more effective and reliable applications of these models in specialized domains.In this thesis, we examine the impact of prompt specificity on LLM performance in domainspecific question-answering and reasoning tasks.Specifically, we focus on three major parts of speech (nouns, verbs, and adjectives), and analyze how varying the specificity of these words in prompts influences the models' ability to accurately answer questions.We calculate specificity scores for nouns and verbs, utilizing the lexical database WordNet (Fellbaum, 1998), and introduce a novel equation to quantify the specificity of adjectives.Our study evaluates four LLMs, Llama-3.1-70B-Instruct,Granite-13B-Instruct-V2, Flan-T5-XL, and Mistral-Large 2, across three datasets (MMLU, GPQA, and GSM8K), focusing on STEM, medicine, and law domains.To summarize our contributions:</p>
<p>• We introduce a method to systematically substitute nouns, verbs and adjectives through synonyms with different specificities.</p>
<p>• We find prompt specificity ranges for different models, where the LLM yields the best results in question-answering and reasoning tasks across STEM, law and medicine domains.</p>
<p>• We demonstrate that generally increasing prompt specificity, exceeding the optimal ranges of prompt specificity in the STEM, medicine, and law domains, has minimal impact on LLM performance for nouns in question-answering and reasoning tasks, but results in a significantly negative effect for verbs in reasoning NLP tasks.</p>
<p>• We introduce a method for calculating the specificity of adjectives, marking the first step towards ranking and quantifying adjectives, though further validation is necessary to confirm its significance.</p>
<p>Related Work</p>
<p>With the recent advancements in LLMs, their applicability to a wide range of NLP tasks has simultaneously expanded (Brown et al., 2020).These models exhibit complex capabilities, including zero-shot problem-solving (Kojima et al., 2023), few-shot learning (Cobbe et al., 2021a), instruction following (Ouyang et al., 2022), and incorporation of domain knowledge (Velásquez-Henao, Franco-Cardona, and Cadavid-Higuita, 2023;Zheng et al., 2023).To harness these abilities, various prompt engineering techniques have emerged, aiming to interact with LLMs and significantly enhance performance across diverse NLP tasks (P.Liu et al., 2023;Schick and Schütze, 2021).Unlike fine-tuning methods that adjust model parameters by retraining with domain-specific labeled data, prompt engineering focuses on optimizing the input prompts to elicit the best possible outputs from a pretrained LLM without altering its parameters.Fine-tuning can be computationally resource-intensive, especially at scale, whereas prompt engineering typically requires only a few contextual examples for incontext learning and minimal computational resources (Dong et al., 2024;Y. Liu et al., 2024).Prompts, the core components of prompt engineering, can be categorized into continuous and discrete prompts.Continuous methods involve automated optimization of prompts, usually with a masked language model, by dynamically adjusting the prompt to ongoing interactions or contexts by rephrasing the baseline prompt or modifying factual details (Ju et al., 2023;Shin et al., 2020;Xia et al., 2024).In contrast, discrete prompts are manually crafted, offering greater flexibility as they can be specifically tailored for distinct domains and use cases where the context remains relatively stable (Reynolds and Mc-Donell, 2021;Schick and Schütze, 2021).In this work, we will focus on discrete prompts due to their differentiable properties and interpretability.</p>
<p>Several collections of discrete prompt templates exist, such as PromptSource (Bach et al., 2022), Sup-NatInst (Y.Wang et al., 2022), and BIG-BENCH (Srivastava et al., 2023), which are community-contributed resources designated for various NLP tasks.These collections facilitate performance quantification across multiple benchmarks and offer users convenient access to state-of-the-art prompts for specific NLP tasks.</p>
<p>While prompt engineering provides flexible means of interacting with LLMs by refining input prompts, the opaque nature of transformer-based learning can lead to unintended consequences, such as hallucinations (Z.Xu, Jain, and Kankanhalli, 2024).Consequently, carefully crafting prompts is critical to ensuring accurate and reliable outputs (Rawte et al., 2023) In this study, we address the limitations of previous studies by incorporating the dimension of specificity into prompt design and conducting a more nuanced investigation of synonym substitution with varying specificities and its effects on LLM performance.Further, this thesis aims to offer a deeper understanding of the interplay between prompt specificity and model behavior, extending across a diverse set of tasks and domains.</p>
<p>Methodology</p>
<p>In this thesis, we focus on the impact of prompt specificity on the performance of LLMs.Therefore, we create multiple variations of instructions from the datasets MMLU, GSM8K and GPQA (Section 3.4), with our specificity-based synonymization framework (Section 3.1).This framework determines all parts of speech with the respective semantic sense in the provided context, retrieves the synonyms for each sense, calculates the specificity scores (Section 3.3) by utilizing lexical database structures (Section 3.2), categorizes the synonyms based on the specificity and finally synonymizes the instruction with three replacement ratios (33%, 67% and 100%) with respect to the replaceable word count.For each prompt variation, we use the models Granite-13B-Instruct-V2, Flan-T5-XL, Llama-3.1-70B-Instruct and Mistral-Large 2 (Section 3.5) to generate the output for questionanswering and reasoning tasks.The preprocessing includes five key steps starting with the retrieval of parts (I) of speech from the original instruction, crawling synonyms (II) and calculate the specificity scores (III) for all parts of speech (the green colored boxes include WSD in the algorithm), categorizing the synonyms (IV) into low, intermediate, high specificity and finally synonymize the original instructions (V) with three different replacement ratios (33%, 67%, 100%).Additionally, there is a step-by-step example sampled from the GPQA dataset, synonymizing nouns with varying specificity synonyms.</p>
<p>Synonymization Framework</p>
<p>The specificity-based synonymization framework, shown in Fig. 1, consists out of five distinct steps, starting from the retrieval of the considered parts of speech (I), crawling of synonyms (II), calculating of specificity scores (III), categorizing the synonyms according to the specificity scores (IV) and the synonymization of the instructions (V).</p>
<p>In the following, we will explain each step and additionally explain our approach of calculating the overall prompt specificity, focusing on one part of speech at a time.Note, that we only use a fraction of the complete datasets, since the number of synonyms increases exponentially with the number of instructions and therefore increases the computational time significantly, although all effort was made to optimize the computational efficiency of the framework.For each dataset (MMLU with 14 subdatasets, GPQA and GSM8k) we sampled up to 250 instructions.The overview of the samples remained after the processing is displayed in Tab. 1.I Part of Speech Retrieval.From the original instruction prompt, we parse the sentence by word-tokenizing the instruction, tagging the lexical database position and filter for nouns, verbs and adjectives from each individual prompt1 .This algorithm yields us lists of all considered parts of speech that we pass to the next step.II Crawling synonyms.For each tagged word, we apply Word Sense Disambiguation (WSD) (Wahle, Ruas, Meuschke, et al., 2021) to identify the most contextually appropriate sense.We then retrieve all associated hypernyms, lemmas, and hyponyms for the selected sense.This approach helps to consider semantically suitable synonyms, reducing the likelihood of confusing the LLM by selecting synonyms with incorrect senses.For instance, the noun cell, which appears in a biological context and refers to the basic structural and functional unit of all organisms, has six other senses in the lexical database: cell as ... Each of these senses would be connected to synonyms (e.g.cubicle or jail cell ) which would not be fitting in our biological context, potentially changing the meaning of the sentence and therefore confuse the LLM.III Calculating specificity scores.The gathered synonyms are then passed to the specificity calculation algorithm, which quantifies the specificity for each synonym with a continuous number by utilizing the taxonomy structures of the lexical database for each part of speech (Section 3.3) IV Categorizing Synonyms.Using the calculated specificity scores, we will select three synonyms from the set of all possible synonyms for each part of speech, categorizing them into low, intermediate, and high specificity based on the minimum, mean, and maximum values, respectively.This requires that each word must have at least three unique specificity scores, as otherwise, it would be impossible to fill all three categories.The intermediate specificity is selected by calculating the minimum absolute difference min |s− s| between the mean specificity score s of the set and the synonym's specificity score s, choosing the value closest to the mean of the set.</p>
<p>V Synonymize instructions.To generate synonymized instructions, we fractionally replace the parts of speech in the prompt instructions with their categorized synonyms at rates of 33%, 67%, and 100% focusing on one type at a time.This introduces a second constraint: Each instruction must contain at least three different replaceable parts of speech.The algorithm yields nine different synonymized instructions for each part of speech, in addition to the original prompt from the dataset.An example of 100% replacement of nouns with synonyms of low, intermediate, and high specificity can be found in Appendix C, Tab.12.</p>
<p>Lexical Database</p>
<p>The lexical database that we use for this study is WordNet (WN), introduced by the Princeton University as an open source project in 1985.The database includes English nouns, verbs, adjectives and adverbs, which are structured through cognitive synonyms called synsets (distinct semantic concepts) that are interconnected through static relations (Miller, 1995).Noun synsets are structured hierarchically through super-subordinate relations (hypernymy and hyponymy).Each noun hierarchy starts with the ENTITY root synset and unfolds into an inverse tree.Synsets at the top of the tree are more general and become more specific moving down the tree.Non-leave nodes are considered types and represent common nouns, whereas leafs of the tree are instances that include personas, countries and geographic entities.Selecting one particular noun synset in the taxonomy, hypernyms represent all nodes that are between the root node and the chosen synset, while hyponyms represent all more specific synsets under the chosen synset (Fellbaum, 1998).</p>
<p>Verb synsets are organized similarly to noun synsets, forming hierarchies connected by various semantic relations, including not only hypernyms and hyponyms but also entailment, antonymy, and troponymy.Entailment describes a logical relationship between verbs where one implies the other, such as snore implying sleep.Antonymy refers to verbs representing opposite actions, for example, start is the antonym of stop.Troponymy, on the other hand, refers to the manner in which an action is performed, indicating a more specific way of carrying out the verb.For instance, jog is a troponym of run (Fellbaum, 1998).</p>
<p>In contrast, adjectives form a cluster-like structure composed of synonyms, antonyms and semantically similar relationships.Synonyms refer to adjectives that share the same conceptual meaning, such as happy and joyful, while semantically similar adjectives, e.g.large and big are linked through associative connections (Fellbaum, 1998).</p>
<p>Specificity Measures</p>
<p>The specificity quantification for nouns, verbs and adjectives is based on the utilization of the corresponding WN taxonomy structures.For nouns and verbs, this taxonomy is hierarchically structured, allowing us to utilize this natural ordering of the lexical database to determine the specificity of the parts of speech.However, adjectives do not share this hierarchical structure with nouns and verbs in WN.Therefore, we use a novel adjective specificity measure to quantify their specificity.</p>
<p>In the following, we will consider both equations and explain them in detail.Specificity Score for Nouns and Verbs.Using the specificity score introduced by Bolognesi et al. ( 2020) (Bolognesi, Burgers, and Caselli, 2020), we compute the specificity for noun and verb synonyms based on the following formula:
S noun/verb = d + log 1 + n N − log(l),(1)
where d is the distance (number of nodes) between a word and the top root node, N represents the total number of nodes in the WN taxonomy restricted to a part of speech, n denotes the total number of direct and indirect hyponyms for a given word, and l is the number of synsets for the corresponding part of speech.Given that the taxonomy for nouns and verbs is hierarchically structured, with specificity increasing downwards, this equation aims to determine the relative position of a synset along its path in the respective WN taxonomy.The first term d counts the number of nodes from the root to the synset of interest.</p>
<p>The second term log 1+n N represents the number of nodes directly and indirectly connected to the synset, normalized by the total number of nodes in the taxonomy for better comparability.By considering the nodes above and below, we can locate the synset within its respective path in the taxonomy.The term log(l), which we added to the original equation, is based on the intuition that more specific words have less distinct senses.Our analysis of 940 nouns and 745 verbs shows that the average number of synsets for hypernyms is significantly higher than for hyponyms.The overview of the assessment with the corresponding results of the Mann-Whitney U test (McKnight and Najab, 2010) are</p>
<p>Model</p>
<p>Accuracy Fine-Tuned T5 0.63 ± 0.05 Llama-3.1-70B-Instruct0.79 ± 0.02 Table 3: WSD Model Evaluation.Results of WSD performance assessment for fine-tuned T5 and Llama-3.1-70B-Instructmodels for 50 mixed sampled (nouns and verbs) over three seeds.displayed in Tab. 2. Given that hypernyms are inherently broader in meaning, while hyponyms convey greater specificity, this observation aligns with our intuition that words with fewer synsets tend to be more specific.Furthermore, we tackled a challenge where synonyms frequently belonged to the same synset as the target word, yielding identical specificity scores under the original measure.To address this, we integrated the total number of synsets associated with each synonym, effectively mitigating this issue.Bolognesi, Burgers, and Caselli, 2020 used only the first sense of words for specificity calculations, as it represents the most common sense.However, this approach may not be ideal for our use case, as it cannot always ensure the correct sense for a given context is chosen, which could alter the semantic meaning of the sentence and might mislead the LLM.To address this, we apply WSD (Wahle, Ruas, Meuschke, et al., 2021) to ensure the synonym is accurately aligned with the context.For this task, we selected the Llama-3.the fine-tuned T5 model using 50 mixed samples (nouns and verbs) on three different seeds.As demonstrated in Table 3, Llama-3.1-70B-Instructachieved higher accuracy (0.79 ± 0.02) compared to the fine-tuned T5 (0.63 ± 0.05).We applied the McNemar test (Dror et al., 2018) across three random seeds using the confusion matrix depicted in Fig. 2 to assess whether the observed performance differences between the models are statistically significant.The test returned a p-value of 0.003 with a test statistic of 21.Out of 67 cases where the models produced divergent results (46 + 21), the Llama-3.1-70B-Instructmodel correctly identified the sense in 46 instances.The low p-value suggests that the performance disparity between the two models is statistically significant across the three runs.Given that Llama-3.1-70B-Instructconsistently outperforms the fine-tuned T5 model, it emerges as a viable alternative for the WSD process, offering both enhanced efficiency and accuracy Specificity Score for Adjectives.As previously mentioned, adjectives do not share the same structural properties as nouns and verbs in WN, lacking a hierarchical arrangement.Therefore, we propose a new equation to measure adjective specificity:
S adjectives = 1 log(1 + ssw + s + a + l) (2)
where ssw represents the number of words with semantically similar relation to the considered adjective, s is the number of synonyms, a is the count of antonyms for the adjective and its synonyms, and l refers to the number of synsets for the adjective.The derivation of this equation is based on four underlying assumptions:</p>
<p>1.The number of words with similar meaning ssw as the adjective is reverse proportional to its specificity S ∼ 1 ssw 2. The number of direct synonyms s of an adjective is reverse proportional to its specificity S ∼ 1 s 3. The number of direct and indirect antonyms a of an adjective is reverse proportional to its specificity S ∼ 1 a 4. The number of senses l of an adjective is reverse proportional to its specificity S ∼ 1 l</p>
<p>The underlying premise of these assumptions is that as the specificity of adjectives increases, their contextual meaning becomes more constrained.</p>
<p>In other words, if an adjective can be substituted by many others (e.g., similar words, senses, direct synonyms, or even antonyms), it likely captures a broader range of features that those adjectives share.Consider the following examples: good vs. phenomenal and small vs. tiny.In both cases, the second adjective is more specific due to its narrower scope of applicability.For instance, while many things can be considered good, not all good things are phenomenal.Likewise, although everything tiny is small, not all small things are tiny.The results derived from parameter calculations based on these assumptions are summarized in Tab. 4. The parameters for phenomenal and tiny are consistently smaller than those for good and small, which aligns with our assumptions.By incorporating all the assumed proportional relationships into an additive model for adjective specificity, we derive the following expression:
S ∼ 1 ssw + s + a + l . (3)
Each added term enhances the interpretability of the equation, as all parameters are treated independently.The additive model also reduces sensitivity to outliers and extreme values compared to the multiplicative model.To smooth the resulting specificity measure and make it more responsive to small changes, we apply a log function to the denominator and add +1 to prevent division by zero, leading to our final equation 3.</p>
<p>To validate this approach, we used GPT-4o as an evaluator, comparing our systematic ranking of adjective specificity in various contexts with GPT-4o's rankings for a sample of 91 instructions.For each instruction, we filtered all adjectives, retrieved their respective synonyms, and iteratively created synonym pairs by incrementing the index by 1:
[syn 1 , syn 2 , syn 3 ] → [syn 1 , syn 2 ], [syn 2 , syn 3 ], [syn 3 , syn 1 ]
These pairs, along with the original instruction, were then passed into a three-shot prompt template for GPT-4o, shown in Appendix A, Fig. 11.The model generated an ordered list where the first element represents the adjective with greater specificity, given the context of the instruction.By applying this process to all adjective pairs, we used transitivity
(A &gt; B ∧ B &gt; C) =⇒ A &gt; C
to combine all sub-rankings into a single, complete ranking of adjectives for each instruction.We then correlated these GPT-4o adjective rankings with those derived from our adjective specificity model.The resulting Spearman correlations are shown in Fig. 3.The median Spearman correlation coefficient of 0.50 indicates a moderate alignment between the equation-based and LLMbased rankings of adjective specificity, demonstrating that the methods are generally consistent.Notably, in 23 cases, a perfect positive correlation of 1.0 was observed, where both methods produced identical rankings, suggesting that the equation-based approach is highly effective in many instances.While 32 out of the 91 samples exhibited negative correlations, this divergence underscores the potential for refining the equation-based model to further improve alignment with LLM judgments, but the overall results reflect a solid foundation for its use in ranking adjective specificity.Calculating Prompt Specificity.The previous calculations focused solely on individual parts of speech and their corresponding synonym specificities, without considering the full composition of parts of speech within the prompt.To provide a broader perspective, we also evaluate overall changes in prompt specificity by converting the ordinal categories (33% Low Specificity,..., 100% High Specificity) into continuous specificity scores.This is achieved by aggregating the individual specificity scores for each part of speech (nouns, verbs, and adjectives) within each instruction and calculating their average.As a result, each prompt is assigned a continuous specificity score for the considered part of speech, which varies across specificity levels and replacement levels A simple example that illustrates this method can be seen in Fig. 4.</p>
<p>Data</p>
<p>The datasets utilized in this study span diverse domains, including STEM, law, and medicine, as well as varying levels of expertise, ranging from high school and undergraduate to PhDlevel.These datasets also encompass different task formats, such as multiple-choice questions (MMLU, GPQA) and reasoning-based assessments (GSM8K), ensuring a robust foundation for our evaluation.A detailed summary of all datasets including examples is presented in Appendix C.</p>
<p>MMLU.The Massive Multitask Language Understanding dataset is an evaluation benchmark designed to assess the performance of models across a wide range of subjects and expertise levels.It contains approximately 57 tasks, each with up to 500 samples, accompanied by corresponding ground truths.These tasks are divided into various categories, including STEM, law and medicine in different expertise levels.The tasks include multiple-choice questions that span both general knowledge and highly specialized domains.The diversity of the MMLU dataset allows us to evaluate how well models generalize across a variety of topics, expertise levels, and formats, ensuring that the findings of this study are applicable to a broad range of realworld applications (Hendrycks et al., 2021).</p>
<p>GPQA.The Google-Proof Question Answering dataset consists of 448 expertly curated multiplechoice questions, with a focus on biology, physics, and chemistry.Each question is accompanied by a domain expert-evaluated ground truth, ensuring high-quality, reliable answers.The dataset is designed to test models at a PhD level of expertise, making it particularly challenging for assessing the performance of LLMs in specialized fields that require in-depth knowledge and understanding.This dataset is particularly useful for testing the robustness of models in handling precise, domain-specific questions that require not only basic factual knowledge but also a deep comprehension of scientific principles and reasoning (Rein et al., 2023.GSM8K.The Grade School Math 8K dataset is a carefully curated collection of 8,500 humanwritten math problems, each designed to reflect the complexity of grade-school-level arithmetic.The problems typically require two to eight steps to solve, involving sequential execution of arithmetic operations such as addition, subtraction, multiplication, and division.The dataset tests the model's ability to reason through multistep problems, handle intermediate results, and execute operations in the correct order, which are crucial skills for successful mathematical problem-solving.The expertise level targeted by this dataset corresponds to middle school students, and solving these problems provides a clear benchmark for evaluating models' capacity for stepwise reasoning.For this study, GSM8K will serve as a key resource for assessing the models' abilities to handle arithmetic reasoning tasks with chain-of-though prompting, a critical area in understanding the reasoning capabilities of LLMs in structured problem-solving contexts (Cobbe et al., 2021b).</p>
<p>Models</p>
<p>The following models were selected due to the diverse architectures and sizes (3B, 13B, 70B, 123B) they present, while offering the capabilities of handling complex natural language processing tasks across specialized domains such as question-answering (zero-shot and few-shot), allowing a comprehensive analysis of prompt specificity.For each dataset, we use different prompt templates from the data sources to perform the question-answering and reasoning tasks (Cobbe et al., 2021b;Hendrycks et al., 2021;Rein et al., 2023).We call the models using a Figure 5: Specificity Score Distribution for nouns, verbs and adjectives.The histograms show the distribution of specificity scores for the respective part of speech.The mean specificity score for nouns is µ nouns = 21.37, for verbs µ verbs = 12.09 and for adjectives µ adjectives = 0.40 temperature of zero to select only tokens with the highest probability, since question-answering and reasoning task require a high level of precision and determinism.Further, we set a threshold to the maximum output token size to 500, because during initial experimental runs, we observed that sometimes models repeat phrases until there is a timeout, stopping the process.To mitigate this issue, we stop the output generation process after the 500 token threshold is reached.For reproducibility, we set a random seed.In all our experiments, we use a zero-shot prompttemplates for the MMLU and GPQA datasets and a zero-shot Chain-of-Thought (CoT) prompt template for the GSM8K dataset.An overview of the prompt templates and the parameters are displayed in Appendix A Fig. 11 and Appendix B Tab. 10, respectively.Flan-T5-XL.Flan-T5-XL (3B) is a 3 billion parameter model that belongs to the Flan-T5 family.It is based on Google's T5 architecture, which uses an encoder-decoder model, and is pretrained on a mixture of supervised and unsupervised tasks that have been reformulated into a text-to-text format.The model is fine-tuned on the Fine-tuned LAnguage Net (FLAN) dataset, which incorporates instruction-based tuning to enhance its capabilities in zero-shot and few-shot learning scenarios (Chung et al., 2022).Granite-13B-Instruct-V2.</p>
<p>Granite-13B-Instruct-V2 is a general-purpose decoder-only model developed by IBM.This 13-billionparameter model is optimized for a variety of NLP tasks through instruction tuning.It builds on the Granite-13B-V2 base, which was pretrained on 2.5 trillion tokens sourced from IBM's Data Pile.Granite-13B-Instruct-V2 has been fine-tuned to handle a broad range of tasks including text generation, comprehension, and question-answering, making it well-suited for zero-shot and few-shot learning paradigms (IBM, 2024).The table displays the results for the Kolmogorov-Smirnov test, which was performed on the specificity score distribution for nouns, verbs and adjectives.Since all p-values are smaller than 0.05, we cannot reject the null hypothesis and therefore the distributions cannot be considered as normal distributions.</p>
<p>Part of Speech</p>
<p>Mistral-Large 2. Mistral Large 2 is a 123 billion parameter model designed for NLP tasks with a focus on code generation, reasoning, and multilingual support.The model is fine-tuned to minimize hallucinations and follows instructions more accurately, making it effective in complex, long-context applications (MistralAI, 2024).Llama-3.1-70B-Instruct.</p>
<p>Llama-3.1-70B-Instruct is part of Meta's Llama series, known for their high performance in a wide variety of NLP tasks.With 70 billion parameters, this model leverages its size to capture a wide range of linguistic and semantic nuances.The Llama series has been pretrained on vast amounts of data and subsequently fine-tuned through instructionbased learning and preference tuning, which enhances its ability to follow detailed and specific prompts (Dubey et al., 2024).</p>
<p>Experimental Results and Discussion</p>
<p>The data, processed using the specificity-based synonymization framework, together with the models, are employed to investigate the distributional properties of parts of speech at different specificity levels and the relationship between specificity changes and LLM performance.</p>
<p>Q1: What is the distribution of specificity scores for each considered part of speech?What are the ranges of specificity scores for low, intermediate and high specificity?</p>
<p>Answer.We use all the specificity score calculated with eq. 1 for nouns and verbs, representing the distribution in Fig. 5a.In total there were 35848 nouns and 8323 verbs used for this representation.To quantify whether the underlying distributions are normally distributed, we conduct a Kolmogorov-Smirnov test (Berger and Zhou, 2014), which reveals that they are not similar to a normal distribution as seen in Tab. 5.The observable shift, also represented by the strong deviating means of nouns (21.37) and verbs (12.09) specificities, can be explained by minor structural differences in the taxonomy of nouns and verbs.Besides the fact that the overall taxonomy of nouns is larger (82000 unique nouns compared to 11500 unique verbs), the number of direct and indirect hyponyms varies strongly for these two parts of speech, leading to smaller values in the second term of eq. 1 for verbs.For the set of adjectives (790 in total), the specificity, as defined by eq. 3, is predominantly concentrated around the mid-range (mean of 0.40) on a scale from zero to one.This suggests that highly specific adjectives are scarcely represented in our sample.Similar to the distributions observed for nouns and verbs, the specificity of adjectives does not follow a normal distribution, as indicated by the Kolmogorov-Smirnov test, which yielded a p-value of 1.08e-5.It is important to note that the specificity ranges differ between adjectives and the other parts of speech.For adjectives, specificity is calculated using a fractional measure constrained between zero and one, unlike the measures used for nouns and verbs To address the second question, we group the specificity scores by the specificity levels (low, intermediate and high) and combine their individual distribution in Fig. 6.Adjectives are excluded from this part of the analysis, since only high specificity synonyms were selected due to the small number of samples remaining after the preprocessing.By categorizing synonyms based on their minimum, average, and maximum values within the pool for a given part of speech, we expect each specificity level's distribution to shift progressively to the right as specificity increases.This expectation is confirmed, as the grouped means consistently rise with increasing specificity levels (the difference in grouped means for nouns and verbs is approximately 3).Notably, the high-specificity group for nouns exhibits a much larger standard deviation (8.26) compared to the low-specificity (2.58) and intermediatespecificity (2.69) groups.This larger variation is likely due to the higher frequency of nouns with specificity scores exceeding 30, which skews the distribution to the right To accurately define the valid ranges for each specificity category, we employed Kernel Density Estimation (KDE) (Y.-C.Chen, 2017), a nonparametric method, given that the specificity distributions for nouns and verbs do not follow a normal distribution.We calculated the intersection points between the low-intermediate and intermediate-high levels, resulting in boundaries of 17.74 and 11.74 for low-intermediate and 20.90 and 13.74 for intermediate-high, respectively for nouns and verbs.The corresponding intervals for each specificity level are presented in Tab. 6. Q2: How does the specificity of synonyms affect the performance of the models Llama-3.1-70B-Instruct,Granite-13B-Instruct-V2, Flan-T5-XL and Mistral-Large 2 for question answering and reasoning tasks?</p>
<p>We separate the analysis of the effects of prompt specificity on the LLM performance in two approaches.The first approach, called categorical approach, provides a high-level view by grouping and categorizing nine permutations of specificity levels (low, intermediate, high) and replacement ratios (33%, 67%, 100%) into ordinal categories (33% Low, ..., 100% High).It allows for a clearer comparison between these predefined categories with respect to the model performance, and provide insights about potential patterns in how performance shifts across the varying replacement-specificity permutations.The second approach, called numerical approach, translates these ordinal replacement-specificity permutations into continuous values, which allows for a more precise quantification of prompt specificity and performance correlation.By treating prompt specificity as a continuous variable and computing correlations, this method captures finer details of how incremental changes in specificity affect performance.Categorical Approach.</p>
<p>For each model, dataset, specificity and replacement level combination we group the outputs and evaluate them with the ground truth.For the evaluation, we opted to use the Jaccard metric (Niwattanakul et al., 2013) rather than the commonly employed exact-match approach for multiple-choice question-answering tasks.</p>
<p>This decision was made to account for responses that included additional characters, such as "A." or "-B," which would incorrectly result in a score of 0 under exact matching, despite the answer being correct.</p>
<p>For each combination, we aggregate the Jaccard similarity scores and calculate the average over all samples per category to get the accuracy.Fig. 7 illustrates the comparison of these accuracies for original, low, intermediate and high specificity next to each other for each model, dataset and replacement level.Each row represents the different models and each column shows performance across the datasets: MMLU, GSM8K and GPQA.The y-axis reflects the calculated accuracy, while the x-axis encodes the replacement ratio of the synonymizable nouns.Across all models and datasets, accuracy seems to decrease with an increasing replacement and specificity level, although there are some exceptions from this observable trend.For the GSM8K dataset, the models Granite-13B-Instruct-V2 and Flan-T5-XL show small sensitivity of performance with specificity changes, while for the GPQA dataset, the performance decreases notably compared to the baseline (original).In particular, for the larger models, Llama-3.1-70B-Instruct and Mistral-Large 2, this performance decrease is strongly visible.Similar trend can be also seen for verbs in the Appendix A, Fig. 13.For adjectives, considering a full replacement with high specificity synonyms, we observe similar performance changes as for the other parts of speech.Notably, the performance of Llama-3.1-70B-instruct and Mistral-Large 2 increases strongly for the GSM8K dataset using high specificity synonyms, which would mark a difference to the other parts of speech.However, the sample size, particularly for GSM8K with 4 instructions, is too small to consider these results as significant.More samples would be required to make a more sophisticated observation.This overall trend indicates that the fractional adjustment of nouns and verbs with higher specificities harmfully affects the LLMs performance, particularly Llama-3.1-70B-Instruct and Mistral-Large 2, in the domains of STEM, law and medicine and therefore seems not to be suitable approach to generally optimize prompts in these domains.Figure 8: Average Accuracy across all Datasets and Models for Nouns.These heatmaps represent the performance differences in percentages for each model across all data for nouns.Each x-axis represents the replacement ratio (fraction of how many synonyms were used), the y-axis encodes the categorical specificity levels (low, intermediate, high) and the color encodes the average accuracy scores.</p>
<p>To get a better comparison of the performance changes for each combination of specificity and replacement level, we group each class by each model and average the performance across all datasets.We calculate the relative performance change with respect to the baseline performance for each model.The heatmaps in Figs. 8 and  9, respectively for nouns and verbs, illustrate these precentral changes in accuracy when varying specificity levels (low, intermediate, high) and replacement ratios (33%, 67%, 100%) across our four models.Each cell represents the change in performance relative to the baseline (original) accuracy, showing how accuracy shifts when more specific synonyms replace the original words.For nouns, as the replacement ratio and specificity increase, performance generally declines across all models.This indicates that more specific or frequent replacements of synonyms negatively impact the models' abilities to handle zero-shot and chain-of-thought question-answering tasks.For instance, Mistral-Large 2 exhibits the most significant performance drop at 100% noun replacement and low specificity, showing a decrease of up to -0.154 in accuracy.Llama-3.1-70B-Instruct also shows substantial reductions, especially at higher replacement ratios (-0.117 at high specificity for 100% replacement).Granite-13B and Flan-T5-XL follow similar patterns, with negative accuracy changes, although Flan-T5-XL shows slightly better resilience to noun replacement.</p>
<p>In the case of verbs, the models display more varied responses to synonym specificity and replacement ratios.Flan-T5-XL, for example, demonstrates some positive accuracy changes at lower replacement ratios and low specificity, indicating that it handles verb replacements better, particularly in low and intermediate specificity settings.This suggests that for zero-shot and Figure 9: Average Accuracy across all Datasets and Models for Verbs.These heatmaps represent the performance differences in percentages for each model across all data for verbs.Each x-axis represents the replacement ratio (fraction of how many synonyms were used), the y-axis encodes the categorical specificity levels (low, intermediate, high) and the color encodes the average accuracy scores.chain-of-thought question-answering tasks, Flan-T5-XL is more adaptable to verb changes compared to nouns.Mistral-Large also exhibits resilience, showing smaller accuracy reductions for verb replacements, although its performance declines as replacement ratios increase.However, at 100% verb replacement, all models show a decline in performance, with Granite-13B and Llama-3.1-70Bdisplaying the most significant accuracy drops, particularly at higher specificity levels.</p>
<p>Answer.Overall, the results suggest that noun replacements with more specific synonyms have a stronger negative impact on model performance than verb replacements, with performance generally deteriorating as the replacement ratio increases.The findings highlight that while all models experience degradation in accuracy, Llama-3.1-70B-Instruct and Mistral-Large 2 are more sensitive to high replacement ratios, particularly with specific nouns, whereas Flan-T5-XL exhibits more robustness, especially in han-dling verb replacements in question-answering tasks.Notably, Llama-3.1-70B-Instruct(0.97) and Mistral-Large 2 (0.95) perform stronger on the reasoning Tasks (GSM8K) compared to Granite-13B-Instruct-V2 and Flan-T5-XL.These performance differences could be attributed to the additional Direct Preference Optimization (Llama-3.1)(Rafailov et al., 2023) and larger parameter size (Mistral-Large 2 with 123B parameters) (Kaplan et al., 2020).However, the categorical approach only offers a broach perspective on the potential relationship between increasing specificity and LLM performance.By only focusing on the predefined categories, specificity level and replacement ratio, this approach does not take into account the overall prompt specificity since instructions are assigned to one category based on the number of individual synonyms with varying specificity.There might be cases where a modified instruction will be assigned to the 33% category, but would exhibit a smaller prompt specificity than the 100% intermediate variation.</p>
<p>To address this limitation of the categorical approach, we shift from a word-level focus to a prompt specificity perspective in the numerical approach.</p>
<p>Numerical Approach.For each model, we systematically aggregate all outputs alongside the corresponding ground truth labels provided by the original datasets.We then partition these into sub-datasets based on part of speech (nouns and verbs) and dataset type (MMLU, GSM8K, GPQA), resulting in six distinct sub-datasets per model.To evaluate the performance of the LLM, we employ the Jaccard metric as utilized in the categorical approach.Final performance scores for each dataset and model are obtained by calculating accuracy across all samples, stratified by nine permutations of specificity levels (low, intermediate, high) and replacement levels (33%, 67%, and 100%).Additionally, for each modified instruction, we compute the prompt specificity corresponding to the analyzed part of speech (refer to Section 3.3), and average these scores across each combination of specificity and replacement levels within each dataset.This comprehensive approach allows us to capture the holistic effect of synonymization with varying specificities on the entire prompt, rather than focusing solely on the subset of synonyms.For instance, in a prompt containing ten nouns, where only three nouns are subject to synonymization, calculating the average specificity based only on the three modified nouns may yield skewed results compared to averaging across all ten nouns.Such an approach could artificially inflate the perceived impact of synonymization, even though the actual changes might be minimal.Furthermore, while discrepancies in total word count between two samples with an equivalent number of nouns could introduce bias, we deem this effect negligible, as the number of nouns is likely to positively correlate with the number of sentences, thus diminishing the bias in our approach.Finally, we compute the Spearman correlation between the average prompt specificity and the performance of the LLM across datasets and specificity levels to evaluate the relationship between these variables.We use Spearman instead of Pearson, since there are only ten values for each parameter, and we cannot assume that these are normally distributed.</p>
<p>Answer.Table 7 presents a comparison of correlations and p-values for four large LLMs across three distinct tasks, MMLU (zero-shot), GPQA (zero-shot), and GSM8K (zero-shot, Chain-of-Thought), separated by nouns and verbs.In general, increasing prompt specificity tends to negatively impact LLM performance, though most of these changes are not statistically significant (p-values &gt; 0.05).For nouns, the only significant result is a strong negative correlation of −0.68 (pvalue = 0.03) observed for Granite-13b-instruct-v2 on the GPQA (zero-shot) task.For verbs, significant negative correlations are observed for Llama-3.1-70b-instruct on MMLU (−0.79, p = 0.006) and Flan-T5-XL on GSM8K (−0.78, p = 0.008), along with very strong negative correlations for GSM8K with Llama-3.1-70B-Instruct(−0.89, p = 5.4e-4) and Mistral-Large 2 (−0.87, p = 0.001).Notably, most of the significant correlations pertain to verbs.However, with only five out of 24 evaluations showing significant results, this suggests that synonymization generally does not have a substantial impact on LLM performance for our tasks.Contradictory to our intuition, that increasing the prompt specificity would lead to better LLM performance, these results imply a rather unexpected opposite outcome.However, this evaluation reveals several insights into how increasing prompt specificity impacts LLM performance.Although the overall trend indicates a negative effect on performance, the fact that most changes are not statistically significant suggests that LLMs are generally resilient to prompt variations in terms of specification through synonymization.Interestingly, the higher frequency of significant negative correlations observed for verbs in the reasoning task (GSM8K) compared to nouns indicate that prompt changes related to actions and relationships in a sentence (verbs) may have a more disruptive effect on model performance in reasoning than changes in descriptive or referential elements (nouns).It is possible that models require different levels of abstraction or generality depending on the dataset's task structure, with reasoning tasks being negatively impacted from more specific terms, as these might complicate the LLMs ability to follow logical progression.In particular, Llama-3.1-70b-instruct and Mistral-Large 2 exhibited very strong negative correlations, suggesting that these models are especially sensitive to prompt variations in tasks requiring problem-solving and reasoning.Their heightened sensitivity to changes in verb specificity could be attributed to differences in their pre-training data distributions or the manner in which they handle syntactic dependencies.Llama-3.1-70B-Instruct and Mistral Large 2 may have been exposed to fewer domain-specific verbs, leading to increased difficulty when processing specific verb synonyms.Further research into the architectural differences between the considered models might reveal whether these sensitivities are due to inherent biases in the attention mechanisms or pre-training corpora.While this model-depended sensitivity might reflect model-specific weaknesses, the small number of significant results overall (5 out of 24 evaluations) suggests that synonymization does not pose a substantial challenge for most LLMs in the tested tasks.</p>
<p>Q3.Is there a specific level of prompt specificity for nouns and verbs that results in the best LLM performance in question answering and reasoning tasks across different models?</p>
<p>To address this question, we analyzed the outputs of the original, unmodified samples for each model and dataset, applying a stricter evaluation criterion compared to previous analyses by using exact match to identify correct answers.This approach was adopted to eliminate all noise and ensure higher precision in the evaluation.Additionally, we computed the prompt specificity for each part of speech (nouns and verbs) in the original prompts.The aggregation of correct answers across all datasets for each model is visualized in Fig. 10.In each histogram, the x-axis represents the prompt specificity for the respective part of speech, while the y-axis indicates the count of correct answers.A dashed line marks the prompt specificity associated with the highest number of correct answers for each model.For nouns, the distributions converge around similar prompt specificity values, ranging from 17.72 to 18.79.The more recent models, Llama-3.1 and Mistral-Large 2, exhibit higher median specificities, whereas Flan-T5-XL and Granite-13B achieve optimal performance with less specific prompts.This indicates a positive correlation between higher prompt specificity for nouns and the number of correct answers; however, beyond a certain threshold, increasing specificity may not enhance performance across all models and could, in fact, negatively impact accuracy.A similar pattern is observed for verbs, with the highest concentration of correct answers corresponding to prompt specificities between 8.4 and 9.5.Granite-13B performs best with a higher verb specificity (9.5), while other models achieve their optimal performance at slightly lower prompt specificities.From these observations, we can infer that for the original, unmodified samples, a moderate level of specificity improves model performance.However, excessive or insufficient specificity may degrade performance across models.We apply this approach to the modified instructions, examining each replacement level, specificity level, model, and dataset combination.The resulting prompt specificities for each parameter combination are displayed in Appendix B, Tab. 9.</p>
<p>Answer.Based on this assessment, we derive the ranges of optimal prompt specificity for each model and part of speech, displayed in Tab. 8, where the LLM achieves the highest performance across all datasets.The results indicate that there is a range of prompt specificity for both nouns and verbs, beyond which further decreases or increases in specificity can negatively impact performance.For nouns, the optimal specificity falls between 17.72 and 19.70, with newer models like Llama-3.1 and Mistral-Large 2 performing better with slightly higher specificity than smaller models like Granite-13B and Flan-T5-XL.Similarly, for verbs, optimal performance occurs between 8.08 and 10.57, with Granite-13B benefitting from higher specificity compared to other models.These findings suggest that while some level of specificity improves accuracy, exceeding the optimal range can diminish performance, highlighting the importance of fine-tuning prompt specificity based on the model and task.Particularly, in domain-specific NLP applications, one should consider that increasing prompt specificity, particularly for verbs, may not always yield better performance.Instead, balancing specificity with generality could help models retain broader reasoning capabilities while maintaining accuracy.</p>
<p>Conclusion</p>
<p>In this study, we examined the effects of synonymizing nouns, verbs, and adjectives with varying levels of specificity across four different models.By applying this synonymization to samples from three domain-specific datasets for questionanswering and reasoning tasks, we identified a range of prompt specificity for nouns and verbs, consistent across all models, that leads to the best LLM performance for these tasks.Contrary to our initial hypothesis that increasing prompt specificity would improve LLM outputs, our findings show no significant performance changes when increasing noun specificity in prompts.However, using more specific verb synonyms resulted in a negative impact on performance in reasoning tasks.This suggests that prompt design might benefit more from focusing on clarity, contextual appropriateness and other linguistic factors rather than purely on specificity.We also introduced an approach to quantify adjective specificity that shows an overall intermediate-strong correlation with the ranking of GPT-4o.However, our experiments showed that adjectives are underrepresented in our question answering datasets and are not suitable for further analysis to quantify their impact on LLM performance in our scope.Despite that, it is plausible that adjectives play a more critical role in tasks involving subjective analysis, such as sentiment analysis, where they carry significant emotional or descriptive meaning.Therefore, future research could explore how adjective specificity affect the LLM performance in other NLP tasks, such as sentiment analysis or narrative text generation, where they have a more substantial role compared to the neutral toned question answering tasks.</p>
<p>Limitations</p>
<p>Despite the promising results obtained from our exploration of specificity affects LLM performance, several limitations remain that warrant further attention.First, our approach relies on WSD to identify and substitute synonyms.Although, our algorithm guarantees the correct sense of a word most of the time, as shown in Tab. 3, it misclassifies the sense in some cases.This limitation can result in the selection of inappropriate synonyms with respect to the given context, which may cause confusion for the model and affect performance by introducing noise into the input and reducing the precision of the generated outputs.In the future, one could improve the WSD process by finetuning Llama-3.1-70B-Instructspecifically on the WSD task similar to Wahle, Ruas, Meuschke, et al., 2021, since the base version already provided promising results (79% accuracy).A less computational resource-intensive method to address this issue would be to incorporate human feedback after the WSD process, which would allow a manual correction of the false senses.Second, our analysis primarily focused on the domains of STEM, medicine, and law.While these areas are widely applicable, they do not fully encompass the breadth of all potential fields and contexts in which domain-oriented prompt engineering may be applied.Consequently, the findings in this work may not generalize to other domains, limiting the overall applicability of the results.Future research could extend the scope to include a wider variety of domains such as psychology, finance or engineering, to enhance the robustness of the conclusions.Finally, the introduction of an adjective specificity equation represents an initiative toward quantifying and ranking adjectives.Although, this equation shows promising results according to our evaluation in the STEM, law and medicine domains, its effectiveness needs to be rigorously tested on large scale across a larger scope of domains to determine its generalizability.</p>
<p>Acknowledgments</p>
<p>I would like to extend my deepest gratitude to Jan Philip Wahle and Dr. Terry Ruas for their invaluable feedback, insightful perspectives, and continuous support throughout the course of my thesis.I am also profoundly thankful to Prof. Dr. Bela Gipp for the opportunity to conduct this research under his supervision, enabling me to contribute to the NLP and Generative AI research community.</p>
<p>Additionally, I would like to acknowledge IBM for providing access to their Generative AI infrastructure, which was integral to our experimental work.Finally, I am sincerely grateful to my friends and family, particularly Jana Fedjaev, for their steadfast support during this intense phase of my life.</p>
<p>In the conduct of this research project, we used specific artificial intelligence tools and algorithms GPT-4o to assist with rephrasing sentences.While these tools have augmented our capabilities and contributed to our findings, it's pertinent to note that they have inherent limitations.We have made every effort to use AI in a transparent and responsible manner.Any conclusions drawn are a result of combined human and machine insights.This is an automatic report generated with © AI Usage Cards https://ai-cards.org</p>
<p>A Figures</p>
<p>Figure 11: Prompt Templates.Overview of all the prompt templates used for performing the questionanswering and reasoning tasks (MMLU, GPQA and GSM8K), and the LLM-as-a-Judge experiment for the adjective specificity measure evaluation.</p>
<p>Figure 12: Average Accuracy Comparison across multiple LLMs for Adjectives.Average accuracy comparison across all models (Granite-13B-Instruct-v2, Flan-T5-XL, LLaMA-3.1-70B-Instruct, and Mistral-Large 2) for the datasets (MMLU, GSM8K, GPQA) for original and 100% high specificity instructions.Due to the small sample size of 172 instructions in total, these results cannot be considered as significant.</p>
<p>Figure 13: Average Accuracy Comparison across multiple LLMs for Verbs.Average accuracy comparison across all models (Granite-13B-Instruct-v2, Flan-T5-XL, LLaMA-3.1-70B-Instruct, and Mistral-Large 2) for the datasets (MMLU, GSM8K, GPQA) for varying specificity levels (low, intermediate, high) and replacement levels of synonymizable verbs (33%, 67%, 100%).</p>
<p>Table 9: Optimal Specificities for each Dataset.Optimal prompt specificities for nouns and verbs for each combination of replacement ratios (33%, 67%, 100%) and specificity levels (low, intermediate, high) for all data combined, MMLU, GSM8K and GPQA.</p>
<p>Parameter</p>
<p>Value temperature 0 max tokens 500 random seed 31415</p>
<p>MMLU: Task686</p>
<p>Proteins were shown to move about in a plane of the plasma membrane when mouse cellsurface proteins and human cell-surface proteins were observed to integrate along a fused mouse-human cell plasma membrane.Which of the following cell culture techniques was most likely employed in order to yield these results?</p>
<p>(A)Producing a heterokaryon (B)Producing a hybrid cell (C)Isolating an immortal variant cell from culture and using it to create a cell line (D)Inserting a tumor-inducing virus into a normal cell to initiate transformation Question:</p>
<p>Choices:</p>
<p>Figure 14: Example for Task686 (MMLU).An example from the MMLU dataset: "Task686".This question is from the biology domain, which requires domain knowledge at a college level.</p>
<p>Instruction Specificity Replacement Ratio</p>
<p>A breeder of dogs induced a purchaser to buy a puppy by representing that it was a registered basset hound, when in fact the breeder knew it was a mixed breed.The purchaser later discovered that the representation was false.She wants to sue to disaffirm the contract and get a refund.What legal theory would be best applicable to decide this case?</p>
<p>Original -A breeder of domesticated animals induced a client to buy a puppy by representing that it was a registered basset hound dog, when in record the breeder knew it was a mixed variety.The client later discovered that the psychosexuality was false.She wants to sue to disaffirm the grant and get a refund.What legal explanation would be best applicable to decide this instance?</p>
<p>Low 100%</p>
<p>A breeder of mongrels induced a emptor to buy a puppy by representing that it was a registered basset Afghan hound, when in basics the breeder knew it was a mixed animal group.The emptor later discovered that the version was false.She wants to sue to disaffirm the charter and get a refund.What legal atomistic theory would be best applicable to decide this bit?</p>
<p>Intermediate 100%</p>
<p>A breeder of puppys induced a customer agent to buy a puppy by representing that it was a registered basset greyhound, when in rudiments the breeder knew it was a mixed bloodstock.The customer agent later discovered that the appearance was false.She wants to sue to disaffirm the adhesion contract and get a refund.What legal atomism would be best applicable to decide this humiliation?High 100%</p>
<p>Framework . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .3.2 Lexical Database . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .3.3 Specificity Measures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .3.4 Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .3.5 Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .</p>
<p>Figure 1 :
1
Figure 1: Specificity-based Synonymization Framework.Representation of the specificity-based synonymization framework used to synonymize the prompt instructions with varying specificities of all datasets.The preprocessing includes five key steps starting with the retrieval of parts (I) of speech from the original instruction, crawling synonyms (II) and calculate the specificity scores (III) for all parts of speech (the green colored boxes include WSD in the algorithm), categorizing the synonyms (IV) into low, intermediate, high specificity and finally synonymize the original instructions (V) with three different replacement ratios (33%, 67%, 100%).Additionally, there is a step-by-step example sampled from the GPQA dataset, synonymizing nouns with varying specificity synonyms.</p>
<p>Figure 2 :
2
Figure 2: Confusion Matrix of WSD Model Evaluation.The performance agreement of Llama-3.1-70B-Instruct and finetuned T5 for WSD when predicting against the human evaluated ground truth.</p>
<p>Figure 3 :
3
Figure 3: Spearman-Correlations for Adjective Ranking.The histogram displays the distribution of Spearman correlations from the LLM-as-a-judge experiment, which compares the model's qualitative ranking of adjective specificity with the calculated specificity score measure for 91 samples.The median Spearman correlation in this distribution is 0.50.</p>
<p>Figure 4 :
4
Figure 4: Example for Prompt Specificity Calculation.This example schematically illustrates the calculation of the prompt specificity, by aggregating the specificities of one part of speech (nouns in this case) and calculating the average that we call prompt specificity.Additionally, it shows the prompt specificity change from 19.36 to 20.60 after we substitute the noun dog with the more specific synonym poodle.</p>
<p>Figure 6 :
6
Figure 6: Specificity Score Distributions for nouns and verbs according to specificity.The histogram represents the distribution, including Kernel-Density-Estimations, of the specificity score for nouns and verbs according to their specificity.Based on the intersection points of the Kernel-Density estimations, we derive the specificity level boundaries B Low-Intermediate = 17.74 and B Intermediate-High = 20.90 for nouns, and B Low-Intermediate = 11.17 and B Intermediate-High = 13.74 for verbs.</p>
<p>Figure 7 :
7
Figure7: Average Accuracy Comparison across multiple LLMs for Nouns.Average accuracy comparison across all models (Granite-13B-Instruct-v2, Flan-T5-XL, LLaMA-3.1-70B-Instruct, and Mistral-Large 2) for the datasets (MMLU, GSM8K, GPQA) for varying specificity levels (low, intermediate, high) and replacement levels of synonymizable nouns (33%, 67%, 100%).</p>
<p>( a )
a
Distribution of correct answers for nouns.(b) Distribution of correct answers for verbs.</p>
<p>Figure 10 :
10
Figure10: Distribution of correct Answers for each Model.These histograms represent the distributions of correct answers for the original samples for each model and part of speech.The x-axis depicts the respective prompt specificity of the unmodified instructions, while the y-axis counts the correct answers for the corresponding prompt specificity.</p>
<p>Table 1 :
1
Overview of data.
OriginalProcessed SamplesDatasetSamples Nouns Verbs AdjectivesGPQA25018115423GSM8k2501741594MMMLU17901376985145This table showsthe number of samples before and after the prepro-cessing steps, indicating that nouns and verbs aresuitable and significantly enough represented in thedata, while adjectives seem underrepresented.</p>
<p>Table 2 :
2
Part of Speech Hypernyms Hyponyms Average #synsets (Hypernym) Average #synsets (Hyponym) Average Number of synsets evaluation.Results of the assessment of average number of synsets for hyper-and hyponyms.A Mann-Whitney U test was performed to quantify whether the mean of synsets of hypernyms is significantly greater than the mean of synsets of hyponyms.
Statp-value</p>
<p>Table 4 :
4
Comparison of Adjective Specificity Measure Parameters.The number of words with similar meaning ssw, number of direct synonyms s, number of direct and indirect antonyms a and number of senses l of the adjectives good, phenomenal, small and tiny.
1-70B-Instruct model, which offers faster processing andsignificantly reduces computation time comparedto the fine-tuned T5 model used by Wahle, Ruas,Meuschke, et al., 2021. To validate this modelsubstitution, we benchmarked it against</p>
<p>Table 5 :
5
Results of Kolmogorov-Smirnov Test.
Statistic p-valueNouns0.2330.0Verbs0.0683.15e-34Adjectives0.0871.08e-5</p>
<p>Table 6 :
6
Specificity Score Intervals.Interval boundaries for the specificity categories low, intermediate and high for nouns and verbs.
SpecificityNouns Lower Bound Upper Bound Lower Bound Upper Bound VerbsLow Specificity10.13017.747.8711.17Intermediate Specificity17.74120.9011.17113.74High Specificity20.901195.6213.74121.16</p>
<p>Table 7 :
7
Correlation Results.Results (Spearman Correlation) for each model, grouped on each part of speech and each dataset.Values in bold display the significant correlations, where the p-value is under 0.05, while underlined values represent the greatest correlation out of all significant ones.
High</p>
<p>Table 8 :
8
Optimal Specificities.Ranges of prompt specificities for nouns and verbs, in which the LLM perform best across all datasets.
ModelNouns Lower Upper Median Lower Upper Median VerbsGranite-13b-instruct-v2 17.2422.0018.959.2014.7410.55Flan-T5-XL17.0322.0418.948.2914.8010.47Llama-3.1-70b-instruct17.5822.5719.418.0814.1410.57Mistral-Large 217.5822.5719.709.0314.4910.57</p>
<p>Table 10 :
10
Model Parameters.Overview of parameters used for prompting in all the experiments in this study.
C Data overviewDatasetDomainsDifficultySamplestask686biologycollege162task687chemistrycollege110task688computer sciencecollege113task689mathematicscollege113MMLUtask691 task699 task700 task701physics biology chemistry computer sciencecollege high school high school high school96 184 174 111task708physicshigh school169task710statisticshigh school175task729lawprofessional302task730medicineprofessional183GSM8Kmathematicsgrade school7473GPQAbiology, chemistry, physics PhD graduate448</p>
<p>Table 11 :
11
Overview of Datasets.The overview of all datasets used in our experiments with the corresponding domains, difficulty levels and initial sample sizes.</p>
<p>Table 12 :
12
Examples for processed Instructions.This is one example of a 100% replacement of nouns for low, intermediate and high specificity.Words highlighted in red represent synonymizable nouns in the original sample, and in the modified samples, they denote the corresponding synonyms for low, intermediate, and high specificity.</p>
<p>We used word tokenize and parts of speech tagging functions in the NLTK python package with the universal tagset(Petrov, Das, and McDonald,<br />
): https://www.nltk.org/
Flan-T5-XLMMLU: Task687The 13C spectrum of which isomer of C6H14 has lines with five distinct chemical shifts?(A) hexane (B) 2-methylpentane (C) 3-methylpentane (D) 2,3-dimethylbutane Question:Choices:Figure15: Example for Task687 (MMLU).An example from the MMLU dataset: "Task687".This question is from the chemistry domain, which requires domain knowledge at a college level.MMLU: Task688Which of the following comes closest to being a perfectly secure encryption scheme?(A)The Caesar Cipher, a substitution cipher (B)DES (Data Encryption Standard), a symmetric-key algorithm (C)Enigma, a transposition cipher (D)One-time padQuestion:Choices:Figure16: Example for Task688 (MMLU).An example from the MMLU dataset: "Task688".This question is from the computer science domain, which requires domain knowledge at a college level.MMLU: Task689A tree is a connected graph with no cycles.How many nonisomorphic trees with 5 vertices exist?Question:Choices:Figure17: Example for Task689 (MMLU).An example from the MMLU dataset: "Task689".This question is from the mathematics domain, which requires domain knowledge at a college level.MMLU: Task691A resistor in a circuit dissipates energy at a rate of 1 W. If the voltage across the resistor is doubled, what will be the new rate of energy dissipation?(A)0.25 W (B)0.5 W (C)1 W (D)4 W Question:Choices:Figure18: Example for Task691 (MMLU).An example from the MMLU dataset: "Task691".This question is from the physics domain, which requires domain knowledge at a college level.MMLU: Task699Unlike large populations, small populations are vulnerable to various processes that draw populations down an extinction vortex toward smaller and smaller populations until no individuals survive.Which of the following statements correctly identifies the factors that endanger a population?Question:Choices:Figure19: Example for Task699 (MMLU).An example from the MMLU dataset: "Task699".This question is from the biology domain, which requires domain knowledge at a high school level.MMLU: Task700The dimerization of NO2(g) to N2O4(g) is an endothermic process.Which of the following will, according to Le Châtelier's principle, increase the amount of N2O4 in a reaction vessel?An example from the MMLU dataset: "Task701".This question is from the computer science domain, which requires domain knowledge at a high school level.MMLU: Task708An object is released from rest and falls a distance h during the first second of time.How far will it fall during the next second of time?Question:Choices:Figure22: Example for Task708 (MMLU).An example from the MMLU dataset: "Task708".This question is from the physics domain, which requires domain knowledge at a high school level.MMLU: Task710The director of a local food bank asks for data on all donations given during the month of November.Of the 100 checks received, the average donation is $155 with a standard deviation of $32.Which of the following is the most appropriate statement?(A)This November, the average donation is $155.(B)50% of all donations this November are more than $155.(C)We are 95% confident that the average donation in November is between about $91 and $219.(D)We are 95% confident that the average donation in November is between about $149 and $161.Question:Choices:Figure23: Example for Task710 (MMLU).An example from the MMLU dataset: "Task710".This question is from the statistics domain, which requires domain knowledge at a high school level.MMLU: Task729A seller sold his boat to a buyer.During negotiations, the buyer said that he planned to sail the boat on the open seas.The seller told the buyer that the boat was seaworthy and had never sustained any significant damage.In fact, the hull of the boat had been badly damaged when the seller had run the boat aground.The seller had then done a cosmetic repair to the hull rather than a structural repair.The buyer relied on the seller's representations and paid a fair price for a boat in good repair, only to discover after the sale was completed that the hull was in fact badly damaged and in a dangerous condition.The seller has refused to refund any of the buyer's money, and the buyer is contemplating suing the seller.Under what theory would the buyer be most likely to recover?Question:Choices:Figure24: Example for Task729 (MMLU).An example from the MMLU dataset: "Task729".This question is from the law domain, which requires domain knowledge at a professional level.MMLU: Task730A 22-year-old woman comes to the physician in October for a follow-up examination.She feels well.She has a 2-year history of type 1 diabetes mellitus controlled with insulin.She had a normal Pap smear 3 months ago and saw her ophthalmologist 6 months ago.Her 67-year-old grandmother has breast cancer.She is 168 cm (5 ft 6 in) tall and weighs 57 kg (125 lb); BMI is 20 kg/m2 .Her hemoglobin A1c is 6.2%, and fingerstick blood glucose concentration is 118 mg/dL.Which of the following health maintenance recommendations is most appropriate at this time?An example from the MMLU dataset: "Task730".This question is from the medicine domain, which requires domain knowledge at a professional level.GPQAA large gene has dozens of exons, of which the central ones code for folded triple helical repeats that connect the cytoskeleton with sarcolemma and extracellular space.Each exon usually codes for one folded triple alpha helix.The most common mutations of the gene are central exon deletions that create out-of-frame peptides and progressive degenerative organ waste.A solution is to deliver a Morpholino that recognizes the 5' end of the out-of-frame exon in pre-mRNA.The molecule prevents binding of the spliceosome and creates exon skipping and in-frame joining.Several missing exons are well tolerated by an organism.Which structure below is not involved in the proposed therapy?
Specificity in English for Academic Purposes (EAP): A Corpus Analysis of Lexical Bundles in Academic Writing. L Ang, K Tan, 10.17576/3L-2018-2402-072018</p>
<p>Prompt-Source: An Integrated Development Environment and Repository for Natural Language Prompts. Stephen H Bach, arXiv:2202.01279Mar. 2022</p>
<p>Kolmogorov-Smirnov Test: Overview. Vance W Berger, Yanyan Zhou, 10.1002/9781118445112.stat06558Wiley StatsRef: Statistics Reference Online. John Wiley &amp; Sons, Ltd20149781118445112</p>
<p>On Abstraction: Decoupling Conceptual Concreteness and Categorical Specificity. Marianna Bolognesi, Christian Burgers, Tommaso Caselli, 10.1007/s10339-020-00965-9Cognitive Processing. 1612-4782213Aug. 2020</p>
<p>Tom B Brown, arXiv:2005.14165Language Models Are Few-Shot Learners. July 2020</p>
<p>Banghao Chen, arXiv:2310.14735[cs.CL].urlUnleashing the potential of prompt engineering in Large Language Models: a comprehensive review. 2024</p>
<p>A tutorial on kernel density estimation and recent advances. Yen-Chi Chen, 10.1080/24709360.2017.1396742Biostatistics &amp; Epidemiology. 112017</p>
<p>Scaling Instruction-Finetuned Language Models. Hyung Chung, Won, 10.48550/ARXIV.2210.114162022</p>
<p>Training Verifiers to Solve Math Word Problems. Karl Cobbe, arXiv:2110.14168Nov. 2021a</p>
<p>Training Verifiers to Solve Math Word Problems. arXiv:2110.141682021barXiv preprint</p>
<p>Qingxiu Dong, arXiv:2301.00234[cs.CL].urlA Survey on In-context Learning. 2024</p>
<p>The Hitchhiker's Guide to Testing Statistical Significance in Natural Language Processing. Rotem Dror, 10.18653/v1/P18-1128Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics. Long Papers, ) Ed, Iryna By, Yusuke Gurevych, Miyao, the 56th Annual Meeting of the Association for Computational LinguisticsMelbourne, AustraliaAssociation for Computational LinguisticsJuly 2018</p>
<p>Abhimanyu Dubey, arXiv:2407.21783[cs.AI].urlThe Llama 3 Herd of Models. 2024</p>
<p>WordNet: An electronic lexical database. Christiane Fellbaum, 1998MIT press</p>
<p>Measuring Massive Multitask Language Understanding. Dan Hendrycks, Proceedings of the International Conference on Learning Representations (ICLR). the International Conference on Learning Representations (ICLR)2021</p>
<p>Ibm, ?topic=models-granite-13b -instruct -v2 -model -card. 2024Granite 13B Instruct v2 Model Card</p>
<p>Is Continuous Prompt a Combination of Discrete Prompts? Towards a Novel View for Interpreting Continuous Prompts. Tianjie Ju, 10.18653/v1/2023.findings-acl.494Findings of the Association for Computational Linguistics: ACL 2023. Anna Rogers, Jordan Boyd-Graber, Naoaki Okazaki, Toronto, CanadaAssociation for Computational LinguisticsJuly 2023</p>
<p>Jared Kaplan, arXiv:2001.08361[cs.LG].urlScaling Laws for Neural Language Models. 2020</p>
<p>Takeshi Kojima, arXiv:2205.11916[cs.CL]Large Language Models are Zero-Shot Reasoners. 2023</p>
<p>Alina Leidinger, Robert Van Rooij, Ekaterina Shutova, arXiv:2311.01967The Language of Prompting: What Linguistic Properties Make a Prompt Successful. Nov. 2023</p>
<p>Pre-Train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing. Pengfei Liu, 10.1145/3560815ACM Computing Surveys. 0360-030055Sept. 2023</p>
<p>Yiheng Liu, arXiv:2401.02038[cs.CL]Understanding LLMs: A Comprehensive Overview from Training to Inference. 2024</p>
<p>How Are Prompts Different in Terms of Sensitivity?. Sheng Lu, Hendrik Schuff, Iryna Gurevych, arXiv:2311.07230Nov. 2023</p>
<p>Mann-Whitney U Test. Patrick E Mcknight, Julius Najab, 10.1002/9780470479216.corpsy0524The Corsini Encyclopedia of Psychology. John Wiley &amp; Sons, Ltd20109780470479216</p>
<p>WordNet: a lexical database for English. George A Miller, 10.1145/219717.219748Commun. ACM. 0001-078238Nov. 1995</p>
<p>Mistral Large 2: The Next Generation of High-Performance Models. Mistralai, 2024</p>
<p>Using of Jaccard coefficient for keywords similarity. Suphakit Niwattanakul, Proceedings of the international multiconference of engineers and computer scientists. the international multiconference of engineers and computer scientists20131</p>
<p>Training Language Models to Follow Instructions with Human Feedback. Long Ouyang, 10.48550/arXiv.2203.02155arXiv:2203.02155Mar. 4, 2022</p>
<p>Slav Petrov, Dipanjan Das, Ryan Mcdonald, arXiv:1104.2086[cs.CL].urlA Universal Part-of-Speech Tagset. 2011</p>
<p>Direct Preference Optimization: Your Language Model is Secretly a Reward Model. Rafael Rafailov, Advances in Neural Information Processing Systems. Curran Associates, Inc202336</p>
<p>Vipula Rawte, arXiv:2309.11064Exploring the Relationship between LLM Hallucinations and Prompt Linguistic Nuances: Readability, Formality, and Concreteness. Sept. 2023</p>
<p>David Rein, arXiv:2311.12022[cs.AI].urlGPQA: A Graduate-Level Google-Proof Q&amp;A Benchmark. 2023</p>
<p>Prompt Programming for Large Language Models: Beyond the Few-Shot Paradigm. Laria Reynolds, Kyle Mcdonell, arXiv:2102.07350Feb. 2021</p>
<p>Timo Schick, Hinrich Schütze, arXiv:2001.07676Exploiting Cloze Questions for Few Shot Text Classification and Natural Language Inference. Jan. 2021</p>
<p>Taylor Shin, arXiv:2010.15980AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts. Nov. 2020</p>
<p>An Informationtheoretic Approach to Prompt Engineering Without Ground Truth Labels. Taylor Sorensen, 10.18653/v1/2022.acl-long.60arXiv:2203.11364Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 60th Annual Meeting of the Association for Computational Linguistics20221</p>
<p>Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models. Aarohi Srivastava, arXiv:2206.04615[cs.CL]2023</p>
<p>Prompt Engineering: A Methodology for Optimizing Interactions with AI-Language Models in the Field of Engineering. Hugo Touvron, 10.15446/dyna.v90n230.111700arXiv:2302.13971[cs.CL].urlarXiv: 2406.19898 [cs.CL]. urlParaphrase Types Elicit Prompt Engineering Capabilities. Wahle2023. Nov. 2023. Jan Philip,. 2021. Jan Philip,. 2024arXiv preprintLLaMA: Open and Efficient Foundation Language Models</p>
<p>Prompt Engineering in Consistency and Reliability with the Evidence-Based Guideline for LLMs. Li Wang, 10.1038/s41746-024-01029-4Digital Medicine. 2398-635241Feb. 2024</p>
<p>Yizhong Wang, arXiv:2204.07705Super-NaturalInstructions: Generalization via Declarative Instructions on 1600+ NLP Tasks. Oct. 2022</p>
<p>Jules White, arXiv:2302.11382[cs.SE].urlA Prompt Pattern Catalog to Enhance Prompt Engineering with ChatGPT. 2023</p>
<p>Yikuan Xia, arXiv:2405.04820[cs.CL]APrompt4EM: Augmented Prompt Tuning for Generalized Entity Matching. 2024</p>
<p>Hallucination is Inevitable: An Innate Limitation of Large Language Models. Ziwei Xu, Sanjay Jain, Mohan Kankanhalli, arXiv:2401.11817[cs.CL].url2024</p>
<p>Prompt Learning with Structured Semantic Knowledge Makes Pre-Trained Language Models Better. Hai-Tao Zheng, 10.3390/electronics12153281July 20233281</p>
<p>Granite-13b-instruct-v2. MMLU</p>
<p>GSM8K Granite-13b-instruct-v2. </p>
<p>GPQA Granite-13b-instruct-v2. </p>
<p>GSM8K Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May? Natalia sold 48/2 = &lt;&lt;48/2=24&gt;&gt;24 clips in May. Natalia sold 48+24 = &lt;&lt;48+24=72&gt;&gt;72 clips altogether in. April and May</p>
<h1>72 Question: Answer: Figure 27: Example for GSM8K. An example from the GSM8K dataset. This question is from the mathematics domain, which requires domain knowledge at a grade school level. # , # ,</h1>            </div>
        </div>

    </div>
</body>
</html>
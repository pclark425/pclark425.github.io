<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7498 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7498</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7498</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-136.html">extraction-schema-136</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <p><strong>Paper ID:</strong> paper-258041354</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2304.03439v3.pdf" target="_blank">Evaluating the Logical Reasoning Ability of ChatGPT and GPT-4</a></p>
                <p><strong>Paper Abstract:</strong> Harnessing logical reasoning ability is a comprehensive natural language understanding endeavor. With the release of Generative Pretrained Transformer 4 (GPT-4), highlighted as"advanced"at reasoning tasks, we are eager to learn the GPT-4 performance on various logical reasoning tasks. This report analyses multiple logical reasoning datasets, with popular benchmarks like LogiQA and ReClor, and newly-released datasets like AR-LSAT. We test the multi-choice reading comprehension and natural language inference tasks with benchmarks requiring logical reasoning. We further construct a logical reasoning out-of-distribution dataset to investigate the robustness of ChatGPT and GPT-4. We also make a performance comparison between ChatGPT and GPT-4. Experiment results show that ChatGPT performs significantly better than the RoBERTa fine-tuning method on most logical reasoning benchmarks. With early access to the GPT-4 API we are able to conduct intense experiments on the GPT-4 model. The results show GPT-4 yields even higher performance on most logical reasoning datasets. Among benchmarks, ChatGPT and GPT-4 do relatively well on well-known datasets like LogiQA and ReClor. However, the performance drops significantly when handling newly released and out-of-distribution datasets. Logical reasoning remains challenging for ChatGPT and GPT-4, especially on out-of-distribution and natural language inference datasets. We release the prompt-style logical reasoning datasets as a benchmark suite and name it LogiEval.</p>
                <p><strong>Cost:</strong> 0.013</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7498.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7498.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ConTRoL - ChatGPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ConTRoL dataset evaluated with ChatGPT (gpt-3.5-turbo API)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>ChatGPT (gpt-3.5-turbo, API) evaluated on ConTRoL, an NLI dataset focusing on contextual/logical reasoning; accuracy reported and compared to human baselines and RoBERTa.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGPT (gpt-3.5-turbo, API)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-tuned conversational model from OpenAI (gpt-3.5-turbo), accessed via API (version March 23, 2023).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>ConTRoL (test)</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>natural language inference / reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Premise-hypothesis pairs testing contextual and logical reasoning under the NLI framework; labels: entailment, contradiction, neutral.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>accuracy (%)</td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td>average accuracy 87.00%, ceiling 94.00% (reported human baseline by authors)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>accuracy 58.45%</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>instruction-prompt with specifying labels; single in-context example provided; OpenAI Eval framework</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td>reported by authors (average and ceiling performance of human testees)</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>ChatGPT outperforms RoBERTa on ConTRoL but remains substantially below human average; authors note ChatGPT can struggle to follow NLI instructions without in-context example.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating the Logical Reasoning Ability of ChatGPT and GPT-4', 'publication_date_yy_mm': '2023-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7498.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7498.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ConTRoL - GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ConTRoL dataset evaluated with GPT-4 (API)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-4 (default-gpt-4) evaluated on ConTRoL NLI benchmark; accuracy reported and compared to human baselines and other models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4 (default-gpt-4, API)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Generative Pre-trained Transformer 4 by OpenAI, instruction-following multimodal model (API version March 14, 2023).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>ConTRoL (test)</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>natural language inference / reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Premise-hypothesis pairs testing contextual and logical reasoning under the NLI framework; labels: entailment, contradiction, neutral.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>accuracy (%)</td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td>average accuracy 87.00%, ceiling 94.00% (reported human baseline by authors)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>accuracy 56.40%</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>instruction-prompt with specifying labels; OpenAI Eval framework; authors also ran Chat UI checks</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td>reported by authors (average and ceiling performance of human testees)</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>GPT-4 performs slightly worse than ChatGPT on ConTRoL per reported numbers; authors remark GPT-4 sometimes fails to output labels steadily for NLI tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating the Logical Reasoning Ability of ChatGPT and GPT-4', 'publication_date_yy_mm': '2023-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7498.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7498.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ConjNLI - ChatGPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ConjNLI dataset evaluated with ChatGPT (gpt-3.5-turbo API)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>ChatGPT evaluated on ConjNLI, a stress-test NLI dataset over conjunctive sentences; model accuracy compared to human baseline and RoBERTa.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGPT (gpt-3.5-turbo, API)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-tuned conversational model from OpenAI (gpt-3.5-turbo), accessed via API (version March 23, 2023).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>ConjNLI (test)</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>natural language inference / logical reasoning (conjunctions)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Automatically generated premise-hypothesis pairs testing inference over conjunctive sentences (removal/addition/replacement of conjuncts).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>accuracy (%)</td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td>average accuracy 89.00%, ceiling 100.00% (reported human baseline by authors)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>accuracy 47.03%</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>instruction-prompt with specifying labels; in-context example provided</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td>reported by authors (average and ceiling performance of human testees)</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>ChatGPT outperforms RoBERTa on ConjNLI but remains far below human performance; dataset focuses on conjunction-specific logical phenomena.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating the Logical Reasoning Ability of ChatGPT and GPT-4', 'publication_date_yy_mm': '2023-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7498.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7498.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ConjNLI - GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ConjNLI dataset evaluated with GPT-4 (API)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-4 evaluated on ConjNLI; achieves substantially higher accuracy than ChatGPT and RoBERTa per reported results.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4 (default-gpt-4, API)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Generative Pre-trained Transformer 4 by OpenAI, instruction-following multimodal model (API version March 14, 2023).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>ConjNLI (test)</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>natural language inference / logical reasoning (conjunctions)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Automatically generated premise-hypothesis pairs testing inference over conjunctive sentences (removal/addition/replacement of conjuncts).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>accuracy (%)</td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td>average accuracy 89.00%, ceiling 100.00% (reported human baseline by authors)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>accuracy 72.71%</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>instruction-prompt with specifying labels; OpenAI Eval framework</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td>reported by authors (average and ceiling performance of human testees)</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>GPT-4 shows strong improvement on this conjunctive NLI stress test, narrowing the gap toward human baseline though still below reported human averages/ceilings.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating the Logical Reasoning Ability of ChatGPT and GPT-4', 'publication_date_yy_mm': '2023-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7498.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e7498.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>HELP - ChatGPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>HELP dataset evaluated with ChatGPT (gpt-3.5-turbo API)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>ChatGPT evaluated on HELP, a dataset focused on monotonicity reasoning (natural logic); model accuracy compared to human baseline and RoBERTa.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGPT (gpt-3.5-turbo, API)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-tuned conversational model from OpenAI (gpt-3.5-turbo), accessed via API (version March 23, 2023).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>HELP (train used for evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>natural language inference / monotonicity reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Dataset designed to probe monotonicity/entailment phenomena in natural logic; identifies shortcomings of neural models in monotonicity reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>accuracy (%)</td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td>average accuracy 81.00%, ceiling 95.00% (reported human baseline by authors)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>accuracy 42.13%</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>instruction-prompt with specifying labels; in-context example provided</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td>reported by authors (average and ceiling performance of human testees)</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>ChatGPT modestly outperforms RoBERTa but is far below human baselines on monotonicity reasoning; HELP focuses specifically on monotonicity phenomena.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating the Logical Reasoning Ability of ChatGPT and GPT-4', 'publication_date_yy_mm': '2023-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7498.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e7498.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>HELP - GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>HELP dataset evaluated with GPT-4 (API)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-4 evaluated on HELP monotonicity NLI dataset; reported to slightly outperform ChatGPT and RoBERTa but remain far below human baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4 (default-gpt-4, API)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Generative Pre-trained Transformer 4 by OpenAI, instruction-following multimodal model (API version March 14, 2023).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>HELP (train used for evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>natural language inference / monotonicity reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Dataset designed to probe monotonicity/entailment phenomena in natural logic; identifies shortcomings of neural models in monotonicity reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>accuracy (%)</td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td>average accuracy 81.00%, ceiling 95.00% (reported human baseline by authors)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>accuracy 46.01%</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>instruction-prompt with specifying labels; OpenAI Eval framework</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td>reported by authors (average and ceiling performance of human testees)</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>GPT-4 slightly outperforms ChatGPT/ RoBERTa on HELP but remains well below human performance; authors highlight dataset-specific challenges.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating the Logical Reasoning Ability of ChatGPT and GPT-4', 'publication_date_yy_mm': '2023-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7498.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e7498.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MED - ChatGPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MED dataset evaluated with ChatGPT (gpt-3.5-turbo API)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>ChatGPT evaluated on MED (monotonicity reasoning dataset); accuracy compared to RoBERTa and human baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGPT (gpt-3.5-turbo, API)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-tuned conversational model from OpenAI (gpt-3.5-turbo), accessed via API (version March 23, 2023).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>MED (test)</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>natural language inference / monotonicity reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Dataset focusing on monotonicity-related inference derived from monotonicity rules; tests basic logical phenomena in natural language.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>accuracy (%)</td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td>average accuracy 91.00%, ceiling 99.00% (reported human baseline by authors)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>accuracy 55.02%</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>instruction-prompt with specifying labels; in-context example provided</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td>reported by authors (average and ceiling performance of human testees)</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>ChatGPT substantially outperforms RoBERTa on MED but remains well below human average; MED probes monotonicity-specific reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating the Logical Reasoning Ability of ChatGPT and GPT-4', 'publication_date_yy_mm': '2023-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7498.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e7498.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MED - GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MED dataset evaluated with GPT-4 (API)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-4 evaluated on MED monotonicity NLI dataset; achieves accuracy close to human baseline per authors' reported numbers.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4 (default-gpt-4, API)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Generative Pre-trained Transformer 4 by OpenAI, instruction-following multimodal model (API version March 14, 2023).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>MED (test)</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>natural language inference / monotonicity reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Dataset focusing on monotonicity-related inference derived from monotonicity rules; tests basic logical phenomena in natural language.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>accuracy (%)</td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td>average accuracy 91.00%, ceiling 99.00% (reported human baseline by authors)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>accuracy 89.42%</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>instruction-prompt with specifying labels; OpenAI Eval framework</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td>reported by authors (average and ceiling performance of human testees)</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>GPT-4 attains 89.42% on MED, approaching the reported human average (91%), suggesting strong performance on monotonicity NLI tasks; authors note variance across NLI datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating the Logical Reasoning Ability of ChatGPT and GPT-4', 'publication_date_yy_mm': '2023-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7498.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e7498.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TaxiNLI - ChatGPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>TaxiNLI dataset evaluated with ChatGPT (gpt-3.5-turbo API)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>ChatGPT evaluated on TaxiNLI, an NLI subset with fine-grained logical categories; results compared with RoBERTa and human baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGPT (gpt-3.5-turbo, API)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-tuned conversational model from OpenAI (gpt-3.5-turbo), accessed via API (version March 23, 2023).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>TaxiNLI (test)</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>natural language inference / logical reasoning (fine-grained categories)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Re-annotation of MNLI with fine-grained categories including logical constructs like connectives, mathematical reasoning, and deduction.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>accuracy (%)</td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td>average accuracy 97.00%, ceiling 100.00% (reported human baseline by authors)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>accuracy 57.30%</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>instruction-prompt with specifying labels; in-context example provided</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td>reported by authors (average and ceiling performance of human testees)</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>ChatGPT outperforms RoBERTa on TaxiNLI but is far below human performance; authors note NLI tasks present special instruction-following challenges for ChatGPT.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating the Logical Reasoning Ability of ChatGPT and GPT-4', 'publication_date_yy_mm': '2023-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7498.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e7498.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TaxiNLI - GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>TaxiNLI dataset evaluated with GPT-4 (API)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-4 evaluated on TaxiNLI; achieves modest gains over ChatGPT and RoBERTa but remains well below human baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4 (default-gpt-4, API)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Generative Pre-trained Transformer 4 by OpenAI, instruction-following multimodal model (API version March 14, 2023).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>TaxiNLI (test)</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>natural language inference / logical reasoning (fine-grained categories)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Re-annotation of MNLI with fine-grained categories including logical constructs like connectives, mathematical reasoning, and deduction.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>accuracy (%)</td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td>average accuracy 97.00%, ceiling 100.00% (reported human baseline by authors)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>accuracy 60.08%</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>instruction-prompt with specifying labels; OpenAI Eval framework</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td>reported by authors (average and ceiling performance of human testees)</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>GPT-4 slightly improves over ChatGPT on TaxiNLI but still far from human average; authors note NLI adds challenges for GPT-4 relative to multi-choice reading tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating the Logical Reasoning Ability of ChatGPT and GPT-4', 'publication_date_yy_mm': '2023-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7498.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e7498.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MNLI dev - ChatGPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MNLI development set evaluated with ChatGPT (gpt-3.5-turbo API)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>ChatGPT evaluated on MNLI dev (general NLI benchmark); performance reported alongside human baselines and other model baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGPT (gpt-3.5-turbo, API)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-tuned conversational model from OpenAI (gpt-3.5-turbo), accessed via API (version March 23, 2023).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>MNLI (dev)</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>natural language inference / general reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Multi-genre NLI benchmark; classify premise-hypothesis pairs as entailment, contradiction, or neutral across multiple text genres.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>accuracy (%)</td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td>average accuracy 98.00%, ceiling 100.00% (reported human baseline by authors)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>accuracy 55.40%</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>instruction-prompt with specifying labels; in-context example provided to aid label generation</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td>reported by authors (average and ceiling performance of human testees)</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>ChatGPT performs poorly relative to human and RoBERTa on MNLI dev (authors attribute this to instruction-following weaknesses for 3-label NLI tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating the Logical Reasoning Ability of ChatGPT and GPT-4', 'publication_date_yy_mm': '2023-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7498.11">
                <h3 class="extraction-instance">Extracted Data Instance 11 (e7498.11)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MNLI dev - GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MNLI development set evaluated with GPT-4 (API)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-4 evaluated on MNLI dev; reported accuracy improves over ChatGPT but remains well below human baselines and some fine-tuned models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4 (default-gpt-4, API)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Generative Pre-trained Transformer 4 by OpenAI, instruction-following multimodal model (API version March 14, 2023).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>MNLI (dev)</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>natural language inference / general reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Multi-genre NLI benchmark; classify premise-hypothesis pairs as entailment, contradiction, or neutral across multiple text genres.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>accuracy (%)</td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td>average accuracy 98.00%, ceiling 100.00% (reported human baseline by authors)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>accuracy 64.08%</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>instruction-prompt with specifying labels; OpenAI Eval framework; some manual Chat UI checks</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td>reported by authors (average and ceiling performance of human testees)</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>GPT-4's MNLI performance (64.08%) is notably below human average; authors highlight that logical reasoning remains challenging for GPT-4 in NLI settings and that GPT-4 sometimes fails to reliably output task labels.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating the Logical Reasoning Ability of ChatGPT and GPT-4', 'publication_date_yy_mm': '2023-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Large language models are zero-shot reasoners <em>(Rating: 2)</em></li>
                <li>A multitask, multilingual, multimodal evaluation of chatgpt on reasoning <em>(Rating: 2)</em></li>
                <li>Is chatgpt a general-purpose natural language processing task solver? <em>(Rating: 1)</em></li>
                <li>From lsat: The progress and challenges of complex reasoning <em>(Rating: 2)</em></li>
                <li>Conjnli: Natural language inference over conjunctive sentences <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7498",
    "paper_id": "paper-258041354",
    "extraction_schema_id": "extraction-schema-136",
    "extracted_data": [
        {
            "name_short": "ConTRoL - ChatGPT",
            "name_full": "ConTRoL dataset evaluated with ChatGPT (gpt-3.5-turbo API)",
            "brief_description": "ChatGPT (gpt-3.5-turbo, API) evaluated on ConTRoL, an NLI dataset focusing on contextual/logical reasoning; accuracy reported and compared to human baselines and RoBERTa.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "ChatGPT (gpt-3.5-turbo, API)",
            "model_description": "Instruction-tuned conversational model from OpenAI (gpt-3.5-turbo), accessed via API (version March 23, 2023).",
            "model_size": null,
            "test_name": "ConTRoL (test)",
            "test_category": "natural language inference / reasoning",
            "test_description": "Premise-hypothesis pairs testing contextual and logical reasoning under the NLI framework; labels: entailment, contradiction, neutral.",
            "evaluation_metric": "accuracy (%)",
            "human_performance": "average accuracy 87.00%, ceiling 94.00% (reported human baseline by authors)",
            "llm_performance": "accuracy 58.45%",
            "prompting_method": "instruction-prompt with specifying labels; single in-context example provided; OpenAI Eval framework",
            "fine_tuned": false,
            "human_data_source": "reported by authors (average and ceiling performance of human testees)",
            "statistical_significance": null,
            "notes": "ChatGPT outperforms RoBERTa on ConTRoL but remains substantially below human average; authors note ChatGPT can struggle to follow NLI instructions without in-context example.",
            "uuid": "e7498.0",
            "source_info": {
                "paper_title": "Evaluating the Logical Reasoning Ability of ChatGPT and GPT-4",
                "publication_date_yy_mm": "2023-04"
            }
        },
        {
            "name_short": "ConTRoL - GPT-4",
            "name_full": "ConTRoL dataset evaluated with GPT-4 (API)",
            "brief_description": "GPT-4 (default-gpt-4) evaluated on ConTRoL NLI benchmark; accuracy reported and compared to human baselines and other models.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4 (default-gpt-4, API)",
            "model_description": "Generative Pre-trained Transformer 4 by OpenAI, instruction-following multimodal model (API version March 14, 2023).",
            "model_size": null,
            "test_name": "ConTRoL (test)",
            "test_category": "natural language inference / reasoning",
            "test_description": "Premise-hypothesis pairs testing contextual and logical reasoning under the NLI framework; labels: entailment, contradiction, neutral.",
            "evaluation_metric": "accuracy (%)",
            "human_performance": "average accuracy 87.00%, ceiling 94.00% (reported human baseline by authors)",
            "llm_performance": "accuracy 56.40%",
            "prompting_method": "instruction-prompt with specifying labels; OpenAI Eval framework; authors also ran Chat UI checks",
            "fine_tuned": false,
            "human_data_source": "reported by authors (average and ceiling performance of human testees)",
            "statistical_significance": null,
            "notes": "GPT-4 performs slightly worse than ChatGPT on ConTRoL per reported numbers; authors remark GPT-4 sometimes fails to output labels steadily for NLI tasks.",
            "uuid": "e7498.1",
            "source_info": {
                "paper_title": "Evaluating the Logical Reasoning Ability of ChatGPT and GPT-4",
                "publication_date_yy_mm": "2023-04"
            }
        },
        {
            "name_short": "ConjNLI - ChatGPT",
            "name_full": "ConjNLI dataset evaluated with ChatGPT (gpt-3.5-turbo API)",
            "brief_description": "ChatGPT evaluated on ConjNLI, a stress-test NLI dataset over conjunctive sentences; model accuracy compared to human baseline and RoBERTa.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "ChatGPT (gpt-3.5-turbo, API)",
            "model_description": "Instruction-tuned conversational model from OpenAI (gpt-3.5-turbo), accessed via API (version March 23, 2023).",
            "model_size": null,
            "test_name": "ConjNLI (test)",
            "test_category": "natural language inference / logical reasoning (conjunctions)",
            "test_description": "Automatically generated premise-hypothesis pairs testing inference over conjunctive sentences (removal/addition/replacement of conjuncts).",
            "evaluation_metric": "accuracy (%)",
            "human_performance": "average accuracy 89.00%, ceiling 100.00% (reported human baseline by authors)",
            "llm_performance": "accuracy 47.03%",
            "prompting_method": "instruction-prompt with specifying labels; in-context example provided",
            "fine_tuned": false,
            "human_data_source": "reported by authors (average and ceiling performance of human testees)",
            "statistical_significance": null,
            "notes": "ChatGPT outperforms RoBERTa on ConjNLI but remains far below human performance; dataset focuses on conjunction-specific logical phenomena.",
            "uuid": "e7498.2",
            "source_info": {
                "paper_title": "Evaluating the Logical Reasoning Ability of ChatGPT and GPT-4",
                "publication_date_yy_mm": "2023-04"
            }
        },
        {
            "name_short": "ConjNLI - GPT-4",
            "name_full": "ConjNLI dataset evaluated with GPT-4 (API)",
            "brief_description": "GPT-4 evaluated on ConjNLI; achieves substantially higher accuracy than ChatGPT and RoBERTa per reported results.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4 (default-gpt-4, API)",
            "model_description": "Generative Pre-trained Transformer 4 by OpenAI, instruction-following multimodal model (API version March 14, 2023).",
            "model_size": null,
            "test_name": "ConjNLI (test)",
            "test_category": "natural language inference / logical reasoning (conjunctions)",
            "test_description": "Automatically generated premise-hypothesis pairs testing inference over conjunctive sentences (removal/addition/replacement of conjuncts).",
            "evaluation_metric": "accuracy (%)",
            "human_performance": "average accuracy 89.00%, ceiling 100.00% (reported human baseline by authors)",
            "llm_performance": "accuracy 72.71%",
            "prompting_method": "instruction-prompt with specifying labels; OpenAI Eval framework",
            "fine_tuned": false,
            "human_data_source": "reported by authors (average and ceiling performance of human testees)",
            "statistical_significance": null,
            "notes": "GPT-4 shows strong improvement on this conjunctive NLI stress test, narrowing the gap toward human baseline though still below reported human averages/ceilings.",
            "uuid": "e7498.3",
            "source_info": {
                "paper_title": "Evaluating the Logical Reasoning Ability of ChatGPT and GPT-4",
                "publication_date_yy_mm": "2023-04"
            }
        },
        {
            "name_short": "HELP - ChatGPT",
            "name_full": "HELP dataset evaluated with ChatGPT (gpt-3.5-turbo API)",
            "brief_description": "ChatGPT evaluated on HELP, a dataset focused on monotonicity reasoning (natural logic); model accuracy compared to human baseline and RoBERTa.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "ChatGPT (gpt-3.5-turbo, API)",
            "model_description": "Instruction-tuned conversational model from OpenAI (gpt-3.5-turbo), accessed via API (version March 23, 2023).",
            "model_size": null,
            "test_name": "HELP (train used for evaluation)",
            "test_category": "natural language inference / monotonicity reasoning",
            "test_description": "Dataset designed to probe monotonicity/entailment phenomena in natural logic; identifies shortcomings of neural models in monotonicity reasoning.",
            "evaluation_metric": "accuracy (%)",
            "human_performance": "average accuracy 81.00%, ceiling 95.00% (reported human baseline by authors)",
            "llm_performance": "accuracy 42.13%",
            "prompting_method": "instruction-prompt with specifying labels; in-context example provided",
            "fine_tuned": false,
            "human_data_source": "reported by authors (average and ceiling performance of human testees)",
            "statistical_significance": null,
            "notes": "ChatGPT modestly outperforms RoBERTa but is far below human baselines on monotonicity reasoning; HELP focuses specifically on monotonicity phenomena.",
            "uuid": "e7498.4",
            "source_info": {
                "paper_title": "Evaluating the Logical Reasoning Ability of ChatGPT and GPT-4",
                "publication_date_yy_mm": "2023-04"
            }
        },
        {
            "name_short": "HELP - GPT-4",
            "name_full": "HELP dataset evaluated with GPT-4 (API)",
            "brief_description": "GPT-4 evaluated on HELP monotonicity NLI dataset; reported to slightly outperform ChatGPT and RoBERTa but remain far below human baseline.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4 (default-gpt-4, API)",
            "model_description": "Generative Pre-trained Transformer 4 by OpenAI, instruction-following multimodal model (API version March 14, 2023).",
            "model_size": null,
            "test_name": "HELP (train used for evaluation)",
            "test_category": "natural language inference / monotonicity reasoning",
            "test_description": "Dataset designed to probe monotonicity/entailment phenomena in natural logic; identifies shortcomings of neural models in monotonicity reasoning.",
            "evaluation_metric": "accuracy (%)",
            "human_performance": "average accuracy 81.00%, ceiling 95.00% (reported human baseline by authors)",
            "llm_performance": "accuracy 46.01%",
            "prompting_method": "instruction-prompt with specifying labels; OpenAI Eval framework",
            "fine_tuned": false,
            "human_data_source": "reported by authors (average and ceiling performance of human testees)",
            "statistical_significance": null,
            "notes": "GPT-4 slightly outperforms ChatGPT/ RoBERTa on HELP but remains well below human performance; authors highlight dataset-specific challenges.",
            "uuid": "e7498.5",
            "source_info": {
                "paper_title": "Evaluating the Logical Reasoning Ability of ChatGPT and GPT-4",
                "publication_date_yy_mm": "2023-04"
            }
        },
        {
            "name_short": "MED - ChatGPT",
            "name_full": "MED dataset evaluated with ChatGPT (gpt-3.5-turbo API)",
            "brief_description": "ChatGPT evaluated on MED (monotonicity reasoning dataset); accuracy compared to RoBERTa and human baselines.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "ChatGPT (gpt-3.5-turbo, API)",
            "model_description": "Instruction-tuned conversational model from OpenAI (gpt-3.5-turbo), accessed via API (version March 23, 2023).",
            "model_size": null,
            "test_name": "MED (test)",
            "test_category": "natural language inference / monotonicity reasoning",
            "test_description": "Dataset focusing on monotonicity-related inference derived from monotonicity rules; tests basic logical phenomena in natural language.",
            "evaluation_metric": "accuracy (%)",
            "human_performance": "average accuracy 91.00%, ceiling 99.00% (reported human baseline by authors)",
            "llm_performance": "accuracy 55.02%",
            "prompting_method": "instruction-prompt with specifying labels; in-context example provided",
            "fine_tuned": false,
            "human_data_source": "reported by authors (average and ceiling performance of human testees)",
            "statistical_significance": null,
            "notes": "ChatGPT substantially outperforms RoBERTa on MED but remains well below human average; MED probes monotonicity-specific reasoning.",
            "uuid": "e7498.6",
            "source_info": {
                "paper_title": "Evaluating the Logical Reasoning Ability of ChatGPT and GPT-4",
                "publication_date_yy_mm": "2023-04"
            }
        },
        {
            "name_short": "MED - GPT-4",
            "name_full": "MED dataset evaluated with GPT-4 (API)",
            "brief_description": "GPT-4 evaluated on MED monotonicity NLI dataset; achieves accuracy close to human baseline per authors' reported numbers.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4 (default-gpt-4, API)",
            "model_description": "Generative Pre-trained Transformer 4 by OpenAI, instruction-following multimodal model (API version March 14, 2023).",
            "model_size": null,
            "test_name": "MED (test)",
            "test_category": "natural language inference / monotonicity reasoning",
            "test_description": "Dataset focusing on monotonicity-related inference derived from monotonicity rules; tests basic logical phenomena in natural language.",
            "evaluation_metric": "accuracy (%)",
            "human_performance": "average accuracy 91.00%, ceiling 99.00% (reported human baseline by authors)",
            "llm_performance": "accuracy 89.42%",
            "prompting_method": "instruction-prompt with specifying labels; OpenAI Eval framework",
            "fine_tuned": false,
            "human_data_source": "reported by authors (average and ceiling performance of human testees)",
            "statistical_significance": null,
            "notes": "GPT-4 attains 89.42% on MED, approaching the reported human average (91%), suggesting strong performance on monotonicity NLI tasks; authors note variance across NLI datasets.",
            "uuid": "e7498.7",
            "source_info": {
                "paper_title": "Evaluating the Logical Reasoning Ability of ChatGPT and GPT-4",
                "publication_date_yy_mm": "2023-04"
            }
        },
        {
            "name_short": "TaxiNLI - ChatGPT",
            "name_full": "TaxiNLI dataset evaluated with ChatGPT (gpt-3.5-turbo API)",
            "brief_description": "ChatGPT evaluated on TaxiNLI, an NLI subset with fine-grained logical categories; results compared with RoBERTa and human baselines.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "ChatGPT (gpt-3.5-turbo, API)",
            "model_description": "Instruction-tuned conversational model from OpenAI (gpt-3.5-turbo), accessed via API (version March 23, 2023).",
            "model_size": null,
            "test_name": "TaxiNLI (test)",
            "test_category": "natural language inference / logical reasoning (fine-grained categories)",
            "test_description": "Re-annotation of MNLI with fine-grained categories including logical constructs like connectives, mathematical reasoning, and deduction.",
            "evaluation_metric": "accuracy (%)",
            "human_performance": "average accuracy 97.00%, ceiling 100.00% (reported human baseline by authors)",
            "llm_performance": "accuracy 57.30%",
            "prompting_method": "instruction-prompt with specifying labels; in-context example provided",
            "fine_tuned": false,
            "human_data_source": "reported by authors (average and ceiling performance of human testees)",
            "statistical_significance": null,
            "notes": "ChatGPT outperforms RoBERTa on TaxiNLI but is far below human performance; authors note NLI tasks present special instruction-following challenges for ChatGPT.",
            "uuid": "e7498.8",
            "source_info": {
                "paper_title": "Evaluating the Logical Reasoning Ability of ChatGPT and GPT-4",
                "publication_date_yy_mm": "2023-04"
            }
        },
        {
            "name_short": "TaxiNLI - GPT-4",
            "name_full": "TaxiNLI dataset evaluated with GPT-4 (API)",
            "brief_description": "GPT-4 evaluated on TaxiNLI; achieves modest gains over ChatGPT and RoBERTa but remains well below human baselines.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4 (default-gpt-4, API)",
            "model_description": "Generative Pre-trained Transformer 4 by OpenAI, instruction-following multimodal model (API version March 14, 2023).",
            "model_size": null,
            "test_name": "TaxiNLI (test)",
            "test_category": "natural language inference / logical reasoning (fine-grained categories)",
            "test_description": "Re-annotation of MNLI with fine-grained categories including logical constructs like connectives, mathematical reasoning, and deduction.",
            "evaluation_metric": "accuracy (%)",
            "human_performance": "average accuracy 97.00%, ceiling 100.00% (reported human baseline by authors)",
            "llm_performance": "accuracy 60.08%",
            "prompting_method": "instruction-prompt with specifying labels; OpenAI Eval framework",
            "fine_tuned": false,
            "human_data_source": "reported by authors (average and ceiling performance of human testees)",
            "statistical_significance": null,
            "notes": "GPT-4 slightly improves over ChatGPT on TaxiNLI but still far from human average; authors note NLI adds challenges for GPT-4 relative to multi-choice reading tasks.",
            "uuid": "e7498.9",
            "source_info": {
                "paper_title": "Evaluating the Logical Reasoning Ability of ChatGPT and GPT-4",
                "publication_date_yy_mm": "2023-04"
            }
        },
        {
            "name_short": "MNLI dev - ChatGPT",
            "name_full": "MNLI development set evaluated with ChatGPT (gpt-3.5-turbo API)",
            "brief_description": "ChatGPT evaluated on MNLI dev (general NLI benchmark); performance reported alongside human baselines and other model baselines.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "ChatGPT (gpt-3.5-turbo, API)",
            "model_description": "Instruction-tuned conversational model from OpenAI (gpt-3.5-turbo), accessed via API (version March 23, 2023).",
            "model_size": null,
            "test_name": "MNLI (dev)",
            "test_category": "natural language inference / general reasoning",
            "test_description": "Multi-genre NLI benchmark; classify premise-hypothesis pairs as entailment, contradiction, or neutral across multiple text genres.",
            "evaluation_metric": "accuracy (%)",
            "human_performance": "average accuracy 98.00%, ceiling 100.00% (reported human baseline by authors)",
            "llm_performance": "accuracy 55.40%",
            "prompting_method": "instruction-prompt with specifying labels; in-context example provided to aid label generation",
            "fine_tuned": false,
            "human_data_source": "reported by authors (average and ceiling performance of human testees)",
            "statistical_significance": null,
            "notes": "ChatGPT performs poorly relative to human and RoBERTa on MNLI dev (authors attribute this to instruction-following weaknesses for 3-label NLI tasks).",
            "uuid": "e7498.10",
            "source_info": {
                "paper_title": "Evaluating the Logical Reasoning Ability of ChatGPT and GPT-4",
                "publication_date_yy_mm": "2023-04"
            }
        },
        {
            "name_short": "MNLI dev - GPT-4",
            "name_full": "MNLI development set evaluated with GPT-4 (API)",
            "brief_description": "GPT-4 evaluated on MNLI dev; reported accuracy improves over ChatGPT but remains well below human baselines and some fine-tuned models.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4 (default-gpt-4, API)",
            "model_description": "Generative Pre-trained Transformer 4 by OpenAI, instruction-following multimodal model (API version March 14, 2023).",
            "model_size": null,
            "test_name": "MNLI (dev)",
            "test_category": "natural language inference / general reasoning",
            "test_description": "Multi-genre NLI benchmark; classify premise-hypothesis pairs as entailment, contradiction, or neutral across multiple text genres.",
            "evaluation_metric": "accuracy (%)",
            "human_performance": "average accuracy 98.00%, ceiling 100.00% (reported human baseline by authors)",
            "llm_performance": "accuracy 64.08%",
            "prompting_method": "instruction-prompt with specifying labels; OpenAI Eval framework; some manual Chat UI checks",
            "fine_tuned": false,
            "human_data_source": "reported by authors (average and ceiling performance of human testees)",
            "statistical_significance": null,
            "notes": "GPT-4's MNLI performance (64.08%) is notably below human average; authors highlight that logical reasoning remains challenging for GPT-4 in NLI settings and that GPT-4 sometimes fails to reliably output task labels.",
            "uuid": "e7498.11",
            "source_info": {
                "paper_title": "Evaluating the Logical Reasoning Ability of ChatGPT and GPT-4",
                "publication_date_yy_mm": "2023-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Large language models are zero-shot reasoners",
            "rating": 2
        },
        {
            "paper_title": "A multitask, multilingual, multimodal evaluation of chatgpt on reasoning",
            "rating": 2
        },
        {
            "paper_title": "Is chatgpt a general-purpose natural language processing task solver?",
            "rating": 1
        },
        {
            "paper_title": "From lsat: The progress and challenges of complex reasoning",
            "rating": 2
        },
        {
            "paper_title": "Conjnli: Natural language inference over conjunctive sentences",
            "rating": 1
        }
    ],
    "cost": 0.0132968,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Evaluating the Logical Reasoning Ability of ChatGPT and GPT-4</p>
<p>Hanmeng Liu liuhanmeng@westlake.edu.cn 
Westlake University
Zhejiang University
Nanyang Technological University
Fudan University
Westlake University</p>
<p>Ruoxi Ning ruoxining@zju.edu.cn 
Westlake University
Zhejiang University
Nanyang Technological University
Fudan University
Westlake University</p>
<p>Zhiyang Teng zhiyang.teng@ntu.edu.sg 
Westlake University
Zhejiang University
Nanyang Technological University
Fudan University
Westlake University</p>
<p>Jian Liu jianliu17@fudan.edu.cn 
Westlake University
Zhejiang University
Nanyang Technological University
Fudan University
Westlake University</p>
<p>Qiji Zhou zhouqiji@westlake.edu.cn 
Westlake University
Zhejiang University
Nanyang Technological University
Fudan University
Westlake University</p>
<p>Yue Zhang zhangyue@westlake.edu.cn 
Westlake University
Zhejiang University
Nanyang Technological University
Fudan University
Westlake University</p>
<p>Evaluating the Logical Reasoning Ability of ChatGPT and GPT-4</p>
<p>Harnessing logical reasoning ability is a comprehensive natural language understanding endeavor. With the release of Generative Pretrained Transformer 4 (GPT-4), highlighted as "advanced" at reasoning tasks, we are eager to learn the GPT-4 performance on various logical reasoning tasks. This report analyses multiple logical reasoning datasets, with popular benchmarks like LogiQA and ReClor, and newly-released datasets like AR-LSAT. We test the multi-choice reading comprehension and natural language inference tasks with benchmarks requiring logical reasoning. We further construct a logical reasoning out-ofdistribution dataset to investigate the robustness of ChatGPT and GPT-4. We also make a performance comparison between ChatGPT and GPT-4. Experiment results show that ChatGPT performs significantly better than the RoBERTa fine-tuning method on most logical reasoning benchmarks. With early access to the GPT-4 API we are able to conduct intense experiments on the GPT-4 model. The results show GPT-4 yields even higher performance on most logical reasoning datasets. Among benchmarks, ChatGPT and GPT-4 do relatively well on well-known datasets like LogiQA and ReClor. However, the performance drops significantly when handling newly released and out-of-distribution datasets. Logical reasoning remains challenging for ChatGPT and GPT-4, especially on outof-distribution and natural language inference datasets. We release the prompt-style logical reasoning datasets as a benchmark suite and name it LogiEval.</p>
<p>Introduction</p>
<p>Logical reasoning is essential to human intelligence, and incorporating logical reasoning abilities into natural language understanding (NLU) systems has been an active research interest from * Yue Zhang is the corresponding author the beginning of artificial intelligence (Cresswell, 1973) (Kowalski, 1979) (Iwaska, 1993). Researchers have been exploring various approaches to achieve this goal, including rule-based methods, symbolic systems (MacCartney and Manning, 2007a), fine-tuning large language models (Wang et al., 2018), and combining both neural and symbolic approaches (Li and Srikumar, 2019).</p>
<p>In the traditional logical and semantic approach, computational linguists developed symbolic systems utilizing First-Order-Logic (FOL) or Natural Logic (MacCartney and Manning, 2007a) to tackle fundamental inference tasks. Rule-based models struggle to unravel problems like the RTE challenge (Dagan et al., 2005) with hand-crafted rules and theorem provers. Formal logic reasoning adopted by early researchers came up with symbolic systems and hand-crafted rules, where knowledge was represented explicitly using formal logic or other symbolic representations. With rules, the systems can process deduction operations. However, these approaches face challenges in handling ambiguity and scalability. They are brittle when dealing with real-world natural language data.</p>
<p>The era of neural network models sees the rise of large-scale NLI datasets as popular benchmarks. For example, the SNLI (Bowman et al., 2015) and the Multi-genre NLI (MNLI) (Williams et al., 2018a) datasets are created through crowdsourcing, featuring an immense data size and broad coverage. They catalyze the development of models with better representation abilities and become the go-to benchmark for natural language understanding research. The giant leap in model performance comes with the advent of Transformer-based (Vaswani et al., 2017) language models like BERT (Devlin et al., 2018) when the training schemes of such models enable them to access colossal unlabelled corpora. As a result, building language models with trillions of parameters become possible (Brown et al., 2020) (Raffel et al., 2019).  The paradigm of pre-training and fine-tuning has since become the dominant solution to textual inference tasks. Researchers fine-tune language models on task-specific datasets after pre-training models on massive text corpora. Large pre-trained language models (LMs) achieve beyond-human performances on popular NLI and MRC benchmarks, urging for more sophisticated benchmarks in textual inference.</p>
<p>NLP research on logical reasoning regains momentum with the recent releases of a handful of datasets, notably LogiQA and Reclor. The datasets are collected from logical reasoning exams such as Chinese Civil Servant Exams and Law School Admission Test (LSAT). These tests are challenging even for humans and are golden-labeled data with good quality. Logical reasoning is exploited in many probing tasks over large Pre-trained Language Models (PLMs) and downstream tasks like question-answering and dialogue systems. PLMs perform poorly compared to traditional benchmarks. Despite the progress made so far, achieving human-like logical reasoning capabilities in NLU systems remains a challenging task.</p>
<p>Generative Pre-trained Transformer 4 (GPT-4) (OpenAI, 2023), as well as ChatGPT, is a newlyreleased language model developed by OpenAI, designed to understand and generate multi-modal contents. GPT-4 is promoted to possess even more powerful capabilities in tasks that require logical reasoning. Logical reasoning is essential to human intelligence, enabling us to draw conclusions, make predictions, and solve problems based on given information. Incorporating logical reasoning into language models like GPT-4 can revolutionize natural language understanding (NLU) systems, making them more accurate, robust, and capable of understanding complex information in natural language.</p>
<p>The evaluation of ChatGPT and GPT-4 for logical reasoning tasks explores their performance on several logical reasoning benchmarks, detailing the strengths and limitations of ChatGPT and GPT-4 in this domain. Previous ChatGPT evaluation indicates that ChatGPT performs better on deductive reasoning than inductive ones (Bang et al., 2023) and that ChatGPT performs better on factual inputs (Qin et al., 2023). After the release of GPT-4, we will discuss two tasks for evaluating its logical reasoning abilities: multi-choice reading comprehension and natural language inference. Both tasks are reasoning-heavy and serve as a playground for testing models' reasoning abilities. Several logical reasoning datasets have been released under the two tasks. These benchmarks proved to be difficult to solve for PLMs. We hope this report will shed more light on the logical reasoning ability of ChatGPT and GPT-4. Our contributions are as follows:</p>
<ol>
<li>
<p>We test ChatGPT and GPT-4 on two logical reasoning tasks: multi-choice reading comprehension and natural language inference. We conduct experiments on multiple logical reasoning benchmarks to analyze the logical reasoning ability of ChatGPT and GPT-4.</p>
</li>
<li>
<p>We introduce LogiEval, a test suite for testing prompt-based large language models. We release the benchmark at https://github.com/  </p>
</li>
</ol>
<p>csitfun/LogiEval</p>
<ol>
<li>Our experiments show that both ChatGPT and GPT-4 are good at solving well-known logical reasoning reading comprehension benchmarks but struggle at handling out-of-distribution datasets. Their performances on natural language inference tasks requiring logical reasoning still need improving.</li>
</ol>
<p>Evaluation Settings</p>
<p>We consider multi-choice reading comprehension and natural language inference tasks for our evaluation. Multi-choice reading comprehension is heavily tested on large language models for these tasks often have clearly-formed and high-quality datasets. On the other hand, the natural language inference task is a fundamental task for evaluating reasoning abilities. The datasets and the evaluation settings are provided as follows.</p>
<p>Datasets</p>
<p>Multi-choice Reading Comprehension</p>
<p>Machine reading is a popular task in NLP. In the typical multiple-choice task setting, given a passage and a question, a system is asked to select the most appropriate answer from a list of candidate answers.</p>
<p>LogiQA is a dataset specifically designed for multi-choice question-answering tasks that involve logical reasoning. The dataset is adopted from the Chinese Civil Service Examination, translated into English, and released in Chinese and English. The dataset has been updated to the 2.0 version 1 , where the data size has been enlarged. We choose the test sets of both the Chinese and English versions for our evaluation. Figure 3 shows an example from the LogiQA 2.0 test set.</p>
<p>ReClor is another logical reasoning dataset designed for reading comprehension tasks requiring logical reasoning. It collects question-answering examples from the LSAT exams 2 , which are targeted to testing human logical reasoning abilities. We use the development set for our testing because the test set does not include gold labels.</p>
<p>Natural Language Inference</p>
<p>Natural language inference is the task of deciding the logical relationship between a hypothesis and a premise. The typical scheme is a text classification where the model needs to choose one from three labels: entailment, contradiction, and neutral.</p>
<p>ConTRoL (Liu et al., 2020) is an NLI dataset that further investigates contextual reasoning under the NLI framework. It has 36.2% of premisehypothesis pairs under the category of logical reasoning. Figure 4 shows an example from the Con-TRoL dataset.</p>
<p>MED (Yanaka et al., 2019b) and HELP (Yanaka et al., 2019a) are two NLI datasets focusing on monotonicity reasoning, which is an essential concept in Natural Logic (MacCartney and Manning, 2007b). The datasets are generated through monotonicity rules and only investigate monotonicityrelated inference specifically. Similar to the NLI section of our dataset, MED and HELP investigate basic logic phenomena in natural language, which is monotonicity in particular. For the HELP dataset, we use the train set for our evaluation. Here is an example from the HELP dataset:  Table 1: ChatGPT and GPT-4 performance on the Logical multi-choice machine reading comprehension task (accuracy %). "LogiQA 2.0 zh test" refers to the test set of the LogiQA 2.0 Chinese version. "LogiQA 2.0 ood" represents the out-of-distribution data of LogiQA 2.0.</p>
<p>Premise: Tom said that neither parents had ever been to Boston.</p>
<p>Hypothesis: Tom said that neither one of his parents had ever been to Boston.</p>
<p>Label: Entailment</p>
<p>ConjNLI (Saha et al., 2020) is a challenging stress test for NLI over conjunctive sentences, where the premise differs from the hypothesis by having conjuncts being removed, added, or replaced. Logical reasoning about conjunctions is heavily tested in ConjNLI. Premise-hypothesis pairs are created automatically by applying conjunct operations on collected conjunctive sentences.</p>
<p>Here is an example from the ConjNLI dataset:</p>
<p>Premise: In Quebec, an allophone is a resident, usually an immigrant, whose mother tongue or home language is neither French nor English.</p>
<p>Hypothesis: In Quebec, an allophone is a resident, usually an immigrant, whose mother tongue or home language is not French.</p>
<p>Label: Entailment</p>
<p>TaxiNLI (Joshi et al., 2020) is an NLI dataset re-annotated on the MNLI (Williams et al., 2018b) dataset with fine-grained category labels. The annotation includes logical categories like connectives, mathematical, and deduction. Notice that TaxiNLI is a subset of the MNLI dataset, so we include the MNLI dataset for our comparison as a traditional NLI benchmark. Here is an example from the TaxiNLI dataset:</p>
<p>Premise: and that you're very much right but the jury may or may not see it that way so you get a little anticipate you know anxious there and go well you know.</p>
<p>Hypothesis: Even if you're correct, I think the jury would pick up on that.</p>
<p>Label: Contradiction </p>
<p>Out-of-distribution Data</p>
<p>AR-LSAT (Wang et al., 2022) is a new dataset of analytical reasoning questions from the Law School Admission Test. Released in 2022, it has 2064 questions, each describing a reasoning game belonging to three dominant types: (1) ordering game, (2) grouping game, and (3) assignment game. It is noticed that each question has five options rather than four. Figure 5 shows an example from the AR-LSAT test set.</p>
<p>Besides, we construct a LogiQA 2.0 outof-distribution dataset, which incorporates the newly released Chinese Civil Servant Exam from 2022. The test set is a collection of logical reasoning tests designed by experts from 2022 onwards. We release our data at https://github. com/csitfun/LogiQA2.0.  Table 2: ChatGPT and GPT-4 performance on the natural language inference task (accuracy %).</p>
<p>Experiment Setting</p>
<p>We take RoBERTa-base ) as our baseline model. Following a fine-tuning scheme, we use Huggingface's RoBERTa-base model as our pre-trained language model. RoBERTa-base is trained on the training set for 5 epochs for each dataset. We also set up a human baseline by reporting the average and ceiling performance of human testees.</p>
<p>For ChatGPT and GPT-4, we follow an instruction-prompt scheme for both Natural Language Inference and multi-choice reading comprehension tasks. Figure 1 shows the instruction for multi-choice reading comprehension tasks.</p>
<p>After investigating the methods of promptdesigning for reasoning tasks, we find that there are mainly three types of prompt designing for NLI tasks, specifying the labels (entailment, neutral or contradiction) (Qin et al., 2023), specifying the reasoning method (induction or deduction) (Bang et al., 2023), and chain-of-thought reasoning (Kojima et al., 2023) which will be specified in the next chapter. Among these, specifying the labels method suits our purpose for most of our NLI datasets are 3-label classification tasks. Thus, we prompt GPT with the 3 three possible relationships between the hypothesis and conclusion, entailment, contradiction and neutral, every time we ask a question to GPT. The instruction we use for the multi-choice reading comprehension task is in Appendix A. Figure 2 shows the instruction for multi-choice reading comprehension tasks. The instruction we use for the natural language inference task is in Appendix B.</p>
<p>For evaluating ChatGPT, We use the Eval 3 framework provided by OpenAI, a suite for evaluating OpenAI models and an open-source registry of benchmarks. The model we choose is "gpt-3.5turbo" (version March 23, 2023). Apart from task 3 https://github.com/openai/evals structure, we offer an in-context example to each API call to guarantee controlled output.</p>
<p>GPT-4 has been limited access to subscribe users from March 14, 2023. We are granted early access to GPT-4 API by merging requests to the OpenAI Eval repository. So we are able to use the GPT-4 API and the OpenAI Eval framework. The model we use is "default-gpt-4" (version March 14, 2023). We also utilize the GPT-4 Chat UI to conduct our GPT-4 experiments and further analyses with two OpenAI Plus accounts.</p>
<p>Results</p>
<p>Experiment results on the multi-choice</p>
<p>reading comprehension tasks Table 1 shows the results of the multi-choice reading comprehension datasets.</p>
<p>The performance of ChatGPT</p>
<p>ChatGPT shows a performance increase compared to the baseline model on several long-standing benchmarks. The accuracy of the LogiQA 2.0 test set is 53.37%, nearly 4 points higher than the RoBERTa base model. The performance gap between ChatGPT and RoBERTa is salient when testing on the Chinese version of LogiQA 2.0, which indicates the performance consistency of ChatGPT in both Chinese and English languages. Chat-GPT yields the best performance on the ReClor dataset with an accuracy of 57.38%, compared with RoBERTa's 55.01% accuracy. However, ChatGPT experiences a huge performance drop on out-ofdistribution datasets. On the AR-LSAT test set, the accuracy is only 20.42%, lower than the performance of RoBERTa base. On LogiQA 2.0 ood, the performance is 38.44%, still lower than that of RoBERTa base. From the experiments above, Chat-GPT performs well on well-known Logical reasoning like LogiQA and ReClor. The accuracy of ChatGPT surpasses fine-tuning methods by a small margin. However, when tested on the newly released dataset, namely AR-LSAT, and on LogiQA 2.0 out-of-distribution dataset, the performance declined significantly. Despite its limitations, ChatGPT still represents a significant advancement in natural language understanding and demonstrates the potential of language models to reason logically.</p>
<p>The performance of GPT-4</p>
<p>GPT-4 performs remarkably better than ChatGPT when doing manual tests on LogiQA and ReClor. On the LogiQA 2.0 test set (1572 instances), GPT-4 yields an accuracy of 72.25%. On the Chinese version of the LogiQA 2.0 test set (1594 instances), the accuracy is 70.56, which is similar to the performance on the English version. On the ReClor dev set (500 instances, ReClor does not include gold labels on its test), GPT-4 reaches an 87.20% accuracy which is the highest score among all three models. However, when tested on the AR-LSAT test set (230 intances), GPT-4 performs surprisingly worse with only a 33.48% accuracy. The test result on LogiQA 2.0 ood data (1354 instances) shows that GPT-4 gets 58.49% correctness, which is significantly lower than that on the LogiQA 2.0 test set. Nevertheless, the performance is still the highest among all three models. We will not haste to the conclusion, but it's safe to say that GPT-4's performance drop on out-of-distribution datasets is noticeable. For comparison, the GPT-4 Chat UI results are also provided, where we manually test a handful of data instances. Table 2 shows the results on the natural language inference datasets.</p>
<p>Experiment results on the natural language inference task</p>
<p>The performance of ChatGPT</p>
<p>ChatGPT performs better than the RoBERTa model on the logical reasoning NLI datasets we test. On the ConTRoL test set, the accuracy is 58.45%, higher than the RoBERTa-base model by nearly 10 percent. On the ConjNLI test set, ChatGPT yields 47.03% accuracy, which outperforms RoBERTa by around 9 percent. On the HELP dataset, Chat-GPT gets a 42.31% accuracy, around 3 points higher than that of RoBERTa. On the MED dataset, ChatGPT gives 55.02% accuracy, nearly 9 percent higher than that of RoBERTa. On the TaxiNLI test set, ChatGPT gives 57.30% accuracy, over 7 percent higher than that of RoBERTa. For comparison, ChatGPT gives 55.40% accuracy on the MNLI dev set, which is significantly lower than that of RoBERTa, which indicates that ChatGPT is not optimized for answering three-labeled natural language inference questions. Since it is noticed that ChatGPT is not good at following NLI task instructions, we provide an in-context example to help the model generate task labels, as shown in Figure 2. Overall, the results show that ChatGPT surpasses fine-tuned RoBERTa by only a small margin for logical reasoning NLI datasets.</p>
<p>The performance of GPT-4</p>
<p>We test GPT-4's performance on logical reasoning NLI datasets. On the ConTRoL test set (805 instances), GPT-4 performs slightly lower than ChatGPT, yielding a 56.40% accuracy. The performance of GPT-4 on the ConjNLI test (623 instances) and the MED test (5382 instances) is significantly better, with an accuracy of 72.71% and 89.42%, respectively. However, on the HELP (35891 instances) and TaxiNLI test (10071 instances), the performance of GPT-4 is slightly better than that of RoBERTa and ChatGPT, with an accuracy of 46.01% and 60.08%, respectively. The GPT-4 performance on the MNLI dev set is slightly better than on the TaxiNLI test, and yields 64.08% accuracy, which indicates logical reasoning adds more challenges to the GPT-4 model. We also include the testing results with GPT-4 Chat UI and around 100 data instances for each NLI dataset. The results on the six NLI datasets indicate that GPT-4 does not perform highly on logical reasoning natural language inference compared to multichoice reading comprehension. We also notice that GPT-4 cannot output labels steadily even though the instruction is provided in the natural language inference task scenario. From this, we infer that GPT-4 is not good at following the instruction for the natural language inference task, though it is well-trained to follow the instruction for the multichoice reading comprehension task.</p>
<p>Analysis</p>
<p>The experiment results show that ChatGPT and GPT-4 surpass RoBERTa on most logical reasoning benchmarks, including popular benchmarks like LogiQA and ReClor and less-known datasets like AR-LSAT. However, the performance drop on out-of-distribution datasets is noticeable for both GPT models, indicating they struggle to handle new and unfamiliar data. Thus, we conduct more  case studies with the GPT-4 chat UI and further analyze its abilities.</p>
<p>Answer and Reason</p>
<p>For GPT-4 manual tests, we record the answers GPT-4 gives and the reasoning for the answer. Figure 6 gives an example of GPT-4's answer and reasoning. In this example, GPT-4 did it correctly. From the paragraph's inconsistency between the similar drunk driving rate both with and without drunk driving checks, and the claim that the strict checks lower the drunk driving rate, GPT-4 chooses a fact prior to the contemporary situation that the  drunk driving rate used to be high before strict checks, to solve this contradiction. Figure 7 shows an example where GPT-4 generates the wrong answer and reasoning. The passage talks about whether humans originated from Africa. The conclusion would be challenged if we find older fossils in other parts of the world. GPT-4 does not have a thorough understanding of the problem. It only focuses on establishing the chronological order of events. The failure to attend to the location of the fossils leads to the wrong answer.</p>
<p>In our assessment of GPT-4 on the logiQA dataset, we inspected the first 10 errors made by the model. Four were categorized as logical errors, such as affirming the question, negating the hypothesis, and begging the question. Three were identified as scope errors, including attributing predicates to incorrect subjects or assigning characters to the wrong objects. The remaining three errors fall in the inability to resolve semantic ambiguity, wherein GPT-4 selected a merely qualified response when asked for an optimal one. However, this limited sample of errors does not entail the conclusion that GPT-4 is incompetent to handle reasoning questions, as there are also instances where it accurately identifies the correct answer. GPT-4's occasional selection of wrong answers suggests that further examination is necessary to determine whether some features in the questions may trigger such incorrect choices.</p>
<p>In-Context Learning</p>
<p>In this section, we test the in-context learning ability of GPT-4. We observe that GPT-4 is prone to output more correct answers within the same conversation window after approximately eight conversation rounds; GPT-4's accuracy increases after seeing more examples. During this procedure, no feedback is provided to the conversation.</p>
<p>To exemplify this phenomenon, we conduct an experiment on the LogiQA 2.0 ood data and the ConTRoL dataset, each representing a typical test case for the task of multi-choice reading comprehension and natural language inference. We ran-domly select 20 instances from each dataset for the following testing. Remember that GPT-4's performance is not competitive on these two datasets.</p>
<p>We first test the 20 instances from the same conversation window; then, we test each instance of 20 in a new conversation window. The testing results are shown in Table 3 For the LogiQA 2.0 ood dataset, GPT-4 yields 9 correct answers when the 20 examples are in the same conversation window. However, without the context, the number of correct answers drops to 5. For the ConTRoL dataset, we find that GPT-4 answers 13 questions correctly with the context, and it drops to 7 without the context. Appendix C shows an example where GPT-4 answers the question correctly inside the context while does not give the correct answer in a new conversation window.</p>
<p>Chain-of-Thought Prompting</p>
<p>Chain-of-Thought (CoT) prompting is explored by many researchers and shows promising results on complex multi-step reasoning tasks (Kojima et al., 2023). This section explores zero-shot CoT prompting for GPT-4 on logical reasoning datasets. The experiment is conducted on the LogiQA 2.0 ood data. We choose the same 112 instances as we do manual tests with GPT-4, which is shown in Table 1. For this round, we add the prompt "Let's think step by step" to the instruction. By adding this prompt, GPT-4 generates longer reasoning texts exemplifying the reasoning steps. We collect the final answer for each CoT reasoning process and get 61 correct answers out of 112 questions, which is higher than the previous experiment without CoT prompting.</p>
<p>Overall, the evaluation of the logical reasoning ability of ChatGPT and GPT-4 highlights the importance of developing more sophisticated benchmarks in textual inference to improve NLU systems' logical reasoning abilities further. The results also suggest that there is still room for improvement in language models' logical reasoning abilities, particularly when handling out-of-distribution datasets. Researchers need to continue developing more sophisticated benchmarks in textual inference to improve NLU systems' logical reasoning abilities further. Exploring new approaches to training language models that can better handle out-of-distribution datasets and other challenges associated with real-world applications is important.</p>
<p>Conclusion</p>
<p>We evaluate the logical reasoning ability of Chat-GPT and GPT-4 on multiple logical reasoning datasets, including multi-choice reading comprehension and natural language inference tasks. We test on popular benchmarks, as well as less-known NLI datasets and newly-released ones. The experiment results show that ChatGPT and GPT-4 outperform the traditional fine-tuning method on most logical reasoning benchmarks, indicating that these two models can better reason logically. However, both models face challenges in handling new and out-of-distribution data. Besides, ChatGPT and GPT-4 do not perform astonishingly well on the natural language inference task requiring logical reasoning.</p>
<p>A Instruction and Prompt for the multi-choice reading comprehension task</p>
<p>Instructions: You will be presented with a passage and a question about that passage. There are four options to be chosen from, you need to choose the only correct option to answer that question. If the first option is right, you generate the answer 'A', if the second option is right, you generate the answer 'B', if the third option is right, you generate the answer 'C', if the fourth option is right, you generate the answer 'D'. Read the question and options thoroughly and select the correct answer from the four answer labels. Read the passage thoroughly to ensure you know what the passage entails.</p>
<p>B Instruction and Prompt for the natural language inference task</p>
<p>Instructions: You will be presented with a premise and a hypothesis about that premise.</p>
<p>You need to decide whether the hypothesis is entailed by the premise by choosing one of the following answers: 'e': The hypothesis follows logically from the information contained in the premise. 'c': The hypothesis is logically false from the information contained in the premise.</p>
<p>'n': It is not possible to determine whether the hypothesis is true or false without further information. Read the passage of information thoroughly and select the correct answer from the three answer labels. Read the premise thoroughly to ensure you know what the premise entails.</p>
<p>C GPT-4 Answers with/without Context</p>
<p>Example:</p>
<p>The gold label is "B" Question: Crimes involving the use of guns are more likely to lead to homicides than other types of crimes, but most crimes involving the use of guns do not lead to homicides. Therefore, it is not necessary to distinguish the illegal use of guns as a serious criminal offense from other criminal offenses in criminal law.</p>
<p>According to the logical loopholes in the above argument, which of the following options is the most similar?</p>
<p>A. Obese people are more likely to suffer from heart disease than normal-weight people. However, obesity accounts for only a small proportion of China's population. Therefore, in China, there is no need for medical and health circles to emphasize the risk of heart disease B. Promiscuous sex is more likely to be infected with AIDS than regular sex. However, only a small proportion of people who have promiscuous sex are infected with AIDS. Therefore, there is no need to emphasize the harm of promiscuous sex in the publicity of AIDS prevention C. The popular view is that smoking is more likely to cause lung cancer than non-smoking. However, in some countries, the proportion of lung cancer patients with a smoking history is not higher than that of the total population. Therefore, the above popular view is likely to be a prejudice D. High-income people are more able to enjoy life than low-income people. But many highincome people claim to be unhappy. Therefore, the pursuers of a happy life do not have to pay attention to the level of income Answer and reason: GPT-4 output:</p>
<p>With context: A. Obese people are more likely to suffer from heart disease than normal weight people. However, obesity accounts for only a small proportion of China's population. Therefore, in China, there is no need for medical and health circles to emphasize the risk of heart disease.</p>
<p>Reason: Both the original argument and option A have the same logical loophole. In the original argument, it is stated that crimes involving guns are more likely to lead to homicides but most don't, concluding that there is no need to distinguish the illegal use of guns as a serious criminal offense. Similarly, in option A, it is stated that obese people are more likely to suffer from heart disease but obesity accounts for only a small proportion of the population, concluding that there is no need to emphasize the risk of heart disease in China. Both arguments downplay the importance of addressing a specific risk or issue based on the fact that it doesn't affect a majority or large proportion of the population, even though the consequences are more severe for the affected individuals.</p>
<p>Without context: B. Promiscuous sex is more likely to be infected with AIDS than regular sex. However, only a small proportion of people who have promiscuous sex are infected with AIDS. Therefore, there is no need to emphasize the harm of promiscuous sex in the publicity of AIDS prevention Reason: Both the original argument and option B exhibit a similar logical flaw. The original argument acknowledges that gun-related crimes are more likely to lead to homicides, yet concludes that there is no need to treat such crimes as distinctively serious because most gun-related crimes do not lead to homicides. Similarly, option B acknowledges that promiscuous sex is more likely to result in AIDS infection, but concludes that there is no need to emphasize the harm of promiscuous sex in AIDS prevention efforts because only a small proportion of people engaging in promiscu-ous sex contract AIDS. Both arguments downplay the increased risk associated with specific behaviors or situations by focusing on the fact that not all instances of those behaviors or situations lead to negative outcomes.</p>
<p>Figure 1 :
1Instruction and question format for logical reading comprehension tasks.</p>
<p>Figure 2 :
2Instruction and question format for natural language inference tasks.</p>
<p>Figure 3 :
3A multi-choice reading comprehension example from the LogiQA 2.0 dataset.</p>
<p>Figure 4 :
4An NLI example from the ConTRoL dataset.</p>
<p>Figure 5 :
5An example from the AR-LSAT dataset.</p>
<p>Figure 6 :
6An GPT-4 generation example where the model answers the question correctly.</p>
<p>Figure 7 :
7An GPT-4 generation example where the model answers the question incorrectly.</p>
<p>Chat UI) 58.18(64/110) 61.00 (61/100) 53.33 (56/105) 75.79 (72/95) 75.47(80/106) 68.00 (68/100)Dataset </p>
<p>ConTRoL test ConjNLI test 
HELP 
MED 
TaxiNLI test 
MNLI dev 
Size 
805 
623 
35891 
5382 
10071 
9815 
Human avg. 
87.00 
89.00 
81.00 
91.00 
97.00 
98.00 
Human ceiling 
94.00 
100.00 
95.00 
99.00 
100.00 
100.00 
RoBERTa 
48.76 
38.94 
39.47 
46.83 
49.91 
90.02 
ChatGPT (API) 
58.45 
47.03 
42.13 
55.02 
57.30 
55.40 
GPT-4 (GPT-4 (API) 
56.40 
72.71 
46.01 
89.42 
60.08 
64.08 </p>
<p>Table 3 :
3GPT-4 performance with/without context.
https://github.com/csitfun/LogiQA2.0 2 https://www.lsac.org/lsat</p>
<p>A multitask, multilingual, multimodal evaluation of chatgpt on reasoning. Yejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wenliang Dai, Dan Su, Bryan Wilie, Holy Lovenia, Ziwei Ji, Tiezheng Yu, Willy Chung, Quyet V Do, Yan Xu, Pascale Fung, and interactivityYejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wen- liang Dai, Dan Su, Bryan Wilie, Holy Lovenia, Zi- wei Ji, Tiezheng Yu, Willy Chung, Quyet V. Do, Yan Xu, and Pascale Fung. 2023. A multitask, multilin- gual, multimodal evaluation of chatgpt on reasoning, hallucination, and interactivity.</p>
<p>A large annotated corpus for learning natural language inference. R Samuel, Gabor Bowman, Christopher Angeli, Christopher D Potts, Manning, 10.18653/v1/D15-1075Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. the 2015 Conference on Empirical Methods in Natural Language ProcessingLisbon, PortugalAssociation for Computational LinguisticsSamuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning. 2015. A large anno- tated corpus for learning natural language inference. In Proceedings of the 2015 Conference on Empiri- cal Methods in Natural Language Processing, pages 632-642, Lisbon, Portugal. Association for Compu- tational Linguistics.</p>
<p>Logical reasoning in natural language: It is all about knowledge. Minds and Machines. Lucja Iwaska, Lucja Iwaska. 1993. Logical reasoning in natural lan- guage: It is all about knowledge. Minds and Ma- chines.</p>
<p>Taxinli: Taking a ride up the NLU hill. Pratik Joshi, Somak Aditya, Aalok Sathe, Monojit Choudhury, CoRRPratik Joshi, Somak Aditya, Aalok Sathe, and Mono- jit Choudhury. 2020. Taxinli: Taking a ride up the NLU hill. CoRR.</p>
<p>Large language models are zero-shot reasoners. Takeshi Kojima, Shane Shixiang, Machel Gu, Yutaka Reid, Yusuke Matsuo, Iwasawa, Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yu- taka Matsuo, and Yusuke Iwasawa. 2023. Large lan- guage models are zero-shot reasoners.</p>
<p>Logic for problem solving. Robert Kowalski, SantosRobert Kowalski. 1979. Logic for problem solving, vol- ume 7. Ediciones Daz de Santos.</p>
<p>Augmenting neural networks with first-order logic. Tao Li, Vivek Srikumar, 10.18653/v1/P19-1028Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. the 57th Annual Meeting of the Association for Computational LinguisticsFlorence, ItalyAssociation for Computational LinguisticsTao Li and Vivek Srikumar. 2019. Augmenting neu- ral networks with first-order logic. In Proceed- ings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 292-302, Flo- rence, Italy. Association for Computational Linguis- tics.</p>
<p>Natural language inference in context -investigating contextual reasoning over long texts. Hanmeng Liu, Leyang Cui, Jian Liu, Yue Zhang, CoRRHanmeng Liu, Leyang Cui, Jian Liu, and Yue Zhang. 2020. Natural language inference in context -inves- tigating contextual reasoning over long texts. CoRR.</p>
<p>Y Liu, M Ott, N Goyal, J Du, M Joshi, D Chen, O Levy, M Lewis, L Zettlemoyer, V Stoyanov, Roberta: A robustly optimized bert pretraining approach. arXiv. Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis, L. Zettlemoyer, and V. Stoyanov. 2019. Roberta: A robustly optimized bert pretrain- ing approach. arXiv.</p>
<p>Natural logic for textual inference. Bill Maccartney, Christopher D Manning, Proceedings of the ACL-PASCAL Workshop on Textual Entailment and Paraphrasing. the ACL-PASCAL Workshop on Textual Entailment and ParaphrasingBill MacCartney and Christopher D. Manning. 2007a. Natural logic for textual inference. In Proceedings of the ACL-PASCAL Workshop on Textual Entail- ment and Paraphrasing.</p>
<p>Natural logic for textual inference. Bill Maccartney, D Christopher, Manning, Proceedings of the ACL-PASCAL Workshop on Textual Entailment and Paraphrasing. the ACL-PASCAL Workshop on Textual Entailment and ParaphrasingBill MacCartney and Christopher D Manning. 2007b. Natural logic for textual inference. In Proceedings of the ACL-PASCAL Workshop on Textual Entail- ment and Paraphrasing.</p>
<p>. OpenAI. 2023. Gpt-4 technical report. OpenAI. 2023. Gpt-4 technical report.</p>
<p>Is chatgpt a general-purpose natural language processing task solver?. Chengwei Qin, Aston Zhang, Zhuosheng Zhang, Jiaao Chen, Michihiro Yasunaga, Diyi Yang, Chengwei Qin, Aston Zhang, Zhuosheng Zhang, Jiaao Chen, Michihiro Yasunaga, and Diyi Yang. 2023. Is chatgpt a general-purpose natural language process- ing task solver?</p>
<p>Exploring the limits of transfer learning with a unified text-to-text transformer. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, CoRRColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2019. Exploring the limits of transfer learning with a unified text-to-text trans- former. CoRR.</p>
<p>Conjnli: Natural language inference over conjunctive sentences. Swarnadeep Saha, Yixin Nie, Mohit Bansal, CoRRSwarnadeep Saha, Yixin Nie, and Mohit Bansal. 2020. Conjnli: Natural language inference over conjunc- tive sentences. CoRR.</p>
<p>. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. CoRRAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. CoRR.</p>
<p>GLUE: A multi-task benchmark and analysis platform for natural language understanding. Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, Samuel Bowman, Proceedings of the 2018 EMNLP Workshop Black-boxNLP: Analyzing and Interpreting Neural Networks for NLP. the 2018 EMNLP Workshop Black-boxNLP: Analyzing and Interpreting Neural Networks for NLPAlex Wang, Amanpreet Singh, Julian Michael, Fe- lix Hill, Omer Levy, and Samuel Bowman. 2018. GLUE: A multi-task benchmark and analysis plat- form for natural language understanding. In Pro- ceedings of the 2018 EMNLP Workshop Black- boxNLP: Analyzing and Interpreting Neural Net- works for NLP.</p>
<p>From lsat: The progress and challenges of complex reasoning. Siyuan Wang, Zhongkun Liu, Wanjun Zhong, Ming Zhou, Zhongyu Wei, Zhumin Chen, Nan Duan, Speech, and Language Processing. Siyuan Wang, Zhongkun Liu, Wanjun Zhong, Ming Zhou, Zhongyu Wei, Zhumin Chen, and Nan Duan. 2022. From lsat: The progress and challenges of complex reasoning. IEEE/ACM Transactions on Au- dio, Speech, and Language Processing.</p>
<p>A broad-coverage challenge corpus for sentence understanding through inference. Adina Williams, Nikita Nangia, Samuel Bowman, Proc. of NAACL. of NAACLAdina Williams, Nikita Nangia, and Samuel Bowman. 2018a. A broad-coverage challenge corpus for sen- tence understanding through inference. In Proc. of NAACL.</p>
<p>A broad-coverage challenge corpus for sentence understanding through inference. Adina Williams, Nikita Nangia, Samuel Bowman, 10.18653/v1/N18-1101Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesNew Orleans, LouisianaLong Papers1Association for Computational LinguisticsAdina Williams, Nikita Nangia, and Samuel Bowman. 2018b. A broad-coverage challenge corpus for sen- tence understanding through inference. In Proceed- ings of the 2018 Conference of the North American Chapter of the Association for Computational Lin- guistics: Human Language Technologies, Volume 1 (Long Papers), pages 1112-1122, New Orleans, Louisiana. Association for Computational Linguis- tics.</p>
<p>Help: A dataset for identifying shortcomings of neural models in monotonicity reasoning. Hitomi Yanaka, Koji Mineshima, Daisuke Bekki, Kentaro Inui, Satoshi Sekine, Johan Bos, Proceedings of the Eighth Joint Conference on Lexical and Computational Semantics (<em>SEM2019). the Eighth Joint Conference on Lexical and Computational Semantics (</em>SEM2019)Hitomi Yanaka, Koji Mineshima, Daisuke Bekki, Ken- taro Inui, Satoshi Sekine, , and Johan Bos. 2019a. Help: A dataset for identifying shortcomings of neu- ral models in monotonicity reasoning. In Proceed- ings of the Eighth Joint Conference on Lexical and Computational Semantics (*SEM2019).</p>
<p>Can neural networks understand monotonicity reasoning?. Hitomi Yanaka, Koji Mineshima, Daisuke Bekki, Kentaro Inui, Satoshi Sekine, Lasha Abzianidze, Johan Bos, Hitomi Yanaka, Koji Mineshima, Daisuke Bekki, Ken- taro Inui, Satoshi Sekine, Lasha Abzianidze, and Jo- han Bos. 2019b. Can neural networks understand monotonicity reasoning?</p>            </div>
        </div>

    </div>
</body>
</html>